[
    {
        "url": "https://arxiv.org/abs/1001.0063",
        "title": "On a Model for Integrated Information",
        "authors": [
            "Alessandro Epasto",
            "Enrico Nardelli"
        ],
        "abstract": "  In this paper we give a thorough presentation of a model proposed by Tononi et al. for modeling \\emph{integrated information}, i.e. how much information is generated in a system transitioning from one state to the next one by the causal interaction of its parts and \\emph{above and beyond} the information given by the sum of its parts. We also provides a more general formulation of such a model, independent from the time chosen for the analysis and from the uniformity of the probability distribution at the initial time instant. Finally, we prove that integrated information is null for disconnected systems.\n    ",
        "submission_date": "2009-12-31T00:00:00",
        "last_modified_date": "2009-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.0820",
        "title": "Abstract Answer Set Solvers with Learning",
        "authors": [
            "Yuliya Lierler"
        ],
        "abstract": "  Nieuwenhuis, Oliveras, and Tinelli (2006) showed how to describe enhancements of the Davis-Putnam-Logemann-Loveland algorithm using transition systems, instead of pseudocode. We design a similar framework for several algorithms that generate answer sets for logic programs: Smodels, Smodels-cc, Asp-Sat with Learning (Cmodels), and a newly designed and implemented algorithm Sup. This approach to describing answer set solvers makes it easier to prove their correctness, to compare them, and to design new systems.\n    ",
        "submission_date": "2010-01-06T00:00:00",
        "last_modified_date": "2010-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.0921",
        "title": "Graph Quantization",
        "authors": [
            "Brijnesh J. Jain",
            "Klaus Obermayer"
        ],
        "abstract": "  Vector quantization(VQ) is a lossy data compression technique from signal processing, which is restricted to feature vectors and therefore inapplicable for combinatorial structures. This contribution presents a theoretical foundation of graph quantization (GQ) that extends VQ to the domain of attributed graphs. We present the necessary Lloyd-Max conditions for optimality of a graph quantizer and consistency results for optimal GQ design based on empirical distortion measures and stochastic optimization. These results statistically justify existing clustering algorithms in the domain of graphs. The proposed approach provides a template of how to link structural pattern recognition methods other than GQ to statistical pattern recognition.\n    ",
        "submission_date": "2010-01-06T00:00:00",
        "last_modified_date": "2010-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.1257",
        "title": "Decisional Processes with Boolean Neural Network: the Emergence of Mental Schemes",
        "authors": [
            "Graziano Barnabei",
            "Franco Bagnoli",
            "Ciro Conversano",
            "Elena Lensi"
        ],
        "abstract": "  Human decisional processes result from the employment of selected quantities of relevant information, generally synthesized from environmental incoming data and stored memories. Their main goal is the production of an appropriate and adaptive response to a cognitive or behavioral task. Different strategies of response production can be adopted, among which haphazard trials, formation of mental schemes and heuristics. In this paper, we propose a model of Boolean neural network that incorporates these strategies by recurring to global optimization strategies during the learning session. The model characterizes as well the passage from an unstructured/chaotic attractor neural network typical of data-driven processes to a faster one, forward-only and representative of schema-driven processes. Moreover, a simplified version of the Iowa Gambling Task (IGT) is introduced in order to test the model. Our results match with experimental data and point out some relevant knowledge coming from psychological domain.\n    ",
        "submission_date": "2010-01-08T00:00:00",
        "last_modified_date": "2010-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.1401",
        "title": "Incorporating characteristics of human creativity into an evolutionary art algorithm",
        "authors": [
            "Steve DiPaola",
            "Liane Gabora"
        ],
        "abstract": "A perceived limitation of evolutionary art and design algorithms is that they rely on human intervention; the artist selects the most aesthetically pleasing variants of one generation to produce the next. This paper discusses how computer generated art and design can become more creatively human-like with respect to both process and outcome. As an example of a step in this direction, we present an algorithm that overcomes the above limitation by employing an automatic fitness function. The goal is to evolve abstract portraits of Darwin, using our 2nd generation fitness function which rewards genomes that not just produce a likeness of Darwin but exhibit certain strategies characteristic of human artists. We note that in human creativity, change is less choosing amongst randomly generated variants and more capitalizing on the associative structure of a conceptual network to hone in on a vision. We discuss how to achieve this fluidity algorithmically.\n    ",
        "submission_date": "2010-01-09T00:00:00",
        "last_modified_date": "2019-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.1836",
        "title": "Web-Based Expert System for Civil Service Regulations: RCSES",
        "authors": [
            "Mofreh Hogo",
            "Khaled Fouad",
            "Fouad Mousa"
        ],
        "abstract": "  Internet and expert systems have offered new ways of sharing and distributing knowledge, but there is a lack of researches in the area of web based expert systems. This paper introduces a development of a web-based expert system for the regulations of civil service in the Kingdom of Saudi Arabia named as RCSES. It is the first time to develop such system (application of civil service regulations) as well the development of it using web based approach. The proposed system considers 17 regulations of the civil service system. The different phases of developing the RCSES system are presented, as knowledge acquiring and selection, ontology and knowledge representations using XML format. XML Rule-based knowledge sources and the inference mechanisms were implemented using ",
        "submission_date": "2010-01-12T00:00:00",
        "last_modified_date": "2010-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.1979",
        "title": "ICD 10 Based Medical Expert System Using Fuzzy Temporal Logic",
        "authors": [
            "P.Chinniah",
            "Dr.S.Muttan"
        ],
        "abstract": "  Medical diagnosis process involves many levels and considerable amount of time and money are invariably spent for the first level of diagnosis usually made by the physician for all the patients every time. Hence there is a need for a computer based system which not only asks relevant questions to the patients but also aids the physician by giving a set of possible diseases from the symptoms obtained using logic at inference. In this work, an ICD10 based Medical Expert System that provides advice, information and recommendation to the physician using fuzzy temporal logic. The knowledge base used in this system consists of facts of symptoms and rules on diseases. It also provides fuzzy severity scale and weight factor for symptom and disease and can vary with respect to time. The system generates the possible disease conditions based on modified Euclidean metric using Elders algorithm for effective clustering. The minimum similarity value is used as the decision parameter to identify a disease.\n    ",
        "submission_date": "2010-01-12T00:00:00",
        "last_modified_date": "2010-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.2155",
        "title": "Cooperative Automated Worm Response and Detection Immune Algorithm",
        "authors": [
            "Jungwon Kim",
            "William Wilson",
            "Uwe Aickelin",
            "Julie McLeod"
        ],
        "abstract": "  The role of T-cells within the immune system is to confirm and assess anomalous situations and then either respond to or tolerate the source of the effect. To illustrate how these mechanisms can be harnessed to solve real-world problems, we present the blueprint of a T-cell inspired algorithm for computer security worm detection. We show how the three central T-cell processes, namely T-cell maturation, differentiation and proliferation, naturally map into this domain and further illustrate how such an algorithm fits into a complete immune inspired computer security system and framework.\n    ",
        "submission_date": "2010-01-13T00:00:00",
        "last_modified_date": "2010-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.2170",
        "title": "Comparing Simulation Output Accuracy of Discrete Event and Agent Based Models: A Quantitive Approach",
        "authors": [
            "Mazlina Abdul Majid",
            "Uwe Aickelin",
            "Peer-Olaf Siebers"
        ],
        "abstract": "  In our research we investigate the output accuracy of discrete event simulation models and agent based simulation models when studying human centric complex systems. In this paper we focus on human reactive behaviour as it is possible in both modelling approaches to implement human reactive behaviour in the model by using standard methods. As a case study we have chosen the retail sector, and here in particular the operations of the fitting room in the women wear department of a large UK department store. In our case study we looked at ways of determining the efficiency of implementing new management policies for the fitting room operation through modelling the reactive behaviour of staff and customers of the department. First, we have carried out a validation experiment in which we compared the results from our models to the performance of the real system. This experiment also allowed us to establish differences in output accuracy between the two modelling methids. In a second step a multi-scenario experiment was carried out to study the behaviour of the models when they are used for the purpose of operational improvement. Overall we have found that for our case study example both discrete event simulation and agent based simulation have the same potential to support the investigation into the efficiency of implementing new management policies.\n    ",
        "submission_date": "2010-01-13T00:00:00",
        "last_modified_date": "2010-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.2195",
        "title": "DCA for Bot Detection",
        "authors": [
            "Yousof Al-Hammadi",
            "Uwe Aickelin",
            "Julie Greensmith"
        ],
        "abstract": "  Ensuring the security of computers is a non-trivial task, with many techniques used by malicious users to compromise these systems. In recent years a new threat has emerged in the form of networks of hijacked zombie machines used to perform complex distributed attacks such as denial of service and to obtain sensitive data such as password information. These zombie machines are said to be infected with a 'bot' - a malicious piece of software which is installed on a host machine and is controlled by a remote attacker, termed the 'botmaster of a botnet'. In this work, we use the biologically inspired Dendritic Cell Algorithm (DCA) to detect the existence of a single bot on a compromised host machine. The DCA is an immune-inspired algorithm based on an abstract model of the behaviour of the dendritic cells of the human body. The basis of anomaly detection performed by the DCA is facilitated using the correlation of behavioural attributes such as keylogging and packet flooding behaviour. The results of the application of the DCA to the detection of a single bot show that the algorithm is a successful technique for the detection of such malicious software without responding to normally running programs.\n    ",
        "submission_date": "2010-01-13T00:00:00",
        "last_modified_date": "2010-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.2208",
        "title": "Biological Inspiration for Artificial Immune Systems",
        "authors": [
            "Jamie Twycross",
            "Uwe Aickelin"
        ],
        "abstract": "  Artificial immune systems (AISs) to date have generally been inspired by naive biological metaphors. This has limited the effectiveness of these systems. In this position paper two ways in which AISs could be made more biologically realistic are discussed. We propose that AISs should draw their inspiration from organisms which possess only innate immune systems, and that AISs should employ systemic models of the immune system to structure their overall design. An outline of plant and invertebrate immune systems is presented, and a number of contemporary research that more biologically-realistic AISs could have is also discussed.\n    ",
        "submission_date": "2010-01-13T00:00:00",
        "last_modified_date": "2010-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.2277",
        "title": "Application of a Fuzzy Programming Technique to Production Planning in the Textile Industry",
        "authors": [
            "I. Elamvazuthi",
            "T. Ganesan",
            "P. Vasant",
            "J. F. Webb"
        ],
        "abstract": "  Many engineering optimization problems can be considered as linear programming problems where all or some of the parameters involved are linguistic in nature. These can only be quantified using fuzzy sets. The aim of this paper is to solve a fuzzy linear programming problem in which the parameters involved are fuzzy quantities with logistic membership functions. To explore the applicability of the method a numerical example is considered to determine the monthly production planning quotas and profit of a home textile group.\n    ",
        "submission_date": "2010-01-13T00:00:00",
        "last_modified_date": "2010-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.2279",
        "title": "The Application of Mamdani Fuzzy Model for Auto Zoom Function of a Digital Camera",
        "authors": [
            "I. Elamvazuthi",
            "P. Vasant",
            "J. F. Webb"
        ],
        "abstract": "  Mamdani Fuzzy Model is an important technique in Computational Intelligence (CI) study. This paper presents an implementation of a supervised learning method based on membership function training in the context of Mamdani fuzzy models. Specifically, auto zoom function of a digital camera is modelled using Mamdani technique. The performance of control method is verified through a series of simulation and numerical results are provided as illustrations.\n    ",
        "submission_date": "2010-01-13T00:00:00",
        "last_modified_date": "2010-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.2405",
        "title": "Dendritic Cells for Real-Time Anomaly Detection",
        "authors": [
            "Julie Greensmith",
            "Uwe Aickelin"
        ],
        "abstract": "  Dendritic Cells (DCs) are innate immune system cells which have the power to activate or suppress the immune system. The behaviour of human of human DCs is abstracted to form an algorithm suitable for anomaly detection. We test this algorithm on the real-time problem of port scan detection. Our results show a significant difference in artificial DC behaviour for an outgoing portscan when compared to behaviour for normal processes.\n    ",
        "submission_date": "2010-01-14T00:00:00",
        "last_modified_date": "2010-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.2411",
        "title": "Dendritic Cells for Anomaly Detection",
        "authors": [
            "Julie Greensmith",
            "Jamie Twycross",
            "Uwe Aickelin"
        ],
        "abstract": "  Artificial immune systems, more specifically the negative selection algorithm, have previously been applied to intrusion detection. The aim of this research is to develop an intrusion detection system based on a novel concept in immunology, the Danger Theory. Dendritic Cells (DCs) are antigen presenting cells and key to the activation of the human signals from the host tissue and correlate these signals with proteins know as antigens. In algorithmic terms, individual DCs perform multi-sensor data fusion based on time-windows. The whole population of DCs asynchronously correlates the fused signals with a secondary data stream. The behaviour of human DCs is abstracted to form the DC Algorithm (DCA), which is implemented using an immune inspired framework, libtissue. This system is used to detect context switching for a basic machine learning dataset and to detect outgoing portscans in real-time. Experimental results show a significant difference between an outgoing portscan and normal traffic.\n    ",
        "submission_date": "2010-01-14T00:00:00",
        "last_modified_date": "2010-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.2665",
        "title": "Detecting Botnets Through Log Correlation",
        "authors": [
            "Yousof Al-Hammadi",
            "Uwe Aickelin"
        ],
        "abstract": "  Botnets, which consist of thousands of compromised machines, can cause significant threats to other systems by launching Distributed Denial of Service (SSoS) attacks, keylogging, and backdoors. In response to these threats, new effective techniques are needed to detect the presence of botnets. In this paper, we have used an interception technique to monitor Windows Application Programming Interface (API) functions calls made by communication applications and store these calls with their arguments in log files. Our algorithm detects botnets based on monitoring abnormal activity by correlating the changes in log file sizes from different hosts.\n    ",
        "submission_date": "2010-01-15T00:00:00",
        "last_modified_date": "2010-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.0102",
        "title": "$\u03b1$-Discounting Multi-Criteria Decision Making ($\u03b1$-D MCDM)",
        "authors": [
            "Florentin Smarandache"
        ],
        "abstract": "In this book we introduce a new procedure called \\alpha-Discounting Method for Multi-Criteria Decision Making (\\alpha-D MCDM), which is as an alternative and extension of Saaty Analytical Hierarchy Process (AHP). It works for any number of preferences that can be transformed into a system of homogeneous linear equations. A degree of consistency (and implicitly a degree of inconsistency) of a decision-making problem are defined. \\alpha-D MCDM is afterwards generalized to a set of preferences that can be transformed into a system of linear and or non-linear homogeneous and or non-homogeneous equations and or inequalities. The general idea of \\alpha-D MCDM is to assign non-null positive parameters \\alpha_1, \\alpha_2, and so on \\alpha_p to the coefficients in the right-hand side of each preference that diminish or increase them in order to transform the above linear homogeneous system of equations which has only the null-solution, into a system having a particular non-null solution. After finding the general solution of this system, the principles used to assign particular values to all parameters \\alpha is the second important part of \\alpha-D, yet to be deeper investigated in the future. In the current book we propose the Fairness Principle, i.e. each coefficient should be discounted with the same percentage (we think this is fair: not making any favoritism or unfairness to any coefficient), but the reader can propose other principles. For consistent decision-making problems with pairwise comparisons, \\alpha-Discounting Method together with the Fairness Principle give the same result as AHP. But for weak inconsistent decision-making problem, \\alpha-Discounting together with the Fairness Principle give a different result from AHP. Many consistent, weak inconsistent, and strong inconsistent examples are given in this book.\n    ",
        "submission_date": "2010-01-31T00:00:00",
        "last_modified_date": "2015-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.0108",
        "title": "Genetic algorithm for robotic telescope scheduling",
        "authors": [
            "Petr Kubanek"
        ],
        "abstract": "  This work was inspired by author experiences with a telescope scheduling. Author long time goal is to develop and further extend software for an autonomous observatory. The software shall provide users with all the facilities they need to take scientific images of the night sky, cooperate with other autonomous observatories, and possibly more. This works shows how genetic algorithm can be used for scheduling of a single observatory, as well as network of observatories.\n    ",
        "submission_date": "2010-01-31T00:00:00",
        "last_modified_date": "2010-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.0134",
        "title": "Constraint solvers: An empirical evaluation of design decisions",
        "authors": [
            "Lars Kotthoff"
        ],
        "abstract": "  This paper presents an evaluation of the design decisions made in four state-of-the-art constraint solvers; Choco, ECLiPSe, Gecode, and Minion. To assess the impact of design decisions, instances of the five problem classes n-Queens, Golomb Ruler, Magic Square, Social Golfers, and Balanced Incomplete Block Design are modelled and solved with each solver. The results of the experiments are not meant to give an indication of the performance of a solver, but rather investigate what influence the choice of algorithms and data structures has.\n",
        "submission_date": "2010-01-31T00:00:00",
        "last_modified_date": "2010-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.0136",
        "title": "Dominion -- A constraint solver generator",
        "authors": [
            "Lars Kotthoff"
        ],
        "abstract": "  This paper proposes a design for a system to generate constraint solvers that are specialised for specific problem models. It describes the design in detail and gives preliminary experimental results showing the feasibility and effectiveness of the approach.\n    ",
        "submission_date": "2010-01-31T00:00:00",
        "last_modified_date": "2010-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.0177",
        "title": "Logical Evaluation of Consciousness: For Incorporating Consciousness into Machine Architecture",
        "authors": [
            "C.N. Padhy",
            "R.R. Panda"
        ],
        "abstract": "  Machine Consciousness is the study of consciousness in a biological, philosophical, mathematical and physical perspective and designing a model that can fit into a programmable system architecture. Prime objective of the study is to make the system architecture behave consciously like a biological model does. Present work has developed a feasible definition of consciousness, that characterizes consciousness with four parameters i.e., parasitic, symbiotic, self referral and reproduction. Present work has also developed a biologically inspired consciousness architecture that has following layers: quantum layer, cellular layer, organ layer and behavioral layer and traced the characteristics of consciousness at each layer. Finally, the work has estimated physical and algorithmic architecture to devise a system that can behave consciously.\n    ",
        "submission_date": "2010-02-01T00:00:00",
        "last_modified_date": "2010-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.0184",
        "title": "Some considerations on how the human brain must be arranged in order to make its replication in a thinking machine possible",
        "authors": [
            "Emanuel Diamant"
        ],
        "abstract": "  For the most of my life, I have earned my living as a computer vision professional busy with image processing tasks and problems. In the computer vision community there is a widespread belief that artificial vision systems faithfully replicate human vision abilities or at least very closely mimic them. It was a great surprise to me when one day I have realized that computer and human vision have next to nothing in common. The former is occupied with extensive data processing, carrying out massive pixel-based calculations, while the latter is busy with meaningful information processing, concerned with smart objects-based manipulations. And the gap between the two is insurmountable. To resolve this confusion, I had had to return and revaluate first the vision phenomenon itself, define more carefully what visual information is and how to treat it properly. In this work I have not been, as it is usually accepted, biologically inspired . On the contrary, I have drawn my inspirations from a pure mathematical theory, the Kolmogorov s complexity theory. The results of my work have been already published elsewhere. So the objective of this paper is to try and apply the insights gained in course of this my enterprise to a more general case of information processing in human brain and the challenging issue of human intelligence.\n    ",
        "submission_date": "2010-02-01T00:00:00",
        "last_modified_date": "2010-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.0276",
        "title": "Dendritic Cells for SYN Scan Detection",
        "authors": [
            "Julie Greensmith",
            "Uwe Aickelin"
        ],
        "abstract": "  Artificial immune systems have previously been applied to the problem of intrusion detection. The aim of this research is to develop an intrusion detection system based on the function of Dendritic Cells (DCs). DCs are antigen presenting cells and key to activation of the human immune system, behaviour which has been abstracted to form the Dendritic Cell Algorithm (DCA). In algorithmic terms, individual DCs perform multi-sensor data fusion, asynchronously correlating the the fused data signals with a secondary data stream. Aggregate output of a population of cells, is analysed and forms the basis of an anomaly detection system. In this paper the DCA is applied to the detection of outgoing port scans using TCP SYN packets. Results show that detection can be achieved with the DCA, yet some false positives can be encountered when simultaneously scanning and using other network services. Suggestions are made for using adaptive signals to alleviate this uncovered problem.\n    ",
        "submission_date": "2010-02-01T00:00:00",
        "last_modified_date": "2010-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.0432",
        "title": "Detecting Motifs in System Call Sequences",
        "authors": [
            "William O. Wilson",
            "Jan Feyereisl",
            "Uwe Aickelin"
        ],
        "abstract": "  The search for patterns or motifs in data represents an area of key interest to many researchers. In this paper we present the Motif Tracking Algorithm, a novel immune inspired pattern identification tool that is able to identify unknown motifs which repeat within time series data. The power of the algorithm is derived from its use of a small number of parameters with minimal assumptions. The algorithm searches from a completely neutral perspective that is independent of the data being analysed, and the underlying motifs. In this paper the motif tracking algorithm is applied to the search for patterns within sequences of low level system calls between the Linux kernel and the operating system's user space. The MTA is able to compress data found in large system call data sets to a limited number of motifs which summarise that data. The motifs provide a resource from which a profile of executed processes can be built. The potential for these profiles and new implications for security research are highlighted. A higher level call system language for measuring similarity between patterns of such calls is also suggested.\n    ",
        "submission_date": "2010-02-02T00:00:00",
        "last_modified_date": "2010-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.0449",
        "title": "Some improved results on communication between information systems",
        "authors": [
            "Ping Zhu",
            "Qiaoyan Wen"
        ],
        "abstract": "To study the communication between information systems, Wang et al. [C. Wang, C. Wu, D. Chen, Q. Hu, and C. Wu, Communicating between information systems, Information Sciences 178 (2008) 3228-3239] proposed two concepts of type-1 and type-2 consistent functions. Some properties of such functions and induced relation mappings have been investigated there. In this paper, we provide an improvement of the aforementioned work by disclosing the symmetric relationship between type-1 and type-2 consistent functions. We present more properties of consistent functions and induced relation mappings and improve upon several deficient assertions in the original work. In particular, we unify and extend type-1 and type-2 consistent functions into the so-called neighborhood-consistent functions. This provides a convenient means for studying the communication between information systems based on various neighborhoods.\n    ",
        "submission_date": "2010-02-02T00:00:00",
        "last_modified_date": "2010-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.0696",
        "title": "Detecting Danger: Applying a Novel Immunological Concept to Intrusion Detection Systems",
        "authors": [
            "Julie Greensmith",
            "Uwe Aickelin",
            "Jamie Twycross"
        ],
        "abstract": "  In recent years computer systems have become increasingly complex and consequently the challenge of protecting these systems has become increasingly difficult. Various techniques have been implemented to counteract the misuse of computer systems in the form of firewalls, anti-virus software and intrusion detection systems. The complexity of networks and dynamic nature of computer systems leaves current methods with significant room for improvement. Computer scientists have recently drawn inspiration from mechanisms found in biological systems and, in the context of computer security, have focused on the human immune system (HIS). The human immune system provides a high level of protection from constant attacks. By examining the precise mechanisms of the human immune system, it is hoped the paradigm will improve the performance of real intrusion detection systems. This paper presents an introduction to recent developments in the field of immunology. It discusses the incorporation of a novel immunological paradigm, Danger Theory, and how this concept is inspiring artificial immune systems (AIS). Applications within the context of computer security are outlined drawing direct reference to the underlying principles of Danger Theory and finally, the current state of intrusion detection systems is discussed and improvements suggested.\n    ",
        "submission_date": "2010-02-03T00:00:00",
        "last_modified_date": "2010-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.0908",
        "title": "Homomorphisms between fuzzy information systems revisited",
        "authors": [
            "Ping Zhu",
            "Qiaoyan Wen"
        ],
        "abstract": "  Recently, Wang et al. discussed the properties of fuzzy information systems under homomorphisms in the paper [C. Wang, D. Chen, L. Zhu, Homomorphisms between fuzzy information systems, Applied Mathematics Letters 22 (2009) 1045-1050], where homomorphisms are based upon the concepts of consistent functions and fuzzy relation mappings. In this paper, we classify consistent functions as predecessor-consistent and successor-consistent, and then proceed to present more properties of consistent functions. In addition, we improve some characterizations of fuzzy relation mappings provided by Wang et al.\n    ",
        "submission_date": "2010-02-04T00:00:00",
        "last_modified_date": "2010-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.1157",
        "title": "Establishment of Relationships between Material Design and Product Design Domains by Hybrid FEM-ANN Technique",
        "authors": [
            "K. Soorya Prakash",
            "S. S. Mohamed Nazirudeen",
            "M. Joseph Malvin Raj"
        ],
        "abstract": "  In this paper, research on AI based modeling technique to optimize development of new alloys with necessitated improvements in properties and chemical mixture over existing alloys as per functional requirements of product is done. The current research work novels AI in lieu of predictions to establish association between material and product customary. Advanced computational simulation techniques like CFD, FEA interrogations are made viable to authenticate product dynamics in context to experimental investigations. Accordingly, the current research is focused towards binding relationships between material design and product design domains. The input to feed forward back propagation prediction network model constitutes of material design features. Parameters relevant to product design strategies are furnished as target outputs. The outcomes of ANN shows good sign of correlation between material and product design domains. The study enriches a new path to illustrate material factors at the time of new product development.\n    ",
        "submission_date": "2010-02-05T00:00:00",
        "last_modified_date": "2010-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.1480",
        "title": "A Minimum Relative Entropy Controller for Undiscounted Markov Decision Processes",
        "authors": [
            "Pedro A. Ortega",
            "Daniel A. Braun"
        ],
        "abstract": "  Adaptive control problems are notoriously difficult to solve even in the presence of plant-specific controllers. One way to by-pass the intractable computation of the optimal policy is to restate the adaptive control as the minimization of the relative entropy of a controller that ignores the true plant dynamics from an informed controller. The solution is given by the Bayesian control rule-a set of equations characterizing a stochastic adaptive controller for the class of possible plant dynamics. Here, the Bayesian control rule is applied to derive BCR-MDP, a controller to solve undiscounted Markov decision processes with finite state and action spaces and unknown dynamics. In particular, we derive a non-parametric conjugate prior distribution over the policy space that encapsulates the agent's whole relevant history and we present a Gibbs sampler to draw random policies from this distribution. Preliminary results show that BCR-MDP successfully avoids sub-optimal limit cycles due to its built-in mechanism to balance exploration versus exploitation.\n    ",
        "submission_date": "2010-02-07T00:00:00",
        "last_modified_date": "2010-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.2034",
        "title": "Dire n'est pas concevoir",
        "authors": [
            "Christophe Roche"
        ],
        "abstract": "  The conceptual modelling built from text is rarely an ontology. As a matter of fact, such a conceptualization is corpus-dependent and does not offer the main properties we expect from ontology. Furthermore, ontology extracted from text in general does not match ontology defined by expert using a formal language. It is not surprising since ontology is an extra-linguistic conceptualization whereas knowledge extracted from text is the concern of textual linguistics. Incompleteness of text and using rhetorical figures, like ellipsis, modify the perception of the conceptualization we may have. Ontological knowledge, which is necessary for text understanding, is not in general embedded into documents.\n    ",
        "submission_date": "2010-02-10T00:00:00",
        "last_modified_date": "2010-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.2202",
        "title": "Modeling of Human Criminal Behavior using Probabilistic Networks",
        "authors": [
            "Ramesh Kumar Gopala Pillai",
            "Dr. Ramakanth Kumar .P"
        ],
        "abstract": "  Currently, criminals profile (CP) is obtained from investigators or forensic psychologists interpretation, linking crime scene characteristics and an offenders behavior to his or her characteristics and psychological profile. This paper seeks an efficient and systematic discovery of nonobvious and valuable patterns between variables from a large database of solved cases via a probabilistic network (PN) modeling approach. The PN structure can be used to extract behavioral patterns and to gain insight into what factors influence these behaviors. Thus, when a new case is being investigated and the profile variables are unknown because the offender has yet to be identified, the observed crime scene variables are used to infer the unknown variables based on their connections in the structure and the corresponding numerical (probabilistic) weights. The objective is to produce a more systematic and empirical approach to profiling, and to use the resulting PN model as a decision tool.\n    ",
        "submission_date": "2010-02-10T00:00:00",
        "last_modified_date": "2010-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.2897",
        "title": "Model-Driven Constraint Programming",
        "authors": [
            "Raphael Chenouard",
            "Laurent Granvilliers",
            "Ricardo Soto"
        ],
        "abstract": "  Constraint programming can definitely be seen as a model-driven paradigm. The users write programs for modeling problems. These programs are mapped to executable models to calculate the solutions. This paper focuses on efficient model management (definition and transformation). From this point of view, we propose to revisit the design of constraint-programming systems. A model-driven architecture is introduced to map solving-independent constraint models to solving-dependent decision models. Several important questions are examined, such as the need for a visual highlevel modeling language, and the quality of metamodeling techniques to implement the transformations. A main result is the s-COMMA platform that efficiently implements the chain from modeling to solving constraint problems\n    ",
        "submission_date": "2010-02-15T00:00:00",
        "last_modified_date": "2010-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.3023",
        "title": "Rewriting Constraint Models with Metamodels",
        "authors": [
            "Raphael Chenouard",
            "Laurent Granvilliers",
            "Ricardo Soto"
        ],
        "abstract": "  An important challenge in constraint programming is to rewrite constraint models into executable programs calculat- ing the solutions. This phase of constraint processing may require translations between constraint programming lan- guages, transformations of constraint representations, model optimizations, and tuning of solving strategies. In this paper, we introduce a pivot metamodel describing the common fea- tures of constraint models including different kinds of con- straints, statements like conditionals and loops, and other first-class elements like object classes and predicates. This metamodel is general enough to cope with the constructions of many languages, from object-oriented modeling languages to logic languages, but it is independent from them. The rewriting operations manipulate metamodel instances apart from languages. As a consequence, the rewriting operations apply whatever languages are selected and they are able to manage model semantic information. A bridge is created between the metamodel space and languages using parsing techniques. Tools from the software engineering world can be useful to implement this framework.\n    ",
        "submission_date": "2010-02-16T00:00:00",
        "last_modified_date": "2010-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.3078",
        "title": "Using ATL to define advanced and flexible constraint model transformations",
        "authors": [
            "Raphael Chenouard",
            "Laurent Granvilliers",
            "Ricardo Soto"
        ],
        "abstract": "  Transforming constraint models is an important task in re- cent constraint programming systems. User-understandable models are defined during the modeling phase but rewriting or tuning them is manda- tory to get solving-efficient models. We propose a new architecture al- lowing to define bridges between any (modeling or solver) languages and to implement model optimizations. This architecture follows a model- driven approach where the constraint modeling process is seen as a set of model transformations. Among others, an interesting feature is the def- inition of transformations as concept-oriented rules, i.e. based on types of model elements where the types are organized into a hierarchy called a metamodel.\n    ",
        "submission_date": "2010-02-16T00:00:00",
        "last_modified_date": "2010-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.3086",
        "title": "Convergence of Bayesian Control Rule",
        "authors": [
            "Pedro A. Ortega",
            "Daniel A. Braun"
        ],
        "abstract": "  Recently, new approaches to adaptive control have sought to reformulate the problem as a minimization of a relative entropy criterion to obtain tractable solutions. In particular, it has been shown that minimizing the expected deviation from the causal input-output dependencies of the true plant leads to a new promising stochastic control rule called the Bayesian control rule. This work proves the convergence of the Bayesian control rule under two sufficient assumptions: boundedness, which is an ergodicity condition; and consistency, which is an instantiation of the sure-thing principle.\n    ",
        "submission_date": "2010-02-16T00:00:00",
        "last_modified_date": "2010-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.3195",
        "title": "Efficiently Discovering Hammock Paths from Induced Similarity Networks",
        "authors": [
            "M. Shahriar Hossain",
            "Michael Narayan",
            "Naren Ramakrishnan"
        ],
        "abstract": "  Similarity networks are important abstractions in many information management applications such as recommender systems, corpora analysis, and medical informatics. For instance, by inducing similarity networks between movies rated similarly by users, or between documents containing common terms, and or between clinical trials involving the same themes, we can aim to find the global structure of connectivities underlying the data, and use the network as a basis to make connections between seemingly disparate entities. In the above applications, composing similarities between objects of interest finds uses in serendipitous recommendation, in storytelling, and in clinical diagnosis, respectively. We present an algorithmic framework for traversing similarity paths using the notion of `hammock' paths which are generalization of traditional paths. Our framework is exploratory in nature so that, given starting and ending objects of interest, it explores candidate objects for path following, and heuristics to admissibly estimate the potential for paths to lead to a desired destination. We present three diverse applications: exploring movie similarities in the Netflix dataset, exploring abstract similarities across the PubMed corpus, and exploring description similarities in a database of clinical trials. Experimental results demonstrate the potential of our approach for unstructured knowledge discovery in similarity networks.\n    ",
        "submission_date": "2010-02-17T00:00:00",
        "last_modified_date": "2010-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.3307",
        "title": "Graph Zeta Function in the Bethe Free Energy and Loopy Belief Propagation",
        "authors": [
            "Yusuke Watanabe",
            "Kenji Fukumizu"
        ],
        "abstract": "  We propose a new approach to the analysis of Loopy Belief Propagation (LBP) by establishing a formula that connects the Hessian of the Bethe free energy with the edge zeta function. The formula has a number of theoretical implications on LBP. It is applied to give a sufficient condition that the Hessian of the Bethe free energy is positive definite, which shows non-convexity for graphs with multiple cycles. The formula clarifies the relation between the local stability of a fixed point of LBP and local minima of the Bethe free energy. We also propose a new approach to the uniqueness of LBP fixed point, and show various conditions of uniqueness.\n    ",
        "submission_date": "2010-02-17T00:00:00",
        "last_modified_date": "2010-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.4522",
        "title": "Feature Importance in Bayesian Assessment of Newborn Brain Maturity from EEG",
        "authors": [
            "L. Jakaite",
            "V. Schetinin",
            "C. Maple"
        ],
        "abstract": "  The methodology of Bayesian Model Averaging (BMA) is applied for assessment of newborn brain maturity from sleep EEG. In theory this methodology provides the most accurate assessments of uncertainty in decisions. However, the existing BMA techniques have been shown providing biased assessments in the absence of some prior information enabling to explore model parameter space in details within a reasonable time. The lack in details leads to disproportional sampling from the posterior distribution. In case of the EEG assessment of brain maturity, BMA results can be biased because of the absence of information about EEG feature importance. In this paper we explore how the posterior information about EEG features can be used in order to reduce a negative impact of disproportional sampling on BMA performance. We use EEG data recorded from sleeping newborns to test the efficiency of the proposed BMA technique.\n    ",
        "submission_date": "2010-02-24T00:00:00",
        "last_modified_date": "2010-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.0034",
        "title": "A New Understanding of Prediction Markets Via No-Regret Learning",
        "authors": [
            "Yiling Chen",
            "Jennifer Wortman Vaughan"
        ],
        "abstract": "  We explore the striking mathematical connections that exist between market scoring rules, cost function based prediction markets, and no-regret learning. We show that any cost function based prediction market can be interpreted as an algorithm for the commonly studied problem of learning from expert advice by equating trades made in the market with losses observed by the learning algorithm. If the loss of the market organizer is bounded, this bound can be used to derive an O(sqrt(T)) regret bound for the corresponding learning algorithm. We then show that the class of markets with convex cost functions exactly corresponds to the class of Follow the Regularized Leader learning algorithms, with the choice of a cost function in the market corresponding to the choice of a regularizer in the learning problem. Finally, we show an equivalence between market scoring rules and prediction markets with convex cost functions. This implies that market scoring rules can also be interpreted naturally as Follow the Regularized Leader algorithms, and may be of independent interest. These connections provide new insight into how it is that commonly studied markets, such as the Logarithmic Market Scoring Rule, can aggregate opinions into accurate estimates of the likelihood of future events.\n    ",
        "submission_date": "2010-02-26T00:00:00",
        "last_modified_date": "2010-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.0319",
        "title": "Further Exploration of the Dendritic Cell Algorithm: Antigen Multiplier and Time Windows",
        "authors": [
            "Feng Gu",
            "Julie Greensmith",
            "Uwe Aickelin"
        ],
        "abstract": "  As an immune-inspired algorithm, the Dendritic Cell Algorithm (DCA), produces promising performances in the field of anomaly detection. This paper presents the application of the DCA to a standard data set, the KDD 99 data set. The results of different implementation versions of the DXA, including the antigen multiplier and moving time windows are reported. The real-valued Negative Selection Algorithm (NSA) using constant-sized detectors and the C4.5 decision tree algorithm are used, to conduct a baseline comparison. The results suggest that the DCA is applicable to KDD 99 data set, and the antigen multiplier and moving time windows have the same effect on the DCA for this particular data set. The real-valued NSA with constant-sized detectors is not applicable to the data set, and the C4.5 decision tree algorithm provides a benchmark of the classification performance for this data set.\n    ",
        "submission_date": "2010-03-01T00:00:00",
        "last_modified_date": "2010-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.0339",
        "title": "libtissue - implementing innate immunity",
        "authors": [
            "Jamie Twycross",
            "Uwe Aickelin"
        ],
        "abstract": "  In a previous paper the authors argued the case for incorporating ideas from innate immunity into articficial immune systems (AISs) and presented an outline for a conceptual framework for such systems. A number of key general properties observed in the biological innate and adaptive immune systems were hughlighted, and how such properties might be instantiated in artificial systems was discussed in detail. The next logical step is to take these ideas and build a software system with which AISs with these properties can be implemented and experimentally evaluated. This paper reports on the results of that step - the libtissue system.\n    ",
        "submission_date": "2010-03-01T00:00:00",
        "last_modified_date": "2010-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.0404",
        "title": "Exploration Of The Dendritic Cell Algorithm Using The Duration Calculus",
        "authors": [
            "Feng Gu",
            "Julie Greensmith",
            "Uwe Aickelin"
        ],
        "abstract": "  As one of the newest members in Artificial Immune Systems (AIS), the Dendritic Cell Algorithm (DCA) has been applied to a range of problems. These applications mainly belong to the field of anomaly detection. However, real-time detection, a new challenge to anomaly detection, requires improvement on the real-time capability of the DCA. To assess such capability, formal methods in the research of rea-time systems can be employed. The findings of the assessment can provide guideline for the future development of the algorithm. Therefore, in this paper we use an interval logic based method, named the Duration Calculus (DC), to specify a simplified single-cell model of the DCA. Based on the DC specifications with further induction, we find that each individual cell in the DCA can perform its function as a detector in real-time. Since the DCA can be seen as many such cells operating in parallel, it is potentially capable of performing real-time detection. However, the analysis process of the standard DCA constricts its real-time capability. As a result, we conclude that the analysis process of the standard DCA should be replaced by a real-time analysis component, which can perform periodic analysis for the purpose of real-time detection.\n    ",
        "submission_date": "2010-03-01T00:00:00",
        "last_modified_date": "2010-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.0590",
        "title": "A new model for solution of complex distributed constrained problems",
        "authors": [
            "Sami Al-Maqtari",
            "Habib Abdulrab",
            "Eduard Babkin"
        ],
        "abstract": "  In this paper we describe an original computational model for solving different types of Distributed Constraint Satisfaction Problems (DCSP). The proposed model is called Controller-Agents for Constraints Solving (CACS). This model is intended to be used which is an emerged field from the integration between two paradigms of different nature: Multi-Agent Systems (MAS) and the Constraint Satisfaction Problem paradigm (CSP) where all constraints are treated in central manner as a black-box. This model allows grouping constraints to form a subset that will be treated together as a local problem inside the controller. Using this model allows also handling non-binary constraints easily and directly so that no translating of constraints into binary ones is needed. This paper presents the implementation outlines of a prototype of DCSP solver, its usage methodology and overview of the CACS application for timetabling problems.\n    ",
        "submission_date": "2010-03-02T00:00:00",
        "last_modified_date": "2010-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.0659",
        "title": "Particle Filtering on the Audio Localization Manifold",
        "authors": [
            "Evan Ettinger",
            "Yoav Freund"
        ],
        "abstract": "  We present a novel particle filtering algorithm for tracking a moving sound source using a microphone array. If there are N microphones in the array, we track all $N \\choose 2$ delays with a single particle filter over time. Since it is known that tracking in high dimensions is rife with difficulties, we instead integrate into our particle filter a model of the low dimensional manifold that these delays lie on. Our manifold model is based off of work on modeling low dimensional manifolds via random projection trees [1]. In addition, we also introduce a new weighting scheme to our particle filtering algorithm based on recent advancements in online learning. We show that our novel TDOA tracking algorithm that integrates a manifold model can greatly outperform standard particle filters on this audio tracking task.\n    ",
        "submission_date": "2010-03-02T00:00:00",
        "last_modified_date": "2010-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.0746",
        "title": "Automatically Discovering Hidden Transformation Chaining Constraints",
        "authors": [
            "Raphael Chenouard",
            "Fr\u00e9d\u00e9ric Jouault"
        ],
        "abstract": "  Model transformations operate on models conforming to precisely defined metamodels. Consequently, it often seems relatively easy to chain them: the output of a transformation may be given as input to a second one if metamodels match. However, this simple rule has some obvious limitations. For instance, a transformation may only use a subset of a metamodel. Therefore, chaining transformations appropriately requires more information. We present here an approach that automatically discovers more detailed information about actual chaining constraints by statically analyzing transformations. The objective is to provide developers who decide to chain transformations with more data on which to base their choices. This approach has been successfully applied to the case of a library of endogenous transformations. They all have the same source and target metamodel but have some hidden chaining constraints. In such a case, the simple metamodel matching rule given above does not provide any useful information.\n    ",
        "submission_date": "2010-03-03T00:00:00",
        "last_modified_date": "2010-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.0789",
        "title": "Information Fusion for Anomaly Detection with the Dendritic Cell Algorithm",
        "authors": [
            "Julie Greensmith",
            "Uwe Aickelin",
            "Gianni Tedesco"
        ],
        "abstract": "  Dendritic cells are antigen presenting cells that provide a vital link between the innate and adaptive immune system, providing the initial detection of pathogenic invaders. Research into this family of cells has revealed that they perform information fusion which directs immune responses. We have derived a Dendritic Cell Algorithm based on the functionality of these cells, by modelling the biological signals and differentiation pathways to build a control mechanism for an artificial immune system. We present algorithmic details in addition to experimental results, when the algorithm was applied to anomaly detection for the detection of port scans. The results show the Dendritic Cell Algorithm is sucessful at detecting port scans.\n    ",
        "submission_date": "2010-03-03T00:00:00",
        "last_modified_date": "2010-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.1256",
        "title": "Integrating Innate and Adaptive Immunity for Intrusion Detection",
        "authors": [
            "Gianni Tedesco",
            "Jamie Twycross",
            "Uwe Aickelin"
        ],
        "abstract": "  Network Intrusion Detection Systems (NDIS) monitor a network with the aim of discerning malicious from benign activity on that network. While a wide range of approaches have met varying levels of success, most IDS's rely on having access to a database of known attack signatures which are written by security experts. Nowadays, in order to solve problems with false positive alters, correlation algorithms are used to add additional structure to sequences of IDS alerts. However, such techniques are of no help in discovering novel attacks or variations of known attacks, something the human immune system (HIS) is capable of doing in its own specialised domain. This paper presents a novel immune algorithm for application to an intrusion detection problem. The goal is to discover packets containing novel variations of attacks covered by an existing signature base.\n    ",
        "submission_date": "2010-03-05T00:00:00",
        "last_modified_date": "2010-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.1493",
        "title": "Integration of Rule Based Expert Systems and Case Based Reasoning in an Acute Bacterial Meningitis Clinical Decision Support System",
        "authors": [
            "Mariana Maceiras Cabrera",
            "Ernesto Ocampo Edye"
        ],
        "abstract": "This article presents the results of the research carried out on the development of a medical diagnostic system applied to the Acute Bacterial Meningitis, using the Case Based Reasoning methodology. The research was focused on the implementation of the adaptation stage, from the integration of Case Based Reasoning and Rule Based Expert Systems. In this adaptation stage we use a higher level RBC that stores and allows reutilizing change experiences, combined with a classic rule-based inference engine. In order to take into account the most evident clinical situation, a pre-diagnosis stage is implemented using a rule engine that, given an evident situation, emits the corresponding diagnosis and avoids the complete process.\n    ",
        "submission_date": "2010-03-07T00:00:00",
        "last_modified_date": "2010-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.1504",
        "title": "Indexer Based Dynamic Web Services Discovery",
        "authors": [
            "Saba Bashir",
            "Farhan Hassan Khan",
            "M.Younus Javed",
            "Aihab Khan",
            "Malik Sikandar Hayat Khiyal"
        ],
        "abstract": "Recent advancement in web services plays an important role in business to business and business to consumer interaction. Discovery mechanism is not only used to find a suitable service but also provides collaboration between service providers and consumers by using standard protocols. A static web service discovery mechanism is not only time consuming but requires continuous human interaction. This paper proposed an efficient dynamic web services discovery mechanism that can locate relevant and updated web services from service registries and repositories with timestamp based on indexing value and categorization for faster and efficient discovery of service. The proposed prototype focuses on quality of service issues and introduces concept of local cache, categorization of services, indexing mechanism, CSP (Constraint Satisfaction Problem) solver, aging and usage of translator. Performance of proposed framework is evaluated by implementing the algorithm and correctness of our method is shown. The results of proposed framework shows greater performance and accuracy in dynamic discovery mechanism of web services resolving the existing issues of flexibility, scalability, based on quality of service, and discovers updated and most relevant services with ease of usage.\n    ",
        "submission_date": "2010-03-07T00:00:00",
        "last_modified_date": "2010-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.1588",
        "title": "On the Failure of the Finite Model Property in some Fuzzy Description Logics",
        "authors": [
            "Fernando Bobillo",
            "Felix Bou",
            "Umberto Straccia"
        ],
        "abstract": "Fuzzy Description Logics (DLs) are a family of logics which allow the representation of (and the reasoning with) structured knowledge affected by vagueness. Although most of the not very expressive crisp DLs, such as ALC, enjoy the Finite Model Property (FMP), this is not the case once we move into the fuzzy case. In this paper we show that if we allow arbitrary knowledge bases, then the fuzzy DLs ALC under Lukasiewicz and Product fuzzy logics do not verify the FMP even if we restrict to witnessed models; in other words, finite satisfiability and witnessed satisfiability are different for arbitrary knowledge bases. The aim of this paper is to point out the failure of FMP because it affects several algorithms published in the literature for reasoning under fuzzy ALC.\n    ",
        "submission_date": "2010-03-08T00:00:00",
        "last_modified_date": "2010-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.1598",
        "title": "Information Fusion in the Immune System",
        "authors": [
            "Jamie Twycross",
            "Uwe Aickelin"
        ],
        "abstract": "Biologically-inspired methods such as evolutionary algorithms and neural networks are proving useful in the field of information fusion. Artificial Immune Systems (AISs) are a biologically-inspired approach which take inspiration from the biological immune system. Interestingly, recent research has show how AISs which use multi-level information sources as input data can be used to build effective algorithms for real time computer intrusion detection. This research is based on biological information fusion mechanisms used by the human immune system and as such might be of interest to the information fusion community. The aim of this paper is to present a summary of some of the biological information fusion mechanisms seen in the human immune system, and of how these mechanisms have been implemented as AISs\n    ",
        "submission_date": "2010-03-08T00:00:00",
        "last_modified_date": "2010-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.1658",
        "title": "A multivalued knowledge-base model",
        "authors": [
            "Agnes Achs"
        ],
        "abstract": "The basic aim of our study is to give a possible model for handling uncertain information. This model is worked out in the framework of DATALOG. At first the concept of fuzzy Datalog will be summarized, then its extensions for intuitionistic- and interval-valued fuzzy logic is given and the concept of bipolar fuzzy Datalog is introduced. Based on these ideas the concept of multivalued knowledge-base will be defined as a quadruple of any background knowledge; a deduction mechanism; a connecting algorithm, and a function set of the program, which help us to determine the uncertainty levels of the results. At last a possible evaluation strategy is given.\n    ",
        "submission_date": "2010-03-08T00:00:00",
        "last_modified_date": "2010-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.2641",
        "title": "Release ZERO.0.1 of package RefereeToolbox",
        "authors": [
            "Fr\u00e9d\u00e9ric Dambreville"
        ],
        "abstract": "RefereeToolbox is a java package implementing combination operators for fusing evidences. It is downloadable from: ",
        "submission_date": "2010-03-12T00:00:00",
        "last_modified_date": "2010-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.3082",
        "title": "Agreement Maintenance Based on Schema and Ontology Change in P2P Environment",
        "authors": [
            "L.Y. Banowosari",
            "I.W.S. Wicaksana",
            "A.B. Mutiara"
        ],
        "abstract": "This paper is concern about developing a semantic agreement maintenance method based on semantic distance by calculating the change of local schema or ontology. This approach is important in dynamic and autonomous environment, in which the current approach assumed that agreement or mapping in static environment. The contribution of this research is to develop a framework based on semantic agreement maintenance approach for P2P environment. This framework based on two level hybrid P2P model architecture, which consist of two peer type: (1) super peer that use to register and manage the other peers, and (2) simple peer, as a simple peer, it exports and shares its contents with others. This research develop a model to maintain the semantic agreement in P2P environment, so the current approach which does not have the mechanism to know the change, since it assumed that ontology and local schema are in the static condition, and it is different in dynamic condition. The main issues are how to calculate the change of local schema or common ontology and the calculation result is used to determine which algorithm in maintaining the agreement. The experiment on the job matching domain in Indonesia have been done to show how far the performance of the approach. From the experiment, the main result are (i) the more change so the F-measure value tend to be decreased, (ii) there is no significant different in F-measure value for various modification type (add, delete, rename), and (iii) the correct choice of algorithm would improve the F-measure value.\n    ",
        "submission_date": "2010-03-16T00:00:00",
        "last_modified_date": "2010-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.3766",
        "title": "Modelling and simulating retail management practices: a first approach",
        "authors": [
            "Peer-Olaf Siebers",
            "Uwe Aickelin",
            "Helen Celia",
            "Chris Clegg"
        ],
        "abstract": "Multi-agent systems offer a new and exciting way of understanding the world of work. We apply agent-based modeling and simulation to investigate a set of problems in a retail context. Specifically, we are working to understand the relationship between people management practices on the shop-floor and retail performance. Despite the fact we are working within a relatively novel and complex domain, it is clear that using an agent-based approach offers great potential for improving organizational capabilities in the future. Our multi-disciplinary research team has worked closely with one of the UK's top ten retailers to collect data and build an understanding of shop-floor operations and the key actors in a department (customers, staff, and managers). Based on this case study we have built and tested our first version of a retail branch agent-based simulation model where we have focused on how we can simulate the effects of people management practices on customer satisfaction and sales. In our experiments we have looked at employee development and cashier empowerment as two examples of shop floor management practices. In this paper we describe the underlying conceptual ideas and the features of our simulation model. We present a selection of experiments we have conducted in order to validate our simulation model and to show its potential for answering \"what-if\" questions in a retail context. We also introduce a novel performance measure which we have created to quantify customers' satisfaction with service, based on their individual shopping experiences.\n    ",
        "submission_date": "2010-03-19T00:00:00",
        "last_modified_date": "2010-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.3767",
        "title": "Multi-Agent Simulation and Management Practices",
        "authors": [
            "Peer-Olaf Siebers",
            "Uwe Aickelin",
            "Helen Celia",
            "Chris Clegg"
        ],
        "abstract": "Intelligent agents offer a new and exciting way of understanding the world of work. Agent-Based Simulation (ABS), one way of using intelligent agents, carries great potential for progressing our understanding of management practices and how they link to retail performance. We have developed simulation models based on research by a multi-disciplinary team of economists, work psychologists and computer scientists. We will discuss our experiences of implementing these concepts working with a well-known retail department store. There is no doubt that management practices are linked to the performance of an organisation (Reynolds et al., 2005; Wall & Wood, 2005). Best practices have been developed, but when it comes down to the actual application of these guidelines considerable ambiguity remains regarding their effectiveness within particular contexts (Siebers et al., forthcoming a). Most Operational Research (OR) methods can only be used as analysis tools once management practices have been implemented. Often they are not very useful for giving answers to speculative 'what-if' questions, particularly when one is interested in the development of the system over time rather than just the state of the system at a certain point in time. Simulation can be used to analyse the operation of dynamic and stochastic systems. ABS is particularly useful when complex interactions between system entities exist, such as autonomous decision making or negotiation. In an ABS model the researcher explicitly describes the decision process of simulated actors at the micro level. Structures emerge at the macro level as a result of the actions of the agents and their interactions with other agents and the environment. 3 We will show how ABS experiments can deal with testing and optimising management practices such as training, empowerment or teamwork. Hence, questions such as \"will staff setting their own break times improve performance?\" can be investigated.\n    ",
        "submission_date": "2010-03-19T00:00:00",
        "last_modified_date": "2010-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.3775",
        "title": "Optimisation of a Crossdocking Distribution Centre Simulation Model",
        "authors": [
            "Adrian Adewunmi",
            "Uwe Aickelin"
        ],
        "abstract": "This paper reports on continuing research into the modelling of an order picking process within a Crossdocking distribution centre using Simulation Optimisation. The aim of this project is to optimise a discrete event simulation model and to understand factors that affect finding its optimal performance. Our initial investigation revealed that the precision of the selected simulation output performance measure and the number of replications required for the evaluation of the optimisation objective function through simulation influences the ability of the optimisation technique. We experimented with Common Random Numbers, in order to improve the precision of our simulation output performance measure, and intended to use the number of replications utilised for this purpose as the initial number of replications for the optimisation of our Crossdocking distribution centre simulation model. Our results demonstrate that we can improve the precision of our selected simulation output performance measure value using Common Random Numbers at various levels of replications. Furthermore, after optimising our Crossdocking distribution centre simulation model, we are able to achieve optimal performance using fewer simulations runs for the simulation model which uses Common Random Numbers as compared to the simulation model which does not use Common Random Numbers.\n    ",
        "submission_date": "2010-03-19T00:00:00",
        "last_modified_date": "2010-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.3821",
        "title": "A Formal Approach to Modeling the Memory of a Living Organism",
        "authors": [
            "Dan Guralnik"
        ],
        "abstract": "We consider a living organism as an observer of the evolution of its environment recording sensory information about the state space X of the environment in real time. Sensory information is sampled and then processed on two levels. On the biological level, the organism serves as an evaluation mechanism of the subjective relevance of the incoming data to the observer: the observer assigns excitation values to events in X it could recognize using its sensory equipment. On the algorithmic level, sensory input is used for updating a database, the memory of the observer whose purpose is to serve as a geometric/combinatorial model of X, whose nodes are weighted by the excitation values produced by the evaluation mechanism. These values serve as a guidance system for deciding how the database should transform as observation data mounts. We define a searching problem for the proposed model and discuss the model's flexibility and its computational efficiency, as well as the possibility of implementing it as a dynamic network of neuron-like units. We show how various easily observable properties of the human memory and thought process can be explained within the framework of this model. These include: reasoning (with efficiency bounds), errors, temporary and permanent loss of information. We are also able to define general learning problems in terms of the new model, such as the language acquisition problem.\n    ",
        "submission_date": "2010-03-19T00:00:00",
        "last_modified_date": "2010-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.4141",
        "title": "Investigating Output Accuracy for a Discrete Event Simulation Model and an Agent Based Simulation Model",
        "authors": [
            "Mazlina Abdul Majid",
            "Uwe Aickelin",
            "Peer-Olaf Siebers"
        ],
        "abstract": "In this paper, we investigate output accuracy for a Discrete Event Simulation (DES) model and Agent Based Simulation (ABS) model. The purpose of this investigation is to find out which of these simulation techniques is the best one for modelling human reactive behaviour in the retail sector. In order to study the output accuracy in both models, we have carried out a validation experiment in which we compared the results from our simulation models to the performance of a real system. Our experiment was carried out using a large UK department store as a case study. We had to determine an efficient implementation of management policy in the store's fitting room using DES and ABS. Overall, we have found that both simulation models were a good representation of the real system when modelling human reactive behaviour.\n    ",
        "submission_date": "2010-03-22T00:00:00",
        "last_modified_date": "2010-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.4142",
        "title": "Malicious Code Execution Detection and Response Immune System inspired by the Danger Theory",
        "authors": [
            "Jungwon Kim",
            "Julie Greensmith",
            "Jamie Twycross",
            "Uwe Aickelin"
        ],
        "abstract": "The analysis of system calls is one method employed by anomaly detection systems to recognise malicious code execution. Similarities can be drawn between this process and the behaviour of certain cells belonging to the human immune system, and can be applied to construct an artificial immune system. A recently developed hypothesis in immunology, the Danger Theory, states that our immune system responds to the presence of intruders through sensing molecules belonging to those invaders, plus signals generated by the host indicating danger and damage. We propose the incorporation of this concept into a responsive intrusion detection system, where behavioural information of the system and running processes is combined with information regarding individual system calls.\n    ",
        "submission_date": "2010-03-22T00:00:00",
        "last_modified_date": "2010-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.4145",
        "title": "Mimicking the Behaviour of Idiotypic AIS Robot Controllers Using Probabilistic Systems",
        "authors": [
            "Amanda Whitbrook",
            "Uwe Aickelin",
            "Jonathan Garibaldi"
        ],
        "abstract": "Previous work has shown that robot navigation systems that employ an architecture based upon the idiotypic network theory of the immune system have an advantage over control techniques that rely on reinforcement learning only. This is thought to be a result of intelligent behaviour selection on the part of the idiotypic robot. In this paper an attempt is made to imitate idiotypic dynamics by creating controllers that use reinforcement with a number of different probabilistic schemes to select robot behaviour. The aims are to show that the idiotypic system is not merely performing some kind of periodic random behaviour selection, and to try to gain further insight into the processes that govern the idiotypic mechanism. Trials are carried out using simulated Pioneer robots that undertake navigation exercises. Results show that a scheme that boosts the probability of selecting highly-ranked alternative behaviours to 50% during stall conditions comes closest to achieving the properties of the idiotypic system, but remains unable to match it in terms of all round performance.\n    ",
        "submission_date": "2010-03-22T00:00:00",
        "last_modified_date": "2010-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.4196",
        "title": "Development of a Cargo Screening Process Simulator: A First Approach",
        "authors": [
            "Peer-Olaf Siebers",
            "Galina Sherman",
            "Uwe Aickelin"
        ],
        "abstract": "The efficiency of current cargo screening processes at sea and air ports is largely unknown as few benchmarks exists against which they could be measured. Some manufacturers provide benchmarks for individual sensors but we found no benchmarks that take a holistic view of the overall screening procedures and no benchmarks that take operator variability into account. Just adding up resources and manpower used is not an effective way for assessing systems where human decision-making and operator compliance to rules play a vital role. Our aim is to develop a decision support tool (cargo-screening system simulator) that will map the right technology and manpower to the right commodity-threat combination in order to maximise detection rates. In this paper we present our ideas for developing such a system and highlight the research challenges we have identified. Then we introduce our first case study and report on the progress we have made so far.\n    ",
        "submission_date": "2010-03-22T00:00:00",
        "last_modified_date": "2010-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.5173",
        "title": "LEXSYS: Architecture and Implication for Intelligent Agent systems",
        "authors": [
            "Charles A. B. Robert"
        ],
        "abstract": "LEXSYS, (Legume Expert System) was a project conceived at IITA (International Institute of Tropical Agriculture) Ibadan Nigeria. It was initiated by the COMBS (Collaborative Group on Maize-Based Systems Research in the 1990. It was meant for a general framework for characterizing on-farm testing for technology design for sustainable cereal-based cropping system. LEXSYS is not a true expert system as the name would imply, but simply a user-friendly information system. This work is an attempt to give a formal representation of the existing system and then present areas where intelligent agent can be applied.\n    ",
        "submission_date": "2010-03-26T00:00:00",
        "last_modified_date": "2010-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.5305",
        "title": "Rational Value of Information Estimation for Measurement Selection",
        "authors": [
            "David Tolpin",
            "Solomon Eyal Shimony"
        ],
        "abstract": "Computing value of information (VOI) is a crucial task in various aspects of decision-making under uncertainty, such as in meta-reasoning for search; in selecting measurements to make, prior to choosing a course of action; and in managing the exploration vs. exploitation tradeoff. Since such applications typically require numerous VOI computations during a single run, it is essential that VOI be computed efficiently.  We examine the issue of anytime estimation of VOI, as frequently it suffices to get a crude estimate of the VOI, thus saving considerable computational resources. As a case study, we examine VOI estimation in the measurement selection problem.  Empirical evaluation of the proposed scheme in this domain shows that computational resources can indeed be significantly reduced, at little cost in expected rewards achieved in the overall decision problem.\n    ",
        "submission_date": "2010-03-27T00:00:00",
        "last_modified_date": "2010-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.5899",
        "title": "Geometric Algebra Model of Distributed Representations",
        "authors": [
            "Agnieszka Patyk"
        ],
        "abstract": "Formalism based on GA is an alternative to distributed representation models developed so far --- Smolensky's tensor product, Holographic Reduced Representations (HRR) and Binary Spatter Code (BSC). Convolutions are replaced by geometric products, interpretable in terms of geometry which seems to be the most natural language for visualization of higher concepts. This paper recalls the main ideas behind the GA model and investigates recognition test results using both inner product and a clipped version of matrix representation. The influence of accidental blade equality on recognition is also studied. Finally, the efficiency of the GA model is compared to that of previously developed models.\n    ",
        "submission_date": "2010-03-30T00:00:00",
        "last_modified_date": "2010-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.1540",
        "title": "Importance of Sources using the Repeated Fusion Method and the Proportional Conflict Redistribution Rules #5 and #6",
        "authors": [
            "Florentin Smarandache",
            "Jean Dezert"
        ],
        "abstract": "  We present in this paper some examples of how to compute by hand the PCR5 fusion rule for three sources, so the reader will better understand its mechanism. We also take into consideration the importance of sources, which is different from the classical discounting of sources.\n    ",
        "submission_date": "2010-04-09T00:00:00",
        "last_modified_date": "2010-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.1772",
        "title": "Terrorism Event Classification Using Fuzzy Inference Systems",
        "authors": [
            "Uraiwan Inyaem",
            "Choochart Haruechaiyasak",
            "Phayung Meesad",
            "Dat Tran"
        ],
        "abstract": "Terrorism has led to many problems in Thai societies, not only property damage but also civilian casualties. Predicting terrorism activities in advance can help prepare and manage risk from sabotage by these activities. This paper proposes a framework focusing on event classification in terrorism domain using fuzzy inference systems (FISs). Each FIS is a decision-making model combining fuzzy logic and approximate reasoning. It is generated in five main parts: the input interface, the fuzzification interface, knowledge base unit, decision making unit and output defuzzification interface. Adaptive neuro-fuzzy inference system (ANFIS) is a FIS model adapted by combining the fuzzy logic and neural network. The ANFIS utilizes automatic identification of fuzzy logic rules and adjustment of membership function (MF). Moreover, neural network can directly learn from data set to construct fuzzy logic rules and MF implemented in various applications. FIS settings are evaluated based on two comparisons. The first evaluation is the comparison between unstructured and structured events using the same FIS setting. The second comparison is the model settings between FIS and ANFIS for classifying structured events. The data set consists of news articles related to terrorism events in three southern provinces of Thailand. The experimental results show that the classification performance of the FIS resulting from structured events achieves satisfactory accuracy and is better than the unstructured events. In addition, the classification of structured events using ANFIS gives higher performance than the events using only FIS in the prediction of terrorism events.\n    ",
        "submission_date": "2010-04-11T00:00:00",
        "last_modified_date": "2010-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.1794",
        "title": "Probabilistic Semantic Web Mining Using Artificial Neural Analysis",
        "authors": [
            "T.Krishna Kishore",
            "T.Sasi Vardhan",
            "N.Lakshmi Narayana"
        ],
        "abstract": "Most of the web user's requirements are search or navigation time and getting correctly matched result. These constrains can be satisfied with some additional modules attached to the existing search engines and web servers. This paper proposes that powerful architecture for search engines with the title of Probabilistic Semantic Web Mining named from the methods used. With the increase of larger and larger collection of various data resources on the World Wide Web (WWW), Web Mining has become one of the most important requirements for the web users. Web servers will store various formats of data including text, image, audio, video etc., but servers can not identify the contents of the data. These search techniques can be improved by adding some special techniques including semantic web mining and probabilistic analysis to get more accurate results. Semantic web mining technique can provide meaningful search of data resources by eliminating useless information with mining process. In this technique web servers will maintain Meta information of each and every data resources available in that particular web server. This will help the search engine to retrieve information that is relevant to user given input string. This paper proposing the idea of combing these two techniques Semantic web mining and Probabilistic analysis for efficient and accurate search results of web mining. SPF can be calculated by considering both semantic accuracy and syntactic accuracy of data with the input string. This will be the deciding factor for producing results.\n    ",
        "submission_date": "2010-04-11T00:00:00",
        "last_modified_date": "2010-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.2003",
        "title": "The Socceral Force",
        "authors": [
            "Norbert B\u00e1tfai"
        ],
        "abstract": "We have an audacious dream, we would like to develop a simulation and virtual reality system to support the decision making in European football (soccer). In this review, we summarize the efforts that we have made to fulfil this dream until recently. In addition, an introductory version of FerSML (Footballer and Football Simulation Markup Language) is presented in this paper.\n    ",
        "submission_date": "2010-04-12T00:00:00",
        "last_modified_date": "2010-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.2008",
        "title": "Matrix Coherence and the Nystrom Method",
        "authors": [
            "Ameet Talwalkar",
            "Afshin Rostamizadeh"
        ],
        "abstract": "The Nystrom method is an efficient technique to speed up large-scale learning applications by generating low-rank approximations.  Crucial to the performance of this technique is the assumption that a matrix can be well approximated by working exclusively with a subset of its columns.  In this work we relate this assumption to the concept of matrix coherence and connect matrix coherence to the performance of the Nystrom method. Making use of related work in the compressed sensing and the matrix completion literature, we derive novel coherence-based bounds for the Nystrom method in the low-rank setting.  We then present empirical results that corroborate these theoretical bounds.  Finally, we present more general empirical results for the full-rank setting that convincingly demonstrate the ability of matrix coherence to measure the degree to which information can be extracted from a subset of columns.\n    ",
        "submission_date": "2010-04-12T00:00:00",
        "last_modified_date": "2010-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.2342",
        "title": "Mean field for Markov Decision Processes: from Discrete to Continuous Optimization",
        "authors": [
            "Nicolas Gast",
            "Bruno Gaujal",
            "Jean-Yves Le Boudec"
        ],
        "abstract": "We study the convergence of Markov Decision Processes made of a large number of objects to optimization problems on ordinary differential equations (ODE). We show that the optimal reward of such a Markov Decision Process, satisfying a Bellman equation, converges to the solution of a continuous Hamilton-Jacobi-Bellman (HJB) equation based on the mean field approximation of the Markov Decision Process. We give bounds on the difference of the rewards, and a constructive algorithm for deriving an approximating solution to the Markov Decision Process from a solution of the HJB equations. We illustrate the method on three examples pertaining respectively to investment strategies, population dynamics control and scheduling in queues are developed. They are used to illustrate and justify the construction of the controlled ODE and to show the gain obtained by solving a continuous HJB equation rather than a large discrete Bellman equation.\n    ",
        "submission_date": "2010-04-14T00:00:00",
        "last_modified_date": "2011-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.2624",
        "title": "Symmetry within Solutions",
        "authors": [
            "Marijn Heule",
            "Toby Walsh"
        ],
        "abstract": "We define the concept of an internal symmetry. This is a symmety within a solution of a constraint satisfaction problem. We compare this to solution symmetry, which is a mapping between different solutions of the same problem. We argue that we may be able to exploit both types of symmetry when finding solutions. We illustrate the potential of exploiting internal symmetries on two benchmark domains: Van der Waerden numbers and graceful graphs. By identifying internal symmetries we are able to extend the state of the art in both cases.\n    ",
        "submission_date": "2010-04-15T00:00:00",
        "last_modified_date": "2010-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.2626",
        "title": "Propagating Conjunctions of AllDifferent Constraints",
        "authors": [
            "Christian Bessiere",
            "George Katsirelos",
            "Nina Narodytska",
            "Claude-Guy Quimper",
            "Toby Walsh"
        ],
        "abstract": "We study propagation algorithms for the conjunction of two AllDifferent constraints. Solutions of an AllDifferent constraint can be seen as perfect matchings on the variable/value bipartite graph. Therefore, we investigate the problem of finding simultaneous bipartite matchings. We present an extension of the famous Hall theorem which characterizes when simultaneous bipartite matchings exists. Unfortunately, finding such matchings is NP-hard in general. However, we prove a surprising result that finding a simultaneous matching on a convex bipartite graph takes just polynomial time. Based on this theoretical result, we provide the first polynomial time bound consistency algorithm for the conjunction of two AllDifferent constraints. We identify a pathological problem on which this propagator is exponentially faster compared to existing propagators. Our experiments show that this new propagator can offer significant benefits over existing methods.\n    ",
        "submission_date": "2010-04-15T00:00:00",
        "last_modified_date": "2010-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.2854",
        "title": "Experimenting with Innate Immunity",
        "authors": [
            "Jamie Twycross",
            "Uwe Aickelin"
        ],
        "abstract": "In a previous paper the authors argued the case for incorporating ideas from innate immunity into artificial immune systems (AISs) and presented an outline for a conceptual framework for such systems. A number of key general properties observed in the biological innate and adaptive immune systems were highlighted, and how such properties might be instantiated in artificial systems was discussed in detail. The next logical step is to take these ideas and build a software system with which AISs with these properties can be implemented and experimentally evaluated. This paper reports on the results of that step - the libtissue system.\n    ",
        "submission_date": "2010-04-16T00:00:00",
        "last_modified_date": "2010-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.2860",
        "title": "Behavioural Correlation for Detecting P2P Bots",
        "authors": [
            "Yousof Al-Hammadi",
            "Uwe Aickelin"
        ],
        "abstract": "In the past few years, IRC bots, malicious programs which are remotely controlled by the attacker through IRC servers, have become a major threat to the Internet and users. These bots can be used in different malicious ways such as issuing distributed denial of services attacks to shutdown other networks and services, keystrokes logging, spamming, traffic sniffing cause serious disruption on networks and users. New bots use peer to peer (P2P) protocols start to appear as the upcoming threat to Internet security due to the fact that P2P bots do not have a centralized point to shutdown or traceback, thus making the detection of P2P bots is a real challenge. In response to these threats, we present an algorithm to detect an individual P2P bot running on a system by correlating its activities. Our evaluation shows that correlating different activities generated by P2P bots within a specified time period can detect these kind of bots.\n    ",
        "submission_date": "2010-04-16T00:00:00",
        "last_modified_date": "2010-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.2870",
        "title": "Nurse Rostering with Genetic Algorithms",
        "authors": [
            "Uwe Aickelin"
        ],
        "abstract": "In recent years genetic algorithms have emerged as a useful tool for the heuristic solution of complex discrete optimisation problems. In particular there has been considerable interest in their use in tackling problems arising in the areas of scheduling and timetabling. However, the classical genetic algorithm paradigm is not well equipped to handle constraints and successful implementations usually require some sort of modification to enable the search to exploit problem specific knowledge in order to overcome this shortcoming. This paper is concerned with the development of a family of genetic algorithms for the solution of a nurse rostering problem at a major UK hospital. The hospital is made up of wards of up to 30 nurses. Each ward has its own group of nurses whose shifts have to be scheduled on a weekly basis. In addition to fulfilling the minimum demand for staff over three daily shifts, nurses' wishes and qualifications have to be taken into account. The schedules must also be seen to be fair, in that unpopular shifts have to be spread evenly amongst all nurses, and other restrictions, such as team nursing and special conditions for senior staff, have to be satisfied. The basis of the family of genetic algorithms is a classical genetic algorithm consisting of n-point crossover, single-bit mutation and a rank-based selection. The solution space consists of all schedules in which each nurse works the required number of shifts, but the remaining constraints, both hard and soft, are relaxed and penalised in the fitness function. The talk will start with a detailed description of the problem and the initial implementation and will go on to highlight the shortcomings of such an approach, in terms of the key element of balancing feasibility, i.e. covering the demand and work regulations, and quality, as measured by the nurses' preferences. A series of experiments involving parameter adaptation, niching, intelligent weights, delta coding, local hill climbing, migration and special selection rules will then be outlined and it will be shown how a series of these enhancements were able to eradicate these difficulties. Results based on several months' real data will be used to measure the impact of each modification, and to show that the final algorithm is able to compete with a tabu search approach currently employed at the hospital. The talk will conclude with some observations as to the overall quality of this approach to this and similar problems.\n    ",
        "submission_date": "2010-03-19T00:00:00",
        "last_modified_date": "2010-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.2880",
        "title": "GRASP for the Coalition Structure Formation Problem",
        "authors": [
            "Nicola Di Mauro",
            "Teresa M.A. Basile",
            "Stefano Ferilli",
            "Floriana Esposito"
        ],
        "abstract": "The  coalition  structure  formation problem  represents  an  active  research area  in  multi-agent systems. A  coalition structure is defined  as a partition of  the agents involved in  a system into disjoint coalitions.  The problem of finding the optimal coalition structure is NP-complete.  In order to find the optimal solution in a combinatorial optimization problem it is theoretically possible to enumerate the  solutions and  evaluate each.  But this approach  is infeasible  since the  number of solutions often grows exponentially with the size of the problem. In this paper we  present a  greedy adaptive  search procedure (GRASP)  to efficiently  search the  space of coalition  structures  in order  to  find  an optimal  one.  Experiments  and  comparisons to  other algorithms prove the validity of the proposed method in solving this hard combinatorial problem.\n    ",
        "submission_date": "2010-04-16T00:00:00",
        "last_modified_date": "2010-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.3196",
        "title": "Introducing Dendritic Cells as a Novel Immune-Inspired Algorithm for Anomoly Detection",
        "authors": [
            "Julie Greensmith",
            "Uwe Aickelin",
            "Steve Cayzer"
        ],
        "abstract": "Dendritic cells are antigen presenting cells that provide a vital link between the innate and adaptive immune system. Research into this family of cells has revealed that they perform the role of coordinating T-cell based immune responses, both reactive and for generating tolerance. We have derived an algorithm based on the functionality of these cells, and have used the signals and differentiation pathways to build a control mechanism for an artificial immune system. We present our algorithmic details in addition to some preliminary results, where the algorithm was applied for the purpose of anomaly detection. We hope that this algorithm will eventually become the key component within a large, distributed immune system, based on sound immunological concepts.\n    ",
        "submission_date": "2010-04-19T00:00:00",
        "last_modified_date": "2010-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.3260",
        "title": "Decision Support Systems (DSS) in Construction Tendering Processes",
        "authors": [
            "Rosmayati Mohemad",
            "Abdul Razak Hamdan",
            "Zulaiha Ali Othman",
            "Noor Maizura Mohamad Noor"
        ],
        "abstract": "The successful execution of a construction project is heavily impacted by making the right decision during tendering processes. Managing tender procedures is very complex and uncertain involving coordination of many tasks and individuals with different priorities and objectives. Bias and inconsistent decision are inevitable if the decision-making process is totally depends on intuition, subjective judgement or emotion. In making transparent decision and healthy competition tendering, there exists a need for flexible guidance tool for decision support. Aim of this paper is to give a review on current practices of Decision Support Systems (DSS) technology in construction tendering processes. Current practices of general tendering processes as applied to the most countries in different regions such as United States, Europe, Middle East and Asia are comprehensively discussed. Applications of Web-based tendering processes is also summarised in terms of its properties. Besides that, a summary of Decision Support System (DSS) components is included in the next section. Furthermore, prior researches on implementation of DSS approaches in tendering processes are discussed in details. Current issues arise from both of paper-based and Web-based tendering processes are outlined. Finally, conclusion is included at the end of this paper.\n    ",
        "submission_date": "2010-04-19T00:00:00",
        "last_modified_date": "2010-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.3460",
        "title": "PCA 4 DCA: The Application Of Principal Component Analysis To The Dendritic Cell Algorithm",
        "authors": [
            "Feng Gu",
            "Julie Greensmith",
            "Robert Oates",
            "Uwe Aickelin"
        ],
        "abstract": "As one of the newest members in the field of artificial immune systems (AIS), the Dendritic Cell Algorithm (DCA) is based on behavioural models of natural dendritic cells (DCs). Unlike other AIS, the DCA does not rely on training data, instead domain or expert knowledge is required to predetermine the mapping between input signals from a particular instance to the three categories used by the DCA. This data preprocessing phase has received the criticism of having manually over-?tted the data to the algorithm, which is undesirable. Therefore, in this paper we have attempted to ascertain if it is possible to use principal component analysis (PCA) techniques to automatically categorise input data while still generating useful and accurate classication results. The integrated system is tested with a biometrics dataset for the stress recognition of automobile drivers. The experimental results have shown the application of PCA to the DCA for the purpose of automated data preprocessing is successful.\n    ",
        "submission_date": "2010-04-20T00:00:00",
        "last_modified_date": "2010-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.3884",
        "title": "Oil Price Trackers Inspired by Immune Memory",
        "authors": [
            "WIlliam Wilson",
            "Phil Birkin",
            "Uwe Aickelin"
        ],
        "abstract": "We outline initial concepts for an immune inspired algorithm to evaluate and predict oil price time series data. The proposed solution evolves a short term pool of trackers dynamically, with each member attempting to map trends and anticipate future price movements. Successful trackers feed into a long term memory pool that can generalise across repeating trend patterns. The resulting sequence of trackers, ordered in time, can be used as a forecasting tool. Examination of the pool of evolving trackers also provides valuable insight into the properties of the crude oil market.\n    ",
        "submission_date": "2010-04-22T00:00:00",
        "last_modified_date": "2010-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.3887",
        "title": "Motif Detection Inspired by Immune Memory",
        "authors": [
            "William Wilson",
            "Phil Birkin",
            "Uwe Aickelin"
        ],
        "abstract": "The  search  for patterns or motifs  in data represents an area of key  interest  to many  researchers. In  this  paper  we  present  the Motif  Tracking Algorithm,  a novel  immune  inspired pattern identification tool  that is able  to identify  variable length  unknown motifs  which  repeat within time series data. The  algorithm searches  from a completely neutral  perspective  that is independent  of the data being  analysed and the underlying motifs.  In  this  paper  we test the flexibility  of  the  motif tracking algorithm by applying it to the search  for patterns in two industrial data sets. The  algorithm is able  to  identify  a population of motifs successfully  in both  cases, and the value  of these motifs  is discussed.\n    ",
        "submission_date": "2010-04-22T00:00:00",
        "last_modified_date": "2010-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.3919",
        "title": "Performance Evaluation of DCA and SRC on a Single Bot Detection",
        "authors": [
            "Yousof Al-Hammadi",
            "Uwe Aickelin",
            "Julie Greensmith"
        ],
        "abstract": "Malicious users try to compromise systems using new techniques. One of the recent techniques used by the attacker is to perform complex distributed attacks such as denial of service and to obtain   sensitive   data   such   as   password   information.   These compromised  machines  are  said  to  be  infected  with  malicious software   termed  a   \"bot\".  In   this  paper,  we   investigate  the correlation of behavioural attributes such as keylogging and packet flooding  behaviour to detect  the existence of a single bot on a compromised machine by applying (1) Spearman's rank correlation (SRC) algorithm and (2) the Dendritic Cell Algorithm (DCA). We also compare the output results generated from these two methods to the detection of a single bot. The results show that the DCA has a better performance in detecting malicious activities.\n    ",
        "submission_date": "2010-04-22T00:00:00",
        "last_modified_date": "2010-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.3932",
        "title": "Modelling Immunological Memory",
        "authors": [
            "Simon Garret",
            "Martin Robbins",
            "Joanne Walker",
            "William Wilson",
            "Uwe Aickelin"
        ],
        "abstract": "Accurate immunological models offer the possibility of performing highthroughput experiments in silico that can predict, or at least suggest, in vivo phenomena. In this chapter, we compare various models of immunological memory. We first validate an experimental immunological simulator, developed by the authors, by simulating several theories of immunological memory with known results. We then use the same system to evaluate the predicted effects of a theory of immunological memory. The resulting model has not been explored before in artificial immune systems research, and we compare the simulated in silico output with in vivo measurements. Although the theory appears valid, we suggest that there are a common set of reasons why immunological memory models are a useful support tool; not conclusive in themselves.\n    ",
        "submission_date": "2010-04-21T00:00:00",
        "last_modified_date": "2010-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.3939",
        "title": "Price Trackers Inspired by Immune Memory",
        "authors": [
            "William Wilson",
            "Phil Birkin",
            "Uwe Aickelin"
        ],
        "abstract": "In this  paper  we outline  initial  concepts  for an  immune  inspired  algorithm  to evaluate price  time  series  data.  The  proposed  solution evolves a short term pool of trackers dynamically through a process of proliferation and  mutation,  with  each  member   attempting to map to trends in price  movements.  Successful  trackers  feed into  a long term memory pool that can generalise  across  repeating trend patterns. Tests are performed to examine  the algorithm's ability to successfully identify trends in a small  data set. The  influence  of the long term memory  pool is then examined. We find the algorithm is able to identify price trends presented successfully  and  efficiently.\n    ",
        "submission_date": "2010-04-22T00:00:00",
        "last_modified_date": "2010-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.4089",
        "title": "Real-Time Alert Correlation with Type Graphs",
        "authors": [
            "Gianni Tedesco",
            "Uwe Aickelin"
        ],
        "abstract": "The premise of automated alert correlation is to accept that false alerts from a low level intrusion detection system are inevitable and use attack models to explain the output in an understandable way. Several algorithms exist for this purpose which use attack graphs to model the ways in which attacks can be combined. These algorithms can be classified in to two broad categories namely scenario-graph approaches, which create an attack model starting from a vulnerability assessment and type-graph approaches which rely on an abstract model of the relations between attack types. Some research in to improving the efficiency of type-graph correlation has been carried out but this research has ignored the hypothesizing of missing alerts. Our work is to present a novel type-graph algorithm which unifies correlation and hypothesizing in to a single operation. Our experimental results indicate that the approach is extremely efficient in the face of intensive alerts and produces compact output graphs comparable to other techniques.\n    ",
        "submission_date": "2010-04-23T00:00:00",
        "last_modified_date": "2010-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.4095",
        "title": "STORM - A Novel Information Fusion and Cluster Interpretation Technique",
        "authors": [
            "Jan Feyereisl",
            "Uwe Aickelin"
        ],
        "abstract": "Analysis  of data without labels is commonly  subject  to scrutiny by unsupervised machine learning  techniques. Such techniques provide  more  meaningful representations,  useful  for  better understanding of a problem  at hand,  than by looking only at the  data itself. Although abundant expert  knowledge exists in many areas where unlabelled data is examined, such knowledge is rarely  incorporated into automatic analysis. Incorporation of expert  knowledge  is frequently a matter of  combining  multiple data sources from disparate hypothetical spaces. In cases where such spaces  belong to different data types, this  task  becomes  even more challenging. In  this  paper  we  present  a novel immune-inspired method  that enables  the  fusion of such disparate types of data for a specific set of problems. We  show that our  method  provides  a better visual  understanding  of one hypothetical space  with  the  help  of data from  another hypothetical space.  We  believe  that our  model  has  implications  for the field of exploratory data analysis  and  knowledge discovery.\n    ",
        "submission_date": "2010-04-23T00:00:00",
        "last_modified_date": "2010-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.4342",
        "title": "Towards Closed World Reasoning in Dynamic Open Worlds (Extended Version)",
        "authors": [
            "Martin Slota",
            "Jo\u00e3o Leite"
        ],
        "abstract": "The need for integration of ontologies with nonmonotonic rules has been gaining importance in a number of areas, such as the Semantic Web. A number of researchers addressed this problem by proposing a unified semantics for hybrid knowledge bases composed of both an ontology (expressed in a fragment of first-order logic) and nonmonotonic rules. These semantics have matured over the years, but only provide solutions for the static case when knowledge does not need to evolve. In this paper we take a first step towards addressing the dynamics of hybrid knowledge bases. We focus on knowledge updates and, considering the state of the art of belief update, ontology update and rule update, we show that current solutions are only partial and difficult to combine. Then we extend the existing work on ABox updates with rules, provide a semantics for such evolving hybrid knowledge bases and study its basic properties. To the best of our knowledge, this is the first time that an update operator is proposed for hybrid knowledge bases.\n    ",
        "submission_date": "2010-04-25T00:00:00",
        "last_modified_date": "2010-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.4734",
        "title": "On the comparison of plans: Proposition of an instability measure for dynamic machine scheduling",
        "authors": [
            "Martin Josef Geiger"
        ],
        "abstract": "On the basis of an analysis of previous research, we present a generalized approach for measuring the difference of plans with an exemplary application to machine scheduling. Our work is motivated by the need for such measures, which are used in dynamic scheduling and planning situations. In this context, quantitative approaches are needed for the assessment of the robustness and stability of schedules. Obviously, any `robustness' or `stability' of plans has to be defined w. r. t. the particular situation and the requirements of the human decision maker. Besides the proposition of an instability measure, we therefore discuss possibilities of obtaining meaningful information from the decision maker for the implementation of the introduced approach.\n    ",
        "submission_date": "2010-04-27T00:00:00",
        "last_modified_date": "2010-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.4801",
        "title": "Ontology-based inference for causal explanation",
        "authors": [
            "Philippe Besnard",
            "Marie-Odile Cordier",
            "Yves Moinard"
        ],
        "abstract": "We define an inference system to capture explanations based on causal statements, using an ontology in the form of an IS-A hierarchy. We first introduce a simple logical language which makes it possible to express that a fact causes another fact and that a fact explains another fact. We present a set of formal inference patterns from causal statements to explanation statements. We introduce an elementary ontology which gives greater expressiveness to the system while staying close to propositional reasoning. We provide an inference system that captures the patterns discussed, firstly in a purely propositional framework, then in a datalog (limited predicate) framework.\n    ",
        "submission_date": "2010-04-27T00:00:00",
        "last_modified_date": "2010-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.5215",
        "title": "System Dynamics Modelling of the Processes Involving the Maintenance of the Naive T Cell Repertoire",
        "authors": [
            "Grazziela P. Figueredo",
            "Uwe Aickelin",
            "Amanda Whitbrook"
        ],
        "abstract": "The  study  of immune  system  aging,  i.e. immunosenescence,  is a relatively  new research  topic.    It  deals  with  understanding the  processes of immunodegradation that indicate   signs  of  functionality  loss possibly  leading  to  death.    Even  though it  is not  possible  to  prevent immunosenescence,  there  is great  benefit  in  comprehending  its  causes,  which  may  help to reverse some of the  damage  done and thus improve life expectancy.   One of the main factors influencing the process of immunosenescence  is the  number  and  phenotypical  variety  of naive T cells in an individual.   This  work presents  a review of immunosenescence, proposes system dynamics modelling of the processes involving the  maintenance  of the  naive  T  cell repertoire and  presents  some preliminary results.\n    ",
        "submission_date": "2010-04-29T00:00:00",
        "last_modified_date": "2010-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.5222",
        "title": "The Application of a Dendritic Cell Algorithm to a Robotic Classifier",
        "authors": [
            "Robert Oates",
            "Julie Greensmith",
            "Uwe Aickelin",
            "Jonathan M. Garibaldi",
            "Graham Kendall"
        ],
        "abstract": "The dendritic cell algorithm is an immune-inspired technique for processing  time-dependant data. Here we propose it as a possible solution for a robotic  classification problem. The  dendritic cell algorithm is implemented on  a  real  robot  and  an  investigation  is performed into the effects of varying  the migration threshold median  for the cell population. The algorithm performs  well on a classification task with very little tuning. Ways of extending the implementation to allow it to be used as a classifier within the field of robotic security are suggested.\n    ",
        "submission_date": "2010-04-29T00:00:00",
        "last_modified_date": "2010-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.0080",
        "title": "Electronic Geometry Textbook: A Geometric Textbook Knowledge Management System",
        "authors": [
            "Xiaoyu Chen"
        ],
        "abstract": "Electronic Geometry Textbook is a knowledge management system that manages geometric textbook knowledge to enable users to construct and share dynamic geometry textbooks interactively and efficiently. Based on a knowledge base organizing and storing the knowledge represented in specific languages, the system implements interfaces for maintaining the data representing that knowledge as well as relations among those data, for automatically generating readable documents for viewing or printing, and for automatically discovering the relations among knowledge data. An interface has been developed for users to create geometry textbooks with automatic checking, in real time, of the consistency of the structure of each resulting textbook. By integrating an external geometric theorem prover and an external dynamic geometry software package, the system offers the facilities for automatically proving theorems and generating dynamic figures in the created textbooks. This paper provides a comprehensive account of the current version of Electronic Geometry Textbook.\n    ",
        "submission_date": "2010-05-01T00:00:00",
        "last_modified_date": "2010-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.0089",
        "title": "The Exact Closest String Problem as a Constraint Satisfaction Problem",
        "authors": [
            "Tom Kelsey",
            "Lars Kotthoff"
        ],
        "abstract": "We report (to our knowledge) the first evaluation of Constraint Satisfaction as a computational framework for solving closest string problems. We show that careful consideration of symbol occurrences can provide search heuristics that provide several orders of magnitude speedup at and above the optimal distance. We also report (to our knowledge) the first analysis and evaluation -- using any technique -- of the computational difficulties involved in the identification of all closest strings for a given input set. We describe algorithms for web-scale distributed solution of closest string problems, both purely based on AI backtrack search and also hybrid numeric-AI methods.\n    ",
        "submission_date": "2010-05-01T00:00:00",
        "last_modified_date": "2010-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.0104",
        "title": "Joint Structured Models for Extraction from Overlapping Sources",
        "authors": [
            "Rahul Gupta",
            "Sunita Sarawagi"
        ],
        "abstract": "We consider the problem of jointly training structured models for extraction from sources whose instances enjoy partial overlap.  This has important applications like user-driven ad-hoc information extraction on the web. Such applications present new challenges in terms of the number of sources and their arbitrary pattern of overlap not seen by earlier collective training schemes applied on two sources. We present an agreement-based learning framework and alternatives within it to trade-off tractability, robustness to noise, and extent of agreement.  We provide a principled scheme to discover low-noise agreement sets in unlabeled data across the sources.  Through extensive experiments over 58 real datasets, we establish that our method of additively rewarding agreement over maximal segments of text provides the best trade-offs, and also scores over alternatives such as collective inference, staged training, and multi-view learning.\n    ",
        "submission_date": "2010-05-01T00:00:00",
        "last_modified_date": "2010-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.0605",
        "title": "An approach to visualize the course of solving of a research task in humans",
        "authors": [
            "Vladimir L. Gavrikov",
            "Rem G. Khlebopros"
        ],
        "abstract": "A technique to study the dynamics of solving of a research task is suggested. The research task was based on specially developed software Right- Wrong Responder (RWR), with the participants having to reveal the response logic of the program. The participants interacted with the program in the form of a semi-binary dialogue, which implies the feedback responses of only two kinds - \"right\" or \"wrong\". The technique has been applied to a small pilot group of volunteer participants. Some of them have successfully solved the task (solvers) and some have not (non-solvers). In the beginning of the work, the solvers did more wrong moves than non-solvers, and they did less wrong moves closer to the finish of the work. A phase portrait of the work both in solvers and non-solvers showed definite cycles that may correspond to sequences of partially true hypotheses that may be formulated by the participants during the solving of the task.\n    ",
        "submission_date": "2010-04-26T00:00:00",
        "last_modified_date": "2010-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.0608",
        "title": "Informal Concepts in Machines",
        "authors": [
            "Kurt Ammon"
        ],
        "abstract": "This paper constructively proves the existence of an effective procedure generating a computable (total) function that is not contained in any given effectively enumerable set of such functions. The proof implies the existence of machines that process informal concepts such as computable (total) functions beyond the limits of any given Turing machine or formal system, that is,  these machines can, in a certain sense, \"compute\" function values beyond these limits. We call these machines creative.  We argue that any \"intelligent\" machine should be capable of processing informal concepts such as computable (total) functions, that is, it should be creative. Finally, we introduce hypotheses on creative machines which were developed on the basis of theoretical investigations and experiments with computer programs. The hypotheses say that machine intelligence is the execution of a self-developing procedure starting from any universal programming language and any input.\n    ",
        "submission_date": "2010-05-04T00:00:00",
        "last_modified_date": "2010-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.0707",
        "title": "The Production of Probabilistic Entropy in Structure/Action Contingency Relations",
        "authors": [
            "Loet Leydesdorff"
        ],
        "abstract": "Luhmann (1984) defined society as a communication system which is structurally coupled to, but not an aggregate of, human action systems. The communication system is then considered as self-organizing (\"autopoietic\"), as are human actors. Communication systems can be studied by using Shannon's (1948) mathematical theory of communication. The update of a network by action at one of the local nodes is then a well-known problem in artificial intelligence (Pearl 1988). By combining these various theories, a general algorithm for probabilistic structure/action contingency can be derived. The consequences of this contingency for each system, its consequences for their further histories, and the stabilization on each side by counterbalancing mechanisms are discussed, in both mathematical and theoretical terms. An empirical example is elaborated.\n    ",
        "submission_date": "2010-05-05T00:00:00",
        "last_modified_date": "2010-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.0896",
        "title": "A two-step fusion process for multi-criteria decision applied to natural hazards in mountains",
        "authors": [
            "Jean-Marc Tacnet",
            "Mireille Batton-Hubert",
            "Jean Dezert"
        ],
        "abstract": "Mountain river torrents and snow avalanches generate human and material damages with dramatic consequences. Knowledge about natural phenomenona is often lacking and expertise is required for decision and risk management purposes using multi-disciplinary quantitative or qualitative approaches. Expertise is considered as a decision process based on imperfect information coming from more or less reliable and conflicting sources. A methodology mixing the Analytic Hierarchy Process (AHP), a multi-criteria aid-decision method, and information fusion using Belief Function Theory is described. Fuzzy Sets and Possibilities theories allow to transform quantitative and qualitative criteria into a common frame of discernment for decision in Dempster-Shafer Theory (DST ) and Dezert-Smarandache Theory (DSmT) contexts. Main issues consist in basic belief assignments elicitation, conflict identification and management, fusion rule choices, results validation but also in specific needs to make a difference between importance and reliability and uncertainty in the fusion process.\n    ",
        "submission_date": "2010-05-06T00:00:00",
        "last_modified_date": "2010-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.0917",
        "title": "On Building a Knowledge Base for Stability Theory",
        "authors": [
            "Agnieszka Rowinska-Schwarzweller",
            "Christoph Schwarzweller"
        ],
        "abstract": "A lot of mathematical knowledge has been formalized and stored in repositories by now: different mathematical theorems and theories have been taken into consideration and included in mathematical repositories. Applications more distant from pure mathematics, however --- though based on these theories --- often need more detailed knowledge about the underlying theories. In this paper we present an example Mizar formalization from the area of electrical engineering focusing on stability theory which is based on complex analysis. We discuss what kind of special knowledge is necessary here and which amount of this knowledge is included in existing repositories.\n    ",
        "submission_date": "2010-05-06T00:00:00",
        "last_modified_date": "2010-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.1475",
        "title": "How to correctly prune tropical trees",
        "authors": [
            "Jean-Vincent Loddo",
            "Luca Saiu"
        ],
        "abstract": "We present tropical games, a generalization of combinatorial min-max games based on tropical algebras. Our model breaks the traditional symmetry of rational zero-sum games where players have exactly opposed goals (min vs. max), is more widely applicable than min-max and also supports a form of pruning, despite it being less effective than alpha-beta. Actually, min-max games may be seen as particular cases where both the game and its dual are tropical: when the dual of a tropical game is also tropical, the power of alpha-beta is completely recovered. We formally develop the model and prove that the tropical pruning strategy is correct, then conclude by showing how the problem of approximated parsing can be modeled as a tropical game, profiting from pruning.\n    ",
        "submission_date": "2010-05-10T00:00:00",
        "last_modified_date": "2010-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.1518",
        "title": "Recognizability of Individual Creative Style Within and Across Domains: Preliminary Studies",
        "authors": [
            "Liane Gabora"
        ],
        "abstract": "It is hypothesized that creativity arises from the self-mending capacity of an internal model of the world, or worldview. The uniquely honed worldview of a creative individual results in a distinctive style that is recognizable within and across domains. It is further hypothesized that creativity is domaingeneral in the sense that there exist multiple avenues by which the distinctiveness of one's worldview can be expressed. These hypotheses were tested using art students and creative writing students. Art students guessed significantly above chance both which painting was done by which of five famous artists, and which artwork was done by which of their peers. Similarly, creative writing students guessed significantly above chance both which passage was written by which of five famous writers, and which passage was written by which of their peers. These findings support the hypothesis that creative style is recognizable. Moreover, creative writing students guessed significantly above chance which of their peers produced particular works of art, supporting the hypothesis that creative style is recognizable not just within but across domains.\n    ",
        "submission_date": "2010-05-10T00:00:00",
        "last_modified_date": "2019-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.1567",
        "title": "On The Power of Tree Projections: Structural Tractability of Enumerating CSP Solutions",
        "authors": [
            "Gianluigi Greco",
            "Francesco Scarcello"
        ],
        "abstract": "The problem of deciding whether CSP instances admit solutions has been deeply studied in the literature, and several structural tractability results have been derived so far. However, constraint satisfaction comes in practice as a computation problem where the focus is either on finding one solution, or on enumerating all solutions, possibly projected to some given set of output variables. The paper investigates the structural tractability of the problem of enumerating (possibly projected) solutions, where tractability means here computable with polynomial delay (WPD), since in general exponentially many solutions may be computed. A general framework based on the notion of tree projection of hypergraphs is considered, which generalizes all known decomposition methods. Tractability results have been obtained both for classes of structures where output variables are part of their specification, and for classes of structures where computability WPD must be ensured for any possible set of output variables. These results are shown to be tight, by exhibiting dichotomies for classes of structures having bounded arity and where the tree decomposition method is considered.\n    ",
        "submission_date": "2010-05-10T00:00:00",
        "last_modified_date": "2010-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.1716",
        "title": "Heuristics in Conflict Resolution",
        "authors": [
            "Christian Drescher",
            "Martin Gebser",
            "Benjamin Kaufmann",
            "Torsten Schaub"
        ],
        "abstract": "Modern solvers for Boolean Satisfiability (SAT) and Answer Set Programming (ASP) are based on sophisticated Boolean constraint solving techniques. In both areas, conflict-driven learning and related techniques constitute key features whose application is enabled by conflict analysis. Although various conflict analysis schemes have been proposed, implemented, and studied both theoretically and practically in the SAT area, the heuristic aspects involved in conflict analysis have not yet received much attention. Assuming a fixed conflict analysis scheme, we address the open question of how to identify \"good'' reasons for conflicts, and we investigate several heuristics for conflict analysis in ASP solving. To our knowledge, a systematic study like ours has not yet been performed in the SAT area, thus, it might be beneficial for both the field of ASP as well as the one of SAT solving.\n    ",
        "submission_date": "2010-05-11T00:00:00",
        "last_modified_date": "2010-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.1860",
        "title": "Feature Selection Using Regularization in Approximate Linear Programs for Markov Decision Processes",
        "authors": [
            "Marek Petrik",
            "Gavin Taylor",
            "Ron Parr",
            "Shlomo Zilberstein"
        ],
        "abstract": "Approximate dynamic programming has been used successfully in a large variety of domains, but it relies on a small set of provided approximation features to calculate solutions reliably. Large and rich sets of features can cause existing algorithms to overfit because of a limited number of samples. We address this shortcoming using $L_1$ regularization in approximate linear programming. Because the proposed method can automatically select the appropriate richness of features, its performance does not degrade with an increasing number of features. These results rely on new and stronger sampling bounds for regularized approximate linear programs. We also propose a computationally efficient homotopy method. The empirical evaluation of the approach shows that the proposed method performs well on simple MDPs and standard benchmark problems.\n    ",
        "submission_date": "2010-05-11T00:00:00",
        "last_modified_date": "2010-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.2815",
        "title": "Evolving Genes to Balance a Pole",
        "authors": [
            "Miguel Nicolau",
            "Marc Schoenauer",
            "W. Banzhaf"
        ],
        "abstract": "We discuss how to use a Genetic Regulatory Network as an evolutionary representation to solve a typical GP reinforcement problem, the pole balancing. The network is a modified version of an Artificial Regulatory Network proposed a few years ago, and the task could be solved only by finding a proper way of connecting inputs and outputs to the network. We show that the representation is able to generalize well over the problem domain, and discuss the performance of different models of this kind.\n    ",
        "submission_date": "2010-05-17T00:00:00",
        "last_modified_date": "2010-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.3502",
        "title": "Using machine learning to make constraint solver implementation decisions",
        "authors": [
            "Lars Kotthoff",
            "Ian Gent",
            "Ian Miguel"
        ],
        "abstract": "Programs to solve so-called constraint problems are complex pieces of software which require many design decisions to be made more or less arbitrarily by the implementer. These decisions affect the performance of the finished solver significantly. Once a design decision has been made, it cannot easily be reversed, although a different decision may be more appropriate for a particular problem.\n",
        "submission_date": "2010-05-19T00:00:00",
        "last_modified_date": "2010-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.4025",
        "title": "A Soft Computing Model for Physicians' Decision Process",
        "authors": [
            "Siddharths Sankar Biswas"
        ],
        "abstract": "In this paper the author presents a kind of Soft Computing Technique, mainly an application of fuzzy set theory of Prof. Zadeh [16], on a problem of Medical Experts Systems. The choosen problem is on design of a physician's decision model which can take crisp as well as fuzzy data as input, unlike the traditional models. The author presents a mathematical model based on fuzzy set theory for physician aided evaluation of a complete representation of information emanating from the initial interview including patient past history, present symptoms, and signs observed upon physical examination and results of clinical and diagnostic tests.\n    ",
        "submission_date": "2010-05-21T00:00:00",
        "last_modified_date": "2010-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.4159",
        "title": "The Complexity of Manipulating $k$-Approval Elections",
        "authors": [
            "Andrew Lin"
        ],
        "abstract": "An important problem in computational social choice theory is the complexity of undesirable behavior among agents, such as control, manipulation, and bribery in election systems. These kinds of voting strategies are often tempting at the individual level but disastrous for the agents as a whole. Creating election systems where the determination of such strategies is difficult is thus an important goal.\n",
        "submission_date": "2010-05-23T00:00:00",
        "last_modified_date": "2012-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.4272",
        "title": "Inaccuracy Minimization by Partioning Fuzzy Data Sets - Validation of Analystical Methodology",
        "authors": [
            "G. Arutchelvan",
            "S. K. Srivatsa",
            "R. Jagannathan"
        ],
        "abstract": "In the last two decades, a number of methods have been proposed for forecasting based on fuzzy time series. Most of the fuzzy time series methods are presented for forecasting of car road accidents. However, the forecasting accuracy rates of the existing methods are not good enough. In this paper, we compared our proposed new method of fuzzy time series forecasting with existing methods. Our method is based on means based partitioning of the historical data of car road accidents. The proposed method belongs to the kth order and time-variant methods. The proposed method can get the best forecasting accuracy rate for forecasting the car road accidents than the existing methods.\n    ",
        "submission_date": "2010-05-24T00:00:00",
        "last_modified_date": "2010-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.4298",
        "title": "Distantly Labeling Data for Large Scale Cross-Document Coreference",
        "authors": [
            "Sameer Singh",
            "Michael Wick",
            "Andrew McCallum"
        ],
        "abstract": "Cross-document coreference, the problem of resolving entity mentions across multi-document collections, is crucial to automated knowledge base construction and data mining tasks. However, the scarcity of large labeled data sets has hindered supervised machine learning research for this task. In this paper we develop and demonstrate an approach based on ``distantly-labeling'' a data set from which we can train a discriminative cross-document coreference model. In particular we build a dataset of more than a million people mentions extracted from 3.5 years of New York Times articles, leverage Wikipedia for distant labeling with a generative model (and measure the reliability of such labeling); then we train and evaluate a conditional random field coreference model that has factors on cross-document entities as well as mention-pairs. This coreference model obtains high accuracy in resolving mentions and entities that are not present in the training data, indicating applicability to non-Wikipedia data. Given the large amount of data, our work is also an exercise demonstrating the scalability of our approach.\n    ",
        "submission_date": "2010-05-24T00:00:00",
        "last_modified_date": "2010-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.4447",
        "title": "Evidence Algorithm and System for Automated Deduction: A Retrospective View",
        "authors": [
            "Alexander Lyaletski",
            "Konstantin Verchinine"
        ],
        "abstract": "A research project aimed at the development of an automated theorem proving system was started in Kiev (Ukraine) in early 1960s. The mastermind of the project, Academician ",
        "submission_date": "2010-05-24T00:00:00",
        "last_modified_date": "2010-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.4496",
        "title": "Combining Naive Bayes and Decision Tree for Adaptive Intrusion Detection",
        "authors": [
            "Dewan Md. Farid",
            "Nouria Harbi",
            "Mohammad Zahidur Rahman"
        ],
        "abstract": "In this paper, a new learning algorithm for adaptive network intrusion detection using naive Bayesian classifier and decision tree is presented, which performs balance detections and keeps false positives at acceptable level for different types of network attacks, and eliminates redundant attributes as well as contradictory examples from training data that make the detection model complex. The proposed algorithm also addresses some difficulties of data mining such as handling continuous attribute, dealing with missing attribute values, and reducing noise in training data. Due to the large volumes of security audit data as well as the complex and dynamic properties of intrusion behaviours, several data miningbased intrusion detection techniques have been applied to network-based traffic data and host-based data in the last decades. However, there remain various issues needed to be examined towards current intrusion detection systems (IDS). We tested the performance of our proposed algorithm with existing learning algorithms by employing on the KDD99 benchmark intrusion detection dataset. The experimental results prove that the proposed algorithm achieved high detection rates (DR) and significant reduce false positives (FP) for different types of network intrusions using limited computational resources.\n    ",
        "submission_date": "2010-05-25T00:00:00",
        "last_modified_date": "2010-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.4592",
        "title": "Automated Reasoning and Presentation Support for Formalizing Mathematics in Mizar",
        "authors": [
            "Josef Urban",
            "Geoff Sutcliffe"
        ],
        "abstract": "This paper presents a combination of several automated reasoning and proof presentation tools with the Mizar system for formalization of mathematics. The combination forms an online service called MizAR, similar to the SystemOnTPTP service for first-order automated reasoning. The main differences to SystemOnTPTP are the use of the Mizar language that is oriented towards human mathematicians (rather than the pure first-order logic used in SystemOnTPTP), and setting the service in the context of the large Mizar Mathematical Library of previous theorems,definitions, and proofs (rather than the isolated problems that are solved in SystemOnTPTP). These differences poses new challenges and new opportunities for automated reasoning and for proof presentation tools. This paper describes the overall structure of MizAR, and presents the automated reasoning systems and proof presentation tools that are combined to make MizAR a useful mathematical service.\n    ",
        "submission_date": "2010-05-25T00:00:00",
        "last_modified_date": "2010-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.4963",
        "title": "Integrating Structured Metadata with Relational Affinity Propagation",
        "authors": [
            "Anon Plangprasopchok",
            "Kristina Lerman",
            "Lise Getoor"
        ],
        "abstract": "Structured and semi-structured data describing entities, taxonomies and ontologies appears in many domains. There is a huge interest in integrating structured information from multiple sources; however integrating structured data to infer complex common structures is a difficult task because the integration must aggregate similar structures while avoiding structural inconsistencies that may appear when the data is combined. In this work, we study the integration of structured social metadata: shallow personal hierarchies specified by many individual users on the SocialWeb, and focus on inferring a collection of integrated, consistent taxonomies. We frame this task as an optimization problem with structural constraints. We propose a new inference algorithm, which we refer to as Relational Affinity Propagation (RAP) that extends affinity propagation (Frey and Dueck 2007) by introducing structural constraints. We validate the approach on a real-world social media dataset, collected from the photosharing website Flickr. Our empirical results show that our proposed approach is able to construct deeper and denser structures compared to an approach using only the standard affinity propagation algorithm.\n    ",
        "submission_date": "2010-05-26T00:00:00",
        "last_modified_date": "2010-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.4989",
        "title": "A Formalization of the Turing Test",
        "authors": [
            "Evgeny Chutchev"
        ],
        "abstract": "The paper offers a mathematical formalization of the Turing test. This formalization makes it possible to establish the conditions under which some Turing machine will pass the Turing test and the conditions under which every Turing machine (or every Turing machine of the special class) will fail the Turing test.\n    ",
        "submission_date": "2010-05-27T00:00:00",
        "last_modified_date": "2010-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.5114",
        "title": "Growing a Tree in the Forest: Constructing Folksonomies by Integrating Structured Metadata",
        "authors": [
            "Anon Plangprasopchok",
            "Kristina Lerman",
            "Lise Getoor"
        ],
        "abstract": "Many social Web sites allow users to annotate the content with descriptive metadata, such as tags, and more recently to organize content hierarchically. These types of structured metadata provide valuable evidence for learning how a community organizes knowledge. For instance, we can aggregate many personal hierarchies into a common taxonomy, also known as a folksonomy, that will aid users in visualizing and browsing social content, and also to help them in organizing their own content. However, learning from social metadata presents several challenges, since it is sparse, shallow, ambiguous, noisy, and inconsistent. We describe an approach to folksonomy learning based on relational clustering, which exploits structured metadata contained in personal hierarchies. Our approach clusters similar hierarchies using their structure and tag statistics, then incrementally weaves them into a deeper, bushier tree. We study folksonomy learning using social metadata extracted from the photo-sharing site Flickr, and demonstrate that the proposed approach addresses the challenges. Moreover, comparing to previous work, the approach produces larger, more accurate folksonomies, and in addition, scales better.\n    ",
        "submission_date": "2010-05-27T00:00:00",
        "last_modified_date": "2010-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.5124",
        "title": "Proofs, proofs, proofs, and proofs",
        "authors": [
            "Manfred Kerber"
        ],
        "abstract": "In logic there is a clear concept of what constitutes a proof and what not. A proof is essentially defined as a finite sequence of formulae which are either axioms or derived by proof rules from formulae earlier in the sequence. Sociologically, however, it is more difficult to say what should constitute a proof and what not. In this paper we will look at different forms of proofs and try to clarify the concept of proof in the wider meaning of the term. This has implications on how proofs should be represented formally.\n    ",
        "submission_date": "2010-05-27T00:00:00",
        "last_modified_date": "2010-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.5268",
        "title": "An Empirical Study of the Manipulability of Single Transferable Voting",
        "authors": [
            "Toby Walsh"
        ],
        "abstract": "Voting is a simple mechanism to combine together the preferences of multiple agents. Agents may try to manipulate the result of voting by mis-reporting their preferences. One barrier that might exist to such manipulation is computational complexity. In particular, it has been shown that it is NP-hard to compute how to manipulate a number of different voting rules. However, NP-hardness only bounds the worst-case complexity. Recent theoretical results suggest that manipulation may often be easy in practice. In this paper, we study empirically the manipulability of single transferable voting (STV) to determine if computational complexity is really a barrier to manipulation. STV was one of the first voting rules shown to be NP-hard. It also appears one of the harder voting rules to manipulate. We sample a number of distributions of votes including uniform and real world elections. In almost every election in our experiments, it was easy to compute how a single agent could manipulate the election or to prove that manipulation by a single agent was impossible.\n    ",
        "submission_date": "2010-05-28T00:00:00",
        "last_modified_date": "2010-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.5270",
        "title": "Symmetries of Symmetry Breaking Constraints",
        "authors": [
            "George Katsirelos",
            "Toby Walsh"
        ],
        "abstract": "Symmetry is an important feature of many constraint programs. We show that any problem symmetry acting on a set of symmetry breaking constraints can be used to break symmetry. Different symmetries pick out different solutions in each symmetry class. This simple but powerful idea can be used in a number of different ways. We describe one application within model restarts, a search technique designed to reduce the conflict between symmetry breaking and the branching heuristic. In model restarts, we restart search periodically with a random symmetry of the symmetry breaking constraints. Experimental results show that this symmetry breaking technique is effective in practice on some standard benchmark problems.\n    ",
        "submission_date": "2010-05-28T00:00:00",
        "last_modified_date": "2010-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.5448",
        "title": "Failover in cellular automata",
        "authors": [
            "Shailesh Kumar",
            "Shrisha Rao"
        ],
        "abstract": "A cellular automata (CA) configuration is constructed that exhibits emergent failover. The configuration is based on standard Game of Life rules. Gliders and glider-guns form the core messaging structure in the configuration. The blinker is represented as the basic computational unit, and it is shown how it can be recreated in case of a failure. Stateless failover using primary-backup mechanism is demonstrated. The details of the CA components used in the configuration and its working are described, and a simulation of the complete configuration is also presented.\n    ",
        "submission_date": "2010-05-29T00:00:00",
        "last_modified_date": "2010-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.0274",
        "title": "Learning Probabilistic Hierarchical Task Networks to Capture User Preferences",
        "authors": [
            "Nan Li",
            "William Cushing",
            "Subbarao Kambhampati",
            "Sungwook Yoon"
        ],
        "abstract": "We propose automatically learning probabilistic Hierarchical Task Networks (pHTNs) in order to capture a user's preferences on plans, by observing only the user's behavior. HTNs are a common choice of representation for a variety of purposes in planning, including work on learning in planning. Our contributions are (a) learning structure and (b) representing preferences. In contrast, prior work employing HTNs considers learning method preconditions (instead of structure) and representing domain physics or search control knowledge (rather than preferences). Initially we will assume that the observed distribution of plans is an accurate representation of user preference, and then generalize to the situation where feasibility constraints frequently prevent the execution of preferred plans. In order to learn a distribution on plans we adapt an Expectation-Maximization (EM) technique from the discipline of (probabilistic) grammar induction, taking the perspective of task reductions as productions in a context-free grammar over primitive actions. To account for the difference between the distributions of possible and preferred plans we subsequently modify this core EM technique, in short, by rescaling its input.\n    ",
        "submission_date": "2010-06-02T00:00:00",
        "last_modified_date": "2010-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.0385",
        "title": "Brain-Like Stochastic Search: A Research Challenge and Funding Opportunity",
        "authors": [
            "Paul J. Werbos"
        ],
        "abstract": "Brain-Like Stochastic Search (BLiSS) refers to this task: given a family of utility functions U(u,A), where u is a vector of parameters or task descriptors, maximize or minimize U with respect to u, using networks (Option Nets) which input A and learn to generate good options u stochastically. This paper discusses why this is crucial to brain-like intelligence (an area funded by NSF) and to many applications, and discusses various possibilities for network design and training. The appendix discusses recent research, relations to work on stochastic optimization in operations research, and relations to engineering-based approaches to understanding neocortex.\n    ",
        "submission_date": "2010-06-01T00:00:00",
        "last_modified_date": "2010-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.0991",
        "title": "Variational Program Inference",
        "authors": [
            "Georges Harik",
            "Noam Shazeer"
        ],
        "abstract": "We introduce a framework for representing a variety of interesting problems as inference over the execution of probabilistic model programs. We represent a \"solution\" to such a problem as a guide program which runs alongside the model program and influences the model program's random choices, leading the model program to sample from a different distribution than from its priors. Ideally the guide program influences the model program to sample from the posteriors given the evidence. We show how the KL- divergence between the true posterior distribution and the distribution induced by the guided model program can be efficiently estimated (up to an additive constant) by sampling multiple executions of the guided model program. In addition, we show how to use the guide program as a proposal distribution in importance sampling to statistically prove lower bounds on the probability of the evidence and on the probability of a hypothesis and the evidence. We can use the quotient of these two bounds as an estimate of the conditional probability of the hypothesis given the evidence. We thus turn the inference problem into a heuristic search for better guide programs.\n    ",
        "submission_date": "2010-06-04T00:00:00",
        "last_modified_date": "2010-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.1030",
        "title": "Rasch-based high-dimensionality data reduction and class prediction with applications to microarray gene expression data",
        "authors": [
            "Andrej Kastrin",
            "Borut Peterlin"
        ],
        "abstract": "Class prediction is an important application of microarray gene expression data analysis. The high-dimensionality of microarray data, where number of genes (variables) is very large compared to the number of samples (obser- vations), makes the application of many prediction techniques (e.g., logistic regression, discriminant analysis) difficult. An efficient way to solve this prob- lem is by using dimension reduction statistical techniques. Increasingly used in psychology-related applications, Rasch model (RM) provides an appealing framework for handling high-dimensional microarray data. In this paper, we study the potential of RM-based modeling in dimensionality reduction with binarized microarray gene expression data and investigate its prediction ac- curacy in the context of class prediction using linear discriminant analysis. Two different publicly available microarray data sets are used to illustrate a general framework of the approach. Performance of the proposed method is assessed by re-randomization scheme using principal component analysis (PCA) as a benchmark method. Our results show that RM-based dimension reduction is as effective as PCA-based dimension reduction. The method is general and can be applied to the other high-dimensional data problems.\n    ",
        "submission_date": "2010-06-05T00:00:00",
        "last_modified_date": "2010-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.1080",
        "title": "The Dilated Triple",
        "authors": [
            "Marko A. Rodriguez",
            "Alberto Pepe",
            "Joshua Shinavier"
        ],
        "abstract": "The basic unit of meaning on the Semantic Web is the RDF statement, or triple, which combines a distinct subject, predicate and object to make a definite assertion about the world. A set of triples constitutes a graph, to which they give a collective meaning. It is upon this simple foundation that the rich, complex knowledge structures of the Semantic Web are built. Yet the very expressiveness of RDF, by inviting comparison with real-world knowledge, highlights a fundamental shortcoming, in that RDF is limited to statements of absolute fact, independent of the context in which a statement is asserted. This is in stark contrast with the thoroughly context-sensitive nature of human thought. The model presented here provides a particularly simple means of contextualizing an RDF triple by associating it with related statements in the same graph. This approach, in combination with a notion of graph similarity, is sufficient to select only those statements from an RDF graph which are subjectively most relevant to the context of the requesting process.\n    ",
        "submission_date": "2010-06-06T00:00:00",
        "last_modified_date": "2010-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.1190",
        "title": "Game Information System",
        "authors": [
            "Spits Warnars H.L.H"
        ],
        "abstract": "In this Information system age many organizations consider information system as their weapon to compete or gain competitive advantage or give the best services for non profit organizations. Game Information System as combining Information System and game is breakthrough to achieve organizations' performance. The Game Information System will run the Information System with game and how game can be implemented to run the Information System. Game is not only for fun and entertainment, but will be a challenge to combine fun and entertainment with Information System. The Challenge to run the information system with entertainment, deliver the entertainment with information system all at once. Game information system can be implemented in many sectors as like the information system itself but in difference's view. A view of game which people can joy and happy and do their transaction as a fun things.\n    ",
        "submission_date": "2010-06-07T00:00:00",
        "last_modified_date": "2010-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.1512",
        "title": "The Deterministic Dendritic Cell Algorithm",
        "authors": [
            "Julie Greensmith",
            "Uwe Aickelin"
        ],
        "abstract": "The Dendritic Cell Algorithm is an immune-inspired algorithm orig- inally based on the function of natural dendritic cells. The original instantiation of the algorithm is a highly stochastic algorithm. While the performance of the algorithm is good when applied to large real-time datasets, it is difficult to anal- yse due to the number of random-based elements. In this paper a deterministic version of the algorithm is proposed, implemented and tested using a port scan dataset to provide a controllable system. This version consists of a controllable amount of parameters, which are experimented with in this paper. In addition the effects are examined of the use of time windows and variation on the number of cells, both which are shown to influence the algorithm. Finally a novel metric for the assessment of the algorithms output is introduced and proves to be a more sensitive metric than the metric used with the original Dendritic Cell Algorithm.\n    ",
        "submission_date": "2010-06-08T00:00:00",
        "last_modified_date": "2010-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.1518",
        "title": "The DCA:SOMe Comparison A comparative study between two biologically-inspired algorithms",
        "authors": [
            "Julie Greensmith",
            "Jan Feyereisl",
            "Uwe Aickelin"
        ],
        "abstract": "The Dendritic Cell Algorithm (DCA) is an immune-inspired algorithm, developed for the purpose of anomaly detection. The algorithm performs multi-sensor data fusion and correlation which results in a 'context aware' detection system. Previous applications of the DCA have included the detection of potentially malicious port scanning activity, where it has produced high rates of true positives and low rates of false positives. In this work we aim to compare the performance of the DCA and of a Self-Organizing Map (SOM) when applied to the detection of SYN port scans, through experimental analysis. A SOM is an ideal candidate for comparison as it shares similarities with the DCA in terms of the data fusion method employed. It is shown that the results of the two systems are comparable, and both produce false positives for the same processes. This shows that the DCA can produce anomaly detection results to the same standard as an established technique.\n    ",
        "submission_date": "2010-06-08T00:00:00",
        "last_modified_date": "2010-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.1526",
        "title": "The Motif Tracking Algorithm",
        "authors": [
            "William Wilson",
            "Philip Birkin",
            "Uwe Aickelin"
        ],
        "abstract": "The search for patterns or motifs in data represents a problem area of key interest to finance and economic researchers. In this paper we introduce the Motif Tracking Algorithm, a novel immune inspired pattern identification tool that is able to identify unknown motifs of a non specified length which repeat within time series data. The power of the algorithm comes from the fact that it uses a small number of parameters with minimal assumptions regarding the data being examined or the underlying motifs. Our interest lies in applying the algorithm to financial time series data to identify unknown patterns that exist. The algorithm is tested using three separate data sets. Particular suitability to financial data is shown by applying it to oil price data. In all cases the algorithm identifies the presence of a motif population in a fast and efficient manner due to the utilisation of an intuitive symbolic representation. The resulting population of motifs is shown to have considerable potential value for other applications such as forecasting and algorithm seeding.\n    ",
        "submission_date": "2010-06-08T00:00:00",
        "last_modified_date": "2010-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.1537",
        "title": "New worst upper bound for #SAT",
        "authors": [
            "Junping Zhou",
            "Minghao Yin",
            "Chunguang Zhou"
        ],
        "abstract": "The rigorous theoretical analyses of algorithms for #SAT have been proposed in the literature. As we know, previous algorithms for solving #SAT have been analyzed only regarding the number of variables as the parameter. However, the time complexity for solving #SAT instances depends not only on the number of variables, but also on the number of clauses. Therefore, it is significant to exploit the time complexity from the other point of view, i.e. the number of clauses. In this paper, we present algorithms for solving #2-SAT and #3-SAT with rigorous complexity analyses using the number of clauses as the parameter. By analyzing the algorithms, we obtain the new worst-case upper bounds O(1.1892m) for #2-SAT and O(1.4142m) for #3-SAT, where m is the number of clauses.\n    ",
        "submission_date": "2010-06-08T00:00:00",
        "last_modified_date": "2010-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.1563",
        "title": "ToLeRating UR-STD",
        "authors": [
            "Jan Feyereisl",
            "Uwe Aickelin"
        ],
        "abstract": "A new emerging paradigm of Uncertain Risk of Suspicion, Threat and Danger, observed across the field of information security, is described. Based on this paradigm a novel approach to anomaly detection is presented. Our approach is based on a simple yet powerful analogy from the innate part of the human immune system, the Toll-Like Receptors. We argue that such receptors incorporated as part of an anomaly detector enhance the detector's ability to distinguish normal and anomalous behaviour. In addition we propose that Toll-Like Receptors enable the classification of detected anomalies based on the types of attacks that perpetrate the anomalous behaviour. Classification of such type is either missing in existing literature or is not fit for the purpose of reducing the burden of an administrator of an intrusion detection system. For our model to work, we propose the creation of a taxonomy of the digital Acytota, based on which our receptors are created.\n    ",
        "submission_date": "2010-06-08T00:00:00",
        "last_modified_date": "2010-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.1568",
        "title": "Towards a Conceptual Framework for Innate Immunity",
        "authors": [
            "Jamie Twycross",
            "Uwe Aickelin"
        ],
        "abstract": "Innate immunity now occupies a central role in immunology. However, artificial immune system models have largely been inspired by adaptive not innate immunity. This paper reviews the biological principles and properties of innate immunity and, adopting a conceptual framework, asks how these can be incorporated into artificial models. The aim is to outline a meta-framework for models of innate immunity.\n    ",
        "submission_date": "2010-06-08T00:00:00",
        "last_modified_date": "2010-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.1681",
        "title": "Towards the Design of Heuristics by Means of Self-Assembly",
        "authors": [
            "German Terrazas",
            "Dario Landa-Silva",
            "Natalio Krasnogor"
        ],
        "abstract": "The current investigations on hyper-heuristics design have sprung up in two different flavours: heuristics that choose heuristics and heuristics that generate heuristics. In the latter, the goal is to develop a problem-domain independent strategy to automatically generate a good performing heuristic for the problem at hand. This can be done, for example, by automatically selecting and combining different low-level heuristics into a problem specific and effective strategy. Hyper-heuristics raise the level of generality on automated problem solving by attempting to select and/or generate tailored heuristics for the problem at hand. Some approaches like genetic programming have been proposed for this. In this paper, we explore an elegant nature-inspired alternative based on self-assembly construction processes, in which structures emerge out of local interactions between autonomous components. This idea arises from previous works in which computational models of self-assembly were subject to evolutionary design in order to perform the automatic construction of user-defined structures. Then, the aim of this paper is to present a novel methodology for the automated design of heuristics by means of self-assembly.\n    ",
        "submission_date": "2010-06-09T00:00:00",
        "last_modified_date": "2010-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.1701",
        "title": "Virtual information system on working area",
        "authors": [
            "Spits Warnars"
        ],
        "abstract": "In order to get strategic positioning for competition in business organization, the information system must be ahead in this information age where the information as one of the weapons to win the competition and in the right hand the information will become a right bullet. The information system with the information technology support isn't enough if just only on internet or implemented with internet technology. The growth of information technology as tools for helping and making people easy to use must be accompanied by wanting to make fun and happy when they make contact with the information technology itself. Basically human like to play, since childhood human have been playing, free and happy and when human grow up they can't play as much as when human was in their childhood. We have to develop the information system which is not perform information system itself but can help human to explore their natural instinct for playing, making fun and happiness when they interact with the information system. Virtual information system is the way to present playing and having fun atmosphere on working area.\n    ",
        "submission_date": "2010-06-09T00:00:00",
        "last_modified_date": "2010-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.1703",
        "title": "Indonesian Earthquake Decision Support System",
        "authors": [
            "Spits Warnars"
        ],
        "abstract": "Earthquake DSS is an information technology environment which can be used by government to sharpen, make faster and better the earthquake mitigation decision. Earthquake DSS can be delivered as E-government which is not only for government itself but in order to guarantee each citizen's rights for education, training and information about earthquake and how to overcome the earthquake. Knowledge can be managed for future use and would become mining by saving and maintain all the data and information about earthquake and earthquake mitigation in Indonesia. Using Web technology will enhance global access and easy to use. Datawarehouse as unNormalized database for multidimensional analysis will speed the query process and increase reports variation. Link with other Disaster DSS in one national disaster DSS, link with other government information system and international will enhance the knowledge and sharpen the reports.\n    ",
        "submission_date": "2010-06-09T00:00:00",
        "last_modified_date": "2010-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.1786",
        "title": "Measuring Meaning on the World-Wide Web",
        "authors": [
            "Diederik Aerts"
        ],
        "abstract": "We introduce the notion of the 'meaning bound' of a word with respect to another word by making use of the World-Wide Web as a conceptual environment for meaning. The meaning of a word with respect to another word is established by multiplying the product of the number of webpages containing both words by the total number of webpages of the World-Wide Web, and dividing the result by the product of the number of webpages for each of the single words. We calculate the meaning bounds for several words and analyze different aspects of these by looking at specific examples.\n    ",
        "submission_date": "2010-06-09T00:00:00",
        "last_modified_date": "2010-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.1930",
        "title": "The Pet-Fish problem on the World-Wide Web",
        "authors": [
            "Diederik Aerts",
            "Marek Czachor",
            "Bart D'Hooghe",
            "Sandro Sozzo"
        ],
        "abstract": "We identify the presence of Pet-Fish problem situations and the corresponding Guppy effect of concept theory on the World-Wide Web. For this purpose, we introduce absolute weights for words expressing concepts and relative weights between words expressing concepts, and the notion of 'meaning bound' between two words expressing concepts, making explicit use of the conceptual structure of the World-Wide Web. The Pet-Fish problem occurs whenever there are exemplars - in the case of Pet and Fish these can be Guppy or Goldfish - for which the meaning bound with respect to the conjunction is stronger than the meaning bounds with respect to the individual concepts.\n    ",
        "submission_date": "2010-06-10T00:00:00",
        "last_modified_date": "2010-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.2204",
        "title": "MDPs with Unawareness",
        "authors": [
            "Joseph Y. Halpern",
            "Nan Rong",
            "Ashutosh Saxena"
        ],
        "abstract": "Markov decision processes (MDPs) are widely used for modeling decision-making problems in robotics, automated control, and economics. Traditional MDPs assume that the decision maker (DM) knows all states and actions. However, this may not be true in many situations of interest. We define a new framework, MDPs with unawareness (MDPUs) to deal with the possibilities that a DM may not be aware of all possible actions. We provide a complete characterization of when a DM can learn to play near-optimally in an MDPU, and give an algorithm that learns to play near-optimally when it is possible to do so, as efficiently as possible. In particular, we characterize when a near-optimal solution can be found in polynomial time.\n    ",
        "submission_date": "2010-06-11T00:00:00",
        "last_modified_date": "2010-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.2289",
        "title": "Unification in the Description Logic EL",
        "authors": [
            "Franz Baader",
            "Barbara Morawska"
        ],
        "abstract": " The Description Logic EL has recently drawn considerable attention since, on the one hand, important inference problems such as the subsumption problem are polynomial. On the other hand, EL is used to define large biomedical ontologies. Unification in Description Logics has been proposed as a novel inference service that can, for example, be used to detect redundancies in ontologies. The main result of this paper is that unification in EL is decidable. More precisely, EL-unification is NP-complete, and thus has the same complexity as EL-matching. We also show that, w.r.t. the unification type, EL is less well-behaved: it is of type zero, which in particular implies that there are unification problems that have no finite complete set of unifiers. \n    ",
        "submission_date": "2010-06-11T00:00:00",
        "last_modified_date": "2010-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.2322",
        "title": "Discovery of a missing disease spreader",
        "authors": [
            "Yoshiharu Maeno"
        ],
        "abstract": "This study presents a method to discover an outbreak of an infectious disease in a region for which data are missing, but which is at work as a disease spreader. Node discovery for the spread of an infectious disease is defined as discriminating between the nodes which are neighboring to a missing disease spreader node, and the rest, given a dataset on the number of cases. The spread is described by stochastic differential equations. A perturbation theory quantifies the impact of the missing spreader on the moments of the number of cases. Statistical discriminators examine the mid-body or tail-ends of the probability density function, and search for the disturbance from the missing spreader. They are tested with computationally synthesized datasets, and applied to the SARS outbreak and flu pandemic.\n    ",
        "submission_date": "2010-06-11T00:00:00",
        "last_modified_date": "2011-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.2718",
        "title": "From RESTful Services to RDF: Connecting the Web and the Semantic Web",
        "authors": [
            "Rosa Alarcon",
            "Erik Wilde"
        ],
        "abstract": "RESTful services on the Web expose information through retrievable resource representations that represent self-describing descriptions of resources, and through the way how these resources are interlinked through the hyperlinks that can be found in those representations. This basic design of RESTful services means that for extracting the most useful information from a service, it is necessary to understand a service's representations, which means both the semantics in terms of describing a resource, and also its semantics in terms of describing its linkage with other resources. Based on the Resource Linking Language (ReLL), this paper describes a framework for how RESTful services can be described, and how these descriptions can then be used to harvest information from these services. Building on this framework, a layered model of RESTful service semantics allows to represent a service's information in RDF/OWL. Because REST is based on the linkage between resources, the same model can be used for aggregating and interlinking multiple services for extracting RDF data from sets of RESTful services.\n    ",
        "submission_date": "2010-06-11T00:00:00",
        "last_modified_date": "2010-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.2743",
        "title": "Global Optimization for Value Function Approximation",
        "authors": [
            "Marek Petrik",
            "Shlomo Zilberstein"
        ],
        "abstract": "Existing value function approximation methods have been successfully used in many applications, but they often lack useful a priori error bounds. We propose a new approximate bilinear programming formulation of value function approximation, which employs global optimization. The formulation provides strong a priori guarantees on both robust and expected policy loss by minimizing specific norms of the Bellman residual. Solving a bilinear program optimally is NP-hard, but this is unavoidable because the Bellman-residual minimization itself is NP-hard. We describe and analyze both optimal and approximate algorithms for solving bilinear programs. The analysis shows that this algorithm offers a convergent generalization of approximate policy iteration. We also briefly analyze the behavior of bilinear programming algorithms under incomplete samples. Finally, we demonstrate that the proposed approach can consistently minimize the Bellman residual on simple benchmark problems.\n    ",
        "submission_date": "2010-06-14T00:00:00",
        "last_modified_date": "2010-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.2945",
        "title": "Two-Timescale Learning Using Idiotypic Behaviour Mediation For A Navigating Mobile Robot",
        "authors": [
            "Amanda Whitbrook",
            "Uwe Aickelin",
            "Jonathan M. Garibaldi"
        ],
        "abstract": "A combined Short-Term Learning (STL) and Long-Term Learning (LTL) approach to solving mobile-robot navigation problems is presented and tested in both the real and virtual domains. The LTL phase consists of rapid simulations that use a Genetic Algorithm to derive diverse sets of behaviours, encoded as variable sets of attributes, and the STL phase is an idiotypic Artificial Immune System. Results from the LTL phase show that sets of behaviours develop very rapidly, and significantly greater diversity is obtained when multiple autonomous populations are used, rather than a single one. The architecture is assessed under various scenarios, including removal of the LTL phase and switching off the idiotypic mechanism in the STL phase. The comparisons provide substantial evidence that the best option is the inclusion of both the LTL phase and the idiotypic system. In addition, this paper shows that structurally different environments can be used for the two phases without compromising transferability.\n    ",
        "submission_date": "2010-06-15T00:00:00",
        "last_modified_date": "2010-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.3021",
        "title": "A General Framework for Equivalences in Answer-Set Programming by Countermodels in the Logic of Here-and-There",
        "authors": [
            "Michael Fink"
        ],
        "abstract": "Different notions of equivalence, such as the prominent notions of strong and uniform equivalence, have been studied in Answer-Set Programming, mainly for the purpose of identifying programs that can serve as substitutes without altering the semantics, for instance in program optimization. Such semantic comparisons are usually characterized by various selections of models in the logic of Here-and-There (HT). For uniform equivalence however, correct characterizations in terms of HT-models can only be obtained for finite theories, respectively programs. In this article, we show that a selection of countermodels in HT captures uniform equivalence also for infinite theories. This result is turned into coherent characterizations of the different notions of equivalence by countermodels, as well as by a mixture of HT-models and countermodels (so-called equivalence interpretations). Moreover, we generalize the so-called notion of relativized hyperequivalence for programs to propositional theories, and apply the same methodology in order to obtain a semantic characterization which is amenable to infinite settings. This allows for a lifting of the results to first-order theories under a very general semantics given in terms of a quantified version of HT. We thus obtain a general framework for the study of various notions of equivalence for theories under answer-set semantics. Moreover, we prove an expedient property that allows for a simplified treatment of extended signatures, and provide further results for non-ground logic programs. In particular, uniform equivalence coincides under open and ordinary answer-set semantics, and for finite non-ground programs under these semantics, also the usual characterization of uniform equivalence in terms of maximal and total HT-models of the grounding is correct, even for infinite domains, when corresponding ground programs are infinite.\n    ",
        "submission_date": "2010-06-15T00:00:00",
        "last_modified_date": "2010-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.3035",
        "title": "Products of Weighted Logic Programs",
        "authors": [
            "Shay B. Cohen",
            "Robert J. Simmons",
            "Noah A. Smith"
        ],
        "abstract": "Weighted logic programming, a generalization of bottom-up logic programming, is a well-suited framework for specifying dynamic programming algorithms. In this setting, proofs correspond to the algorithm's output space, such as a path through a graph or a grammatical derivation, and are given a real-valued score (often interpreted as a probability) that depends on the real weights of the base axioms used in the proof. The desired output is a function over all possible proofs, such as a sum of scores or an optimal score. We describe the PRODUCT transformation, which can merge two weighted logic programs into a new one. The resulting program optimizes a product of proof scores from the original programs, constituting a scoring function known in machine learning as a ``product of experts.'' Through the addition of intuitive constraining side conditions, we show that several important dynamic programming algorithms can be derived by applying PRODUCT to weighted logic programs corresponding to simpler weighted logic programs. In addition, we show how the computation of Kullback-Leibler divergence, an information-theoretic measure, can be interpreted using PRODUCT.\n    ",
        "submission_date": "2010-06-15T00:00:00",
        "last_modified_date": "2010-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.3215",
        "title": "Solving Functional Constraints by Variable Substitution",
        "authors": [
            "Yuanlin Zhang",
            "Roland H.C. Yap"
        ],
        "abstract": "Functional constraints and bi-functional constraints are an important constraint class in Constraint Programming (CP) systems, in particular for Constraint Logic Programming (CLP) systems. CP systems with finite domain constraints usually employ CSP-based solvers which use local consistency, for example, arc consistency. We introduce a new approach which is based instead on variable substitution. We obtain efficient algorithms for reducing systems involving functional and bi-functional constraints together with other non-functional constraints. It also solves globally any CSP where there exists a variable such that any other variable is reachable from it through a sequence of functional constraints. Our experiments on random problems show that variable elimination can significantly improve the efficiency of solving problems with functional constraints.\n    ",
        "submission_date": "2010-06-16T00:00:00",
        "last_modified_date": "2010-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.3650",
        "title": "The Use of Probabilistic Systems to Mimic the Behaviour of Idiotypic AIS Robot Controllers",
        "authors": [
            "Amanda Whitbrook",
            "Uwe Aickelin",
            "Jonathan M. Garibaldi"
        ],
        "abstract": "Previous work has shown that robot navigation systems that employ an architecture based upon the idiotypic network theory of the immune system have an advantage over control techniques that rely on reinforcement learning only. This is thought to be a result of intelligent behaviour selection on the part of the idiotypic robot. In this paper an attempt is made to imitate idiotypic dynamics by creating controllers that use reinforcement with a number of different probabilistic schemes to select robot behaviour. The aims are to show that the idiotypic system is not merely performing some kind of periodic random behaviour selection, and to try to gain further insight into the processes that govern the idiotypic mechanism. Trials are carried out using simulated Pioneer robots that undertake navigation exercises. Results show that a scheme that boosts the probability of selecting highly-ranked alternative behaviours to 50% during stall conditions comes closest to achieving the properties of the idiotypic system, but remains unable to match it in terms of all round performance.\n    ",
        "submission_date": "2010-06-18T00:00:00",
        "last_modified_date": "2010-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.3652",
        "title": "Modelling Reactive and Proactive Behaviour in Simulation",
        "authors": [
            "Mazlina Abdul Majid",
            "Peer-Olaf Siebers",
            "Uwe Aickelin"
        ],
        "abstract": "This research investigated the simulation model behaviour of a traditional and combined discrete event as well as agent based simulation models when modelling human reactive and proactive behaviour in human centric complex systems. A departmental store was chosen as human centric complex case study where the operation system of a fitting room in WomensWear department was investigated. We have looked at ways to determine the efficiency of new management policies for the fitting room operation through simulating the reactive and proactive behaviour of staff towards customers. Once development of the simulation models and their verification had been done, we carried out a validation experiment in the form of a sensitivity analysis. Subsequently, we executed a statistical analysis where the mixed reactive and proactive behaviour experimental results were compared with some reactive experimental results from previously published works. Generally, this case study discovered that simple proactive individual behaviour could be modelled in both simulation models. In addition, we found the traditional discrete event model performed similar in the simulation model output compared to the combined discrete event and agent based simulation when modelling similar human behaviour.\n    ",
        "submission_date": "2010-06-18T00:00:00",
        "last_modified_date": "2010-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.3654",
        "title": "Detecting Anomalous Process Behaviour using Second Generation Artificial Immune Systems",
        "authors": [
            "Jamie Twycross",
            "Uwe Aickelin",
            "Amanda Whitbrook"
        ],
        "abstract": "Artificial Immune Systems have been successfully applied to a number of problem domains including fault tolerance and data mining, but have been shown to scale poorly when applied to computer intrusion detec- tion despite the fact that the biological immune system is a very effective anomaly detector. This may be because AIS algorithms have previously been based on the adaptive immune system and biologically-naive mod- els. This paper focuses on describing and testing a more complex and biologically-authentic AIS model, inspired by the interactions between the innate and adaptive immune systems. Its performance on a realistic process anomaly detection problem is shown to be better than standard AIS methods (negative-selection), policy-based anomaly detection methods (systrace), and an alternative innate AIS approach (the DCA). In addition, it is shown that runtime information can be used in combination with system call information to enhance detection capability.\n    ",
        "submission_date": "2010-06-18T00:00:00",
        "last_modified_date": "2010-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.4035",
        "title": "Towards the Development of a Simulator for Investigating the Impact of People Management Practices on Retail Performance",
        "authors": [
            "Peer-Olaf Siebers",
            "Uwe Aickelin",
            "Helen Celia",
            "Chris Clegg"
        ],
        "abstract": "Often models for understanding the impact of management practices on retail performance are developed under the assumption of stability, equilibrium and linearity, whereas retail operations are considered in reality to be dynamic, non-linear and complex. Alternatively, discrete event and agent-based modelling are approaches that allow the development of simulation models of heterogeneous non-equilibrium systems for testing out different scenarios. When developing simulation models one has to abstract and simplify from the real world, which means that one has to try and capture the 'essence' of the system required for developing a representation of the mechanisms that drive the progression in the real system. Simulation models can be developed at different levels of abstraction. To know the appropriate level of abstraction for a specific application is often more of an art than a science. We have developed a retail branch simulation model to investigate which level of model accuracy is required for such a model to obtain meaningful results for practitioners.\n    ",
        "submission_date": "2010-06-21T00:00:00",
        "last_modified_date": "2010-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.4544",
        "title": "Human Disease Diagnosis Using a Fuzzy Expert System",
        "authors": [
            "Mir Anamul Hasan",
            "Khaja Md. Sher-E-Alam",
            "Ahsan Raja Chowdhury"
        ],
        "abstract": "Human disease diagnosis is a complicated process and requires high level of expertise. Any attempt of developing a web-based expert system dealing with human disease diagnosis has to overcome various difficulties. This paper describes a project work aiming to develop a web-based fuzzy expert system for diagnosing human diseases. Now a days fuzzy systems are being used successfully in an increasing number of application areas; they use linguistic rules to describe systems. This research project focuses on the research and development of a web-based clinical tool designed to improve the quality of the exchange of health information between health care professionals and patients. Practitioners can also use this web-based tool to corroborate diagnosis. The proposed system is experimented on various scenarios in order to evaluate it's performance. In all the cases, proposed system exhibits satisfactory results.\n    ",
        "submission_date": "2010-06-23T00:00:00",
        "last_modified_date": "2010-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.4551",
        "title": "Vagueness of Linguistic variable",
        "authors": [
            "Supriya Raheja",
            "Smita Rajpal"
        ],
        "abstract": "In the area of computer science focusing on creating machines that can engage on behaviors that humans consider intelligent. The ability to create intelligent machines has intrigued humans since ancient times and today with the advent of the computer and 50 years of research into various programming techniques, the dream of smart machines is becoming a reality. Researchers are creating systems which can mimic human thought, understand speech, beat the best human chessplayer, and countless other feats never before possible. Ability of the human to estimate the information is most brightly shown in using of natural languages. Using words of a natural language for valuation qualitative attributes, for example, the person pawns uncertainty in form of vagueness in itself estimations. Vague sets, vague judgments, vague conclusions takes place there and then, where and when the reasonable subject exists and also is interested in something. The vague sets theory has arisen as the answer to an illegibility of language the reasonable subject speaks. Language of a reasonable subject is generated by vague events which are created by the reason and which are operated by the mind. The theory of vague sets represents an attempt to find such approximation of vague grouping which would be more convenient, than the classical theory of sets in situations where the natural language plays a significant role. Such theory has been offered by known American mathematician Gau and Buehrer .In our paper we are describing how vagueness of linguistic variables can be solved by using the vague set ",
        "submission_date": "2010-06-23T00:00:00",
        "last_modified_date": "2010-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.4561",
        "title": "An Efficient Technique for Similarity Identification between Ontologies",
        "authors": [
            "Amjad Farooq",
            "Syed Ahsan",
            "Abad Shah"
        ],
        "abstract": "Ontologies usually suffer from the semantic heterogeneity when simultaneously used in information sharing, merging, integrating and querying processes. Therefore, the similarity identification between ontologies being used becomes a mandatory task for all these processes to handle the problem of semantic heterogeneity. In this paper, we propose an efficient technique for similarity measurement between two ontologies. The proposed technique identifies all candidate pairs of similar concepts without omitting any similar pair. The proposed technique can be used in different types of operations on ontologies such as merging, mapping and aligning. By analyzing its results a reasonable improvement in terms of completeness, correctness and overall quality of the results has been found.\n    ",
        "submission_date": "2010-06-23T00:00:00",
        "last_modified_date": "2010-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.4563",
        "title": "The State of the Art: Ontology Web-Based Languages: XML Based",
        "authors": [
            "Mohammad Mustafa Taye"
        ],
        "abstract": "Many formal languages have been proposed to express or represent Ontologies, including RDF, RDFS, DAML+OIL and OWL. Most of these languages are based on XML syntax, but with various terminologies and expressiveness. Therefore, choosing a language for building an Ontology is the main step. The main point of choosing language to represent Ontology is based mainly on what the Ontology will represent or be used for. That language should have a range of quality support features such as ease of use, expressive power, compatibility, sharing and versioning, internationalisation. This is because different kinds of knowledge-based applications need different language features. The main objective of these languages is to add semantics to the existing information on the web. The aims of this paper is to provide a good knowledge of existing language and understanding of these languages and how could be used.\n    ",
        "submission_date": "2010-06-23T00:00:00",
        "last_modified_date": "2010-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.4567",
        "title": "Understanding Semantic Web and Ontologies: Theory and Applications",
        "authors": [
            "Mohammad Mustafa Taye"
        ],
        "abstract": "Semantic Web is actually an extension of the current one in that it represents information more meaningfully for humans and computers alike. It enables the description of contents and services in machine-readable form, and enables annotating, discovering, publishing, advertising and composing services to be automated. It was developed based on Ontology, which is considered as the backbone of the Semantic Web. In other words, the current Web is transformed from being machine-readable to machine-understandable. In fact, Ontology is a key technique with which to annotate semantics and provide a common, comprehensible foundation for resources on the Semantic Web. Moreover, Ontology can provide a common vocabulary, a grammar for publishing data, and can supply a semantic description of data which can be used to preserve the Ontologies and keep them ready for inference. This paper provides basic concepts of web services and the Semantic Web, defines the structure and the main applications of ontology, and provides many relevant terms are explained in order to provide a basic understanding of ontologies.\n    ",
        "submission_date": "2010-06-23T00:00:00",
        "last_modified_date": "2010-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.4949",
        "title": "Artificial Immune Systems (2010)",
        "authors": [
            "Julie Greensmith",
            "Amanda Whitbrook",
            "Uwe Aickelin"
        ],
        "abstract": "The human immune system has numerous properties that make it ripe for exploitation in the computational domain, such as robustness and fault tolerance, and many different algorithms, collectively termed Artificial Immune Systems (AIS), have been inspired by it. Two generations of AIS are currently in use, with the first generation relying on simplified immune models and the second generation utilising interdisciplinary collaboration to develop a deeper understanding of the immune system and hence produce more complex models. Both generations of algorithms have been successfully applied to a variety of problems, including anomaly detection, pattern recognition, optimisation and robotics. In this chapter an overview of AIS is presented, its evolution is discussed, and it is shown that the diversification of the field is linked to the diversity of the immune system itself, leading to a number of algorithms as opposed to one archetypal system. Two case studies are also presented to help provide insight into the mechanisms of AIS; these are the idiotypic network approach and the Dendritic Cell Algorithm.\n    ",
        "submission_date": "2010-06-25T00:00:00",
        "last_modified_date": "2010-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.5008",
        "title": "Detecting Danger: The Dendritic Cell Algorithm",
        "authors": [
            "Julie Greensmith",
            "Uwe Aickelin",
            "Steve Cayzer"
        ],
        "abstract": "The Dendritic Cell Algorithm (DCA) is inspired by the function of the dendritic cells of the human immune system. In nature, dendritic cells are the intrusion detection agents of the human body, policing the tissue and organs for potential invaders in the form of pathogens. In this research, and abstract model of DC behaviour is developed and subsequently used to form an algorithm, the DCA. The abstraction process was facilitated through close collaboration with laboratory- based immunologists, who performed bespoke experiments, the results of which are used as an integral part of this algorithm. The DCA is a population based algorithm, with each agent in the system represented as an 'artificial DC'. Each DC has the ability to combine multiple data streams and can add context to data suspected as anomalous. In this chapter the abstraction process and details of the resultant algorithm are given. The algorithm is applied to numerous intrusion detection problems in computer security including the detection of port scans and botnets, where it has produced impressive results with relatively low rates of false positives.\n    ",
        "submission_date": "2010-06-25T00:00:00",
        "last_modified_date": "2010-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.5041",
        "title": "GroupLiNGAM: Linear non-Gaussian acyclic models for sets of variables",
        "authors": [
            "Yoshinobu Kawahara",
            "Kenneth Bollen",
            "Shohei Shimizu",
            "Takashi Washio"
        ],
        "abstract": "Finding the structure of a graphical model has been received much attention in many fields. Recently, it is reported that the non-Gaussianity of data enables us to identify the structure of a directed acyclic graph without any prior knowledge on the structure. In this paper, we propose a novel non-Gaussianity based algorithm for more general type of models; chain graphs. The algorithm finds an ordering of the disjoint subsets of variables by iteratively evaluating the independence between the variable subset and the residuals when the remaining variables are regressed on those. However, its computational cost grows exponentially according to the number of variables. Therefore, we further discuss an efficient approximate approach for applying the algorithm to large sized graphs. We illustrate the algorithm with artificial and real-world datasets.\n    ",
        "submission_date": "2010-06-24T00:00:00",
        "last_modified_date": "2010-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.5188",
        "title": "Feature Construction for Relational Sequence Learning",
        "authors": [
            "Nicola Di Mauro",
            "Teresa M.A. Basile",
            "Stefano Ferilli",
            "Floriana Esposito"
        ],
        "abstract": "We tackle the problem of multi-class relational sequence learning using relevant patterns discovered from a set of labelled sequences. To deal with this problem, firstly each relational sequence is mapped into a feature vector using the result of a feature construction method. Since, the efficacy of sequence learning algorithms strongly depends on the features used to represent the sequences, the second step is to find an optimal subset of the constructed features leading to high classification accuracy. This feature selection task has been solved adopting a wrapper approach that uses a stochastic local search algorithm embedding a naive Bayes classifier. The performance of the proposed method applied to a real-world dataset shows an improvement when compared to other established methods, such as hidden Markov models, Fisher kernels and conditional random fields for relational sequences.\n    ",
        "submission_date": "2010-06-27T00:00:00",
        "last_modified_date": "2010-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.5511",
        "title": "Soft Approximations and uni-int Decision Making",
        "authors": [
            "Athar Kharal"
        ],
        "abstract": "Notions of core, support and inversion of a soft set have been defined and studied. Soft approximations are soft sets developed through core and support, and are used for granulating the soft space. Membership structure of a soft set has been probed in and many interesting properties presented. The mathematical apparatus developed so far in this paper yields a detailed analysis of two works viz. [N. Cagman, S. Enginoglu, Soft set theory and uni-int decision making, European Jr. of Operational Research (article in press, available online 12 May 2010)] and [N. Cagman, S. Enginoglu, Soft matrix theory and its decision making, Computers and Mathematics with Applications 59 (2010) 3308 - 3314.]. We prove (Theorem 8.1) that uni-int method of Cagman is equivalent to a core-support expression which is computationally far less expansive than uni-int. This also highlights some shortcomings in Cagman's uni-int method and thus motivates us to improve the method. We first suggest an improvement in uni-int method and then present a new conjecture to solve the optimum choice problem given by Cagman and Enginoglu. Our Example 8.6 presents a case where the optimum choice is intuitively clear yet both uni-int methods (Cagman's and our improved one) give wrong answer but the new conjecture solves the problem correctly.\n    ",
        "submission_date": "2010-06-29T00:00:00",
        "last_modified_date": "2010-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.5657",
        "title": "Reasoning Support for Risk Prediction and Prevention in Independent Living",
        "authors": [
            "A. Mileo",
            "D. Merico",
            "R. Bisiani"
        ],
        "abstract": "In recent years there has been growing interest in solutions for the delivery of clinical care for the elderly, due to the large increase in aging population. Monitoring a patient in his home environment is necessary to ensure continuity of care in home settings, but, to be useful, this activity must not be too invasive for patients and a burden for caregivers. We prototyped a system called SINDI (Secure and INDependent lIving), focused on i) collecting a limited amount of data about the person and the environment through Wireless Sensor Networks (WSN), and ii) inferring from these data enough information to support caregivers in understanding patients' well being and in predicting possible evolutions of their health. Our hierarchical logic-based model of health combines data from different sources, sensor data, tests results, common-sense knowledge and patient's clinical profile at the lower level, and correlation rules between health conditions across upper levels. The logical formalization and the reasoning process are based on Answer Set Programming. The expressive power of this logic programming paradigm makes it possible to reason about health evolution even when the available information is incomplete and potentially incoherent, while declarativity simplifies rules specification by caregivers and allows automatic encoding of knowledge. This paper describes how these issues have been targeted in the application scenario of the SINDI system.\n    ",
        "submission_date": "2010-06-29T00:00:00",
        "last_modified_date": "2010-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.5896",
        "title": "Counterexample Guided Abstraction Refinement Algorithm for Propositional Circumscription",
        "authors": [
            "Mikol\u00e1\u0161 Janota",
            "Joao Marques-Silva",
            "Radu Grigore"
        ],
        "abstract": "Circumscription is a representative example of a nonmonotonic reasoning inference technique. Circumscription has often been studied for first order theories, but its propositional version has also been the subject of extensive research, having been shown equivalent to extended closed world assumption (ECWA). Moreover, entailment in propositional circumscription is a well-known example of a decision problem in the second level of the polynomial hierarchy. This paper proposes a new Boolean Satisfiability (SAT)-based algorithm for entailment in propositional circumscription that explores the relationship of propositional circumscription to minimal models. The new algorithm is inspired by ideas commonly used in SAT-based model checking, namely counterexample guided abstraction refinement. In addition, the new algorithm is refined to compute the theory closure for generalized close world assumption (GCWA). Experimental results show that the new algorithm can solve problem instances that other solutions are unable to solve.\n    ",
        "submission_date": "2010-06-30T00:00:00",
        "last_modified_date": "2010-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.0412",
        "title": "Improving Iris Recognition Accuracy By Score Based Fusion Method",
        "authors": [
            "Ujwalla Gawande",
            "Mukesh Zaveri",
            "Avichal Kapur"
        ],
        "abstract": "Iris recognition technology, used to identify individuals by photographing the iris of their eye, has become popular in security applications because of its ease of use, accuracy, and safety in controlling access to high-security areas. Fusion of multiple algorithms for biometric verification performance improvement has received considerable attention. The proposed method combines the zero-crossing 1 D wavelet Euler number, and genetic algorithm based for feature extraction. The output from these three algorithms is normalized and their score are fused to decide whether the user is genuine or imposter. This new strategies is discussed in this paper, in order to compute a multimodal combined score.\n    ",
        "submission_date": "2010-07-01T00:00:00",
        "last_modified_date": "2010-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.0546",
        "title": "Computational Model of Music Sight Reading: A Reinforcement Learning Approach",
        "authors": [
            "Keyvan Yahya",
            "Pouyan Rafiei Fard"
        ],
        "abstract": "Although the Music Sight Reading process has been studied from the cognitive psychology view points, but the computational learning methods like the Reinforcement Learning have not yet been used to modeling of such processes. In this paper, with regards to essential properties of our specific problem, we consider the value function concept and will indicate that the optimum policy can be obtained by the method we offer without to be getting involved with computing of the complex value functions. Also, we will offer a normative behavioral model for the interaction of the agent with the musical pitch environment and by using a slightly different version of Partially observable Markov decision processes we will show that our method helps for faster learning of state-action pairs in our implemented agents.\n    ",
        "submission_date": "2010-07-04T00:00:00",
        "last_modified_date": "2013-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.0602",
        "title": "On The Complexity and Completeness of Static Constraints for Breaking Row and Column Symmetry",
        "authors": [
            "George Katsirelos",
            "Nina Narodytska",
            "Toby Walsh"
        ],
        "abstract": "We consider a common type of symmetry where we have a matrix of decision variables with interchangeable rows and columns. A simple and efficient method to deal with such row and column symmetry is to post symmetry breaking constraints like DOUBLELEX and SNAKELEX. We provide a number of positive and negative results on posting such symmetry breaking constraints. On the positive side, we prove that we can compute in polynomial time a unique representative of an equivalence class in a matrix model with row and column symmetry if the number of rows (or of columns) is bounded and in a number of other special cases. On the negative side, we show that whilst DOUBLELEX and SNAKELEX are often effective in practice, they can leave a large number of symmetric solutions in the worst case. In addition, we prove that propagating DOUBLELEX completely is NP-hard. Finally we consider how to break row, column and value symmetry, correcting a result in the literature about the safeness of combining different symmetry breaking constraints. We end with the first experimental study on how much symmetry is left by DOUBLELEX and SNAKELEX on some benchmark problems.\n    ",
        "submission_date": "2010-07-05T00:00:00",
        "last_modified_date": "2010-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.0603",
        "title": "Decomposition of the NVALUE constraint",
        "authors": [
            "Christian Bessiere",
            "George Katsirelos",
            "Nina Narodytska",
            "Claude-Guy Quimper",
            "Toby Walsh"
        ],
        "abstract": "We study decompositions of the global NVALUE constraint. Our main contribution is theoretical: we show that there are propagators for global constraints like NVALUE which decomposition can simulate with the same time complexity but with a much greater space complexity. This suggests that the benefit of a global propagator may often not be in saving time but in saving space. Our other theoretical contribution is to show for the first time that range consistency can be enforced on NVALUE with the same worst-case time complexity as bound consistency. Finally, the decompositions we study are readily encoded as linear inequalities. We are therefore able to use them in integer linear programs.\n    ",
        "submission_date": "2010-07-05T00:00:00",
        "last_modified_date": "2010-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.0604",
        "title": "Symmetry within and between solutions",
        "authors": [
            "Toby Walsh"
        ],
        "abstract": "Symmetry can be used to help solve many problems. For instance, Einstein's famous 1905 paper (\"On the Electrodynamics of Moving Bodies\") uses symmetry to help derive the laws of special relativity. In artificial intelligence, symmetry has played an important role in both problem representation and reasoning. I describe recent work on using symmetry to help solve constraint satisfaction problems. Symmetries occur within individual solutions of problems as well as between different solutions of the same problem. Symmetry can also be applied to the constraints in a problem to give new symmetric constraints. Reasoning about symmetry can speed up problem solving, and has led to the discovery of new results in both graph and number theory.\n    ",
        "submission_date": "2010-07-05T00:00:00",
        "last_modified_date": "2010-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.0614",
        "title": "Online Cake Cutting",
        "authors": [
            "Toby Walsh"
        ],
        "abstract": "We propose an online form of the cake cutting problem. This models situations where players arrive and depart during the process of dividing a resource. We show that well known fair division procedures like cut-and-choose and the Dubins-Spanier moving knife procedure can be adapted to apply to such online problems. We propose some desirable properties that online cake cutting procedures might possess like online forms of proportionality and envy-freeness, and identify which properties are in fact possessed by the different online cake procedures.\n    ",
        "submission_date": "2010-07-05T00:00:00",
        "last_modified_date": "2010-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.0637",
        "title": "Local search for stable marriage problems with ties and incomplete lists",
        "authors": [
            "Mirco Gelain",
            "Maria Silvia Pini",
            "Francesca RossI",
            "Kristen Brent Venable",
            "Toby Walsh"
        ],
        "abstract": "The stable marriage problem has a wide variety of practical applications, ranging from matching resident doctors to hospitals, to matching students to schools, or more generally to any two-sided market. We consider a useful variation of the stable marriage problem, where the men and women express their preferences using a preference list with ties over a subset of the members of the other sex. Matchings are permitted only with people who appear in these preference lists. In this setting, we study the problem of finding a stable matching that marries as many people as possible. Stability is an envy-free notion: no man and woman who are not married to each other would both prefer each other to their partners or to being single. This problem is NP-hard. We tackle this problem using local search, exploiting properties of the problem to reduce the size of the neighborhood and to make local moves efficiently. Experimental results show that this approach is able to solve large problems, quickly returning stable matchings of large and often optimal size.\n    ",
        "submission_date": "2010-07-05T00:00:00",
        "last_modified_date": "2010-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.0690",
        "title": "A unified view of Automata-based algorithms for Frequent Episode Discovery",
        "authors": [
            "Avinash Achar",
            "Srivatsan Laxman",
            "P. S. Sastry"
        ],
        "abstract": "Frequent Episode Discovery framework is a popular framework in Temporal Data Mining with many applications. Over the years many different notions of frequencies of episodes have been proposed along with different algorithms for episode discovery. In this paper we present a unified view of all such frequency counting algorithms. We present a generic algorithm such that all current algorithms are special cases of it. This unified view allows one to gain insights into different frequencies and we present quantitative relationships among different frequencies. Our unified view also helps in obtaining correctness proofs for various algorithms as we show here. We also point out how this unified view helps us to consider generalization of the algorithm so that they can discover episodes with general partial orders.\n    ",
        "submission_date": "2010-07-05T00:00:00",
        "last_modified_date": "2010-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.0728",
        "title": "Artificial Learning in Artificial Memories",
        "authors": [
            "John Robert Burger"
        ],
        "abstract": "Memory refinements are designed below to detect those sequences of actions that have been repeated a given number n. Subsequently such sequences are permitted to run without CPU involvement. This mimics human learning. Actions are rehearsed and once learned, they are performed automatically without conscious involvement.\n    ",
        "submission_date": "2010-07-05T00:00:00",
        "last_modified_date": "2010-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.0776",
        "title": "Is Computational Complexity a Barrier to Manipulation?",
        "authors": [
            "Toby Walsh"
        ],
        "abstract": "When agents are acting together, they may need a simple mechanism to decide on joint actions. One possibility is to have the agents express their preferences in the form of a ballot and use a voting rule to decide the winning action(s). Unfortunately, agents may try to manipulate such an election by misreporting their preferences. Fortunately, it has been shown that it is NP-hard to compute how to manipulate a number of different voting rules. However, NP-hardness only bounds the worst-case complexity. Recent theoretical results suggest that manipulation may often be easy in practice. To address this issue, I suggest studying empirically if computational complexity is in practice a barrier to manipulation. The basic tool used in my investigations is the identification of computational \"phase transitions\". Such an approach has been fruitful in identifying hard instances of propositional satisfiability and other NP-hard problems. I show that phase transition behaviour gives insight into the hardness of manipulating voting rules, increasing concern that computational complexity is indeed any sort of barrier. Finally, I look at the problem of computing manipulation of other, related problems like stable marriage and tournament problems.\n    ",
        "submission_date": "2010-07-05T00:00:00",
        "last_modified_date": "2010-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.0859",
        "title": "Local search for stable marriage problems",
        "authors": [
            "M. Gelain",
            "M. S. Pini",
            "F. Rossi",
            "K. B. Venable",
            "T. Walsh"
        ],
        "abstract": "The stable marriage (SM) problem has a wide variety of practical applications, ranging from matching resident doctors to hospitals, to matching students to schools, or more generally to any two-sided market. In the classical formulation, n men and n women express their preferences (via a strict total order) over the members of the other sex. Solving a SM problem means finding a stable marriage where stability is an envy-free notion: no man and woman who are not married to each other would both prefer each other to their partners or to being single. We consider both the classical stable marriage problem and one of its useful variations (denoted SMTI) where the men and women express their preferences in the form of an incomplete preference list with ties over a subset of the members of the other sex. Matchings are permitted only with people who appear in these lists, an we try to find a stable matching that marries as many people as possible. Whilst the SM problem is polynomial to solve, the SMTI problem is NP-hard. We propose to tackle both problems via a local search approach, which exploits properties of the problems to reduce the size of the neighborhood and to make local moves efficiently. We evaluate empirically our algorithm for SM problems by measuring its runtime behaviour and its ability to sample the lattice of all possible stable marriages. We evaluate our algorithm for SMTI problems in terms of both its runtime behaviour and its ability to find a maximum cardinality stable ",
        "submission_date": "2010-07-06T00:00:00",
        "last_modified_date": "2010-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.0940",
        "title": "An axiomatic formalization of bounded rationality based on a utility-information equivalence",
        "authors": [
            "Pedro A. Ortega",
            "Daniel A. Braun"
        ],
        "abstract": "Classic decision-theory is based on the maximum expected utility (MEU) principle, but crucially ignores the resource costs incurred when determining optimal decisions. Here we propose an axiomatic framework for bounded decision-making that considers resource costs. Agents are formalized as probability measures over input-output streams. We postulate that any such probability measure can be assigned a corresponding conjugate utility function based on three axioms: utilities should be real-valued, additive and monotonic mappings of probabilities. We show that these axioms enforce a unique conversion law between utility and probability (and thereby, information). Moreover, we show that this relation can be characterized as a variational principle: given a utility function, its conjugate probability measure maximizes a free utility functional. Transformations of probability measures can then be formalized as a change in free utility due to the addition of new constraints expressed by a target utility function. Accordingly, one obtains a criterion to choose a probability measure that trades off the maximization of a target utility function and the cost of the deviation from a reference distribution. We show that optimal control, adaptive estimation and adaptive control problems can be solved this way in a resource-efficient way. When resource costs are ignored, the MEU principle is recovered. Our formalization might thus provide a principled approach to bounded rationality that establishes a close link to information theory.\n    ",
        "submission_date": "2010-07-06T00:00:00",
        "last_modified_date": "2010-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.1024",
        "title": "Model Counting in Product Configuration",
        "authors": [
            "Andreas K\u00fcbler",
            "Christoph Zengler",
            "Wolfgang K\u00fcchlin"
        ],
        "abstract": "We describe how to use propositional model counting for a quantitative analysis of product configuration data. Our approach computes valuable meta information such as the total number of valid configurations or the relative frequency of components. This information can be used to assess the severity of documentation errors or to measure documentation quality. As an application example we show how we apply these methods to product documentation formulas of the Mercedes-Benz line of vehicles. In order to process these large formulas we developed and implemented a new model counter for non-CNF formulas. Our model counter can process formulas, whose CNF representations could not be processed up till now.\n    ",
        "submission_date": "2010-07-07T00:00:00",
        "last_modified_date": "2010-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.1766",
        "title": "An svm multiclassifier approach to land cover mapping",
        "authors": [
            "Gidudu Anthony",
            "Hulley Gregg",
            "Marwala Tshilidzi"
        ],
        "abstract": "From the advent of the application of satellite imagery to land cover mapping, one of the growing areas of research interest has been in the area of image classification. Image classifiers are algorithms used to extract land cover information from satellite imagery. Most of the initial research has focussed on the development and application of algorithms to better existing and emerging classifiers. In this paper, a paradigm shift is proposed whereby a committee of classifiers is used to determine the final classification output. Two of the key components of an ensemble system are that there should be diversity among the classifiers and that there should be a mechanism through which the results are combined. In this paper, the members of the ensemble system include: Linear SVM, Gaussian SVM and Quadratic SVM. The final output was determined through a simple majority vote of the individual classifiers. From the results obtained it was observed that the final derived map generated by an ensemble system can potentially improve on the results derived from the individual classifiers making up the ensemble system. The ensemble system classification accuracy was, in this case, better than the linear and quadratic SVM result. It was however less than that of the RBF SVM. Areas for further research could focus on improving the diversity of the ensemble system used in this research.\n    ",
        "submission_date": "2010-07-11T00:00:00",
        "last_modified_date": "2010-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.2364",
        "title": "A Note on Semantic Web Services Specification and Composition in Constructive Description Logics",
        "authors": [
            "Loris Bozzato",
            "Mauro Ferrari"
        ],
        "abstract": "The idea of the Semantic Web is to annotate Web content and services with computer interpretable descriptions with the aim to automatize many tasks currently performed by human users. In the context of Web services, one of the most interesting tasks is their composition. In this paper we formalize this problem in the framework of a constructive description logic. In particular we propose a declarative service specification language and a calculus for service composition. We show by means of an example how this calculus can be used to define composed Web services and we discuss the problem of automatic service synthesis.\n    ",
        "submission_date": "2010-07-14T00:00:00",
        "last_modified_date": "2010-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.2534",
        "title": "A general method for deciding about logically constrained issues",
        "authors": [
            "Rosa Camps",
            "Xavier Mora",
            "Laia Saumell"
        ],
        "abstract": "A general method is given for revising degrees of belief and arriving at consistent decisions about a system of logically constrained issues. In contrast to other works about belief revision, here the constraints are assumed to be fixed. The method has two variants, dual of each other, whose revised degrees of belief are respectively above and below the original ones. The upper [resp. lower] revised degrees of belief are uniquely characterized as the lowest [resp. highest] ones that are invariant by a certain max-min [resp. min-max] operation determined by the logical constraints. In both variants, making balance between the revised degree of belief of a proposition and that of its negation leads to decisions that are ensured to be consistent with the logical constraints. These decisions are ensured to agree with the majority criterion as applied to the original degrees of belief whenever this gives a consistent result. They are also also ensured to satisfy a property of respect for unanimity about any particular issue, as well as a property of monotonicity with respect to the original degrees of belief. The application of the method to certain special domains comes down to well established or increasingly accepted methods, such as the single-link method of cluster analysis and the method of paths in preferential voting.\n    ",
        "submission_date": "2010-07-15T00:00:00",
        "last_modified_date": "2012-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.3159",
        "title": "Logic-Based Decision Support for Strategic Environmental Assessment",
        "authors": [
            "Marco Gavanelli",
            "Fabrizio Riguzzi",
            "Michela Milano",
            "Paolo Cagnoli"
        ],
        "abstract": "Strategic Environmental Assessment is a procedure aimed at introducing systematic assessment of the environmental effects of plans and programs. This procedure is based on the so-called coaxial matrices that define dependencies between plan activities (infrastructures, plants, resource extractions, buildings, etc.) and positive and negative environmental impacts, and dependencies between these impacts and environmental receptors. Up to now, this procedure is manually implemented by environmental experts for checking the environmental effects of a given plan or program, but it is never applied during the plan/program construction. A decision support system, based on a clear logic semantics, would be an invaluable tool not only in assessing a single, already defined plan, but also during the planning process in order to produce an optimized, environmentally assessed plan and to study possible alternative scenarios. We propose two logic-based approaches to the problem, one based on Constraint Logic Programming and one on Probabilistic Logic Programming that could be, in the future, conveniently merged to exploit the advantages of both. We test the proposed approaches on a real energy plan and we discuss their limitations and advantages.\n    ",
        "submission_date": "2010-07-19T00:00:00",
        "last_modified_date": "2010-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.3223",
        "title": "Testing and Debugging Techniques for Answer Set Solver Development",
        "authors": [
            "Robert Brummayer",
            "Matti J\u00e4rvisalo"
        ],
        "abstract": "This paper develops automated testing and debugging techniques for answer set solver development. We describe a flexible grammar-based black-box ASP fuzz testing tool which is able to reveal various defects such as unsound and incomplete behavior, i.e. invalid answer sets and inability to find existing solutions, in state-of-the-art answer set solver implementations. Moreover, we develop delta debugging techniques for shrinking failure-inducing inputs on which solvers exhibit defective behavior. In particular, we develop a delta debugging algorithm in the context of answer set solving, and evaluate two different elimination strategies for the algorithm.\n    ",
        "submission_date": "2010-07-19T00:00:00",
        "last_modified_date": "2010-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.3515",
        "title": "Query-driven Procedures for Hybrid MKNF Knowledge Bases",
        "authors": [
            "Jos\u00e9 J\u00falio Alferes",
            "Matthias Knorr",
            "Terrance Swift"
        ],
        "abstract": "Hybrid MKNF knowledge bases are one of the most prominent tightly integrated combinations of open-world ontology languages with closed-world (non-monotonic) rule paradigms. The definition of Hybrid MKNF is parametric on the description logic (DL) underlying the ontology language, in the sense that non-monotonic rules can extend any decidable DL language. Two related semantics have been defined for Hybrid MKNF: one that is based on the Stable Model Semantics for logic programs and one on the Well-Founded Semantics (WFS). Under WFS, the definition of Hybrid MKNF relies on a bottom-up computation that has polynomial data complexity whenever the DL language is tractable. Here we define a general query-driven procedure for Hybrid MKNF that is sound with respect to the stable model-based semantics, and sound and complete with respect to its WFS variant. This procedure is able to answer a slightly restricted form of conjunctive queries, and is based on tabled rule evaluation extended with an external oracle that captures reasoning within the ontology. Such an (abstract) oracle receives as input a query along with knowledge already derived, and replies with a (possibly empty) set of atoms, defined in the rules, whose truth would suffice to prove the initial query. With appropriate assumptions on the complexity of the abstract oracle, the general procedure maintains the data complexity of the WFS for Hybrid MKNF knowledge bases.\n",
        "submission_date": "2010-07-20T00:00:00",
        "last_modified_date": "2011-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.3663",
        "title": "A decidable subclass of finitary programs",
        "authors": [
            "Sabrina Baselice",
            "Piero A. Bonatti"
        ],
        "abstract": "Answer set programming - the most popular problem solving paradigm based on logic programs - has been recently extended to support uninterpreted function symbols. All of these approaches have some limitation. In this paper we propose a class of programs called FP2 that enjoys a different trade-off between expressiveness and complexity. FP2 programs enjoy the following unique combination of properties: (i) the ability of expressing predicates with infinite extensions; (ii) full support for predicates with arbitrary arity; (iii) decidability of FP2 membership checking; (iv) decidability of skeptical and credulous stable model reasoning for call-safe queries. Odd cycles are supported by composing FP2 programs with argument restricted programs.\n    ",
        "submission_date": "2010-07-21T00:00:00",
        "last_modified_date": "2010-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.3700",
        "title": "Logic Programming for Finding Models in the Logics of Knowledge and its Applications: A Case Study",
        "authors": [
            "Chitta Baral",
            "Gregory Gelfond",
            "Enrico Pontelli",
            "Tran Cao Son"
        ],
        "abstract": "The logics of knowledge are modal logics that have been shown to be effective in representing and reasoning about knowledge in multi-agent domains. Relatively few computational frameworks for dealing with computation of models and useful transformations in logics of knowledge (e.g., to support multi-agent planning with knowledge actions and degrees of visibility) have been proposed. This paper explores the use of logic programming (LP) to encode interesting forms of logics of knowledge and compute Kripke models. The LP modeling is expanded with useful operators on Kripke structures, to support multi-agent planning in the presence of both world-altering and knowledge actions. This results in the first ever implementation of a planner for this type of complex multi-agent domains.\n    ",
        "submission_date": "2010-07-21T00:00:00",
        "last_modified_date": "2010-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.3884",
        "title": "New Results for the MAP Problem in Bayesian Networks",
        "authors": [
            "Cassio P. de Campos"
        ],
        "abstract": "This paper presents new results for the (partial) maximum a posteriori (MAP) problem in Bayesian networks, which is the problem of querying the most probable state configuration of some of the network variables given evidence. First, it is demonstrated that the problem remains hard even in networks with very simple topology, such as binary polytrees and simple trees (including the Naive Bayes structure). Such proofs extend previous complexity results for the problem. Inapproximability results are also derived in the case of trees if the number of states per variable is not bounded. Although the problem is shown to be hard and inapproximable even in very simple scenarios, a new exact algorithm is described that is empirically fast in networks of bounded treewidth and bounded number of states per variable. The same algorithm is used as basis of a Fully Polynomial Time Approximation Scheme for MAP under such assumptions. Approximation schemes were generally thought to be impossible for this problem, but we show otherwise for classes of networks that are important in practice. The algorithms are extensively tested using some well-known networks as well as random generated cases to show their effectiveness.\n    ",
        "submission_date": "2010-07-22T00:00:00",
        "last_modified_date": "2010-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.4040",
        "title": "Loop Formulas for Description Logic Programs",
        "authors": [
            "Yisong Wang",
            "Jia-Huai You",
            "Li Yan Yuan",
            "Yi-Dong Shen"
        ],
        "abstract": "Description Logic Programs (dl-programs) proposed by Eiter et al. constitute an elegant yet powerful formalism for the integration of answer set programming with description logics, for the Semantic Web. In this paper, we generalize the notions of completion and loop formulas of logic programs to description logic programs and show that the answer sets of a dl-program can be precisely captured by the models of its completion and loop formulas. Furthermore, we propose a new, alternative semantics for dl-programs, called the {\\em canonical answer set semantics}, which is defined by the models of completion that satisfy what are called canonical loop formulas. A desirable property of canonical answer sets is that they are free of circular justifications. Some properties of canonical answer sets are also explored.\n    ",
        "submission_date": "2010-07-23T00:00:00",
        "last_modified_date": "2010-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.4767",
        "title": "Formalization of Psychological Knowledge in Answer Set Programming and its Application",
        "authors": [
            "Marcello Balduccini",
            "Sara Girotto"
        ],
        "abstract": "In this paper we explore the use of Answer Set Programming (ASP) to formalize, and reason about, psychological knowledge. In the field of psychology, a considerable amount of knowledge is still expressed using only natural language. This lack of a formalization complicates accurate studies, comparisons, and verification of theories. We believe that ASP, a knowledge representation formalism allowing for concise and simple representation of defaults, uncertainty, and evolving domains, can be used successfully for the formalization of psychological knowledge. To demonstrate the viability of ASP for this task, in this paper we develop an ASP-based formalization of the mechanics of Short-Term Memory. We also show that our approach can have rather immediate practical uses by demonstrating an application of our formalization to the task of predicting a user's interaction with a graphical interface.\n    ",
        "submission_date": "2010-07-27T00:00:00",
        "last_modified_date": "2011-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.4868",
        "title": "Predicting Suicide Attacks: A Fuzzy Soft Set Approach",
        "authors": [
            "Athar Kharal"
        ],
        "abstract": "This paper models a decision support system to predict the occurance of suicide attack in a given collection of cities. The system comprises two parts. First part analyzes and identifies the factors which affect the prediction. Admitting incomplete information and use of linguistic terms by experts, as two characteristic features of this peculiar prediction problem we exploit the Theory of Fuzzy Soft Sets. Hence the Part 2 of the model is an algorithm vz. FSP which takes the assessment of factors given in Part 1 as its input and produces a possibility profile of cities likely to receive the accident. The algorithm is of O(2^n) complexity. It has been illustrated by an example solved in detail. Simulation results for the algorithm have been presented which give insight into the strengths and weaknesses of FSP. Three different decision making measures have been simulated and compared in our discussion.\n    ",
        "submission_date": "2010-07-28T00:00:00",
        "last_modified_date": "2010-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.5024",
        "title": "A Program-Level Approach to Revising Logic Programs under the Answer Set Semantics",
        "authors": [
            "James P. Delgrande"
        ],
        "abstract": "An approach to the revision of logic programs under the answer set semantics is presented. For programs P and Q, the goal is to determine the answer sets that correspond to the revision of P by Q, denoted P * Q. A fundamental principle of classical (AGM) revision, and the one that guides the approach here, is the success postulate. In AGM revision, this stipulates that A is in K * A. By analogy with the success postulate, for programs P and Q, this means that the answer sets of Q will in some sense be contained in those of P * Q. The essential idea is that for P * Q, a three-valued answer set for Q, consisting of positive and negative literals, is first determined. The positive literals constitute a regular answer set, while the negated literals make up a minimal set of naf literals required to produce the answer set from Q. These literals are propagated to the program P, along with those rules of Q that are not decided by these literals. The approach differs from work in update logic programs in two main respects. First, we ensure that the revising logic program has higher priority, and so we satisfy the success postulate; second, for the preference implicit in a revision P * Q, the program Q as a whole takes precedence over P, unlike update logic programs, since answer sets of Q are propagated to P. We show that a core group of the AGM postulates are satisfied, as are the postulates that have been proposed for update logic programs.\n    ",
        "submission_date": "2010-07-28T00:00:00",
        "last_modified_date": "2010-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.5104",
        "title": "An Empirical Study of Borda Manipulation",
        "authors": [
            "Jessica Davies",
            "George Katsirelos",
            "Nina Narodystka",
            "Toby Walsh"
        ],
        "abstract": "We study the problem of coalitional manipulation in elections using the unweighted Borda rule. We provide empirical evidence of the manipulability of Borda elections in the form of two new greedy manipulation algorithms based on intuitions from the bin-packing and multiprocessor scheduling domains. Although we have not been able to show that these algorithms beat existing methods in the worst-case, our empirical evaluation shows that they significantly outperform the existing method and are able to find optimal manipulations in the vast majority of the randomly generated elections that we tested. These empirical results provide further evidence that the Borda rule provides little defense against coalitional manipulation.\n    ",
        "submission_date": "2010-07-29T00:00:00",
        "last_modified_date": "2010-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.5114",
        "title": "Where are the hard manipulation problems?",
        "authors": [
            "Toby Walsh"
        ],
        "abstract": "One possible escape from the Gibbard-Satterthwaite theorem is computational complexity. For example, it is NP-hard to compute if the STV rule can be manipulated. However, there is increasing concern that such results may not re ect the difficulty of manipulation in practice. In this tutorial, I survey recent results in this area.\n    ",
        "submission_date": "2010-07-29T00:00:00",
        "last_modified_date": "2010-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.5120",
        "title": "Stable marriage problems with quantitative preferences",
        "authors": [
            "Maria Silvia Pini",
            "Francesca Rossi",
            "Brent Venable",
            "Toby Walsh"
        ],
        "abstract": "The stable marriage problem is a well-known problem of matching men to women so that no man and woman, who are not married to each other, both prefer each other. Such a problem has a wide variety of practical applications, ranging from matching resident doctors to hospitals, to matching students to schools or more generally to any two-sided market. In the classical stable marriage problem, both men and women express a strict preference order over the members of the other sex, in a qualitative way. Here we consider stable marriage problems with quantitative preferences: each man (resp., woman) provides a score for each woman (resp., man). Such problems are more expressive than the classical stable marriage problems. Moreover, in some real-life situations it is more natural to express scores (to model, for example, profits or costs) rather than a qualitative preference ordering. In this context, we define new notions of stability and optimality, and we provide algorithms to find marriages which are stable and/or optimal according to these notions. While expressivity greatly increases by adopting quantitative preferences, we show that in most cases the desired solutions can be found by adapting existing algorithms for the classical stable marriage problem.\n    ",
        "submission_date": "2010-07-29T00:00:00",
        "last_modified_date": "2010-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.5130",
        "title": "Resource-Optimal Planning For An Autonomous Planetary Vehicle",
        "authors": [
            "Giuseppe Della Penna",
            "Benedetto Intrigila",
            "Daniele Magazzeni",
            "Fabio Mercorio"
        ],
        "abstract": "Autonomous planetary vehicles, also known as rovers, are small autonomous vehicles equipped with a variety of sensors used to perform exploration and experiments on a planet's surface. Rovers work in a partially unknown environment, with narrow energy/time/movement constraints and, typically, small computational resources that limit the complexity of on-line planning and scheduling, thus they represent a great challenge in the field of autonomous vehicles. Indeed, formal models for such vehicles usually involve hybrid systems with nonlinear dynamics, which are difficult to handle by most of the current planning algorithms and tools. Therefore, when offline planning of the vehicle activities is required, for example for rovers that operate without a continuous Earth supervision, such planning is often performed on simplified models that are not completely realistic. In this paper we show how the UPMurphi model checking based planning tool can be used to generate resource-optimal plans to control the engine of an autonomous planetary vehicle, working directly on its hybrid model and taking into account several safety constraints, thus achieving very accurate results.\n    ",
        "submission_date": "2010-07-29T00:00:00",
        "last_modified_date": "2010-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.5180",
        "title": "CLP-based protein fragment assembly",
        "authors": [
            "Alessandro Dal Palu'",
            "Agostino Dovier",
            "Federico Fogolari",
            "Enrico Pontelli"
        ],
        "abstract": "The paper investigates a novel approach, based on Constraint Logic Programming (CLP), to predict the 3D conformation of a protein via fragments assembly. The fragments are extracted by a preprocessor-also developed for this work- from a database of known protein structures that clusters and classifies the fragments according to similarity and frequency. The problem of assembling fragments into a complete conformation is mapped to a constraint solving problem and solved using CLP. The constraint-based model uses a medium discretization degree Ca-side chain centroid protein model that offers efficiency and a good approximation for space filling. The approach adapts existing energy models to the protein representation used and applies a large neighboring search strategy. The results shows the feasibility and efficiency of the method. The declarative nature of the solution allows to include future extensions, e.g., different size fragments for better accuracy.\n    ",
        "submission_date": "2010-07-29T00:00:00",
        "last_modified_date": "2010-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.5421",
        "title": "Inference with Constrained Hidden Markov Models in PRISM",
        "authors": [
            "Henning Christiansen",
            "Christian Theil Have",
            "Ole Torp Lassen",
            "Matthieu Petit"
        ],
        "abstract": "A Hidden Markov Model (HMM) is a common statistical model which is widely used for analysis of biological sequence data and other sequential phenomena. In the present paper we show how HMMs can be extended with side-constraints and present constraint solving techniques for efficient inference. Defining HMMs with side-constraints in Constraint Logic Programming have advantages in terms of more compact expression and pruning opportunities during inference.\n",
        "submission_date": "2010-07-30T00:00:00",
        "last_modified_date": "2010-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.0273",
        "title": "Threat assessment of a possible Vehicle-Born Improvised Explosive Device using DSmT",
        "authors": [
            "Jean Dezert",
            "Florentin Smarandache"
        ],
        "abstract": "This paper presents the solution about the threat of a VBIED (Vehicle-Born Improvised Explosive Device) obtained with the DSmT (Dezert-Smarandache Theory). This problem has been proposed recently to the authors by Simon Maskell and John Lavery as a typical illustrative example to try to compare the different approaches for dealing with uncertainty for decision-making support. The purpose of this paper is to show in details how a solid justified solution can be obtained from DSmT approach and its fusion rules thanks to a proper modeling of the belief functions involved in this problem.\n    ",
        "submission_date": "2010-08-02T00:00:00",
        "last_modified_date": "2010-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.0659",
        "title": "Evaluating and Improving Modern Variable and Revision Ordering Strategies in CSPs",
        "authors": [
            "Thanasis Balafoutis",
            "Kostas Stergiou"
        ],
        "abstract": "A key factor that can dramatically reduce the search space during constraint solving is the criterion under which the variable to be instantiated next is selected. For this purpose numerous heuristics have been proposed. Some of the best of such heuristics exploit information about failures gathered throughout search and recorded in the form of constraint weights, while others measure the importance of variable assignments in reducing the search space. In this work we experimentally evaluate the most recent and powerful variable ordering heuristics, and new variants of them, over a wide range of benchmarks. Results demonstrate that heuristics based on failures are in general more efficient. Based on this, we then derive new revision ordering heuristics that exploit recorded failures to efficiently order the propagation list when arc consistency is maintained during search. Interestingly, in addition to reducing the number of constraint checks and list operations, these heuristics are also able to cut down the size of the explored search tree.\n    ",
        "submission_date": "2010-08-03T00:00:00",
        "last_modified_date": "2010-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.0660",
        "title": "Adaptive Branching for Constraint Satisfaction Problems",
        "authors": [
            "Thanasis Balafoutis",
            "Kostas Stergiou"
        ],
        "abstract": "The two standard branching schemes for CSPs are d-way and 2-way branching. Although it has been shown that in theory the latter can be exponentially more effective than the former, there is a lack of empirical evidence showing such differences. To investigate this, we initially make an experimental comparison of the two branching schemes over a wide range of benchmarks. Experimental results verify the theoretical gap between d-way and 2-way branching as we move from a simple variable ordering heuristic like dom to more sophisticated ones like dom/ddeg. However, perhaps surprisingly, experiments also show that when state-of-the-art variable ordering heuristics like dom/wdeg are used then d-way can be clearly more efficient than 2-way branching in many cases. Motivated by this observation, we develop two generic heuristics that can be applied at certain points during search to decide whether 2-way branching or a restricted version of 2-way branching, which is close to d-way branching, will be followed. The application of these heuristics results in an adaptive branching scheme. Experiments with instantiations of the two generic heuristics confirm that search with adaptive branching outperforms search with a fixed branching scheme on a wide range of problems.\n    ",
        "submission_date": "2010-08-03T00:00:00",
        "last_modified_date": "2010-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.0823",
        "title": "A Homogeneous Reaction Rule Language for Complex Event Processing",
        "authors": [
            "Adrian Paschke",
            "Alexander Kozlenkov",
            "Harold Boley"
        ],
        "abstract": "Event-driven automation of reactive functionalities for complex event processing is an urgent need in today's distributed service-oriented architectures and Web-based event-driven environments. An important problem to be addressed is how to correctly and efficiently capture and process the event-based behavioral, reactive logic embodied in reaction rules, and combining this with other conditional decision logic embodied, e.g., in derivation rules. This paper elaborates a homogeneous integration approach that combines derivation rules, reaction rules and other rule types such as integrity constraints into the general framework of logic programming, the industrial-strength version of declarative programming. We describe syntax and semantics of the language, implement a distributed web-based middleware using enterprise service technologies and illustrate its adequacy in terms of expressiveness, efficiency and scalability through examples extracted from industrial use cases. The developed reaction rule language provides expressive features such as modular ID-based updates with support for external imports and self-updates of the intensional and extensional knowledge bases, transactions including integrity testing and roll-backs of update transition paths. It also supports distributed complex event processing, event messaging and event querying via efficient and scalable enterprise middleware technologies and event/action reasoning based on an event/action algebra implemented by an interval-based event calculus variant as a logic inference formalism.\n    ",
        "submission_date": "2010-08-04T00:00:00",
        "last_modified_date": "2010-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.1328",
        "title": "Semantic Oriented Agent based Approach towards Engineering Data Management, Web Information Retrieval and User System Communication Problems",
        "authors": [
            "Zeeshan Ahmed",
            "Detlef Gerhard"
        ],
        "abstract": "The four intensive problems to the software rose by the software industry .i.e., User System Communication / Human Machine Interface, Meta Data extraction, Information processing & management and Data representation are discussed in this research paper. To contribute in the field we have proposed and described an intelligent semantic oriented agent based search engine including the concepts of intelligent graphical user interface, natural language based information processing, data management and data reconstruction for the final user end information representation.\n    ",
        "submission_date": "2010-08-07T00:00:00",
        "last_modified_date": "2010-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.1333",
        "title": "An Agent based Approach towards Metadata Extraction, Modelling and Information Retrieval over the Web",
        "authors": [
            "Zeeshan Ahmed",
            "Detlef Gerhard"
        ],
        "abstract": "Web development is a challenging research area for its creativity and complexity. The existing raised key challenge in web technology technologic development is the presentation of data in machine read and process able format to take advantage in knowledge based information extraction and maintenance. Currently it is not possible to search and extract optimized results using full text queries because there is no such mechanism exists which can fully extract the semantic from full text queries and then look for particular knowledge based information.\n    ",
        "submission_date": "2010-08-07T00:00:00",
        "last_modified_date": "2010-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.1484",
        "title": "A note on communicating between information systems based on including degrees",
        "authors": [
            "Ping Zhu",
            "Qiaoyan Wen"
        ],
        "abstract": "In order to study the communication between information systems, Gong and Xiao [Z. Gong and Z. Xiao, Communicating between information systems based on including degrees, International Journal of General Systems 39 (2010) 189--206] proposed the concept of general relation mappings based on including degrees. Some properties and the extension for fuzzy information systems of the general relation mappings have been investigated there. In this paper, we point out by counterexamples that several assertions (Lemma 3.1, Lemma 3.2, Theorem 4.1, and Theorem 4.3) in the aforementioned work are not true in general.\n    ",
        "submission_date": "2010-08-09T00:00:00",
        "last_modified_date": "2010-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.1643",
        "title": "A Learning Algorithm based on High School Teaching Wisdom",
        "authors": [
            "Ninan Sajeeth Philip"
        ],
        "abstract": "A learning algorithm based on primary school teaching and learning is presented. The methodology is to continuously evaluate a student and to give them training on the examples for which they repeatedly fail, until, they can correctly answer all types of questions. This incremental learning procedure produces better learning curves by demanding the student to optimally dedicate their learning time on the failed examples. When used in machine learning, the algorithm is found to train a machine on a data with maximum variance in the feature space so that the generalization ability of the network improves. The algorithm has interesting applications in data mining, model evaluations and rare objects discovery.\n    ",
        "submission_date": "2010-08-10T00:00:00",
        "last_modified_date": "2010-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.1710",
        "title": "Introduction to the 26th International Conference on Logic Programming Special Issue",
        "authors": [
            "Manuel Hermenegildo",
            "Torsten Schaub"
        ],
        "abstract": "This is the preface to the 26th International Conference on Logic Programming Special Issue\n    ",
        "submission_date": "2010-08-10T00:00:00",
        "last_modified_date": "2010-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.1723",
        "title": "Role of Ontology in Semantic Web Development",
        "authors": [
            "Zeeshan Ahmed",
            "Detlef Gerhard"
        ],
        "abstract": "World Wide Web (WWW) is the most popular global information sharing and communication system consisting of three standards .i.e., Uniform Resource Identifier (URL), Hypertext Transfer Protocol (HTTP) and Hypertext Mark-up Language (HTML). Information is provided in text, image, audio and video formats over the web by using HTML which is considered to be unconventional in defining and formalizing the meaning of the context...\n    ",
        "submission_date": "2010-08-07T00:00:00",
        "last_modified_date": "2010-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.2514",
        "title": "Epistemic irrelevance in credal nets: the case of imprecise Markov trees",
        "authors": [
            "Gert de Cooman",
            "Filip Hermans",
            "Alessandro Antonucci",
            "Marco Zaffalon"
        ],
        "abstract": "We focus on credal nets, which are graphical models that generalise Bayesian nets to imprecise probability. We replace the notion of strong independence commonly used in credal nets with the weaker notion of epistemic irrelevance, which is arguably more suited for a behavioural theory of probability. Focusing on directed trees, we show how to combine the given local uncertainty models in the nodes of the graph into a global model, and we use this to construct and justify an exact message-passing algorithm that computes updated beliefs for a variable in the tree. The algorithm, which is linear in the number of nodes, is formulated entirely in terms of coherent lower previsions, and is shown to satisfy a number of rationality requirements. We supply examples of the algorithm's operation, and report an application to on-line character recognition that illustrates the advantages of our approach for prediction. We comment on the perspectives, opened by the availability, for the first time, of a truly efficient algorithm based on epistemic irrelevance.\n    ",
        "submission_date": "2010-08-15T00:00:00",
        "last_modified_date": "2010-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.3314",
        "title": "Maximum entropy models and subjective interestingness: an application to tiles in binary databases",
        "authors": [
            "Tijl De Bie"
        ],
        "abstract": "  Recent research has highlighted the practical benefits of subjective interestingness measures, which quantify the novelty or unexpectedness of a pattern when contrasted with any prior information of the data miner (Silberschatz and Tuzhilin, 1995; Geng and Hamilton, 2006). A key challenge here is the formalization of this prior information in a way that lends itself to the definition of an interestingness subjective measure that is both meaningful and practical.\n",
        "submission_date": "2010-08-19T00:00:00",
        "last_modified_date": "2010-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.3879",
        "title": "A formalism for causal explanations with an Answer Set Programming translation",
        "authors": [
            "Yves Moinard"
        ],
        "abstract": "We examine the practicality for a user of using Answer Set Programming (ASP) for representing logical formalisms. Our example is a formalism aiming at capturing causal explanations from causal information. We show the naturalness and relative efficiency of this translation job. We are interested in the ease for writing an ASP program. Limitations of the earlier systems made that in practice, the ``declarative aspect'' was more theoretical than practical. We show how recent improvements in working ASP systems facilitate the translation.\n    ",
        "submission_date": "2010-08-23T00:00:00",
        "last_modified_date": "2010-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.4071",
        "title": "Hybrid tractability of soft constraint problems",
        "authors": [
            "Martin C. Cooper",
            "Stanislav Zivny"
        ],
        "abstract": "The constraint satisfaction problem (CSP) is a central generic problem in computer science and artificial intelligence: it provides a common framework for many theoretical problems as well as for many real-life applications. Soft constraint problems are a generalisation of the CSP which allow the user to model optimisation problems. Considerable effort has been made in identifying properties which ensure tractability in such problems. In this work, we initiate the study of hybrid tractability of soft constraint problems; that is, properties which guarantee tractability of the given soft constraint problem, but which do not depend only on the underlying structure of the instance (such as being tree-structured) or only on the types of soft constraints in the instance (such as submodularity). We present several novel hybrid classes of soft constraint problems, which include a machine scheduling problem, constraint problems of arbitrary arities with no overlapping nogoods, and the SoftAllDiff constraint with arbitrary unary soft constraints. An important tool in our investigation will be the notion of forbidden substructures.\n    ",
        "submission_date": "2010-08-24T00:00:00",
        "last_modified_date": "2010-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.4257",
        "title": "Learning from Profession Knowledge: Application on Knitting",
        "authors": [
            "Nada Matta",
            "Oswaldo Castillo"
        ],
        "abstract": "Knowledge Management is a global process in companies. It includes all the processes that allow capitalization, sharing and evolution of the Knowledge Capital of the firm, generally recognized as a critical resource of the organization. Several approaches have been defined to capitalize knowledge but few of them study how to learn from this knowledge. We present in this paper an approach that helps to enhance learning from profession knowledge in an organisation. We apply our approach on knitting industry.\n    ",
        "submission_date": "2010-08-25T00:00:00",
        "last_modified_date": "2010-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.4310",
        "title": "Mod\u00e9lisation d'une analyse pragma-linguistique d'un forum de discussion",
        "authors": [
            "Nada Matta",
            "Karima Sidoumou",
            "Goritsa Ninova",
            "Hassan Atifi"
        ],
        "abstract": "We present in this paper, a modelling of an expertise in pragmatics. We follow knowledge engineering techniques and observe the expert when he analyses a social discussion forum. Then a number of models are defined. These models emphasises the process followed by the expert and a number of criteria used in his analysis. Results can be used as guides that help to understand and annotate discussion forum. We aim at modelling other pragmatics analysis in order to complete the base of guides; criteria, process, etc. of discussion analysis\n    ",
        "submission_date": "2010-08-25T00:00:00",
        "last_modified_date": "2010-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.4326",
        "title": "Machine learning for constraint solver design -- A case study for the alldifferent constraint",
        "authors": [
            "Ian Gent",
            "Lars Kotthoff",
            "Ian Miguel",
            "Peter Nightingale"
        ],
        "abstract": "Constraint solvers are complex pieces of software which require many design decisions to be made by the implementer based on limited information. These decisions affect the performance of the finished solver significantly. Once a design decision has been made, it cannot easily be reversed, although a different decision may be more appropriate for a particular problem.\n",
        "submission_date": "2010-08-25T00:00:00",
        "last_modified_date": "2010-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.4328",
        "title": "Distributed solving through model splitting",
        "authors": [
            "Lars Kotthoff",
            "Neil C.A. Moore"
        ],
        "abstract": "Constraint problems can be trivially solved in parallel by exploring different branches of the search tree concurrently. Previous approaches have focused on implementing this functionality in the solver, more or less transparently to the user. We propose a new approach, which modifies the constraint model of the problem. An existing model is split into new models with added constraints that partition the search space. Optionally, additional constraints are imposed that rule out the search already done. The advantages of our approach are that it can be implemented easily, computations can be stopped and restarted, moved to different machines and indeed solved on machines which are not able to communicate with each other at all.\n    ",
        "submission_date": "2010-08-25T00:00:00",
        "last_modified_date": "2010-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.5161",
        "title": "Artificial Brain Based on Credible Neural Circuits in a Human Brain",
        "authors": [
            "John Robert Burger"
        ],
        "abstract": "Neurons are individually translated into simple gates to plan a brain based on human psychology and intelligence. State machines, assumed previously learned in subconscious associative memory are shown to enable equation solving and rudimentary thinking using nanoprocessing within short term memory.\n    ",
        "submission_date": "2010-08-30T00:00:00",
        "last_modified_date": "2010-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.5163",
        "title": "Learning Multi-modal Similarity",
        "authors": [
            "Brian McFee",
            "Gert Lanckriet"
        ],
        "abstract": "In many applications involving multi-media data, the definition of similarity between items is integral to several key tasks, e.g., nearest-neighbor retrieval, classification, and recommendation. Data in such regimes typically exhibits multiple modalities, such as acoustic and visual content of video. Integrating such heterogeneous data to form a holistic similarity space is therefore a key challenge to be overcome in many real-world applications.\n",
        "submission_date": "2010-08-30T00:00:00",
        "last_modified_date": "2010-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.5188",
        "title": "Totally Corrective Boosting for Regularized Risk Minimization",
        "authors": [
            "Chunhua Shen",
            "Hanxi Li",
            "Nick Barnes"
        ],
        "abstract": "Consideration of the primal and dual problems together leads to important new insights into the characteristics of boosting algorithms. In this work, we propose a general framework that can be used to design new boosting algorithms. A wide variety of machine learning problems essentially minimize a regularized risk functional. We show that the proposed boosting framework, termed CGBoost, can accommodate various loss functions and different regularizers in a totally-corrective optimization fashion. We show that, by solving the primal rather than the dual, a large body of totally-corrective boosting algorithms can actually be efficiently solved and no sophisticated convex optimization solvers are needed. We also demonstrate that some boosting algorithms like AdaBoost can be interpreted in our framework--even their optimization is not totally corrective. We empirically show that various boosting algorithms based on the proposed framework perform similarly on the UCIrvine machine learning datasets [1] that we have used in the experiments.\n    ",
        "submission_date": "2010-08-30T00:00:00",
        "last_modified_date": "2011-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.5189",
        "title": "Improving the Performance of maxRPC",
        "authors": [
            "Thanasis Balafoutis",
            "Anastasia Paparrizou",
            "Kostas Stergiou",
            "Toby Walsh"
        ],
        "abstract": "Max Restricted Path Consistency (maxRPC) is a local consistency for binary constraints that can achieve considerably stronger pruning than arc consistency. However, existing maxRRC algorithms suffer from overheads and redundancies as they can repeatedly perform many constraint checks without triggering any value deletions. In this paper we propose techniques that can boost the performance of maxRPC algorithms. These include the combined use of two data structures to avoid many redundant constraint checks, and heuristics for the efficient ordering and execution of certain operations. Based on these, we propose two closely related algorithms. The first one which is a maxRPC algorithm with optimal O(end^3) time complexity, displays good performance when used stand-alone, but is expensive to apply during search. The second one approximates maxRPC and has O(en^2d^4) time complexity, but a restricted version with O(end^4) complexity can be very efficient when used during search. Both algorithms have O(ed) space complexity. Experimental results demonstrate that the resulting methods constantly outperform previous algorithms for maxRPC, often by large margins, and constitute a more than viable alternative to arc consistency on many problems.\n    ",
        "submission_date": "2010-08-30T00:00:00",
        "last_modified_date": "2010-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.5387",
        "title": "Pattern Recognition in Collective Cognitive Systems: Hybrid Human-Machine Learning (HHML) By Heterogeneous Ensembles",
        "authors": [
            "Hesam T. Dashti",
            "Adel Ardalan",
            "Alireza F. Siahpirani",
            "Jernej Tonejc",
            "Ioan V. Uilecan",
            "Tiago Simas",
            "Bruno Miranda",
            "Rita Ribeiro",
            "Liya Wang",
            "Amir H. Assadi"
        ],
        "abstract": "The ubiquitous role of the cyber-infrastructures, such as the WWW, provides myriad opportunities for machine learning and its broad spectrum of application domains taking advantage of digital communication. Pattern classification and feature extraction are among the first applications of machine learning that have received extensive attention. The most remarkable achievements have addressed data sets of moderate-to-large size. The 'data deluge' in the last decade or two has posed new challenges for AI researchers to design new, effective and accurate algorithms for similar tasks using ultra-massive data sets and complex (natural or synthetic) dynamical systems. We propose a novel principled approach to feature extraction in hybrid architectures comprised of humans and machines in networked communication, who collaborate to solve a pre-assigned pattern recognition (feature extraction) task. There are two practical considerations addressed below: (1) Human experts, such as plant biologists or astronomers, often use their visual perception and other implicit prior knowledge or expertise without any obvious constraints to search for the significant features, whereas machines are limited to a pre-programmed set of criteria to work with; (2) in a team collaboration of collective problem solving, the human experts have diverse abilities that are complementary, and they learn from each other to succeed in cognitively complex tasks in ways that are still impossible imitate by machines.\n    ",
        "submission_date": "2010-08-31T00:00:00",
        "last_modified_date": "2010-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.0077",
        "title": "Not only a lack of right definitions: Arguments for a shift in information-processing paradigm",
        "authors": [
            "Emanuel Diamant"
        ],
        "abstract": "Machine Consciousness and Machine Intelligence are not simply new buzzwords that occupy our imagination. Over the last decades, we witness an unprecedented rise in attempts to create machines with human-like features and capabilities. However, despite widespread sympathy and abundant funding, progress in these enterprises is far from being satisfactory. The reasons for this are twofold: First, the notions of cognition and intelligence (usually borrowed from human behavior studies) are notoriously blurred and ill-defined, and second, the basic concepts underpinning the whole discourse are by themselves either undefined or defined very vaguely. That leads to improper and inadequate research goals determination, which I will illustrate with some examples drawn from recent documents issued by DARPA and the European Commission. On the other hand, I would like to propose some remedies that, I hope, would improve the current state-of-the-art disgrace.\n    ",
        "submission_date": "2010-09-01T00:00:00",
        "last_modified_date": "2010-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.0347",
        "title": "Solving the Resource Constrained Project Scheduling Problem with Generalized Precedences by Lazy Clause Generation",
        "authors": [
            "Andreas Schutt",
            "Thibaut Feydy",
            "Peter J. Stuckey",
            "Mark G. Wallace"
        ],
        "abstract": "The technical report presents a generic exact solution approach for minimizing the project duration of the resource-constrained project scheduling problem with generalized precedences (Rcpsp/max). The approach uses lazy clause generation, i.e., a hybrid of finite domain and Boolean satisfiability solving, in order to apply nogood learning and conflict-driven search on the solution generation. Our experiments show the benefit of lazy clause generation for finding an optimal solutions and proving its optimality in comparison to other state-of-the-art exact and non-exact methods. The method is highly robust: it matched or bettered the best known results on all of the 2340 instances we examined except 3, according to the currently available data on the PSPLib. Of the 631 open instances in this set it closed 573 and improved the bounds of 51 of the remaining 58 instances.\n    ",
        "submission_date": "2010-09-02T00:00:00",
        "last_modified_date": "2010-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.0407",
        "title": "Experimental Evaluation of Branching Schemes for the CSP",
        "authors": [
            "Thanasis Balafoutis",
            "Anastasia Paparrizou",
            "Kostas Stergiou"
        ],
        "abstract": "The search strategy of a CP solver is determined by the variable and value ordering heuristics it employs and by the branching scheme it follows. Although the effects of variable and value ordering heuristics on search effort have been widely studied, the effects of different branching schemes have received less attention. In this paper we study this effect through an experimental evaluation that includes standard branching schemes such as 2-way, d-way, and dichotomic domain splitting, as well as variations of set branching where branching is performed on sets of values. We also propose and evaluate a generic approach to set branching where the partition of a domain into sets is created using the scores assigned to values by a value ordering heuristic, and a clustering algorithm from machine learning. Experimental results demonstrate that although exponential differences between branching schemes, as predicted in theory between 2-way and d-way branching, are not very common, still the choice of branching scheme can make quite a difference on certain classes of problems. Set branching methods are very competitive with 2-way branching and outperform it on some problem classes. A statistical analysis of the results reveals that our generic clustering-based set branching method is the best among the methods compared.\n    ",
        "submission_date": "2010-09-02T00:00:00",
        "last_modified_date": "2010-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.0451",
        "title": "The Challenge of Believability in Video Games: Definitions, Agents Models and Imitation Learning",
        "authors": [
            "Fabien Tenc\u00e9",
            "C\u00e9dric Buche",
            "Pierre De Loor",
            "Olivier Marc"
        ],
        "abstract": "In this paper, we address the problem of creating believable agents (virtual characters) in video games. We consider only one meaning of believability, ``giving the feeling of being controlled by a player'', and outline the problem of its evaluation. We present several models for agents in games which can produce believable behaviours, both from industry and research. For high level of believability, learning and especially imitation learning seems to be the way to go. We make a quick overview of different approaches to make video games' agents learn from players. To conclude we propose a two-step method to develop new models for believable agents. First we must find the criteria for believability for our application and define an evaluation method. Then the model and the learning algorithm can be designed.\n    ",
        "submission_date": "2010-09-02T00:00:00",
        "last_modified_date": "2010-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.0501",
        "title": "Automatable Evaluation Method Oriented toward Behaviour Believability for Video Games",
        "authors": [
            "Fabien Tenc\u00e9",
            "C\u00e9dric Buche"
        ],
        "abstract": "Classic evaluation methods of believable agents are time-consuming because they involve many human to judge agents. They are well suited to validate work on new believable behaviours models. However, during the implementation, numerous experiments can help to improve agents' believability. We propose a method which aim at assessing how much an agent's behaviour looks like humans' behaviours. By representing behaviours with vectors, we can store data computed for humans and then evaluate as many agents as needed without further need of humans. We present a test experiment which shows that even a simple evaluation following our method can reveal differences between quite believable agents and humans. This method seems promising although, as shown in our experiment, results' analysis can be difficult.\n    ",
        "submission_date": "2010-09-02T00:00:00",
        "last_modified_date": "2010-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.0550",
        "title": "Optimizing Selective Search in Chess",
        "authors": [
            "Omid David-Tabibi",
            "Moshe Koppel",
            "Nathan S. Netanyahu"
        ],
        "abstract": "In this paper we introduce a novel method for automatically tuning the search parameters of a chess program using genetic algorithms. Our results show that a large set of parameter values can be learned automatically, such that the resulting performance is comparable with that of manually tuned parameters of top tournament-playing chess programs.\n    ",
        "submission_date": "2010-09-02T00:00:00",
        "last_modified_date": "2010-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.1174",
        "title": "Parameterized Complexity Results in Symmetry Breaking",
        "authors": [
            "Toby Walsh"
        ],
        "abstract": "Symmetry is a common feature of many combinatorial problems. Unfortunately eliminating all symmetry from a problem is often computationally intractable. This paper argues that recent parameterized complexity results provide insight into that intractability and help identify special cases in which symmetry can be dealt with more tractably\n    ",
        "submission_date": "2010-09-06T00:00:00",
        "last_modified_date": "2010-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.2003",
        "title": "AI 3D Cybug Gaming",
        "authors": [
            "Zeeshan Ahmed"
        ],
        "abstract": "In this short paper I briefly discuss 3D war Game based on artificial intelligence concepts called AI WAR. Going in to the details, I present the importance of CAICL language and how this language is used in AI WAR. Moreover I also present a designed and implemented 3D War Cybug for AI WAR using CAICL and discus the implemented strategy to defeat its enemies during the game life.\n    ",
        "submission_date": "2010-09-10T00:00:00",
        "last_modified_date": "2010-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.2041",
        "title": "Multi-Agent Only-Knowing Revisited",
        "authors": [
            "Vaishak Belle",
            "Gerhard Lakemeyer"
        ],
        "abstract": "  Levesque introduced the notion of only-knowing to precisely capture the beliefs of a knowledge base. He also showed how only-knowing can be used to formalize non-monotonic behavior within a monotonic logic. Despite its appeal, all attempts to extend only-knowing to the many agent case have undesirable properties. A belief model by Halpern and Lakemeyer, for instance, appeals to proof-theoretic constructs in the semantics and needs to axiomatize validity as part of the logic. It is also not clear how to generalize their ideas to a first-order case. In this paper, we propose a new account of multi-agent only-knowing which, for the first time, has a natural possible-world semantics for a quantified language with equality. We then provide, for the propositional fragment, a sound and complete axiomatization that faithfully lifts Levesque's proof theory to the many agent case. We also discuss comparisons to the earlier approach by Halpern and Lakemeyer.\n    ",
        "submission_date": "2010-09-10T00:00:00",
        "last_modified_date": "2010-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.2084",
        "title": "Ontology Temporal Evolution for Multi-Entity Bayesian Networks under Exogenous and Endogenous Semantic Updating",
        "authors": [
            "Massimiliano Dal Mas"
        ],
        "abstract": "It is a challenge for any Knowledge Base reasoning to manage ubiquitous uncertain ontology as well as uncertain updating times, while achieving acceptable service levels at minimum computational cost. This paper proposes an application-independent merging ontologies for any open interaction system. A solution that uses Multi-Entity Bayesan Networks with SWRL rules, and a Java program is presented to dynamically monitor Exogenous and Endogenous temporal evolution on updating merging ontologies on a probabilistic framework for the Semantic Web.\n    ",
        "submission_date": "2010-09-10T00:00:00",
        "last_modified_date": "2010-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.4586",
        "title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
        "authors": [
            "Md. Hijbul Alam",
            "Abdul Kadar Muhammad Masum",
            "Mohammad Mahadi Hassan",
            "S. M. Kamruzzaman"
        ],
        "abstract": "In this paper we present an optimal Bangla Keyboard Layout, which distributes the load equally on both hands so that maximizing the ease and minimizing the effort. Bangla alphabet has a large number of letters, for this it is difficult to type faster using Bangla keyboard. Our proposed keyboard will maximize the speed of operator as they can type with both hands parallel. Here we use the association rule of data mining to distribute the Bangla characters in the keyboard. First, we analyze the frequencies of data consisting of monograph, digraph and trigraph, which are derived from data wire-house, and then used association rule of data mining to distribute the Bangla characters in the layout. Finally, we propose a Bangla Keyboard Layout. Experimental results on several keyboard layout shows the effectiveness of the proposed approach with better performance.\n    ",
        "submission_date": "2010-09-23T00:00:00",
        "last_modified_date": "2010-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.4982",
        "title": "Optimal Bangla Keyboard Layout using Data Mining Technique",
        "authors": [
            "S. M. Kamruzzaman",
            "Md. Hijbul Alam",
            "Abdul Kadar Muhammad Masum",
            "Md. Mahadi Hassan"
        ],
        "abstract": "This paper presents an optimal Bangla Keyboard Layout, which distributes the load equally on both hands so that maximizing the ease and minimizing the effort. Bangla alphabet has a large number of letters, for this it is difficult to type faster using Bangla keyboard. Our proposed keyboard will maximize the speed of operator as they can type with both hands parallel. Here we use the association rule of data mining to distribute the Bangla characters in the keyboard. First, we analyze the frequencies of data consisting of monograph, digraph and trigraph, which are derived from data wire-house, and then used association rule of data mining to distribute the Bangla characters in the layout. Experimental results on several data show the effectiveness of the proposed approach with better performance.\n    ",
        "submission_date": "2010-09-25T00:00:00",
        "last_modified_date": "2010-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.5048",
        "title": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique",
        "authors": [
            "Abdul Kadar Muhammad Masum",
            "Mohammad Mahadi Hassan",
            "S. M. Kamruzzaman"
        ],
        "abstract": "Bangla alphabet has a large number of letters, for this it is complicated to type faster using Bangla keyboard. The proposed keyboard will maximize the speed of operator as they can type with both hands parallel. Association rule of data mining to distribute the Bangla characters in the keyboard is used here. The frequencies of data consisting of monograph, digraph and trigraph are analyzed, which are derived from data wire-house, and then used association rule of data mining to distribute the Bangla characters in the layout. Experimental results on several data show the effectiveness of the proposed approach with better performance. This paper presents an optimal Bangla Keyboard Layout, which distributes the load equally on both hands so that maximizing the ease and minimizing the effort.\n    ",
        "submission_date": "2010-09-26T00:00:00",
        "last_modified_date": "2010-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.5268",
        "title": "General Scaled Support Vector Machines",
        "authors": [
            "Xin Liu",
            "Ying Ding",
            "Forrest Sheng Bao"
        ],
        "abstract": "Support Vector Machines (SVMs) are popular tools for data mining tasks such as classification, regression, and density estimation. However, original SVM (C-SVM) only considers local information of data points on or over the margin. Therefore, C-SVM loses robustness. To solve this problem, one approach is to translate (i.e., to move without rotation or change of shape) the hyperplane according to the distribution of the entire data. But existing work can only be applied for 1-D case. In this paper, we propose a simple and efficient method called General Scaled SVM (GS-SVM) to extend the existing approach to multi-dimensional case. Our method translates the hyperplane according to the distribution of data projected on the normal vector of the hyperplane. Compared with C-SVM, GS-SVM has better performance on several data sets.\n    ",
        "submission_date": "2010-09-27T00:00:00",
        "last_modified_date": "2010-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.5290",
        "title": "Measuring Similarity of Graphs and their Nodes by Neighbor Matching",
        "authors": [
            "Mladen Nikolic"
        ],
        "abstract": "The problem of measuring similarity of graphs and their nodes is important in a range of practical problems. There is a number of proposed measures, some of them being based on iterative calculation of similarity between two graphs and the principle that two nodes are as similar as their neighbors are. In our work, we propose one novel method of that sort, with a refined concept of similarity of two nodes that involves matching of their neighbors. We prove convergence of the proposed method and show that it has some additional desirable properties that, to our knowledge, the existing methods lack. We illustrate the method on two specific problems and empirically compare it to other methods.\n    ",
        "submission_date": "2010-09-27T00:00:00",
        "last_modified_date": "2010-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.6119",
        "title": "A Comprehensive Survey of Data Mining-based Fraud Detection Research",
        "authors": [
            "Clifton Phua",
            "Vincent Lee",
            "Kate Smith",
            "Ross Gayler"
        ],
        "abstract": "This survey paper categorises, compares, and summarises from almost all published technical and review articles in automated fraud detection within the last 10 years. It defines the professional fraudster, formalises the main types and subtypes of known fraud, and presents the nature of data evidence collected within affected industries. Within the business context of mining the data to achieve higher cost savings, this research presents methods and techniques together with their problems. Compared to all related reviews on fraud detection, this survey covers much more technical articles and is the only one, to the best of our knowledge, which proposes alternative data and solutions from related domains.\n    ",
        "submission_date": "2010-09-30T00:00:00",
        "last_modified_date": "2010-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.6127",
        "title": "Efficient Knowledge Base Management in DCSP",
        "authors": [
            "Hong Jiang"
        ],
        "abstract": "DCSP (Distributed Constraint Satisfaction Problem) has been a very important research area in AI (Artificial Intelligence). There are many application problems in distributed AI that can be formalized as DSCPs. With the increasing complexity and problem size of the application problems in AI, the required storage place in searching and the average searching time are increasing too. Thus, to use a limited storage place efficiently in solving DCSP becomes a very important problem, and it can help to reduce searching time as well. This paper provides an efficient knowledge base management approach based on general usage of hyper-resolution-rule in consistence algorithm. The approach minimizes the increasing of the knowledge base by eliminate sufficient constraint and false nogood. These eliminations do not change the completeness of the original knowledge base increased. The proofs are given as well. The example shows that this approach decrease both the new nogoods generated and the knowledge base greatly. Thus it decreases the required storage place and simplify the searching process.\n    ",
        "submission_date": "2010-09-30T00:00:00",
        "last_modified_date": "2010-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.0298",
        "title": "Steepest Ascent Hill Climbing For A Mathematical Problem",
        "authors": [
            "Siby Abraham",
            "Imre Kiss",
            "Sugata Sanyal",
            "Mukund Sanglikar"
        ],
        "abstract": "The paper proposes artificial intelligence technique called hill climbing to find numerical solutions of Diophantine Equations. Such equations are important as they have many applications in fields like public key cryptography, integer factorization, algebraic curves, projective curves and data dependency in super computers. Importantly, it has been proved that there is no general method to find solutions of such equations. This paper is an attempt to find numerical solutions of Diophantine equations using steepest ascent version of Hill Climbing. The method, which uses tree representation to depict possible solutions of Diophantine equations, adopts a novel methodology to generate successors. The heuristic function used help to make the process of finding solution as a minimization process. The work illustrates the effectiveness of the proposed methodology using a class of Diophantine equations given by a1. x1 p1 + a2. x2 p2 + ...... + an . xn pn = N where ai and N are integers. The experimental results validate that the procedure proposed is successful in finding solutions of Diophantine Equations with sufficiently large powers and large number of variables.\n    ",
        "submission_date": "2010-10-02T00:00:00",
        "last_modified_date": "2010-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.2102",
        "title": "Hierarchical Multiclass Decompositions with Application to Authorship Determination",
        "authors": [
            "Ran El-Yaniv",
            "Noam Etzion-Rosenberg"
        ],
        "abstract": "This paper is mainly concerned with the question of how to decompose multiclass classification problems into binary subproblems. We extend known Jensen-Shannon bounds on the Bayes risk of binary problems to hierarchical multiclass problems and use these bounds to develop a heuristic procedure for constructing hierarchical multiclass decomposition for multinomials. We test our method and compare it to the well known \"all-pairs\" decomposition. Our tests are performed using a new authorship determination benchmark test of machine learning authors. The new method consistently outperforms the all-pairs decomposition when the number of classes is small and breaks even on larger multiclass problems. Using both methods, the classification accuracy we achieve, using an SVM over a feature set consisting of both high frequency single tokens and high frequency token-pairs, appears to be exceptionally high compared to known results in authorship determination.\n    ",
        "submission_date": "2010-10-11T00:00:00",
        "last_modified_date": "2010-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.3177",
        "title": "Introduction to the iDian",
        "authors": [
            "Xin Rong"
        ],
        "abstract": "The iDian (previously named as the Operation Agent System) is a framework designed to enable computer users to operate software in natural language. Distinct from current speech-recognition systems, our solution supports format-free combinations of orders, and is open to both developers and customers. We used a multi-layer structure to build the entire framework, approached rule-based natural language processing, and implemented demos narrowing down to Windows, text-editing and a few other applications. This essay will firstly give an overview of the entire system, and then scrutinize the functions and structure of the system, and finally discuss the prospective de-velopment, esp. on-line interaction functions.\n    ",
        "submission_date": "2010-10-15T00:00:00",
        "last_modified_date": "2010-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.4385",
        "title": "A Protocol for Self-Synchronized Duty-Cycling in Sensor Networks: Generic Implementation in Wiselib",
        "authors": [
            "Hugo Hern\u00e1ndez",
            "Tobias Baumgartner",
            "Maria J. Blesa",
            "Christian Blum",
            "Alexander Kr\u00f6ller",
            "Sandor P. Fekete"
        ],
        "abstract": "In this work we present a protocol for self-synchronized duty-cycling in wireless sensor networks with energy harvesting capabilities. The protocol is implemented in Wiselib, a library of generic algorithms for sensor networks. Simulations are conducted with the sensor network simulator Shawn. They are based on the specifications of real hardware known as iSense sensor nodes. The experimental results show that the proposed mechanism is able to adapt to changing energy availabilities. Moreover, it is shown that the system is very robust against packet loss.\n    ",
        "submission_date": "2010-10-21T00:00:00",
        "last_modified_date": "2010-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.4561",
        "title": "New S-norm and T-norm Operators for Active Learning Method",
        "authors": [
            "Ali Akbar Kiaei",
            "Saeed Bagheri Shouraki",
            "Seyed Hossein Khasteh",
            "Mahmoud Khademi",
            "Ali Reza Ghatreh Samani"
        ],
        "abstract": "Active Learning Method (ALM) is a soft computing method used for modeling and control based on fuzzy logic. All operators defined for fuzzy sets must serve as either fuzzy S-norm or fuzzy T-norm. Despite being a powerful modeling method, ALM does not possess operators which serve as S-norms and T-norms which deprive it of a profound analytical expression/form. This paper introduces two new operators based on morphology which satisfy the following conditions: First, they serve as fuzzy S-norm and T-norm. Second, they satisfy Demorgans law, so they complement each other perfectly. These operators are investigated via three viewpoints: Mathematics, Geometry and fuzzy logic.\n    ",
        "submission_date": "2010-10-21T00:00:00",
        "last_modified_date": "2011-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.4609",
        "title": "A Partial Taxonomy of Substitutability and Interchangeability",
        "authors": [
            "Shant Karakashian",
            "Robert Woodward",
            "Berthe Y. Choueiry",
            "Steven Prestwhich",
            "Eugene C. Freuder"
        ],
        "abstract": "Substitutability, interchangeability and related concepts in Constraint Programming were introduced approximately twenty years ago and have given rise to considerable subsequent research. We survey this work, classify, and relate the different concepts, and indicate directions for future work, in particular with respect to making connections with research into symmetry breaking. This paper is a condensed version of a larger work in progress.\n    ",
        "submission_date": "2010-10-22T00:00:00",
        "last_modified_date": "2010-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.4784",
        "title": "Learning under Concept Drift: an Overview",
        "authors": [
            "Indr\u0117 \u017dliobait\u0117"
        ],
        "abstract": "Concept drift refers to a non stationary learning problem over time. The training and the application data often mismatch in real life problems. In this report we present a context of concept drift problem 1. We focus on the issues relevant to adaptive training set formation. We present the framework and terminology, and formulate a global picture of concept drift learners design. We start with formalizing the framework for the concept drifting data in Section 1. In Section 2 we discuss the adaptivity mechanisms of the concept drift learners. In Section 3 we overview the principle mechanisms of concept drift learners. In this chapter we give a general picture of the available algorithms and categorize them based on their properties. Section 5 discusses the related research fields and Section 5 groups and presents major concept drift applications. This report is intended to give a bird's view of concept drift research field, provide a context of the research and position it within broad spectrum of research fields and applications.\n    ",
        "submission_date": "2010-10-22T00:00:00",
        "last_modified_date": "2010-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.4830",
        "title": "A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models",
        "authors": [
            "Neil D. Lawrence"
        ],
        "abstract": "We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian Markov random fields (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting model, which we call maximum entropy unfolding (MEU) is a nonlinear generalization of principal component analysis. We relate the model to Laplacian eigenmaps and isomap. We show that parameter fitting in the locally linear embedding (LLE) is approximate maximum likelihood MEU. We introduce a variant of LLE that performs maximum likelihood exactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random field via the graphical lasso.\n    ",
        "submission_date": "2010-10-22T00:00:00",
        "last_modified_date": "2012-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.5426",
        "title": "Translation-Invariant Representation for Cumulative Foot Pressure Images",
        "authors": [
            "Shuai Zheng",
            "Kaiqi Huang",
            "Tieniu Tan"
        ],
        "abstract": "Human can be distinguished by different limb movements and unique ground reaction force. Cumulative foot pressure image is a 2-D cumulative ground reaction force during one gait cycle. Although it contains pressure spatial distribution information and pressure temporal distribution information, it suffers from several problems including different shoes and noise, when putting it into practice as a new biometric for pedestrian identification. In this paper, we propose a hierarchical translation-invariant representation for cumulative foot pressure images, inspired by the success of Convolutional deep belief network for digital classification. Key contribution in our approach is discriminative hierarchical sparse coding scheme which helps to learn useful discriminative high-level visual features. Based on the feature representation of cumulative foot pressure images, we develop a pedestrian recognition system which is invariant to three different shoes and slight local shape change. Experiments are conducted on a proposed open dataset that contains more than 2800 cumulative foot pressure images from 118 subjects. Evaluations suggest the effectiveness of the proposed method and the potential of cumulative foot pressure images as a biometric.\n    ",
        "submission_date": "2010-10-26T00:00:00",
        "last_modified_date": "2010-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.5943",
        "title": "Random Graph Generator for Bipartite Networks Modeling",
        "authors": [
            "Szymon Chojnacki",
            "Mieczys\u0142aw K\u0142opotek"
        ],
        "abstract": "The purpose of this article is to introduce a new iterative algorithm with properties resembling real life bipartite graphs. The algorithm enables us to generate wide range of random bigraphs, which features are determined by a set of ",
        "submission_date": "2010-10-28T00:00:00",
        "last_modified_date": "2010-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.5954",
        "title": "Random Graphs for Performance Evaluation of Recommender Systems",
        "authors": [
            "Szymon Chojnacki",
            "Mieczys\u0142aw K\u0142opotek"
        ],
        "abstract": "The purpose of this article is to introduce a new analytical framework dedicated to measuring performance of recommender systems. The standard approach is to assess the quality of a system by means of accuracy related statistics. However, the specificity of the environments in which recommender systems are deployed requires to pay much attention to speed and memory requirements of the algorithms. Unfortunately, it is implausible to assess accurately the complexity of various algorithms with formal tools. This can be attributed to the fact that such analyses are usually based on an assumption of dense representation of underlying data structures. Whereas, in real life the algorithms operate on sparse data and are implemented with collections dedicated for them. Therefore, we propose to measure the complexity of recommender systems with artificial datasets that posses real-life properties. We utilize recently developed bipartite graph generator to evaluate how state-of-the-art recommender systems' behavior is determined and diversified by topological properties of the generated datasets.\n    ",
        "submission_date": "2010-10-28T00:00:00",
        "last_modified_date": "2010-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.6234",
        "title": "Analysing the behaviour of robot teams through relational sequential pattern mining",
        "authors": [
            "Grazia Bombini",
            "Raquel Ros",
            "Stefano Ferilli",
            "Ramon Lopez de Mantaras"
        ],
        "abstract": "This report outlines the use of a relational representation in a Multi-Agent domain to model the behaviour of the whole system. A desired property in this systems is the ability of the team members to work together to achieve a common goal in a cooperative manner. The aim is to define a systematic method to verify the effective collaboration among the members of a team and comparing the different multi-agent behaviours. Using external observations of a Multi-Agent System to analyse, model, recognize agent behaviour could be very useful to direct team actions. In particular, this report focuses on the challenge of autonomous unsupervised sequential learning of the team's behaviour from observations. Our approach allows to learn a symbolic sequence (a relational representation) to translate raw multi-agent, multi-variate observations of a dynamic, complex environment, into a set of sequential behaviours that are characteristic of the team in question, represented by a set of sequences expressed in first-order logic atoms. We propose to use a relational learning algorithm to mine meaningful frequent patterns among the relational sequences to characterise team behaviours. We compared the performance of two teams in the RoboCup four-legged league environment, that have a very different approach to the game. One uses a Case Based Reasoning approach, the other uses a pure reactive behaviour.\n    ",
        "submission_date": "2010-10-29T00:00:00",
        "last_modified_date": "2010-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.0098",
        "title": "Qualitative Reasoning about Relative Direction on Adjustable Levels of Granularity",
        "authors": [
            "Till Mossakowski",
            "Reinhard Moratz"
        ],
        "abstract": "An important issue in Qualitative Spatial Reasoning is the representation of relative direction. In this paper we present simple geometric rules that enable reasoning about relative direction between oriented points. This framework, the Oriented Point Algebra OPRA_m, has a scalable granularity m. We develop a simple algorithm for computing the OPRA_m composition tables and prove its correctness. Using a composition table, algebraic closure for a set of OPRA statements is sufficient to solve spatial navigation tasks. And it turns out that scalable granularity is useful in these navigation tasks.\n    ",
        "submission_date": "2010-10-30T00:00:00",
        "last_modified_date": "2010-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.0187",
        "title": "A Distributed AI Aided 3D Domino Game",
        "authors": [
            "\u015eahin Emrah Amrahov",
            "Orhan A. Nooraden"
        ],
        "abstract": "In the article a turn-based game played on four computers connected via network is investigated. There are three computers with natural intelligence and one with artificial intelligence. Game table is seen by each player's own view point in all players' monitors. Domino pieces are three dimensional. For distributed systems TCP/IP protocol is used. In order to get 3D image, Microsoft XNA technology is applied. Domino 101 game is nondeterministic game that is result of the game depends on the initial random distribution of the pieces. Number of the distributions is equal to the multiplication of following combinations: . Moreover, in this game that is played by four people, players are divided into 2 pairs. Accordingly, we cannot predict how the player uses the dominoes that is according to the dominoes of his/her partner or according to his/her own dominoes. The fact that the natural intelligence can be a player in any level affects the outcome. These reasons make it difficult to develop an AI. In the article four levels of AI are developed. The AI in the first level is equivalent to the intelligence of a child who knows the rules of the game and recognizes the numbers. The AI in this level plays if it has any domino, suitable to play or says pass. In most of the games which can be played on the internet, the AI does the same. But the AI in the last level is a master player, and it can develop itself according to its competitors' levels.\n    ",
        "submission_date": "2010-10-31T00:00:00",
        "last_modified_date": "2010-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.0190",
        "title": "Prunnig Algorithm of Generation a Minimal Set of Rule Reducts Based on Rough Set Theory",
        "authors": [
            "\u015eahin Emrah Amrahov",
            "Fatih Aybar",
            "Serhat Do\u011fan"
        ],
        "abstract": "In this paper it is considered rule reduct generation problem, based on Rough Set Theory. Rule Reduct Generation (RG) and Modified Rule Generation (MRG) algorithms are well-known. Alternative to these algorithms Pruning Algorithm of Generation A Minimal Set of Rule Reducts, or briefly Pruning Rule Generation (PRG) algorithm is developed. PRG algorithm uses tree structured data type. PRG algorithm is compared with RG and MRG algorithms\n    ",
        "submission_date": "2010-10-31T00:00:00",
        "last_modified_date": "2010-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.0233",
        "title": "Reasoning about Cardinal Directions between Extended Objects: The Hardness Result",
        "authors": [
            "Weiming Liu",
            "Sanjiang Li"
        ],
        "abstract": "The cardinal direction calculus (CDC) proposed by Goyal and Egenhofer is a very expressive qualitative calculus for directional information of extended objects. Early work has shown that consistency checking of complete networks of basic CDC constraints is tractable while reasoning with the CDC in general is NP-hard. This paper shows, however, if allowing some constraints unspecified, then consistency checking of possibly incomplete networks of basic CDC constraints is already intractable. This draws a sharp boundary between the tractable and intractable subclasses of the CDC. The result is achieved by a reduction from the well-known 3-SAT problem.\n    ",
        "submission_date": "2010-11-01T00:00:00",
        "last_modified_date": "2010-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.0330",
        "title": "Imitation learning of motor primitives and language bootstrapping in robots",
        "authors": [
            "Thomas Cederborg",
            "Pierre-Yves Oudeyer"
        ],
        "abstract": "Imitation learning in robots, also called programing by demonstration, has made important advances in recent years, allowing humans to teach context dependant motor skills/tasks to robots. We propose to extend the usual contexts investigated to also include acoustic linguistic expressions that might denote a given motor skill, and thus we target joint learning of the motor skills and their potential acoustic linguistic name. In addition to this, a modification of a class of existing algorithms within the imitation learning framework is made so that they can handle the unlabeled demonstration of several tasks/motor primitives without having to inform the imitator of what task is being demonstrated or what the number of tasks are, which is a necessity for language learning, i.e; if one wants to teach naturally an open number of new motor skills together with their acoustic names. Finally, a mechanism for detecting whether or not linguistic input is relevant to the task is also proposed, and our architecture also allows the robot to find the right framing for a given identified motor primitive. With these additions it becomes possible to build an imitator that bridges the gap between imitation learning and language learning by being able to learn linguistic expressions using methods from the imitation learning community. In this sense the imitator can learn a word by guessing whether a certain speech pattern present in the context means that a specific task is to be executed. The imitator is however not assumed to know that speech is relevant and has to figure this out on its own by looking at the demonstrations: indeed, the architecture allows the robot to transparently also learn tasks which should not be triggered by an acoustic word, but for example by the color or position of an object or a gesture made by someone in the environment. To demonstrate this ability to find the ...\n    ",
        "submission_date": "2010-11-01T00:00:00",
        "last_modified_date": "2012-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.0628",
        "title": "Significance of Classification Techniques in Prediction of Learning Disabilities",
        "authors": [
            "Julie M. David And Kannan Balakrishnan"
        ],
        "abstract": "The aim of this study is to show the importance of two classification techniques, viz. decision tree and clustering, in prediction of learning disabilities (LD) of school-age children. LDs affect about 10 percent of all children enrolled in schools. The problems of children with specific learning disabilities have been a cause of concern to parents and teachers for some time. Decision trees and clustering are powerful and popular tools used for classification and prediction in Data mining. Different rules extracted from the decision tree are used for prediction of learning disabilities. Clustering is the assignment of a set of observations into subsets, called clusters, which are useful in finding the different signs and symptoms (attributes) present in the LD affected child. In this paper, J48 algorithm is used for constructing the decision tree and K-means algorithm is used for creating the clusters. By applying these classification techniques, LD in any child can be identified.\n    ",
        "submission_date": "2010-11-02T00:00:00",
        "last_modified_date": "2010-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.0935",
        "title": "Probabilistic Inferences in Bayesian Networks",
        "authors": [
            "Jianguo Ding"
        ],
        "abstract": "Bayesian network is a complete model for the variables and their relationships, it can be used to answer probabilistic queries about them. A Bayesian network can thus be considered a mechanism for automatically applying Bayes' theorem to complex problems. In the application of Bayesian networks, most of the work is related to probabilistic inferences. Any variable updating in any node of Bayesian networks might result in the evidence propagation across the Bayesian networks. This paper sums up various inference techniques in Bayesian networks and provide guidance for the algorithm calculation in probabilistic inference in Bayesian networks.\n    ",
        "submission_date": "2010-11-03T00:00:00",
        "last_modified_date": "2010-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.0950",
        "title": "Detecting Ontological Conflicts in Protocols between Semantic Web Services",
        "authors": [
            "Priyankar Ghosh",
            "Pallab Dasgupta"
        ],
        "abstract": "The task of verifying the compatibility between interacting web services has traditionally been limited to checking the compatibility of the interaction protocol in terms of message sequences and the type of data being exchanged. Since web services are developed largely in an uncoordinated way, different services often use independently developed ontologies for the same domain instead of adhering to a single ontology as standard. In this work we investigate the approaches that can be taken by the server to verify the possibility to reach a state with semantically inconsistent results during the execution of a protocol with a client, if the client ontology is published. Often database is used to store the actual data along with the ontologies instead of storing the actual data as a part of the ontology description. It is important to observe that at the current state of the database the semantic conflict state may not be reached even if the verification done by the server indicates the possibility of reaching a conflict state. A relational algebra based decision procedure is also developed to incorporate the current state of the client and the server databases in the overall verification procedure.\n    ",
        "submission_date": "2010-11-03T00:00:00",
        "last_modified_date": "2010-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.1478",
        "title": "Gradient Computation In Linear-Chain Conditional Random Fields Using The Entropy Message Passing Algorithm",
        "authors": [
            "Velimir M. Ilic",
            "Dejan I. Mancev",
            "Branimir T. Todorovic",
            "Miomir S. Stankovic"
        ],
        "abstract": "The paper proposes a numerically stable recursive algorithm for the exact computation of the linear-chain conditional random field gradient. It operates as a forward algorithm over the log-domain expectation semiring and has the purpose of enhancing memory efficiency when applied to long observation sequences. Unlike the traditional algorithm based on the forward-backward recursions, the memory complexity of our algorithm does not depend on the sequence length. The experiments on real data show that it can be useful for the problems which deal with long sequences.\n    ",
        "submission_date": "2010-11-05T00:00:00",
        "last_modified_date": "2012-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.1660",
        "title": "Reinforcement Learning Based on Active Learning Method",
        "authors": [
            "Hesam Sagha",
            "Saeed Bagheri Shouraki",
            "Hosein Khasteh",
            "Ali Akbar Kiaei"
        ],
        "abstract": "In this paper, a new reinforcement learning approach is proposed which is based on a powerful concept named Active Learning Method (ALM) in modeling. ALM expresses any multi-input-single-output system as a fuzzy combination of some single-input-singleoutput systems. The proposed method is an actor-critic system similar to Generalized Approximate Reasoning based Intelligent Control (GARIC) structure to adapt the ALM by delayed reinforcement signals. Our system uses Temporal Difference (TD) learning to model the behavior of useful actions of a control system. The goodness of an action is modeled on Reward- Penalty-Plane. IDS planes will be updated according to this plane. It is shown that the system can learn with a predefined fuzzy system or without it (through random actions).\n    ",
        "submission_date": "2010-11-07T00:00:00",
        "last_modified_date": "2010-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.1662",
        "title": "A New Sufficient Condition for 1-Coverage to Imply Connectivity",
        "authors": [
            "Seyed Hossein Khasteh",
            "Saeid Bagheri Shouraki",
            "Ali Akbar Kiaei"
        ],
        "abstract": "An effective approach for energy conservation in wireless sensor networks is scheduling sleep intervals for extraneous nodes while the remaining nodes stay active to provide continuous service. For the sensor network to operate successfully the active nodes must maintain both sensing coverage and network connectivity, It proved before if the communication range of nodes is at least twice the sensing range, complete coverage of a convex area implies connectivity among the working set of nodes. In this paper we consider a rectangular region A = a *b, such that R a R b s s \u00a3, \u00a3, where s R is the sensing range of nodes. and put a constraint on minimum allowed distance between nodes(s). according to this constraint we present a new lower bound for communication range relative to sensing range of sensors(s 2 + 3 *R) that complete coverage of considered area implies connectivity among the working set of nodes; also we present a new distribution method, that satisfy our constraint.\n    ",
        "submission_date": "2010-11-07T00:00:00",
        "last_modified_date": "2010-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.2304",
        "title": "Target tracking in the recommender space: Toward a new recommender system based on Kalman filtering",
        "authors": [
            "Samuel Nowakowski",
            "C\u00e9dric Bernier",
            "Anne Boyer"
        ],
        "abstract": "In this paper, we propose a new approach for recommender systems based on target tracking by Kalman filtering. We assume that users and their seen resources are vectors in the multidimensional space of the categories of the resources. Knowing this space, we propose an algorithm based on a Kalman filter to track users and to predict the best prediction of their future position in the recommendation space.\n    ",
        "submission_date": "2010-11-10T00:00:00",
        "last_modified_date": "2010-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.2512",
        "title": "Extended Active Learning Method",
        "authors": [
            "Ali Akbar Kiaei",
            "Saeed Bagheri Shouraki",
            "Seyed Hossein Khasteh",
            "Mahmoud Khademi",
            "Alireza Ghatreh Samani"
        ],
        "abstract": "Active Learning Method (ALM) is a soft computing method which is used for modeling and control, based on fuzzy logic. Although ALM has shown that it acts well in dynamic environments, its operators cannot support it very well in complex situations due to losing data. Thus ALM can find better membership functions if more appropriate operators be chosen for it. This paper substituted two new operators instead of ALM original ones; which consequently renewed finding membership functions in a way superior to conventional ALM. This new method is called Extended Active Learning Method (EALM).\n    ",
        "submission_date": "2010-11-10T00:00:00",
        "last_modified_date": "2011-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.3241",
        "title": "New Methods of Analysis of Narrative and Semantics in Support of Interactivity",
        "authors": [
            "Fionn Murtagh",
            "Adam Ganz",
            "Joe Reddington"
        ],
        "abstract": "Our work has focused on support for film or television scriptwriting. Since this involves potentially varied story-lines, we note the implicit or latent support for interactivity. Furthermore the film, television, games, publishing and other sectors are converging, so that cross-over and re-use of one form of product in another of these sectors is ever more common. Technically our work has been largely based on mathematical algorithms for data clustering and display. Operationally, we also discuss how our algorithms can support collective, distributed problem-solving.\n    ",
        "submission_date": "2010-11-14T00:00:00",
        "last_modified_date": "2010-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.3557",
        "title": "A Probabilistic Approach for Learning Folksonomies from Structured Data",
        "authors": [
            "Anon Plangprasopchok",
            "Kristina Lerman",
            "Lise Getoor"
        ],
        "abstract": "Learning structured representations has emerged as an important problem in many domains, including document and Web data mining, bioinformatics, and image analysis. One approach to learning complex structures is to integrate many smaller, incomplete and noisy structure fragments. In this work, we present an unsupervised probabilistic approach that extends affinity propagation to combine the small ontological fragments into a collection of integrated, consistent, and larger folksonomies. This is a challenging task because the method must aggregate similar structures while avoiding structural inconsistencies and handling noise. We validate the approach on a real-world social media dataset, comprised of shallow personal hierarchies specified by many individual users, collected from the photosharing website Flickr. Our empirical results show that our proposed approach is able to construct deeper and denser structures, compared to an approach using only the standard affinity propagation algorithm. Additionally, the approach yields better overall integration quality than a state-of-the-art approach based on incremental relational clustering.\n    ",
        "submission_date": "2010-11-16T00:00:00",
        "last_modified_date": "2010-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.3595",
        "title": "Optimizing real-time RDF data streams",
        "authors": [
            "Joshua Shinavier"
        ],
        "abstract": "The Resource Description Framework (RDF) provides a common data model for the integration of \"real-time\" social and sensor data streams with the Web and with each other. While there exist numerous protocols and data formats for exchanging dynamic RDF data, or RDF updates, these options should be examined carefully in order to enable a Semantic Web equivalent of the high-throughput, low-latency streams of typical Web 2.0, multimedia, and gaming applications. This paper contains a brief survey of RDF update formats and a high-level discussion of both TCP and UDP-based transport protocols for updates. Its main contribution is the experimental evaluation of a UDP-based architecture which serves as a real-world example of a high-performance RDF streaming application in an Internet-scale distributed environment.\n    ",
        "submission_date": "2010-11-16T00:00:00",
        "last_modified_date": "2010-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.4362",
        "title": "Should one compute the Temporal Difference fix point or minimize the Bellman Residual? The unified oblique projection view",
        "authors": [
            "Bruno Scherrer"
        ],
        "abstract": "We investigate projection methods, for evaluating a linear approximation of the value function of a policy in a Markov Decision Process context. We consider two popular approaches, the one-step Temporal Difference fix-point computation (TD(0)) and the Bellman Residual (BR) minimization. We describe examples, where each method outperforms the other. We highlight a simple relation between the objective function they minimize, and show that while BR enjoys a performance guarantee, TD(0) does not in general. We then propose a unified view in terms of oblique projections of the Bellman equation, which substantially simplifies and extends the characterization of (schoknecht,2002) and the recent analysis of (Yu & Bertsekas, 2008). Eventually, we describe some simulations that suggest that if the TD(0) solution is usually slightly better than the BR solution, its inherent numerical instability makes it very bad in some cases, and thus worse on average.\n    ",
        "submission_date": "2010-11-19T00:00:00",
        "last_modified_date": "2010-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.4632",
        "title": "Random Projections for $k$-means Clustering",
        "authors": [
            "Christos Boutsidis",
            "Anastasios Zouzias",
            "Petros Drineas"
        ],
        "abstract": "This paper discusses the topic of dimensionality reduction for $k$-means clustering. We prove that any set of $n$ points in $d$ dimensions (rows in a matrix $A \\in \\RR^{n \\times d}$) can be projected into $t = \\Omega(k / \\eps^2)$ dimensions, for any $\\eps \\in (0,1/3)$, in $O(n d \\lceil \\eps^{-2} k/ \\log(d) \\rceil )$ time, such that with constant probability the optimal $k$-partition of the point set is preserved within a factor of $2+\\eps$. The projection is done by post-multiplying $A$ with a $d \\times t$ random matrix $R$ having entries $+1/\\sqrt{t}$ or $-1/\\sqrt{t}$ with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.\n    ",
        "submission_date": "2010-11-21T00:00:00",
        "last_modified_date": "2010-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.5349",
        "title": "Distributed Graph Coloring: An Approach Based on the Calling Behavior of Japanese Tree Frogs",
        "authors": [
            "Hugo Hern\u00e1ndez",
            "Christian Blum"
        ],
        "abstract": "Graph coloring, also known as vertex coloring, considers the problem of assigning colors to the nodes of a graph such that adjacent nodes do not share the same color. The optimization version of the problem concerns the minimization of the number of used colors. In this paper we deal with the problem of finding valid colorings of graphs in a distributed way, that is, by means of an algorithm that only uses local information for deciding the color of the nodes. Such algorithms prescind from any central control. Due to the fact that quite a few practical applications require to find colorings in a distributed way, the interest in distributed algorithms for graph coloring has been growing during the last decade. As an example consider wireless ad-hoc and sensor networks, where tasks such as the assignment of frequencies or the assignment of TDMA slots are strongly related to graph coloring.\n",
        "submission_date": "2010-11-24T00:00:00",
        "last_modified_date": "2010-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.5480",
        "title": "Bayesian Modeling of a Human MMORPG Player",
        "authors": [
            "Gabriel Synnaeve",
            "Pierre Bessiere"
        ],
        "abstract": "This paper describes an application of Bayesian programming to the control of an autonomous avatar in a multiplayer role-playing game (the example is based on World of Warcraft). We model a particular task, which consists of choosing what to do and to select which target in a situation where allies and foes are present. We explain the model in Bayesian programming and show how we could learn the conditional probabilities from data gathered during human-played sessions.\n    ",
        "submission_date": "2010-11-24T00:00:00",
        "last_modified_date": "2010-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.5951",
        "title": "Reinforcement Learning in Partially Observable Markov Decision Processes using Hybrid Probabilistic Logic Programs",
        "authors": [
            "Emad Saad"
        ],
        "abstract": "We present a probabilistic logic programming framework to reinforcement learning, by integrating reinforce-ment learning, in POMDP environments, with normal hybrid probabilistic logic programs with probabilistic answer set seman-tics, that is capable of representing domain-specific knowledge. We formally prove the correctness of our approach. We show that the complexity of finding a policy for a reinforcement learning problem in our approach is NP-complete. In addition, we show that any reinforcement learning problem can be encoded as a classical logic program with answer set semantics. We also show that a reinforcement learning problem can be encoded as a SAT problem. We present a new high level action description language that allows the factored representation of POMDP. Moreover, we modify the original model of POMDP so that it be able to distinguish between knowledge producing actions and actions that change the environment.\n    ",
        "submission_date": "2010-11-27T00:00:00",
        "last_modified_date": "2010-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.6220",
        "title": "Multimodal Biometric Systems - Study to Improve Accuracy and Performance",
        "authors": [
            "K.Sasidhar",
            "Vijaya L Kakulapati",
            "Kolikipogu Ramakrishna",
            "K.KailasaRao"
        ],
        "abstract": "Biometrics is the science and technology of measuring and analyzing biological data of human body, extracting a feature set from the acquired data, and comparing this set against to the template set in the database. Experimental studies show that Unimodal biometric systems had many disadvantages regarding performance and accuracy. Multimodal biometric systems perform better than unimodal biometric systems and are popular even more complex also. We examine the accuracy and performance of multimodal biometric authentication systems using state of the art Commercial Off- The-Shelf (COTS) products. Here we discuss fingerprint and face biometric systems, decision and fusion techniques used in these systems. We also discuss their advantage over unimodal biometric systems.\n    ",
        "submission_date": "2010-11-29T00:00:00",
        "last_modified_date": "2010-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.0084",
        "title": "Survey on Various Gesture Recognition Techniques for Interfacing Machines Based on Ambient Intelligence",
        "authors": [
            "Harshith C",
            "Karthik R. Shastry",
            "Manoj Ravindran",
            "M.V.V.N.S. Srikanth",
            "Naveen Lakshmikhanth"
        ],
        "abstract": "Gesture recognition is mainly apprehensive on analyzing the functionality of human wits. The main goal of gesture recognition is to create a system which can recognize specific human gestures and use them to convey information or for device control. Hand gestures provide a separate complementary modality to speech for expressing ones ideas. Information associated with hand gestures in a conversation is degree,discourse structure, spatial and temporal structure. The approaches present can be mainly divided into Data-Glove Based and Vision Based approaches. An important face feature point is the nose tip. Since nose is the highest protruding point from the face. Besides that, it is not affected by facial ",
        "submission_date": "2010-12-01T00:00:00",
        "last_modified_date": "2010-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.0322",
        "title": "A Bayesian Methodology for Estimating Uncertainty of Decisions in Safety-Critical Systems",
        "authors": [
            "Vitaly Schetinin",
            "Jonathan Fieldsend",
            "Derek Partridge",
            "Wojtek Krzanowski",
            "Richard Everson",
            "Trevor Bailey",
            "Adolfo Hernandez"
        ],
        "abstract": "Uncertainty of decisions in safety-critical engineering applications can be estimated on the basis of the Bayesian Markov Chain Monte Carlo (MCMC) technique of averaging over decision models. The use of decision tree (DT) models assists experts to interpret causal relations and find factors of the uncertainty. Bayesian averaging also allows experts to estimate the uncertainty accurately when a priori information on the favored structure of DTs is available. Then an expert can select a single DT model, typically the Maximum a Posteriori model, for interpretation purposes. Unfortunately, a priori information on favored structure of DTs is not always available. For this reason, we suggest a new prior on DTs for the Bayesian MCMC technique. We also suggest a new procedure of selecting a single DT and describe an application scenario. In our experiments on the Short-Term Conflict Alert data our technique outperforms the existing Bayesian techniques in predictive accuracy of the selected single DTs.\n    ",
        "submission_date": "2010-12-01T00:00:00",
        "last_modified_date": "2010-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.0742",
        "title": "Border Algorithms for Computing Hasse Diagrams of Arbitrary Lattices",
        "authors": [
            "Jos\u00e9 L. Balc\u00e1zar",
            "Cristina T\u00eern\u0103uc\u0103"
        ],
        "abstract": "The Border algorithm and the iPred algorithm find the Hasse diagrams of FCA lattices. We show that they can be generalized to arbitrary lattices. In the case of iPred, this requires the identification of a join-semilattice homomorphism into a distributive lattice.\n    ",
        "submission_date": "2010-12-03T00:00:00",
        "last_modified_date": "2010-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.0830",
        "title": "Using ASP with recent extensions for causal explanations",
        "authors": [
            "Yves Moinard"
        ],
        "abstract": "We examine the practicality for a user of using Answer Set Programming (ASP) for representing logical formalisms. We choose as an example a formalism aiming at capturing causal explanations from causal information. We provide an implementation, showing the naturalness and relative efficiency of this translation job. We are interested in the ease for writing an ASP program, in accordance with the claimed ``declarative'' aspect of ASP. Limitations of the earlier systems (poor data structure and difficulty in reusing pieces of programs) made that in practice, the ``declarative aspect'' was more theoretical than practical. We show how recent improvements in working ASP systems facilitate a lot the translation, even if a few improvements could still be useful.\n    ",
        "submission_date": "2010-12-03T00:00:00",
        "last_modified_date": "2010-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.0841",
        "title": "Automated Query Learning with Wikipedia and Genetic Programming",
        "authors": [
            "Pekka Malo",
            "Pyry Siitari",
            "Ankur Sinha"
        ],
        "abstract": "Most of the existing information retrieval systems are based on bag of words model and are not equipped with common world knowledge. Work has been done towards improving the efficiency of such systems by using intelligent algorithms to generate search queries, however, not much research has been done in the direction of incorporating human-and-society level knowledge in the queries. This paper is one of the first attempts where such information is incorporated into the search queries using Wikipedia semantics. The paper presents an essential shift from conventional token based queries to concept based queries, leading to an enhanced efficiency of information retrieval systems. To efficiently handle the automated query learning problem, we propose Wikipedia-based Evolutionary Semantics (Wiki-ES) framework where concept based queries are learnt using a co-evolving evolutionary procedure. Learning concept based queries using an intelligent evolutionary procedure yields significant improvement in performance which is shown through an extensive study using Reuters newswire documents. Comparison of the proposed framework is performed with other information retrieval systems. Concept based approach has also been implemented on other information retrieval systems to justify the effectiveness of a transition from token based queries to concept based queries.\n    ",
        "submission_date": "2010-12-03T00:00:00",
        "last_modified_date": "2010-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.1255",
        "title": "URSA: A System for Uniform Reduction to SAT",
        "authors": [
            "Predrag Janicic"
        ],
        "abstract": " There are a huge number of problems, from various areas, being solved by reducing them to SAT. However, for many applications, translation into SAT is performed by specialized, problem-specific tools. In this paper we describe a new system for uniform solving of a wide class of problems by reducing them to SAT. The system uses a new specification language URSA that combines imperative and declarative programming paradigms. The reduction to SAT is defined precisely by the semantics of the specification language. The domain of the approach is wide (e.g., many NP-complete problems can be simply specified and then solved by the system) and there are problems easily solvable by the proposed system, while they can be hardly solved by using other programming languages or constraint programming systems. So, the system can be seen not only as a tool for solving problems by reducing them to SAT, but also as a general-purpose constraint solving system (for finite domains). In this paper, we also describe an open-source implementation of the described approach. The performed experiments suggest that the system is competitive to state-of-the-art related modelling systems. \n    ",
        "submission_date": "2010-12-06T00:00:00",
        "last_modified_date": "2012-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.1552",
        "title": "Bridging the Gap between Reinforcement Learning and Knowledge Representation: A Logical Off- and On-Policy Framework",
        "authors": [
            "Emad Saad"
        ],
        "abstract": "Knowledge Representation is important issue in reinforcement learning. In this paper, we bridge the gap between reinforcement learning and knowledge representation, by providing a rich knowledge representation framework, based on normal logic programs with answer set semantics, that is capable of solving model-free reinforcement learning problems for more complex do-mains and exploits the domain-specific knowledge. We prove the correctness of our approach. We show that the complexity of finding an offline and online policy for a model-free reinforcement learning problem in our approach is NP-complete. Moreover, we show that any model-free reinforcement learning problem in MDP environment can be encoded as a SAT problem. The importance of that is model-free reinforcement\n    ",
        "submission_date": "2010-12-07T00:00:00",
        "last_modified_date": "2010-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.1619",
        "title": "Are SNOMED CT Browsers Ready for Institutions? Introducing MySNOM",
        "authors": [
            "Pablo Lopez-Garcia"
        ],
        "abstract": "SNOMED Clinical Terms (SNOMED CT) is one of the most widespread ontologies in the life sciences, with more than 300,000 concepts and relationships, but is distributed with no associated software tools. In this paper we present MySNOM, a web-based SNOMED CT browser. MySNOM allows organizations to browse their own distribution of SNOMED CT under a controlled environment, focuses on navigating using the structure of SNOMED CT, and has diagramming capabilities.\n    ",
        "submission_date": "2010-12-07T00:00:00",
        "last_modified_date": "2010-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.1635",
        "title": "A study on the relation between linguistics-oriented and domain-specific semantics",
        "authors": [
            "He Tan"
        ],
        "abstract": "In this paper we dealt with the comparison and linking between lexical resources with domain knowledge provided by ontologies. It is one of the issues for the combination of the Semantic Web Ontologies and Text Mining. We investigated the relations between the linguistics oriented and domain-specific semantics, by associating the GO biological process concepts to the FrameNet semantic frames. The result shows the gaps between the linguistics-oriented and domain-specific semantics on the classification of events and the grouping of target words. The result provides valuable information for the improvement of domain ontologies supporting for text mining systems. And also, it will result in benefits to language understanding technology.\n    ",
        "submission_date": "2010-12-07T00:00:00",
        "last_modified_date": "2010-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.1643",
        "title": "Process Makna - A Semantic Wiki for Scientific Workflows",
        "authors": [
            "Adrian Paschke",
            "Zhili Zhao"
        ],
        "abstract": "Virtual e-Science infrastructures supporting Web-based scientific workflows are an example for knowledge-intensive collaborative and weakly-structured processes where the interaction with the human scientists during process execution plays a central role. In this paper we propose the lightweight dynamic user-friendly interaction with humans during execution of scientific workflows via the low-barrier approach of Semantic Wikis as an intuitive interface for non-technical scientists. Our Process Makna Semantic Wiki system is a novel combination of an business process management system adapted for scientific workflows with a Corporate Semantic Web Wiki user interface supporting knowledge intensive human interaction tasks during scientific workflow execution.\n    ",
        "submission_date": "2010-12-07T00:00:00",
        "last_modified_date": "2010-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.1646",
        "title": "Use of semantic technologies for the development of a dynamic trajectories generator in a Semantic Chemistry eLearning platform",
        "authors": [
            "Richard Huber",
            "Kirsten Hantelmann",
            "Alexandru Todor",
            "Sebastian Krebs",
            "Ralf Heese",
            "Adrian Paschke"
        ],
        "abstract": "ChemgaPedia is a multimedia, webbased eLearning service platform that currently contains about 18.000 pages organized in 1.700 chapters covering the complete bachelor studies in chemistry and related topics of chemistry, pharmacy, and life sciences. The eLearning encyclopedia contains some 25.000 media objects and the eLearning platform provides services such as virtual and remote labs for experiments. With up to 350.000 users per month the platform is the most frequently used scientific educational service in the German spoken Internet. In this demo we show the benefit of mapping the static eLearning contents of ChemgaPedia to a Linked Data representation for Semantic Chemistry which allows for generating dynamic eLearning paths tailored to the semantic profiles of the users.\n    ",
        "submission_date": "2010-12-07T00:00:00",
        "last_modified_date": "2010-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.1648",
        "title": "Analysis Of Cancer Omics Data In A Semantic Web Framework",
        "authors": [
            "Matt Holford",
            "James McCusker",
            "Kei Cheung",
            "Michael Krauthammer"
        ],
        "abstract": "Our work concerns the elucidation of the cancer (epi)genome, transcriptome and proteome to better understand the complex interplay between a cancer cell's molecular state and its response to anti-cancer therapy. To study the problem, we have previously focused on data warehousing technologies and statistical data integration. In this paper, we present recent work on extending our analytical capabilities using Semantic Web technology. A key new component presented here is a SPARQL endpoint to our existing data warehouse. This endpoint allows the merging of observed quantitative data with existing data from semantic knowledge sources such as Gene Ontology (GO). We show how such variegated quantitative and functional data can be integrated and accessed in a universal manner using Semantic Web tools. We also demonstrate how Description Logic (DL) reasoning can be used to infer previously unstated conclusions from existing knowledge bases. As proof of concept, we illustrate the ability of our setup to answer complex queries on resistance of cancer cells to Decitabine, a demethylating agent.\n    ",
        "submission_date": "2010-12-08T00:00:00",
        "last_modified_date": "2010-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.1654",
        "title": "Using Semantic Wikis for Structured Argument in Medical Domain",
        "authors": [
            "Adrian Groza",
            "Radu Balaj"
        ],
        "abstract": "This research applies ideas from argumentation theory in the context of semantic wikis, aiming to provide support for structured-large scale argumentation between human agents. The implemented prototype is exemplified by modelling the MMR vaccine controversy.\n    ",
        "submission_date": "2010-12-08T00:00:00",
        "last_modified_date": "2010-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.1658",
        "title": "Creating a new Ontology: a Modular Approach",
        "authors": [
            "Julia Dmitrieva",
            "Fons J. Verbeek"
        ],
        "abstract": "Creating a new Ontology: a Modular Approach\n    ",
        "submission_date": "2010-12-08T00:00:00",
        "last_modified_date": "2010-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.1659",
        "title": "First steps in the logic-based assessment of post-composed phenotypic descriptions",
        "authors": [
            "Ernesto Jimenez-Ruiz",
            "Bernardo Cuenca Grau",
            "Rafael Berlanga",
            "Dietrich Rebholz-Schuhmann"
        ],
        "abstract": "In this paper we present a preliminary logic-based evaluation of the integration of post-composed phenotypic descriptions with domain ontologies. The evaluation has been performed using a description logic reasoner together with scalable techniques: ontology modularization and approximations of the logical difference between ontologies.\n    ",
        "submission_date": "2010-12-08T00:00:00",
        "last_modified_date": "2010-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.1661",
        "title": "Analysis and visualisation of RDF resources in Ondex",
        "authors": [
            "Catherine Canevet",
            "Artem Lysenko",
            "Andrea Splendiani",
            "Matthew Pocock",
            "Christopher Rawlings"
        ],
        "abstract": "Ondex is a data integration and visualization platform developed to support Systems Biology Research. At its core is a data model based on two main principles: first, all information can be represented as a graph and, second, all elements of the graph can be annotated with ontologies. This data model is conformant to the Semantic Web framework, in particular to RDF, and therefore Ondex is ideally positioned as a platform that can exploit the semantic web.\n    ",
        "submission_date": "2010-12-08T00:00:00",
        "last_modified_date": "2010-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.1667",
        "title": "A semantic approach for the requirement-driven discovery of web services in the Life Sciences",
        "authors": [
            "Maria Perez",
            "Rafael Berlanga",
            "Ismael Sanz"
        ],
        "abstract": "Research in the Life Sciences depends on the integration of large, distributed and heterogeneous data sources and web services. The discovery of which of these resources are the most appropriate to solve a given task is a complex research question, since there is a large amount of plausible candidates and there is little, mostly unstructured, metadata to be able to decide among ",
        "submission_date": "2010-12-08T00:00:00",
        "last_modified_date": "2010-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.1743",
        "title": "Scientific Collaborations: principles of WikiBridge Design",
        "authors": [
            "Eric Leclercq",
            "Marinette Savonnet"
        ],
        "abstract": "Semantic wikis, wikis enhanced with Semantic Web technologies, are appropriate systems for community-authored knowledge models. They are particularly suitable for scientific collaboration. This paper details the design principles ofWikiBridge, a semantic wiki.\n    ",
        "submission_date": "2010-12-08T00:00:00",
        "last_modified_date": "2010-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.1745",
        "title": "Populous: A tool for populating ontology templates",
        "authors": [
            "Simon Jupp",
            "Matthew Horridge",
            "Luigi Iannone",
            "Julie Klein",
            "Stuart Owen",
            "Joost Schanstra",
            "Robert Stevens",
            "Katy Wolstencroft"
        ],
        "abstract": "We present Populous, a tool for gathering content with which to populate an ontology. Domain experts need to add content, that is often repetitive in its form, but without having to tackle the underlying ontological representation. Populous presents users with a table based form in which columns are constrained to take values from particular ontologies; the user can select a concept from an ontology via its meaningful label to give a value for a given entity attribute. Populated tables are mapped to patterns that can then be used to automatically generate the ontology's content. Populous's contribution is in the knowledge gathering stage of ontology development. It separates knowledge gathering from the conceptualisation and also separates the user from the standard ontology authoring environments. As a result, Populous can allow knowledge to be gathered in a straight-forward manner that can then be used to do mass production of ontology content.\n    ",
        "submission_date": "2010-12-08T00:00:00",
        "last_modified_date": "2010-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.1899",
        "title": "Querying Biomedical Ontologies in Natural Language using Answer Set",
        "authors": [
            "Halit Erdogan",
            "Umut Oztok",
            "Yelda Erdem",
            "Esra Erdem"
        ],
        "abstract": "In this work, we develop an intelligent user interface that allows users to enter biomedical queries in a natural language, and that presents the answers (possibly with explanations if requested) in a natural language. We develop a rule layer over biomedical ontologies and databases, and use automated reasoners to answer queries considering relevant parts of the rule layer.\n    ",
        "submission_date": "2010-12-09T00:00:00",
        "last_modified_date": "2010-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.2148",
        "title": "Bisimulations for fuzzy transition systems",
        "authors": [
            "Yongzhi Cao",
            "Guoqing Chen",
            "Etienne Kerre"
        ],
        "abstract": "There has been a long history of using fuzzy language equivalence to compare the behavior of fuzzy systems, but the comparison at this level is too coarse. Recently, a finer behavioral measure, bisimulation, has been introduced to fuzzy finite automata. However, the results obtained are applicable only to finite-state systems. In this paper, we consider bisimulation for general fuzzy systems which may be infinite-state or infinite-event, by modeling them as fuzzy transition systems. To help understand and check bisimulation, we characterize it in three ways by enumerating whole transitions, comparing individual transitions, and using a monotonic function. In addition, we address composition operations, subsystems, quotients, and homomorphisms of fuzzy transition systems and discuss their properties connected with bisimulation. The results presented here are useful for comparing the behavior of general fuzzy systems. In particular, this makes it possible to relate an infinite fuzzy system to a finite one, which is easier to analyze, with the same behavior.\n    ",
        "submission_date": "2010-12-10T00:00:00",
        "last_modified_date": "2010-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.2162",
        "title": "Nondeterministic fuzzy automata",
        "authors": [
            "Yongzhi Cao",
            "Yoshinori Ezawa"
        ],
        "abstract": "Fuzzy automata have long been accepted as a generalization of nondeterministic finite automata. A closer examination, however, shows that the fundamental property---nondeterminism---in nondeterministic finite automata has not been well embodied in the generalization. In this paper, we introduce nondeterministic fuzzy automata with or without $\\el$-moves and fuzzy languages recognized by them. Furthermore, we prove that (deterministic) fuzzy automata, nondeterministic fuzzy automata, and nondeterministic fuzzy automata with $\\el$-moves are all equivalent in the sense that they recognize the same class of fuzzy languages.\n    ",
        "submission_date": "2010-12-10T00:00:00",
        "last_modified_date": "2010-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.2713",
        "title": "Phase Transitions of Plan Modification in Conformant Planning",
        "authors": [
            "Junping Zhou",
            "Minghao Yin"
        ],
        "abstract": "We explore phase transitions of plan modification, which mainly focus on the conformant planning problems. By analyzing features of plan modification in conformant planning problems, quantitative results are obtained. If the number of operators is less than, almost all conformant planning problems can't be solved with plan modification. If the number of operators is more than, almost all conformant planning problems can be solved with plan modification. The results of the experiments also show that there exists an experimental threshold of density (ratio of number of operators to number of propositions), which separates the region where almost all conformant planning problems can't be solved with plan modification from the region where almost all conformant planning problems can be solved with plan modification.\n    ",
        "submission_date": "2010-12-13T00:00:00",
        "last_modified_date": "2010-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.2789",
        "title": "Experimental Comparison of Representation Methods and Distance Measures for Time Series Data",
        "authors": [
            "Xiaoyue Wang",
            "Hui Ding",
            "Goce Trajcevski",
            "Peter Scheuermann",
            "Eamonn Keogh"
        ],
        "abstract": "The previous decade has brought a remarkable increase of the interest in applications that deal with querying and mining of time series data. Many of the research efforts in this context have focused on introducing new representation methods for dimensionality reduction or novel similarity measures for the underlying data. In the vast majority of cases, each individual work introducing a particular method has made specific claims and, aside from the occasional theoretical justifications, provided quantitative experimental observations. However, for the most part, the comparative aspects of these experiments were too narrowly focused on demonstrating the benefits of the proposed methods over some of the previously introduced ones. In order to provide a comprehensive validation, we conducted an extensive experimental study re-implementing eight different time series representations and nine similarity measures and their variants, and testing their effectiveness on thirty-eight time series data sets from a wide variety of application domains. In this paper, we give an overview of these different techniques and present our comparative experimental findings regarding their effectiveness. In addition to providing a unified validation of some of the existing achievements, our experiments also indicate that, in some cases, certain claims in the literature may be unduly optimistic.\n    ",
        "submission_date": "2010-12-09T00:00:00",
        "last_modified_date": "2010-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.3018",
        "title": "On the size of data structures used in symbolic model checking",
        "authors": [
            "Paolo Liberatore",
            "Marco Schaerf"
        ],
        "abstract": "Temporal Logic Model Checking is a verification method in which we describe a system, the model, and then we verify whether some properties, expressed in a temporal logic formula, hold in the system. It has many industrial applications. In order to improve performance, some tools allow preprocessing of the model, verifying on-line a set of properties reusing the same compiled model; we prove that the complexity of the Model Checking problem, without any preprocessing or preprocessing the model or the formula in a polynomial data structure, is the same. As a result preprocessing does not always exponentially improve performance.\n",
        "submission_date": "2010-12-14T00:00:00",
        "last_modified_date": "2010-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.3148",
        "title": "To study the phenomenon of the Moravec's Paradox",
        "authors": [
            "Kush Agrawal"
        ],
        "abstract": "\"Encoded in the large, highly evolved sensory and motor portions of the human brain is a billion years of experience about the nature of the world and how to survive in it. The deliberate process we call reasoning is, I believe, the thinnest veneer of human thought, effective only because it is supported by this much older and much powerful, though usually unconscious, sensor motor knowledge. We are all prodigious Olympians in perceptual and motor areas, so good that we make the difficult look easy. Abstract thought, though, is a new trick, perhaps less than 100 thousand years old. We have not yet mastered it. It is not all that intrinsically difficult; it just seems so when we do it.\"- Hans Moravec Moravec's paradox is involved with the fact that it is the seemingly easier day to day problems that are harder to implement in a machine, than the seemingly complicated logic based problems of today. The results prove that most artificially intelligent machines are as adept if not more than us at under-taking long calculations or even play chess, but their logic brings them nowhere when it comes to carrying out everyday tasks like walking, facial gesture recognition or speech recognition.\n    ",
        "submission_date": "2010-12-14T00:00:00",
        "last_modified_date": "2010-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.3280",
        "title": "A new Recommender system based on target tracking: a Kalman Filter approach",
        "authors": [
            "Samuel Nowakowski",
            "C\u00e9dric Bernier",
            "Anne Boyer"
        ],
        "abstract": "In this paper, we propose a new approach for recommender systems based on target tracking by Kalman filtering. We assume that users and their seen resources are vectors in the multidimensional space of the categories of the resources. Knowing this space, we propose an algorithm based on a Kalman filter to track users and to predict the best prediction of their future position in the recommendation space.\n    ",
        "submission_date": "2010-12-15T00:00:00",
        "last_modified_date": "2010-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.3312",
        "title": "Dynamic Capitalization and Visualization Strategy in Collaborative Knowledge Management System for EI Process",
        "authors": [
            "Bolanle Oladejo",
            "Victor Odumuyiwa",
            "Amos David"
        ],
        "abstract": "Knowledge is attributed to human whose problem-solving behavior is subjective and complex. In today's knowledge economy, the need to manage knowledge produced by a community of actors cannot be overemphasized. This is due to the fact that actors possess some level of tacit knowledge which is generally difficult to articulate. Problem-solving requires searching and sharing of knowledge among a group of actors in a particular context. Knowledge expressed within the context of a problem resolution must be capitalized for future reuse. In this paper, an approach that permits dynamic capitalization of relevant and reliable actors' knowledge in solving decision problem following Economic Intelligence process is proposed. Knowledge annotation method and temporal attributes are used for handling the complexity in the communication among actors and in contextualizing expressed knowledge. A prototype is built to demonstrate the functionalities of a collaborative Knowledge Management system based on this approach. It is tested with sample cases and the result showed that dynamic capitalization leads to knowledge validation hence increasing reliability of captured knowledge for reuse. The system can be adapted to various domains\n    ",
        "submission_date": "2010-12-15T00:00:00",
        "last_modified_date": "2010-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.3336",
        "title": "Dynamic Knowledge Capitalization through Annotation among Economic Intelligence Actors in a Collaborative Environment",
        "authors": [
            "Olusoji Okunoye",
            "Bolanle Oladejo",
            "Victor Odumuyiwa"
        ],
        "abstract": "The shift from industrial economy to knowledge economy in today's world has revolutionalized strategic planning in organizations as well as their problem solving approaches. The point of focus today is knowledge and service production with more emphasis been laid on knowledge capital. Many organizations are investing on tools that facilitate knowledge sharing among their employees and they are as well promoting and encouraging collaboration among their staff in order to build the organization's knowledge capital with the ultimate goal of creating a lasting competitive advantage for their organizations. One of the current leading approaches used for solving organization's decision problem is the Economic Intelligence (EI) approach which involves interactions among various actors called EI actors. These actors collaborate to ensure the overall success of the decision problem solving process. In the course of the collaboration, the actors express knowledge which could be capitalized for future reuse. In this paper, we propose in the first place, an annotation model for knowledge elicitation among EI actors. Because of the need to build a knowledge capital, we also propose a dynamic knowledge capitalisation approach for managing knowledge produced by the actors. Finally, the need to manage the interactions and the interdependencies among collaborating EI actors, led to our third proposition which constitute an awareness mechanism for group work management.\n    ",
        "submission_date": "2010-12-15T00:00:00",
        "last_modified_date": "2010-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.3410",
        "title": "Descriptive-complexity based distance for fuzzy sets",
        "authors": [
            "Laszlo Kovacs",
            "Joel Ratsaby"
        ],
        "abstract": "A new distance function dist(A,B) for fuzzy sets A and B is introduced. It is based on the descriptive complexity, i.e., the number of bits (on average) that are needed to describe an element in the symmetric difference of the two sets. The distance gives the amount of additional information needed to describe any one of the two sets given the other. We prove its mathematical properties and perform pattern clustering on data based on this distance.\n    ",
        "submission_date": "2010-12-15T00:00:00",
        "last_modified_date": "2010-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.3853",
        "title": "On the CNF encoding of cardinality constraints and beyond",
        "authors": [
            "Olivier Bailleux"
        ],
        "abstract": "In this report, we propose a quick survey of the currently known techniques for encoding a Boolean cardinality constraint into a CNF formula, and we discuss about the relevance of these encodings. We also propose models to facilitate analysis and design of CNF encodings for Boolean constraints.\n    ",
        "submission_date": "2010-12-17T00:00:00",
        "last_modified_date": "2010-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.4046",
        "title": "Artificial Intelligence in Reverse Supply Chain Management: The State of the Art",
        "authors": [
            "Bo Xing",
            "Wen-Jing Gao",
            "Kimberly Battle",
            "Tshildzi Marwala",
            "Fulufhelo V. Nelwamondo"
        ],
        "abstract": "Product take-back legislation forces manufacturers to bear the costs of collection and disposal of products that have reached the end of their useful lives. In order to reduce these costs, manufacturers can consider reuse, remanufacturing and/or recycling of components as an alternative to disposal. The implementation of such alternatives usually requires an appropriate reverse supply chain management. With the concepts of reverse supply chain are gaining popularity in practice, the use of artificial intelligence approaches in these areas is also becoming popular. As a result, the purpose of this paper is to give an overview of the recent publications concerning the application of artificial intelligence techniques to reverse supply chain with emphasis on certain types of product returns.\n    ",
        "submission_date": "2010-12-18T00:00:00",
        "last_modified_date": "2010-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.4776",
        "title": "Automatic Estimation of the Exposure to Lateral Collision in Signalized Intersections using Video Sensors",
        "authors": [
            "Nicolas Saunier",
            "Sophie Midenet"
        ],
        "abstract": "Intersections constitute one of the most dangerous elements in road systems. Traffic signals remain the most common way to control traffic at high-volume intersections and offer many opportunities to apply intelligent transportation systems to make traffic more efficient and safe. This paper describes an automated method to estimate the temporal exposure of road users crossing the conflict zone to lateral collision with road users originating from a different approach. This component is part of a larger system relying on video sensors to provide queue lengths and spatial occupancy that are used for real time traffic control and monitoring. The method is evaluated on data collected during a real world experiment.\n    ",
        "submission_date": "2010-12-21T00:00:00",
        "last_modified_date": "2010-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.4824",
        "title": "Input Parameters Optimization in Swarm DS-CDMA Multiuser Detectors",
        "authors": [
            "Taufik Abr\u00e3o",
            "Leonardo D. Oliveira",
            "Bruno A. Angelico",
            "Paul Jean E. Jeszensky"
        ],
        "abstract": "In this paper, the uplink direct sequence code division multiple access (DS-CDMA) multiuser detection problem (MuD) is studied into heuristic perspective, named particle swarm optimization (PSO). Regarding different system improvements for future technologies, such as high-order modulation and diversity exploitation, a complete parameter optimization procedure for the PSO applied to MuD problem is provided, which represents the major contribution of this paper. Furthermore, the performance of the PSO-MuD is briefly analyzed via Monte-Carlo simulations. Simulation results show that, after convergence, the performance reached by the PSO-MuD is much better than the conventional detector, and somewhat close to the single user bound (SuB). Rayleigh flat channel is initially considered, but the results are further extend to diversity (time and spatial) channels.\n    ",
        "submission_date": "2010-12-21T00:00:00",
        "last_modified_date": "2010-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.5506",
        "title": "Ontology-based Queries over Cancer Data",
        "authors": [
            "Alejandra Gonzalez-Beltran",
            "Ben Tagger",
            "Anthony Finkelstein"
        ],
        "abstract": "The ever-increasing amount of data in biomedical research, and in cancer research in particular, needs to be managed to support efficient data access, exchange and integration. Existing software infrastructures, such caGrid, support access to distributed information annotated with a domain ontology. However, caGrid's current querying functionality depends on the structure of individual data resources without exploiting the semantic annotations. In this paper, we present the design and development of an ontology-based querying functionality that consists of: the generation of OWL2 ontologies from the underlying data resources metadata and a query rewriting and translation process based on reasoning, which converts a query at the domain ontology level into queries at the software infrastructure level. We present a detailed analysis of our approach as well as an extensive performance evaluation. While the implementation and evaluation was performed for the caGrid infrastructure, the approach could be applicable to other model and metadata-driven environments for data sharing.\n    ",
        "submission_date": "2010-12-26T00:00:00",
        "last_modified_date": "2010-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.5585",
        "title": "Symmetry Breaking with Polynomial Delay",
        "authors": [
            "Tim januschowski",
            "Barbara M. Smith",
            "M. R. C. van Dongen"
        ],
        "abstract": "A conservative class of constraint satisfaction problems CSPs is a class for which membership is preserved under arbitrary domain reductions. Many well-known tractable classes of CSPs are conservative. It is well known that lexleader constraints may significantly reduce the number of solutions by excluding symmetric solutions of CSPs. We show that adding certain lexleader constraints to any instance of any conservative class of CSPs still allows us to find all solutions with a time which is polynomial between successive solutions. The time is polynomial in the total size of the instance and the additional lexleader constraints. It is well known that for complete symmetry breaking one may need an exponential number of lexleader constraints. However, in practice, the number of additional lexleader constraints is typically polynomial number in the size of the instance. For polynomially many lexleader constraints, we may in general not have complete symmetry breaking but polynomially many lexleader constraints may provide practically useful symmetry breaking -- and they sometimes exclude super-exponentially many solutions. We prove that for any instance from a conservative class, the time between finding successive solutions of the instance with polynomially many additional lexleader constraints is polynomial even in the size of the instance without lexleaderconstraints.\n    ",
        "submission_date": "2010-12-27T00:00:00",
        "last_modified_date": "2010-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.5594",
        "title": "The Ethics of Robotics",
        "authors": [
            "Kush Agrawal"
        ],
        "abstract": "The three laws of Robotics first appeared together in Isaac Asimov's story 'Runaround' after being mentioned in some form or the other in previous works by Asimov. These three laws commonly known as the three laws of robotics are the earliest forms of depiction for the needs of ethics in Robotics. In simplistic language Isaac Asimov is able to explain what rules a robot must confine itself to in order to maintain societal sanctity. However, even though they are outdated they still represent some of our innate fears which are beginning to resurface in present day 21st Century. Our society is on the advent of a new revolution; a revolution led by advances in Computer Science, Artificial Intelligence & Nanotechnology. Some of our advances have been so phenomenal that we surpassed what was predicted by the Moore's law. With these advancements comes the fear that our future may be at the mercy of these androids. Humans today are scared that we, ourselves, might create something which we cannot control. We may end up creating something which can not only learn much faster than anyone of us can, but also evolve faster than what the theory of evolution has allowed us to. The greatest fear is not only that we might lose our jobs to these intelligent beings, but that these beings might end up replacing us at the top of the cycle. The public hysteria has been heightened more so by a number of cultural works which depict annihilation of the human race by robots. Right from Frankenstein to I, Robot mass media has also depicted such issues. This paper is an effort to understand the need for ethics in Robotics or simply termed as Roboethics. This is achieved by the study of artificial beings and the thought being put behind them. By the end of the paper, however, it is concluded that there isn't a need for ethical robots but more so ever a need for ethical roboticists.\n    ",
        "submission_date": "2010-12-27T00:00:00",
        "last_modified_date": "2010-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.5705",
        "title": "Looking for plausibility",
        "authors": [
            "Wan Ahmad Tajuddin Wan Abdullah"
        ],
        "abstract": "In the interpretation of experimental data, one is actually looking for plausible explanations. We look for a measure of plausibility, with which we can compare different possible explanations, and which can be combined when there are different sets of data. This is contrasted to the conventional measure for probabilities as well as to the proposed measure of possibilities. We define what characteristics this measure of plausibility should have.\n",
        "submission_date": "2010-12-28T00:00:00",
        "last_modified_date": "2010-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.5813",
        "title": "Neural Network Influence in Group Technology: A Chronological Survey and Critical Analysis",
        "authors": [
            "Sourav Sengupta",
            "Tamal Ghosh",
            "Pranab K Dan",
            "Manojit Chattopadhyay"
        ],
        "abstract": "This article portrays a chronological review of the influence of Artificial Neural Network in group technology applications in the vicinity of Cellular Manufacturing Systems. The research trend is identified and the evolvement is captured through a critical analysis of the literature accessible from the very beginning of its practice in the early 90's till the 2010. Analysis of the diverse ANN approaches, spotted research pattern, comparison of the clustering efficiencies, the solutions obtained and the tools used make this study exclusive in its class.\n    ",
        "submission_date": "2010-12-28T00:00:00",
        "last_modified_date": "2012-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.5815",
        "title": "SAPFOCS: a metaheuristic based approach to part family formation problems in group technology",
        "authors": [
            "Tamal Ghosh",
            "Mousumi Modak",
            "Pranab K Dan"
        ],
        "abstract": "This article deals with Part family formation problem which is believed to be moderately complicated to be solved in polynomial time in the vicinity of Group Technology (GT). In the past literature researchers investigated that the part family formation techniques are principally based on production flow analysis (PFA) which usually considers operational requirements, sequences and time. Part Coding Analysis (PCA) is merely considered in GT which is believed to be the proficient method to identify the part families. PCA classifies parts by allotting them to different families based on their resemblances in: (1) design characteristics such as shape and size, and/or (2) manufacturing characteristics (machining requirements). A novel approach based on simulated annealing namely SAPFOCS is adopted in this study to develop effective part families exploiting the PCA technique. Thereafter Taguchi's orthogonal design method is employed to solve the critical issues on the subject of parameters selection for the proposed metaheuristic algorithm. The adopted technique is therefore tested on 5 different datasets of size 5 {\\times} 9 to 27 {\\times} 9 and the obtained results are compared with C-Linkage clustering technique. The experimental results reported that the proposed metaheuristic algorithm is extremely effective in terms of the quality of the solution obtained and has outperformed C-Linkage algorithm in most instances.\n    ",
        "submission_date": "2010-12-28T00:00:00",
        "last_modified_date": "2011-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.5847",
        "title": "On Elementary Loops of Logic Programs",
        "authors": [
            "Martin Gebser",
            "Joohyung Lee",
            "Yuliya Lierler"
        ],
        "abstract": "Using the notion of an elementary loop, Gebser and Schaub refined the theorem on loop formulas due to Lin and Zhao by considering loop formulas of elementary loops only. In this article, we reformulate their definition of an elementary loop, extend it to disjunctive programs, and study several properties of elementary loops, including how maximal elementary loops are related to minimal unfounded sets. The results provide useful insights into the stable model semantics in terms of elementary loops. For a nondisjunctive program, using a graph-theoretic characterization of an elementary loop, we show that the problem of recognizing an elementary loop is tractable. On the other hand, we show that the corresponding problem is {\\sf coNP}-complete for a disjunctive program. Based on the notion of an elementary loop, we present the class of Head-Elementary-loop-Free (HEF) programs, which strictly generalizes the class of Head-Cycle-Free (HCF) programs due to Ben-Eliyahu and Dechter. Like an HCF program, an HEF program can be turned into an equivalent nondisjunctive program in polynomial time by shifting head atoms into the body.\n    ",
        "submission_date": "2010-12-28T00:00:00",
        "last_modified_date": "2011-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.5960",
        "title": "Extending Binary Qualitative Direction Calculi with a Granular Distance Concept: Hidden Feature Attachment",
        "authors": [
            "Reinhard Moratz"
        ],
        "abstract": "In this paper we introduce a method for extending binary qualitative direction calculi with adjustable granularity like OPRAm or the star calculus with a granular distance concept. This method is similar to the concept of extending points with an internal reference direction to get oriented points which are the basic entities in the OPRAm calculus. Even if the spatial objects are from a geometrical point of view infinitesimal small points locally available reference measures are attached. In the case of OPRAm, a reference direction is attached. The same principle works also with local reference distances which are called elevations. The principle of attaching references features to a point is called hidden feature attachment.\n    ",
        "submission_date": "2010-12-29T00:00:00",
        "last_modified_date": "2010-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.6018",
        "title": "Learning a Representation of a Believable Virtual Character's Environment with an Imitation Algorithm",
        "authors": [
            "Fabien Tenc\u00e9",
            "C\u00e9dric Buche",
            "Pierre De Loor",
            "Olivier Marc"
        ],
        "abstract": "In video games, virtual characters' decision systems often use a simplified representation of the world. To increase both their autonomy and believability we want those characters to be able to learn this representation from human players. We propose to use a model called growing neural gas to learn by imitation the topology of the environment. The implementation of the model, the modifications and the parameters we used are detailed. Then, the quality of the learned representations and their evolution during the learning are studied using different measures. Improvements for the growing neural gas to give more information to the character's model are given in the conclusion.\n    ",
        "submission_date": "2010-12-29T00:00:00",
        "last_modified_date": "2010-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.0054",
        "title": "Cryptographic Implications for Artificially Mediated Games",
        "authors": [
            "Thomas Kellam Meyer"
        ],
        "abstract": "  There is currently an intersection in the research of game theory and cryptography. Generally speaking, there are two aspects to this partnership. First there is the application of game theory to cryptography. Yet, the purpose of this paper is to focus on the second aspect, the converse of the first, the application of cryptography to game theory. Chiefly, there exist a branch of non-cooperative games which have a correlated equilibrium as their solution. These equilibria tend to be superior to the conventional Nash equilibria. The primary condition for a correlated equilibrium is the presence of a mediator within the game. This is simply a neutral and mutually trusted entity. It is the role of the mediator to make recommendations in terms of strategy profiles to all players, who then act (supposedly) on this advice. Each party privately provides the mediator with the necessary information, and the referee responds privately with their optimized strategy set. However, there seem to be a multitude of situations in which no mediator could exist. Thus, games modeling these sorts of cases could not use these entities as tools for analysis. Yet, if these equilibria are in the best interest of players, it would be rational to construct a machine, or protocol, to calculate them. Of course, this machine would need to satisfy some standard for secure transmission between a player and itself. The requirement that no third party could detect either the input or strategy profile would need to be satisfied by this scheme. Here is the synthesis of cryptography into game theory; analyzing the ability of the players to construct a protocol which can be used successfully in the place of a mediator.\n    ",
        "submission_date": "2009-12-30T00:00:00",
        "last_modified_date": "2009-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.0735",
        "title": "Named Models in Coalgebraic Hybrid Logic",
        "authors": [
            "Lutz Schroeder",
            "Dirk Pattinson"
        ],
        "abstract": "  Hybrid logic extends modal logic with support for reasoning about individual states, designated by so-called nominals. We study hybrid logic in the broad context of coalgebraic semantics, where Kripke frames are replaced with coalgebras for a given functor, thus covering a wide range of reasoning principles including, e.g., probabilistic, graded, default, or coalitional operators. Specifically, we establish generic criteria for a given coalgebraic hybrid logic to admit named canonical models, with ensuing completeness proofs for pure extensions on the one hand, and for an extended hybrid language with local binding on the other. We instantiate our framework with a number of examples. Notably, we prove completeness of graded hybrid logic with local binding.\n    ",
        "submission_date": "2010-01-05T00:00:00",
        "last_modified_date": "2010-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.0746",
        "title": "Alternation-Trading Proofs, Linear Programming, and Lower Bounds",
        "authors": [
            "Ryan Williams"
        ],
        "abstract": "  A fertile area of recent research has demonstrated concrete polynomial time lower bounds for solving natural hard problems on restricted computational models. Among these problems are Satisfiability, Vertex Cover, Hamilton Path, Mod6-SAT, Majority-of-Majority-SAT, and Tautologies, to name a few. The proofs of these lower bounds follow a certain proof-by-contradiction strategy that we call alternation-trading. An important open problem is to determine how powerful such proofs can possibly be.\n",
        "submission_date": "2010-01-05T00:00:00",
        "last_modified_date": "2010-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.0827",
        "title": "Document Clustering with K-tree",
        "authors": [
            "Christopher M. De Vries",
            "Shlomo Geva"
        ],
        "abstract": "  This paper describes the approach taken to the XML Mining track at INEX 2008 by a group at the Queensland University of Technology. We introduce the K-tree clustering algorithm in an Information Retrieval context by adapting it for document clustering. Many large scale problems exist in document clustering. K-tree scales well with large inputs due to its low complexity. It offers promising results both in terms of efficiency and quality. Document classification was completed using Support Vector Machines.\n    ",
        "submission_date": "2010-01-06T00:00:00",
        "last_modified_date": "2010-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.0830",
        "title": "K-tree: Large Scale Document Clustering",
        "authors": [
            "Christopher M. De Vries",
            "Shlomo Geva"
        ],
        "abstract": "  We introduce K-tree in an information retrieval context. It is an efficient approximation of the k-means clustering algorithm. Unlike k-means it forms a hierarchy of clusters. It has been extended to address issues with sparse representations. We compare performance and quality to CLUTO using document collections. The K-tree has a low time complexity that is suitable for large document collections. This tree structure allows for efficient disk based implementations where space requirements exceed that of main memory.\n    ",
        "submission_date": "2010-01-06T00:00:00",
        "last_modified_date": "2010-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.0833",
        "title": "Random Indexing K-tree",
        "authors": [
            "Christopher M. De Vries",
            "Lance De Vine",
            "Shlomo Geva"
        ],
        "abstract": "  Random Indexing (RI) K-tree is the combination of two algorithms for clustering. Many large scale problems exist in document clustering. RI K-tree scales well with large inputs due to its low complexity. It also exhibits features that are useful for managing a changing collection. Furthermore, it solves previous issues with sparse document vectors when using K-tree. The algorithms and data structures are defined, explained and motivated. Specific modifications to K-tree are made for use with RI. Experiments have been executed to measure quality. The results indicate that RI K-tree improves document cluster quality over the original K-tree algorithm.\n    ",
        "submission_date": "2010-01-06T00:00:00",
        "last_modified_date": "2010-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.1020",
        "title": "An Empirical Evaluation of Four Algorithms for Multi-Class Classification: Mart, ABC-Mart, Robust LogitBoost, and ABC-LogitBoost",
        "authors": [
            "Ping Li"
        ],
        "abstract": "  This empirical study is mainly devoted to comparing four tree-based boosting algorithms: mart, abc-mart, robust logitboost, and abc-logitboost, for multi-class classification on a variety of publicly available datasets. Some of those datasets have been thoroughly tested in prior studies using a broad range of classification algorithms including SVM, neural nets, and deep learning.\n",
        "submission_date": "2010-01-07T00:00:00",
        "last_modified_date": "2010-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.1122",
        "title": "Principal manifolds and graphs in practice: from molecular biology to dynamical systems",
        "authors": [
            "A. N. Gorban",
            "A. Zinovyev"
        ],
        "abstract": "We present several applications of non-linear data modeling, using principal manifolds and principal graphs constructed using the metaphor of elasticity (elastic principal graph approach). These approaches are generalizations of the Kohonen's self-organizing maps, a class of artificial neural networks. On several examples we show advantages of using non-linear objects for data approximation in comparison to the linear ones. We propose four numerical criteria for comparing linear and non-linear mappings of datasets into the spaces of lower dimension. The examples are taken from comparative political science, from analysis of high-throughput data in molecular biology, from analysis of dynamical systems.\n    ",
        "submission_date": "2010-01-07T00:00:00",
        "last_modified_date": "2010-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.1653",
        "title": "A betting interpretation for probabilities and Dempster-Shafer degrees of belief",
        "authors": [
            "Glenn Shafer"
        ],
        "abstract": "  There are at least two ways to interpret numerical degrees of belief in terms of betting: (1) you can offer to bet at the odds defined by the degrees of belief, or (2) you can judge that a strategy for taking advantage of such betting offers will not multiply the capital it risks by a large factor. Both interpretations can be applied to ordinary additive probabilities and used to justify updating by conditioning. Only the second can be applied to Dempster-Shafer degrees of belief and used to justify Dempster's rule of combination.\n    ",
        "submission_date": "2010-01-11T00:00:00",
        "last_modified_date": "2010-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.1889",
        "title": "Cheating for Problem Solving: A Genetic Algorithm with Social Interactions",
        "authors": [
            "Rafeal Lahoz-Beltra",
            "Gabriela Ochoa",
            "Uwe Aickelin"
        ],
        "abstract": "  We propose a variation of the standard genetic algorithm that incorporates social interaction between the individuals in the population. Our goal is to understand the evolutionary role of social systems and its possible application as a non-genetic new step in evolutionary algorithms. In biological populations, ie animals, even human beings and microorganisms, social interactions often affect the fitness of individuals. It is conceivable that the perturbation of the fitness via social interactions is an evolutionary strategy to avoid trapping into local optimum, thus avoiding a fast convergence of the population. We model the social interactions according to Game Theory. The population is, therefore, composed by cooperator and defector individuals whose interactions produce payoffs according to well known game models (prisoner's dilemma, chicken game, and others). Our results on Knapsack problems show, for some game models, a significant performance improvement as compared to a standard genetic algorithm.\n    ",
        "submission_date": "2010-01-12T00:00:00",
        "last_modified_date": "2010-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.2391",
        "title": "A Little More, a Lot Better: Improving Path Quality by a Simple Path Merging Algorithm",
        "authors": [
            "Barak Raveh",
            "Angela Enosh",
            "Dan Halperin"
        ],
        "abstract": "Sampling-based motion planners are an effective means for generating collision-free motion paths. However, the quality of these motion paths (with respect to quality measures such as path length, clearance, smoothness or energy) is often notoriously low, especially in high-dimensional configuration spaces. We introduce a simple algorithm for merging an arbitrary number of input motion paths into a hybrid output path of superior quality, for a broad and general formulation of path quality. Our approach is based on the observation that the quality of certain sub-paths within each solution may be higher than the quality of the entire path. A dynamic-programming algorithm, which we recently developed for comparing and clustering multiple motion paths, reduces the running time of the merging algorithm significantly. We tested our algorithm in motion-planning problems with up to 12 degrees of freedom. We show that our algorithm is able to merge a handful of input paths produced by several different motion planners to produce output paths of much higher quality.\n    ",
        "submission_date": "2010-01-14T00:00:00",
        "last_modified_date": "2010-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.2709",
        "title": "Kernel machines with two layers and multiple kernel learning",
        "authors": [
            "Francesco Dinuzzo"
        ],
        "abstract": "  In this paper, the framework of kernel machines with two layers is introduced, generalizing classical kernel methods. The new learning methodology provide a formal connection between computational architectures with multiple layers and the theme of kernel learning in standard regularization methods. First, a representer theorem for two-layer networks is presented, showing that finite linear combinations of kernels on each layer are optimal architectures whenever the corresponding functions solve suitable variational problems in reproducing kernel Hilbert spaces (RKHS). The input-output map expressed by these architectures turns out to be equivalent to a suitable single-layer kernel machines in which the kernel function is also learned from the data. Recently, the so-called multiple kernel learning methods have attracted considerable attention in the machine learning literature. In this paper, multiple kernel learning methods are shown to be specific cases of kernel machines with two layers in which the second layer is linear. Finally, a simple and effective multiple kernel learning method called RLS2 (regularized least squares with two layers) is introduced, and his performances on several learning problems are extensively analyzed. An open source MATLAB toolbox to train and validate RLS2 models with a Graphic User Interface is available.\n    ",
        "submission_date": "2010-01-15T00:00:00",
        "last_modified_date": "2010-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.2813",
        "title": "A Monte Carlo Algorithm for Universally Optimal Bayesian Sequence Prediction and Planning",
        "authors": [
            "Anthony Di Franco"
        ],
        "abstract": "  The aim of this work is to address the question of whether we can in principle design rational decision-making agents or artificial intelligences embedded in computable physics such that their decisions are optimal in reasonable mathematical senses. Recent developments in rare event probability estimation, recursive bayesian inference, neural networks, and probabilistic planning are sufficient to explicitly approximate reinforcement learners of the AIXI style with non-trivial model classes (here, the class of resource-bounded Turing machines). Consideration of the effects of resource limitations in a concrete implementation leads to insights about possible architectures for learning systems using optimal decision makers as components.\n    ",
        "submission_date": "2010-01-18T00:00:00",
        "last_modified_date": "2010-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.3113",
        "title": "An Immuno-Inspired Approach to Misbehavior Detection in Ad Hoc Wireless Networks",
        "authors": [
            "Martin Drozda",
            "Sebastian Schildt",
            "Sven Schaust",
            "Helena Szczerbicka"
        ],
        "abstract": "We propose and evaluate an immuno-inspired approach to misbehavior detection in ad hoc wireless networks. Node misbehavior can be the result of an intrusion, or a software or hardware failure. Our approach is motivated by co-stimulatory signals present in the Biological immune system. The results show that co-stimulation in ad hoc wireless networks can both substantially improve energy efficiency of detection and, at the same time, help achieve low false positives rates. The energy efficiency improvement is almost two orders of magnitude, if compared to misbehavior detection based on watchdogs.\n",
        "submission_date": "2010-01-18T00:00:00",
        "last_modified_date": "2010-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.3745",
        "title": "The effect of discrete vs. continuous-valued ratings on reputation and ranking systems",
        "authors": [
            "Matus Medo",
            "Joseph Rushton Wakeling"
        ],
        "abstract": "When users rate objects, a sophisticated algorithm that takes into account ability or reputation may produce a fairer or more accurate aggregation of ratings than the straightforward arithmetic average. Recently a number of authors have proposed different co-determination algorithms where estimates of user and object reputation are refined iteratively together, permitting accurate measures of both to be derived directly from the rating data. However, simulations demonstrating these methods' efficacy assumed a continuum of rating values, consistent with typical physical modelling practice, whereas in most actual rating systems only a limited range of discrete values (such as a 5-star system) is employed. We perform a comparative test of several co-determination algorithms with different scales of discrete ratings and show that this seemingly minor modification in fact has a significant impact on algorithms' performance. Paradoxically, where rating resolution is low, increased noise in users' ratings may even improve the overall performance of the system.\n    ",
        "submission_date": "2010-01-21T00:00:00",
        "last_modified_date": "2010-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.4251",
        "title": "A Decidable Class of Nested Iterated Schemata (extended version)",
        "authors": [
            "Vincent Aravantinos",
            "Ricardo Caferra",
            "Nicolas Peltier"
        ],
        "abstract": "  Many problems can be specified by patterns of propositional formulae depending on a parameter, e.g. the specification of a circuit usually depends on the number of bits of its input. We define a logic whose formulae, called \"iterated schemata\", allow to express such patterns. Schemata extend propositional logic with indexed propositions, e.g. P_i, P_i+1, P_1, and with generalized connectives, e.g. /\u0131=1..n or i=1..n (called \"iterations\") where n is an (unbound) integer variable called a \"parameter\". The expressive power of iterated schemata is strictly greater than propositional logic: it is even out of the scope of first-order logic. We define a proof procedure, called DPLL*, that can prove that a schema is satisfiable for at least one value of its parameter, in the spirit of the DPLL procedure. However the converse problem, i.e. proving that a schema is unsatisfiable for every value of the parameter, is undecidable so DPLL* does not terminate in general. Still, we prove that it terminates for schemata of a syntactic subclass called \"regularly nested\". This is the first non trivial class for which DPLL* is proved to terminate. Furthermore the class of regularly nested schemata is the first decidable class to allow nesting of iterations, i.e. to allow schemata of the form /\u0131=1..n (/\\j=1..n ...).\n    ",
        "submission_date": "2010-01-24T00:00:00",
        "last_modified_date": "2010-01-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.4405",
        "title": "A Formal Framework of Virtual Organisations as Agent Societies",
        "authors": [
            "Jarred McGinnis",
            "Kostas Stathis",
            "Francesca Toni"
        ],
        "abstract": "  We propose a formal framework that supports a model of agent-based Virtual Organisations (VOs) for service grids and provides an associated operational model for the creation of VOs. The framework is intended to be used for describing different service grid applications based on multiple agents and, as a result, it abstracts away from any realisation choices of the service grid application, the agents involved to support the applications and their interactions. Within the proposed framework VOs are seen as emerging from societies of agents, where agents are abstractly characterised by goals and roles they can play within VOs. In turn, VOs are abstractly characterised by the agents participating in them with specific roles, as well as the workflow of services and corresponding contracts suitable for achieving the goals of the participating agents. We illustrate the proposed framework with an earth observation scenario.\n    ",
        "submission_date": "2010-01-25T00:00:00",
        "last_modified_date": "2010-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.4892",
        "title": "Janus: Automatic Ontology Builder from XSD Files",
        "authors": [
            "Ivan Bedini",
            "Benjamin Nguyen",
            "Georges Gardarin"
        ],
        "abstract": "  The construction of a reference ontology for a large domain still remains an hard human task. The process is sometimes assisted by software tools that facilitate the information extraction from a textual corpus. Despite of the great use of XML Schema files on the internet and especially in the B2B domain, tools that offer a complete semantic analysis of XML schemas are really rare. In this paper we introduce Janus, a tool for automatically building a reference knowledge base starting from XML Schema files. Janus also provides different useful views to simplify B2B application integration.\n    ",
        "submission_date": "2010-01-27T00:00:00",
        "last_modified_date": "2010-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1001.5244",
        "title": "Computing Networks: A General Framework to Contrast Neural and Swarm Cognitions",
        "authors": [
            "Carlos Gershenson"
        ],
        "abstract": "This paper presents the Computing Networks (CNs) framework. CNs are used to generalize neural and swarm architectures. Artificial neural networks, ant colony optimization, particle swarm optimization, and realistic biological models are used as examples of instantiations of CNs. The description of these architectures as CNs allows their comparison. Their differences and similarities allow the identification of properties that enable neural and swarm architectures to perform complex computations and exhibit complex cognitive abilities. In this context, the most relevant characteristics of CNs are the existence multiple dynamical and functional scales. The relationship between multiple dynamical and functional scales with adaptation, cognition (of brains and swarms) and computation is discussed.\n    ",
        "submission_date": "2010-01-28T00:00:00",
        "last_modified_date": "2010-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.0378",
        "title": "A Grey-Box Approach to Automated Mechanism Design",
        "authors": [
            "Jinzhong Niu",
            "Kai Cai",
            "Simon Parsons"
        ],
        "abstract": "  Auctions play an important role in electronic commerce, and have been used to solve problems in distributed computing. Automated approaches to designing effective auction mechanisms are helpful in reducing the burden of traditional game theoretic, analytic approaches and in searching through the large space of possible auction mechanisms. This paper presents an approach to automated mechanism design (AMD) in the domain of double auctions. We describe a novel parametrized space of double auctions, and then introduce an evolutionary search method that searches this space of parameters. The approach evaluates auction mechanisms using the framework of the TAC Market Design Game and relates the performance of the markets in that game to their constituent parts using reinforcement learning. Experiments show that the strongest mechanisms we found using this approach not only win the Market Design Game against known, strong opponents, but also exhibit desirable economic properties when they run in isolation.\n    ",
        "submission_date": "2010-02-02T00:00:00",
        "last_modified_date": "2010-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.0382",
        "title": "Face Recognition by Fusion of Local and Global Matching Scores using DS Theory: An Evaluation with Uni-classifier and Multi-classifier Paradigm",
        "authors": [
            "Dakshina Ranjan Kisku",
            "Massimo Tistarelli",
            "Jamuna Kanta Sing",
            "Phalguni Gupta"
        ],
        "abstract": "  Faces are highly deformable objects which may easily change their appearance over time. Not all face areas are subject to the same variability. Therefore decoupling the information from independent areas of the face is of paramount importance to improve the robustness of any face recognition technique. This paper presents a robust face recognition technique based on the extraction and matching of SIFT features related to independent face areas. Both a global and local (as recognition from parts) matching strategy is proposed. The local strategy is based on matching individual salient facial SIFT features as connected to facial landmarks such as the eyes and the mouth. As for the global matching strategy, all SIFT features are combined together to form a single feature. In order to reduce the identification errors, the Dempster-Shafer decision theory is applied to fuse the two matching techniques. The proposed algorithms are evaluated with the ORL and the IITK face databases. The experimental results demonstrate the effectiveness and potential of the proposed face recognition techniques also in the case of partially occluded faces or with missing information.\n    ",
        "submission_date": "2010-02-02T00:00:00",
        "last_modified_date": "2010-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.0411",
        "title": "Face Identification by SIFT-based Complete Graph Topology",
        "authors": [
            "Dakshina Ranjan Kisku",
            "Ajita Rattani",
            "Enrico Grosso",
            "Massimo Tistarelli"
        ],
        "abstract": "  This paper presents a new face identification system based on Graph Matching Technique on SIFT features extracted from face images. Although SIFT features have been successfully used for general object detection and recognition, only recently they were applied to face recognition. This paper further investigates the performance of identification techniques based on Graph matching topology drawn on SIFT features which are invariant to rotation, scaling and translation. Face projections on images, represented by a graph, can be matched onto new images by maximizing a similarity function taking into account spatial distortions and the similarities of the local features. Two graph based matching techniques have been investigated to deal with false pair assignment and reducing the number of features to find the optimal feature set between database and query face SIFT features. The experimental results, performed on the BANCA database, demonstrate the effectiveness of the proposed system for automatic face identification.\n    ",
        "submission_date": "2010-02-02T00:00:00",
        "last_modified_date": "2010-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.0412",
        "title": "SIFT-based Ear Recognition by Fusion of Detected Keypoints from Color Similarity Slice Regions",
        "authors": [
            "Dakshina Ranjan Kisku",
            "Hunny Mehrotra",
            "Phalguni Gupta",
            "Jamuna Kanta Sing"
        ],
        "abstract": "  Ear biometric is considered as one of the most reliable and invariant biometrics characteristics in line with iris and fingerprint characteristics. In many cases, ear biometrics can be compared with face biometrics regarding many physiological and texture characteristics. In this paper, a robust and efficient ear recognition system is presented, which uses Scale Invariant Feature Transform (SIFT) as feature descriptor for structural representation of ear images. In order to make it more robust to user authentication, only the regions having color probabilities in a certain ranges are considered for invariant SIFT feature extraction, where the K-L divergence is used for keeping color consistency. Ear skin color model is formed by Gaussian mixture model and clustering the ear color pattern using vector quantization. Finally, K-L divergence is applied to the GMM framework for recording the color similarity in the specified ranges by comparing color similarity between a pair of reference model and probe ear images. After segmentation of ear images in some color slice regions, SIFT keypoints are extracted and an augmented vector of extracted SIFT features are created for matching, which is accomplished between a pair of reference model and probe ear images. The proposed technique has been tested on the IITK Ear database and the experimental results show improvements in recognition accuracy while invariant features are extracted from color slice regions to maintain the robustness of the system.\n    ",
        "submission_date": "2010-02-02T00:00:00",
        "last_modified_date": "2010-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.0414",
        "title": "Feature Level Fusion of Biometrics Cues: Human Identification with Doddingtons Caricature",
        "authors": [
            "Dakshina Ranjan Kisku",
            "Phalguni Gupta",
            "Jamuna Kanta Sing"
        ],
        "abstract": "  This paper presents a multimodal biometric system of fingerprint and ear biometrics. Scale Invariant Feature Transform (SIFT) descriptor based feature sets extracted from fingerprint and ear are fused. The fused set is encoded by K-medoids partitioning approach with less number of feature points in the set. K-medoids partition the whole dataset into clusters to minimize the error between data points belonging to the clusters and its center. Reduced feature set is used to match between two biometric sets. Matching scores are generated using wolf-lamb user-dependent feature weighting scheme introduced by Doddington. The technique is tested to exhibit its robust performance.\n    ",
        "submission_date": "2010-02-02T00:00:00",
        "last_modified_date": "2010-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.0745",
        "title": "Using CODEQ to Train Feed-forward Neural Networks",
        "authors": [
            "Mahamed G. H. Omran",
            "Faisal al-Adwani"
        ],
        "abstract": "  CODEQ is a new, population-based meta-heuristic algorithm that is a hybrid of concepts from chaotic search, opposition-based learning, differential evolution and quantum mechanics. CODEQ has successfully been used to solve different types of problems (e.g. constrained, integer-programming, engineering) with excellent results. In this paper, CODEQ is used to train feed-forward neural networks. The proposed method is compared with particle swarm optimization and differential evolution algorithms on three data sets with encouraging results.\n    ",
        "submission_date": "2010-02-03T00:00:00",
        "last_modified_date": "2010-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.1200",
        "title": "Detecting Bots Based on Keylogging Activities",
        "authors": [
            "Yousof Al-Hammadi",
            "Uwe Aickelin"
        ],
        "abstract": "  A bot is a piece of software that is usually installed on an infected machine without the user's knowledge. A bot is controlled remotely by the attacker under a Command and Control structure. Recent statistics show that bots represent one of the fastest growing threats to our network by performing malicious activities such as email spamming or keylogging. However, few bot detection techniques have been developed to date. In this paper, we investigate a behavioural algorithm to detect a single bot that uses keylogging activity. Our approach involves the use of function calls analysis for the detection of the bot with a keylogging component. Correlation of the frequency of a specified time-window is performed to enhance he detection scheme. We perform a range of experiments with the spybot. Our results show that there is a high correlation between some function calls executed by this bot which indicates abnormal activity in our system.\n    ",
        "submission_date": "2010-02-05T00:00:00",
        "last_modified_date": "2010-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.2240",
        "title": "A Generalization of the Chow-Liu Algorithm and its Application to Statistical Learning",
        "authors": [
            "Joe Suzuki"
        ],
        "abstract": "  We extend the Chow-Liu algorithm for general random variables while the previous versions only considered finite cases. In particular, this paper applies the generalization to Suzuki's learning algorithm that generates from data forests rather than trees based on the minimum description length by balancing the fitness of the data to the forest and the simplicity of the forest. As a result, we successfully obtain an algorithm when both of the Gaussian and finite random variables are present.\n    ",
        "submission_date": "2010-02-10T00:00:00",
        "last_modified_date": "2010-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.2523",
        "title": "Feature Level Fusion of Face and Fingerprint Biometrics",
        "authors": [
            "Ajita Rattani",
            "Dakshina Ranjan Kisku",
            "Manuele Bicego",
            "Massimo Tistarelli"
        ],
        "abstract": "  The aim of this paper is to study the fusion at feature extraction level for face and fingerprint biometrics. The proposed approach is based on the fusion of the two traits by extracting independent feature pointsets from the two modalities, and making the two pointsets compatible for concatenation. Moreover, to handle the problem of curse of dimensionality, the feature pointsets are properly reduced in dimension. Different feature reduction techniques are implemented, prior and after the feature pointsets fusion, and the results are duly recorded. The fused feature pointset for the database and the query face and fingerprint images are matched using techniques based on either the point pattern matching, or the Delaunay triangulation. Comparative experiments are conducted on chimeric and real databases, to assess the actual advantage of the fusion performed at the feature extraction level, in comparison to the matching score level.\n    ",
        "submission_date": "2010-02-12T00:00:00",
        "last_modified_date": "2010-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.2755",
        "title": "Multibiometrics Belief Fusion",
        "authors": [
            "Dakshina Ranjan Kisku",
            "Jamuna Kanta Sing",
            "Phalguni Gupta"
        ],
        "abstract": "  This paper proposes a multimodal biometric system through Gaussian Mixture Model (GMM) for face and ear biometrics with belief fusion of the estimated scores characterized by Gabor responses and the proposed fusion is accomplished by Dempster-Shafer (DS) decision theory. Face and ear images are convolved with Gabor wavelet filters to extracts spatially enhanced Gabor facial features and Gabor ear features. Further, GMM is applied to the high-dimensional Gabor face and Gabor ear responses separately for quantitive measurements. Expectation Maximization (EM) algorithm is used to estimate density parameters in GMM. This produces two sets of feature vectors which are then fused using Dempster-Shafer theory. Experiments are conducted on multimodal database containing face and ear images of 400 individuals. It is found that use of Gabor wavelet filters along with GMM and DS theory can provide robust and efficient multimodal fusion strategy.\n    ",
        "submission_date": "2010-02-14T00:00:00",
        "last_modified_date": "2010-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.3174",
        "title": "A new approach to content-based file type detection",
        "authors": [
            "M. C. Amirani",
            "M. Toorani",
            "A. A. Beheshti"
        ],
        "abstract": "File type identification and file type clustering may be difficult tasks that have an increasingly importance in the field of computer and network security. Classical methods of file type detection including considering file extensions and magic bytes can be easily spoofed. Content-based file type detection is a newer way that is taken into account recently. In this paper, a new content-based method for the purpose of file type detection and file type clustering is proposed that is based on the PCA and neural networks. The proposed method has a good accuracy and is fast enough.\n    ",
        "submission_date": "2010-02-17T00:00:00",
        "last_modified_date": "2012-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.3239",
        "title": "Message-Passing Algorithms: Reparameterizations and Splittings",
        "authors": [
            "Nicholas Ruozzi",
            "Sekhar Tatikonda"
        ],
        "abstract": "The max-product algorithm, a local message-passing scheme that attempts to compute the most probable assignment (MAP) of a given probability distribution, has been successfully employed as a method of approximate inference for applications arising in coding theory, computer vision, and machine learning. However, the max-product algorithm is not guaranteed to converge to the MAP assignment, and if it does, is not guaranteed to recover the MAP assignment.\n",
        "submission_date": "2010-02-17T00:00:00",
        "last_modified_date": "2012-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.4014",
        "title": "A fuzzified BRAIN algorithm for learning DNF from incomplete data",
        "authors": [
            "Salvatore Rampone",
            "Ciro Russo"
        ],
        "abstract": "Aim of this paper is to address the problem of learning Boolean functions from training data with missing values. We present an extension of the BRAIN algorithm, called U-BRAIN (Uncertainty-managing Batch Relevance-based Artificial INtelligence), conceived for learning DNF Boolean formulas from partial truth tables, possibly with uncertain values or missing bits.\n",
        "submission_date": "2010-02-21T00:00:00",
        "last_modified_date": "2011-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.4286",
        "title": "Redundancy, Deduction Schemes, and Minimum-Size Bases for Association Rules",
        "authors": [
            "Jose L. Balcazar"
        ],
        "abstract": "Association rules are among the most widely employed data analysis methods in the field of Data Mining. An association rule is a form of partial implication between two sets of binary variables. In the most common approach, association rules are parameterized by a lower bound on their confidence, which is the empirical conditional probability of their consequent given the antecedent, and/or by some other parameter bounds such as \"support\" or deviation from independence. We study here notions of redundancy among association rules from a fundamental perspective. We see each transaction in a dataset as an interpretation (or model) in the propositional logic sense, and consider existing notions of redundancy, that is, of logical entailment, among association rules, of the form \"any dataset in which this first rule holds must obey also that second rule, therefore the second is redundant\". We discuss several existing alternative definitions of redundancy between association rules and provide new characterizations and relationships among them. We show that the main alternatives we discuss correspond actually to just two variants, which differ in the treatment of full-confidence implications. For each of these two notions of redundancy, we provide a sound and complete deduction calculus, and we show how to construct complete bases (that is, axiomatizations) of absolutely minimum size in terms of the number of rules. We explore finally an approach to redundancy with respect to several association rules, and fully characterize its simplest case of two partial premises.\n    ",
        "submission_date": "2010-02-23T00:00:00",
        "last_modified_date": "2010-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.4453",
        "title": "Nonparametric Estimation and On-Line Prediction for General Stationary Ergodic Sources",
        "authors": [
            "Joe Suzuki"
        ],
        "abstract": "We proposed a learning algorithm for nonparametric estimation and on-line prediction for general stationary ergodic sources. We prepare histograms each of which estimates the probability as a finite distribution, and mixture them with weights to construct an estimator. The whole analysis is based on measure theory. The estimator works whether the source is discrete or continuous. If it is stationary ergodic, then the measure theoretically given Kullback-Leibler information divided by the sequence length $n$ converges to zero as $n$ goes to infinity. In particular, for continuous sources, the method does not require existence of a probability density function.\n    ",
        "submission_date": "2010-02-24T00:00:00",
        "last_modified_date": "2010-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.4665",
        "title": "Syntactic Topic Models",
        "authors": [
            "Jordan Boyd-Graber",
            "David M. Blei"
        ],
        "abstract": "  The syntactic topic model (STM) is a Bayesian nonparametric model of language that discovers latent distributions of words (topics) that are both semantically and syntactically coherent. The STM models dependency parsed corpora where sentences are grouped into documents. It assumes that each word is drawn from a latent topic chosen by combining document-level features and the local syntactic context. Each document has a distribution over latent topics, as in topic models, which provides the semantic consistency. Each element in the dependency parse tree also has a distribution over the topics of its children, as in latent-state syntax models, which provides the syntactic consistency. These distributions are convolved so that the topic of each word is likely under both its document and syntactic context. We derive a fast posterior inference algorithm based on variational methods. We report qualitative and quantitative studies on both synthetic data and hand-parsed documents. We show that the STM is a more predictive model of language than current models based only on syntax or only on topics.\n    ",
        "submission_date": "2010-02-25T00:00:00",
        "last_modified_date": "2010-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1002.4862",
        "title": "Less Regret via Online Conditioning",
        "authors": [
            "Matthew Streeter",
            "H. Brendan McMahan"
        ],
        "abstract": "  We analyze and evaluate an online gradient descent algorithm with adaptive per-coordinate adjustment of learning rates. Our algorithm can be thought of as an online version of batch gradient descent with a diagonal preconditioner. This approach leads to regret bounds that are stronger than those of standard online gradient descent for general online convex optimization problems. Experimentally, we show that our algorithm is competitive with state-of-the-art algorithms for large scale machine learning problems.\n    ",
        "submission_date": "2010-02-25T00:00:00",
        "last_modified_date": "2010-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.0120",
        "title": "Learning from Logged Implicit Exploration Data",
        "authors": [
            "Alex Strehl",
            "John Langford",
            "Sham Kakade",
            "Lihong Li"
        ],
        "abstract": "We provide a sound and consistent foundation for the use of \\emph{nonrandom} exploration data in \"contextual bandit\" or \"partially labeled\" settings where only the value of a chosen action is learned.\n",
        "submission_date": "2010-02-27T00:00:00",
        "last_modified_date": "2010-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.0146",
        "title": "A Contextual-Bandit Approach to Personalized News Article Recommendation",
        "authors": [
            "Lihong Li",
            "Wei Chu",
            "John Langford",
            "Robert E. Schapire"
        ],
        "abstract": "Personalized web services strive to adapt their services (advertisements, news articles, etc) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation.\n",
        "submission_date": "2010-02-28T00:00:00",
        "last_modified_date": "2012-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.0358",
        "title": "Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition",
        "authors": [
            "Dan Claudiu Ciresan",
            "Ueli Meier",
            "Luca Maria Gambardella",
            "Juergen Schmidhuber"
        ],
        "abstract": "  Good old on-line back-propagation for plain multi-layer perceptrons yields a very low 0.35% error rate on the famous MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images, and graphics cards to greatly speed up learning.\n    ",
        "submission_date": "2010-03-01T00:00:00",
        "last_modified_date": "2010-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.0617",
        "title": "Agent Based Approaches to Engineering Autonomous Space Software",
        "authors": [
            "Louise A. Dennis",
            "Michael Fisher",
            "Nicholas Lincoln",
            "Alexei Lisitsa",
            "Sandor M. Veres"
        ],
        "abstract": "  Current approaches to the engineering of space software such as satellite control systems are based around the development of feedback controllers using packages such as MatLab's Simulink toolbox. These provide powerful tools for engineering real time systems that adapt to changes in the environment but are limited when the controller itself needs to be adapted.\n",
        "submission_date": "2010-03-02T00:00:00",
        "last_modified_date": "2010-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.1343",
        "title": "What does Newcomb's paradox teach us?",
        "authors": [
            "David H. Wolpert",
            "Gregory Benford"
        ],
        "abstract": "In Newcomb's paradox you choose to receive either the contents of a particular closed box, or the contents of both that closed box and another one. Before you choose, a prediction algorithm deduces your choice, and fills the two boxes based on that deduction. Newcomb's paradox is that game theory appears to provide two conflicting recommendations for what choice you should make in this scenario. We analyze Newcomb's paradox using a recent extension of game theory in which the players set conditional probability distributions in a Bayes net. We show that the two game theory recommendations in Newcomb's scenario have different presumptions for what Bayes net relates your choice and the algorithm's prediction. We resolve the paradox by proving that these two Bayes nets are incompatible. We also show that the accuracy of the algorithm's prediction, the focus of much previous work, is irrelevant. In addition we show that Newcomb's scenario only provides a contradiction between game theory's expected utility and dominance principles if one is sloppy in specifying the underlying Bayes net. We also show that Newcomb's paradox is time-reversal invariant; both the paradox and its resolution are unchanged if the algorithm makes its `prediction' after you make your choice rather than before.\n    ",
        "submission_date": "2010-03-06T00:00:00",
        "last_modified_date": "2010-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.1954",
        "title": "Estimation of R\u00e9nyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs",
        "authors": [
            "D\u00e1vid P\u00e1l",
            "Barnab\u00e1s P\u00f3czos",
            "Csaba Szepesv\u00e1ri"
        ],
        "abstract": "We present simple and computationally efficient nonparametric estimators of R\u00e9nyi entropy and mutual information based on an i.i.d. sample drawn from an unknown, absolutely continuous distribution over $\\R^d$. The estimators are calculated as the sum of $p$-th powers of the Euclidean lengths of the edges of the `generalized nearest-neighbor' graph of the sample and the empirical copula of the sample respectively. For the first time, we prove the almost sure consistency of these estimators and upper bounds on their rates of convergence, the latter of which under the assumption that the density underlying the sample is Lipschitz continuous. Experiments demonstrate their usefulness in independent subspace analysis.\n    ",
        "submission_date": "2010-03-09T00:00:00",
        "last_modified_date": "2010-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.2429",
        "title": "Predicting Positive and Negative Links in Online Social Networks",
        "authors": [
            "Jure Leskovec",
            "Daniel Huttenlocher",
            "Jon Kleinberg"
        ],
        "abstract": "We study online social networks in which relationships can be either positive (indicating relations such as friendship) or negative (indicating relations such as opposition or antagonism).  Such a mix of positive and negative links arise in a variety of online settings; we study datasets from Epinions, Slashdot and Wikipedia. We find that the signs of links in the underlying social networks can be predicted with high accuracy, using models that generalize across this diverse range of sites. These models provide insight into some of the fundamental principles that drive the formation of signed links in networks, shedding light on theories of balance and status from social psychology; they also suggest social computing applications by which the attitude of one user toward another can be estimated from evidence provided by their relationships with other members of the surrounding social network.\n    ",
        "submission_date": "2010-03-11T00:00:00",
        "last_modified_date": "2010-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.2586",
        "title": "Inductive Logic Programming in Databases: from Datalog to DL+log",
        "authors": [
            "Francesca A. Lisi"
        ],
        "abstract": "In this paper we address an issue that has been brought to the attention of the database community with the advent of the Semantic Web, i.e. the issue of how ontologies (and semantics conveyed by them) can help solving typical database problems, through a better understanding of KR aspects related to databases. In particular, we investigate this issue from the ILP perspective by considering two database problems, (i) the definition of views and (ii) the definition of constraints, for a database whose schema is represented also by means of an ontology. Both can be reformulated as ILP problems and can benefit from the expressive and deductive power of the KR framework DL+log. We illustrate the application scenarios by means of examples. Keywords: Inductive Logic Programming, Relational Databases, Ontologies, Description Logics, Hybrid Knowledge Representation and Reasoning Systems. Note: To appear in Theory and Practice of Logic Programming (TPLP).\n    ",
        "submission_date": "2010-03-12T00:00:00",
        "last_modified_date": "2010-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.2700",
        "title": "The role of semantics in mining frequent patterns from knowledge bases in description logics with rules",
        "authors": [
            "Joanna Jozefowska",
            "Agnieszka Lawrynowicz",
            "Tomasz Lukaszewski"
        ],
        "abstract": "We propose a new method for mining frequent patterns in a language that combines both Semantic Web ontologies and rules. In particular we consider the setting of using a language that combines description logics with DL-safe rules. This setting is important for the practical application of data mining to the Semantic Web. We focus on the relation of the semantics of the representation formalism to the task of frequent pattern discovery, and for the core of our method, we propose an algorithm that exploits the semantics of the combined knowledge base. We have developed a proof-of-concept data mining implementation of this. Using this we have empirically shown that using the combined knowledge base to perform semantic tests can make data mining faster by pruning useless candidate patterns before their evaluation. We have also shown that the quality of the set of patterns produced may be improved: the patterns are more compact, and there are fewer patterns. We conclude that exploiting the semantics of a chosen representation formalism is key to the design and application of (onto-)relational frequent pattern discovery methods. Note: To appear in Theory and Practice of Logic Programming (TPLP)\n    ",
        "submission_date": "2010-03-13T00:00:00",
        "last_modified_date": "2010-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.3967",
        "title": "Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization",
        "authors": [
            "Daniel Golovin",
            "Andreas Krause"
        ],
        "abstract": "Solving stochastic optimization problems under partial observability, where one needs to adaptively make decisions with uncertain outcomes, is a fundamental but notoriously difficult challenge. In this paper, we introduce the concept of adaptive submodularity, generalizing submodular set functions to adaptive policies. We prove that if a problem satisfies this property, a simple adaptive greedy algorithm is guaranteed to be competitive with the optimal policy. In addition to providing performance guarantees for both stochastic maximization and coverage, adaptive submodularity can be exploited to drastically speed up the greedy algorithm by using lazy evaluations. We illustrate the usefulness of the concept by giving several examples of adaptive submodular objectives arising in diverse applications including sensor placement, viral marketing and active learning. Proving adaptive submodularity for these problems allows us to recover existing results in these applications as special cases, improve approximation guarantees and handle natural generalizations.\n    ",
        "submission_date": "2010-03-21T00:00:00",
        "last_modified_date": "2017-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.4140",
        "title": "Integrating Real-Time Analysis With The Dendritic Cell Algorithm Through Segmentation",
        "authors": [
            "Feng Gu",
            "Julie Greensmith",
            "Uwe Aickelin"
        ],
        "abstract": "As an immune inspired algorithm, the Dendritic Cell Algorithm (DCA) has been applied to a range of problems, particularly in the area of intrusion detection. Ideally, the intrusion detection should be performed in real-time, to continuously detect misuses as soon as they occur. Consequently, the analysis process performed by an intrusion detection system must operate in real-time or near-to real-time. The analysis process of the DCA is currently performed offline, therefore to improve the algorithm's performance we suggest the development of a real-time analysis component. The initial step of the development is to apply segmentation to the DCA. This involves segmenting the current output of the DCA into slices and performing the analysis in various ways. Two segmentation approaches are introduced and tested in this paper, namely antigen based segmentation (ABS) and time based segmentation (TBS). The results of the corresponding experiments suggest that applying segmentation produces different and significantly better results in some cases, when compared to the standard DCA without segmentation. Therefore, we conclude that the segmentation is applicable to the DCA for the purpose of real-time analysis.\n    ",
        "submission_date": "2010-03-22T00:00:00",
        "last_modified_date": "2010-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.4781",
        "title": "Large Margin Boltzmann Machines and Large Margin Sigmoid Belief Networks",
        "authors": [
            "Xu Miao",
            "Rajesh P.N. Rao"
        ],
        "abstract": "Current statistical models for structured prediction make simplifying assumptions about the underlying output graph structure, such as assuming a low-order Markov chain, because exact inference becomes intractable as the tree-width of the underlying graph increases. Approximate inference algorithms, on the other hand, force one to trade off representational power with computational efficiency. In this paper, we propose two new types of probabilistic graphical models, large margin Boltzmann machines (LMBMs) and large margin sigmoid belief networks (LMSBNs), for structured prediction. LMSBNs in particular  allow a very fast inference algorithm for arbitrary graph structures that runs in polynomial time with a high probability. This probability is data-distribution dependent and is maximized in learning. The new approach overcomes the representation-efficiency trade-off in previous models and allows fast structured prediction with complicated graph structures. We present results from applying a fully connected model to multi-label scene classification and demonstrate that the proposed approach can yield significant performance gains over current state-of-the-art methods.\n    ",
        "submission_date": "2010-03-25T00:00:00",
        "last_modified_date": "2010-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1003.5956",
        "title": "Unbiased Offline Evaluation of Contextual-bandit-based News Article Recommendation Algorithms",
        "authors": [
            "Lihong Li",
            "Wei Chu",
            "John Langford",
            "Xuanhui Wang"
        ],
        "abstract": "Contextual bandit algorithms have become popular for online recommendation systems such as Digg, Yahoo! Buzz, and news recommendation in general. \\emph{Offline} evaluation of the effectiveness of new algorithms in these applications is critical for protecting online user experiences but very challenging due to their \"partial-label\" nature. Common practice is to create a simulator which simulates the online environment for the problem at hand and then run an algorithm against this simulator. However, creating simulator itself is often difficult and modeling bias is usually unavoidably introduced. In this paper, we introduce a \\emph{replay} methodology for contextual bandit algorithm evaluation. Different from simulator-based approaches, our method is completely data-driven and very easy to adapt to different applications. More importantly, our method can provide provably unbiased evaluations. Our empirical results on a large-scale news article recommendation dataset collected from Yahoo! Front Page conform well with our theoretical results. Furthermore, comparisons between our offline replay and online bucket evaluation of several contextual bandit algorithms show accuracy and effectiveness of our offline evaluation method.\n    ",
        "submission_date": "2010-03-31T00:00:00",
        "last_modified_date": "2012-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.1061",
        "title": "On Tsallis Entropy Bias and Generalized Maximum Entropy Models",
        "authors": [
            "Yuexian Hou",
            "Tingxu Yan",
            "Peng Zhang",
            "Dawei Song",
            "Wenjie Li"
        ],
        "abstract": "In density estimation task, maximum entropy model (Maxent) can effectively use reliable prior information via certain constraints, i.e., linear constraints without empirical parameters. However, reliable prior information is often insufficient, and the selection of uncertain constraints becomes necessary but poses considerable implementation complexity. Improper setting of uncertain constraints can result in overfitting or underfitting. To solve this problem, a generalization of Maxent, under Tsallis entropy framework, is proposed. The proposed method introduces a convex quadratic constraint for the correction of (expected) Tsallis entropy bias (TEB). Specifically, we demonstrate that the expected Tsallis entropy of sampling distributions is smaller than the Tsallis entropy of the underlying real distribution. This expected entropy reduction is exactly the (expected) TEB, which can be expressed by a closed-form formula and act as a consistent and unbiased correction. TEB indicates that the entropy of a specific sampling distribution should be increased accordingly. This entails a quantitative re-interpretation of the Maxent principle. By compensating TEB and meanwhile forcing the resulting distribution to be close to the sampling distribution, our generalized TEBC Maxent can be expected to alleviate the overfitting and underfitting. We also present a connection between TEB and Lidstone estimator. As a result, TEB-Lidstone estimator is developed by analytically identifying the rate of probability correction in Lidstone. Extensive empirical evaluation shows promising performance of both TEBC Maxent and TEB-Lidstone in comparison with various state-of-the-art density estimation methods.\n    ",
        "submission_date": "2010-04-07T00:00:00",
        "last_modified_date": "2010-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.1230",
        "title": "Ontology-supported processing of clinical text using medical knowledge integration for multi-label classification of diagnosis coding",
        "authors": [
            "Phanu Waraporn",
            "Phayung Meesad",
            "Gareth Clayton"
        ],
        "abstract": "This paper discusses the knowledge integration of clinical information extracted from distributed medical ontology in order to ameliorate a machine learning-based multi-label coding assignment system. The proposed approach is implemented using a decision tree based cascade hierarchical technique on the university hospital data for patients with Coronary Heart Disease (CHD). The preliminary results obtained show a satisfactory finding.\n    ",
        "submission_date": "2010-04-08T00:00:00",
        "last_modified_date": "2010-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.1586",
        "title": "Belief Propagation for Min-cost Network Flow: Convergence and Correctness",
        "authors": [
            "David Gamarnik",
            "Devavrat Shah",
            "Yehua Wei"
        ],
        "abstract": "  Message passing type algorithms such as the so-called Belief Propagation algorithm have recently gained a lot of attention in the statistics, signal processing and machine learning communities as attractive algorithms for solving a variety of optimization and inference problems. As a decentralized, easy to implement and empirically successful algorithm, BP deserves attention from the theoretical standpoint, and here not much is known at the present stage. In order to fill this gap we consider the performance of the BP algorithm in the context of the capacitated minimum-cost network flow problem - the classical problem in the operations research field. We prove that BP converges to the optimal solution in the pseudo-polynomial time, provided that the optimal solution of the underlying problem is unique and the problem input is integral. Moreover, we present a simple modification of the BP algorithm which gives a fully polynomial-time randomized approximation scheme (FPRAS) for the same problem, which no longer requires the uniqueness of the optimal solution. This is the first instance where BP is proved to have fully-polynomial running time. Our results thus provide a theoretical justification for the viability of BP as an attractive method to solve an important class of optimization problems.\n    ",
        "submission_date": "2010-04-09T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.2027",
        "title": "Dynamic Policy Programming",
        "authors": [
            "Mohammad Gheshlaghi Azar",
            "Vicenc Gomez",
            "Hilbert J. Kappen"
        ],
        "abstract": "In this paper, we propose a novel policy iteration method, called dynamic policy programming (DPP), to estimate the optimal policy in the infinite-horizon Markov decision processes. We prove the finite-iteration and asymptotic l\\infty-norm performance-loss bounds for DPP in the presence of approximation/estimation error. The bounds are expressed in terms of the l\\infty-norm of the average accumulated error as opposed to the l\\infty-norm of the error in the case of the standard approximate value iteration (AVI) and the approximate policy iteration (API). This suggests that DPP can achieve a better performance than AVI and API since it averages out the simulation noise caused by Monte-Carlo sampling throughout the learning process. We examine this theoretical results numerically by com- paring the performance of the approximate variants of DPP with existing reinforcement learning (RL) methods on different problem domains. Our results show that, in all cases, DPP-based algorithms outperform other RL methods by a wide margin.\n    ",
        "submission_date": "2010-04-12T00:00:00",
        "last_modified_date": "2011-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.2304",
        "title": "Spatio-Temporal Graphical Model Selection",
        "authors": [
            "Patrick L. Harrington Jr.",
            "Alfred O. Hero III"
        ],
        "abstract": "We consider the problem of estimating the topology of spatial interactions in a discrete state, discrete time spatio-temporal graphical model where the interactions affect the temporal evolution of each agent in a network.  Among other models, the susceptible, infected, recovered ($SIR$) model for interaction events fall into this framework.  We pose the problem as a structure learning problem and solve it using an $\\ell_1$-penalized likelihood convex program.  We evaluate the solution on a simulated spread of infectious over a complex network.  Our topology estimates outperform those of a standard spatial Markov random field graphical model selection using $\\ell_1$-regularized logistic regression.\n    ",
        "submission_date": "2010-04-14T00:00:00",
        "last_modified_date": "2010-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.3147",
        "title": "Genetic Algorithms for Multiple-Choice Problems",
        "authors": [
            "Uwe Aickelin"
        ],
        "abstract": "This thesis investigates the use of problem-specific knowledge to enhance a genetic algorithm approach to multiple-choice optimisation ",
        "submission_date": "2010-04-19T00:00:00",
        "last_modified_date": "2010-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.3390",
        "title": "Publishing Math Lecture Notes as Linked Data",
        "authors": [
            "Catalin David",
            "Michael Kohlhase",
            "Christoph Lange",
            "Florian Rabe",
            "Nikita Zhiltsov",
            "Vyacheslav Zholudev"
        ],
        "abstract": "We mark up a corpus of LaTeX lecture notes semantically and expose them as Linked Data in XHTML+MathML+RDFa.   Our application makes the resulting documents interactively browsable for   students.  Our ontology helps to answer queries from students and lecturers, and paves   the path towards an integration of our corpus with external sites.\n    ",
        "submission_date": "2010-04-20T00:00:00",
        "last_modified_date": "2010-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.3478",
        "title": "Learning Better Context Characterizations: An Intelligent Information Retrieval Approach",
        "authors": [
            "Carlos M. Lorenzetti",
            "Ana G. Maguitman"
        ],
        "abstract": "This paper proposes an incremental method that can be used by an intelligent system to learn better descriptions of a thematic context. The method starts with a small number of terms selected from a simple description of the topic under analysis and uses this description as the initial search context. Using these terms, a set of queries are built and submitted to a search engine. New documents and terms are used to refine the learned vocabulary. Evaluations performed on a large number of topics indicate that the learned vocabulary is much more effective than the original one at the time of constructing queries to retrieve relevant material.\n    ",
        "submission_date": "2010-04-20T00:00:00",
        "last_modified_date": "2010-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.3568",
        "title": "Integrating User's Domain Knowledge with Association Rule Mining",
        "authors": [
            "Vikram Singh",
            "Sapna Nagpal"
        ],
        "abstract": "This paper presents a variation of Apriori algorithm that includes the role of domain expert to guide and speed up the overall knowledge discovery task. Usually, the user is interested in finding relationships between certain attributes instead of the whole dataset. Moreover, he can help the mining algorithm to select the target database which in turn takes less time to find the desired association rules. Variants of the standard Apriori and Interactive Apriori algorithms have been run on artificial datasets. The results show that incorporating user's preference in selection of target attribute helps to search the association rules efficiently both in terms of space and time.\n    ",
        "submission_date": "2010-04-20T00:00:00",
        "last_modified_date": "2010-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.3708",
        "title": "Parcellation of fMRI Datasets with ICA and PLS-A Data Driven Approach",
        "authors": [
            "Yongnan Ji",
            "Pierre-Yves Herve",
            "Uwe Aickelin",
            "Alain Pitiot"
        ],
        "abstract": "Inter-subject parcellation of functional Magnetic Resonance Imaging (fMRI) data based on a standard General Linear Model (GLM)and spectral clustering was recently proposed as a means to alleviate the issues associated with spatial normalization in fMRI. However, for all its appeal, a GLM-based parcellation approach introduces its own biases, in the form of a priori knowledge about the shape of Hemodynamic Response Function (HRF) and task-related signal changes, or about the subject behaviour during the task. In this paper, we introduce a data-driven version of the spectral clustering parcellation, based on Independent Component Analysis (ICA) and Partial Least Squares (PLS) instead of the GLM. First, a number of independent components are automatically selected. Seed voxels are then obtained from the associated ICA maps and we compute the PLS latent variables between the fMRI signal of the seed voxels (which covers regional variations of the HRF) and the principal components of the signal across all voxels. Finally, we parcellate all subjects data with a spectral clustering of the PLS latent variables. We present results of the application of the proposed method on both single-subject and multi-subject fMRI datasets. Preliminary experimental results, evaluated with intra-parcel variance of GLM t-values and PLS derived t-values, indicate that this data-driven approach offers improvement in terms of parcellation accuracy over GLM based techniques.\n    ",
        "submission_date": "2010-04-21T00:00:00",
        "last_modified_date": "2010-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.3809",
        "title": "Artificial Immune Systems Metaphor for Agent Based Modeling of Crisis Response Operations",
        "authors": [
            "Khaled M. Khalil",
            "M. Abdel-Aziz",
            "Taymour T. Nazmy",
            "Abdel-Badeeh M. Salem"
        ],
        "abstract": "Crisis response requires information intensive efforts utilized for reducing uncertainty, calculating and comparing costs and benefits, and managing resources in a fashion beyond those regularly available to handle routine problems. This paper presents an Artificial Immune Systems (AIS) metaphor for agent based modeling of crisis response operations. The presented model proposes integration of hybrid set of aspects (multi-agent systems, built-in defensive model of AIS, situation management, and intensity-based learning) for crisis response operations. In addition, the proposed response model is applied on the spread of pandemic influenza in Egypt as a case study.\n    ",
        "submission_date": "2010-04-21T00:00:00",
        "last_modified_date": "2010-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.5071",
        "title": "Dimensions of Formality: A Case Study for MKM in Software Engineering",
        "authors": [
            "Andrea Kohlhase",
            "Michael Kohlhase",
            "Christoph Lange"
        ],
        "abstract": "We study the formalization of a collection of documents created for a Software Engineering project from an MKM perspective. We analyze how document and collection markup formats can cope with an open-ended, multi-dimensional space of primary and secondary classifications and relationships. We show that RDFa-based extensions of MKM formats, employing flexible \"metadata\" relationships referencing specific vocabularies for distinct dimensions, are well-suited to encode this and to put it into service. This formalized knowledge can be used for enriching interactive document browsing, for enabling multi-dimensional metadata queries over documents and collections, and for exporting Linked Data to the Semantic Web and thus enabling further reuse.\n    ",
        "submission_date": "2010-04-28T00:00:00",
        "last_modified_date": "2010-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.5326",
        "title": "Designing neural networks that process mean values of random variables",
        "authors": [
            "Michael J. Barber",
            "John W. Clark"
        ],
        "abstract": "We introduce a class of neural networks derived from probabilistic models in the form of Bayesian networks. By imposing additional assumptions about the nature of the probabilistic models represented in the networks, we derive neural networks with standard dynamics that require no training to determine the synaptic weights, that perform accurate calculation of the mean values of the random variables, that can pool multiple sources of evidence, and that deal cleanly and consistently with inconsistent or contradictory evidence. The presented neural networks capture many properties of Bayesian networks, providing distributed versions of probabilistic models.\n    ",
        "submission_date": "2010-04-29T00:00:00",
        "last_modified_date": "2010-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.5339",
        "title": "Query strategy for sequential ontology debugging",
        "authors": [
            "Kostyantyn Shchekotykhin",
            "Gerhard Friedrich",
            "Philipp Fleiss",
            "Patrick Rodler"
        ],
        "abstract": "Debugging of ontologies is an important prerequisite for their wide-spread application, especially in areas that rely upon everyday users to create and maintain knowledge bases, as in the case of the Semantic Web. Recent approaches use diagnosis methods to identify causes of inconsistent or incoherent ontologies. However, in most debugging scenarios these methods return many alternative diagnoses, thus placing the burden of fault localization on the user. This paper demonstrates how the target diagnosis can be identified by performing a sequence of observations, that is, by querying an oracle about entailments of the target ontology. We exploit a-priori probabilities of typical user errors to formulate information-theoretic concepts for query selection. Our evaluation showed that the proposed method significantly reduces the number of required queries compared to myopic strategies. We experimented with different probability distributions of user errors and different qualities of the a-priori probabilities. Our measurements showed the advantageousness of information-theoretic approach to query selection even in cases where only a rough estimate of the priors is available.\n    ",
        "submission_date": "2010-04-29T00:00:00",
        "last_modified_date": "2011-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1004.5500",
        "title": "Simple Type Theory as Framework for Combining Logics",
        "authors": [
            "Christoph Benzmueller"
        ],
        "abstract": " Simple type theory is suited as framework for combining classical and non-classical logics. This claim is based on the observation that various prominent logics, including (quantified) multimodal logics and intuitionistic logics, can be elegantly  embedded in simple type theory. Furthermore, simple type theory is sufficiently expressive to model combinations of embedded logics and it has a well understood semantics. Off-the-shelf reasoning systems for simple type theory exist that can be uniformly employed for reasoning within and about combinations of logics.\n    ",
        "submission_date": "2010-04-30T00:00:00",
        "last_modified_date": "2010-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.0125",
        "title": "Adaptive Bases for Reinforcement Learning",
        "authors": [
            "Dotan Di Castro",
            "Shie Mannor"
        ],
        "abstract": "We consider the problem of reinforcement learning using function approximation, where the approximating basis can change dynamically while interacting with the environment. A motivation for such an approach is maximizing the value function fitness to the problem faced. Three errors are considered: approximation square error, Bellman residual, and projected Bellman residual. Algorithms under the actor-critic framework are presented, and shown to converge. The advantage of such an adaptive basis is demonstrated in simulations.\n    ",
        "submission_date": "2010-05-02T00:00:00",
        "last_modified_date": "2010-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.0530",
        "title": "Feature Selection with Conjunctions of Decision Stumps and Learning from Microarray Data",
        "authors": [
            "Mohak Shah",
            "Mario Marchand",
            "Jacques Corbeil"
        ],
        "abstract": "One of the objectives of designing feature selection learning algorithms is to obtain classifiers that depend on a small number of attributes and have verifiable future performance guarantees. There are few, if any, approaches that successfully address the two goals simultaneously. Performance guarantees become crucial for tasks such as microarray data analysis due to very small sample sizes resulting in limited empirical evaluation. To the best of our knowledge, such algorithms that give theoretical bounds on the future performance have not been proposed so far in the context of the classification of gene expression data. In this work, we investigate the premise of learning a conjunction (or disjunction) of decision stumps in Occam's Razor, Sample Compression, and PAC-Bayes learning settings for identifying a small subset of attributes that can be used to perform reliable classification tasks. We apply the proposed approaches for gene identification from DNA microarray data and compare our results to those of well known successful approaches proposed for the task. We show that our algorithm not only finds hypotheses with much smaller number of genes while giving competitive classification accuracy but also have tight risk guarantees on future performance unlike other approaches. The proposed approaches are general and extensible in terms of both designing novel algorithms and application to other domains.\n    ",
        "submission_date": "2010-05-04T00:00:00",
        "last_modified_date": "2010-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.0749",
        "title": "Integrating multiple sources to answer questions in Algebraic Topology",
        "authors": [
            "Jonathan Heras",
            "Vico Pascual",
            "Ana Romero",
            "Julio Rubio"
        ],
        "abstract": "We present in this paper an evolution of a tool from a user interface for a concrete Computer Algebra system for Algebraic Topology (the Kenzo system), to a front-end allowing the interoperability among different sources for computation and deduction. The architecture allows the system not only to interface several systems, but also to make them cooperate in shared calculations.\n    ",
        "submission_date": "2010-04-27T00:00:00",
        "last_modified_date": "2010-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.0957",
        "title": "ECG Feature Extraction Techniques - A Survey Approach",
        "authors": [
            "S. Karpagachelvi",
            "M.Arthanari",
            "M. Sivakumar"
        ],
        "abstract": "ECG Feature Extraction plays a significant role in diagnosing most of the cardiac diseases. One cardiac cycle in an ECG signal consists of the P-QRS-T waves. This feature extraction scheme determines the amplitudes and intervals in the ECG signal for subsequent analysis. The amplitudes and intervals value of P-QRS-T segment determines the functioning of heart of every human. Recently, numerous research and techniques have been developed for analyzing the ECG signal. The proposed schemes were mostly based on Fuzzy Logic Methods, Artificial Neural Networks (ANN), Genetic Algorithm (GA), Support Vector Machines (SVM), and other Signal Analysis techniques. All these techniques and algorithms have their advantages and limitations. This proposed paper discusses various techniques and transformations proposed earlier in literature for extracting feature from an ECG signal. In addition this paper also provides a comparative study of various methods proposed by researchers in extracting the feature from ECG signal.\n    ",
        "submission_date": "2010-05-06T00:00:00",
        "last_modified_date": "2010-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.1684",
        "title": "On Macroscopic Complexity and Perceptual Coding",
        "authors": [
            "John Scoville"
        ],
        "abstract": "The theoretical limits of 'lossy' data compression algorithms are considered. The complexity of an object as seen by a macroscopic observer is the size of the perceptual code which discards all information that can be lost without altering the perception of the specified observer. The complexity of this macroscopically observed state is the simplest description of any microstate comprising that macrostate. Inference and pattern recognition based on macrostate rather than microstate complexities will take advantage of the complexity of the macroscopic observer to ignore irrelevant noise.\n    ",
        "submission_date": "2010-05-10T00:00:00",
        "last_modified_date": "2011-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.1934",
        "title": "Scalable Probabilistic Databases with Factor Graphs and MCMC",
        "authors": [
            "Michael Wick",
            "Andrew McCallum",
            "Gerome Miklau"
        ],
        "abstract": "Probabilistic databases play a crucial role in the management and understanding of uncertain data. However, incorporating probabilities into the semantics of incomplete databases has posed many challenges, forcing systems to sacrifice modeling power, scalability, or restrict the class of relational algebra formula under which they are closed. We propose an alternative approach where the underlying relational database always represents a single world, and an external factor graph encodes a distribution over possible worlds; Markov chain Monte Carlo (MCMC) inference is then used to recover this uncertainty to a desired level of fidelity. Our approach allows the efficient evaluation of arbitrary queries over probabilistic databases with arbitrary dependencies expressed by graphical models with structure that changes during inference. MCMC sampling provides efficiency by hypothesizing {\\em modifications} to possible worlds rather than generating entire worlds from scratch. Queries are then run over the portions of the world that change, avoiding the onerous cost of running full queries over each sampled world. A significant innovation of this work is the connection between MCMC sampling and materialized view maintenance techniques: we find empirically that using view maintenance techniques is several orders of magnitude faster than naively querying each sampled world. We also demonstrate our system's ability to answer relational queries with aggregation, and demonstrate additional scalability through the use of parallelization.\n    ",
        "submission_date": "2010-05-11T00:00:00",
        "last_modified_date": "2010-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.2303",
        "title": "Towards Physarum Binary Adders",
        "authors": [
            "Jeff Jones",
            "Andrew Adamatzky"
        ],
        "abstract": "Plasmodium of \\emph{Physarum polycephalum} is a single cell visible by unaided eye. The plasmodium's foraging behaviour is interpreted in terms of computation. Input data is a configuration of nutrients, result of computation is a network of plasmodium's cytoplasmic tubes spanning sources of nutrients. Tsuda et al (2004) experimentally demonstrated that basic logical gates can be implemented in foraging behaviour of the plasmodium. We simplify the original designs of the gates and show --- in computer models --- that the plasmodium is capable for computation of two-input two-output gate $<x, y> \\to <xy, x+y>$ and  three-input two-output $<x, y, z> \\to < \\bar{x}yz, x+y+z>$. We assemble the gates in a binary one-bit adder and demonstrate validity of the design using computer simulation.\n    ",
        "submission_date": "2010-05-13T00:00:00",
        "last_modified_date": "2010-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.4032",
        "title": "Combining Multiple Feature Extraction Techniques for Handwritten Devnagari Character Recognition",
        "authors": [
            "Sandhya Arora",
            "Debotosh Bhattacharjee",
            "Mita Nasipuri",
            "Dipak Kumar Basu",
            "Mahantapas Kundu"
        ],
        "abstract": "In this paper we present an OCR for Handwritten Devnagari Characters. Basic symbols are recognized by neural classifier. We have used four feature extraction techniques namely, intersection, shadow feature, chain code histogram and straight line fitting features. Shadow features are computed globally for character image while intersection features, chain code histogram features and line fitting features are computed by dividing the character image into different segments. Weighted majority voting technique is used for combining the classification decision obtained from four Multi Layer Perceptron(MLP) based classifier. On experimentation with a dataset of 4900 samples the overall recognition rate observed is 92.80% as we considered top five choices results. This method is compared with other recent methods for Handwritten Devnagari Character Recognition and it has been observed that this approach has better success rate than other methods.\n    ",
        "submission_date": "2010-05-21T00:00:00",
        "last_modified_date": "2010-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.4446",
        "title": "Genetic algorithms and the art of Zen",
        "authors": [
            "Jack Coldridge",
            "Martyn Amos"
        ],
        "abstract": "In this paper we present a novel genetic algorithm (GA) solution to a simple yet challenging commercial puzzle game known as the Zen Puzzle Garden (ZPG). We describe the game in detail, before presenting a suitable encoding scheme and fitness function for candidate solutions. We then compare the performance of the genetic algorithm with that of the A* algorithm. Our results show that the GA is competitive with informed search in terms of solution quality, and significantly out-performs it in terms of computational resource requirements. We conclude with a brief discussion of the implications of our findings for game solving and other \"real world\" problems.\n    ",
        "submission_date": "2010-05-24T00:00:00",
        "last_modified_date": "2010-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.5253",
        "title": "Using Soft Constraints To Learn Semantic Models Of Descriptions Of Shapes",
        "authors": [
            "Sergio Guadarrama",
            "David P. Pancho"
        ],
        "abstract": "The contribution of this paper is to provide a semantic model (using soft constraints) of the words used by web-users to describe objects in a language game; a game in which one user describes a selected object of those composing the scene, and another user has to guess which object has been described. The given description needs to be non ambiguous and accurate enough to allow other users to guess the described shape correctly.\n",
        "submission_date": "2010-05-28T00:00:00",
        "last_modified_date": "2010-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1005.5556",
        "title": "Empirical learning aided by weak domain knowledge in the form of feature importance",
        "authors": [
            "Ridwan Al Iqbal"
        ],
        "abstract": "Standard hybrid learners that use domain knowledge require stronger knowledge that is hard and expensive to acquire. However, weaker domain knowledge can benefit from prior knowledge while being cost effective. Weak knowledge in the form of feature relative importance (FRI) is presented and explained. Feature relative importance is a real valued approximation of a feature's importance provided by experts. Advantage of using this knowledge is demonstrated by IANN, a modified multilayer neural network algorithm. IANN is a very simple modification of standard neural network algorithm but attains significant performance gains. Experimental results in the field of molecular biology show higher performance over other empirical learning algorithms including standard backpropagation and support vector machines. IANN performance is even comparable to a theory refinement system KBANN that uses stronger domain knowledge. This shows Feature relative importance can improve performance of existing empirical learning algorithms significantly with minimal effort.\n    ",
        "submission_date": "2010-05-30T00:00:00",
        "last_modified_date": "2010-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.0289",
        "title": "M\u00e9todos para la Selecci\u00f3n y el Ajuste de Caracter\u00edsticas en el Problema de la Detecci\u00f3n de Spam",
        "authors": [
            "Carlos M. Lorenzetti",
            "Roc\u00edo L. Cecchini",
            "Ana G. Maguitman",
            "Andr\u00e1s A. Bencz\u00far"
        ],
        "abstract": "The email is used daily by millions of people to communicate around the globe and it is a mission-critical application for many businesses. Over the last decade, unsolicited bulk email has become a major problem for email users. An overwhelming amount of spam is flowing into users' mailboxes daily. In 2004, an estimated 62% of all email was attributed to spam. Spam is not only frustrating for most email users, it strains the IT infrastructure of organizations and costs businesses billions of dollars in lost productivity. In recent years, spam has evolved from an annoyance into a serious security threat, and is now a prime medium for phishing of sensitive information, as well the spread of malicious software. This work presents a first approach to attack the spam problem. We propose an algorithm that will improve a classifier's results by adjusting its training set data. It improves the document's vocabulary representation by detecting good topic descriptors and discriminators.\n    ",
        "submission_date": "2010-06-02T00:00:00",
        "last_modified_date": "2010-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.1328",
        "title": "Uncovering the Riffled Independence Structure of Rankings",
        "authors": [
            "Jonathan Huang",
            "Carlos Guestrin"
        ],
        "abstract": "Representing distributions over permutations can be a daunting task due to the fact that the number of permutations of $n$ objects scales factorially in $n$. One recent way that has been used to reduce storage complexity has been to exploit probabilistic independence, but as we argue, full independence assumptions impose strong sparsity constraints on distributions and are unsuitable for modeling rankings. We identify a novel class of independence structures, called \\emph{riffled independence}, encompassing a more expressive family of distributions while retaining many of the properties necessary for performing efficient inference and reducing sample complexity. In riffled independence, one draws two permutations independently, then performs the \\emph{riffle shuffle}, common in card games, to combine the two permutations to form a single permutation. Within the context of ranking, riffled independence corresponds to ranking disjoint sets of objects independently, then interleaving those rankings. In this paper, we provide a formal introduction to riffled independence and present algorithms for using riffled independence within Fourier-theoretic frameworks which have been explored by a number of recent papers. Additionally, we propose an automated method for discovering sets of items which are riffle independent from a training set of rankings. We show that our clustering-like algorithms can be used to discover meaningful latent coalitions from real preference ranking datasets and to learn the structure of hierarchically decomposable models based on riffled independence.\n    ",
        "submission_date": "2010-06-07T00:00:00",
        "last_modified_date": "2010-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.1407",
        "title": "Begin, After, and Later: a Maximal Decidable Interval Temporal Logic",
        "authors": [
            "Davide Bresolin",
            "Pietro Sala",
            "Guido Sciavicco"
        ],
        "abstract": "Interval temporal logics (ITLs) are logics for reasoning about temporal statements expressed over intervals, i.e., periods of time. The most famous ITL studied so far is Halpern and Shoham's HS, which is the logic of the thirteen Allen's interval relations. Unfortunately, HS and most of its fragments have an undecidable satisfiability problem. This discouraged the research in this area until recently, when a number non-trivial decidable ITLs have been discovered. \n",
        "submission_date": "2010-06-08T00:00:00",
        "last_modified_date": "2010-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.1434",
        "title": "Computing by Means of Physics-Based Optical Neural Networks",
        "authors": [
            "A. Steven Younger",
            "Emmett Redd"
        ],
        "abstract": "We report recent research on computing with biology-based neural network models by means of physics-based opto-electronic hardware.  New technology provides opportunities for very-high-speed computation and uncovers problems obstructing the wide-spread use of this new capability.  The Computation Modeling community may be able to offer solutions to these cross-boundary research problems.\n    ",
        "submission_date": "2010-06-08T00:00:00",
        "last_modified_date": "2010-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.1692",
        "title": "Measuring interesting rules in Characteristic rule",
        "authors": [
            "Spits Warnars"
        ],
        "abstract": "Finding interesting rule in the sixth strategy step about threshold control on generalized relations in attribute oriented induction, there is possibility to select candidate attribute for further generalization and merging of identical tuples until the number of tuples is no greater than the threshold value, as implemented in basic attribute oriented induction algorithm. At this strategy step there is possibility the number of tuples in final generalization result still greater than threshold value. In order to get the final generalization result which only small number of tuples and can be easy to transfer into simple logical formula, the seventh strategy step about rule transformation is evolved where there will be simplification by unioning or grouping the identical attribute. Our approach to measure interesting rule is opposite with heuristic measurement approach by Fudger and Hamilton where the more complex concept hierarchies, more interesting results are likely to be found, but our approach the simpler concept hierarchies, more interesting results are likely to be found and the more complex concept hierarchies, more complex process generalization in concept tree. The decision to find interesting rule is influenced with wide or length and depth or level of concept tree.\n    ",
        "submission_date": "2010-06-09T00:00:00",
        "last_modified_date": "2010-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.1828",
        "title": "Landau Theory of Adaptive Integration in Computational Intelligence",
        "authors": [
            "Dariusz Plewczynski"
        ],
        "abstract": "Computational Intelligence (CI) is a sub-branch of Artificial Intelligence paradigm focusing on the study of adaptive mechanisms to enable or facilitate intelligent behavior in complex and changing environments. There are several paradigms of CI [like artificial neural networks, evolutionary computations, swarm intelligence, artificial immune systems, fuzzy systems and many others], each of these has its origins in biological systems [biological neural systems, natural Darwinian evolution, social behavior, immune system, interactions of organisms with their environment]. Most of those paradigms evolved into separate machine learning (ML) techniques, where probabilistic methods are used complementary with CI techniques in order to effectively combine elements of learning, adaptation, evolution and Fuzzy logic to create heuristic algorithms that are, in some sense, intelligent. The current trend is to develop consensus techniques, since no single machine learning algorithms is superior to others in all possible situations. In order to overcome this problem several meta-approaches were proposed in ML focusing on the integration of results from different methods into single prediction. We discuss here the Landau theory for the nonlinear equation that can describe the adaptive integration of information acquired from an ensemble of independent learning agents. The influence of each individual agent on other learners is described similarly to the social impact theory. The final decision outcome for the consensus system is calculated using majority rule in the stationary limit, yet the minority solutions can survive inside the majority population as the complex intermittent clusters of opposite opinion.\n    ",
        "submission_date": "2010-06-09T00:00:00",
        "last_modified_date": "2010-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.1916",
        "title": "Building Computer Network Attacks",
        "authors": [
            "Ariel Futoransky",
            "Luciano Notarfrancesco",
            "Gerardo Richarte",
            "Carlos Sarraute"
        ],
        "abstract": "In this work we start walking the path to a new perspective for viewing cyberwarfare scenarios, by introducing conceptual tools (a formal model) to evaluate the costs of an attack, to describe the theater of operations, targets, missions, actions, plans and assets involved in cyberwarfare attacks. We also describe two applications of this model: autonomous planning leading to automated penetration tests, and attack simulations, allowing a system administrator to evaluate the vulnerabilities of his network.\n    ",
        "submission_date": "2010-06-09T00:00:00",
        "last_modified_date": "2010-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.2165",
        "title": "A Probabilistic Perspective on Gaussian Filtering and Smoothing",
        "authors": [
            "Marc Peter Deisenroth",
            "Henrik Ohlsson"
        ],
        "abstract": "We present a general probabilistic perspective on Gaussian filtering and smoothing. This allows us to show that common approaches to Gaussian filtering/smoothing can be distinguished solely by their methods of computing/approximating the means and covariances of joint probabilities. This implies that novel filters and smoothers can be derived straightforwardly by providing methods for computing these moments. Based on this insight, we derive the cubature Kalman smoother and propose a novel robust filtering and smoothing algorithm based on Gibbs sampling.\n    ",
        "submission_date": "2010-06-10T00:00:00",
        "last_modified_date": "2011-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.2495",
        "title": "Mirrored Language Structure and Innate Logic of the Human Brain as a Computable Model of the Oracle Turing Machine",
        "authors": [
            "Han Xiao Wen"
        ],
        "abstract": "We wish to present a mirrored language structure (MLS) and four logic rules determined by this structure for the model of a computable Oracle Turing machine. MLS has novel features that are of considerable biological and computational significance. It suggests an algorithm of relation learning and recognition (RLR) that enables the deterministic computers to simulate the mechanism of the Oracle Turing machine, or P = NP in a mathematical term.\n    ",
        "submission_date": "2010-06-12T00:00:00",
        "last_modified_date": "2010-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.2844",
        "title": "Outrepasser les limites des techniques classiques de Prise d'Empreintes grace aux Reseaux de Neurones",
        "authors": [
            "Javier Burroni",
            "Carlos Sarraute"
        ],
        "abstract": "We present an application of Artificial Intelligence techniques to the field of Information Security. The problem of remote Operating System (OS) Detection, also called OS Fingerprinting, is a crucial step of the penetration testing process, since the attacker (hacker or security professional) needs to know the OS of the target host in order to choose the exploits that he will use. OS Detection is accomplished by passively sniffing network packets and actively sending test packets to the target host, to study specific variations in the host responses revealing information about its operating system.\n",
        "submission_date": "2010-06-14T00:00:00",
        "last_modified_date": "2010-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.2899",
        "title": "Approximated Structured Prediction for Learning Large Scale Graphical Models",
        "authors": [
            "Tamir Hazan",
            "Raquel Urtasun"
        ],
        "abstract": "This manuscripts contains the proofs for \"A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction\".\n    ",
        "submission_date": "2010-06-15T00:00:00",
        "last_modified_date": "2012-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.3678",
        "title": "Functional Answer Set Programming",
        "authors": [
            "Pedro Cabalar"
        ],
        "abstract": "In this paper we propose an extension of Answer Set Programming (ASP), and in particular, of its most general logical counterpart, Quantified Equilibrium Logic (QEL), to deal with partial functions. Although the treatment of equality in QEL can be established in different ways, we first analyse the choice of decidable equality with complete functions and Herbrand models, recently proposed in the literature. We argue that this choice yields some counterintuitive effects from a logic programming and knowledge representation point of view. We then propose a variant called QELF where the set of functions is partitioned into partial and Herbrand functions (we also call constructors). In the rest of the paper, we show a direct connection to Scott's Logic of Existence and present a practical application, proposing an extension of normal logic programs to deal with partial functions and equality, so that they can be translated into function-free normal programs, being possible in this way to compute their answer sets with any standard ASP solver.\n    ",
        "submission_date": "2010-06-18T00:00:00",
        "last_modified_date": "2010-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.4039",
        "title": "Distributed Autonomous Online Learning: Regrets and Intrinsic Privacy-Preserving Properties",
        "authors": [
            "Feng Yan",
            "Shreyas Sundaram",
            "S. V. N. Vishwanathan",
            "Yuan Qi"
        ],
        "abstract": "Online learning has become increasingly popular on handling massive data. The sequential nature of online learning, however, requires a centralized learner to store data and update parameters. In this paper, we consider online learning with {\\em distributed} data sources. The autonomous learners update local parameters based on local data sources and periodically exchange information with a small subset of neighbors in a communication network. We derive the regret bound for strongly convex functions that generalizes the work by Ram et al. (2010) for convex functions. Most importantly, we show that our algorithm has \\emph{intrinsic} privacy-preserving properties, and we prove the sufficient and necessary conditions for privacy preservation in the network. These conditions imply that for networks with greater-than-one connectivity, a malicious learner cannot reconstruct the subgradients (and sensitive raw data) of other learners, which makes our algorithm appealing in privacy sensitive applications.\n    ",
        "submission_date": "2010-06-21T00:00:00",
        "last_modified_date": "2011-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.4474",
        "title": "sTeX+ - a System for Flexible Formalization of Linked Data",
        "authors": [
            "Andrea Kohlhase",
            "Michael Kohlhase",
            "Christoph Lange"
        ],
        "abstract": "We present the sTeX+ system, a user-driven advancement of sTeX - a semantic extension of LaTeX that allows for producing high-quality PDF documents for (proof)reading and printing, as well as semantic XML/OMDoc documents for the Web or further processing. Originally sTeX had been created as an invasive, semantic frontend for authoring XML documents. Here, we used sTeX in a Software Engineering case study as a formalization tool. In order to deal with modular pre-semantic vocabularies and relations, we upgraded it to sTeX+ in a participatory design process. We present a tool chain that starts with an sTeX+ editor and ultimately serves the generated documents as XHTML+RDFa Linked Data via an OMDoc-enabled, versioned XML database. In the final output, all structural annotations are preserved in order to enable semantic information retrieval services.\n    ",
        "submission_date": "2010-06-23T00:00:00",
        "last_modified_date": "2010-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.4540",
        "title": "A Novel Rough Set Reduct Algorithm for Medical Domain Based on Bee Colony Optimization",
        "authors": [
            "N. Suguna",
            "K. Thanushkodi"
        ],
        "abstract": "Feature selection refers to the problem of selecting relevant features which produce the most predictive outcome. In particular, feature selection task is involved in datasets containing huge number of features. Rough set theory has been one of the most successful methods used for feature selection. However, this method is still not able to find optimal subsets. This paper proposes a new feature selection method based on Rough set theory hybrid with Bee Colony Optimization (BCO) in an attempt to combat this. This proposed work is applied in the medical domain to find the minimal reducts and experimentally compared with the Quick Reduct, Entropy Based Reduct, and other hybrid Rough Set methods such as Genetic Algorithm (GA), Ant Colony Optimization (ACO) and Particle Swarm Optimization (PSO).\n    ",
        "submission_date": "2010-06-23T00:00:00",
        "last_modified_date": "2010-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.4645",
        "title": "SPOT: An R Package For Automatic and Interactive Tuning of Optimization Algorithms by Sequential Parameter Optimization",
        "authors": [
            "Thomas Bartz-Beielstein"
        ],
        "abstract": "The sequential parameter optimization (SPOT) package for R is a toolbox for tuning and understanding simulation and optimization algorithms. Model-based investigations are common approaches in simulation and optimization. Sequential parameter optimization has been developed, because there is a strong need for sound statistical analysis of simulation and optimization algorithms. SPOT includes methods for tuning based on classical regression and analysis of variance techniques; tree-based models such as CART and random forest; Gaussian process models (Kriging), and combinations of different meta-modeling approaches. This article exemplifies how SPOT can be used for automatic and interactive tuning.\n    ",
        "submission_date": "2010-06-23T00:00:00",
        "last_modified_date": "2010-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1006.4948",
        "title": "Automatic Music Composition using Answer Set Programming",
        "authors": [
            "Georg Boenn",
            "Martin Brain",
            "Marina De Vos",
            "John ffitch"
        ],
        "abstract": "Music composition used to be a pen and paper activity. These these days music is often composed with the aid of computer software, even to the point where the computer compose parts of the score autonomously. The composition of most styles of music is governed by rules. We show that by approaching the automation, analysis and verification of composition as a knowledge representation task and formalising these rules in a suitable logical language, powerful and expressive intelligent composition tools can be easily built. This application paper describes the use of answer set programming to construct an automated system, named ANTON, that can compose melodic, harmonic and rhythmic music, diagnose errors in human compositions and serve as a computer-aided composition tool. The combination of harmonic, rhythmic and melodic composition in a single framework makes ANTON unique in the growing area of algorithmic composition. With near real-time composition, ANTON reaches the point where it can not only be used as a component in an interactive composition tool but also has the potential for live performances and concerts or automatically generated background music in a variety of applications. With the use of a fully declarative language and an \"off-the-shelf\" reasoning engine, ANTON provides the human composer a tool which is significantly simpler, more compact and more versatile than other existing systems. This paper has been accepted for publication in Theory and Practice of Logic Programming (TPLP).\n    ",
        "submission_date": "2010-06-25T00:00:00",
        "last_modified_date": "2010-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.1268",
        "title": "Application of Data Mining to Network Intrusion Detection: Classifier Selection Model",
        "authors": [
            "Huy Nguyen",
            "Deokjai Choi"
        ],
        "abstract": "As network attacks have increased in number and severity over the past few years, intrusion detection system (IDS) is increasingly becoming a critical component to secure the network. Due to large volumes of security audit data as well as complex and dynamic properties of intrusion behaviors, optimizing performance of IDS becomes an important open problem that is receiving more and more attention from the research community. The uncertainty to explore if certain algorithms perform better for certain attack classes constitutes the motivation for the reported herein. In this paper, we evaluate performance of a comprehensive set of classifier algorithms using KDD99 dataset. Based on evaluation results, best algorithms for each attack category is chosen and two classifier algorithm selection models are proposed. The simulation result comparison indicates that noticeable performance improvement and real-time intrusion detection can be achieved as we apply the proposed models to detect different kinds of network attacks.\n    ",
        "submission_date": "2010-07-08T00:00:00",
        "last_modified_date": "2010-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.1270",
        "title": "How to Maximize User Satisfaction Degree in Multi-service IP Networks",
        "authors": [
            "Huy Nguyen",
            "Tam Van Nguyen",
            "Deokjai Choi"
        ],
        "abstract": "Bandwidth allocation is a fundamental problem in communication networks. With current network moving towards the Future Internet model, the problem is further intensified as network traffic demanding far from exceeds network bandwidth capability. Maintaining a certain user satisfaction degree therefore becomes a challenge research topic. In this paper, we deal with the problem by proposing BASMIN, a novel bandwidth allocation scheme that aims to maximize network user's happiness. We also defined a new metric for evaluating network user satisfaction degree: network worth. A three-step evaluation process is then conducted to compare BASMIN efficiency with other three popular bandwidth allocation schemes. Throughout the tests, we experienced BASMIN's advantages over the others; we even found out that one of the most widely used bandwidth allocation schemes, in fact, is not effective at all.\n    ",
        "submission_date": "2010-07-08T00:00:00",
        "last_modified_date": "2010-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.2449",
        "title": "A Brief Introduction to Temporality and Causality",
        "authors": [
            "Kamran Karimi"
        ],
        "abstract": "Causality is a non-obvious concept that is often considered to be related to temporality. In this paper we present a number of past and present approaches to the definition of temporality and causality from philosophical, physical, and computational points of view. We note that time is an important ingredient in many relationships and phenomena. The topic is then divided into the two main areas of temporal discovery, which is concerned with finding relations that are stretched over time, and causal discovery, where a claim is made as to the causal influence of certain events on others. We present a number of computational tools used for attempting to automatically discover temporal and causal relations in data.\n    ",
        "submission_date": "2010-07-14T00:00:00",
        "last_modified_date": "2010-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1007.3858",
        "title": "CHR(PRISM)-based Probabilistic Logic Learning",
        "authors": [
            "Jon Sneyers",
            "Wannes Meert",
            "Joost Vennekens",
            "Yoshitaka Kameya",
            "Taisuke Sato"
        ],
        "abstract": "PRISM is an extension of Prolog with probabilistic predicates and built-in support for expectation-maximization learning. Constraint Handling Rules (CHR) is a high-level programming language based on multi-headed multiset rewrite rules.\n",
        "submission_date": "2010-07-22T00:00:00",
        "last_modified_date": "2010-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.0322",
        "title": "Co-evolution is Incompatible with the Markov Assumption in Phylogenetics",
        "authors": [
            "Tamir Tuller",
            "Elchanan Mossel"
        ],
        "abstract": "Markov models are extensively used in the analysis of molecular evolution. A recent line of research suggests that pairs of proteins with functional and physical interactions co-evolve with each other. Here, by analyzing hundreds of orthologous sets of three fungi and their co-evolutionary relations, we demonstrate that co-evolutionary assumption may violate the Markov assumption. Our results encourage developing alternative probabilistic models for the cases of extreme co-evolution.\n    ",
        "submission_date": "2010-08-02T00:00:00",
        "last_modified_date": "2010-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.0775",
        "title": "Systems Theoretic Techniques for Modeling, Control, and Decision Support in Complex Dynamic Systems",
        "authors": [
            "Armen Bagdasaryan"
        ],
        "abstract": "We discuss the problems of modeling, control, and decision support in complex dynamic systems from a general system theoretic point of view. The main characteristics of complex systems and of system approach to complex system study are considered. We provide an overview and analysis of known existing paradigms and methods of mathematical modeling and simulation of complex systems, which support the processes of control and decision making. Then we continue with the general dynamic modeling and simulation technique for complex hierarchical systems functioning in control loop. Architectural and structural models of computer information system intended for simulation and decision support in complex systems are presented.\n    ",
        "submission_date": "2010-08-04T00:00:00",
        "last_modified_date": "2013-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.0838",
        "title": "Associative control processor with a rigid structure",
        "authors": [
            "Isa Magomedov",
            "Omar Khazamov"
        ],
        "abstract": "The approach of applying associative processor for decision making problem was proposed. It focuses on hardware implementations of fuzzy processing systems, associativity as effective management basis of fuzzy processor. The structural approach is being developed resulting in a quite simple and compact parallel associative memory unit (PAMU). The memory cost and speed comparison of processors with rigid and soft-variable structure is given. Also the example PAMU flashing is considered.\n    ",
        "submission_date": "2010-08-04T00:00:00",
        "last_modified_date": "2010-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.1309",
        "title": "Towards arrow-theoretic semantics of ontologies: conceptories",
        "authors": [
            "Osman Bineev"
        ],
        "abstract": "In context of efforts of composing category-theoretic and logical methods in the area of knowledge representation we propose the notion of conceptory. We consider intersection/union and other constructions in conceptories as expressive alternative to category-theoretic (co)limits and show they have features similar to (pro-, in-)jections. Then we briefly discuss approaches to development of formal systems built on the base of conceptories and describe possible application of such system to the specific ontology.\n    ",
        "submission_date": "2010-08-07T00:00:00",
        "last_modified_date": "2010-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.1566",
        "title": "Separate Training for Conditional Random Fields Using Co-occurrence Rate Factorization",
        "authors": [
            "Zhemin Zhu",
            "Djoerd Hiemstra",
            "Peter Apers",
            "Andreas Wombacher"
        ],
        "abstract": "The standard training method of Conditional Random Fields (CRFs) is very slow for large-scale applications. As an alternative, piecewise training divides the full graph into pieces, trains them independently, and combines the learned weights at test time. In this paper, we present \\emph{separate} training for undirected models based on the novel Co-occurrence Rate Factorization (CR-F). Separate training is a local training method. In contrast to MEMMs, separate training is unaffected by the label bias problem. Experiments show that separate training (i) is unaffected by the label bias problem; (ii) reduces the training time from weeks to seconds; and (iii) obtains competitive results to the standard and piecewise training on linear-chain CRFs.\n    ",
        "submission_date": "2010-08-09T00:00:00",
        "last_modified_date": "2012-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.2028",
        "title": "Discovering shared and individual latent structure in multiple time series",
        "authors": [
            "Suchi Saria",
            "Daphne Koller",
            "Anna Penn"
        ],
        "abstract": "This paper proposes a nonparametric Bayesian method for exploratory data analysis and feature construction in continuous time series. Our method focuses on understanding shared features in a set of time series that exhibit significant individual variability. Our method builds on the framework of latent Diricihlet allocation (LDA) and its extension to hierarchical Dirichlet processes, which allows us to characterize each series as switching between latent ``topics'', where each topic is characterized as a distribution over ``words'' that specify the series dynamics. However, unlike standard applications of LDA, we discover the words as we learn the model. We apply this model to the task of tracking the physiological signals of premature infants; our model obtains clinically significant insights as well as useful features for supervised learning tasks.\n    ",
        "submission_date": "2010-08-12T00:00:00",
        "last_modified_date": "2010-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.2121",
        "title": "Constraint Propagation for First-Order Logic and Inductive Definitions",
        "authors": [
            "Johan Wittocx",
            "Marc Denecker",
            "Maurice Bruynooghe"
        ],
        "abstract": "Constraint propagation is one of the basic forms of inference in many logic-based reasoning systems. In this paper, we investigate constraint propagation for first-order logic (FO), a suitable language to express a wide variety of constraints. We present an algorithm with polynomial-time data complexity for constraint propagation in the context of an FO theory and a finite structure. We show that constraint propagation in this manner can be represented by a datalog program and that the algorithm can be executed symbolically, i.e., independently of a structure. Next, we extend the algorithm to FO(ID), the extension of FO with inductive definitions. Finally, we discuss several applications.\n    ",
        "submission_date": "2010-08-12T00:00:00",
        "last_modified_date": "2011-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.2186",
        "title": "RDFViewS: A Storage Tuning Wizard for RDF Applications",
        "authors": [
            "Fran\u00e7ois Goasdou\u00e9",
            "Konstantinos Karanasos",
            "Julien Leblay",
            "Ioana Manolescu"
        ],
        "abstract": "In recent years, the significant growth of RDF data used in numerous applications has made its efficient and scalable manipulation an important issue. In this paper, we present RDFViewS, a system capable of choosing the most suitable views to materialize, in order to minimize the query response time for a specific SPARQL query workload, while taking into account the view maintenance cost and storage space constraints. Our system employs practical algorithms and heuristics to navigate through the search space of potential view configurations, and exploits the possibly available semantic information - expressed via an RDF Schema - to ensure the completeness of the query evaluation.\n    ",
        "submission_date": "2010-08-12T00:00:00",
        "last_modified_date": "2010-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.2277",
        "title": "Faithfulness in Chain Graphs: The Gaussian Case",
        "authors": [
            "Jose M. Pe\u00f1a"
        ],
        "abstract": "This paper deals with chain graphs under the classic Lauritzen-Wermuth-Frydenberg interpretation. We prove that the regular Gaussian distributions that factorize with respect to a chain graph $G$ with $d$ parameters have positive Lebesgue measure with respect to $\\mathbb{R}^d$, whereas those that factorize with respect to $G$ but are not faithful to it have zero Lebesgue measure with respect to $\\mathbb{R}^d$. This means that, in the measure-theoretic sense described, almost all the regular Gaussian distributions that factorize with respect to $G$ are faithful to it.\n    ",
        "submission_date": "2010-08-13T00:00:00",
        "last_modified_date": "2010-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.2626",
        "title": "Mining tree-query associations in graphs",
        "authors": [
            "Eveline Hoekx",
            "Jan Van den Bussche"
        ],
        "abstract": "New applications of data mining, such as in biology, bioinformatics, or sociology, are faced with large datasetsstructured as graphs. We introduce a novel class of tree-shapedpatterns called tree queries, and present algorithms for miningtree queries and tree-query associations in a large data graph. Novel about our class of patterns is that they can containconstants, and can contain existential nodes which are not counted when determining the number of occurrences of the patternin the data graph. Our algorithms have a number of provableoptimality properties, which are based on the theory of conjunctive database queries. We propose a practical, database-oriented implementation in SQL, and show that the approach works in practice through experiments on data about food webs, protein interactions, and citation analysis.\n    ",
        "submission_date": "2010-08-16T00:00:00",
        "last_modified_date": "2010-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.2743",
        "title": "PMOG: The projected mixture of Gaussians model with application to blind source separation",
        "authors": [
            "Gautam V. Pendse"
        ],
        "abstract": "We extend the mixtures of Gaussians (MOG) model to the projected mixture of Gaussians (PMOG) model. In the PMOG model, we assume that q dimensional input data points z_i are projected by a q dimensional vector w into 1-D variables u_i. The projected variables u_i are assumed to follow a 1-D MOG model. In the PMOG model, we maximize the likelihood of observing u_i to find both the model parameters for the 1-D MOG as well as the projection vector w. First, we derive an EM algorithm for estimating the PMOG model. Next, we show how the PMOG model can be applied to the problem of blind source separation (BSS). In contrast to conventional BSS where an objective function based on an approximation to differential entropy is minimized, PMOG based BSS simply minimizes the differential entropy of projected sources by fitting a flexible MOG model in the projected 1-D space while simultaneously optimizing the projection vector w. The advantage of PMOG over conventional BSS algorithms is the more flexible fitting of non-Gaussian source densities without assuming near-Gaussianity (as in conventional BSS) and still retaining computational feasibility.\n    ",
        "submission_date": "2010-08-16T00:00:00",
        "last_modified_date": "2010-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.3282",
        "title": "Modeling Spammer Behavior: Na\u00efve Bayes vs. Artificial Neural Networks",
        "authors": [
            "Md. Saiful Islam",
            "Shah Mostafa Khaled",
            "Khalid Farhan",
            "Md. Abdur Rahman",
            "Joy Rahman"
        ],
        "abstract": "Addressing the problem of spam emails in the Internet, this paper presents a comparative study on Na\u00efve Bayes and Artificial Neural Networks (ANN) based modeling of spammer behavior. Keyword-based spam email filtering techniques fall short to model spammer behavior as the spammer constantly changes tactics to circumvent these filters. The evasive tactics that the spammer uses are themselves patterns that can be modeled to combat spam. It has been observed that both Na\u00efve Bayes and ANN are best suitable for modeling spammer common patterns. Experimental results demonstrate that both of them achieve a promising detection rate of around 92%, which is considerably an improvement of performance compared to the keyword-based contemporary filtering approaches.\n    ",
        "submission_date": "2010-08-19T00:00:00",
        "last_modified_date": "2010-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.3829",
        "title": "Approximate Judgement Aggregation",
        "authors": [
            "Ilan Nehama"
        ],
        "abstract": "In this paper we analyze judgement aggregation problems in which a group of agents independently votes on a set of complex propositions that has some interdependency constraint between them(e.g., transitivity when describing preferences). We consider the issue of judgement aggregation from the perspective of approximation. That is, we generalize the previous results by studying approximate judgement aggregation. We relax the main two constraints assumed in the current literature, Consistency and Independence and consider mechanisms that only approximately satisfy these constraints, that is, satisfy them up to a small portion of the inputs. The main question we raise is whether the relaxation of these notions significantly alters the class of satisfying aggregation mechanisms. The recent works for preference aggregation of Kalai, Mossel, and Keller fit into this framework. The main result of this paper is that, as in the case of preference aggregation, in the case of a subclass of a natural class of aggregation problems termed `truth-functional agendas', the set of satisfying aggregation mechanisms does not extend non-trivially when relaxing the constraints. Our proof techniques involve Boolean Fourier transform and analysis of voter influences for voting protocols. The question we raise for Approximate Aggregation can be stated in terms of Property Testing. For instance, as a corollary from our result we get a generalization of the classic result for property testing of linearity of Boolean functions.\n",
        "submission_date": "2010-08-23T00:00:00",
        "last_modified_date": "2011-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.4249",
        "title": "Machine Learning Approaches for Modeling Spammer Behavior",
        "authors": [
            "Md. Saiful Islam",
            "Abdullah Al Mahmud",
            "Md. Rafiqul Islam"
        ],
        "abstract": "Spam is commonly known as unsolicited or unwanted email messages in the Internet causing potential threat to Internet Security. Users spend a valuable amount of time deleting spam emails. More importantly, ever increasing spam emails occupy server storage space and consume network bandwidth. Keyword-based spam email filtering strategies will eventually be less successful to model spammer behavior as the spammer constantly changes their tricks to circumvent these filters. The evasive tactics that the spammer uses are patterns and these patterns can be modeled to combat spam. This paper investigates the possibilities of modeling spammer behavioral patterns by well-known classification algorithms such as Na\u00efve Bayesian classifier (Na\u00efve Bayes), Decision Tree Induction (DTI) and Support Vector Machines (SVMs). Preliminary experimental results demonstrate a promising detection rate of around 92%, which is considerably an enhancement of performance compared to similar spammer behavior modeling research.\n    ",
        "submission_date": "2010-08-25T00:00:00",
        "last_modified_date": "2010-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.4268",
        "title": "An Influence Diagram-Based Approach for Estimating Staff Training in Software Industry",
        "authors": [
            "Kawal Jeet",
            "Vijay Kumar Mago",
            "Bhanu Prasad",
            "Rajinder Singh Minhas"
        ],
        "abstract": "The successful completion of a software development process depends on the analytical capability and foresightedness of the project manager. For the project manager, the main intriguing task is to manage the risk factors as they adversely influence the completion deadline. One such key risk factor is staff training. The risk of this factor can be avoided by pre-judging the amount of training required by the staff. So, a procedure is required to help the project manager make this decision. This paper presents a system that uses influence diagrams to implement the risk model to aid decision making. The system also considers the cost of conducting the training, based on various risk factors such as, (i) Lack of experience with project software; (ii) Newly appointed staff; (iii) Staff not well versed with the required quality standards; and (iv) Lack of experience with project environment. The system provides estimated requirement details for staff training at the beginning of a software development project.\n    ",
        "submission_date": "2010-08-25T00:00:00",
        "last_modified_date": "2010-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.4831",
        "title": "Foundations of Inference",
        "authors": [
            "Kevin H. Knuth",
            "John Skilling"
        ],
        "abstract": "We present a simple and clear foundation for finite inference that unites and significantly extends the approaches of Kolmogorov and Cox. Our approach is based on quantifying lattices of logical statements in a way that satisfies general lattice symmetries. With other applications such as measure theory in mind, our derivations assume minimal symmetries, relying on neither negation nor continuity nor differentiability. Each relevant symmetry corresponds to an axiom of quantification, and these axioms are used to derive a unique set of quantifying rules that form the familiar probability calculus. We also derive a unique quantification of divergence, entropy and information.\n    ",
        "submission_date": "2010-08-28T00:00:00",
        "last_modified_date": "2012-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.5078",
        "title": "Prediction by Compression",
        "authors": [
            "Joel Ratsaby"
        ],
        "abstract": "It is well known that text compression can be achieved by predicting the next symbol in the stream of text data based on the history seen up to the current symbol. The better the prediction the more skewed the conditional probability distribution of the next symbol and the shorter the codeword that needs to be assigned to represent this next symbol. What about the opposite direction ? suppose we have a black box that can compress text stream. Can it be used to predict the next symbol in the stream ? We introduce a criterion based on the length of the compressed data and use it to predict the next symbol. We examine empirically the prediction error rate and its dependency on some compression parameters.\n    ",
        "submission_date": "2010-08-30T00:00:00",
        "last_modified_date": "2010-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1008.5133",
        "title": "Memristor Crossbar-based Hardware Implementation of IDS Method",
        "authors": [
            "Farnood Merrikh-Bayat",
            "Saeed Bagheri-Shouraki",
            "Ali Rohani"
        ],
        "abstract": "Ink Drop Spread (IDS) is the engine of Active Learning Method (ALM), which is the methodology of soft computing. IDS, as a pattern-based processing unit, extracts useful information from a system subjected to modeling. In spite of its excellent potential in solving problems such as classification and modeling compared to other soft computing tools, finding its simple and fast hardware implementation is still a challenge. This paper describes a new hardware implementation of IDS method based on the memristor crossbar structure. In addition of simplicity, being completely real-time, having low latency and the ability to continue working after the occurrence of power breakdown are some of the advantages of our proposed circuit.\n    ",
        "submission_date": "2010-08-22T00:00:00",
        "last_modified_date": "2010-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.0108",
        "title": "Emotional State Categorization from Speech: Machine vs. Human",
        "authors": [
            "Arslan Shaukat",
            "Ke Chen"
        ],
        "abstract": "This paper presents our investigations on emotional state categorization from speech signals with a psychologically inspired computational model against human performance under the same experimental setup. Based on psychological studies, we propose a multistage categorization strategy which allows establishing an automatic categorization model flexibly for a given emotional speech categorization task. We apply the strategy to the Serbian Emotional Speech Corpus (GEES) and the Danish Emotional Speech Corpus (DES), where human performance was reported in previous psychological studies. Our work is the first attempt to apply machine learning to the GEES corpus where the human recognition rates were only available prior to our study. Unlike the previous work on the DES corpus, our work focuses on a comparison to human performance under the same experimental settings. Our studies suggest that psychology-inspired systems yield behaviours that, to a great extent, resemble what humans perceived and their performance is close to that of humans under the same experimental setup. Furthermore, our work also uncovers some differences between machine and humans in terms of emotional state recognition from speech.\n    ",
        "submission_date": "2010-09-01T00:00:00",
        "last_modified_date": "2010-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.0605",
        "title": "Gaussian Process Bandits for Tree Search: Theory and Application to Planning in Discounted MDPs",
        "authors": [
            "Louis Dorard",
            "John Shawe-Taylor"
        ],
        "abstract": "We motivate and analyse a new Tree Search algorithm, GPTS, based on recent theoretical advances in the use of Gaussian Processes for Bandit problems. We consider tree paths as arms and we assume the target/reward function is drawn from a GP distribution. The posterior mean and variance, after observing data, are used to define confidence intervals for the function values, and we sequentially play arms with highest upper confidence bounds. We give an efficient implementation of GPTS and we adapt previous regret bounds by determining the decay rate of the eigenvalues of the kernel matrix on the whole set of tree paths. We consider two kernels in the feature space of binary vectors indexed by the nodes of the tree: linear and Gaussian. The regret grows in square root of the number of iterations T, up to a logarithmic factor, with a constant that improves with bigger Gaussian kernel widths. We focus on practical values of T, smaller than the number of arms. Finally, we apply GPTS to Open Loop Planning in discounted Markov Decision Processes by modelling the reward as a discounted sum of independent Gaussian Processes. We report similar regret bounds to those of the OLOP algorithm.\n    ",
        "submission_date": "2010-09-03T00:00:00",
        "last_modified_date": "2011-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.0861",
        "title": "On the Estimation of Coherence",
        "authors": [
            "Mehryar Mohri",
            "Ameet Talwalkar"
        ],
        "abstract": "Low-rank matrix approximations are often used to help scale standard machine learning algorithms to large-scale problems. Recently, matrix coherence has been used to characterize the ability to extract global information from a subset of matrix entries in the context of these low-rank approximations and other sampling-based algorithms, e.g., matrix com- pletion, robust PCA. Since coherence is defined in terms of the singular vectors of a matrix and is expensive to compute, the practical significance of these results largely hinges on the following question: Can we efficiently and accurately estimate the coherence of a matrix? In this paper we address this question. We propose a novel algorithm for estimating coherence from a small number of columns, formally analyze its behavior, and derive a new coherence-based matrix approximation bound based on this analysis. We then present extensive experimental results on synthetic and real datasets that corroborate our worst-case theoretical analysis, yet provide strong support for the use of our proposed algorithm whenever low-rank approximation is being considered. Our algorithm efficiently and accurately estimates matrix coherence across a wide range of datasets, and these coherence estimates are excellent predictors of the effectiveness of sampling-based matrix approximation on a case-by-case basis.\n    ",
        "submission_date": "2010-09-04T00:00:00",
        "last_modified_date": "2010-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.0896",
        "title": "Memristor Crossbar-based Hardware Implementation of Fuzzy Membership Functions",
        "authors": [
            "Farnood Merrikh-Bayat",
            "Saeed Bagheri Shouraki"
        ],
        "abstract": "In May 1, 2008, researchers at Hewlett Packard (HP) announced the first physical realization of a fundamental circuit element called memristor that attracted so much interest worldwide. This newly found element can easily be combined with crossbar interconnect technology which this new structure has opened a new field in designing configurable or programmable electronic systems. These systems in return can have applications in signal processing and artificial intelligence. In this paper, based on the simple memristor crossbar structure, we propose new and simple circuits for hardware implementation of fuzzy membership functions. In our proposed circuits, these fuzzy membership functions can have any shapes and resolutions. In addition, these circuits can be used as a basis in the construction of evolutionary systems.\n    ",
        "submission_date": "2010-09-05T00:00:00",
        "last_modified_date": "2010-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.1446",
        "title": "Comparing Prediction Market Structures, With an Application to Market Making",
        "authors": [
            "Aseem Brahma",
            "Sanmay Das",
            "Malik Magdon-Ismail"
        ],
        "abstract": "Ensuring sufficient liquidity is one of the key challenges for designers of prediction markets. Various market making algorithms have been proposed in the literature and deployed in practice, but there has been little effort to evaluate their benefits and disadvantages in a systematic manner. We introduce a novel experimental design for comparing market structures in live trading that ensures fair comparison between two different microstructures with the same trading population. Participants trade on outcomes related to a two-dimensional random walk that they observe on their computer screens. They can simultaneously trade in two markets, corresponding to the independent horizontal and vertical random walks. We use this experimental design to compare the popular inventory-based logarithmic market scoring rule (LMSR) market maker and a new information based Bayesian market maker (BMM). Our experiments reveal that BMM can offer significant benefits in terms of price stability and expected loss when controlling for liquidity; the caveat is that, unlike LMSR, BMM does not guarantee bounded loss. Our investigation also elucidates some general properties of market makers in prediction markets. In particular, there is an inherent tradeoff between adaptability to market shocks and convergence during market equilibrium.\n    ",
        "submission_date": "2010-09-08T00:00:00",
        "last_modified_date": "2010-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.1720",
        "title": "Is there a physically universal cellular automaton or Hamiltonian?",
        "authors": [
            "Dominik Janzing"
        ],
        "abstract": "It is known that both quantum and classical cellular automata (CA) exist that are computationally universal in the sense that they can simulate, after appropriate initialization, any quantum or classical computation, respectively. Here we introduce a different notion of universality: a CA is called physically universal if every transformation on any finite region can be (approximately) implemented by the autonomous time evolution of the system after the complement of the region has been initialized in an appropriate way. We pose the question of whether physically universal CAs exist. Such CAs would provide a model of the world where the boundary between a physical system and its controller can be consistently shifted, in analogy to the Heisenberg cut for the quantum measurement problem. We propose to study the thermodynamic cost of computation and control within such a model because implementing a cyclic process on a microsystem may require a non-cyclic process for its controller, whereas implementing a cyclic process on system and controller may require the implementation of a non-cyclic process on a \"meta\"-controller, and so on. Physically universal CAs avoid this infinite hierarchy of controllers and the cost of implementing cycles on a subsystem can be described by mixing properties of the CA dynamics. We define a physical prior on the CA configurations by applying the dynamics to an initial state where half of the CA is in the maximum entropy state and half of it is in the all-zero state (thus reflecting the fact that life requires non-equilibrium states like the boundary between a hold and a cold reservoir). As opposed to Solomonoff's prior, our prior does not only account for the Kolmogorov complexity but also for the cost of isolating the system during the state preparation if the preparation process is not robust.\n    ",
        "submission_date": "2010-09-09T00:00:00",
        "last_modified_date": "2010-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.1990",
        "title": "Complexity of Non-Monotonic Logics",
        "authors": [
            "Michael Thomas",
            "Heribert Vollmer"
        ],
        "abstract": "Over the past few decades, non-monotonic reasoning has developed to be one of the most important topics in computational logic and artificial intelligence. Different ways to introduce non-monotonic aspects to classical logic have been considered, e.g., extension with default rules, extension with modal belief operators, or modification of the semantics. In this survey we consider a logical formalism from each of the above possibilities, namely Reiter's default logic, Moore's autoepistemic logic and McCarthy's circumscription. Additionally, we consider abduction, where one is not interested in inferences from a given knowledge base but in computing possible explanations for an observation with respect to a given knowledge base.\n",
        "submission_date": "2010-09-10T00:00:00",
        "last_modified_date": "2010-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.2009",
        "title": "Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data",
        "authors": [
            "Tran The Truyen",
            "Dinh Q. Phung",
            "Hung H. Bui",
            "Svetha Venkatesh"
        ],
        "abstract": "Inspired by the hierarchical hidden Markov models (HHMM), we present the hierarchical semi-Markov conditional random field (HSCRF), a generalisation of embedded undirectedMarkov chains tomodel complex hierarchical, nestedMarkov processes. It is parameterised in a discriminative framework and has polynomial time algorithms for learning and inference. Importantly, we consider partiallysupervised learning and propose algorithms for generalised partially-supervised learning and constrained inference. We demonstrate the HSCRF in two applications: (i) recognising human activities of daily living (ADLs) from indoor surveillance cameras, and (ii) noun-phrase chunking. We show that the HSCRF is capable of learning rich hierarchical models with reasonable accuracy in both fully and partially observed data cases.\n    ",
        "submission_date": "2010-09-10T00:00:00",
        "last_modified_date": "2010-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.2021",
        "title": "The Complexity of Causality and Responsibility for Query Answers and non-Answers",
        "authors": [
            "Alexandra Meliou",
            "Wolfgang Gatterbauer",
            "Katherine F. Moore",
            "Dan Suciu"
        ],
        "abstract": "An answer to a query has a well-defined lineage expression (alternatively called how-provenance) that explains how the answer was derived. Recent work has also shown how to compute the lineage of a non-answer to a query. However, the cause of an answer or non-answer is a more subtle notion and consists, in general, of only a fragment of the lineage. In this paper, we adapt Halpern, Pearl, and Chockler's recent definitions of causality and responsibility to define the causes of answers and non-answers to queries, and their degree of responsibility. Responsibility captures the notion of degree of causality and serves to rank potentially many causes by their relative contributions to the effect. Then, we study the complexity of computing causes and responsibilities for conjunctive queries. It is known that computing causes is NP-complete in general. Our first main result shows that all causes to conjunctive queries can be computed by a relational query which may involve negation. Thus, causality can be computed in PTIME, and very efficiently so. Next, we study computing responsibility. Here, we prove that the complexity depends on the conjunctive query and demonstrate a dichotomy between PTIME and NP-complete cases. For the PTIME cases, we give a non-trivial algorithm, consisting of a reduction to the max-flow computation problem. Finally, we prove that, even when it is in PTIME, responsibility is complete for LOGSPACE, implying that, unlike causality, it cannot be computed by a relational query.\n    ",
        "submission_date": "2010-09-10T00:00:00",
        "last_modified_date": "2011-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.2054",
        "title": "Multiplex Structures: Patterns of Complexity in Real-World Networks",
        "authors": [
            "Bo Yang",
            "Jiming Liu"
        ],
        "abstract": "Complex network theory aims to model and analyze complex systems that consist of multiple and interdependent components. Among all studies on complex networks, topological structure analysis is of the most fundamental importance, as it represents a natural route to understand the dynamics, as well as to synthesize or optimize the functions, of networks. A broad spectrum of network structural patterns have been respectively reported in the past decade, such as communities, multipartites, hubs, authorities, outliers, bow ties, and others. Here, we show that most individual real-world networks demonstrate multiplex structures. That is, a multitude of known or even unknown (hidden) patterns can simultaneously situate in the same network, and moreover they may be overlapped and nested with each other to collaboratively form a heterogeneous, nested or hierarchical organization, in which different connective phenomena can be observed at different granular levels. In addition, we show that the multiplex structures hidden in exploratory networks can be well defined as well as effectively recognized within an unified framework consisting of a set of proposed concepts, models, and algorithms. Our findings provide a strong evidence that most real-world complex systems are driven by a combination of heterogeneous mechanisms that may collaboratively shape their ubiquitous multiplex structures as we observe currently. This work also contributes a mathematical tool for analyzing different sources of networks from a new perspective of unveiling multiplex structures, which will be beneficial to multiple disciplines including sociology, economics and computer science.\n    ",
        "submission_date": "2010-09-10T00:00:00",
        "last_modified_date": "2010-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.5346",
        "title": "A Novel Approach for Cardiac Disease Prediction and Classification Using Intelligent Agents",
        "authors": [
            "Murugesan Kuttikrishnan"
        ],
        "abstract": "The goal is to develop a novel approach for cardiac disease prediction and diagnosis using intelligent agents. Initially the symptoms are preprocessed using filter and wrapper based agents. The filter removes the missing or irrelevant symptoms. Wrapper is used to extract the data in the data set according to the threshold limits. Dependency of each symptom is identified using dependency checker agent. The classification is based on the prior and posterior probability of the symptoms with the evidence value. Finally the symptoms are classified in to five classes namely absence, starting, mild, moderate and serious. Using the cooperative approach the cardiac problem is solved and verified.\n    ",
        "submission_date": "2010-09-27T00:00:00",
        "last_modified_date": "2010-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1009.5473",
        "title": "The thermodynamic temperature of a rhythmic spiking network",
        "authors": [
            "Paul Merolla",
            "Tristan Ursell",
            "John Arthur"
        ],
        "abstract": "Artificial neural networks built from two-state neurons are powerful computational substrates, whose computational ability is well understood by analogy with statistical mechanics. In this work, we introduce similar analogies in the context of spiking neurons in a fixed time window, where excitatory and inhibitory inputs drawn from a Poisson distribution play the role of temperature. For single neurons with a \"bandgap\" between their inputs and the spike threshold, this temperature allows for stochastic spiking. By imposing a global inhibitory rhythm over the fixed time windows, we connect neurons into a network that exhibits synchronous, clock-like updating akin to neural networks. We implement a single-layer Boltzmann machine without learning to demonstrate our model.\n    ",
        "submission_date": "2010-09-28T00:00:00",
        "last_modified_date": "2010-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.0012",
        "title": "An Embarrassingly Simple Speed-Up of Belief Propagation with Robust Potentials",
        "authors": [
            "James M. Coughlan",
            "Huiying Shen"
        ],
        "abstract": "We present an exact method of greatly speeding up belief propagation (BP) for a wide variety of potential functions in pairwise MRFs and other graphical models. Specifically, our technique applies whenever the pairwise potentials have been {\\em truncated} to a constant value for most pairs of states, as is commonly done in MRF models with robust potentials (such as stereo) that impose an upper bound on the penalty assigned to discontinuities; for each of the $M$ possible states in one node, only a smaller number $m$ of compatible states in a neighboring node are assigned milder penalties. The computational complexity of our method is $O(mM)$, compared with $O(M^2)$ for standard BP, and we emphasize that the method is {\\em exact}, in contrast with related techniques such as pruning; moreover, the method is very simple and easy to implement. Unlike some previous work on speeding up BP, our method applies both to sum-product and max-product BP, which makes it useful in any applications where marginal probabilities are required, such as maximum likelihood estimation. We demonstrate the technique on a stereo MRF example, confirming that the technique speeds up BP without altering the solution.\n    ",
        "submission_date": "2010-09-30T00:00:00",
        "last_modified_date": "2010-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.0019",
        "title": "Mantis: Predicting System Performance through Program Analysis and Modeling",
        "authors": [
            "Byung-Gon Chun",
            "Ling Huang",
            "Sangmin Lee",
            "Petros Maniatis",
            "Mayur Naik"
        ],
        "abstract": "We present Mantis, a new framework that automatically predicts program performance with high accuracy. Mantis integrates techniques from programming language and machine learning for performance modeling, and is a radical departure from traditional approaches. Mantis extracts program features, which are information about program execution runs, through program instrumentation. It uses machine learning techniques to select features relevant to performance and creates prediction models as a function of the selected features. Through program analysis, it then generates compact code slices that compute these feature values for prediction. Our evaluation shows that Mantis can achieve more than 93% accuracy with less than 10% training data set, which is a significant improvement over models that are oblivious to program features. The system generates code slices that are cheap to compute feature values.\n    ",
        "submission_date": "2010-09-30T00:00:00",
        "last_modified_date": "2010-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.1437",
        "title": "Mixed-Membership Stochastic Block-Models for Transactional Networks",
        "authors": [
            "Mahdi Shafiei",
            "Hugh Chipman"
        ],
        "abstract": "Transactional network data can be thought of as a list of one-to-many communications(e.g., email) between nodes in a social network. Most social network models convert this type of data into binary relations between pairs of nodes. We develop a latent mixed membership model capable of modeling richer forms of transactional network data, including relations between more than two nodes. The model can cluster nodes and predict transactions. The block-model nature of the model implies that groups can be characterized in very general ways. This flexible notion of group structure enables discovery of rich structure in transactional networks. Estimation and inference are accomplished via a variational EM algorithm. Simulations indicate that the learning algorithm can recover the correct generative model. Interesting structure is discovered in the Enron email dataset and another dataset extracted from the Reddit website. Analysis of the Reddit data is facilitated by a novel performance measure for comparing two soft clusterings. The new model is superior at discovering mixed membership in groups and in predicting transactions.\n    ",
        "submission_date": "2010-10-07T00:00:00",
        "last_modified_date": "2010-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.2439",
        "title": "Conservation Law of Utility and Equilibria in Non-Zero Sum Games",
        "authors": [
            "Roman V. Belavkin"
        ],
        "abstract": "This short note demonstrates how one can define a transformation of a non-zero sum game into a zero sum, so that the optimal mixed strategy achieving equilibrium always exists. The transformation is equivalent to introduction of a passive player into a game (a player with a singleton set of pure strategies), whose payoff depends on the actions of the active players, and it is justified by the law of conservation of utility in a game. In a transformed game, each participant plays against all other players, including the passive player. The advantage of this approach is that the transformed game is zero-sum and has an equilibrium solution. The optimal strategy and the value of the new game, however, can be different from strategies that are rational in the original game. We demonstrate the principle using the Prisoner's Dilemma example.\n    ",
        "submission_date": "2010-10-12T00:00:00",
        "last_modified_date": "2010-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.3091",
        "title": "Near-Optimal Bayesian Active Learning with Noisy Observations",
        "authors": [
            "Daniel Golovin",
            "Andreas Krause",
            "Debajyoti Ray"
        ],
        "abstract": "We tackle the fundamental problem of Bayesian active learning with noise, where we need to adaptively select from a number of expensive tests in order to identify an unknown hypothesis sampled from a known prior distribution. In the case of noise-free observations, a greedy algorithm called generalized binary search (GBS) is known to perform near-optimally. We show that if the observations are noisy, perhaps surprisingly, GBS can perform very poorly. We develop EC2, a novel, greedy active learning algorithm and prove that it is competitive with the optimal policy, thus obtaining the first competitiveness guarantees for Bayesian active learning with noisy observations. Our bounds rely on a recently discovered diminishing returns property called adaptive submodularity, generalizing the classical notion of submodular set functions to adaptive policies. Our results hold even if the tests have non-uniform cost and their noise is correlated. We also propose EffECXtive, a particularly fast approximation of EC2, and evaluate it on a Bayesian experimental design problem involving human subjects, intended to tease apart competing economic theories of how people make decisions under uncertainty.\n    ",
        "submission_date": "2010-10-15T00:00:00",
        "last_modified_date": "2013-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.3425",
        "title": "Identifying the consequences of dynamic treatment strategies: A decision-theoretic overview",
        "authors": [
            "A. Philip Dawid",
            "Vanessa Didelez"
        ],
        "abstract": "We consider the problem of learning about and comparing the consequences of dynamic treatment strategies on the basis of observational data. We formulate this within a probabilistic decision-theoretic framework. Our approach is compared with related work by Robins and others: in particular, we show how Robins's 'G-computation' algorithm arises naturally from this decision-theoretic perspective. Careful attention is paid to the mathematical and substantive conditions required to justify the use of this formula. These conditions revolve around a property we term stability, which relates the probabilistic behaviours of observational and interventional regimes. We show how an assumption of 'sequential randomization' (or 'no unmeasured confounders'), or an alternative assumption of 'sequential irrelevance', can be used to infer stability. Probabilistic influence diagrams are used to simplify manipulations, and their power and limitations are discussed. We compare our approach with alternative formulations based on causal DAGs or potential response models. We aim to show that formulating the problem of assessing dynamic treatment strategies as a problem of decision analysis brings clarity, simplicity and generality.\n    ",
        "submission_date": "2010-10-17T00:00:00",
        "last_modified_date": "2010-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.3796",
        "title": "Mining Knowledge in Astrophysical Massive Data Sets",
        "authors": [
            "M. Brescia",
            "G. Longo",
            "F. Pasian"
        ],
        "abstract": "Modern scientific data mainly consist of huge datasets gathered by a very large number of techniques and stored in very diversified and often incompatible data repositories. More in general, in the e-science environment, it is considered as a critical and urgent requirement to integrate services across distributed, heterogeneous, dynamic \"virtual organizations\" formed by different resources within a single enterprise. In the last decade, Astronomy has become an immensely data rich field due to the evolution of detectors (plates to digital to mosaics), telescopes and space instruments. The Virtual Observatory approach consists into the federation under common standards of all astronomical archives available worldwide, as well as data analysis, data mining and data exploration applications. The main drive behind such effort being that once the infrastructure will be completed, it will allow a new type of multi-wavelength, multi-epoch science which can only be barely imagined. Data Mining, or Knowledge Discovery in Databases, while being the main methodology to extract the scientific information contained in such MDS (Massive Data Sets), poses crucial problems since it has to orchestrate complex problems posed by transparent access to different computing environments, scalability of algorithms, reusability of resources, etc. In the present paper we summarize the present status of the MDS in the Virtual Observatory and what is currently done and planned to bring advanced Data Mining methodologies in the case of the DAME (DAta Mining & Exploration) project.\n    ",
        "submission_date": "2010-10-19T00:00:00",
        "last_modified_date": "2010-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.4466",
        "title": "On the Foundations of Adversarial Single-Class Classification",
        "authors": [
            "Ran El-Yaniv",
            "Mordechai Nisenson"
        ],
        "abstract": "Motivated by authentication, intrusion and spam detection applications we consider single-class classification (SCC) as a two-person game between the learner and an adversary. In this game the learner has a sample from a target distribution and the goal is to construct a classifier capable of distinguishing observations from the target distribution from observations emitted from an unknown other distribution. The ideal SCC classifier must guarantee a given tolerance for the false-positive error (false alarm rate) while minimizing the false negative error (intruder pass rate). Viewing SCC as a two-person zero-sum game we identify both deterministic and randomized optimal classification strategies for different game variants. We demonstrate that randomized classification can provide a significant advantage. In the deterministic setting we show how to reduce SCC to two-class classification where in the two-class problem the other class is a synthetically generated distribution. We provide an efficient and practical algorithm for constructing and solving the two class problem. The algorithm distinguishes low density regions of the target distribution and is shown to be consistent.\n    ",
        "submission_date": "2010-10-21T00:00:00",
        "last_modified_date": "2010-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1010.4504",
        "title": "Reading Dependencies from Covariance Graphs",
        "authors": [
            "Jose M. Pe\u00f1a"
        ],
        "abstract": "The covariance graph (aka bi-directed graph) of a probability distribution $p$ is the undirected graph $G$ where two nodes are adjacent iff their corresponding random variables are marginally dependent in $p$. In this paper, we present a graphical criterion for reading dependencies from $G$, under the assumption that $p$ satisfies the graphoid properties as well as weak transitivity and composition. We prove that the graphical criterion is sound and complete in certain sense. We argue that our assumptions are not too restrictive. For instance, all the regular Gaussian probability distributions satisfy them.\n    ",
        "submission_date": "2010-10-21T00:00:00",
        "last_modified_date": "2012-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.0041",
        "title": "Predictive State Temporal Difference Learning",
        "authors": [
            "Byron Boots",
            "Geoffrey J. Gordon"
        ],
        "abstract": "We propose a new approach to value function approximation which combines linear temporal difference reinforcement learning with subspace identification. In practical applications, reinforcement learning (RL) is complicated by the fact that state is either high-dimensional or partially observable. Therefore, RL methods are designed to work with features of state rather than state itself, and the success or failure of learning is often determined by the suitability of the selected features. By comparison, subspace identification (SSID) methods are designed to select a feature set which preserves as much information as possible about state. In this paper we connect the two approaches, looking at the problem of reinforcement learning with a large set of features, each of which may only be marginally useful for value function approximation. We introduce a new algorithm for this situation, called Predictive State Temporal Difference (PSTD) learning. As in SSID for predictive state representations, PSTD finds a linear compression operator that projects a large set of features down to a small set that preserves the maximum amount of predictive information. As in RL, PSTD then uses a Bellman recursion to estimate a value function. We discuss the connection between PSTD and prior approaches in RL and SSID. We prove that PSTD is statistically consistent, perform several experiments that illustrate its properties, and demonstrate its potential on a difficult optimal stopping problem.\n    ",
        "submission_date": "2010-10-30T00:00:00",
        "last_modified_date": "2011-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.0362",
        "title": "Optimization of artificial flockings by means of anisotropy measurements",
        "authors": [
            "Motohiro Makiguchi",
            "Jun-ichi Inoue"
        ],
        "abstract": "An effective procedure to determine the optimal parameters appearing in artificial flockings is proposed in terms of optimization problems. We numerically examine genetic algorithms (GAs) to determine the optimal set of parameters such as the weights for three essential interactions in BOIDS by Reynolds (1987) under `zero-collision' and `no-breaking-up' constraints. As a fitness function (the energy function) to be maximized by the GA, we choose the so-called the $\\gamma$-value of anisotropy which can be observed empirically in typical flocks of starling. We confirm that the GA successfully finds the solution having a large $\\gamma$-value leading-up to a strong anisotropy. The numerical experience shows that the procedure might enable us to make more realistic and efficient artificial flocking of starling even in our personal computers. We also evaluate two distinct types of interactions in agents, namely, metric and topological definitions of interactions. We confirmed that the topological definition can explain the empirical evidence much better than the metric definition does.\n    ",
        "submission_date": "2010-11-01T00:00:00",
        "last_modified_date": "2011-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.0686",
        "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning",
        "authors": [
            "Stephane Ross",
            "Geoffrey J. Gordon",
            "J. Andrew Bagnell"
        ],
        "abstract": "Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.\n    ",
        "submission_date": "2010-11-02T00:00:00",
        "last_modified_date": "2011-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.1841",
        "title": "Fundamentals of Mathematical Theory of Emotional Robots",
        "authors": [
            "Oleg Pensky",
            "Kirill Chernikov"
        ],
        "abstract": "In this book we introduce a mathematically formalized concept of emotion, robot's education and other psychological parameters of intelligent robots. We also introduce unitless coefficients characterizing an emotional memory of a robot. Besides, the effect of a robot's memory upon its emotional behavior is studied, and theorems defining fellowship and conflicts in groups of robots are proved. Also unitless parameters describing emotional states of those groups are introduced, and a rule of making alternative (binary) decisions based on emotional selection is given. We introduce a concept of equivalent educational process for robots and a concept of efficiency coefficient of an educational process, and suggest an algorithm of emotional contacts within a group of robots. And generally, we present and describe a model of a virtual reality with emotional robots. The book is meant for mathematical modeling specialists and emotional robot software developers.\n    ",
        "submission_date": "2010-10-17T00:00:00",
        "last_modified_date": "2010-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.2173",
        "title": "Photometric Catalogue of Quasars and Other Point Sources in the Sloan Digital Sky Survey",
        "authors": [
            "Sheelu Abraham",
            "Ninan Sajeeth Philip",
            "Ajit Kembhavi",
            "Yogesh G Wadadekar",
            "Rita Sinha"
        ],
        "abstract": "We present a catalogue of about 6 million unresolved photometric detections in the Sloan Digital Sky Survey Seventh Data Release classifying them into stars, galaxies and quasars. We use a machine learning classifier trained on a subset of spectroscopically confirmed objects from 14th to 22nd magnitude in the SDSS {\\it i}-band. Our catalogue consists of 2,430,625 quasars, 3,544,036 stars and 63,586 unresolved galaxies from 14th to 24th magnitude in the SDSS {\\it i}-band. Our algorithm recovers 99.96% of spectroscopically confirmed quasars and 99.51% of stars to i $\\sim$21.3 in the colour window that we study. The level of contamination due to data artefacts for objects beyond $i=21.3$ is highly uncertain and all mention of completeness and contamination in the paper are valid only for objects brighter than this magnitude. However, a comparison of the predicted number of quasars with the theoretical number counts shows reasonable agreement.\n    ",
        "submission_date": "2010-11-09T00:00:00",
        "last_modified_date": "2011-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.2898",
        "title": "Reified unit resolution and the failed literal rule",
        "authors": [
            "Olivier Bailleux"
        ],
        "abstract": "Unit resolution can simplify a CNF formula or detect an inconsistency by repeatedly assign the variables occurring in unit clauses. Given any CNF formula sigma, we show that there exists a satisfiable CNF formula psi with size polynomially related to the size of sigma such that applying unit resolution to psi simulates all the effects of applying it to sigma. The formula psi is said to be the reified counterpart of sigma. This approach can be used to prove that the failed literal rule, which is an inference rule used by some SAT solvers, can be entirely simulated by unit resolution. More generally, it sheds new light on the expressive power of unit resolution.\n    ",
        "submission_date": "2010-11-12T00:00:00",
        "last_modified_date": "2010-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.3244",
        "title": "\"Meaning\" as a sociological concept: A review of the modeling, mapping, and simulation of the communication of knowledge and meaning",
        "authors": [
            "Loet Leydesdorff"
        ],
        "abstract": "The development of discursive knowledge presumes the communication of meaning as analytically different from the communication of information. Knowledge can then be considered as a meaning which makes a difference. Whereas the communication of information is studied in the information sciences and scientometrics, the communication of meaning has been central to Luhmann's attempts to make the theory of autopoiesis relevant for sociology. Analytical techniques such as semantic maps and the simulation of anticipatory systems enable us to operationalize the distinctions which Luhmann proposed as relevant to the elaboration of Husserl's \"horizons of meaning\" in empirical research: interactions among communications, the organization of meaning in instantiations, and the self-organization of interhuman communication in terms of symbolically generalized media such as truth, love, and power. Horizons of meaning, however, remain uncertain orders of expectations, and one should caution against reification from the meta-biological perspective of systems theory.\n    ",
        "submission_date": "2010-11-14T00:00:00",
        "last_modified_date": "2011-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.3257",
        "title": "Integration of Flexible Web Based GUI in I-SOAS",
        "authors": [
            "Zeeshan Ahmed",
            "Vasil Popov"
        ],
        "abstract": "It is necessary to improve the concepts of the present web based graphical user interface for the development of more flexible and intelligent interface to provide ease and increase the level of comfort at user end like most of the desktop based applications. This research is conducted targeting the goal of implementing flexible GUI consisting of a visual component manager with different components by functionality, design and purpose. In this research paper we present a Rich Internet Application (RIA) based graphical user interface for web based product development, and going into the details we present a comparison between existing RIA Technologies, adopted methodology in the GUI development and developed prototype.\n    ",
        "submission_date": "2010-11-14T00:00:00",
        "last_modified_date": "2010-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.3397",
        "title": "The Inverse Task of the Reflexive Game Theory: Theoretical Matters, Practical Applications and Relationship with Other Issues",
        "authors": [
            "Sergey Tarasenko"
        ],
        "abstract": "The Reflexive Game Theory (RGT) has been recently proposed by Vladimir Lefebvre to model behavior of individuals in groups. The goal of this study is to introduce the Inverse task. We consider methods of solution together with practical applications. We present a brief overview of the RGT for easy understanding of the problem. We also develop the schematic representation of the RGT inference algorithms to create the basis for soft- and hardware solutions of the RGT tasks. We propose a unified hierarchy of schemas to represent humans and robots. This hierarchy is considered as a unified framework to solve the entire spectrum of the RGT tasks. We conclude by illustrating how this framework can be applied for modeling of mixed groups of humans and robots. All together this provides the exhaustive solution of the Inverse task and clearly illustrates its role and relationships with other issues considered in the RGT.\n    ",
        "submission_date": "2010-11-15T00:00:00",
        "last_modified_date": "2010-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.3400",
        "title": "Prize insights in probability, and one goat of a recycled error: Jason Rosenhouse's The Monty Hall Problem",
        "authors": [
            "Anthony B. Morton"
        ],
        "abstract": "The Monty Hall problem is the TV game scenario where you, the contestant, are presented with three doors, with a car hidden behind one and goats hidden behind the other two. After you select a door, the host (Monty Hall) opens a second door to reveal a goat. You are then invited to stay with your original choice of door, or to switch to the remaining unopened door, and claim whatever you find behind it. Assuming your objective is to win the car, is your best strategy to stay or switch, or does it not matter? Jason Rosenhouse has provided the definitive analysis of this game, along with several intriguing variations, and discusses some of its psychological and philosophical implications. This extended review examines several themes from the book in some detail from a Bayesian perspective, and points out one apparently inadvertent error.\n    ",
        "submission_date": "2010-11-15T00:00:00",
        "last_modified_date": "2010-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.3494",
        "title": "Learning Planar Ising Models",
        "authors": [
            "Jason K. Johnson",
            "Praneeth Netrapalli",
            "Michael Chertkov"
        ],
        "abstract": "Inference and learning of graphical models are both well-studied problems in statistics and machine learning that have found many applications in science and engineering. However, exact inference is intractable in general graphical models, which suggests the problem of seeking the best approximation to a collection of random variables within some tractable family of graphical models. In this paper, we focus our attention on the class of planar Ising models, for which inference is tractable using techniques of statistical physics [Kac and Ward; Kasteleyn]. Based on these techniques and recent methods for planarity testing and planar embedding [Chrobak and Payne], we propose a simple greedy algorithm for learning the best planar Ising model to approximate an arbitrary collection of binary random variables (possibly from sample data). Given the set of all pairwise correlations among variables, we select a planar graph and optimal planar Ising model defined on this graph to best approximate that set of correlations. We demonstrate our method in some simulations and for the application of modeling senate voting records.\n    ",
        "submission_date": "2010-11-15T00:00:00",
        "last_modified_date": "2010-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.3912",
        "title": "Artificial Hormone Reaction Networks: Towards Higher Evolvability in Evolutionary Multi-Modular Robotics",
        "authors": [
            "Heiko Hamann",
            "J\u00fcrgen Stradner",
            "Thomas Schmickl",
            "Karl Crailsheim"
        ],
        "abstract": "The semi-automatic or automatic synthesis of robot controller software is both desirable and challenging. Synthesis of rather simple behaviors such as collision avoidance by applying artificial evolution has been shown multiple times. However, the difficulty of this synthesis increases heavily with increasing complexity of the task that should be performed by the robot. We try to tackle this problem of complexity with Artificial Homeostatic Hormone Systems (AHHS), which provide both intrinsic, homeostatic processes and (transient) intrinsic, variant behavior. By using AHHS the need for pre-defined controller topologies or information about the field of application is minimized. We investigate how the principle design of the controller and the hormone network size affects the overall performance of the artificial evolution (i.e., evolvability). This is done by comparing two variants of AHHS that show different effects when mutated. We evolve a controller for a robot built from five autonomous, cooperating modules. The desired behavior is a form of gait resulting in fast locomotion by using the modules' main hinges.\n    ",
        "submission_date": "2010-11-17T00:00:00",
        "last_modified_date": "2010-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.4071",
        "title": "Supervised Random Walks: Predicting and Recommending Links in Social Networks",
        "authors": [
            "L. Backstrom",
            "J. Leskovec"
        ],
        "abstract": "Predicting the occurrence of links is a fundamental problem in networks. In the link prediction problem we are given a snapshot of a network and would like to infer which interactions among existing members are likely to occur in the near future or which existing interactions are we missing. Although this problem has been extensively studied, the challenge of how to effectively combine the information from the network structure with rich node and edge attribute data remains largely open.\n",
        "submission_date": "2010-11-17T00:00:00",
        "last_modified_date": "2010-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.4833",
        "title": "A Logical Charaterisation of Ordered Disjunction",
        "authors": [
            "Pedro Cabalar"
        ],
        "abstract": "In this paper we consider a logical treatment for the ordered disjunction operator 'x' introduced by Brewka, Niemel\u00e4 and Syrj\u00e4nen in their Logic Programs with Ordered Disjunctions (LPOD). LPODs are used to represent preferences in logic programming under the answer set semantics. Their semantics is defined by first translating the LPOD into a set of normal programs (called split programs) and then imposing a preference relation among the answer sets of these split programs. We concentrate on the first step and show how a suitable translation of the ordered disjunction as a derived operator into the logic of Here-and-There allows capturing the answer sets of the split programs in a direct way. We use this characterisation not only for providing an alternative implementation for LPODs, but also for checking several properties (under strongly equivalent transformations) of the 'x' operator, like for instance, its distributivity with respect to conjunction or regular disjunction. We also make a comparison to an extension proposed by K\u00e4rger, Lopes, Olmedilla and Polleres, that combines 'x' with regular disjunction.\n    ",
        "submission_date": "2010-11-22T00:00:00",
        "last_modified_date": "2010-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.5202",
        "title": "Covered Clause Elimination",
        "authors": [
            "Marijn Heule",
            "Matti J\u00e4rvisalo",
            "Armin Biere"
        ],
        "abstract": "Generalizing the novel clause elimination procedures developed in [M. Heule, M. J\u00e4rvisalo, and A. Biere. Clause elimination procedures for CNF formulas. In Proc. LPAR-17, volume 6397 of LNCS, pages 357-371. Springer, 2010.], we introduce explicit (CCE), hidden (HCCE), and asymmetric (ACCE) variants of a procedure that eliminates covered clauses from CNF formulas. We show that these procedures are more effective in reducing CNF formulas than the respective variants of blocked clause elimination, and may hence be interesting as new preprocessing/simplification techniques for SAT solving.\n    ",
        "submission_date": "2010-11-23T00:00:00",
        "last_modified_date": "2010-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1011.6293",
        "title": "Nonparametric Bayesian sparse factor models with application to gene expression modeling",
        "authors": [
            "David Knowles",
            "Zoubin Ghahramani"
        ],
        "abstract": "A nonparametric Bayesian extension of Factor Analysis (FA) is proposed where observed data $\\mathbf{Y}$ is modeled as a linear superposition, $\\mathbf{G}$, of a potentially infinite number of hidden factors, $\\mathbf{X}$. The Indian Buffet Process (IBP) is used as a prior on $\\mathbf{G}$ to incorporate sparsity and to allow the number of latent features to be inferred. The model's utility for modeling gene expression data is investigated using randomly generated data sets based on a known sparse connectivity matrix for E. Coli, and on three biological data sets of increasing complexity.\n    ",
        "submission_date": "2010-11-29T00:00:00",
        "last_modified_date": "2011-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.0065",
        "title": "Counting in Graph Covers: A Combinatorial Characterization of the Bethe Entropy Function",
        "authors": [
            "Pascal O. Vontobel"
        ],
        "abstract": "We present a combinatorial characterization of the Bethe entropy function of a factor graph, such a characterization being in contrast to the original, analytical, definition of this function. We achieve this combinatorial characterization by counting valid configurations in finite graph covers of the factor graph. Analogously, we give a combinatorial characterization of the Bethe partition function, whose original definition was also of an analytical nature. As we point out, our approach has similarities to the replica method, but also stark differences. The above findings are a natural backdrop for introducing a decoder for graph-based codes that we will call symbolwise graph-cover decoding, a decoder that extends our earlier work on blockwise graph-cover decoding. Both graph-cover decoders are theoretical tools that help towards a better understanding of message-passing iterative decoding, namely blockwise graph-cover decoding links max-product (min-sum) algorithm decoding with linear programming decoding, and symbolwise graph-cover decoding links sum-product algorithm decoding with Bethe free energy function minimization at temperature one. In contrast to the Gibbs entropy function, which is a concave function, the Bethe entropy function is in general not concave everywhere. In particular, we show that every code picked from an ensemble of regular low-density parity-check codes with minimum Hamming distance growing (with high probability) linearly with the block length has a Bethe entropy function that is convex in certain regions of its domain.\n    ",
        "submission_date": "2010-12-01T00:00:00",
        "last_modified_date": "2012-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.0365",
        "title": "A Block Lanczos with Warm Start Technique for Accelerating Nuclear Norm Minimization Algorithms",
        "authors": [
            "Zhouchen Lin",
            "Siming Wei"
        ],
        "abstract": "Recent years have witnessed the popularity of using rank minimization as a regularizer for various signal processing and machine learning problems. As rank minimization problems are often converted to nuclear norm minimization (NNM) problems, they have to be solved iteratively and each iteration requires computing a singular value decomposition (SVD). Therefore, their solution suffers from the high computation cost of multiple SVDs. To relieve this issue, we propose using the block Lanczos method to compute the partial SVDs, where the principal singular subspaces obtained in the previous iteration are used to start the block Lanczos procedure. To avoid the expensive reorthogonalization in the Lanczos procedure, the block Lanczos procedure is performed for only a few steps. Our block Lanczos with warm start (BLWS) technique can be adopted by different algorithms that solve NNM problems. We present numerical results on applying BLWS to Robust PCA and Matrix Completion problems. Experimental results show that our BLWS technique usually accelerates its host algorithm by at least two to three times.\n    ",
        "submission_date": "2010-12-02T00:00:00",
        "last_modified_date": "2010-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.0729",
        "title": "Agnostic Learning of Monomials by Halfspaces is Hard",
        "authors": [
            "Vitaly Feldman",
            "Venkatesan Guruswami",
            "Prasad Raghavendra",
            "Yi Wu"
        ],
        "abstract": "We prove the following strong hardness result for learning: Given a distribution of labeled examples from the hypercube such that there exists a monomial consistent with $(1-\\eps)$ of the examples, it is NP-hard to find a halfspace that is correct on $(1/2+\\eps)$ of the examples, for arbitrary constants $\\eps > 0$. In learning theory terms, weak agnostic learning of monomials is hard, even if one is allowed to output a hypothesis from the much bigger concept class of halfspaces. This hardness result subsumes a long line of previous results, including two recent hardness results for the proper learning of monomials and halfspaces. As an immediate corollary of our result we show that weak agnostic learning of decision lists is NP-hard.\n",
        "submission_date": "2010-12-03T00:00:00",
        "last_modified_date": "2010-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.0735",
        "title": "Closed-set-based Discovery of Bases of Association Rules",
        "authors": [
            "Jos\u00e9 L. Balc\u00e1zar",
            "Diego Garc\u00eda-Saiz",
            "Domingo G\u00f3mez-P\u00e9rez",
            "Cristina T\u00eern\u0103uc\u0103"
        ],
        "abstract": "  The output of an association rule miner is often huge in practice. This is why several concise lossless representations have been proposed, such as the \"essential\" or \"representative\" rules. We revisit the algorithm given by Kryszkiewicz (Int. Symp. Intelligent Data Analysis 2001, Springer-Verlag LNCS 2189, 350-359) for mining representative rules. We show that its output is sometimes incomplete, due to an oversight in its mathematical validation. We propose alternative complete generators and we extend the approach to an existing closure-aware basis similar to, and often smaller than, the representative rules, namely the basis B*.\n    ",
        "submission_date": "2010-12-03T00:00:00",
        "last_modified_date": "2011-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.0930",
        "title": "Efficient Optimization of Performance Measures by Classifier Adaptation",
        "authors": [
            "Nan Li",
            "Ivor W. Tsang",
            "Zhi-Hua Zhou"
        ],
        "abstract": "In practical applications, machine learning algorithms are often needed to learn classifiers that optimize domain specific performance measures. Previously, the research has focused on learning the needed classifier in isolation, yet learning nonlinear classifier for nonlinear and nonsmooth performance measures is still hard. In this paper, rather than learning the needed classifier by optimizing specific performance measure directly, we circumvent this problem by proposing a novel two-step approach called as CAPO, namely to first train nonlinear auxiliary classifiers with existing learning methods, and then to adapt auxiliary classifiers for specific performance measures. In the first step, auxiliary classifiers can be obtained efficiently by taking off-the-shelf learning algorithms. For the second step, we show that the classifier adaptation problem can be reduced to a quadratic program problem, which is similar to linear SVMperf and can be efficiently solved. By exploiting nonlinear auxiliary classifiers, CAPO can generate nonlinear classifier which optimizes a large variety of performance measures including all the performance measure based on the contingency table and AUC, whilst keeping high computational efficiency. Empirical studies show that CAPO is effective and of high computational efficiency, and even it is more efficient than linear SVMperf.\n    ",
        "submission_date": "2010-12-04T00:00:00",
        "last_modified_date": "2012-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.1615",
        "title": "Argudas: arguing with gene expression information",
        "authors": [
            "Kenneth McLeod",
            "Gus Ferguson",
            "Albert Burger"
        ],
        "abstract": "In situ hybridisation gene expression information helps biologists identify where a gene is expressed. However, the databases that republish the experimental information are often both incomplete and inconsistent. This paper examines a system, Argudas, designed to help tackle these issues. Argudas is an evolution of an existing system, and so that system is reviewed as a means of both explaining and justifying the behaviour of Argudas. Throughout the discussion of Argudas a number of issues will be raised including the appropriateness of argumentation in biology and the challenges faced when integrating apparently similar online biological databases.\n    ",
        "submission_date": "2010-12-07T00:00:00",
        "last_modified_date": "2010-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.2042",
        "title": "MUDOS-NG: Multi-document Summaries Using N-gram Graphs (Tech Report)",
        "authors": [
            "George Giannakopoulos",
            "George Vouros",
            "Vangelis Karkaletsis"
        ],
        "abstract": "This report describes the MUDOS-NG summarization system, which applies a set of language-independent and generic methods for generating extractive summaries. The proposed methods are mostly combinations of simple operators on a generic character n-gram graph representation of texts. This work defines the set of used operators upon n-gram graphs and proposes using these operators within the multi-document summarization process in such subtasks as document analysis, salient sentence selection, query expansion and redundancy control. Furthermore, a novel chunking methodology is used, together with a novel way to assign concepts to sentences for query expansion. The experimental results of the summarization system, performed upon widely used corpora from the Document Understanding and the Text Analysis Conferences, are promising and provide evidence for the potential of the generic methods introduced. This work aims to designate core methods exploiting the n-gram graph representation, providing the basis for more advanced summarization systems.\n    ",
        "submission_date": "2010-12-09T00:00:00",
        "last_modified_date": "2010-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.2496",
        "title": "On the Implementation of GNU Prolog",
        "authors": [
            "Daniel Diaz",
            "Salvador Abreu",
            "Philippe Codognet"
        ],
        "abstract": "GNU Prolog is a general-purpose implementation of the Prolog language, which distinguishes itself from most other systems by being, above all else, a native-code compiler which produces standalone executables which don't rely on any byte-code emulator or meta-interpreter. Other aspects which stand out include the explicit organization of the Prolog system as a multipass compiler, where intermediate representations are materialized, in Unix compiler tradition. GNU Prolog also includes an extensible and high-performance finite domain constraint solver, integrated with the Prolog language but implemented using independent lower-level mechanisms. This article discusses the main issues involved in designing and implementing GNU Prolog: requirements, system organization, performance and portability issues as well as its position with respect to other Prolog system implementations and the ISO standardization initiative.\n    ",
        "submission_date": "2010-12-11T00:00:00",
        "last_modified_date": "2010-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.2609",
        "title": "Inverse-Category-Frequency based supervised term weighting scheme for text categorization",
        "authors": [
            "Deqing Wang",
            "Hui Zhang"
        ],
        "abstract": "Term weighting schemes often dominate the performance of many classifiers, such as kNN, centroid-based classifier and SVMs. The widely used term weighting scheme in text categorization, i.e., ",
        "submission_date": "2010-12-13T00:00:00",
        "last_modified_date": "2012-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.3320",
        "title": "Data Conflict Resolution Using Trust Mappings",
        "authors": [
            "Wolfgang Gatterbauer",
            "Dan Suciu"
        ],
        "abstract": "In massively collaborative projects such as scientific or community databases, users often need to agree or disagree on the content of individual data items. On the other hand, trust relationships often exist between users, allowing them to accept or reject other users' beliefs by default. As those trust relationships become complex, however, it becomes difficult to define and compute a consistent snapshot of the conflicting information. Previous solutions to a related problem, the update reconciliation problem, are dependent on the order in which the updates are processed and, therefore, do not guarantee a globally consistent snapshot. This paper proposes the first principled solution to the automatic conflict resolution problem in a community database. Our semantics is based on the certain tuples of all stable models of a logic program. While evaluating stable models in general is well known to be hard, even for very simple logic programs, we show that the conflict resolution problem admits a PTIME solution. To the best of our knowledge, ours is the first PTIME algorithm that allows conflict resolution in a principled way. We further discuss extensions to negative beliefs and prove that some of these extensions are hard. This work is done in the context of the BeliefDB project at the University of Washington, which focuses on the efficient management of conflicts in community databases.\n    ",
        "submission_date": "2010-12-15T00:00:00",
        "last_modified_date": "2010-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.3947",
        "title": "Interpolation in Equilibrium Logic and Answer Set Programming: the Propositional Case",
        "authors": [
            "Dov Gabbay",
            "David Pearce",
            "Agust\u00ed n Valverde"
        ],
        "abstract": "Interpolation is an important property of classical and many non classical logics that has been shown to have interesting applications in computer science and AI. Here we study the Interpolation Property for the propositional version of the non-monotonic system of equilibrium logic, establishing weaker or stronger forms of interpolation depending on the precise interpretation of the inference relation. These results also yield a form of interpolation for ground logic programs under the answer sets semantics. For disjunctive logic programs we also study the property of uniform interpolation that is closely related to the concept of variable forgetting.\n    ",
        "submission_date": "2010-12-17T00:00:00",
        "last_modified_date": "2010-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.5546",
        "title": "Mining Multi-Level Frequent Itemsets under Constraints",
        "authors": [
            "Mohamed Salah Gouider",
            "Amine Farhat"
        ],
        "abstract": "Mining association rules is a task of data mining, which extracts knowledge in the form of significant implication relation of useful items (objects) from a database. Mining multilevel association rules uses concept hierarchies, also called taxonomies and defined as relations of type 'is-a' between objects, to extract rules that items belong to different levels of abstraction. These rules are more useful, more refined and more interpretable by the user. Several algorithms have been proposed in the literature to discover the multilevel association rules. In this article, we are interested in the problem of discovering multi-level frequent itemsets under constraints, involving the user in the research process. We proposed a technique for modeling and interpretation of constraints in a context of use of concept hierarchies. Three approaches for discovering multi-level frequent itemsets under constraints were proposed and discussed: Basic approach, \"Test and Generate\" approach and Pruning based Approach.\n    ",
        "submission_date": "2010-12-26T00:00:00",
        "last_modified_date": "2010-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.5754",
        "title": "Software Effort Estimation with Ridge Regression and Evolutionary Attribute Selection",
        "authors": [
            "Efi Papatheocharous",
            "Harris Papadopoulos",
            "Andreas S. Andreou"
        ],
        "abstract": "Software cost estimation is one of the prerequisite managerial activities carried out at the software development initiation stages and also repeated throughout the whole software life-cycle so that amendments to the total cost are made. In software cost estimation typically, a selection of project attributes is employed to produce effort estimations of the expected human resources to deliver a software product. However, choosing the appropriate project cost drivers in each case requires a lot of experience and knowledge on behalf of the project manager which can only be obtained through years of software engineering practice. A number of studies indicate that popular methods applied in the literature for software cost estimation, such as linear regression, are not robust enough and do not yield accurate predictions. Recently the dual variables Ridge Regression (RR) technique has been used for effort estimation yielding promising results. In this work we show that results may be further improved if an AI method is used to automatically select appropriate project cost drivers (inputs) for the technique. We propose a hybrid approach combining RR with a Genetic Algorithm, the latter evolving the subset of attributes for approximating effort more accurately. The proposed hybrid cost model has been applied on a widely known high-dimensional dataset of software project samples and the results obtained show that accuracy may be increased if redundant attributes are eliminated.\n    ",
        "submission_date": "2010-12-28T00:00:00",
        "last_modified_date": "2010-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1012.5755",
        "title": "DD-EbA: An algorithm for determining the number of neighbors in cost estimation by analogy using distance distributions",
        "authors": [
            "Makrina Viola Kosti",
            "Nikolaos Mittas",
            "Lefteris Angelis"
        ],
        "abstract": "Case Based Reasoning and particularly Estimation by Analogy, has been used in a number of problem-solving areas, such as cost estimation. Conventional methods, despite the lack of a sound criterion for choosing nearest projects, were based on estimation using a fixed and predetermined number of neighbors from the entire set of historical instances. This approach puts boundaries to the estimation ability of such algorithms, for they do not take into consideration that every project under estimation is unique and requires different handling. The notion of distributions of distances together with a distance metric for distributions help us to adapt the proposed method (we call it DD-EbA) each time to a specific case that is to be estimated without loosing in prediction power or computational cost. The results of this paper show that the proposed technique achieves the above idea in a very efficient way.\n    ",
        "submission_date": "2010-12-28T00:00:00",
        "last_modified_date": "2010-12-28T00:00:00"
    }
]