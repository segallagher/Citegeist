[
    {
        "url": "https://arxiv.org/abs/1301.0173",
        "title": "Knowledge Discovery System For Fiber Reinforced Polymer Matrix Composite Laminate",
        "authors": [
            "Doreswamy"
        ],
        "abstract": "In this paper Knowledge Discovery System (KDS) is proposed and implemented for the extraction of knowledge-mean stiffness of a polymer composite material in which when fibers are placed at different orientations. Cosine amplitude method is implemented for retrieving compatible polymer matrix and reinforcement fiber which is coming under predicted fiber class, from the polymer and reinforcement database respectively, based on the design requirements. Fuzzy classification rules to classify fibers into short, medium and long fiber classes are derived based on the fiber length and the computed or derive critical length of fiber. Longitudinal and Transverse module of Polymer Matrix Composite consisting of seven layers with different fiber volume fractions and different fibers orientations at 0,15,30,45,60,75 and 90 degrees are analyzed through Rule-of Mixture material design model. The analysis results are represented in different graphical steps and have been measured with statistical parameters. This data mining application implemented here has focused the mechanical problems of material design and analysis. Therefore, this system is an expert decision support system for optimizing the materials performance for designing light-weight and strong, and cost effective polymer composite materials.\n    ",
        "submission_date": "2013-01-02T00:00:00",
        "last_modified_date": "2013-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0176",
        "title": "Similarity Measuring Approuch for Engineering Materials Selection",
        "authors": [
            "Doreswamy",
            "M.N.Vanajakshi"
        ],
        "abstract": "Advanced engineering materials design involves the exploration of massive multidimensional feature spaces, the correlation of materials properties and the processing parameters derived from disparate sources. The search for alternative materials or processing property strategies, whether through analytical, experimental or simulation approaches, has been a slow and arduous task, punctuated by infrequent and often expected discoveries. A few systematic efforts have been made to analyze the trends in data as a basis for classifications and predictions. This is particularly due to the lack of large amounts of organized data and more importantly the challenging of shifting through them in a timely and efficient manner. The application of recent advances in Data Mining on materials informatics is the state of art of computational and experimental approaches for materials discovery. In this paper similarity based engineering materials selection model is proposed and implemented to select engineering materials based on the composite materials constraints. The result reviewed from this model is sustainable for effective decision making in advanced engineering materials design applications.\n    ",
        "submission_date": "2013-01-02T00:00:00",
        "last_modified_date": "2013-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0216",
        "title": "Applying Strategic Multiagent Planning to Real-World Travel Sharing Problems",
        "authors": [
            "Jan Hrn\u010d\u00ed\u0159",
            "Michael Rovatsos"
        ],
        "abstract": "Travel sharing, i.e., the problem of finding parts of routes which can be shared by several travellers with different points of departure and destinations, is a complex multiagent problem that requires taking into account individual agents' preferences to come up with mutually acceptable joint plans. In this paper, we apply state-of-the-art planning techniques to real-world public transportation data to evaluate the feasibility of multiagent planning techniques in this domain. The potential application value of improving travel sharing technology has great application value due to its ability to reduce the environmental impact of travelling while providing benefits to travellers at the same time. We propose a three-phase algorithm that utilises performant single-agent planners to find individual plans in a simplified domain first, then merges them using a best-response planner which ensures resulting solutions are individually rational, and then maps the resulting plan onto the full temporal planning domain to schedule actual journeys. The evaluation of our algorithm on real-world, multi-modal public transportation data for the United Kingdom shows linear scalability both in the scenario size and in the number of agents, where trade-offs have to be made between total cost improvement, the percentage of feasible timetables identified for journeys, and the prolongation of these journeys. Our system constitutes the first implementation of strategic multiagent planning algorithms in large-scale domains and provides insights into the engineering process of translating general domain-independent multiagent planning algorithms to real-world applications.\n    ",
        "submission_date": "2013-01-02T00:00:00",
        "last_modified_date": "2013-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0302",
        "title": "MANCaLog: A Logic for Multi-Attribute Network Cascades (Technical Report)",
        "authors": [
            "Paulo Shakarian",
            "Gerardo I. Simari",
            "Robert Schroeder"
        ],
        "abstract": "The modeling of cascade processes in multi-agent systems in the form of complex networks has in recent years become an important topic of study due to its many applications: the adoption of commercial products, spread of disease, the diffusion of an idea, etc. In this paper, we begin by identifying a desiderata of seven properties that a framework for modeling such processes should satisfy: the ability to represent attributes of both nodes and edges, an explicit representation of time, the ability to represent non-Markovian temporal relationships, representation of uncertain information, the ability to represent competing cascades, allowance of non-monotonic diffusion, and computational tractability. We then present the MANCaLog language, a formalism based on logic programming that satisfies all these desiderata, and focus on algorithms for finding minimal models (from which the outcome of cascades can be obtained) as well as how this formalism can be applied in real world scenarios. We are not aware of any other formalism in the literature that meets all of the above requirements.\n    ",
        "submission_date": "2013-01-02T00:00:00",
        "last_modified_date": "2013-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0550",
        "title": "Markov Equivalence Classes for Maximal Ancestral Graphs",
        "authors": [
            "Ayesha R. Ali",
            "Thomas S. Richardson"
        ],
        "abstract": "Ancestral graphs are a class of graphs that encode conditional independence relations arising in DAG models with latent and selection variables, corresponding to marginalization and conditioning.  However, for any ancestral graph, there may be several other graphs to which it is Markov equivalent.  We introduce a simple representation of a Markov equivalence class of ancestral graphs, thereby facilitating model search. \\ More specifically, we define a join operation on ancestral graphs which will associate a unique graph with a Markov equivalence class.  We also extend the separation criterion for ancestral graphs (which is an extension of d-separation) and provide a proof of the pairwise Markov property for joined ancestral graphs. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0552",
        "title": "A constraint satisfaction approach to the robust spanning tree problem with interval data",
        "authors": [
            "Ionut Aron",
            "Pascal Van Hentenryck"
        ],
        "abstract": "Robust optimization is one of the fundamental approaches to deal with uncertainty in combinatorial optimization. This paper considers the robust spanning tree problem with interval data, which arises in a variety of telecommunication applications. It proposes a constraint satisfaction approach using a combinatorial lower bound, a pruning component that removes infeasible and suboptimal edges, as well as a search strategy exploring the most uncertain edges first.  The resulting algorithm is shown to produce very dramatic improvements over the mathematical programming approach of Yaman et al. and to enlarge considerably the class of problems amenable to effective solutions \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0553",
        "title": "On the Construction of the Inclusion Boundary Neighbourhood for Markov Equivalence Classes of Bayesian Network Structures",
        "authors": [
            "Vincent Auvray",
            "Louis Wehenkel"
        ],
        "abstract": "The problem of learning Markov equivalence classes of Bayesian network structures may be solved by searching for the maximum of a scoring metric in a space of these classes. This paper deals with the definition and analysis of one such search space. We use a theoretically motivated neighbourhood, the inclusion boundary, and represent equivalence classes by essential graphs. We show that this search space is connected and that the score of the neighbours can be evaluated incrementally. We devise a practical way of building this neighbourhood for an essential graph that is purely graphical and does not explicitely refer to the underlying independences. We find that its size can be intractable, depending on the complexity of the essential graph of the equivalence class. The emphasis is put on the potential use of this space with greedy hill -climbing search \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0555",
        "title": "Bipolar Possibilistic Representations",
        "authors": [
            "Salem Benferhat",
            "Didier Dubois",
            "Souhila Kaci",
            "Henri Prade"
        ],
        "abstract": "Recently, it has been emphasized that the possibility theory framework allows us to distinguish between i) what is possible because it is not ruled out by the available knowledge, and ii) what is possible for sure. This distinction may be useful when representing knowledge, for modelling values which are not impossible because they are consistent with the available knowledge on the one hand, and values guaranteed to be possible because reported from observations on the other hand. It is also of interest when expressing preferences, to point out values which are positively desired among those which are not rejected. This distinction can be encoded by two types of constraints expressed in terms of necessity measures and in terms of guaranteed possibility functions, which induce a pair of possibility distributions at the semantic level. A consistency condition should ensure that what is claimed to be guaranteed as possible is indeed not impossible. The present paper investigates the representation of this bipolar view, including the case when it is stated by means of conditional measures, or by means of comparative context-dependent constraints. The interest of this bipolar framework, which has been recently stressed for expressing preferences, is also pointed out in the representation of diagnostic knowledge. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0557",
        "title": "Qualitative MDPs and POMDPs: An Order-Of-Magnitude Approximation",
        "authors": [
            "Blai Bonet",
            "Judea Pearl"
        ],
        "abstract": "We develop a qualitative theory of Markov Decision Processes (MDPs) and Partially Observable MDPs that can be used to model sequential decision making tasks when only qualitative information is available. Our approach is based upon an order-of-magnitude approximation of both probabilities and utilities, similar to epsilon-semantics. The result is a qualitative theory that has close ties with the standard maximum-expected-utility theory and is amenable to general planning techniques.\n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0558",
        "title": "Introducing Variable Importance Tradeoffs into CP-Nets",
        "authors": [
            "Ronen I. Brafman",
            "Carmel Domshlak"
        ],
        "abstract": "The ability to make decisions and to assess potential courses of action is a corner-stone of many AI applications, and usually this requires explicit information about the decision-maker      s preferences. IN many applications,     preference elicitation IS a serious ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0559",
        "title": "Planning under Continuous Time and Resource Uncertainty: A Challenge for AI",
        "authors": [
            "John Bresina",
            "Richard Dearden",
            "Nicolas Meuleau",
            "Sailesh Ramkrishnan",
            "David Smith",
            "Richard Washington"
        ],
        "abstract": "We outline a class of problems, typical of Mars rover operations, that are problematic for current methods of planning under uncertainty.  The existing methods fail because they suffer from one or more of the following limitations: 1) they rely on very simple models of actions and time, 2) they assume that uncertainty is manifested in discrete action outcomes, 3) they are only practical for very small problems.  For many real world problems, these assumptions fail to hold.  In particular, when planning the activities for a Mars rover, none of the above assumptions is valid: 1) actions can be concurrent and have differing durations, 2) there is uncertainty concerning action durations and consumption of continuous resources like power, and 3) typical daily plans involve on the order of a hundred actions.  This class of problems may be of particular interest to the UAI community because both classical and decision-theoretic planning techniques may be useful in solving it.  We describe the rover problem, discuss previous work on planning under uncertainty, and present a detailed, but very small, example illustrating some of the difficulties of finding good plans. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0560",
        "title": "Generalized Instrumental Variables",
        "authors": [
            "Carlos Brito",
            "Judea Pearl"
        ],
        "abstract": "This paper concerns the assessment of direct causal effects from a combination of: (i) non-experimental data, and (ii) qualitative domain knowledge. Domain knowledge is encoded in the form of a directed acyclic graph (DAG), in which all interactions are assumed linear, and some variables are presumed to be unobserved. We provide a generalization of the well-known method of Instrumental Variables, which allows its application to models with few conditional independeces. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0561",
        "title": "Finding Optimal Bayesian Networks",
        "authors": [
            "David Maxwell Chickering",
            "Christopher Meek"
        ],
        "abstract": "In this paper, we derive optimality results for greedy Bayesian-network search algorithms that perform single-edge modifications at each step and use asymptotically consistent scoring criteria. Our results extend those of Meek (1997) and Chickering (2002), who demonstrate that in the limit of large datasets, if the generative distribution is perfect with respect to a DAG defined over the observable variables, such search algorithms will identify this optimal (i.e. generative) DAG model. We relax their assumption about the generative distribution, and assume only that this distribution satisfies the {em composition property} over the observable variables, which is a more realistic assumption for real domains. Under this assumption, we guarantee that the search algorithms identify an {em inclusion-optimal} model; that is, a model that (1) contains the generative distribution and (2) has no sub-model that contains this distribution. In addition, we show that the composition property is guaranteed to hold whenever the dependence relationships in the generative distribution can be characterized by paths between singleton elements in some generative graphical model (e.g. a DAG, a chain graph, or a Markov network) even when the generative model includes unobserved variables, and even when the observed data is subject to selection bias. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0564",
        "title": "Iterative Join-Graph Propagation",
        "authors": [
            "Rina Dechter",
            "Kalev Kask",
            "Robert Mateescu"
        ],
        "abstract": "The paper presents an iterative version of join-tree clustering that applies the message passing of join-tree clustering algorithm to join-graphs rather than to join-trees, iteratively. It is inspired by the success of Pearl's belief propagation algorithm as an iterative approximation scheme on one hand, and by a recently introduced mini-clustering i. success as an anytime approximation method, on the other. The proposed Iterative Join-graph Propagation IJGP belongs to the class of generalized belief propagation methods, recently proposed using analogy with algorithms in statistical physics. Empirical evaluation of this approach on a number of problem classes demonstrates that even the most time-efficient variant is almost always superior to IBP and MC i, and is sometimes more accurate by as much as several orders of magnitude. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0566",
        "title": "Causes and Explanations in the Structural-Model Approach: Tractable Cases",
        "authors": [
            "Thomas Eiter",
            "Thomas Lukasiewicz"
        ],
        "abstract": "In this paper, we continue our research on the algorithmic aspects of Halpern and Pearl's causes and explanations in the structural-model approach. To this end, we present new characterizations of weak causes for certain classes of causal models, which show that under suitable restrictions deciding causes and explanations is tractable. To our knowledge, these are the first explicit tractability results for the structural-model approach.\n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0568",
        "title": "Factorization of Discrete Probability Distributions",
        "authors": [
            "Dan Geiger",
            "Christopher Meek",
            "Bernd Sturmfels"
        ],
        "abstract": "We formulate necessary and sufficient conditions for an arbitrary discrete probability distribution to factor according to an undirected graphical model, or a log-linear model, or other more general exponential models. This result generalizes the well known Hammersley-Clifford Theorem. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0569",
        "title": "Statistical Decisions Using Likelihood Information Without Prior Probabilities",
        "authors": [
            "Phan H. Giang",
            "Prakash P. Shenoy"
        ],
        "abstract": "This paper presents a decision-theoretic approach to statistical inference that satisfies the likelihood principle (LP) without using prior information. Unlike the Bayesian approach, which also satisfies LP, we do not assume knowledge of the prior distribution of the unknown parameter. With respect to information that can be obtained from an experiment, our solution is more efficient than Wald's minimax ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0570",
        "title": "Reduction of Maximum Entropy Models to Hidden Markov Models",
        "authors": [
            "Joshua Goodman"
        ],
        "abstract": "We show that maximum entropy (maxent) models can be modeled with certain kinds of HMMs, allowing us to construct maxent models with hidden variables, hidden state sequences, or other characteristics. The models can be trained using the forward-backward algorithm.  While the results are primarily of theoretical interest, unifying apparently unrelated concepts, we also give experimental results for a maxent model with a hidden variable on a word disambiguation task; the model outperforms standard  techniques. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0571",
        "title": "Distributed Planning in Hierarchical Factored MDPs",
        "authors": [
            "Carlos E. Guestrin",
            "Geoffrey Gordon"
        ],
        "abstract": "We present a principled and efficient planning algorithm for collaborative multiagent dynamical systems.  All computation, during both the planning and the execution phases, is distributed among the agents; each agent only needs to model and plan for a small part of the system.  Each of these local subsystems is small, but once they are combined they can represent an exponentially larger problem.  The subsystems are connected through a subsystem hierarchy.  Coordination and communication between the agents is not imposed, but derived directly from the structure of this hierarchy.  A globally consistent plan is achieved by a message passing algorithm, where messages correspond to natural local reward functions and are computed by local linear programs; another message passing algorithm allows us to execute the resulting policy.  When two portions of the hierarchy share the same structure, our algorithm can reuse plans and messages to speed up computation. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0572",
        "title": "Expectation Propogation for approximate inference in dynamic Bayesian networks",
        "authors": [
            "Tom Heskes",
            "Onno Zoeter"
        ],
        "abstract": "We describe expectation propagation for approximate inference in dynamic Bayesian networks as a natural extension of Pearl      s exact belief ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0574",
        "title": "Unconstrained Influence Diagrams",
        "authors": [
            "Finn Verner Jensen",
            "Marta Vomlelova"
        ],
        "abstract": "We extend the language of influence diagrams to cope with decision scenarios where the order of decisions and observations is not determined.  As the ordering of decisions is dependent on the evidence, a step-strategy of such a scenario is a sequence of dependent choices of the next action. A strategy is a step-strategy together with selection functions for decision actions. The structure of a step-strategy can be represented as a DAG with nodes labeled with action variables. We introduce the concept of GS-DAG: a DAG incorporating an optimal step-strategy for any instantiation. We give a method for constructing GS-DAGs, and we show how to use a GS-DAG for determining an optimal strategy. Finally we discuss how analysis of relevant past can be used to reduce the size of the GS-DAG. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0576",
        "title": "A Bayesian Network Scoring Metric That Is Based On Globally Uniform Parameter Priors",
        "authors": [
            "Mehmet Kayaalp",
            "Gregory F. Cooper"
        ],
        "abstract": "We introduce a new Bayesian network (BN) scoring metric called the Global Uniform (GU) metric. This metric is based on a particular type of default parameter prior. Such priors may be useful when a BN developer is not willing or able to specify domain-specific parameter priors. The GU parameter prior specifies that every prior joint probability distribution P consistent with a BN structure S is considered to be equally likely. Distribution P is consistent with S if P includes just the set of independence relations defined by S. We show that the GU metric addresses some undesirable behavior of the BDeu and K2 Bayesian network scoring metrics, which also use particular forms of default parameter priors.  A closed form formula for computing GU for special classes of BNs is derived. Efficiently computing GU for an arbitrary BN remains an open problem. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0580",
        "title": "Value Function Approximation in Zero-Sum Markov Games",
        "authors": [
            "Michail Lagoudakis",
            "Ron Parr"
        ],
        "abstract": "This paper investigates value function approximation in the context of zero-sum Markov games, which can be viewed as a generalization of the Markov decision process (MDP) framework to the two-agent case.  We generalize error bounds from MDPs to Markov games and describe generalizations of reinforcement learning algorithms to Markov games. We present a generalization of the optimal stopping problem to a two-player simultaneous move Markov game.  For this special problem, we provide stronger bounds and can guarantee convergence for LSTD and temporal difference learning with linear value function approximation. We demonstrate the viability of value function approximation for Markov games by using the Least squares policy iteration (LSPI) algorithm to learn good policies for a soccer domain and a flow control problem. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0582",
        "title": "Monitoring a Complez Physical System using a Hybrid Dynamic Bayes Net",
        "authors": [
            "Uri Lerner",
            "Brooks Moses",
            "Maricia Scott",
            "Sheila McIlraith",
            "Daphne Koller"
        ],
        "abstract": "The Reverse Water Gas Shift system (RWGS) is a complex physical system designed to produce oxygen from the carbon dioxide atmosphere on Mars.  If sent to Mars, it would operate without human supervision, thus requiring a reliable automated system for monitoring and control.  The RWGS presents many challenges typical of real-world systems, including:  noisy and biased sensors, nonlinear behavior, effects that are manifested over different time granularities, and unobservability of many important quantities.  In this paper we model the RWGS using a hybrid (discrete/continuous) Dynamic Bayesian Network (DBN), where the state at each time slice contains 33 discrete and 184 continuous variables.  We show how the system state can be tracked using probabilistic inference over the model.  We discuss how to deal with the various challenges presented by the RWGS, providing a suite of techniques that are likely to be useful in a wide range of applications.  In particular, we describe a general framework for dealing with nonlinear behavior using numerical integration techniques, extending the successful Unscented Filter.  We also show how to use a fixed-point computation to deal with effects that develop at different time scales, specifically rapid changes occurring during slowly changing processes.  We test our model using real data collected from the RWGS, demonstrating the feasibility of hybrid DBNs for monitoring complex real-world physical systems. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0583",
        "title": "Polynomial Value Iteration Algorithms for Detrerminstic MDPs",
        "authors": [
            "Omid Madani"
        ],
        "abstract": "Value iteration is a commonly used and empirically competitive method in solving many Markov decision process problems.  However, it is known that value iteration has only pseudo-polynomial complexity in general.  We establish a somewhat surprising polynomial bound for value iteration on deterministic Markov decision (DMDP) problems.  We show that the basic value iteration procedure converges to the highest average reward cycle on a DMDP problem in heta(n^2) iterations, or heta(mn^2) total time, where n denotes the number of states, and m the number of edges.  We give two extensions of value iteration that solve the DMDP in heta(mn) time. We explore the analysis of policy iteration algorithms and report on an empirical study of value iteration showing that its convergence is much faster on random sparse graphs. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0584",
        "title": "Decayed MCMC Filtering",
        "authors": [
            "Bhaskara Marthi",
            "Hanna Pasula",
            "Stuart Russell",
            "Yuval Peres"
        ],
        "abstract": "Filtering---estimating the state of a partially observable Markov process from a sequence of observations---is one of the most widely studied problems in control theory, AI, and computational statistics. Exact computation of the posterior distribution is generally intractable for large discrete systems and for nonlinear continuous systems, so a good deal of effort has gone into developing robust approximation algorithms.  This paper describes a simple stochastic approximation algorithm for filtering called {em decayed MCMC}. The algorithm applies Markov chain Monte Carlo sampling to the space of state trajectories using a proposal distribution that favours flips of more recent state variables.  The formal analysis of the algorithm involves a generalization of standard coupling arguments for MCMC convergence.  We prove that for any ergodic underlying Markov process, the convergence time of decayed MCMC with inverse-polynomial decay remains bounded as the length of the observation sequence grows.  We show experimentally that decayed MCMC is at least competitive with other approximation algorithms such as particle filtering.\n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0585",
        "title": "Formalizing Scenario Analysis",
        "authors": [
            "Peter McBurney",
            "Simon Parsons"
        ],
        "abstract": "We propose a formal treatment of scenarios in the context of a dialectical argumentation formalism for qualitative reasoning about uncertain propositions. Our formalism extends prior work in which arguments for and against uncertain propositions were presented and compared in interaction spaces called Agoras. We now define the notion of a scenario in this framework and use it to define a set of qualitative uncertainty labels for propositions across a collection of scenarios. This work is intended to lead to a formal theory of scenarios and scenario analysis. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0589",
        "title": "Real-valued All-Dimensions search: Low-overhead rapid searching over subsets of attributes",
        "authors": [
            "Andrew Moore",
            "Jeff Schneider"
        ],
        "abstract": "This paper is about searching the combinatorial space of contingency tables during the inner loop of a nonlinear statistical optimization. Examples of this operation in various data analytic communities include searching for nonlinear combinations of attributes that contribute significantly to a regression (Statistics), searching for items to include in a decision list (machine learning) and association rule hunting (Data Mining).\n",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0590",
        "title": "Factored Particles for Scalable Monitoring",
        "authors": [
            "Brenda Ng",
            "Leonid Peshkin",
            "Avi Pfeffer"
        ],
        "abstract": "Exact monitoring in dynamic Bayesian networks is intractable, so approximate algorithms are necessary.  This paper presents a new family of  approximate monitoring algorithms that combine the best qualities of the particle filtering and Boyen-Koller methods.  Our algorithms maintain an approximate representation the belief state in the form of sets of factored particles, that correspond to samples of clusters of state variables.   Empirical results show that our algorithms outperform both ordinary particle filtering and the Boyen-Koller algorithm on large systems. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0591",
        "title": "Continuous Time Bayesian Networks",
        "authors": [
            "Uri Nodelman",
            "Christian R. Shelton",
            "Daphne Koller"
        ],
        "abstract": "In this paper we present a language for finite state continuous time Bayesian networks (CTBNs), which describe structured stochastic processes that evolve over continuous time.  The state of the system is decomposed into a set of local variables whose values change over time.  The dynamics of the system are described by specifying the behavior of each local variable as a function of its parents in a directed (possibly cyclic) graph.  The model specifies, at any given point in time, the distribution over two aspects: when a local variable changes its value and the next value it takes.  These distributions are determined by the variable      s CURRENT value AND the CURRENT VALUES OF its parents IN the ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0592",
        "title": "MAP Complexity Results and Approximation Methods",
        "authors": [
            "James D. Park"
        ],
        "abstract": "MAP is the problem of finding a most probable instantiation of a set of nvariables in a Bayesian network, given some evidence.  MAP appears to be a significantly harder problem than the related problems of computing the probability of evidence Pr, or MPE a special case of MAP.  Because of the complexity of MAP, and the lack of viable algorithms to approximate it,MAP computations are generally avoided by practitioners. This paper investigates the complexity of MAP.  We show that MAP is complete for NP.  We also provide negative complexity results for elimination based algorithms.  It turns out that MAP remains hard even when MPE, and Pr are easy.  We show that MAP is NPcomplete when the networks are restricted to polytrees, and even then can not be effectively approximated. Because there is no approximation algorithm with guaranteed results, we investigate best effort approximations. We introduce a generic MAP approximation framework.  As one instantiation of it, we implement local search coupled with belief propagation BP to approximate MAP.  We show how to extract approximate evidence retraction information from belief propagation which allows us to perform efficient local search. This allows MAP approximation even on networks that are too complex to even exactly solve the easier problems of computing Pr or MPE.  Experimental results indicate that using BP and local search provides accurate MAP estimates in many cases.\n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0594",
        "title": "Modelling Information Incorporation in Markets, with Application to Detecting and Explaining Events",
        "authors": [
            "David M Pennock",
            "Sandip Debnath",
            "Eric Glover",
            "C. Lee Giles"
        ],
        "abstract": "We develop a model of how information flows into a market, and derive algorithms for automatically detecting and explaining relevant events. We analyze data from twenty-two \"political stock markets\" (i.e., betting markets on political outcomes) on the Iowa Electronic Market (IEM).  We prove that, under certain efficiency assumptions, prices in such betting markets will on average approach the correct outcomes over time, and show that IEM data conforms closely to the theory. We present a simple model of a betting market where information is revealed over time, and show a qualitative correspondence between the model and real market data. We also present an algorithm for automatically detecting significant events and generating semantic explanations of their origin. The algorithm operates by discovering significant changes in vocabulary on online news sources (using expected entropy loss) that align with major price spikes in related betting markets. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0596",
        "title": "From Qualitative to Quantitative Probabilistic Networks",
        "authors": [
            "Silja Renooij",
            "Linda C. van der Gaag"
        ],
        "abstract": "Quantification is well known to be a major obstacle in the construction of a probabilistic network, especially when relying on human experts for this purpose.  The construction of a qualitative probabilistic network has been proposed as an initial step in a network      s quantification,     since the qualitative network can be used TO gain preliminary insight      IN the projected networks reasoning behaviour.  We extend on this idea and present a new type of network in which both signs and numbers are specified; we further present an associated algorithm for probabilistic inference.  Building upon these semi-qualitative networks, a probabilistic network can be quantified and studied in a stepwise manner.  As a result, modelling inadequacies can be detected and amended at an early stage in the quantification process. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0597",
        "title": "Inference with Seperately Specified Sets of Probabilities in Credal Networks",
        "authors": [
            "Jose Carlos Ferreira da Rocha",
            "Fabio Gagliardi Cozman"
        ],
        "abstract": "We present new algorithms for inference in  credal networks --- directed acyclic graphs associated with sets of probabilities. Credal networks are here interpreted as encoding strong independence relations among variables. We first present a theory of credal networks based on separately specified sets of probabilities. We also show that inference with polytrees is NP-hard in this setting. We then introduce new techniques that reduce the computational effort demanded by inference, particularly in polytrees, by exploring separability of credal sets. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0598",
        "title": "Asymptotic Model Selection for Naive Bayesian Networks",
        "authors": [
            "Dmitry Rusakov",
            "Dan Geiger"
        ],
        "abstract": "We develop a closed form asymptotic formula to compute the marginal likelihood of data given a naive Bayesian network model with two hidden states and binary features. This formula deviates from the standard BIC score. Our work provides a concrete example that the BIC score is generally not valid for statistical models that belong to a stratified exponential family. This stands in contrast to linear and curved exponential families, where the BIC score has been proven to provide a correct approximation for the marginal likelihood. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0603",
        "title": "Real-Time Inference with Large-Scale Temporal Bayes Nets",
        "authors": [
            "Masami Takikawa",
            "Bruce D'Ambrosio",
            "Ed Wright"
        ],
        "abstract": "An increasing number of applications require real-time reasoning under uncertainty with streaming input. The temporal (dynamic) Bayes net formalism provides a powerful representational framework for such applications.  However, existing exact inference algorithms for dynamic Bayes nets do not scale to the size of models required for real world applications which often contain hundreds or even thousands of variables for each time slice. In addition, existing algorithms were not developed with real-time processing in mind.  We have developed a new computational approach to support real-time exact inference in large temporal Bayes nets.  Our approach tackles scalability by recognizing that the complexity of the inference depends on the number of interface nodes between time slices and by exploiting the distinction between static and dynamic nodes in order to reduce the number of interface nodes and to factorize their joint probability distribution. We approach the real-time issue by organizing temporal Bayes nets into static representations, and then using the symbolic probabilistic inference algorithm to derive analytic expressions for the static representations. The parts of these expressions that do not change at each time step are pre-computed. The remaining parts are compiled into efficient procedural code so that the memory and CPU resources required by the inference are small and  fixed. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0605",
        "title": "Loopy Belief Propogation and Gibbs Measures",
        "authors": [
            "Sekhar Tatikonda",
            "Michael I. Jordan"
        ],
        "abstract": "We address the question of convergence in the loopy belief propagation (LBP) algorithm.  Specifically, we relate convergence of LBP to the existence of a weak limit for a sequence of Gibbs measures defined on the LBP      s associated computation ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0606",
        "title": "Anytime State-Based Solution Methods for Decision Processes with non-Markovian Rewards",
        "authors": [
            "Sylvie Thiebaux",
            "Froduald Kabanza",
            "John Slanley"
        ],
        "abstract": "A popular approach to solving a decision process with non-Markovian rewards (NMRDP) is to exploit a compact representation of the reward function to automatically translate the NMRDP into an equivalent Markov decision process (MDP) amenable to our favorite MDP solution method. The contribution of this paper is a representation of non-Markovian reward functions and a translation into MDP aimed at making the best possible use of state-based anytime algorithms as the solution method. By explicitly constructing and exploring only parts of the state space, these algorithms are able to trade computation time for policy quality, and have proven quite effective in dealing with large MDPs. Our representation extends future linear temporal logic (FLTL) to express rewards. Our translation has the effect of embedding model-checking in the solution method. It results in an MDP of the minimal size achievable without stepping outside the anytime framework, and consequently in better policies by the deadline. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0608",
        "title": "On the Testable Implications of Causal Models with Hidden Variables",
        "authors": [
            "Jin Tian",
            "Judea Pearl"
        ],
        "abstract": "The validity OF a causal model can be tested ONLY IF      the model imposes constraints ON     the probability distribution that governs the generated data. IN the      presence OF unmeasured variables,     causal models may impose two types OF constraints : conditional      independencies,     AS READ through the d - separation criterion, AND     functional constraints, FOR     which no general criterion IS ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0609",
        "title": "Exploiting Functional Dependence in Bayesian Network Inference",
        "authors": [
            "Jirka Vomlel"
        ],
        "abstract": "We propose an efficient method for Bayesian network inference in models with functional dependence. We generalize the multiplicative factorization method originally designed by Takikawa and D      Ambrosio(1999) FOR     models WITH independence OF causal ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0611",
        "title": "Decision Principles to justify Carnap's Updating Method and to Suggest Corrections of Probability Judgments (Invited Talks)",
        "authors": [
            "Peter P. Wakker"
        ],
        "abstract": "This paper uses decision-theoretic principles to obtain new insights into the assessment and updating of probabilities. First, a new foundation of Bayesianism is given. It does not require infinite atomless uncertainties as did Savage      s classical result, AND     can therefore be applied TO ANY finite Bayesian ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0614",
        "title": "Inductive Policy Selection for First-Order MDPs",
        "authors": [
            "Sung Wook Yoon",
            "Alan Fern",
            "Robert Givan"
        ],
        "abstract": "We select policies for large Markov Decision Processes (MDPs) with compact first-order representations. We find policies that generalize well as the number of objects in the domain grows, potentially without bound. Existing dynamic-programming approaches based on flat, propositional, or first-order representations either are impractical here or do not naturally scale as the number of objects grows without bound. We implement and evaluate an alternative approach that induces first-order policies using training data constructed by solving small problem instances using PGraphplan (Blum & Langford, 1999). Our policies are represented as ensembles of decision lists, using a taxonomic concept language. This approach extends the work of Martin and Geffner (2000) to stochastic domains, ensemble learning, and a wider variety of problems. Empirically, we find \"good\" policies for several stochastic first-order MDPs that are beyond the scope of previous approaches. We also discuss the application of this work to the relational reinforcement-learning  problem. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.1385",
        "title": "Translating NP-SPEC into ASP",
        "authors": [
            "Mario Alviano",
            "Wolfgang Faber"
        ],
        "abstract": "NP-SPEC is a language for specifying problems in NP in a declarative way. Despite the fact that the semantics of the language was given by referring to Datalog with circumscription, which is very close to ASP, so far the only existing implementations are by means of ECLiPSe Prolog and via Boolean satisfiability solvers. In this paper, we present translations from NP-SPEC into various forms of ASP and analyze them. We also argue that it might be useful to incorporate certain language constructs of NP-SPEC into mainstream ASP.\n    ",
        "submission_date": "2013-01-08T00:00:00",
        "last_modified_date": "2013-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.1387",
        "title": "Language ASP{f} with Arithmetic Expressions and Consistency-Restoring Rules",
        "authors": [
            "Marcello Balduccini",
            "Michael Gelfond"
        ],
        "abstract": "In this paper we continue the work on our extension of Answer Set Programming by non-Herbrand functions and add to the language support for arithmetic expressions and various inequality relations over non-Herbrand functions, as well as consistency-restoring rules from CR-Prolog. We demonstrate the use of this latest version of the language in the representation of important kinds of knowledge.\n    ",
        "submission_date": "2013-01-08T00:00:00",
        "last_modified_date": "2013-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.1388",
        "title": "Utilizing ASP for Generating and Visualizing Argumentation Frameworks",
        "authors": [
            "G\u00fcnther Charwat",
            "Johannes Peter Wallner",
            "Stefan Woltran"
        ],
        "abstract": "Within the area of computational models of argumentation, the instantiation-based approach is gaining more and more attention, not at least because meaningful input for Dung's abstract frameworks is provided in that way. In a nutshell, the aim of instantiation-based argumentation is to form, from a given knowledge base, a set of arguments and to identify the conflicts between them. The resulting network is then evaluated by means of extension-based semantics on an abstract level, i.e. on the resulting graph. While several systems are nowadays available for the latter step, the automation of the instantiation process itself has received less attention. In this work, we provide a novel approach to construct and visualize an argumentation framework from a given knowledge base. The system we propose relies on Answer-Set Programming and follows a two-step approach. A first program yields the logic-based arguments as its answer-sets; a second program is then used to specify the relations between arguments based on the answer-sets of the first program. As it turns out, this approach not only allows for a flexible and extensible tool for instantiation-based argumentation, but also provides a new method for answer-set visualization in general.\n    ",
        "submission_date": "2013-01-08T00:00:00",
        "last_modified_date": "2013-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.1389",
        "title": "Planning and Scheduling in Hybrid Domains Using Answer Set Programming",
        "authors": [
            "Sandeep Chintabathina"
        ],
        "abstract": "In this paper we present an Action Language-Answer Set Programming based approach to solving planning and scheduling problems in hybrid domains - domains that exhibit both discrete and continuous behavior. We use action language H to represent the domain and then translate the resulting theory into an A-Prolog program. In this way, we reduce the problem of finding solutions to planning and scheduling problems to computing answer sets of A-Prolog programs. We cite a planning and scheduling example from the literature and show how to model it in H. We show how to translate the resulting H theory into an equivalent A-Prolog program. We compute the answer sets of the resulting program using a hybrid solver called EZCSP which loosely integrates a constraint solver with an answer set solver. The solver allows us reason about constraints over reals and compute solutions to complex planning and scheduling problems. Results have shown that our approach can be applied to any planning and scheduling problem in hybrid domains.\n    ",
        "submission_date": "2013-01-08T00:00:00",
        "last_modified_date": "2013-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.1392",
        "title": "Answer Set Programming for Stream Reasoning",
        "authors": [
            "Martin Gebser",
            "Torsten Grote",
            "Roland Kaminski",
            "Philipp Obermeier",
            "Orkunt Sabuncu",
            "Torsten Schaub"
        ],
        "abstract": "The advance of Internet and Sensor technology has brought about new challenges evoked by the emergence of continuous data streams. Beyond rapid data processing, application areas like ambient assisted living, robotics, or dynamic scheduling involve complex reasoning tasks. We address such scenarios and elaborate upon approaches to knowledge-intense stream reasoning, based on Answer Set Programming (ASP). While traditional ASP methods are devised for singular problem solving, we develop new techniques to formulate and process problems dealing with emerging as well as expiring data in a seamless way.\n    ",
        "submission_date": "2013-01-08T00:00:00",
        "last_modified_date": "2013-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.1444",
        "title": "Object-oriented Bayesian networks for a decision support system for antitrust enforcement",
        "authors": [
            "Julia Mortera",
            "Paola Vicard",
            "Cecilia Vergari"
        ],
        "abstract": "We study an economic decision problem where the actors are two firms and the Antitrust Authority whose main task is to monitor and prevent firms' potential anti-competitive behaviour and its effect on the market. The Antitrust Authority's decision process is modelled using a Bayesian network where both the relational structure and the parameters of the model are estimated from a data set provided by the Authority itself. A number of economic variables that influence this decision process are also included in the model. We analyse how monitoring by the Antitrust Authority affects firms' strategies about cooperation. Firms' strategies are modelled as a repeated prisoner's dilemma using object-oriented Bayesian networks. We show how the integration of firms' decision process and external market information can be modelled in this way. Various decision scenarios and strategies are illustrated.\n    ",
        "submission_date": "2013-01-08T00:00:00",
        "last_modified_date": "2013-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.1502",
        "title": "Fuzzy Soft Set Based Classification for Gene Expression Data",
        "authors": [
            "N. Kalaiselvi",
            "H. Hannah Inbarani"
        ],
        "abstract": "Classification is one of the major issues in Data Mining Research fields. The classification problems in medical area often classify medical dataset based on the result of medical diagnosis or description of medical treatment by the medical practitioner. This research work discusses the classification process of Gene Expression data for three different cancers which are breast cancer, lung cancer and leukemia cancer with two classes which are cancerous stage and non cancerous stage. We have applied a fuzzy soft set similarity based classifier to enhance the accuracy to predict the stages among cancer genes and the informative genes are selected by using Entopy filtering.\n    ",
        "submission_date": "2013-01-08T00:00:00",
        "last_modified_date": "2013-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2005",
        "title": "A Distance-based Paraconsistent Semantics for DL-Lite",
        "authors": [
            "Xiaowang Zhang",
            "Kewen Wang",
            "Zhe Wang",
            "Yue Ma",
            "Guilin Qi"
        ],
        "abstract": "DL-Lite is an important family of description logics. Recently, there is an increasing interest in handling inconsistency in DL-Lite as the constraint imposed by a TBox can be easily violated by assertions in ABox in DL-Lite. In this paper, we present a distance-based paraconsistent semantics based on the notion of feature in DL-Lite, which provides a novel way to rationally draw meaningful conclusions even from an inconsistent knowledge base. Finally, we investigate several important logical properties of this entailment relation based on the new semantics and show its promising advantages in non-monotonic reasoning for DL-Lite.\n    ",
        "submission_date": "2013-01-09T00:00:00",
        "last_modified_date": "2015-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2137",
        "title": "A Forgetting-based Approach to Merging Knowledge Bases",
        "authors": [
            "Dai Xu",
            "Xiaowang Zhang",
            "Zuoquan Lin"
        ],
        "abstract": "This paper presents a novel approach based on variable forgetting, which is a useful tool in resolving contradictory by filtering some given variables, to merging multiple knowledge bases. This paper first builds a relationship between belief merging and variable forgetting by using dilation. Variable forgetting is applied to capture belief merging operation. Finally, some new merging operators are developed by modifying candidate variables to amend the shortage of traditional merging operators. Different from model selection of traditional merging operators, as an alternative approach, variable selection in those new operators could provide intuitive information about an atom variable among whole knowledge bases.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2146",
        "title": "A Paraconsistent Tableau Algorithm Based on Sign Transformation in Semantic Web",
        "authors": [
            "Xiaowang Zhang",
            "Guohui Xiao",
            "Zuoquan Lin"
        ],
        "abstract": "In an open, constantly changing and collaborative environment like the forthcoming Semantic Web, it is reasonable to expect that knowledge sources will contain noise and inaccuracies. It is well known, as the logical foundation of the Semantic Web, description logic is lack of the ability of tolerating inconsistent or incomplete data. Recently, the ability of paraconsistent approaches in Semantic Web is weaker in this paper, we present a tableau algorithm based on sign transformation in Semantic Web which holds the stronger ability of reasoning. We prove that the tableau algorithm is decidable which hold the same function of classical tableau algorithm for consistent knowledge bases.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2158",
        "title": "Artificial Intelligence Framework for Simulating Clinical Decision-Making: A Markov Decision Process Approach",
        "authors": [
            "Casey C. Bennett",
            "Kris Hauser"
        ],
        "abstract": "In the modern healthcare system, rapidly expanding costs/complexity, the growing myriad of treatment options, and exploding information streams that often do not effectively reach the front lines hinder the ability to choose optimal treatment decisions over time. The goal in this paper is to develop a general purpose (non-disease-specific) computational/artificial intelligence (AI) framework to address these challenges. This serves two potential functions: 1) a simulation environment for exploring various healthcare policies, payment methodologies, etc., and 2) the basis for clinical artificial intelligence - an AI that can think like a doctor. This approach combines Markov decision processes and dynamic decision networks to learn from clinical data and develop complex plans via simulation of alternative sequential decision paths while capturing the sometimes conflicting, sometimes synergistic interactions of various components in the healthcare system. It can operate in partially observable environments (in the case of missing observations or data) by maintaining belief states about patient health status and functions as an online agent that plans and re-plans. This framework was evaluated using real patient data from an electronic health record. Such an AI framework easily outperforms the current treatment-as-usual (TAU) case-rate/fee-for-service models of healthcare (Cost per Unit Change: $189 vs. $497) while obtaining a 30-35% increase in patient outcomes. Tweaking certain model parameters further enhances this advantage, obtaining roughly 50% more improvement for roughly half the costs. Given careful design and problem formulation, an AI simulation framework can approximate optimal decisions even in complex and uncertain environments. Future work is described that outlines potential lines of research and integration of machine learning algorithms for personalized medicine.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2215",
        "title": "Proceedings of Answer Set Programming and Other Computing Paradigms (ASPOCP 2012), 5th International Workshop, September 4, 2012, Budapest, Hungary",
        "authors": [
            "Michael Fink",
            "Yuliya Lierler"
        ],
        "abstract": "This volume contains the papers presented at the fifth workshop on Answer Set Programming and Other Computing Paradigms (ASPOCP 2012) held on September 4th, 2012 in Budapest, co-located with the 28th International Conference on Logic Programming (ICLP 2012). It thus continues a series of previous events co-located with ICLP, aiming at facilitating the discussion about crossing the boundaries of current ASP techniques in theory, solving, and applications, in combination with or inspired by other computing paradigms.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2254",
        "title": "Markov Chain Monte Carlo using Tree-Based Priors on Model Structure",
        "authors": [
            "Nicos Angelopoulos",
            "James Cussens"
        ],
        "abstract": "We present a general framework for defining priors on model structure and sampling from the posterior using the Metropolis-Hastings algorithm. The key idea is that structure priors are  defined via a probability tree and that the proposal mechanism for the Metropolis-Hastings algorithm operates by traversing this tree, thereby defining a cheaply computable acceptance probability. We have applied this approach to Bayesian net structure learning using a number of priors and tree traversal strategies. Our results show that these must be chosen appropriately for this approach to be successful.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2255",
        "title": "Graphical readings of possibilistic logic bases",
        "authors": [
            "Salem Benferhat",
            "Didier Dubois",
            "Souhila Kaci",
            "Henri Prade"
        ],
        "abstract": "Possibility theory offers either a qualitive, or a numerical framework for representing uncertainty, in terms of dual measures of possibility and necessity. This leads to the existence of two kinds of possibilistic causal graphs where the conditioning is either based on the minimum, or the product operator. Benferhat et al. (1999) have investigated the connections between min-based graphs and possibilistic logic bases (made of classical formulas weighted in terms of certainty). This paper deals with a more difficult issue : the product-based graphical representations of possibilistic bases, which provides an easy structural reading of possibilistic bases. Moreover, this paper also provides another reading of possibilistic bases in terms of comparative preferences of the form \"in the context p, q is preferred to not q\". This enables us to explicit preferences underlying a set of goals with different levels of priority.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2256",
        "title": "Pre-processing for Triangulation of Probabilistic Networks",
        "authors": [
            "Hans L. Bodlaender",
            "Arie M.C.A. Koster",
            "Frank van den Eijkhof",
            "Linda C. van der Gaag"
        ],
        "abstract": "The currently most efficient algorithm for inference with a probabilistic network builds upon a triangulation of a network's graph.  In this paper, we show that pre-processing can help in finding good triangulations forprobabilistic networks, that is, triangulations with a minimal maximum clique size.  We provide a set of rules for stepwise reducing a graph, without losing optimality.  This reduction allows us to solve the triangulation problem on a smaller graph. From the smaller graph's triangulation, a triangulation of the original graph is obtained by reversing the reduction steps.  Our experimental results show that the graphs of some well-known real-life probabilistic networks can be triangulated optimally just by preprocessing; for other networks, huge reductions in their graph's size are obtained.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2257",
        "title": "A Calculus for Causal Relevance",
        "authors": [
            "Blai Bonet"
        ],
        "abstract": "This paper presents a sound and completecalculus for causal relevance, based onPearl's functional models ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2258",
        "title": "Instrumentality Tests Revisited",
        "authors": [
            "Blai Bonet"
        ],
        "abstract": "An instrument is a random variable thatallows the identification of parameters inlinear models when the error terms arenot ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2259",
        "title": "UCP-Networks: A Directed Graphical Representation of Conditional Utilities",
        "authors": [
            "Craig Boutilier",
            "Fahiem Bacchus",
            "Ronen I. Brafman"
        ],
        "abstract": "We propose a new directed graphical representation of utility functions, called UCP-networks, that combines aspects of two existing graphical models: generalized additive models and CP-networks. The network decomposes a utility function into a number of additive factors, with the directionality of the arcs reflecting conditional dependence of preference statements - in the underlying (qualitative) preference ordering - under a {em ceteris paribus} (all else being equal) interpretation.  This representation is arguably natural in many settings. Furthermore, the strong CP-semantics ensures that computation of optimization and dominance queries is very efficient. We also demonstrate the value of this representation in decision making. Finally, we describe an interactive elicitation procedure that takes advantage of the linear nature of the constraints on \"`tradeoff weights\" imposed by a UCP-network. This procedure allows the network to be refined until the regret of the decision with minimax regret (with respect to the incompletely specified utility function) falls below a specified threshold (e.g., the cost of further questioning.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2260",
        "title": "Confidence Inference in Bayesian Networks",
        "authors": [
            "Jian Cheng",
            "Marek J. Druzdzel"
        ],
        "abstract": "We present two sampling algorithms for probabilistic confidence inference in Bayesian networks. These two algorithms (we call them AIS-BN-mu and AIS-BN-sigma algorithms) guarantee that estimates of posterior probabilities are with a given probability within a desired precision bound. Our algorithms are based on recent advances in sampling algorithms for (1) estimating the mean of bounded random variables and (2) adaptive importance sampling in Bayesian networks. In addition to a simple stopping rule for sampling that they provide, the AIS-BN-mu and AIS-BN-sigma algorithms are capable of guiding the learning process in the AIS-BN algorithm. An empirical evaluation of the proposed algorithms shows excellent performance, even for very unlikely evidence.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2262",
        "title": "Conditions Under Which Conditional Independence and Scoring Methods Lead to Identical Selection of Bayesian Network Models",
        "authors": [
            "Robert G. Cowell"
        ],
        "abstract": "It is often stated in papers tackling  the task of inferring  Bayesian network structures from data that there are these two  distinct approaches: (i) Apply conditional independence  tests when testing for the presence or otherwise of edges; (ii)  Search the model space using a scoring metric.     Here I argue that for complete data and a given node ordering this  division is a myth, by showing that cross entropy methods for  checking conditional independence are mathematically identical to  methods based upon discriminating between models by their overall  goodness-of-fit  logarithmic scores.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2263",
        "title": "Linearity Properties of Bayes Nets with Binary Variables",
        "authors": [
            "David Danks",
            "Clark Glymour"
        ],
        "abstract": "It is \"well known\" that in linear models: (1) testable constraints on the marginal distribution of observed variables distinguish certain cases in which an unobserved cause jointly influences several observed variables; (2) the technique of \"instrumental variables\" sometimes permits an estimation of the influence of one variable on another even when the association between the variables may be confounded by unobserved common causes; (3) the association (or conditional probability distribution of one variable given another) of two variables connected by a path or trek can be computed directly from the parameter values associated with each edge in the path or trek; (4) the association of two variables produced by multiple treks can be computed from the parameters associated with each trek; and (5) the independence of two variables conditional on a third implies the corresponding independence of the sums of the variables over all units conditional on the sums over all units of each of the original conditioning ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2264",
        "title": "Using Bayesian Networks to Identify the Causal Effect of Speeding in Individual Vehicle/Pedestrian Collisions",
        "authors": [
            "Gary A. Davis"
        ],
        "abstract": "On roads showing significant violations of posted speed limits, one measure of the safety effect of speeding is the difference between the road's actual accident count and the count that would have occurred if the posted speed limit had been strictly obeyed. An estimate of this accident reduction can be had by computing the probability that speeding was a necessary condition for each of set of accidents. This is an instance of assessing individual probabilities of causation, which is generally not possible absent prior knowledge of causal structure. For traffic accidents such prior knowledge is often available and this paper illustrates how, for a commonly occurring class of vehicle/pedestrian accidents, approaches to uncertainty and causal analyses appearing in the accident reconstruction literature can be unified using Bayesian networks. Measured skidmarks, pedestrian throw distances, and pedestrian injury severity are treated as evidence, and using the Gibbs Sampling routine BUGS, the posterior probability distribution over exogenous variables, such as the vehicle's initial speed, location, and driver reaction time, is computed. This posterior distribution is then used to compute the \"probability of necessity\" for speeding.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2265",
        "title": "Hybrid Processing of Beliefs and Constraints",
        "authors": [
            "Rina Dechter",
            "David Ephraim Larkin"
        ],
        "abstract": "This paper explores algorithms for processing probabilistic and deterministic information when the former is represented as a belief network and the latter as  a set of boolean clauses. The motivating tasks are 1. evaluating beliefs networks having a large number of deterministic  relationships and2. evaluating probabilities of complex boolean querie over a belief network.  We propose a parameterized  family of  variable elimination algorithms that exploit both types  of information, and that allows varying levels of  constraint propagation inferences.  The complexity of the scheme is  controlled by the induced-width of the graph {em augmented} by the dependencies introduced  by the boolean constraints. Preliminary empirical evaluation demonstrate the effect of constraint propagation  on probabilistic computation.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2267",
        "title": "Efficient Stepwise Selection in Decomposable Models",
        "authors": [
            "Amol Deshpande",
            "Minos Garofalakis",
            "Michael I. Jordan"
        ],
        "abstract": "In this paper, we present an efficient way of performing stepwise selection in the class of decomposable models. The main contribution of the paper is a simple characterization of the edges that canbe added to a decomposable model while keeping the resulting model decomposable and an efficient algorithm for enumerating all such edges for a given model in essentially O(1) time per edge. We also discuss how backward selection can be performed efficiently using our data ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2268",
        "title": "Incorporating Expressive Graphical Models in Variational Approximations: Chain-Graphs and Hidden Variables",
        "authors": [
            "Tal El-Hay",
            "Nir Friedman"
        ],
        "abstract": "Global variational approximation methods in graphical models allow efficient approximate inference of complex posterior distributions by using a simpler model. The choice of the approximating model determines a tradeoff between the complexity of the approximation procedure and the quality of the approximation. In this paper, we consider variational approximations based on two classes of models that are richer than standard Bayesian networks, Markov networks or mixture models. As such, these classes allow to find better tradeoffs in the spectrum of approximations. The first class of models are chain graphs, which capture distributions that are partially directed. The second class of models are directed graphs (Bayesian networks) with additional latent variables. Both classes allow representation of multi-variable dependencies that cannot be easily represented within a Bayesian network. \n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2271",
        "title": "A Comparison of Axiomatic Approaches to Qualitative Decision Making Using Possibility Theory",
        "authors": [
            "Phan H. Giang",
            "Prakash P. Shenoy"
        ],
        "abstract": "In this paper we analyze two recent axiomatic approaches proposed by Dubois et al and by Giang and Shenoy  to qualitative decision making where uncertainty is described by possibility theory. Both axiomtizations are inspired by von Neumann and Morgenstern's system of axioms for the case of probability theory. We show that our approach naturally unifies two axiomatic systems that correspond respectively to pessimistic and optimistic decision criteria proposed by Dubois et al. The simplifying unification is achieved by (i) replacing axioms that are supposed to reflect two informational attitudes (uncertainty aversion and uncertainty attraction) by an axiom that imposes order on set of standard lotteries and (ii) using a binary utility scale in which each utility level is represented by a pair of numbers.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2272",
        "title": "Enumerating Markov Equivalence Classes of Acyclic Digraph Models",
        "authors": [
            "Steven B. Gillispie",
            "Michael D. Perlman"
        ],
        "abstract": "Graphical Markov models determined by acyclic digraphs (ADGs), also called directed acyclic graphs (DAGs), are widely studied in statistics, computer science (as Bayesian networks), operations research (as influence diagrams), and many related fields.  Because different ADGs may determine the same Markov equivalence class, it long has been of interest to determine the efficiency gained in model specification and search by working directly with Markov equivalence classes of ADGs rather than with ADGs themselves.  A computer program was written to enumerate the equivalence classes of ADG models as specified by Pearl & Verma's equivalence criterion.  The program counted equivalence classes for models up to and including 10 vertices.  The ratio of number of classes to ADGs appears to approach an asymptote of about 0.267.  Classes were analyzed according to number of edges and class size.  By edges, the distribution of number of classes approaches a Gaussian shape.  By class size, classes of size 1 are most common, with the proportions for larger sizes initially decreasing but then following a more irregular pattern.  The maximum number of classes generated by any undirected graph was found to increase approximately factorially.  The program also includes a new variation of orderly algorithm for generating undirected graphs.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2273",
        "title": "Robust Combination of Local Controllers",
        "authors": [
            "Carlos E. Guestrin",
            "Dirk Ormoneit"
        ],
        "abstract": "Planning problems are hard, motion planning, for example, isPSPACE-hard. Such problems are even more difficult in the presence of uncertainty. Although, Markov Decision Processes (MDPs) provide a formal  framework for such problems, finding solutions to high dimensional continuous MDPs is usually difficult, especially when the actions and time measurements are continuous. Fortunately, problem-specific knowledge allows us to design controllers that are good locally, though having no global guarantees. We propose a method of nonparametrically combining local controllers to obtain globally good solutions. We apply this formulation to two types of problems : motion planning (stochastic shortest path) and discounted MDPs. For motion planning, we argue that usual MDP optimality criterion (expected cost) may not be practically relevant. Wepropose an alternative: finding the minimum cost path,subject to the constraint that the robot must reach the goal withhigh probability. For this problem, we prove that a polynomial number of samples is sufficient to obtain a high probability path. For discounted MDPs, we propose a formulation that explicitly deals with model uncertainty, i.e., the problem introduced when transition probabilities are not known exactly. We formulate the problem as a robust linear program which directly incorporates this type of uncertainty.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2274",
        "title": "Similarity Measures on Preference Structures, Part II: Utility Functions",
        "authors": [
            "Vu A. Ha",
            "Peter Haddawy",
            "John Miyamoto"
        ],
        "abstract": "In previous work cite{Ha98:Towards} we presented a case-based approach to eliciting and reasoning with preferences. A key issue in this approach is the definition of similarity between user preferences. We introduced the probabilistic distance as a measure of similarity on user preferences, and provided an algorithm to compute the distance between two partially specified {em value} functions. This is for the case of decision making under {em certainty}. In this paper we address the more challenging issue of computing the probabilistic distance in the case of decision making under{em uncertainty}. We provide an algorithm to compute the probabilistic distance between two partially specified {em utility} functions. We demonstrate the use of this algorithm with a medical data set of partially specified patient preferences,where none of the other existing distancemeasures appear definable.  Using this data set, we also demonstrate that the case-based approach to preference elicitation isapplicable in domains with uncertainty.  Finally, we provide a comprehensive analytical comparison of the probabilistic distance with some existing distance measures on preferences.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2275",
        "title": "Causes and Explanations: A Structural-Model Approach --- Part 1: Causes",
        "authors": [
            "Joseph Y. Halpern",
            "Judea Pearl"
        ],
        "abstract": "We propose a new definition of actual causes, using structural equations to model ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2277",
        "title": "A Clustering Approach to Solving Large Stochastic Matching Problems",
        "authors": [
            "Milos Hauskrecht",
            "Eli Upfal"
        ],
        "abstract": "In this work we focus on efficient heuristics for solving a class of stochastic planning problems that arise in a variety of business, investment, and industrial applications. The problem is best described in terms of future buy and sell contracts. By buying less reliable, but less expensive, buy (supply) contracts, a company or a trader can cover a position of more reliable and more expensive sell contracts. The goal is to maximize the expected net gain (profit) by constructing a dose to optimum portfolio out of the available buy and sell contracts. This stochastic planning problem can be formulated as a two-stage stochastic linear programming problem with recourse. However, this formalization leads to solutions that are exponential in the number of possible failure combinations. Thus, this approach is not feasible for large scale problems. In this work we investigate heuristic approximation techniques alleviating the efficiency problem. We primarily focus on the clustering approach and devise heuristics for finding clusterings leading to good approximations. We illustrate the quality and feasibility of the approach through experimental data.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2279",
        "title": "A Bayesian Approach to Tackling Hard Computational Problems",
        "authors": [
            "Eric J. Horvitz",
            "Yongshao Ruan",
            "Carla P. Gomes",
            "Henry Kautz",
            "Bart Selman",
            "David Maxwell Chickering"
        ],
        "abstract": "We are developing a general framework for using learned Bayesian models for decision-theoretic control of search and reasoningalgorithms.  We illustrate the approach on the specific task of controlling both general and domain-specific solvers on a hard class of structured constraint satisfaction problems.  A successful strategyfor reducing the high (and even infinite) variance in running time typically exhibited by backtracking search algorithms is to cut off and restart the search if a solution is not found within a certainamount of time.  Previous work on restart strategies have employed fixed cut off values.  We show how to create a dynamic cut off strategy by learning a Bayesian model that predicts the ultimate length of a trial based on observing the early behavior of the search algorithm.  Furthermore, we describe the general conditions under which a dynamic restart strategy can outperform the theoretically optimal fixed strategy.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2282",
        "title": "On characterizing Inclusion of Bayesian Networks",
        "authors": [
            "Tomas Kocka",
            "Remco R. Bouckaert",
            "Milan Studeny"
        ],
        "abstract": "Every directed acyclic graph (DAG) over a finite non-empty set of variables (= nodes) N induces an independence model over N, which is a list of conditional independence statements over ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2285",
        "title": "Plausible reasoning from spatial observations",
        "authors": [
            "Jerome Lang",
            "Philippe Muller"
        ],
        "abstract": "This article deals with plausible reasoning from incomplete knowledge about large-scale spatial properties. The availableinformation, consisting of a set of pointwise observations,is extrapolated to neighbour points. We make use of belief functions to represent the influence of the knowledge at a given point to another point; the quantitative strength of this influence decreases when the distance between both points increases. These influences arethen aggregated using a variant of Dempster's rule of combination which takes into account the relative dependence between observations.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2287",
        "title": "Hypothesis Management in Situation-Specific Network Construction",
        "authors": [
            "Kathryn Blackmond Laskey",
            "Suzanne M. Mahoney",
            "Ed Wright"
        ],
        "abstract": "This paper considers the problem of knowledge-based model construction in the presence of uncertainty about the association of domain entities to random variables. Multi-entity Bayesian networks (MEBNs) are defined as a representation for knowledge in domains characterized by uncertainty in the number of relevant entities, their interrelationships, and their association with observables. An MEBN implicitly specifies a probability distribution in terms of a hierarchically structured collection of Bayesian network fragments that together encode a joint probability distribution over arbitrarily many interrelated hypotheses.  Although a finite query-complete model can always be constructed, association uncertainty typically makes exact model construction and evaluation intractable. The objective of hypothesis management is to balance tractability against accuracy.  We describe an application to the problem of using intelligence reports to infer the organization and activities of groups of military vehicles. Our approach is compared to related work in the tracking and fusion literature.  \n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2288",
        "title": "Inference in Hybrid Networks: Theoretical Limits and Practical Algorithms",
        "authors": [
            "Uri Lerner",
            "Ron Parr"
        ],
        "abstract": "An important subclass of hybrid Bayesian networks are those that represent Conditional Linear Gaussian (CLG) distributions --- a distribution with a multivariate Gaussian component for each instantiation of the discrete variables.  In this paper we explore the problem of inference in CLGs.  We show that inference in CLGs can be significantly harder than inference in Bayes Nets.  In particular, we prove that even if the CLG is restricted to an extremely simple structure of a polytree in which every continuous node has at most one discrete ancestor, the inference task is ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2289",
        "title": "Exact Inference in Networks with Discrete Children of Continuous Parents",
        "authors": [
            "Uri Lerner",
            "Eran Segal",
            "Daphne Koller"
        ],
        "abstract": "Many real life domains contain a mixture of discrete and continuous variables and can be modeled as hybrid Bayesian Networks. Animportant subclass of hybrid BNs are conditional linear Gaussian (CLG) networks, where the conditional distribution of the continuous variables given an assignment to the discrete variables is a multivariate Gaussian. Lauritzen's extension to the clique tree algorithm can be used for exact inference in CLG networks. However, many domains also include discrete variables that depend on continuous ones, and CLG networks do not allow such dependencies to berepresented.  No exact inference algorithm has been proposed for these enhanced CLG networks. In this paper, we generalize Lauritzen's algorithm, providing the first \"exact\" inference algorithm for augmented CLG networks - networks where continuous nodes are conditional linear Gaussians but that also allow discrete children ofcontinuous parents. Our algorithm is exact in the sense that it computes the exact distributions over the discrete nodes, and the exact first and second moments of the continuous ones, up to the accuracy obtained by numerical integration used within thealgorithm. When the discrete children are modeled with softmax CPDs (as is the case in many real world domains) the approximation of the continuous distributions using the first two moments is particularly accurate. Our algorithm is simple to implement and often comparable in its complexity to Lauritzen's algorithm. We show empirically that it achieves substantially higher accuracy than previous approximate algorithms.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2290",
        "title": "Probabilistic Logic Programming under Inheritance with Overriding",
        "authors": [
            "Thomas Lukasiewicz"
        ],
        "abstract": "We present probabilistic logic programming under inheritance with overriding. This approach is based on new notions of entailment for reasoning with conditional constraints, which are obtained from the classical notion of logical entailment by adding the principle of inheritance with overriding. This is done by using recent approaches to probabilistic default reasoning with conditional constraints. We analyze the semantic properties of the new entailment relations. We also present algorithms for probabilistic logic programming under inheritance with overriding, and program transformations for an increased efficiency.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2291",
        "title": "Solving Influence Diagrams using HUGIN, Shafer-Shenoy and Lazy Propagation",
        "authors": [
            "Anders L. Madsen",
            "Dennis Nilsson"
        ],
        "abstract": "In this paper we compare three different architectures for the evaluation of influence diagrams: HUGIN, Shafer-Shenoy, and Lazy Evaluation architecture. The computational complexity of the architectures are compared on the LImited Memory Influence Diagram (LIMID): a diagram where only the requiste information for the computation of the optimal policies are depicted. Because the requsite information is explicitly represented in the LIMID the evaluation can take advantage of it, and significant savings in computational can be obtained. In this paper we show how the obtained savings is considerably increased when the computations performed on the LIMID is according to the Lazy Evaluation scheme. \n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2292",
        "title": "A Bayesian Multiresolution Independence Test for Continuous Variables",
        "authors": [
            "Dimitris Margaritis",
            "Sebastian Thrun"
        ],
        "abstract": "In this paper we present a method ofcomputing the posterior probability ofconditional independence of two or morecontinuous variables from data,examined at several resolutions.  Ourapproach is motivated by theobservation that the appearance ofcontinuous data varies widely atvarious resolutions, producing verydifferent independence estimatesbetween the variablesinvolved.  Therefore, it is difficultto ascertain independence withoutexamining data at several carefullyselected resolutions.  In our paper, weaccomplish this using the exactcomputation of the posteriorprobability of independence, calculatedanalytically given a resolution.  Ateach examined resolution, we assume amultinomial distribution with Dirichletpriors for the discretized tableparameters, and compute the posteriorusing Bayesian integration.  Acrossresolutions, we use a search procedureto approximate the Bayesian integral ofprobability over an exponential numberof possible histograms.  Our methodgeneralizes to an arbitrary numbervariables in a straightforward ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2293",
        "title": "Aggregating Learned Probabilistic Beliefs",
        "authors": [
            "Pedrito Maynard-Reid II",
            "Urszula Chajewska"
        ],
        "abstract": "We consider the task of aggregating beliefs of severalexperts.  We assume that these beliefs are represented as probabilitydistributions.  We argue that the evaluation of any aggregationtechnique depends on the semantic context of this task.  We propose aframework, in which we assume that nature generates samples from a`true' distribution and different experts form their beliefs based onthe subsets of the data they have a chance to observe.  Naturally, theideal aggregate distribution would be the one learned from thecombined sample sets.  Such a formulation leads to a natural way tomeasure the accuracy of the aggregation ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2294",
        "title": "Expectation Propagation for approximate Bayesian inference",
        "authors": [
            "Thomas P. Minka"
        ],
        "abstract": "This paper presents a new deterministic approximation technique in Bayesian networks.  This method, \"Expectation Propagation\", unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks.  All three algorithms try to recover an approximate distribution which is close in KL divergence to the true distribution. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete.  Expectation Propagation approximates the belief states by only retaining certain expectations, such as mean and variance, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes.  Expectation Propagation also extends belief propagation in the opposite direction - it can propagate richer belief states that incorporate correlations between nodes.  Experiments with Gaussian mixture models show Expectation Propagation to be convincingly better than methods with similar computational cost: Laplace's method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2295",
        "title": "Recognition Networks for Approximate Inference in BN20 Networks",
        "authors": [
            "Quaid Morris"
        ],
        "abstract": "We propose using recognition networks for approximate inference inBayesian networks (BNs).  A recognition network is a multilayerperception (MLP) trained to predict posterior marginals given observedevidence in a particular BN.  The input to the MLP is a vector of thestates of the evidential nodes.  The activity of an output unit isinterpreted as a prediction of the posterior marginal of thecorresponding variable.  The MLP is trained using samples generated fromthe corresponding ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2296",
        "title": "The Factored Frontier Algorithm for Approximate Inference in DBNs",
        "authors": [
            "Kevin Murphy",
            "Yair Weiss"
        ],
        "abstract": "The Factored Frontier (FF) algorithm is a simple approximate inferencealgorithm for Dynamic Bayesian Networks (DBNs).  It is very similar tothe fully factorized version of the Boyen-Koller (BK) algorithm, butinstead of doing an exact update at every step followed bymarginalisation (projection), it always works with factoreddistributions.  Hence it can be applied to models for which the exactupdate step is intractable.  We show that FF is equivalent to (oneiteration of) loopy belief propagation (LBP) on the original DBN, andthat BK is equivalent (to one iteration of) LBP on a DBN where wecluster some of the nodes.  We then show empirically that byiterating, LBP can improve on the accuracy of both FF and BK.  Wecompare these algorithms on two real-world DBNs: the first is a modelof a water treatment plant, and the second is a coupled HMM, used tomodel freeway traffic.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2297",
        "title": "A Case Study in Knowledge Discovery and Elicitation in an Intelligent Tutoring Application",
        "authors": [
            "Ann Nicholson",
            "Tal Boneh",
            "Tim Wilkin",
            "Kaye Stacey",
            "Liz Sonenberg",
            "Vicki Steinle"
        ],
        "abstract": "Most successful Bayesian network (BN) applications to datehave been built through knowledge elicitation from ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2298",
        "title": "Lattice Particle Filters",
        "authors": [
            "Dirk Ormoneit",
            "Christiane Lemieux",
            "David J. Fleet"
        ],
        "abstract": "A standard approach to approximate inference in state-space models isto apply a particle filter, e.g., the Condensation ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2299",
        "title": "Approximating MAP using Local Search",
        "authors": [
            "James D. Park",
            "Adnan Darwiche"
        ],
        "abstract": "MAP is the problem of finding a most probable instantiation of a set of variables in a Bayesian network, given evidence.  Unlike computing marginals, posteriors, and MPE (a special case of MAP), the time and space complexity of MAP is not only exponential in the network treewidth, but also in a larger parameter known as the \"constrained\" treewidth. In practice, this means that computing MAP can be orders of magnitude more expensive than computingposteriors or MPE. Thus, practitioners generally avoid MAP computations, resorting instead to approximating them by the most likely value for each MAP variableseparately, or by ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2300",
        "title": "Direct and Indirect Effects",
        "authors": [
            "Judea Pearl"
        ],
        "abstract": "The direct effect of one eventon another can be defined and measured byholding constant all intermediate variables between the ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2301",
        "title": "Sufficiency, Separability and Temporal Probabilistic Models",
        "authors": [
            "Avi Pfeffer"
        ],
        "abstract": "Suppose we are given the conditional probability of one variable given some other ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2302",
        "title": "Toward General Analysis of Recursive Probability Models",
        "authors": [
            "Daniel Pless",
            "George Luger"
        ],
        "abstract": "There is increasing interest within the research community in the design and use of recursive probability models. Although there still remains concern about computational complexity costs and the fact that computing exact solutions can be intractable for many nonrecursive models and impossible in the general case for recursive problems, several research groups are actively developing computational techniques for recursive stochastic languages. We have developed an extension to the traditional lambda-calculus as a framework for families of Turing complete stochastic languages. We have also developed a class of exact inference algorithms based on the traditional reductions of the lambda-calculus. We further propose that using the deBruijn notation (a lambda-calculus notation with nameless dummies) supports effective caching in such systems (caching being an essential component of efficient computation). Finally, our extension to the lambda-calculus offers a foundation and general theory for the construction of recursive stochastic modeling languages as well as promise for effective caching and efficient approximation algorithms for inference.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2304",
        "title": "Vector-space Analysis of Belief-state Approximation for POMDPs",
        "authors": [
            "Pascal Poupart",
            "Craig Boutilier"
        ],
        "abstract": "We propose a new approach to value-directed belief state approximation for POMDPs. The value-directed model allows one to choose approximation methods for belief state monitoring that have a small impact on decision quality. Using a vector space analysis of the problem, we devise two new search procedures for selecting an approximation scheme that have much better computational properties than existing methods. Though these provide looser error bounds, we show empirically that they have a similar impact on decision quality in practice, and run up to two orders of magnitude more quickly.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2305",
        "title": "Value-Directed Sampling Methods for POMDPs",
        "authors": [
            "Pascal Poupart",
            "Luis E. Ortiz",
            "Craig Boutilier"
        ],
        "abstract": "We consider the problem of approximate belief-state monitoring using particle filtering for the purposes of implementing a policy for a partially-observable Markov decision process (POMDP). While particle filtering has become a widely-used tool in AI for monitoring dynamical systems, rather scant attention has been paid to their use in the context of decision making. Assuming the existence of a value function, we derive error bounds on decision quality associated with filtering using importance sampling. We also describe an adaptive procedure that can be used to dynamically determine the number of samples required to meet specific error bounds. Empirical evidence is offered supporting this technique as a profitable means of directing sampling effort where it is needed to distinguish policies.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2306",
        "title": "A Mixed Graphical Model for Rhythmic Parsing",
        "authors": [
            "Christopher S Raphael"
        ],
        "abstract": "A method is presented for the rhythmic parsing problem: Given a sequence of observed musical note onset times, we estimate the corresponding notated rhythm and tempo process.  A graphical model is developed that represents the simultaneous evolution of tempo and rhythm and relates these hidden quantities to observations. The rhythm variables are discrete and the tempo and observation variables are continuous.  We show how to compute the globally most likely configuration of the tempo and rhythm variables given an observation of note onset times. Preliminary experiments are presented on a small data set.  A generalization to arbitrary  conditional Gaussian distributions is outlined.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2307",
        "title": "Decision-Theoretic Planning with Concurrent Temporally Extended Actions",
        "authors": [
            "Khashayar Rohanimanesh",
            "Sridhar Mahadevan"
        ],
        "abstract": "We investigate a model for planning under uncertainty with temporallyextended actions, where multiple actions can be taken concurrently at each decision epoch.  Our model is based on the options framework, and combines it with factored state space models,where the set of options can be partitioned into classes that affectdisjoint state variables. We show that the set of decisionepochs for concurrent options defines a semi-Markov decisionprocess, if the underlying temporally extended actions being parallelized arerestricted to Markov options. This property allows us to use SMDPalgorithms for computing the value function over concurrentoptions. The concurrent options model allows overlapping execution ofoptions in order to achieve higher performance or in order to performa complex task. We describe a simple experiment using a navigationtask which illustrates how concurrent options results in a faster planwhen compared to the case when only one option is taken at a time.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2308",
        "title": "A Tractable POMDP for a Class of Sequencing Problems",
        "authors": [
            "Paat Rusmevichientong",
            "Benjamin van Roy"
        ],
        "abstract": "We consider a partially observable Markov decision problem (POMDP) that models a class of sequencing problems. Although POMDPs are typically intractable, our formulation admits tractable solution. Instead of maintaining a value function over a high-dimensional set of belief states, we reduce the state space to one of smaller dimension, in which grid-based dynamic programming techniques are effective. We develop an error bound for the resulting approximation, and discuss an application of the model to a problem in targeted advertising.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2310",
        "title": "Policy Improvement for POMDPs Using Normalized Importance Sampling",
        "authors": [
            "Christian R. Shelton"
        ],
        "abstract": "We present a new method for estimating the expected return of a POMDP from experience.  The method does not assume any knowledge of the POMDP and allows the experience to be gathered from an arbitrary sequence of policies.  The return is estimated for any new policy of the POMDP.  We motivate the estimator from function-approximation and importance sampling points-of-view and derive its theoretical properties.  Although the estimator is biased, it has low variance and the bias is often irrelevant when the estimator is used for pair-wise comparisons.  We conclude by extending the estimator to policies with memory and compare its performance in a greedy search algorithm to REINFORCE algorithms showing an order of magnitude reduction in the number of trials required.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2312",
        "title": "Causal Discovery from Changes",
        "authors": [
            "Jin Tian",
            "Judea Pearl"
        ],
        "abstract": "We propose a new method of discovering causal structures, based on the detection of local, spontaneous changes in the underlying data-generating model. We analyze the classes of structures that are equivalent relative to a stream of distributions produced by local changes, and devise algorithms that output graphical representations of these equivalence classes. We present experimental results, using simulated data, and examine the errors associated with detection of changes and recovery of structures. \n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2313",
        "title": "Bayesian Error-Bars for Belief Net Inference",
        "authors": [
            "Tim Van Allen",
            "Russell Greiner",
            "Peter Hooper"
        ],
        "abstract": "A Bayesian Belief Network (BN) is a model of a joint distribution over a setof n variables, with a DAG structure to represent the immediate dependenciesbetween the variables, and a set of parameters (aka CPTables) to represent thelocal conditional probabilities of a node, given each assignment to itsparents.  In many situations, these parameters are themselves random variables - this may reflect the uncertainty of the domain expert, or may come from atraining sample used to estimate the parameter values.  The distribution overthese \"CPtable variables\" induces a distribution over the response the BNwill return to any \"What is Pr(H | E)?\"  query.  This paper investigates thevariance of this response, showing first that it is asymptotically normal,then providing its mean and asymptotical variance.  We then present aneffective general algorithm for computing this variance, which has the samecomplexity as simply computing the (mean value of) the response itself - ie,O(n 2^w), where n is the number of variables and w is the effective treewidth.  Finally, we provide empirical evidence that this algorithm, whichincorporates assumptions and approximations, works effectively in practice,given only small samples.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2314",
        "title": "Analysing Sensitivity Data from Probabilistic Networks",
        "authors": [
            "Linda C. van der Gaag",
            "Silja Renooij"
        ],
        "abstract": "With the advance of efficient analytical methods for sensitivity analysis ofprobabilistic networks, the interest in the sensitivities revealed by real-life networks is rekindled.  As the amount of data resulting from a sensitivity analysis of even a moderately-sized network is alreadyoverwhelming, methods for extracting relevant information are called for.  One such methodis to study the derivative of the sensitivity functions yielded for a network's parameters. We further propose to build upon the concept of admissible deviation, that is, the extent to which a parameter can deviate from the true value without inducing a change in the most likely outcome.  We illustrate these concepts by means of a sensitivity analysis of a real-life probabilistic network in oncology.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2317",
        "title": "Belief Optimization for Binary Networks: A Stable Alternative to Loopy Belief Propagation",
        "authors": [
            "Max Welling",
            "Yee Whye Teh"
        ],
        "abstract": "We present a novel inference algorithm for arbitrary, binary, undirected graphs. Unlike loopy belief propagation, which iterates fixed point equations, we directly descend on the Bethe free energy. The algorithm consists of two phases, first we update the pairwise probabilities, given the marginal probabilities at each unit,using an analytic expression. Next, we update the marginal probabilities, given the pairwise probabilities by following the negative gradient of the Bethe free energy. Both steps are guaranteed to decrease the Bethe free energy, and since it is lower bounded, the algorithm is guaranteed to converge to a local minimum. We also show that the Bethe free energy is equal to the TAP free energy up to second order in the weights. In experiments we confirm that when belief propagation converges it usually finds identical solutions as our belief optimization method. However, in cases where belief propagation fails to converge, belief optimization continues to converge to reasonable beliefs. The stable nature of belief optimization makes it ideally suited for learning graphical models from data.    \n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2319",
        "title": "Planning and Acting under Uncertainty: A New Model for Spoken Dialogue Systems",
        "authors": [
            "Bo Zhang",
            "Qingsheng Cai",
            "Jianfeng Mao",
            "Baining Guo"
        ],
        "abstract": "Uncertainty plays a central role in spoken dialogue systems. Some stochastic models like Markov decision process (MDP) are used to model the dialogue manager. But the partially observable system state and user intention hinder the natural representation of the dialogue state. MDP-based system degrades fast when uncertainty about a user's intention increases. We propose a novel dialogue model based on the partially observable Markov decision process (POMDP). We use hidden system states and user intentions as the state set, parser results and low-level information as the observation set, domain actions and dialogue repair actions as the action set. Here the low-level information is extracted from different input modals, including speech, keyboard, mouse, etc., using Bayesian networks. Because of the limitation of the exact algorithms, we focus on heuristic approximation algorithms and their applicability in POMDP for dialogue management. We also propose two methods for grid point selection in grid-based approximation algorithms.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2343",
        "title": "Planning by Prioritized Sweeping with Small Backups",
        "authors": [
            "Harm van Seijen",
            "Richard S. Sutton"
        ],
        "abstract": "Efficient planning plays a crucial role in model-based reinforcement learning. Traditionally, the main planning operation is a full backup based on the current estimates of the successor states. Consequently, its computation time is proportional to the number of successor states. In this paper, we introduce a new planning backup that uses only the current value of a single successor state and has a computation time independent of the number of successor states. This new backup, which we call a small backup, opens the door to a new class of model-based reinforcement learning methods that exhibit much finer control over their planning process than traditional methods. We empirically demonstrate that this increased flexibility allows for more efficient planning by showing that an implementation of prioritized sweeping based on small backups achieves a substantial performance improvement over classical implementations.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2683",
        "title": "BliStr: The Blind Strategymaker",
        "authors": [
            "Josef Urban"
        ],
        "abstract": "BliStr is a system that automatically develops strategies for E prover on a large set of problems. The main idea is to interleave (i) iterated low-timelimit local search for new strategies on small sets of similar easy problems with (ii) higher-timelimit evaluation of the new strategies on all problems. The accumulated results of the global higher-timelimit runs are used to define and evolve the notion of \"similar easy problems\", and to control the selection of the next strategy to be improved. The technique was used to significantly strengthen the set of E strategies used by the MaLARea, PS-E, E-MaLeS, and E systems in the CASC@Turing 2012 competition, particularly in the Mizar division. Similar improvement was obtained on the problems created from the Flyspeck corpus.\n    ",
        "submission_date": "2013-01-12T00:00:00",
        "last_modified_date": "2014-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2774",
        "title": "Crowd Labeling: a survey",
        "authors": [
            "Jafar Muhammadi",
            "Hamid Reza Rabiee",
            "Abbas Hosseini"
        ],
        "abstract": "Recently, there has been a burst in the number of research projects on human computation via crowdsourcing. Multiple choice (or labeling) questions could be referred to as a common type of problem which is solved by this approach. As an application, crowd labeling is applied to find true labels for large machine learning datasets. Since crowds are not necessarily experts, the labels they provide are rather noisy and erroneous. This challenge is usually resolved by collecting multiple labels for each sample, and then aggregating them to estimate the true label. Although the mechanism leads to high-quality labels, it is not actually cost-effective. As a result, efforts are currently made to maximize the accuracy in estimating true labels, while fixing the number of acquired labels.\n",
        "submission_date": "2013-01-13T00:00:00",
        "last_modified_date": "2014-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3537",
        "title": "Learning Stable Group Invariant Representations with Convolutional Networks",
        "authors": [
            "Joan Bruna",
            "Arthur Szlam",
            "Yann LeCun"
        ],
        "abstract": "Transformation groups, such as translations or rotations, effectively express part of the variability observed in many recognition problems. The group structure enables the construction of invariant signal representations with appealing mathematical properties, where convolutions, together with pooling operators, bring stability to additive and geometric perturbations of the input. Whereas physical transformation groups are ubiquitous in image and audio applications, they do not account for all the variability of complex signal classes.\n",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3720",
        "title": "The IBMAP approach for Markov networks structure learning",
        "authors": [
            "Federico Schl\u00fcter",
            "Facundo Bromberg",
            "Alejandro Edera"
        ],
        "abstract": "  In this work we consider the problem of learning the structure of Markov networks from data. We present an approach for tackling this problem called IBMAP, together with an efficient instantiation of the approach: the IBMAP-HC algorithm, designed for avoiding important limitations of existing independence-based algorithms. These algorithms proceed by performing statistical independence tests on data, trusting completely the outcome of each test. In practice tests may be incorrect, resulting in potential cascading errors and the consequent reduction in the quality of the structures learned. IBMAP contemplates this uncertainty in the outcome of the tests through a probabilistic maximum-a-posteriori approach. The approach is instantiated in the IBMAP-HC algorithm, a structure selection strategy that performs a polynomial heuristic local search in the space of possible structures. We present an extensive empirical evaluation on synthetic and real data, showing that our algorithm outperforms significantly the current independence-based algorithms, in terms of data efficiency and quality of learned structures, with equivalent computational complexities. We also show the performance of IBMAP-HC in a real-world application of knowledge discovery: EDAs, which are evolutionary algorithms that use structure learning on each generation for modeling the distribution of populations. The experiments show that when IBMAP-HC is used to learn the structure, EDAs improve the convergence to the optimum.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2014-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3832",
        "title": "A Complete Calculus for Possibilistic Logic Programming with Fuzzy Propositional Variables",
        "authors": [
            "Teresa Alsinet",
            "Lluis Godo"
        ],
        "abstract": "In this paper we present a propositional logic programming language for reasoning under possibilistic uncertainty and representing vague knowledge. Formulas are represented by pairs (A, c), where A is a many-valued proposition and c is value in the unit interval [0,1] which denotes a lower bound on the belief on A in terms of necessity measures. Belief states are modeled by possibility distributions on the set of all many-valued interpretations. In this framework, (i) we define a syntax and a semantics of the general underlying uncertainty logic; (ii) we provide a modus ponens-style calculus for a sublanguage of Horn-rules and we prove that it is complete for determining the maximum degree of possibilistic belief with which a fuzzy propositional variable can be entailed from a set of formulas; and finally, (iii) we show how the computation of a partial matching between fuzzy propositional variables, in terms of necessity measures for fuzzy sets, can be included in our logic programming system. \n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3834",
        "title": "Perfect Tree-Like Markovian Distributions",
        "authors": [
            "Ann Becker",
            "Dan Geiger",
            "Christopher Meek"
        ],
        "abstract": "We show that if a strictly positive joint probability distribution for a set of binary random variables factors according to a tree, then vertex separation represents all and only the independence relations enclosed in the distribution.  The same result is shown to hold also for multivariate strictly positive normal distributions. Our proof uses a new property of conditional independence that holds for these two classes of probability distributions.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3835",
        "title": "A Principled Analysis of Merging Operations in Possibilistic Logic",
        "authors": [
            "Salem Benferhat",
            "Didier Dubois",
            "Souhila Kaci",
            "Henri Prade"
        ],
        "abstract": "Possibilistic logic offers a qualitative framework for representing pieces of information associated with levels of uncertainty of priority.  The fusion of multiple sources information is discussed in this setting. Different classes of merging operators are considered including conjunctive, disjunctive, reinforcement, adaptive and averaging operators.  Then we propose to analyse these classes in terms of postulates.  This is done by first extending the postulate for merging classical bases to the case where priorites are avaialbe.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3836",
        "title": "The Complexity of Decentralized Control of Markov Decision Processes",
        "authors": [
            "Daniel S Bernstein",
            "Shlomo Zilberstein",
            "Neil Immerman"
        ],
        "abstract": "Planning for distributed agents with partial state information is considered from a decision- theoretic perspective.  We describe generalizations of both the MDP and POMDP models that allow for decentralized control.  For even a small number of agents, the finite-horizon problems corresponding to both of our models are complete for nondeterministic exponential time.  These complexity results illustrate a fundamental difference between centralized and decentralized control of Markov processes.  In contrast to the MDP and POMDP problems, the problems we consider provably do not admit polynomial-time algorithms and most likely require doubly exponential time to solve in the worst case.  We have thus provided mathematical evidence corresponding to the intuition that decentralized planning problems cannot easily be reduced to centralized problems and solved exactly using established techniques. \n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3839",
        "title": "Approximately Optimal Monitoring of Plan Preconditions",
        "authors": [
            "Craig Boutilier"
        ],
        "abstract": "Monitoring plan preconditions can allow for replanning when a precondition fails, generally far in advance of the point in the plan where the precondition is relevant. However, monitoring is generally costly, and some precondition failures have a very small impact on plan quality. We formulate a model for optimal precondition monitoring, using partially-observable Markov decisions processes, and describe methods for solving this model efficitively, though approximately.  Specifically, we show that the single-precondition monitoring problem is generally tractable, and the multiple-precondition monitoring policies can be efficitively approximated using single-precondition soultions.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3840",
        "title": "Utilities as Random Variables: Density Estimation and Structure Discovery",
        "authors": [
            "Urszula Chajewska",
            "Daphne Koller"
        ],
        "abstract": "Decision theory does not traditionally include uncertainty over utility functions.  We argue that the a person's utility value for a given outcome can be treated as we treat other domain attributes: as a random variable with a density function over its possible values.  We show that we can apply statistical density estimation techniques to learn such a density function from a database of partially elicited utility functions.  In particular, we define a Bayesian learning framework for this problem, assuming the distribution over utilities is a mixture of Gaussians, where the mixture components represent statistically coherent subpopulations.  We can also extend our techniques to the problem of discovering generalized additivity structure in the utility functions in the population.  We define a Bayesian model selection criterion for utility function structure and a search procedure over structures.  The factorization of the utilities in the learned model, and the generalization obtained from density estimation, allows us to provide robust estimates of utilities using a significantly smaller number of utility elicitation questions.  We experiment with our technique on synthetic utility data and on a real database of utility functions in the domain of prenatal diagnosis. \n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3841",
        "title": "Computational Investigation of Low-Discrepancy Sequences in Simulation Algorithms for Bayesian Networks",
        "authors": [
            "Jian Cheng",
            "Marek J. Druzdzel"
        ],
        "abstract": "Monte Carlo sampling has become a major vehicle for approximate inference in Bayesian networks. In this paper, we investigate a family of related simulation approaches, known collectively as quasi-Monte Carlo methods based on deterministic low-discrepancy sequences. We first outline several theoretical aspects of deterministic low-discrepancy sequences, show three examples of such sequences, and then discuss practical issues related to applying them to belief updating in Bayesian networks. We propose an algorithm for selecting direction numbers for Sobol sequence. Our experimental results show that low-discrepancy sequences (especially Sobol sequence) significantly improve the performance of simulation algorithms in Bayesian networks compared to Monte Carlo sampling.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3842",
        "title": "A Decision Theoretic Approach to Targeted Advertising",
        "authors": [
            "David Maxwell Chickering",
            "David Heckerman"
        ],
        "abstract": "A simple advertising strategy that can be used to help increase sales of a product is to mail out special offers to selected potential customers. Because there is a cost associated with sending each offer, the optimal mailing strategy depends on both the benefit obtained from a purchase and how the offer affects the buying behavior of the customers. In this paper, we describe two methods for partitioning the potential customers into groups, and show how to perform a simple cost-benefit analysis to decide which, if any, of the groups should be targeted. In particular, we consider two decision-tree learning algorithms. The first is an \"off the shelf\" algorithm used to model the probability that groups of customers will buy the product. The second is a new algorithm that is similar to the first, except that for each group, it explicitly models the probability of purchase under the two mailing scenarios: (1) the mail is sent to members of that group and (2) the mail is not sent to members of that group.  Using data from a real-world advertising experiment, we compare the algorithms to each other and to a naive mail-to-all strategy.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3844",
        "title": "A Bayesian Method for Causal Modeling and Discovery Under Selection",
        "authors": [
            "Gregory F. Cooper"
        ],
        "abstract": "This paper describes a Bayesian method for learning causal networks using samples that were selected in a non-random manner from a population of interest. Examples of data obtained by non-random sampling include convenience samples and case-control data in which a fixed number of samples with and without some condition is collected; such data are not uncommon. The paper describes a method for combining data under selection with prior beliefs in order to derive a posterior probability for a model of the causal processes that are generating the data in the population of interest. The priors include beliefs about the nature of the non-random sampling procedure. Although exact application of the method would be computationally intractable for most realistic datasets, efficient special-case and approximation methods are discussed. Finally, the paper describes how to combine learning under selection with previous methods for learning from observational and experimental data that are obtained on random samples of the population of interest. The net result is a Bayesian methodology that supports causal modeling and discovery from a rich mixture of different types of data. \n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3845",
        "title": "Separation Properties of Sets of Probability Measures",
        "authors": [
            "Fabio Gagliardi Cozman"
        ],
        "abstract": "This paper analyzes independence concepts for sets of probability measures associated with directed acyclic graphs. The paper shows that epistemic independence and the standard Markov condition violate desirable separation properties. The adoption of a contraction condition leads to d-separation but still fails to guarantee a belief separation property. To overcome this unsatisfactory situation, a  strong Markov condition is proposed, based on epistemic independence. The main result is that the strong Markov condition leads to strong independence and does enforce separation properties; this result implies that (1) separation properties of Bayesian networks do extend     to epistemic independence and sets of probability measures, and (2) strong independence has a clear justification based on     epistemic independence and the strong Markov condition. \n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3846",
        "title": "Stochastic Logic Programs: Sampling, Inference and Applications",
        "authors": [
            "James Cussens"
        ],
        "abstract": "Algorithms for exact and approximate inference in stochastic logic   programs (SLPs) are presented, based respectively, on variable   elimination and importance sampling. We then show how SLPs can be   used to represent prior distributions for machine learning, using   (i) logic programs and (ii) Bayes net structures as examples.   Drawing on existing work in statistics, we apply the   Metropolis-Hasting algorithm to construct a Markov chain which   samples from the posterior distribution. A Prolog implementation for   this is described. We also discuss the possibility of constructing   explicit representations of the posterior.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3847",
        "title": "A Differential Approach to Inference in Bayesian Networks",
        "authors": [
            "Adnan Darwiche"
        ],
        "abstract": "We present a new approach for inference in Bayesian networks, which is mainly based on partial differentiation. According to this approach, one compiles a Bayesian network into a multivariate polynomial and then computes the partial derivatives of this polynomial with respect to each variable. We show that once such derivatives are made available, one can compute in constant-time answers to a large class of probabilistic queries, which are central to classical inference, parameter estimation, model validation and sensitivity analysis. We present a number of complexity results relating to the compilation of such polynomials and to the computation of their partial derivatives. We argue that the combined simplicity, comprehensiveness and computational complexity of the presented framework is unique among existing frameworks for inference in Bayesian networks.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3848",
        "title": "Any-Space Probabilistic Inference",
        "authors": [
            "Adnan Darwiche"
        ],
        "abstract": "We have recently introduced an any-space algorithm for exact inference in Bayesian networks, called Recursive Conditioning, RC, which allows one to trade space with time at increments of X-bytes, where X is the number of bytes needed to cache a floating point number. In this paper, we present three key extensions of RC. First, we modify the algorithm so it applies to more general factorization of probability distributions, including (but not limited to) Bayesian network factorizations. Second, we present a forgetting mechanism which reduces the space requirements of RC considerably and then compare such requirmenets with those of variable elimination on a number of realistic networks, showing orders of magnitude improvements in certain cases. Third, we present a version of RC for computing maximum a posteriori hypotheses (MAP), which turns out to be the first MAP algorithm allowing a smooth time-space tradeoff. A key advantage of presented MAP algorithm is that it does not have to start from scratch each time a new query is presented, but can reuse some of its computations across multiple queries, leading to significant savings in ceratain cases.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3855",
        "title": "Likelihood Computations Using Value Abstractions",
        "authors": [
            "Nir Friedman",
            "Dan Geiger",
            "Noam Lotner"
        ],
        "abstract": "In this paper, we use evidence-specific value abstraction for speeding Bayesian networks inference. This is done by grouping variable values and treating the combined values as a single entity. As we show, such abstractions can exploit regularities in conditional probability distributions and also the specific values of observed variables. To formally justify value abstraction, we define the notion of safe value abstraction and devise inference algorithms that use it to reduce the cost of inference. Our procedure is particularly useful for learning complex networks with many hidden variables. In such cases, repeated likelihood computations are required for EM or other parameter optimization techniques. Since these computations are repeated with respect to the same evidence set, our methods can provide significant speedup to the learning procedure. We demonstrate the algorithm on genetic linkage problems where the use of value abstraction sometimes differentiates between a feasible and non-feasible solution. \n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3857",
        "title": "Gaussian Process Networks",
        "authors": [
            "Nir Friedman",
            "Iftach Nachman"
        ],
        "abstract": "In this paper we address the problem of learning the structure of a Bayesian network in domains with continuous variables.  This task requires a procedure for comparing different candidate structures. In the Bayesian framework, this is done by evaluating the {em marginal likelihood/} of the data given a candidate structure. This term can be computed in closed-form for standard parametric families (e.g., Gaussians), and can be approximated, at some computational cost, for some semi-parametric families (e.g., mixtures of Gaussians).\n",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3858",
        "title": "A Qualitative Linear Utility Theory for Spohn's Theory of Epistemic Beliefs",
        "authors": [
            "Phan H. Giang",
            "Prakash P. Shenoy"
        ],
        "abstract": "In this paper, we formulate a qualitative \"linear\" utility theory for lotteries in which uncertainty is expressed qualitatively using a Spohnian disbelief function. We argue that a rational decision maker facing an uncertain decision problem in which the uncertainty is expressed qualitatively should behave so as to maximize \"qualitative expected utility.\" Our axiomatization of the qualitative utility is similar to the axiomatization developed by von Neumann and Morgenstern for probabilistic lotteries. We compare our results with other recent results in qualitative decision making.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3859",
        "title": "Building a Stochastic Dynamic Model of Application Use",
        "authors": [
            "Peter J. Gorniak",
            "David L. Poole"
        ],
        "abstract": "Many intelligent user interfaces employ application and user models to determine the user's preferences, goals and likely future actions. Such models require application analysis, adaptation and expansion. Building and maintaining such models adds a substantial amount of time and labour to the application development cycle.  We present a system that observes the interface of an unmodified application and records users' interactions with the application. From a history of such observations we build a coarse state space of observed interface states and actions between them.  To refine the space, we hypothesize sub-states based upon the histories that led users to a given state.  We evaluate the information gain of possible state splits, varying the length of the histories considered in such splits.  In this way, we automatically produce a stochastic dynamic model of the application and of how it is used.  To evaluate our approach, we present models derived from real-world application usage data.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3860",
        "title": "Maximum Entropy and the Glasses You Are Looking Through",
        "authors": [
            "Peter D. Grunwald"
        ],
        "abstract": "We give an interpretation of the Maximum Entropy (MaxEnt) Principle in game-theoretic terms.  Based on this interpretation, we make a formal distinction between different ways of {em applying/} Maximum Entropy distributions. MaxEnt has frequently been criticized on the grounds that it leads to highly representation dependent results. Our distinction allows us to avoid this problem in many cases.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3861",
        "title": "Inference for Belief Networks Using Coupling From the Past",
        "authors": [
            "Michael Harvey",
            "Radford M. Neal"
        ],
        "abstract": " Inference for belief networks using Gibbs sampling produces a distribution for unobserved variables that differs from the correct distribution by a (usually) unknown error, since convergence to the right distribution occurs only asymptotically.  The method of \"coupling from the past\" samples from exactly the correct distribution by (conceptually) running dependent Gibbs sampling simulations from every possible starting state from a time far enough in the past that all runs reach the same state at time t=0. Explicitly considering every possible state is intractable for large networks, however.  We propose a method for layered noisy-or networks that uses a compact, but often imprecise, summary of a set of states. This method samples from exactly the correct distribution, and requires only about twice the time per step as ordinary Gibbs sampling, but it may require more simulation steps than would be needed if chains were tracked exactly.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3862",
        "title": "Dependency Networks for Collaborative Filtering and Data Visualization",
        "authors": [
            "David Heckerman",
            "David Maxwell Chickering",
            "Christopher Meek",
            "Robert Rounthwaite",
            "Carl Kadie"
        ],
        "abstract": "We describe a graphical model for probabilistic relationships---an alternative to the Bayesian network---called a dependency network.  The graph of a dependency network, unlike a Bayesian network, is potentially cyclic.  The probability component of a dependency network, like a Bayesian network, is a set of conditional distributions, one for each node given its parents.  We identify several basic properties of this representation and describe a computationally efficient procedure for learning the graph and probability components from data.  We describe the application of this representation to probabilistic inference, collaborative filtering (the task of predicting preferences), and the visualization of acausal predictive relationships.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3863",
        "title": "YGGDRASIL - A Statistical Package for Learning Split Models",
        "authors": [
            "Soren Hojsgaard"
        ],
        "abstract": "There are two main objectives of this paper. The first is to present a statistical framework for models with context specific independence structures, i.e., conditional independences holding only for sepcific values of the conditioning variables. This framework is constituted by the class of split models.  Split models are extension of graphical models for contigency tables and allow for a more sophisticiated modelling than graphical models. The treatment of split models include estimation, representation and a Markov property for reading off those independencies holding in a specific context.  The second objective is to present a software package named YGGDRASIL which is designed for statistical inference in split models, i.e., for learning such models on the basis of data.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3864",
        "title": "Probabilistic Arc Consistency: A Connection between Constraint Reasoning and Probabilistic Reasoning",
        "authors": [
            "Michael C. Horsch",
            "Bill Havens"
        ],
        "abstract": "We document a connection between constraint reasoning and probabilistic reasoning. We present an algorithm, called {em probabilistic arc consistency}, which is both a generalization of a well known algorithm for arc consistency used in constraint reasoning, and a specialization of the belief updating algorithm for singly-connected networks. Our algorithm is exact for singly- connected constraint problems, but can work well as an approximation for arbitrary problems. We briefly discuss some empirical results, and related methods.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3866",
        "title": "Marginalization in Composed Probabilistic Models",
        "authors": [
            "Radim Jirousek"
        ],
        "abstract": "Composition of low-dimensional distributions, whose foundations were laid in the papaer published in the Proceeding of UAI'97 (Jirousek 1997), appeared to be an alternative apparatus to describe multidimensional probabilistic models. In contrast to Graphical Markov Models, which define multidomensinoal distributions in a declarative way, this approach is rather procedural.  Ordering of low-dimensional distributions into a proper sequence fully defines the resepctive computational procedure; therefore, a stury of different type of generating sequences is one fo the central problems in this field.  Thus, it appears that an important role is played by special sequences that are called perfect.  Their main characterization theorems are presetned in this paper.  However, the main result of this paper is a solution to the problem of margnialization for general sequences.  The main theorem describes a way to obtain a generating sequence that defines the model corresponding to the marginal of the distribution defined by an arbitrary genearting sequence.  From this theorem the reader can see to what extent these comutations are local; i.e., the sequence consists of marginal distributions whose computation must be made by summing up over the values of the variable eliminated (the paper deals with finite model).\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3868",
        "title": "Making Sensitivity Analysis Computationally Efficient",
        "authors": [
            "Uffe Kj\u00e6rulff",
            "Linda C. van der Gaag"
        ],
        "abstract": "To investigate the robustness of the output probabilities of a Bayesian network, a sensitivity analysis can be performed.  A one-way sensitivity analysis establishes, for each of the probability parameters of a network, a function expressing a posterior marginal probability of interest in terms of the parameter.  Current methods for computing the coefficients in such a function rely on a large number of network evaluations.  In this paper, we present a method that requires just a single outward propagation in a junction tree for establishing the coefficients in the functions for all possible parameters; in addition, an inward propagation is required for processing evidence.  Conversely, the method requires a single outward propagation for computing the coefficients in the functions expressing all possible posterior marginals in terms of a single parameter.  We extend these results to an n-way sensitivity analysis in which sets of parameters are studied.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3869",
        "title": "Policy Iteration for Factored MDPs",
        "authors": [
            "Daphne Koller",
            "Ron Parr"
        ],
        "abstract": "Many large MDPs can be represented compactly using a dynamic Bayesian network.  Although the structure of the value function does not retain the structure of the process, recent work has shown that value functions in factored MDPs can often be approximated well using a decomposed value function: a linear combination of <I>restricted</I> basis functions, each of which refers only to a small subset of variables.  An approximate value function for a particular policy can be computed using approximate dynamic programming, but this approach (and others) can only produce an approximation relative to a distance metric which is weighted by the stationary distribution of the current policy.  This type of weighted projection is ill-suited to policy improvement.  We present a new approach to value determination, that uses a simple closed-form computation to directly compute a least-squares decomposed approximation to the value function <I>for any weights</I>.  We then use this value determination algorithm as a subroutine in a policy iteration process.  We show that, under reasonable restrictions, the policies induced by a factored value function are compactly represented, and can be manipulated efficiently in a policy iteration process.  We also present a method for computing error bounds for decomposed value functions using a variable-elimination algorithm for function optimization.  The complexity of all of our algorithms depends on the factorization of system dynamics and of the approximate value function. \n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3871",
        "title": "Combinatorial Optimization by Learning and Simulation of Bayesian Networks",
        "authors": [
            "Pedro Larra\u00f1aga",
            "Ramon Etxeberria",
            "Jose A. Lozano",
            "Jose M. Pena"
        ],
        "abstract": "This paper shows how the Bayesian network paradigm can be used in order to solve combinatorial optimization problems. To do it some methods of structure learning from data and simulation of Bayesian networks are inserted inside Estimation of Distribution Algorithms (EDA). EDA are a new tool for evolutionary computation in which populations of individuals are created by estimation and simulation of the joint probability distribution of the selected individuals. We propose new approaches to EDA for combinatorial optimization based on the theory of probabilistic graphical models. Experimental results are also presented.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3872",
        "title": "Causal Mechanism-based Model Construction",
        "authors": [
            "Tsai-Ching Lu",
            "Marek J. Druzdzel",
            "Tze-Yun Leong"
        ],
        "abstract": "We propose a framework for building graphical causal model that is based on the concept of causal mechanisms. Causal models are intuitive for human users and, more importantly, support the prediction of the effect of manipulation. We describe an implementation of the proposed framework as an interactive model construction module, ImaGeNIe, in SMILE (Structural Modeling, Inference, and Learning Engine) and in GeNIe (SMILE's Windows user interface).\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3873",
        "title": "Credal Networks under Maximum Entropy",
        "authors": [
            "Thomas Lukasiewicz"
        ],
        "abstract": "We apply the principle of maximum entropy to select a unique joint probability distribution from the set of all joint probability distributions specified by a credal network. In detail, we start by showing that the unique joint distribution of a Bayesian tree coincides with the maximum entropy model of its conditional distributions. This result, however, does not hold anymore for general Bayesian networks. We thus present a new kind of maximum entropy models, which are computed sequentially. We then show that for all general Bayesian networks, the sequential maximum entropy model coincides with the unique joint distribution. Moreover, we apply the new principle of sequential maximum entropy to interval Bayesian networks and more generally to credal networks. We especially show that this application is equivalent to a number of small local entropy maximizations. \n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3874",
        "title": "Risk Agoras: Dialectical Argumentation for Scientific Reasoning",
        "authors": [
            "Peter McBurney",
            "Simon Parsons"
        ],
        "abstract": "We propose a formal framework for intelligent systems which can reason about scientific domains, in particular about the carcinogenicity of chemicals, and we study its properties. Our framework is grounded in a philosophy of scientific enquiry and discourse, and uses a model of dialectical argumentation. The formalism enables representation of scientific uncertainty and conflict in a manner suitable for qualitative reasoning about the domain. \n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3876",
        "title": "Probabilistic Models for Agents' Beliefs and Decisions",
        "authors": [
            "Brian Milch",
            "Daphne Koller"
        ],
        "abstract": "Many applications of intelligent systems require reasoning about the mental states of agents in the domain.  We may want to reason about an agent's beliefs, including beliefs about other agents; we may also want to reason about an agent's preferences, and how his beliefs and preferences relate to his behavior.  We define a probabilistic epistemic logic (PEL) in which belief statements are given a formal semantics, and provide an algorithm for asserting and querying PEL formulas in Bayesian networks.  We then show how to reason about an agent's behavior by modeling his decision process as an influence diagram and assuming that he behaves rationally.  PEL can then be used for reasoning from an agent's observed actions to conclusions about other aspects of the domain, including unobserved domain variables and the agent's mental states. \n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3878",
        "title": "PEGASUS: A Policy Search Method for Large MDPs and POMDPs",
        "authors": [
            "Andrew Y. Ng",
            "Michael I. Jordan"
        ],
        "abstract": "We propose a new approach to the problem of searching a space of policies for a Markov decision process (MDP) or a partially observable Markov decision process (POMDP), given a model.  Our approach is based on the following observation: Any (PO)MDP can be transformed into an \"equivalent\" POMDP in which all state transitions (given the current state and action) are deterministic.  This reduces the general problem of policy search to one in which we need only consider POMDPs with deterministic transitions.  We give a natural way of estimating the value of all policies in these transformed POMDPs.  Policy search is then simply performed by searching for a policy with high estimated value.  We also establish conditions under which our value estimates will be good, recovering theoretical results similar to those of Kearns, Mansour and Ng (1999), but with \"sample complexity\" bounds that have only a polynomial rather than exponential dependence on the horizon time.  Our method applies to arbitrary POMDPs, including ones with infinite state and action spaces.  We also present empirical results for our approach on a small discrete problem, and on a complex continuous state/continuous action problem involving learning to ride a bicycle. \n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3879",
        "title": "Representing and Solving Asymmetric Bayesian Decision Problems",
        "authors": [
            "Thomas D. Nielsen",
            "Finn Verner Jensen"
        ],
        "abstract": "This paper deals with the representation and solution of asymmetric Bayesian decision problems. We present a formal framework, termed asymmetric influence diagrams, that is based on the influence diagram and allows an efficient representation of asymmetric decision problems. As opposed to existing frameworks, the asymmetric influece diagram primarily encodes asymmetry at the qualitative level and it can therefore be read directly from the model. We give an algorithm for solving asymmetric influence diagrams. The algorithm initially decomposes the asymmetric decision problem into a structure of symmetric subproblems organized as a tree. A solution to the decision problem can then be found by propagating from the leaves toward the root using existing evaluation methods to solve the sub-problems.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3880",
        "title": "Using ROBDDs for Inference in Bayesian Networks with Troubleshooting as an Example",
        "authors": [
            "Thomas D. Nielsen",
            "Pierre-Henri Wuillemin",
            "Finn Verner Jensen",
            "Uffe Kj\u00e6rulff"
        ],
        "abstract": "When using Bayesian networks for modelling the behavior of man-made machinery, it usually happens that a large part of the model is deterministic. For such Bayesian networks deterministic part of the model can be represented as a Boolean function, and a central part of belief updating reduces to the task of calculating the number of satisfying configurations in a Boolean function. In this paper we explore how advances in the calculation of Boolean functions can be adopted for belief updating, in particular within the context of troubleshooting. We present experimental results indicating a substantial speed-up compared to traditional junction tree propagation.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3881",
        "title": "Evaluating Influence Diagrams using LIMIDs",
        "authors": [
            "Dennis Nilsson",
            "Steffen L. Lauritzen"
        ],
        "abstract": "We present a new approach to the solution of decision problems formulated as influence diagrams. The approach converts the influence diagram into a simpler structure, the LImited Memory Influence Diagram (LIMID), where only the requisite information for the computation of optimal policies is depicted. Because the requisite information is explicitly represented in the diagram, the evaluation procedure can take advantage of it. In this paper we show how to convert an influence diagram to a LIMID and describe the procedure for finding an optimal strategy. Our approach can yield significant savings of memory and computational time when compared to traditional methods.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3882",
        "title": "Adaptive Importance Sampling for Estimation in Structured Domains",
        "authors": [
            "Luis E. Ortiz",
            "Leslie Pack Kaelbling"
        ],
        "abstract": "Sampling is an important tool for estimating large, complex sums and integrals over high dimensional spaces. For instance, important sampling has been used as an alternative to exact methods for inference in belief networks. Ideally, we want to have a sampling distribution that provides optimal-variance estimators. In this paper, we present methods that improve the sampling distribution by systematically adapting it as we obtain information from the samples. We present a stochastic-gradient-descent method for sequentially updating the sampling distribution based on the direct minization of the variance. We also present other stochastic-gradient-descent methods based on the minimizationof typical notions of distance between the current sampling distribution and approximations of the target, optimal distribution. We finally validate and compare the different methods empirically by applying them to the problem of action evaluation in influence diagrams.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3883",
        "title": "Conversation as Action Under Uncertainty",
        "authors": [
            "Tim Paek",
            "Eric J. Horvitz"
        ],
        "abstract": "Conversations abound with uncetainties of various kinds. Treating conversation as inference and decision making under uncertainty, we propose a task independent, multimodal architecture for supporting robust continuous spoken dialog called Quartet. We introduce four interdependent levels of analysis, and describe representations, inference procedures, and decision strategies for managing uncertainties within and between the levels. We highlight the approach by reviewing interactions between a user and two spoken dialog systems developed using the Quartet architecture: Prsenter, a prototype system for navigating Microsoft PowerPoint presentations, and the Bayesian Receptionist, a prototype system for dealing with tasks typically handled by front desk receptionists at the Microsoft corporate campus.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3884",
        "title": "Probabilistic Models for Query Approximation with Large Sparse Binary Datasets",
        "authors": [
            "Dmitry Y. Pavlov",
            "Heikki Mannila",
            "Padhraic Smyth"
        ],
        "abstract": "Large sparse sets of binary transaction data with millions of records and thousands of attributes occur in various domains: customers purchasing products, users visiting web pages, and documents containing words are just three typical examples. Real-time query selectivity estimation (the problem of estimating the number of rows in the data satisfying a given predicate) is an important practical problem for such databases.\n",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3887",
        "title": "Value-Directed Belief State Approximation for POMDPs",
        "authors": [
            "Pascal Poupart",
            "Craig Boutilier"
        ],
        "abstract": "We consider the problem belief-state monitoring for the purposes of implementing a policy for a partially-observable Markov decision process (POMDP), specifically how one might approximate the belief state.  Other schemes for belief-state approximation (e.g., based on minimixing a measures such as KL-diveregence between the true and estimated state) are not necessarily appropriate for POMDPs. Instead we propose a framework for analyzing value-directed approximation schemes, where approximation quality is determined by the expected error in utility rather than by the error in the belief state itself.  We propose heuristic methods for finding good projection schemes for belief state estimation - exhibiting anytime characteristics - given a POMDP value fucntion. We also describe several algorithms for constructing bounds on the error in decision quality (expected utility) associated with acting in accordance with a given belief state approximation.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3888",
        "title": "Probabilistic State-Dependent Grammars for Plan Recognition",
        "authors": [
            "David V. Pynadath",
            "Michael P. Wellman"
        ],
        "abstract": "Techniques for plan recognition under uncertainty require a stochastic model of the plan-generation process.  We introduce Probabilistic State-Dependent Grammars (PSDGs) to represent an agent's plan-generation process.  The PSDG language model extends probabilistic context-free grammars (PCFGs) by allowing production probabilities to depend on an explicit model of the planning agent's internal and external state.  Given a PSDG description of the plan-generation process, we can then use inference algorithms that exploit the particular independence properties of the PSDG language to efficiently answer plan-recognition queries.  The combination of the PSDG language model and inference algorithms extends the range of plan-recognition domains for which practical probabilistic inference is possible, as illustrated by applications in traffic monitoring and air combat. \n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3889",
        "title": "Pivotal Pruning of Trade-offs in QPNs",
        "authors": [
            "Silja Renooij",
            "Linda C. van der Gaag",
            "Simon Parsons",
            "Shaw Green"
        ],
        "abstract": "Qualitative probabilistic networks have been designed for probabilistic reasoning in a qualitative way.  Due to their coarse level of representation detail, qualitative probabilistic networks do not provide for resolving trade-offs and typically yield ambiguous results upon inference.  We present an algorithm for computing more insightful results for unresolved trade-offs. The algorithm builds upon the idea of using pivots to zoom in on the trade-offs and identifying the information that would serve to resolve them. \n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3893",
        "title": "A Knowledge Acquisition Tool for Bayesian-Network Troubleshooters",
        "authors": [
            "Claus Skaanning"
        ],
        "abstract": "This paper describes a domain-specific knowledge acquisition tool for intelligent automated troubleshooters based on Bayesian networks.  No Bayesian network knowledge is required to use the tool, and troubleshooting information can be specified as natural and intuitive as possible.  Probabilities can be specified in the direction that is most natural to the domain expert.  Thus, the knowledge acquisition efficiently removes the traditional knowledge acquisition bottleneck of Bayesian networks.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3894",
        "title": "On the Use of Skeletons when Learning in Bayesian Networks",
        "authors": [
            "Harald Steck"
        ],
        "abstract": "In this paper, we present a heuristic operator  which aims at simultaneously optimizing the orientations of all the edges in an intermediate Bayesian network structure during the search process.  This is done by alternating  between the space of directed acyclic graphs (DAGs) and the space of skeletons.  The found orientations of the edges are based on a scoring function rather than on induced conditional independences. This operator can be used as an extension to commonly employed search strategies.  It is evaluated in experiments with artificial and real-world data.  \n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3897",
        "title": "A Branch-and-Bound Algorithm for MDL Learning Bayesian Networks",
        "authors": [
            "Jin Tian"
        ],
        "abstract": "This paper extends the work in [Suzuki, 1996] and presents an efficient depth-first branch-and-bound algorithm for learning Bayesian network structures, based on the minimum description length (MDL) principle, for a given (consistent) variable ordering. The algorithm exhaustively searches through all network structures and guarantees to find the network with the best MDL score. Preliminary experiments show that the algorithm is efficient, and that the time complexity grows slowly with the sample size. The algorithm is useful for empirically studying both the performance of suboptimal heuristic search algorithms and the adequacy of the MDL principle in learning Bayesian networks. \n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3898",
        "title": "Probabilities of Causation: Bounds and Identification",
        "authors": [
            "Jin Tian",
            "Judea Pearl"
        ],
        "abstract": "This paper deals with the problem of estimating the probability that one event was a cause of another in a given scenario. Using structural-semantical definitions of the probabilities of necessary or sufficient causation (or both), we show how to optimally bound these quantities from data obtained in experimental and observational studies, making minimal assumptions concerning the data-generating process. In particular, we strengthen the results  of Pearl (1999) by weakening the data-generation assumptions and deriving theoretically sharp bounds on the probabilities of causation. These results delineate precisely how empirical data can be used both in settling questions of attribution and in solving attribution-related problems of decision making. \n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3900",
        "title": "Conditional Independence and Markov Properties in Possibility Theory",
        "authors": [
            "Jirina Vejnarova"
        ],
        "abstract": "Conditional independence and Markov properties are powerful tools allowing expression of multidimensional probability distributions by means of low-dimensional ones.  As multidimensional possibilistic models have been studied for several years, the demand for analogous tools in possibility theory seems to be quite natural.  This paper is intended to be a promotion of de Cooman's measure-theoretic approcah to possibility theory, as this approach allows us to find analogies to many important results obtained in probabilistic framework.  First, we recall semi-graphoid properties of conditional possibilistic independence, parameterized by a continuous t-norm, and find sufficient conditions for a class of Archimedean t-norms to have the graphoid property.  Then we introduce Markov properties and factorization of possibility distrubtions (again parameterized by a continuous t-norm) and find the relationships between them.  These results are accompanied by a number of conterexamples, which show that the assumptions of specific theorems are substantial.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3902",
        "title": "Model Criticism of Bayesian Networks with Latent Variables",
        "authors": [
            "David M. Williamson",
            "Russell Almond",
            "Robert Mislevy"
        ],
        "abstract": "The application of Bayesian networks (BNs) to cognitive assessment and intelligent tutoring systems poses new challenges for model construction.  When cognitive task analyses suggest constructing a BN with several latent variables, empirical model criticism of the latent structure becomes both critical and complex.  This paper introduces a methodology for criticizing models both globally (a BN in its entirety) and locally (observable nodes), and explores its value in identifying several kinds of misfit: node errors, edge errors, state errors, and prior probability errors in the latent structure.  The results suggest the indices have potential for detecting model misfit and assisting in locating problematic components of the model.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3903",
        "title": "Exploiting Qualitative Knowledge in the Learning of Conditional Probabilities of Bayesian Networks",
        "authors": [
            "Frank Wittig",
            "Anthony Jameson"
        ],
        "abstract": "Algorithms for learning the conditional probabilities of Bayesian networks with hidden variables typically operate within a high-dimensional search space and yield only locally optimal solutions. One way of limiting the search space and avoiding local optima is to impose qualitative constraints that are based on background knowledge concerning the domain. We present a method for integrating formal statements of qualitative constraints into two learning algorithms, APN and EM. In our experiments with synthetic data, this method yielded networks that satisfied the constraints almost perfectly. The accuracy of the learned networks was consistently superior to that of corresponding networks learned without constraints. The exploitation of qualitative constraints therefore appears to be a promising way to increase both the interpretability and the accuracy of learned Bayesian networks with known structure. \n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.4137",
        "title": "When you talk about \"Information processing\" what actually do you have in mind?",
        "authors": [
            "Emanuel Diamant"
        ],
        "abstract": "\"Information Processing\" is a recently launched buzzword whose meaning is vague and obscure even for the majority of its users. The reason for this is the lack of a suitable definition for the term \"information\". In my attempt to amend this bizarre situation, I have realized that, following the insights of Kolmogorov's Complexity theory, information can be defined as a description of structures observable in a given data set. Two types of structures could be easily distinguished in every data set - in this regard, two types of information (information descriptions) should be designated: physical information and semantic information. Kolmogorov's theory also posits that the information descriptions should be provided as a linguistic text structure. This inevitably leads us to an assertion that information processing has to be seen as a kind of text processing. The idea is not new - inspired by the observation that human information processing is deeply rooted in natural language handling customs, Lotfi Zadeh and his followers have introduced the so-called \"Computing With Words\" paradigm. Despite of promotional efforts, the idea is not taking off yet. The reason - a lack of a coherent understanding of what should be called \"information\", and, as a result, misleading research roadmaps and objectives. I hope my humble attempt to clarify these issues would be helpful in avoiding common traps and pitfalls.\n    ",
        "submission_date": "2012-12-19T00:00:00",
        "last_modified_date": "2012-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.4272",
        "title": "View-based propagation of decomposable constraints",
        "authors": [
            "Marco Correia",
            "Pedro Barahona"
        ],
        "abstract": "Constraints that may be obtained by composition from simpler constraints are present, in some way or another, in almost every constraint program. The decomposition of such constraints is a standard technique for obtaining an adequate propagation algorithm from a combination of propagators designed for simpler constraints. The decomposition approach is appealing in several ways. Firstly because creating a specific propagator for every constraint is clearly infeasible since the number of constraints is infinite. Secondly, because designing a propagation algorithm for complex constraints can be very challenging. Finally, reusing existing propagators allows to reduce the size of code to be developed and maintained. Traditionally, constraint solvers automatically decompose constraints into simpler ones using additional auxiliary variables and propagators, or expect the users to perform such decomposition themselves, eventually leading to the same propagation model. In this paper we explore views, an alternative way to create efficient propagators for such constraints in a modular, simple and correct way, which avoids the introduction of auxiliary variables and propagators.\n    ",
        "submission_date": "2013-01-17T00:00:00",
        "last_modified_date": "2013-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.4430",
        "title": "User Interface Tools for Navigation in Conditional Probability Tables and Elicitation of Probabilities in Bayesian Networks",
        "authors": [
            "Haiqin Wang",
            "Marek J. Druzdzel"
        ],
        "abstract": "Elicitation of probabilities is one of the most laborious tasks in  building decision-theoretic models, and one that has so far  received only moderate attention in decision-theoretic systems. We  propose a set of user interface tools for graphical probabilistic  models, focusing on two aspects of probability elicitation: (1)  navigation through conditional probability tables and (2)  interactive graphical assessment of discrete probability  distributions. We propose two new graphical views that aid  navigation in very large conditional probability tables: the  CPTree (Conditional Probability Tree) and the SCPT  (shrinkable Conditional Probability Table). Based on what is known  about graphical presentation of quantitative data to humans, we  offer several useful enhancements to probability wheel and bar  graph, including different chart styles and options that can be  adapted to user preferences and needs. We present the results of a  simple usability study that proves the value of the proposed  tools.\n    ",
        "submission_date": "2013-01-18T00:00:00",
        "last_modified_date": "2013-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.4604",
        "title": "Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence (2012)",
        "authors": [
            "Nando de Freitas",
            "Kevin Murphy"
        ],
        "abstract": "This is the Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, which was held on Catalina Island, CA August 14-18 2012.\n    ",
        "submission_date": "2013-01-19T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.4606",
        "title": "Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence (2003)",
        "authors": [
            "Christopher Meek",
            "Uffe Kjaerulff"
        ],
        "abstract": "This is the Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence, which was held in Acapulco, Mexico, August 7-10 2003\n    ",
        "submission_date": "2013-01-19T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.4607",
        "title": "Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (2001)",
        "authors": [
            "John Breese",
            "Daphne Koller"
        ],
        "abstract": "This is the Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence, which was held in Seattle, WA, August 2-5 2001\n    ",
        "submission_date": "2013-01-19T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.4608",
        "title": "Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence (2002)",
        "authors": [
            "Adnan Darwiche",
            "Nir Friedman"
        ],
        "abstract": "This is the Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence, which was held in Alberta, Canada, August 1-4 2002\n    ",
        "submission_date": "2013-01-19T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.4659",
        "title": "English Sentence Recognition using Artificial Neural Network through Mouse-based Gestures",
        "authors": [
            "Firoj Parwej"
        ],
        "abstract": "Handwriting is one of the most important means of daily communication. Although the problem of handwriting recognition has been considered for more than 60 years there are still many open issues, especially in the task of unconstrained handwritten sentence recognition. This paper focuses on the automatic system that recognizes continuous English sentence through a mouse-based gestures in real-time based on Artificial Neural Network. The proposed Artificial Neural Network is trained using the traditional backpropagation algorithm for self supervised neural network which provides the system with great learning ability and thus has proven highly successful in training for feed-forward Artificial Neural Network. The designed algorithm is not only capable of translating discrete gesture moves, but also continuous gestures through the mouse. In this paper we are using the efficient neural network approach for recognizing English sentence drawn by mouse. This approach shows an efficient way of extracting the boundary of the English Sentence and specifies the area of the recognition English sentence where it has been drawn in an image and then used Artificial Neural Network to recognize the English sentence. The proposed approach English sentence recognition (ESR) system is designed and tested successfully. Experimental results show that the higher speed and accuracy were examined.\n    ",
        "submission_date": "2013-01-20T00:00:00",
        "last_modified_date": "2013-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.4991",
        "title": "Knowledge Base Approach for 3D Objects Detection in Point Clouds Using 3D Processing and Specialists Knowledge",
        "authors": [
            "Helmi Ben Hmida",
            "Christophe Cruz",
            "Frank Boochs",
            "Christophe Nicolle"
        ],
        "abstract": "This paper presents a knowledge-based detection of objects approach using the OWL ontology language, the Semantic Web Rule Language, and 3D processing built-ins aiming at combining geometrical analysis of 3D point clouds and specialist's knowledge. Here, we share our experience regarding the creation of 3D semantic facility model out of unorganized 3D point clouds. Thus, a knowledge-based detection approach of objects using the OWL ontology language is presented. This knowledge is used to define SWRL detection rules. In addition, the combination of 3D processing built-ins and topological Built-Ins in SWRL rules allows a more flexible and intelligent detection, and the annotation of objects contained in 3D point clouds. The created WiDOP prototype takes a set of 3D point clouds as input, and produces as output a populated ontology corresponding to an indexed scene visualized within VRML language. The context of the study is the detection of railway objects materialized within the Deutsche Bahn scene such as signals, technical cupboards, electric poles, etc. Thus, the resulting enriched and populated ontology, that contains the annotations of objects in the point clouds, is used to feed a GIS system or an IFC file for architecture purposes.\n    ",
        "submission_date": "2013-01-21T00:00:00",
        "last_modified_date": "2013-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.4992",
        "title": "From 9-IM Topological Operators to Qualitative Spatial Relations using 3D Selective Nef Complexes and Logic Rules for bodies",
        "authors": [
            "Helmi Ben Hmida",
            "Christophe Cruz",
            "Frank Boochs",
            "Christophe Nicolle"
        ],
        "abstract": "This paper presents a method to compute automatically topological relations using SWRL rules. The calculation of these rules is based on the definition of a Selective Nef Complexes Nef Polyhedra structure generated from standard Polyhedron. The Selective Nef Complexes is a data model providing a set of binary Boolean operators such as Union, Difference, Intersection and Symmetric difference, and unary operators such as Interior, Closure and Boundary. In this work, these operators are used to compute topological relations between objects defined by the constraints of the 9 Intersection Model (9-IM) from Egenhofer. With the help of these constraints, we defined a procedure to compute the topological relations on Nef polyhedra. These topological relationships are Disjoint, Meets, Contains, Inside, Covers, CoveredBy, Equals and Overlaps, and defined in a top-level ontology with a specific semantic definition on relation such as Transitive, Symmetric, Asymmetric, Functional, Reflexive, and Irreflexive. The results of the computation of topological relationships are stored in an OWL-DL ontology allowing after what to infer on these new relationships between objects. In addition, logic rules based on the Semantic Web Rule Language allows the definition of logic programs that define which topological relationships have to be computed on which kind of objects with specific attributes. For instance, a \"Building\" that overlaps a \"Railway\" is a \"RailStation\".\n    ",
        "submission_date": "2013-01-21T00:00:00",
        "last_modified_date": "2013-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.5943",
        "title": "Identifying Player\u015b Strategies in No Limit Texas Hold\u00e9m Poker through the Analysis of Individual Moves",
        "authors": [
            "Lu\u00eds Filipe Te\u00f3filo",
            "Luis Paulo Reis"
        ],
        "abstract": "The development of competitive artificial Poker playing agents has proven to be a challenge, because agents must deal with unreliable information and deception which make it essential to model the opponents in order to achieve good results. This paper presents a methodology to develop opponent modeling techniques for Poker agents. The approach is based on applying clustering algorithms to a Poker game database in order to identify player types based on their actions. First, common game moves were identified by clustering all players\\' moves. Then, player types were defined by calculating the frequency with which the players perform each type of movement. With the given dataset, 7 different types of players were identified with each one having at least one tactic that characterizes him. The identification of player types may improve the overall performance of Poker agents, because it helps the agents to predict the opponent\u015b moves, by associating each opponent to a distinct cluster.\n    ",
        "submission_date": "2013-01-25T00:00:00",
        "last_modified_date": "2013-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.5946",
        "title": "Computer Poker Research at LIACC",
        "authors": [
            "Lu\u00eds Filipe Te\u00f3filo",
            "Lu\u00eds Paulo Reis",
            "Henrique Lopes Cardoso",
            "Dinis F\u00e9lix",
            "Rui S\u00eaca",
            "Jo\u00e3o Ferreira",
            "Pedro Mendes",
            "Nuno Cruz",
            "Vitor Pereira",
            "Nuno Passos"
        ],
        "abstract": "Computer Poker's unique characteristics present a well-suited challenge for research in artificial intelligence. For that reason, and due to the Poker's market increase in popularity in Portugal since 2008, several members of LIACC have researched in this field. Several works were published as papers and master theses and more recently a member of LIACC engaged on a research in this area as a Ph.D. thesis in order to develop a more extensive and in-depth work. This paper describes the existing research in LIACC about Computer Poker, with special emphasis on the completed master's theses and plans for future work. This paper means to present a summary of the lab's work to the research community in order to encourage the exchange of ideas with other labs / individuals. LIACC hopes this will improve research in this area so as to reach the goal of creating an agent that surpasses the best human players.\n    ",
        "submission_date": "2013-01-25T00:00:00",
        "last_modified_date": "2013-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6011",
        "title": "A Framework for Intelligent Medical Diagnosis using Rough Set with Formal Concept Analysis",
        "authors": [
            "B.K.Tripathy",
            "D.P.Acharjya",
            "V.Cynthya"
        ],
        "abstract": "Medical diagnosis process vary in the degree to which they attempt to deal with different complicating aspects of diagnosis such as relative importance of symptoms, varied symptom pattern and the relation between diseases them selves. Based on decision theory, in the past many mathematical models such as crisp set, probability distribution, fuzzy set, intuitionistic fuzzy set were developed to deal with complicating aspects of diagnosis. But, many such models are failed to include important aspects of the expert decisions. Therefore, an effort has been made to process inconsistencies in data being considered by Pawlak with the introduction of rough set theory. Though rough set has major advantages over the other methods, but it generates too many rules that create many difficulties while taking decisions. Therefore, it is essential to minimize the decision rules. In this paper, we use two processes such as pre process and post process to mine suitable rules and to explore the relationship among the attributes. In pre process we use rough set theory to mine suitable rules, whereas in post process we use formal concept analysis from these suitable rules to explore better knowledge and most important factors affecting the decision making.\n    ",
        "submission_date": "2013-01-25T00:00:00",
        "last_modified_date": "2013-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6039",
        "title": "Recycling Proof Patterns in Coq: Case Studies",
        "authors": [
            "J\u00f3nathan Heras",
            "Ekaterina Komendantskaya"
        ],
        "abstract": "Development of Interactive Theorem Provers has led to the creation of big libraries and varied infrastructures for formal proofs. However, despite (or perhaps due to) their sophistication, the re-use of libraries by non-experts or across domains is a challenge. In this paper, we provide detailed case studies and evaluate the machine-learning tool ML4PG built to interactively data-mine the electronic libraries of proofs, and to provide user guidance on the basis of proof patterns found in the existing libraries.\n    ",
        "submission_date": "2013-01-25T00:00:00",
        "last_modified_date": "2014-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6262",
        "title": "Developing Parallel Dependency Graph In Improving Game Balancing",
        "authors": [
            "Sim-Hui Tee"
        ],
        "abstract": "The dependency graph is a data architecture that models all the dependencies between the different types of assets in the game. It depicts the dependency-based relationships between the assets of a game. For example, a player must construct an arsenal before he can build weapons. It is vital that the dependency graph of a game is designed logically to ensure a logical sequence of game play. However, a mere logical dependency graph is not sufficient in sustaining the players' enduring interests in a game, which brings the problem of game balancing into picture. The issue of game balancing arises when the players do not feel the chances of winning the game over their AI opponents who are more skillful in the game play. At the current state of research, the architecture of dependency graph is monolithic for the players. The sequence of asset possession is always foreseeable because there is only a single dependency graph. Game balancing is impossible when the assets of AI players are overwhelmingly outnumbering that of human players. This paper proposes a parallel architecture of dependency graph for the AI players and human players. Instead of having a single dependency graph, a parallel architecture is proposed where the dependency graph of AI player is adjustable with that of human player using a support dependency as a game balancing mechanism. This paper exhibits that the parallel dependency graph helps to improve game balancing.\n    ",
        "submission_date": "2013-01-26T00:00:00",
        "last_modified_date": "2013-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6359",
        "title": "Subjective Reality and Strong Artificial Intelligence",
        "authors": [
            "Alexander Serov"
        ],
        "abstract": "The main prospective aim of modern research related to Artificial Intelligence is the creation of technical systems that implement the idea of Strong Intelligence. According our point of view the path to the development of such systems comes through the research in the field related to perceptions. Here we formulate the model of the perception of external world which may be used for the description of perceptual activity of intelligent beings. We consider a number of issues related to the development of the set of patterns which will be used by the intelligent system when interacting with environment. The key idea of the presented perception model is the idea of subjective reality. The principle of the relativity of perceived world is formulated. It is shown that this principle is the immediate consequence of the idea of subjective reality. In this paper we show how the methodology of subjective reality may be used for the creation of different types of Strong AI systems.\n    ",
        "submission_date": "2013-01-27T00:00:00",
        "last_modified_date": "2013-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6675",
        "title": "A Temporal Bayesian Network for Diagnosis and Prediction",
        "authors": [
            "Gustavo Arroyo-Figueroa",
            "Luis Enrique Sucar"
        ],
        "abstract": "Diagnosis and prediction in some domains, like medical and industrial diagnosis, require a representation that combines uncertainty management and temporal reasoning. Based on the fact that in many cases there are few state changes in the temporal range of interest, we propose a novel representation called Temporal Nodes Bayesian Networks (TNBN). In a TNBN  each node represents an event or state change of a variable, and an arc corresponds to a causal-temporal relationship. The temporal intervals can differ in number and size for each temporal node, so this allows multiple granularity. Our approach is contrasted with a dynamic Bayesian network for a simple medical example. An empirical evaluation is presented for a more complex problem, a subsystem of a fossil power plant, in which this approach is used for fault diagnosis and prediction with good results.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6679",
        "title": "Possibilistic logic bases and possibilistic graphs",
        "authors": [
            "Salem Benferhat",
            "Didier Dubois",
            "Laurent Garcia",
            "Henri Prade"
        ],
        "abstract": "Possibilistic logic bases and possibilistic graphs are two different frameworks of interest for representing knowledge. The former stratifies the pieces of knowledge (expressed by logical formulas) according to their level of certainty, while the latter exhibits relationships between variables. The two types of representations are semantically equivalent when they lead to the same possibility distribution (which rank-orders the possible interpretations). A possibility distribution can be decomposed using a chain rule which may be based on two different kinds of conditioning which exist in possibility theory (one based on product in a numerical setting, one based on minimum operation in a qualitative setting). These two types of conditioning induce two kinds of possibilistic graphs. In both cases, a translation of these graphs into possibilistic bases is provided. The converse translation from a possibilistic knowledge base into a min-based graph is also described.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6680",
        "title": "Artificial Decision Making Under Uncertainty in Intelligent Buildings",
        "authors": [
            "Magnus Boman",
            "Paul Davidsson",
            "Hakan L. Younes"
        ],
        "abstract": "Our hypothesis is that by equipping certain agents in a multi-agent system controlling an intelligent building with automated decision support, two important factors will be increased. The first is energy saving in the building. The second is customer value---how the people in the building experience the effects of the actions of the agents. We give evidence for the truth of this hypothesis through experimental findings related to tools for artificial decision making. A number of assumptions related to agent control, through monitoring and delegation of tasks to other kinds of agents, of rooms at a test site are relaxed. Each assumption controls at least one uncertainty that complicates considerably the procedures for selecting actions part of each such agent. We show that in realistic decision situations, room-controlling agents can make bounded rational decisions even under dynamic real-time constraints. This result can be, and has been, generalized to other domains with even harsher time constraints.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6681",
        "title": "Reasoning With Conditional Ceteris Paribus Preference Statem",
        "authors": [
            "Craig Boutilier",
            "Ronen I. Brafman",
            "Holger H. Hoos",
            "David L. Poole"
        ],
        "abstract": "In many domains it is desirable to assess the preferences of users in a qualitative rather than quantitative way. Such representations of qualitative preference orderings form an importnat component of automated decision tools. We propose a graphical representation of preferences that reflects conditional dependence and independence of preference statements under a ceteris paribus (all else being equal) interpretation. Such a representation is ofetn compact and arguably natural. We describe several search algorithms for dominance testing based on this representation; these algorithms are quite effective, especially in specific network topologies, such as chain-and tree- structured networks, as well as polytrees.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6682",
        "title": "Continuous Value Function Approximation for Sequential Bidding Policies",
        "authors": [
            "Craig Boutilier",
            "Moises Goldszmidt",
            "Bikash Sabata"
        ],
        "abstract": "Market-based mechanisms such as auctions are being studied as an appropriate means for resource allocation in distributed and mulitagent decision problems.  When agents value resources in combination rather than in isolation, they must often deliberate about appropriate bidding strategies for a sequence of auctions offering resources of interest.  We briefly describe a discrete dynamic programming model for constructing appropriate bidding policies for resources exhibiting both complementarities and substitutability.  We then introduce a continuous approximation of this model, assuming that money (or the numeraire good) is infinitely divisible.  Though this has the potential to reduce the computational cost of computing policies, value functions in the transformed problem do not have a convenient closed form representation.  We develop {em grid-based} approximation for such value functions, representing value functions using piecewise linear approximations.  We show that these methods can offer significant computational savings with relatively small cost in solution quality.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6683",
        "title": "Discovering the Hidden Structure of Complex Dynamic Systems",
        "authors": [
            "Xavier Boyen",
            "Nir Friedman",
            "Daphne Koller"
        ],
        "abstract": "Dynamic Bayesian networks provide a compact and natural representation for complex dynamic systems. However, in many cases, there is no expert available from whom a model can be elicited. Learning provides an alternative approach for constructing models of dynamic systems. In this paper, we address some of the crucial computational aspects of learning the structure of dynamic systems, particularly those where some relevant variables are partially observed or even entirely unknown. Our approach is based on the Structural Expectation Maximization (SEM) algorithm. The main computational cost of the SEM algorithm is the gathering of expected sufficient statistics. We propose a novel approximation scheme that allows these sufficient statistics to be computed efficiently. We also investigate the fundamental problem of discovering the existence of hidden variables without exhaustive and expensive search. Our approach is based on the observation that, in dynamic systems, ignoring a hidden variable typically results in a violation of the Markov property. Thus, our algorithm searches for such violations in the data, and introduces hidden variables to explain them. We provide empirical results showing that the algorithm is able to learn the dynamics of complex systems in a computationally tractable way.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6686",
        "title": "Causal Discovery from a Mixture of Experimental and Observational Data",
        "authors": [
            "Gregory F. Cooper",
            "Changwon Yoo"
        ],
        "abstract": "This paper describes a Bayesian method for combining an arbitrary mixture of observational and experimental data in order to learn causal Bayesian networks. Observational data are passively observed. Experimental data, such as that produced by randomized controlled trials, result from the experimenter manipulating one or more variables (typically randomly) and observing the states of other variables. The paper presents a Bayesian method for learning the causal structure and parameters of the underlying causal process that is generating the data, given that (1) the data contains a mixture of observational and experimental case records, and (2) the causal process is modeled as a causal Bayesian network. This learning  method was applied using as input various mixtures of experimental and  observational data that were generated from the ALARM causal Bayesian network. In these experiments, the absolute and relative quantities of experimental and observational data were varied systematically. For each of these training datasets, the learning method was applied to predict the causal structure and to estimate the causal parameters that exist among randomly selected pairs of nodes in ALARM that are not confounded. The paper reports how these structure predictions and parameter estimates compare with the true causal structures and parameters as given by the ALARM network.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6687",
        "title": "Loglinear models for first-order probabilistic reasoning",
        "authors": [
            "James Cussens"
        ],
        "abstract": "Recent work on loglinear models in probabilistic constraint logic programming is applied to first-order probabilistic reasoning. Probabilities are defined directly on the proofs of atomic formulae, and by marginalisation on the atomic formulae themselves. We use Stochastic Logic Programs (SLPs) composed of labelled and unlabelled definite clauses to define the proof probabilities. We have a conservative extension of first-order reasoning, so that, for example, there is a one-one mapping between logical and random variables. We show how, in this framework, Inductive Logic Programming (ILP) can be used to induce the features of a loglinear model from data. We also compare the presented framework with other approaches to first-order probabilistic reasoning.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6688",
        "title": "Learning Polytrees",
        "authors": [
            "Sanjoy Dasgupta"
        ],
        "abstract": "We consider the task of learning the maximum-likelihood polytree from data. Our first result is a performance guarantee establishing that the optimal branching (or Chow-Liu tree), which can be computed very easily, constitutes a good approximation to the best polytree. We then show that it is not possible to do very much better, since the learning problem is NP-hard even to approximately solve within some constant factor.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6689",
        "title": "A Hybrid Anytime Algorithm for the Constructiion of Causal Models From Sparse Data",
        "authors": [
            "Denver Dash",
            "Marek J. Druzdzel"
        ],
        "abstract": "We present a hybrid constraint-based/Bayesian algorithm for learning causal networks in the presence of sparse data.  The algorithm searches the space of equivalence classes of models (essential graphs) using a heuristic based on conventional constraint-based techniques. Each essential graph is then converted into a directed acyclic graph and scored using a Bayesian scoring metric.  Two variants of the algorithm are developed and tested using data from randomly generated networks of sizes from 15 to 45 nodes with data sizes ranging from 250 to 2000 records. Both variations are compared to, and found to consistently outperform two variations of greedy search with restarts.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6690",
        "title": "Model-Based Bayesian Exploration",
        "authors": [
            "Richard Dearden",
            "Nir Friedman",
            "David Andre"
        ],
        "abstract": "Reinforcement learning systems are often concerned with balancing exploration of untested actions against exploitation of actions that are known to be good. The benefit of exploration can be estimated using the classical notion of Value of Information - the expected improvement in future decision quality arising from the information acquired by exploration. Estimating this quantity requires an assessment of the agent's uncertainty about its current value estimates for states. In this paper we investigate ways of representing and reasoning about this uncertainty in algorithms where the system attempts to learn a model of its environment. We explicitly represent uncertainty about the parameters of the model and build probability distributions over Q-values based on these. These distributions are used to compute a myopic approximation to the value of information for each action and hence to select the action that best balances exploration and exploitation.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6691",
        "title": "Hybrid Probabilistic Programs: Algorithms and Complexity",
        "authors": [
            "Michael I. Dekhtyar",
            "Alex Dekhtyar",
            "V. S. Subrahmanian"
        ],
        "abstract": "Hybrid Probabilistic Programs (HPPs) are logic programs that allow the programmer to explicitly encode his knowledge of the dependencies between events being described in the program.   In this paper, we classify HPPs into three classes called HPP_1,HPP_2 and HPP_r,r>= 3. For these classes, we provide three types of results for HPPs.  First, we develop algorithms to compute the set of all ground consequences of an HPP. Then we provide algorithms and complexity results for the problems of entailment (\"Given an HPP P and a query Q as input, is Q a logical consequence of P?\") and consistency (\"Given an HPP P as input, is P consistent?\").  Our results provide a fine characterization of when polynomial algorithms exist for the above problems, and when these problems become intractable.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6692",
        "title": "Assessing the value of a candidate. Comparing belief function and possibility theories",
        "authors": [
            "Didier Dubois",
            "Michel Grabisch",
            "Henri Prade",
            "Philippe Smets"
        ],
        "abstract": "The problem of assessing the value of a candidate is viewed here as a multiple combination problem. On the one hand a candidate can be evaluated according to different criteria, and on the other hand several experts are supposed to assess the value of candidates according to each criterion. Criteria are not equally important, experts are not equally competent or reliable. Moreover levels of satisfaction of criteria, or levels of confidence are only assumed to take their values in qualitative scales which are just linearly ordered. The problem is discussed within two frameworks, the transferable belief model and the qualitative possibility theory. They respectively offer a quantitative and a qualitative setting for handling the problem, providing thus a way to compare the nature of the underlying assumptions.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6694",
        "title": "Qualitative Models for Decision Under Uncertainty without the Commensurability Assumption",
        "authors": [
            "Helene Fargier",
            "Patrice Perny"
        ],
        "abstract": "This paper investigates a purely qualitative version of Savage's theory for decision making under uncertainty. Until now, most representation theorems for preference over acts rely on a numerical representation of utility and uncertainty where utility and uncertainty are commensurate. Disrupting the tradition, we relax this assumption and introduce a purely ordinal axiom requiring that the Decision Maker (DM) preference between two acts only depends on the relative position of their consequences for each state. Within this qualitative framework, we determine the only possible form of the decision rule and investigate some instances compatible with the transitivity of the strict preference. Finally we propose a mild relaxation of our ordinality axiom, leaving room for a new family of qualitative decision rules compatible with transitivity.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6698",
        "title": "Quantifier Elimination for Statistical Problems",
        "authors": [
            "Dan Geiger",
            "Christopher Meek"
        ],
        "abstract": "Recent improvement on Tarski's procedure for quantifier elimination in the first order theory of real numbers makes it feasible to solve small instances of the following problems completely automatically: 1. listing all equality and inequality constraints implied by a graphical model with hidden variables. 2. Comparing graphyical models with hidden variables (i.e., model equivalence, inclusion, and overlap). 3. Answering questions about the identification of a model or portion of a model, and about bounds on quantities derived from a model. 4. Determing whether a given set of independence assertions. We discuss the foundation of quantifier elimination and demonstrate its application to these problems.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6699",
        "title": "On Transformations between Probability and Spohnian Disbelief Functions",
        "authors": [
            "Phan H. Giang",
            "Prakash P. Shenoy"
        ],
        "abstract": "In this paper, we analyze the relationship between probability and Spohn's theory for representation of uncertain beliefs.  Using the intuitive idea that the more probable a proposition is, the more believable it is, we study transformations from probability to Sphonian disbelief and vice-versa.  The transformations described in this paper are different from those described in the literature.  In particular, the former satisfies the principles of ordinal congruence while the latter does not.  Such transformations between probability and Spohn's calculi can contribute to (1) a clarification of the semantics of nonprobabilistic degree of uncertain belief, and (2) to a construction of a decision theory for such calculi.  In practice, the transformations will allow a meaningful combination of more than one calculus in different stages of using an expert system such as knowledge acquisition, inference, and interpretation of results.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6700",
        "title": "A New Model of Plan Recognition",
        "authors": [
            "Robert P. Goldman",
            "Christopher W. Geib",
            "Christopher A. Miller"
        ],
        "abstract": "We present a new abductive, probabilistic theory of plan recognition. This model differs from previous plan recognition theories in being centered around a model of plan execution: most previous methods have been based on plans as formal objects or on rules describing the recognition process. We show that our new model accounts for phenomena omitted from most previous plan recognition theories: notably the cumulative effect of a sequence of observations of partially-ordered, interleaved plans and the effect of context on plan adoption.  The model also supports inferences about the evolution of plan execution in situations where another agent intervenes in plan execution.  This facility provides support for using plan recognition to build systems that will intelligently assist a user.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6701",
        "title": "Multi-objects association in perception of dynamical situation",
        "authors": [
            "Dominique Gruyer",
            "Veronique Berge-Cherfaoui"
        ],
        "abstract": "In current perception systems applied to the rebuilding of the environment for intelligent vehicles, the part reserved to object association for the tracking is increasingly significant. This allows firstly to follow the objects temporal evolution and secondly to increase the reliability of environment perception. We propose in this communication the development of a multi-objects association algorithm with ambiguity removal entering into the design of such a dynamic perception system for intelligent vehicles. This algorithm uses the belief theory and data modelling with fuzzy mathematics in order to be able to handle inaccurate as well as uncertain information due to imperfect sensors. These theories also allow the fusion of numerical as well as symbolic data. We develop in this article the problem of matching between known and perceived objects. This makes it possible to update a dynamic environment map for a vehicle. The belief theory will enable us to quantify the belief in the association of each perceived object with each known object. Conflicts can appear in the case of object appearance or disappearance, or in the case of a confused situation or bad perception. These conflicts are removed or solved using an assignment algorithm, giving a solution called the \" best \" and so ensuring the tracking of some objects present in our environment.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6702",
        "title": "A Hybrid Approach to Reasoning with Partially Elicited Preference Models",
        "authors": [
            "Vu A. Ha",
            "Peter Haddawy"
        ],
        "abstract": "Classical Decision Theory provides a normative framework for representing and reasoning about complex preferences. Straightforward application of this theory to automate decision making is difficult due to high elicitation cost. In response to this problem, researchers have recently developed a number of qualitative, logic-oriented approaches for representing and reasoning about references. While effectively addressing some expressiveness issues, these logics have not proven powerful enough for building practical automated decision making systems. In this paper we present a hybrid approach to preference elicitation and decision making that is grounded in classical multi-attribute utility theory, but can make effective use of the expressive power of qualitative approaches.  Specifically, assuming a partially specified multilinear utility function, we show how comparative statements about classes of decision alternatives can be used to further constrain the utility function and thus identify sup-optimal alternatives. This work demonstrates that quantitative and qualitative approaches can be synergistically integrated to provide effective and flexible decision support.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6703",
        "title": "Faithful Approximations of Belief Functions",
        "authors": [
            "David Harmanec"
        ],
        "abstract": "A conceptual foundation for approximation of belief functions is proposed and investigated. It is based on the requirements of consistency and closeness. An optimal approximation is studied. Unfortunately, the computation of the optimal approximation turns out to be intractable. Hence, various heuristic methods are proposed and experimantally evaluated both in terms of their accuracy and in terms of the speed of computation. These methods are compared to the earlier proposed approximations of belief functions.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6704",
        "title": "SPUDD: Stochastic Planning using Decision Diagrams",
        "authors": [
            "Jesse Hoey",
            "Robert St-Aubin",
            "Alan Hu",
            "Craig Boutilier"
        ],
        "abstract": "Markov decisions processes (MDPs) are becoming increasing popular as models of decision theoretic planning. While traditional dynamic programming methods perform well for problems with small state spaces, structured methods are needed for large problems. We propose and examine a value iteration algorithm for MDPs that uses algebraic decision diagrams(ADDs) to represent value functions and policies. An MDP is represented using Bayesian networks and ADDs and dynamic programming is applied directly to these ADDs. We demonstrate our method on large MDPs (up to 63 million states) and show that significant gains can be had when compared to tree-structured representations (with up to a thirty-fold reduction in the number of nodes required to represent optimal value functions).\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6706",
        "title": "Estimating the Value of Computation in Flexible Information Refinement",
        "authors": [
            "Michael C. Horsch",
            "David L. Poole"
        ],
        "abstract": "We outline a method to estimate the value of computation for a flexible algorithm using empirical data. To determine a reasonable trade-off between cost and value, we build an empirical model of the value obtained through computation, and apply this model to estimate the value of computation for quite different problems. In particular, we investigate this trade-off for the problem of constructing policies for decision problems represented as influence diagrams. We show how two features of our anytime algorithm provide reasonable estimates of the value of computation in this domain.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6707",
        "title": "Attention-Sensitive Alerting",
        "authors": [
            "Eric J. Horvitz",
            "Andy Jacobs",
            "David Hovel"
        ],
        "abstract": "We introduce utility-directed procedures for mediating the flow of potentially distracting alerts and communications to computer users. We present models and inference procedures that balance the context-sensitive costs of deferring alerts with the cost of interruption. We describe the challenge of reasoning about such costs under uncertainty via an analysis of user activity and the content of notifications. After introducing principles of attention-sensitive alerting, we focus on the problem of guiding alerts about email messages. We dwell on the problem of inferring the expected criticality of email and discuss work on the Priorities system, centering on prioritizing email by criticality and modulating the communication of notifications to users about the presence and nature of incoming email. \n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6708",
        "title": "Mini-Bucket Heuristics for Improved Search",
        "authors": [
            "Kalev Kask",
            "Rina Dechter"
        ],
        "abstract": "The paper is a second in a series of two papers evaluating the power of a new scheme that generates search heuristics mechanically. The heuristics are extracted from an approximation scheme called mini-bucket elimination that was recently introduced. The first paper introduced the idea and evaluated it within Branch-and-Bound search. In the current paper the idea is further extended and evaluated within Best-First search. The resulting algorithms are compared on coding and medical diagnosis problems, using varying strength of the mini-bucket heuristics.\n",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6709",
        "title": "A General Algorithm for Approximate Inference and its Application to Hybrid Bayes Nets",
        "authors": [
            "Daphne Koller",
            "Uri Lerner",
            "Dragomir Anguelov"
        ],
        "abstract": "The clique tree algorithm is the standard method for doing  inference in Bayesian networks. It works by manipulating clique potentials - distributions over the variables in a clique. While this approach works well for many networks, it is limited by the need to maintain an exact representation of the clique potentials. This paper presents a new unified approach that combines approximate inference and the clique tree algorithm, thereby circumventing this limitation. Many known approximate inference algorithms can be viewed as instances of this approach. The algorithm essentially does clique tree propagation, using approximate inference to estimate the densities in each clique. In many settings, the computation of the approximate clique potential can be done easily using statistical importance sampling. Iterations are used to gradually improve the quality of the estimation.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6711",
        "title": "Bayesian Poker",
        "authors": [
            "Kevin B. Korb",
            "Ann Nicholson",
            "Nathalie Jitnah"
        ],
        "abstract": "Poker is ideal for testing automated reasoning under uncertainty.  It introduces uncertainty both by physical randomization and by incomplete information about opponents ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6712",
        "title": "On Quantified Linguistic Approximation",
        "authors": [
            "Ryszard Kowalczyk"
        ],
        "abstract": "Most fuzzy systems including fuzzy decision support and fuzzy control systems provide out-puts in the form of fuzzy sets that represent the inferred conclusions. Linguistic interpretation of such outputs often involves the use of linguistic approximation that assigns a linguistic label to a fuzzy set based on the predefined primary terms, linguistic modifiers and linguistic connectives. More generally, linguistic approximation can be formalized in the terms of the re-translation rules that correspond to the translation rules in ex-plicitation (e.g. simple, modifier, composite, quantification and qualification rules) in com-puting with words [Zadeh 1996]. However most existing methods of linguistic approximation use the simple, modifier and composite re-translation rules only. Although these methods can provide a sufficient approximation of simple fuzzy sets the approximation of more complex ones that are typical in many practical applications of fuzzy systems may be less satisfactory. Therefore the question arises why not use in linguistic ap-proximation also other re-translation rules corre-sponding to the translation rules in explicitation to advantage. In particular linguistic quantifica-tion may be desirable in situations where the conclusions interpreted as quantified linguistic propositions can be more informative and natu-ral. This paper presents some aspects of linguis-tic approximation in the context of the re-translation rules and proposes an approach to linguistic approximation with the use of quantifi-cation rules, i.e. quantified linguistic approxima-tion. Two methods of the quantified linguistic approximation are considered with the use of lin-guistic quantifiers based on the concepts of the non-fuzzy and fuzzy cardinalities of fuzzy sets. A number of examples are provided to illustrate the proposed approach.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6713",
        "title": "Choosing Among Interpretations of Probability",
        "authors": [
            "Henry E. Kyburg Jr.",
            "Choh Man Teng"
        ],
        "abstract": "There is available an ever-increasing variety of procedures for managing uncertainty.  These methods are discussed in the literature of artificial intelligence, as well as in the literature of philosophy of science.  Heretofore these methods have been evaluated by intuition, discussion, and the general philosophical method of argument and counterexample.  Almost any method of uncertainty management will have the property that in the long run it will deliver numbers approaching the relative frequency of the kinds of events at issue.  To find a measure that will provide a meaningful evaluation of these treatments of uncertainty, we must look, not at the long run, but at the short or intermediate run.  Our project attempts to develop such a measure in terms of short or intermediate length performance. We represent the effects of practical choices by the outcomes of bets offered to agents characterized by two uncertainty management approaches: the subjective Bayesian approach and the Classical confidence interval approach.  Experimental evaluation suggests that the confidence interval approach can outperform the subjective approach in the relatively short run.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6715",
        "title": "My Brain is Full: When More Memory Helps",
        "authors": [
            "Christopher Lusena",
            "Tong Li",
            "Shelia Sittinger",
            "Chris Wells",
            "Judy Goldsmith"
        ],
        "abstract": "We consider the problem of finding good finite-horizon policies for POMDPs under the expected reward metric.  The policies considered are {em free finite-memory policies with limited memory}; a policy is a mapping from the space of observation-memory pairs to the space of action-memeory pairs (the policy updates the memory as it goes), and the number of possible memory states is a parameter of the input to the policy-finding algorithms.  The algorithms considered here are preliminary implementations of three search heuristics:  local search, simulated annealing, and genetic algorithms.  We compare their outcomes to each other and to the optimal policies for each instance.  We compare run times of each policy and of a dynamic programming algorithm for POMDPs developed by Hansen that iteratively improves a finite-state controller --- the previous state of the art for finite memory policies.  The value of the best policy can only improve as the amount of memory increases, up to the amount needed for an optimal finite-memory policy.  Our most surprising finding is that more memory helps in another way: given more memory than is needed for an optimal policy, the algorithms are more likely to converge to optimal-valued policies.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6716",
        "title": "Lazy Evaluation of Symmetric Bayesian Decision Problems",
        "authors": [
            "Anders L. Madsen",
            "Finn Verner Jensen"
        ],
        "abstract": "Solving symmetric Bayesian decision problems is a computationally intensive task to perform regardless of the algorithm used. In this paper we propose a method for improving the efficiency of algorithms for solving Bayesian decision problems. The method is based on the principle of lazy evaluation - a principle recently shown to improve the efficiency of inference in Bayesian networks. The basic idea is to maintain decompositions of potentials and to postpone computations for as long as possible.  The efficiency improvements obtained with the lazy evaluation based method is emphasized through examples. Finally, the lazy evaluation based method is compared with the hugin and valuation-based systems architectures for solving symmetric Bayesian decision problems.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6717",
        "title": "Representing and Combining Partially Specified CPTs",
        "authors": [
            "Suzanne M. Mahoney",
            "Kathryn Blackmond Laskey"
        ],
        "abstract": "This paper extends previous work with network fragments and situation-specific network construction. We formally define the asymmetry network, an alternative representation for a conditional probability table. We also present an object-oriented representation for partially specified asymmetry networks. We show that the representation is parsimonious. We define an algebra for the elements of the representation that allows us to 'factor' any CPT and to soundly combine the partially specified asymmetry networks.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6718",
        "title": "On the Complexity of Policy Iteration",
        "authors": [
            "Yishay Mansour",
            "Satinder Singh"
        ],
        "abstract": "Decision-making problems in uncertain or stochastic domains are often formulated as Markov decision processes (MDPs). Policy iteration (PI) is a popular algorithm for searching over policy-space, the size of which is exponential in the number of states. We are interested in bounds on the complexity of PI that do not depend on the value of the discount factor. In this paper we prove the first such non-trivial, worst-case, upper bounds on the number of iterations required by PI to converge to the optimal policy. Our analysis also sheds new light on the manner in which PI progresses through the space of policies.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6719",
        "title": "Approximate Planning for Factored POMDPs using Belief State Simplification",
        "authors": [
            "David A. McAllester",
            "Satinder Singh"
        ],
        "abstract": "We are interested in the problem of planning for factored POMDPs.  Building on the recent results of Kearns, Mansour and Ng, we provide a planning algorithm for factored POMDPs that exploits the accuracy-efficiency tradeoff in the belief state simplification introduced by Boyen and Koller.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6720",
        "title": "Solving POMDPs by Searching the Space of Finite Policies",
        "authors": [
            "Nicolas Meuleau",
            "Kee-Eung Kim",
            "Leslie Pack Kaelbling",
            "Anthony R. Cassandra"
        ],
        "abstract": "Solving partially observable Markov decision processes (POMDPs) is highly intractable in general, at least in part because the optimal policy may be infinitely large. In this paper, we explore the problem of finding the optimal policy from a restricted set of policies, represented as finite state automata of a given size. This problem is also intractable, but we show that the complexity can be greatly reduced when the POMDP and/or policy are further constrained. We demonstrate good empirical results with a branch-and-bound method for finding globally optimal deterministic policies, and a gradient-ascent method for finding locally optimal stochastic policies.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6721",
        "title": "Learning Finite-State Controllers for Partially Observable Environments",
        "authors": [
            "Nicolas Meuleau",
            "Leonid Peshkin",
            "Kee-Eung Kim",
            "Leslie Pack Kaelbling"
        ],
        "abstract": "Reactive (memoryless) policies are sufficient in completely observable Markov decision processes (MDPs), but some kind of memory is usually necessary for optimal control of a partially observable MDP. Policies with finite memory can be represented as finite-state automata. In this paper, we extend Baird and Moore's VAPS algorithm to the problem of learning general finite-state automata. Because it performs stochastic gradient descent, this algorithm can be shown to converge to a locally optimal finite-state controller. We provide the details of the algorithm and then consider the question of under what conditions stochastic gradient descent will outperform exact gradient descent. We conclude with empirical results comparing the performance of stochastic and exact gradient descent, and showing the ability of our algorithm to extract the useful information contained in the sequence of past observations to compensate for the lack of observability at each time-step.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6722",
        "title": "Bayes Nets in Educational Assessment: Where Do the Numbers Come From?",
        "authors": [
            "Robert Mislevy",
            "Russell Almond",
            "Duanli Yan",
            "Linda S. Steinberg"
        ],
        "abstract": "As observations and student models become complex, educational assessments that exploit advances in technology and cognitive psychology can outstrip familiar testing models and analytic methods. Within the Portal conceptual framework for assessment design, Bayesian inference networks (BINs) record beliefs about students' knowledge and skills, in light of what they say and do.  Joining evidence model BIN fragments- which contain observable variables and pointers to student model variables - to the student model allows one to update belief about knowledge and skills as observations arrive.  Markov Chain Monte Carlo (MCMC) techniques can estimate the required conditional probabilities from empirical data, supplemented by expert judgment or substantive theory.  Details for the special cases of item response theory (IRT) and multivariate latent class modeling are given, with a numerical example of the latter.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6724",
        "title": "A Variational Approximation for Bayesian Networks with Discrete and Continuous Latent Variables",
        "authors": [
            "Kevin Murphy"
        ],
        "abstract": "We show how to use a variational approximation to the logistic function to perform approximate inference in Bayesian networks containing discrete nodes with continuous parents. Essentially, we convert the logistic function to a Gaussian, which facilitates exact inference, and then iteratively adjust the variational parameters to improve the quality of the approximation. We demonstrate experimentally that this approximation is faster and potentially more accurate than sampling. We also introduce a simple new technique for handling evidence, which allows us to handle arbitrary distributions on observed nodes, as well as achieving a significant speedup in networks with discrete variables of large cardinality.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6725",
        "title": "Loopy Belief Propagation for Approximate Inference: An Empirical Study",
        "authors": [
            "Kevin Murphy",
            "Yair Weiss",
            "Michael I. Jordan"
        ],
        "abstract": "Recently, researchers have demonstrated that loopy belief propagation - the use of Pearls polytree algorithm IN a Bayesian network WITH loops OF error- correcting ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6726",
        "title": "Learning Bayesian Networks from Incomplete Data with Stochastic Search Algorithms",
        "authors": [
            "James W. Myers",
            "Kathryn Blackmond Laskey",
            "Tod S. Levitt"
        ],
        "abstract": "This paper describes stochastic search approaches, including a new stochastic algorithm and an adaptive mutation operator, for learning Bayesian networks from incomplete data.  This problem is characterized by a huge solution space with a highly multimodal landscape.  State-of-the-art approaches all involve using deterministic approaches such as the expectation-maximization algorithm.  These approaches are guaranteed to find local maxima, but do not explore the landscape for other modes.  Our approach evolves structure and the missing data.  We compare our stochastic algorithms and show they all produce accurate results.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6727",
        "title": "Learning Bayesian Networks with Restricted Causal Interactions",
        "authors": [
            "Julian R. Neil",
            "Chris S. Wallace",
            "Kevin B. Korb"
        ],
        "abstract": "A major problem for the learning of Bayesian networks (BNs) is the exponential number of parameters needed for conditional probability tables.  Recent research reduces this complexity by modeling local structure in the probability tables.  We examine the use of log-linear local models.  While log-linear models in this context are not new (Whittaker, 1990; Buntine, 1991; Neal, 1992; Heckerman and Meek, 1997), for structure learning they are generally subsumed under a naive Bayes model.  We describe an alternative interpretation, and use a Minimum Message Length (MML) (Wallace, 1987) metric for structure learning of networks exhibiting causal independence, which we term first-order networks (FONs).  We also investigate local model selection on a node-by-node basis.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6729",
        "title": "Welldefined Decision Scenarios",
        "authors": [
            "Thomas D. Nielsen",
            "Finn Verner Jensen"
        ],
        "abstract": "Influence diagrams serve as a powerful tool for modelling symmetric decision problems. When solving an influence diagram we determine a set of strategies for the decisions involved. A strategy for a decision variable is in principle a function over its past. However, some of the past may be irrelevant for the decision, and for computational reasons it is important not to deal with redundant variables in the strategies. We show that current methods (e.g. the \"Decision Bayes-ball\" algorithm by Shachter UAI98) do not determine the relevant past, and we present a complete algorithm.\n",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6732",
        "title": "Graphical Representations of Consensus Belief",
        "authors": [
            "David M. Pennock",
            "Michael P. Wellman"
        ],
        "abstract": "Graphical models based on conditional independence support concise encodings of the subjective belief of a single agent. A natural question is whether the consensus belief of a group of agents can be represented with equal parsimony. We prove, under relatively mild assumptions, that even if everyone agrees on a common graph topology, no method of combining beliefs can maintain that structure.  Even weaker conditions rule out local aggregation within conditional probability tables. On a more positive note, we show that if probabilities are combined with the logarithmic opinion pool (LogOP), then commonly held Markov independencies are maintained. This suggests a straightforward procedure for constructing a consensus Markov network. We describe an algorithm for computing the LogOP with time complexity comparable to that of exact Bayesian inference.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6733",
        "title": "SPOOK: A System for Probabilistic Object-Oriented Knowledge Representation",
        "authors": [
            "Avi Pfeffer",
            "Daphne Koller",
            "Brian Milch",
            "Ken T. Takusagawa"
        ],
        "abstract": "In previous work, we pointed out the limitations of standard Bayesian networks as a modeling framework for large, complex domains.  We proposed a new, richly structured modeling language, {em Object-oriented Bayesian Netorks}, that we argued would be able to deal with such domains.  However, it turns out that OOBNs are not expressive enough to model many interesting aspects of complex domains: the existence of specific named objects, arbitrary relations between objects, and uncertainty over domain structure.  These aspects are crucial in real-world domains such as battlefield awareness.  In this paper, we present SPOOK, an implemented system that addresses these limitations.  SPOOK implements a more expressive language that allows it to represent the battlespace domain naturally and compactly.  We present a new inference algorithm that utilizes the model structure in a fundamental way, and show empirically that it achieves orders of magnitude speedup over existing approaches.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6734",
        "title": "Bayesian Networks for Dependability Analysis: an Application to Digital Control Reliability",
        "authors": [
            "Luigi Portinale",
            "Andrea Bobbio"
        ],
        "abstract": "Bayesian Networks (BN) provide robust probabilistic methods of reasoning under uncertainty, but despite their formal grounds are strictly based on the notion of conditional dependence, not much attention has been paid so far to their use in dependability analysis. The aim of this paper is to propose BN as a suitable tool for dependability analysis, by challenging the formalism with basic issues arising in dependability tasks. We will discuss how both modeling and analysis issues can be naturally dealt with by BN. Moreover, we will show how some limitations intrinsic to combinatorial dependability methods such as Fault Trees can be overcome using BN. This will be pursued through the study of a real-world example concerning the reliability analysis of a redundant digital Programmable Logic Controller (PLC) with majority voting 2:3\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6735",
        "title": "Enhancing QPNs for Trade-off Resolution",
        "authors": [
            "Silja Renooij",
            "Linda C. van der Gaag"
        ],
        "abstract": "Qualitative probabilistic networks have been introduced as qualitative abstractions of Bayesian belief networks. One of the major drawbacks of these qualitative networks is their coarse level of detail, which may lead to unresolved trade-offs during inference. We present an enhanced formalism for qualitative networks with a finer level of detail. An enhanced qualitative probabilistic network differs from a regular qualitative network in that it distinguishes between strong and weak influences. Enhanced qualitative probabilistic networks are purely qualitative in nature, as regular qualitative networks are, yet allow for efficiently resolving trade-offs during inference.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6736",
        "title": "A Possibilistic Model for Qualitative Sequential Decision Problems under Uncertainty in Partially Observable Environments",
        "authors": [
            "Regis Sabbadin"
        ],
        "abstract": "In this article we propose a qualitative (ordinal) counterpart for the Partially Observable Markov Decision Processes model (POMDP) in which the uncertainty, as well as the preferences of the agent, are modeled by possibility distributions. This qualitative counterpart of the POMDP model relies on a possibilistic theory of decision under uncertainty, recently developed. One advantage of such a qualitative framework is its ability to escape from the classical obstacle of stochastic POMDPs, in which even with a finite state space, the obtained belief state space of the POMDP is infinite. Instead, in the possibilistic framework even if exponentially larger than the state space, the belief state space remains finite.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6737",
        "title": "Inference Networks and the Evaluation of Evidence: Alternative Analyses",
        "authors": [
            "David A. Schum"
        ],
        "abstract": "Inference networks have a variety of important uses and are constructed by persons having quite different standpoints.  Discussed in this paper are three different but complementary methods for generating and analyzing probabilistic inference networks.  The first method, though over eighty years old, is very useful for knowledge representation in the task of constructing probabilistic arguments.  It is also useful as a heuristic device in generating new forms of evidence.  The other two methods are formally equivalent ways for combining probabilities in the analysis of inference networks.  The use of these three methods is illustrated in an analysis of a mass of evidence in a celebrated American law case.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6739",
        "title": "Efficient Value of Information Computation",
        "authors": [
            "Ross D. Shachter"
        ],
        "abstract": "One of the most useful sensitivity analysis techniques of decision analysis is the computation of value of information (or clairvoyance), the difference in value obtained by changing the decisions by which some of the uncertainties are observed.  In this paper, some simple but powerful extensions to previous algorithms are introduced which allow an efficient value of information calculation on the rooted cluster tree (or strong junction tree) used to solve the original decision problem.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6740",
        "title": "Learning Hidden Markov Models with Geometrical Constraints",
        "authors": [
            "Hagit Shatkay"
        ],
        "abstract": "Hidden Markov models (HMMs) and partially observable Markov decision processes (POMDPs) form a useful tool for modeling dynamical systems. They are particularly useful for representing environments such as road networks and office buildings, which are typical for robot navigation and planning. The work presented here is concerned with acquiring such models. We demonstrate how domain-specific information and constraints can be incorporated into the statistical estimation process, greatly improving the learned models in terms of the model quality, the number of iterations required for convergence and robustness to reduction in the amount of available data. We present new initialization heuristics which can be used even when the data suffers from cumulative rotational error, new update rules for the model parameters, as an instance of generalized EM, and a strategy for enforcing complete geometrical consistency in the model. Experimental results demonstrate the effectiveness of our approach for both simulated and real robot data, in traditionally hard-to-learn environments.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6741",
        "title": "Practical Uses of Belief Functions",
        "authors": [
            "Philippe Smets"
        ],
        "abstract": "We present examples where the use of belief functions provided sound and elegant solutions to real life problems. These are essentially characterized by  ?missing' information. The examples deal with 1) discriminant analysis using a learning set where classes are only partially known; 2) an information retrieval systems handling inter-documents relationships; 3) the combination of data from sensors competent on partially overlapping frames; 4) the determination of the number of sources in a multi-sensor environment by studying the inter-sensors contradiction.  The purpose of the paper is to report on such applications where the use of belief functions provides a convenient tool to handle ?messy' data problems.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6742",
        "title": "Multiplicative Factorization of Noisy-Max",
        "authors": [
            "Masami Takikawa",
            "Bruce D'Ambrosio"
        ],
        "abstract": "The noisy-or and its generalization noisy-max have been utilized to reduce the complexity of knowledge acquisition.  In this paper, we present a new representation of noisy-max that allows for efficient inference in general Bayesian networks.  Empirical studies show that our method is capable of computing queries in well-known large medical networks, QMR-DT and CPCS, for which no previous exact inference method has been shown to perform well.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6743",
        "title": "An Update Semantics for Defeasible Obligations",
        "authors": [
            "Leendert van der Torre",
            "Yao-Hua Tan"
        ],
        "abstract": "The deontic logic DUS is a Deontic Update Semantics for prescriptive obligations based on the update semantics of Veltman. In DUS the definition of logical validity of obligations is not based on static truth values but on dynamic action transitions. In this paper prescriptive defeasible obligations are formalized in update semantics and the diagnostic problem of defeasible deontic logic is discussed. Assume a defeasible obligation `normally A ought to be (done)' together withthe fact `A is not (done).' Is this an exception of the normality claim, or is it a violation of the obligation? In this paper we formalize the heuristic principle that it is a violation, unless there is a more specific overriding obligation. The underlying motivation from legal reasoning is that criminals should have as little opportunities as possible to excuse themselves by claiming that their behavior was exceptional rather than criminal.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6744",
        "title": "Mixture Approximations to Bayesian Networks",
        "authors": [
            "Volker Tresp",
            "Michael Haft",
            "Reimar Hofmann"
        ],
        "abstract": "Structure and parameters in a Bayesian network uniquely specify the probability distribution of the modeled domain.  The locality of both structure and probabilistic information are the great benefits of Bayesian networks and require the modeler to only specify local information.  On the other hand this locality of information might prevent the modeler - and even more any other person - from obtaining a general overview of the important relationships within the domain.  The goal of the work presented in this paper is to provide an \"alternative\" view on the knowledge encoded in a Bayesian network which might sometimes be very helpful for providing insights into the underlying domain.  The basic idea is to calculate a mixture approximation to the probability distribution represented by the Bayesian network.  The mixture component densities can be thought of as representing typical scenarios implied by the Bayesian model, providing intuition about the basic relationships.  As an additional benefit, performing inference in the approximate model is very simple and intuitive and can provide additional insights.  The computational complexity for the calculation of the mixture approximations criticaly depends on the measure which defines the distance between the probability distribution represented by the Bayesian network and the approximate distribution.  Both the KL-divergence and the backward KL-divergence lead to inefficient algorithms.  Incidentally, the latter is used in recent work on mixtures of mean field solutions to which the work presented here is closely related.  We show, however, that using a mean squared error cost function leads to update equations which can be solved using the junction tree algorithm.  We conclude that the mean squared error cost function can be used for Bayesian networks in which inference based on the junction tree is tractable.  For large networks, however, one may have to rely on mean field approximations.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6745",
        "title": "How to Elicit Many Probabilities",
        "authors": [
            "Linda C. van der Gaag",
            "Silja Renooij",
            "Cilia L. M. Witteman",
            "Berthe M. P. Aleman",
            "Babs G. Taal"
        ],
        "abstract": "In building Bayesian belief networks, the elicitation of all probabilities required can be a major obstacle. We learned the extent of this often-cited observation in the construction of the probabilistic part of a complex influence diagram in the field of cancer treatment. Based upon our negative experiences with existing methods, we designed a new method for probability elicitation from domain experts. The method combines various ideas, among which are the ideas of transcribing probabilities and of using a scale with both numerical and verbal anchors for marking assessments. In the construction of the probabilistic part of our influence diagram, the method proved to allow for the elicitation of many probabilities in little time.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6746",
        "title": "Probabilistic Belief Change: Expansion, Conditioning and Constraining",
        "authors": [
            "Frans Voorbraak"
        ],
        "abstract": "The AGM theory of belief revision has become an important paradigm for investigating rational belief changes. Unfortunately, researchers working in this paradigm have restricted much of their attention to rather simple representations of belief states, namely logically closed sets of propositional sentences. In our opinion, this has resulted in a too abstract categorisation of belief change operations: expansion, revision, or contraction. Occasionally, in the AGM paradigm, also probabilistic belief changes have been considered, and it is widely accepted that the probabilistic version of expansion is conditioning. However, we argue that it may be more correct to view conditioning and expansion as two essentially different kinds of belief change, and that what we call constraining is a better candidate for being considered probabilistic expansion.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6747",
        "title": "Bayesian Control for Concentrating Mixed Nuclear Waste",
        "authors": [
            "Robert L. Welch",
            "Clayton Smith"
        ],
        "abstract": "A control algorithm for batch processing of mixed waste is proposed based on conditional Gaussian Bayesian networks. The network is compiled during batch staging for real-time response to sensor input.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6748",
        "title": "Contextual Weak Independence in Bayesian Networks",
        "authors": [
            "Michael S. K. M. Wong",
            "C. J. Butz"
        ],
        "abstract": "It is well-known that the notion of (strong) conditional independence (CI) is too restrictive to capture independencies that only hold in certain contexts. This kind of contextual independency, called context-strong independence (CSI), can be used to facilitate the acquisition, representation, and inference of probabilistic knowledge. In this paper, we suggest the use of contextual weak independence (CWI) in Bayesian networks. It should be emphasized that the notion of CWI is a more general form of contextual independence than CSI. Furthermore, if the contextual strong independence holds for all contexts, then the notion of CSI becomes strong CI. On the other hand, if the weak contextual independence holds for all contexts, then the notion of CWI becomes weak independence (WI) nwhich is a more general noncontextual independency than strong CI. More importantly, complete axiomatizations are studied for both the class of WI and the class of CI and WI together. Finally, the interesting property of WI being a necessary and sufficient condition for ensuring consistency in granular probabilistic networks is shown.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6749",
        "title": "Inference in Multiply Sectioned Bayesian Networks with Extended Shafer-Shenoy and Lazy Propagation",
        "authors": [
            "Yanping Xiang",
            "Finn Verner Jensen"
        ],
        "abstract": "As Bayesian networks are applied to larger and more complex problem domains, search for flexible modeling and more efficient inference methods is an ongoing effort.  Multiply sectioned Bayesian networks (MSBNs) extend the HUGIN inference for Bayesian networks into a coherent framework for flexible modeling and distributed ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6750",
        "title": "Time-Critical Dynamic Decision Making",
        "authors": [
            "Yanping Xiang",
            "Kim-Leng Poh"
        ],
        "abstract": "Recent interests in dynamic decision modeling have led to the development of several representation and inference methods.  These methods however, have limited application under time critical conditions where a trade-off between model quality and computational tractability is essential. This paper presents an approach to time-critical dynamic decision modeling.  A knowledge representation and modeling method called the time-critical dynamic influence diagram is proposed.  The formalism has two forms.  The condensed form is used for modeling and model  abstraction, while the deployed form which can be converted from the condensed form is used for inference purposes. The proposed approach has the ability to represent space-temporal abstraction within the model.  A knowledge-based meta-reasoning approach is proposed for the purpose of selecting the best abstracted model that provide the optimal trade-off between model quality and model tractability.  An outline of the knowledge-based model construction algorithm is also provided.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6751",
        "title": "A Method for Speeding Up Value Iteration in Partially Observable Markov Decision Processes",
        "authors": [
            "Nevin Lianwen Zhang",
            "Stephen S. Lee",
            "Weihong Zhang"
        ],
        "abstract": "We present a technique for speeding up the convergence of  value iteration for partially observable Markov decisions processes (POMDPs). The underlying idea is similar to that behind modified policy iteration for fully observable Markov  decision processes (MDPs). The technique can be easily  incorporated into any existing POMDP value iteration algorithms. Experiments have been conducted on several test problems with one POMDP value iteration algorithm called incremental pruning. We find that the technique can make incremental pruning run several orders of magnitude faster.  \n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6789",
        "title": "Approximation of Classification and Measures of Uncertainty in Rough Set on Two Universal Sets",
        "authors": [
            "B.K.Tripathy",
            "D.P.Acharjya"
        ],
        "abstract": "The notion of rough set captures indiscernibility of elements in a set. But, in many real life situations, an information system establishes the relation between different universes. This gave the extension of rough set on single universal set to rough set on two universal sets. In this paper, we introduce approximation of classifications and measures of uncertainty basing upon rough set on two universal sets employing the knowledge due to binary relations.\n    ",
        "submission_date": "2013-01-25T00:00:00",
        "last_modified_date": "2013-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6975",
        "title": "Quantifying Morphological Computation",
        "authors": [
            "Keyan Zahedi",
            "Nihat Ay"
        ],
        "abstract": "The field of embodied intelligence emphasises the importance of the morphology and environment with respect to the behaviour of a cognitive system. The contribution of the morphology to the behaviour, commonly known as morphological computation, is well-recognised in this community. We believe that the field would benefit from a formalisation of this concept as we would like to ask how much the morphology and the environment contribute to an embodied agent's behaviour, or how an embodied agent can maximise the exploitation of its morphology within its environment. In this work we derive two concepts of measuring morphological computation, and we discuss their relation to the Information Bottleneck Method. The first concepts asks how much the world contributes to the overall behaviour and the second concept asks how much the agent's action contributes to a behaviour. Various measures are derived from the concepts and validated in two experiments which highlight their strengths and weaknesses.\n    ",
        "submission_date": "2013-01-29T00:00:00",
        "last_modified_date": "2013-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7251",
        "title": "On the Semantics and Automated Deduction for PLFC, a Logic of Possibilistic Uncertainty and Fuzziness",
        "authors": [
            "Teresa Alsinet",
            "Lluis Godo",
            "Sandra Sandri"
        ],
        "abstract": "Possibilistic logic is a well-known graded logic of uncertainty suitable to reason under incomplete information and partially inconsistent knowledge, which is built upon classical first order logic. There exists for Possibilistic logic a proof procedure based on a refutation complete resolution-style calculus. Recently, a syntactical extension of first order Possibilistic logic (called PLFC) dealing with fuzzy constants and fuzzily restricted quantifiers has been proposed. Our aim is to present steps towards both the formalization of PLFC itself and an automated deduction system for it by (i) providing a formal semantics; (ii) defining a sound resolution-style calculus by refutation; and (iii) describing a first-order proof procedure for PLFC clauses based on (ii) and on a novel notion of most general substitution of two literals in a resolution step. In contrast to standard Possibilistic logic semantics, truth-evaluation of formulas with fuzzy constants are many-valued instead of boolean, and consequently an extended notion of possibilistic uncertainty is also needed.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7358",
        "title": "On the Acceptability of Arguments in Preference-Based Argumentation",
        "authors": [
            "Leila Amgoud",
            "Claudette Cayrol"
        ],
        "abstract": "Argumentation is a promising model for reasoning with uncertain knowledge. The key concept of acceptability enables to differentiate arguments and counterarguments: The certainty of a proposition can then be evaluated through the most acceptable arguments for that proposition. In this paper, we investigate different complementary points of view: - an acceptability based on the existence of direct counterarguments, - an acceptability based on the existence of defenders. Pursuing previous work on preference-based argumentation principles, we enforce both points of view by taking into account preference orderings for comparing arguments. Our approach is illustrated in the context of reasoning with stratified knowldge bases.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7359",
        "title": "Merging Uncertain Knowledge Bases in a Possibilistic Logic Framework",
        "authors": [
            "Salem Benferhat",
            "Claudio Sossai"
        ],
        "abstract": "This paper addresses the problem of merging uncertain information in the framework of possibilistic logic. It presents several syntactic combination rules to merge possibilistic knowledge bases, provided by different sources, into a new possibilistic knowledge base. These combination rules are first described at the meta-level outside the language of possibilistic logic. Next, an extension of possibilistic logic, where the combination rules are inside the language, is proposed. A proof system in a sequent form, which is sound and complete with respect to the possibilistic logic semantics, is given.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7360",
        "title": "A Hybrid Algorithm to Compute Marginal and Joint Beliefs in Bayesian Networks and Its Complexity",
        "authors": [
            "Mark Bloemeke",
            "Marco Valtorta"
        ],
        "abstract": "There exist two general forms of exact algorithms for updating probabilities in Bayesian Networks. The first approach involves using a structure, usually a clique tree, and performing local message based calculation to extract the belief in each variable. The second general class of algorithm involves the use of non-serial dynamic programming techniques to extract the belief in some desired group of variables. In this paper we present a hybrid algorithm based on the latter approach yet possessing the ability to retrieve the belief in all single variables. The technique is advantageous in that it saves a NP-hard computation step over using one algorithm of each type. Furthermore, this technique re-enforces a conjecture of Jensen and Jensen [JJ94] in that it still requires a single NP-hard step to set up the structure on which inference is performed, as we show by confirming Li and D'Ambrosio's [LD94] conjectured NP-hardness of OFP.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7361",
        "title": "Structured Reachability Analysis for Markov Decision Processes",
        "authors": [
            "Craig Boutilier",
            "Ronen I. Brafman",
            "Christopher W. Geib"
        ],
        "abstract": "Recent research in decision theoretic planning has focussed on making the solution of Markov decision processes (MDPs) more feasible. We develop a family of algorithms for structured reachability analysis of MDPs that are suitable when an initial state (or set of states) is known. Using compact, structured representations of MDPs (e.g., Bayesian networks), our methods, which vary in the tradeoff between complexity and accuracy, produce structured descriptions of (estimated) reachable states that can be used to eliminate variables or variable values from the problem description, reducing the size of the MDP and making it easier to solve. One contribution of our work is the extension of ideas from GRAPHPLAN to deal with the distributed nature of action representations typically embodied within Bayes nets and the problem of correlated action effects. We also demonstrate that our algorithm can be made more complete by using k-ary constraints instead of binary constraints. Another contribution is the illustration of how the compact representation of reachability constraints can be exploited by several existing (exact and approximate) abstraction algorithms for MDPs.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7362",
        "title": "Tractable Inference for Complex Stochastic Processes",
        "authors": [
            "Xavier Boyen",
            "Daphne Koller"
        ],
        "abstract": "The monitoring and control of any dynamic system depends crucially on the ability to reason about its current status and its future trajectory. In the case of a stochastic system, these tasks typically involve the use of a belief state- a probability distribution over the state of the process at a given point in time. Unfortunately, the state spaces of complex processes are very large, making an explicit representation of a belief state intractable. Even in dynamic Bayesian networks (DBNs), where the process itself can be represented compactly, the representation of the belief state is intractable. We investigate the idea of maintaining a compact approximation to the true belief state, and analyze the conditions under which the errors due to the approximations taken over the lifetime of the process do not accumulate to make our answers completely irrelevant. We show that the error in a belief state contracts exponentially as the process evolves. Thus, even with multiple approximations, the error in our process remains bounded indefinitely. We show how the additional structure of a DBN can be used to design our approximation scheme, improving its performance significantly. We demonstrate the applicability of our ideas in the context of a monitoring task, showing that orders of magnitude faster inference can be achieved with only a small degradation in accuracy.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7365",
        "title": "Dealing with Uncertainty in Situation Assessment: towards a Symbolic Approach",
        "authors": [
            "Charles Castel",
            "Corine Cossart",
            "Catherine Tessier"
        ],
        "abstract": "The situation assessment problem is considered, in terms of object, condition, activity, and plan recognition, based on data coming from the real-word {em via} various sensors. It is shown that uncertainty issues are linked both to the models and to the matching algorithm. Three different types of uncertainties are identified, and within each one, the numerical and the symbolic cases are distinguished. The emphasis is then put on purely symbolic uncertainties: it is shown that they can be dealt with within a purely symbolic framework resulting from a transposition of classical numerical estimation tools.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7366",
        "title": "Marginalizing in Undirected Graph and Hypergraph Models",
        "authors": [
            "Enrique F. Castillo",
            "Juan Ferr\u00e1ndiz",
            "Pilar Sanmartin"
        ],
        "abstract": "Given an undirected graph G or hypergraph X model for a given set of variables V, we introduce two marginalization operators for obtaining the undirected graph GA or hypergraph HA associated with a given subset A c V such that the marginal distribution of A factorizes according to GA or HA, respectively. Finally, we illustrate the method by its application to some practical examples. With them we show that hypergraph models allow defining a finer factorization or performing a more precise conditional independence analysis than undirected graph models.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7367",
        "title": "Utility Elicitation as a Classification Problem",
        "authors": [
            "Urszula Chajewska",
            "Lise Getoor",
            "Joseph Norman",
            "Yuval Shahar"
        ],
        "abstract": "We investigate the application of classification techniques to utility elicitation. In a decision problem, two sets of parameters must generally be elicited: the probabilities and the utilities. While the prior and conditional probabilities in the model do not change from user to user, the utility models do. Thus it is necessary to elicit a utility model separately for each new user. Elicitation is long and tedious, particularly if the outcome space is large and not decomposable. There are two common approaches to utility function elicitation. The first is to base the determination of the users utility function solely ON elicitation OF qualitative ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7368",
        "title": "Irrelevance and Independence Relations in Quasi-Bayesian Networks",
        "authors": [
            "Fabio Gagliardi Cozman"
        ],
        "abstract": "This paper analyzes irrelevance and independence relations in graphical models associated with convex sets of probability distributions (called Quasi-Bayesian networks). The basic question in Quasi-Bayesian networks is, How can irrelevance/independence relations in Quasi-Bayesian networks be detected, enforced and exploited? This paper addresses these questions through Walley's definitions of irrelevance and independence. Novel algorithms and results are presented for inferences with the so-called natural extensions using fractional linear programming, and the properties of the so-called type-1 extensions are clarified through a new generalization of d-separation.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7369",
        "title": "Dynamic Jointrees",
        "authors": [
            "Adnan Darwiche"
        ],
        "abstract": "It is well known that one can ignore parts of a belief network when computing answers to certain probabilistic queries. It is also well known that the ignorable parts (if any) depend on the specific query of interest and, therefore, may change as the query changes. Algorithms based on jointrees, however, do not seem to take computational advantage of these facts given that they typically construct jointrees for worst-case queries; that is, queries for which every part of the belief network is considered relevant. To address this limitation, we propose in this paper a method for reconfiguring jointrees dynamically as the query changes. The reconfiguration process aims at maintaining a jointree which corresponds to the underlying belief network after it has been pruned given the current query. Our reconfiguration method is marked by three characteristics: (a) it is based on a non-classical definition of jointrees; (b) it is relatively efficient; and (c) it can reuse some of the computations performed before a jointree is reconfigured. We present preliminary experimental results which demonstrate significant savings over using static jointrees when query changes are considerable.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7370",
        "title": "On the Semi-Markov Equivalence of Causal Models",
        "authors": [
            "Benoit Desjardins"
        ],
        "abstract": "The variability of structure in a finite Markov equivalence class of causally sufficient models represented by directed acyclic graphs has been fully characterized. Without causal sufficiency, an infinite semi-Markov equivalence class of models has only been characterized by the fact that each model in the equivalence class entails the same marginal statistical dependencies. In this paper, we study the variability of structure of causal models within a semi-Markov equivalence class and propose a systematic approach to construct models entailing any specific marginal statistical dependencies.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7371",
        "title": "Comparative Uncertainty, Belief Functions and Accepted Beliefs",
        "authors": [
            "Didier Dubois",
            "Helene Fargier",
            "Henri Prade"
        ],
        "abstract": "This paper relates comparative belief structures and a general view of belief management in the setting of deductively closed logical representations of accepted beliefs. We show that the range of compatibility between the  classical deductive closure and uncertain reasoning covers precisely the nonmonotonic 'preferential' inference system of Kraus, Lehmann and Magidor and nothing else. In terms of uncertain reasoning any possibility or necessity measure gives birth to a structure of accepted beliefs. The classes of probability functions and of Shafer's belief functions which yield belief sets prove to be very special ones.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7372",
        "title": "Qualitative Decision Theory with Sugeno Integrals",
        "authors": [
            "Didier Dubois",
            "Henri Prade",
            "Regis Sabbadin"
        ],
        "abstract": "This paper presents an axiomatic framework for qualitative decision under uncertainty in a finite setting. The corresponding utility is expressed by a sup-min expression, called Sugeno (or fuzzy) integral.  Technically speaking, Sugeno integral is a median, which is indeed a qualitative counterpart to the averaging operation underlying expected utility.  The axiomatic justification of Sugeno integral-based utility is expressed in terms of preference between acts as in Savage decision theory.  Pessimistic and optimistic qualitative utilities, based on necessity and possibility measures, previously introduced by two of the authors, can be retrieved in this setting by adding appropriate axioms.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7374",
        "title": "Learning the Structure of Dynamic Probabilistic Networks",
        "authors": [
            "Nir Friedman",
            "Kevin Murphy",
            "Stuart Russell"
        ],
        "abstract": "Dynamic probabilistic networks are a compact representation of complex stochastic processes. In this paper we examine how to learn the structure of a DPN from data. We extend structure scoring rules for standard probabilistic networks to the dynamic case, and show how to search for structure when some of the variables are hidden. Finally, we examine two applications where such a technology might be useful: predicting and classifying dynamic behaviors, and learning causal orderings in biological processes. We provide empirical results that demonstrate the applicability of our methods in both domains.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7377",
        "title": "Psychological and Normative Theories of Causal Power and the Probabilities of Causes",
        "authors": [
            "Clark Glymour"
        ],
        "abstract": "This paper (1)shows that the best supported current psychological theory (Cheng, 1997) of how human subjects judge the causal power or influence of variations in presence or absence of one feature on another, given data on their covariation, tacitly uses a Bayes network which is either a noisy or gate (for causes that promote the effect) or a noisy and gate (for causes that inhibit the effect); (2)generalizes Chengs theory to arbitrary acyclic networks of noisy or and noisy and gates; (3)gives various sufficient conditions for the estimation of the parameters in such networks when there are independent, unobserved causes; (4)distinguishes direct causal influence of one feature on another (influence along a path with one edge) from total influence (influence along all paths from one variable to another) and gives sufficient conditions for estimating each when there are unobserved causes of the outcome variable; (5)describes the relation between Cheng models and a simplified version of the Rubin framework for representing causal relations.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7379",
        "title": "Towards Case-Based Preference Elicitation: Similarity Measures on Preference Structures",
        "authors": [
            "Vu A. Ha",
            "Peter Haddawy"
        ],
        "abstract": "While decision theory provides an appealing normative framework for representing rich preference structures, eliciting utility or value functions typically incurs a large cost. For many applications involving interactive systems this overhead precludes the use of formal decision-theoretic models of preference. Instead of performing elicitation in a vacuum, it would be useful if we could augment directly elicited preferences with some appropriate default information. In this paper we propose a case-based approach to alleviating the preference elicitation bottleneck. Assuming the existence of a population of users from whom we have elicited complete or incomplete preference structures, we propose eliciting the preferences of a new user interactively and incrementally, using the closest existing preference structures as potential defaults. Since a notion of closeness demands a measure of distance among preference structures, this paper takes the first step of studying various distance measures over fully and partially specified preference structures. We explore the use of Euclidean distance, Spearmans footrule, and define a new measure, the probabilistic distance. We provide computational techniques for all three measures.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7380",
        "title": "Solving POMDPs by Searching in Policy Space",
        "authors": [
            "Eric A. Hansen"
        ],
        "abstract": "Most algorithms for solving POMDPs iteratively improve a value function that implicitly represents a policy and are said to search in value function space. This paper presents an approach to solving POMDPs that represents a policy explicitly as a finite-state controller and iteratively improves the controller by search in policy space. Two related algorithms illustrate this approach. The first is a policy iteration algorithm that can outperform value iteration in solving infinitehorizon POMDPs. It provides the foundation for a new heuristic search algorithm that promises further speedup by focusing computational effort on regions of the problem space that are reachable, or likely to be reached, from a start state.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7381",
        "title": "Hierarchical Solution of Markov Decision Processes using Macro-actions",
        "authors": [
            "Milos Hauskrecht",
            "Nicolas Meuleau",
            "Leslie Pack Kaelbling",
            "Thomas L. Dean",
            "Craig Boutilier"
        ],
        "abstract": "We investigate the use of temporally abstract actions, or macro-actions, in the solution of Markov decision processes. Unlike current models that combine both primitive actions and macro-actions and leave the state space unchanged, we propose a hierarchical model (using an abstract MDP) that works with macro-actions only, and that significantly reduces the size of the state space. This is achieved by treating macroactions as local policies that act in certain regions of state space, and by restricting states in the abstract MDP to those at the boundaries of regions. The abstract MDP approximates the original and can be solved more efficiently. We discuss several ways in which macro-actions can be generated to ensure good solution quality. Finally, we consider ways in which macro-actions can be reused to solve multiple, related MDPs; and we show that this can justify the computational overhead of macro-action generation.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7383",
        "title": "Evaluating Las Vegas Algorithms - Pitfalls and Remedies",
        "authors": [
            "Holger H. Hoos",
            "Thomas Stutzle"
        ],
        "abstract": "Stochastic search algorithms are among the most sucessful approaches for solving hard combinatorial problems. A large class of stochastic search approaches can be cast into the framework of Las Vegas Algorithms (LVAs). As the run-time behavior of LVAs is characterized by random variables, the detailed knowledge of run-time distributions provides important information for the analysis of these algorithms. In this paper we propose a novel methodology for evaluating the performance of LVAs, based on the identification of empirical run-time distributions. We exemplify our approach by applying it to Stochastic Local Search (SLS) algorithms for the satisfiability problem (SAT) in propositional logic. We point out pitfalls arising from the use of improper empirical methods and discuss the benefits of the proposed methodology for evaluating and comparing LVAs.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7384",
        "title": "An Anytime Algorithm for Decision Making under Uncertainty",
        "authors": [
            "Michael C. Horsch",
            "David L. Poole"
        ],
        "abstract": "We present an anytime algorithm which computes policies for decision problems represented as multi-stage influence diagrams. Our algorithm constructs policies incrementally, starting from a policy which makes no use of the available information. The incremental process constructs policies which includes more of the information available to the decision maker at each step. While the process converges to the optimal policy, our approach is designed for situations in which computing the optimal policy is infeasible. We provide examples of the process on several large decision problems, showing that, for these examples, the process constructs valuable (but sub-optimal) policies before the optimal policy would be available by traditional methods.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7385",
        "title": "The Lumiere Project: Bayesian User Modeling for Inferring the Goals and Needs of Software Users",
        "authors": [
            "Eric J. Horvitz",
            "John S. Breese",
            "David Heckerman",
            "David Hovel",
            "Koos Rommelse"
        ],
        "abstract": "The Lumiere Project centers on harnessing probability and utility to provide assistance to computer software users.  We review work on Bayesian user models that can be employed to infer a users needs by considering a user's background, actions, and queries. Several problems were tackled in Lumiere research, including (1) the construction of Bayesian models for reasoning about the time-varying goals of computer users from their observed actions and queries, (2) gaining access to a stream of events from software applications, (3) developing a language for transforming system events into observational variables represented in Bayesian user models, (4) developing persistent profiles to capture changes in a user expertise, and (5) the development of an overall architecture for an intelligent user interface. Lumiere prototypes served as the basis for the Office Assistant in the Microsoft Office '97 suite of productivity applications.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7386",
        "title": "Any Time Probabilistic Reasoning for Sensor Validation",
        "authors": [
            "Pablo H. Ibarguengoytia",
            "Luis Enrique Sucar",
            "Sunil Vadera"
        ],
        "abstract": "For many real time applications, it is important to validate the information received from the sensors before entering higher levels of reasoning. This paper presents an any time probabilistic algorithm for validating the information provided by sensors. The system consists of two Bayesian network models. The first one is a model of the dependencies between sensors and it is used to validate each sensor. It provides a list of potentially faulty sensors. To isolate the real faults, a second Bayesian network is used, which relates the potential faults with the real faults. This second model is also used to make the validation algorithm any time, by validating first the sensors that provide more information. To select the next sensor to validate, and measure the quality of the results at each stage, an entropy function is used. This function captures in a single quantity both the certainty and specificity measures of any time algorithms. Together, both models constitute a mechanism for validating sensors in an any time fashion, providing at each step the probability of correct/faulty for each sensor, and the total quality of the results. The algorithm has been tested in the validation of temperature sensors of a power plant.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7387",
        "title": "Measure Selection: Notions of Rationality and Representation Independence",
        "authors": [
            "Manfred Jaeger"
        ],
        "abstract": "We take another look at the general problem of selecting a preferred probability measure among those that comply with some given constraints. The dominant role that entropy maximization has obtained in this context is questioned by arguing that the minimum information principle on which it is based could be supplanted by an at least as plausible \"likelihood of evidence\" principle. We then review a method for turning given selection functions into representation independent variants, and discuss the tradeoffs involved in this transformation.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7389",
        "title": "Dealing with Uncertainty on the Initial State of a Petri Net",
        "authors": [
            "Iman Jarkass",
            "Michele Rombaut"
        ],
        "abstract": "This paper proposes a method to find the actual state of a complex dynamic system from information coming from the sensors on the system himself, or on its environment. The nominal evolution of the system is a priori known and can be modeled (by an expert, for example), by different methods. In this paper, the Petri nets have been chosen. Contrary to the usual use of the Petri nets, the initial state of the system is unknown. So a degree of belief is bound to each places, or set of places. The theory used to model this uncertainty is the Dempster-Shafer's one which is well adapted to this type of problems. From the given Petri net characterizing the nominal evolution of the dynamic system, and from  the observation inputs, the proposed method allows to determine according to the reliability of the model and the inputs, the state of the system at any time.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7391",
        "title": "Exact Inference of Hidden Structure from Sample Data in Noisy-OR Networks",
        "authors": [
            "Michael Kearns",
            "Yishay Mansour"
        ],
        "abstract": "In the literature on graphical models, there has been increased attention paid to the problems of learning hidden structure (see Heckerman [H96] for survey) and causal mechanisms from sample data [H96, P88, S93, P95, F98].  In most settings we should expect the former to be difficult, and the latter potentially impossible without experimental intervention.  In this work, we examine some restricted settings in which perfectly reconstruct the hidden structure solely on the basis of observed sample data.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7394",
        "title": "A Comparison of Lauritzen-Spiegelhalter, Hugin, and Shenoy-Shafer Architectures for Computing Marginals of Probability Distributions",
        "authors": [
            "Vasilica Lepar",
            "Prakash P. Shenoy"
        ],
        "abstract": "In the last decade, several architectures have been proposed for exact computation of marginals using local computation. In this paper, we compare three architectures - Lauritzen-Spiegelhalter, Hugin, and Shenoy-Shafer - from the perspective of graphical structure for message propagation, message-passing scheme, computational efficiency, and storage efficiency.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7395",
        "title": "Incremental Tradeoff Resolution in Qualitative Probabilistic Networks",
        "authors": [
            "Chao-Lin Liu",
            "Michael P. Wellman"
        ],
        "abstract": "Qualitative probabilistic reasoning in a Bayesian network often reveals tradeoffs: relationships that are ambiguous due to competing qualitative influences. We present two techniques that combine qualitative and numeric probabilistic reasoning to resolve such tradeoffs, inferring the qualitative relationship between nodes in a Bayesian network. The first approach incrementally marginalizes nodes that contribute to the ambiguous qualitative relationships. The second approach evaluates approximate Bayesian networks for bounds of probability distributions, and uses these bounds to determinate qualitative relationships in question. This approach is also incremental in that the algorithm refines the state spaces of random variables for tighter bounds until the qualitative relationships are resolved. Both approaches provide systematic methods for tradeoff resolution at potentially lower computational cost than application of purely numeric methods.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7396",
        "title": "Using Qualitative Relationships for Bounding Probability Distributions",
        "authors": [
            "Chao-Lin Liu",
            "Michael P. Wellman"
        ],
        "abstract": "We exploit qualitative probabilistic relationships among variables for computing bounds of conditional probability distributions of interest in Bayesian networks. Using the signs of qualitative relationships, we can implement abstraction operations that are guaranteed to bound the distributions of interest in the desired direction. By evaluating incrementally improved approximate networks, our algorithm obtains monotonically tightening bounds that converge to exact distributions.  For supermodular utility functions, the tightening bounds monotonically reduce the set of admissible decision alternatives as well.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7397",
        "title": "Magic Inference Rules for Probabilistic Deduction under Taxonomic Knowledge",
        "authors": [
            "Thomas Lukasiewicz"
        ],
        "abstract": "We present locally complete inference rules for probabilistic deduction from taxonomic  and probabilistic knowledge-bases over conjunctive events. Crucially, in contrast to similar inference rules in the literature, our inference rules are locally complete for conjunctive events and under additional taxonomic knowledge. We discover that our inference rules are extremely complex and that it is at first glance not clear at all where the deduced tightest bounds come from. Moreover, analyzing the global completeness of our inference rules, we find examples of globally very incomplete probabilistic deductions.  More generally, we even show that all systems of inference rules for taxonomic and probabilistic knowledge-bases over conjunctive events are globally incomplete. We conclude that probabilistic deduction by the iterative application of inference rules on interval restrictions for conditional probabilities, even though considered very promising in the literature so far, seems very limited in its field of application.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7398",
        "title": "Lazy Propagation in Junction Trees",
        "authors": [
            "Anders L. Madsen",
            "Finn Verner Jensen"
        ],
        "abstract": "The efficiency of algorithms using secondary structures for probabilistic inference in Bayesian networks can be improved by exploiting independence relations induced by evidence and the direction of the links in the original network. In this paper we present an algorithm that on-line exploits independence relations induced by evidence and the direction of the links in the original network to reduce both time and space costs. Instead of multiplying the conditional probability distributions for the various cliques, we determine on-line which potentials to multiply when a message is to be produced. The performance improvement of the algorithm is emphasized through empirical evaluations involving large real world Bayesian networks, and we compare the method with the HUGIN and Shafer-Shenoy inference algorithms.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7399",
        "title": "Constructing Situation Specific Belief Networks",
        "authors": [
            "Suzanne M. Mahoney",
            "Kathryn Blackmond Laskey"
        ],
        "abstract": "This paper describes a process for constructing situation-specific belief networks from a knowledge base of network fragments. A situation-specific network is a minimal query complete network constructed from a knowledge base in response to a query for the probability distribution on a set of target variables given evidence and context variables. We present definitions of query completeness and situation-specific networks. We describe conditions on the knowledge base that guarantee query completeness. The relationship of our work to earlier work on KBMC is also discussed.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7402",
        "title": "From Likelihood to Plausibility",
        "authors": [
            "Paul-Andre Monney"
        ],
        "abstract": "Several authors have explained that the likelihood ratio measures the strength of the evidence represented by observations in statistical problems. This idea works fine when the goal is to evaluate the strength of the available evidence for a simple hypothesis versus another simple hypothesis. However, the applicability of this idea is limited to simple hypotheses because the likelihood function is primarily defined on points (simple hypotheses) of the parameter space. In this paper we define a general weight of evidence that is applicable to both simple and composite hypotheses. It is based on the Dempster-Shafer concept of plausibility and is shown to be a generalization of the likelihood ratio. Functional models are of a fundamental importance for the general weight of evidence proposed in this paper. The relevant concepts and ideas are explained by means of a familiar urn problem and the general analysis of a real-world medical problem is presented.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7403",
        "title": "A Multivariate Discretization Method for Learning Bayesian Networks from Mixed Data",
        "authors": [
            "Stefano Monti",
            "Gregory F. Cooper"
        ],
        "abstract": "In this paper we address the problem of discretization in the context of learning Bayesian networks (BNs) from data containing both continuous and discrete variables. We describe a new technique for <EM>multivariate</EM> discretization, whereby each continuous variable is discretized while taking into account its interaction with the other variables. The technique is based on the use of a Bayesian scoring metric that scores the discretization policy for a continuous variable given a BN structure and the observed data. Since the metric is relative to the BN structure currently being evaluated, the discretization of a variable needs to be dynamically adjusted as the BN structure changes.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7404",
        "title": "Resolving Conflicting Arguments under Uncertainties",
        "authors": [
            "Benson Hin Kwong Ng",
            "Kam-Fai Wong",
            "Boon-Toh Low"
        ],
        "abstract": "Distributed knowledge based applications in open domain rely on common sense information which is bound to be uncertain and incomplete. To draw the useful conclusions from ambiguous data, one must address uncertainties and conflicts incurred in a holistic view.  No integrated frameworks are viable without an in-depth analysis of conflicts incurred by uncertainties. In this paper, we give such an analysis and based on the result, propose an integrated framework. Our framework extends definite argumentation theory to model uncertainty. It supports three views over conflicting and uncertain knowledge. Thus, knowledge engineers can draw different conclusions depending on the application context (i.e. view). We also give an illustrative example on strategical decision support to show the practical usefulness of our framework.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7405",
        "title": "Flexible Decomposition Algorithms for Weakly Coupled Markov Decision Problems",
        "authors": [
            "Ron Parr"
        ],
        "abstract": "This paper presents two new approaches to decomposing and solving large Markov decision problems (MDPs), a partial decoupling method and a complete decoupling method.  In these approaches, a large, stochastic decision problem is divided into smaller pieces.  The first approach builds a cache of policies for each part of the problem independently, and then combines the pieces in a separate, light-weight step.  A second approach also divides the problem into smaller pieces, but information is communicated between the different problem pieces, allowing intelligent decisions to be made about which piece requires the most attention.  Both approaches can be used to find optimal policies or approximately optimal policies with provable bounds. These algorithms also provide a framework for the efficient transfer of knowledge across problems that share similar structure.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7406",
        "title": "Logarithmic Time Parallel Bayesian Inference",
        "authors": [
            "David M. Pennock"
        ],
        "abstract": "I present a parallel algorithm for exact probabilistic inference in Bayesian networks.  For polytree networks with n variables, the worst-case time complexity is O(log n) on a CREW PRAM (concurrent-read, exclusive-write parallel random-access machine) with n processors, for any constant number of evidence variables. For arbitrary networks, the time complexity is O(r^{3w}*log n) for n processors, or O(w*log n) for r^{3w}*n processors, where r is the maximum range of any variable, and w is the induced width (the maximum clique size), after moralizing and triangulating the network.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7407",
        "title": "Learning From What You Don't Observe",
        "authors": [
            "Mark Alan Peot",
            "Ross D. Shachter"
        ],
        "abstract": "The process of diagnosis involves learning about the state of a system from various observations of symptoms or findings about the system.  Sophisticated Bayesian (and other) algorithms have been developed to revise and maintain beliefs about the system as observations are made.  Nonetheless, diagnostic models have tended to ignore some common sense reasoning exploited by human diagnosticians; In particular, one can learn from which observations have not been made, in the spirit of conversational implicature.  There are two concepts that we describe to extract information from the observations not made.  First, some symptoms, if present, are more likely to be reported before others.  Second, most human diagnosticians and expert systems are economical in their data-gathering, searching first where they are more likely to find symptoms present.  Thus, there is a desirable bias toward reporting symptoms that are present.  We develop a simple model for these concepts that can significantly improve diagnostic inference.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7408",
        "title": "Context-Specific Approximation in Probabilistic Inference",
        "authors": [
            "David L. Poole"
        ],
        "abstract": "There is evidence that the numbers in probabilistic inference don't really matter. This paper considers the idea that we can make a probabilistic model simpler by making fewer distinctions. Unfortunately, the level of a Bayesian network seems too coarse; it is unlikely that a parent will make little difference for all values of the other parents. In this paper we consider an approximation scheme where distinctions can be ignored in some contexts, but not in other contexts. We elaborate on a notion of a parent context that allows a structured context-specific decomposition of a probability distribution and the associated probabilistic inference scheme called probabilistic partial evaluation (Poole 1997). This paper shows a way to simplify a probabilistic model by ignoring distinctions which have similar probabilities, a method to exploit the simpler model, a bound on the resulting errors, and some preliminary empirical results on simple networks.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7409",
        "title": "Empirical Evaluation of Approximation Algorithms for Probabilistic Decoding",
        "authors": [
            "Irina Rish",
            "Kalev Kask",
            "Rina Dechter"
        ],
        "abstract": "It was recently  shown that the problem of decoding messages transmitted through a noisy channel can be formulated as a belief updating task over a probabilistic network [McEliece]. Moreover, it was observed that iterative application of the (linear time) Pearl's belief propagation algorithm  designed for polytrees outperformed state of the art decoding algorithms, even though the corresponding networks may have many cycles. This paper demonstrates empirically that an approximation algorithm approx-mpe for solving the most probable explanation (MPE) problem, developed within the recently proposed mini-bucket elimination framework [Dechter96], outperforms  iterative belief propagation on classes of coding networks that have bounded induced width. Our experiments suggest that approximate MPE decoders can be good competitors to the approximate belief updating decoders.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7410",
        "title": "Decision Theoretic Foundations of Graphical Model Selection",
        "authors": [
            "Paola Sebastiani",
            "Marco Ramoni"
        ],
        "abstract": "This paper describes a decision theoretic formulation of learning the graphical structure of a Bayesian Belief Network from data. This framework subsumes the standard Bayesian approach of choosing the model with the largest posterior probability as the solution of a decision problem with a 0-1 loss function and allows the use of more general loss functions able to trade-off the complexity of the selected model and the error of choosing an oversimplified model.  A new class of loss functions, called disintegrable, is introduced, to allow the decision problem to match the decomposability of the graphical model.  With this class of loss functions, the optimal solution to the decision problem can be found using an efficient bottom-up search strategy.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7412",
        "title": "Bayes-Ball: The Rational Pastime (for Determining Irrelevance and Requisite Information in Belief Networks and Influence Diagrams)",
        "authors": [
            "Ross D. Shachter"
        ],
        "abstract": "One of the benefits of belief networks and influence diagrams is that so much knowledge is captured in the graphical structure.  In particular, statements of conditional irrelevance (or independence) can be verified in time linear in the size of the graph.  To resolve a particular inference query or decision problem, only some of the possible states and probability distributions must be specified, the \"requisite information.\"\n",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7414",
        "title": "Bayesian Networks from the Point of View of Chain Graphs",
        "authors": [
            "Milan Studeny"
        ],
        "abstract": "AThe paper gives a few arguments in favour of the use of chain graphs for description of probabilistic conditional independence structures.  Every Bayesian network model can be equivalently introduced by means of a factorization formula with respect to a chain graph which is Markov equivalent to the Bayesian network.  A graphical characterization of such graphs is given. The class of equivalent graphs can be represented by a distinguished graph which is called the largest chain graph. The factorization formula with respect to the largest chain graph is a basis of a proposal of how to represent the corresponding (discrete) probability distribution in a computer (i.e. parametrize it). This way does not depend on the choice of a particular Bayesian network from the class of equivalent networks and seems to be the most efficient way from the point of view of memory demands. A separation criterion for reading independency statements from a chain graph is formulated in a simpler way. It resembles the well-known d-separation criterion for Bayesian networks and can be implemented locally.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7416",
        "title": "Probabilistic Inference in Influence Diagrams",
        "authors": [
            "Nevin Lianwen Zhang"
        ],
        "abstract": "This paper is about reducing influence diagram (ID) evaluation into Bayesian network (BN) inference problems. Such reduction is interesting because it enables one to readily use one's favorite BN inference algorithm to efficiently evaluate IDs. Two such reduction methods have been proposed previously (Cooper 1988, Shachter and Peot 1992). This paper proposes a new method. The BN inference problems induced by the mew method are much easier to solve than those induced by the two previous methods.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7417",
        "title": "Planning with Partially Observable Markov Decision Processes: Advances in Exact Solution Method",
        "authors": [
            "Nevin Lianwen Zhang",
            "Stephen S. Lee"
        ],
        "abstract": "There is much interest in using partially observable Markov decision processes (POMDPs) as a formal model for planning in stochastic domains.  This paper is concerned with finding optimal policies for POMDPs.  We propose several improvements to incremental pruning, presently the most efficient exact algorithm for solving POMDPs.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7418",
        "title": "Flexible and Approximate Computation through State-Space Reduction",
        "authors": [
            "Weixiong Zhang"
        ],
        "abstract": "In the real world, insufficient information, limited computation resources, and complex problem structures often force an autonomous agent to make a decision in time less than that required to solve the problem at hand completely.  Flexible and approximate computations are two approaches to decision making under limited computation resources. Flexible computation helps an agent to flexibly allocate limited computation resources so that the overall system utility is maximized. Approximate computation enables an agent to find the best satisfactory solution within a deadline. In this paper, we present two state-space reduction methods for flexible and approximate computation: quantitative reduction to deal with inaccurate heuristic information, and structural reduction to handle complex problem structures. These two methods can be applied successively to continuously improve solution quality if more computation is available. Our results show that these reduction methods are effective and efficient, finding better solutions with less computation than some existing well-known methods.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7676",
        "title": "Efficient Partial Order CDCL Using Assertion Level Choice Heuristics",
        "authors": [
            "Anthony Monnet",
            "Roger Villemaire"
        ],
        "abstract": "We previously designed Partial Order Conflict Driven Clause Learning (PO-CDCL), a variation of the satisfiability solving CDCL algorithm with a partial order on decision levels, and showed that it can speed up the solving on problems with a high independence between decision levels. In this paper, we more thoroughly analyze the reasons of the efficiency of PO-CDCL. Of particular importance is that the partial order introduces several candidates for the assertion level. By evaluating different heuristics for this choice, we show that the assertion level selection has an important impact on solving and that a carefully designed heuristic can significantly improve performances on relevant benchmarks.\n    ",
        "submission_date": "2013-01-31T00:00:00",
        "last_modified_date": "2013-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.0216",
        "title": "Comparison between the two definitions of AI",
        "authors": [
            "Dimiter Dobrev"
        ],
        "abstract": "Two different definitions of the Artificial Intelligence concept have been proposed in papers [1] and [2]. The first definition is informal. It says that any program that is cleverer than a human being, is acknowledged as Artificial Intelligence. The second definition is formal because it avoids reference to the concept of human being. The readers of papers [1] and [2] might be left with the impression that both definitions are equivalent and the definition in [2] is simply a formal version of that in [1]. This paper will compare both definitions of Artificial Intelligence and, hopefully, will bring a better understanding of the concept.\n    ",
        "submission_date": "2013-01-31T00:00:00",
        "last_modified_date": "2013-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.0334",
        "title": "Class Algebra for Ontology Reasoning",
        "authors": [
            "Daniel Buehrer",
            "Chee-Hwa Lee"
        ],
        "abstract": "Class algebra provides a natural framework for sharing of ISA hierarchies between users that may be unaware of each other's definitions. This permits data from relational databases, object-oriented databases, and tagged XML documents to be unioned into one distributed ontology, sharable by all users without the need for prior negotiation or the development of a \"standard\" ontology for each field. Moreover, class algebra produces a functional correspondence between a class's class algebraic definition (i.e. its \"intent\") and the set of all instances which satisfy the expression (i.e. its \"extent\"). The framework thus provides assistance in quickly locating examples and counterexamples of various definitions. This kind of information is very valuable when developing models of the real world, and serves as an invaluable tool assisting in the proof of theorems concerning these class algebra expressions. Finally, the relative frequencies of objects in the ISA hierarchy can produce a useful Boolean algebra of probabilities. The probabilities can be used by traditional information-theoretic classification methodologies to obtain optimal ways of classifying objects in the database.\n    ",
        "submission_date": "2013-02-02T00:00:00",
        "last_modified_date": "2013-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1155",
        "title": "An Effective Procedure for Computing \"Uncomputable\" Functions",
        "authors": [
            "Kurt Ammon"
        ],
        "abstract": "We give an effective procedure that produces a natural number in its output from any natural number in its input, that is, it computes a total function. The elementary operations of the procedure are Turing-computable. The procedure has a second input which can contain the Goedel number of any Turing-computable total function whose range is a subset of the set of the Goedel numbers of all Turing-computable total functions. We prove that the second input cannot be set to the Goedel number of any Turing-computable function that computes the output from any natural number in its first input. In this sense, there is no Turing program that computes the output from its first input. The procedure is used to define creative procedures which compute functions that are not Turing-computable. We argue that creative procedures model an aspect of reasoning that cannot be modeled by Turing machines.\n    ",
        "submission_date": "2013-02-05T00:00:00",
        "last_modified_date": "2013-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1334",
        "title": "Principles of modal and vector theory of formal intelligence systems",
        "authors": [
            "Yuri Parzhin"
        ],
        "abstract": "The paper considers the class of information systems capable of solving heuristic problems on basis of formal theory that was termed modal and vector theory of formal intelligent systems (FIS). The paper justifies the construction of FIS resolution algorithm, defines the main features of these systems and proves theorems that underlie the theory. The principle of representation diversity of FIS construction is formulated. The paper deals with the main principles of constructing and functioning formal intelligent system (FIS) on basis of FIS modal and vector theory. The following phenomena are considered: modular architecture of FIS presentation sub-system, algorithms of data processing at every step of the stage of creating presentations. Besides the paper suggests the structure of neural elements, i.e. zone detectors and processors that are the basis for FIS construction.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1520",
        "title": "Bayes Networks for Sonar Sensor Fusion",
        "authors": [
            "Ami Berler",
            "Solomon Eyal Shimony"
        ],
        "abstract": "Wide-angle sonar mapping of the environment by mobile robot is nontrivial due to several sources of uncertainty: dropouts due to \"specular\" reflections, obstacle location uncertainty due to the wide beam, and distance measurement error.  Earlier papers address the latter problems, but dropouts remain a problem in many environments.  We present an approach that lifts the overoptimistic independence assumption used in earlier work, and use Bayes nets to represent the dependencies between objects of the model. Objects of the model consist of readings, and of regions in which \"quasi location invariance\" of the (possible) obstacles exists, with respect to the readings.  Simulation supports the method's feasibility.  The model is readily extensible to allow for prior distributions, as well as other types of sensing operations.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1521",
        "title": "Exploiting Uncertain and Temporal Information in Correlation",
        "authors": [
            "John Bigham"
        ],
        "abstract": "A modelling language is described which is suitable for the correlation of information when the underlying functional model of the system is incomplete or uncertain and the temporal dependencies are imprecise.  An efficient and incremental implementation is outlined which depends on cost functions satisfying certain criteria.  Possibilistic logic and probability theory (as it is used in the applications targetted) satisfy these criteria.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1522",
        "title": "Correlated Action Effects in Decision Theoretic Regression",
        "authors": [
            "Craig Boutilier"
        ],
        "abstract": "Much recent research in decision theoretic planning has adopted Markov decision processes (MDPs) as the model of choice, and has attempted to make their solution more tractable by exploiting problem structure. One particular algorithm, structured policy construction achieves this by means of a decision theoretic analog of goal regression using action descriptions based on Bayesian networks with tree-structured conditional probability tables.  The algorithm as presented is not able to deal with actions with correlated effects. We describe a new decision theoretic regression operator that corrects this weakness.  While conceptually straightforward, this extension requires a somewhat more complicated technical approach.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1523",
        "title": "Corporate Evidential Decision Making in Performance Prediction Domains",
        "authors": [
            "Alex G. Buchner",
            "Werner Dubitzky",
            "Alfons Schuster",
            "Philippe Lopes",
            "Peter G. O'Donoghue",
            "John G. Hughes",
            "David A. Bell",
            "Kenny Adamson",
            "John A. White",
            "John M.C.C. Anderson",
            "Maurice D. Mulvenna"
        ],
        "abstract": "Performance prediction or forecasting sporting outcomes involves a great deal of insight into the particular area one is dealing with, and a considerable amount of intuition about the factors that bear on such outcomes and performances.  The mathematical Theory of Evidence offers representation formalisms which grant experts a high degree of freedom when expressing their subjective beliefs in the context of decision-making situations like performance prediction.  Furthermore, this reasoning framework incorporates a powerful mechanism to systematically pool the decisions made by individual subject matter experts.  The idea behind such a combination of knowledge is to improve the competence (quality) of the overall decision-making process. This paper reports on a performance prediction experiment carried out during the European Football Championship in 1996. Relying on the knowledge of four predictors, Evidence Theory was used to forecast the final scores of all 31 matches.  The results of this empirical study are very encouraging.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1524",
        "title": "Algorithms for Learning Decomposable Models and Chordal Graphs",
        "authors": [
            "Luis M. de Campos",
            "Juan F. Huete"
        ],
        "abstract": "Decomposable dependency models and their graphical counterparts, i.e., chordal graphs, possess a number of interesting and useful properties. On the basis of two characterizations of decomposable models in terms of independence relationships, we develop an exact algorithm for recovering the chordal graphical representation of any given decomposable model. We also propose an algorithm for learning chordal approximations of dependency models isomorphic to general undirected graphs.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1525",
        "title": "Incremental Pruning: A Simple, Fast, Exact Method for Partially Observable Markov Decision Processes",
        "authors": [
            "Anthony R. Cassandra",
            "Michael L. Littman",
            "Nevin Lianwen Zhang"
        ],
        "abstract": "Most exact algorithms for general partially observable Markov decision processes (POMDPs) use a form of dynamic programming in which a piecewise-linear and convex representation of one value function is transformed into another.  We examine variations of the \"incremental pruning\" method for solving this problem and compare them to earlier algorithms from theoretical and empirical perspectives.  We find that incremental pruning is presently the most efficient exact method for solving POMDPs.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1526",
        "title": "Defining Explanation in Probabilistic Systems",
        "authors": [
            "Urszula Chajewska",
            "Joseph Y. Halpern"
        ],
        "abstract": "As probabilistic systems gain popularity and are coming into wider use, the need for a mechanism that explains the system's findings and recommendations becomes more critical.  The system will also need a mechanism for ordering competing explanations. We examine two representative approaches to explanation in the literature - one due to G\u00e4rdenfors and one due to Pearl - and show that both suffer from significant problems.  We propose an approach to defining a notion of \"better explanation\" that combines some of the features of both together with more recent work by Pearl and others on causality.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1527",
        "title": "Structured Arc Reversal and Simulation of Dynamic Probabilistic Networks",
        "authors": [
            "Adrian Y. W. Cheuk",
            "Craig Boutilier"
        ],
        "abstract": "We present an algorithm for arc reversal in Bayesian networks with tree-structured conditional probability tables, and consider some of its advantages, especially for the simulation of dynamic probabilistic networks. In particular, the method allows one to produce CPTs for nodes involved in the reversal that exploit regularities in the conditional distributions. We argue that this approach alleviates some of the overhead associated with arc reversal, plays an important role in evidence integration and can be used to restrict sampling of variables in DPNs.  We also provide an algorithm that detects the dynamic irrelevance of state variables in forward simulation. This algorithm exploits the structured CPTs in a reversed network to determine, in a time-independent fashion, the conditions under which a variable does or does not need to be sampled.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1529",
        "title": "Exploring Parallelism in Learning Belief Networks",
        "authors": [
            "TongSheng Chu",
            "Yang Xiang"
        ],
        "abstract": "It has been shown that a class of probabilistic domain models cannot be learned correctly by several existing algorithms which employ a single-link look ahead search. When a multi-link look ahead search is used, the computational complexity of the learning algorithm increases.  We study how to use parallelism to tackle the increased complexity in learning such models and to speed up learning in large domains.  An algorithm is proposed to decompose the learning task for parallel processing.  A further task decomposition is used to balance load among processors and to increase the speed-up and efficiency.  For learning from very large datasets, we present a regrouping of the available processors such that slow data access through file can be replaced by fast memory access.  Our implementation in a parallel computer demonstrates the effectiveness of the algorithm.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1530",
        "title": "Efficient Induction of Finite State Automata",
        "authors": [
            "Matthew S. Collins",
            "Jonathan Oliver"
        ],
        "abstract": "This paper introduces a new algorithm for the induction if complex finite state automata from samples of behavior.  The algorithm is based on information theoretic principles.  The algorithm reduces the search space by many orders of magnitude over what was previously thought possible.  We compare the algorithm with some existing induction techniques for finite state automata and show that the algorithm is much superior in both run time and quality of inductions.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1531",
        "title": "Robustness Analysis of Bayesian Networks with Local Convex Sets of Distributions",
        "authors": [
            "Fabio Gagliardi Cozman"
        ],
        "abstract": "Robust Bayesian inference is the calculation of posterior probability bounds given perturbations in a probabilistic model.  This paper focuses on perturbations that can be expressed locally in Bayesian networks through convex sets of distributions.  Two approaches for combination of local models are considered.  The first approach takes the largest set of joint distributions that is compatible with the local sets of distributions; we show how to reduce this type of robust inference to a linear programming problem.  The second approach takes the convex hull of joint distributions generated from the local sets of distributions; we demonstrate how to apply interior-point optimization methods to generate posterior bounds and how to generate approximations that are guaranteed to converge to correct posterior bounds.  We also discuss calculation of bounds for expected utilities and variances, and global perturbation models.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1532",
        "title": "A Standard Approach for Optimizing Belief Network Inference using Query DAGs",
        "authors": [
            "Adnan Darwiche",
            "Gregory M. Provan"
        ],
        "abstract": "This paper proposes a novel, algorithm-independent approach to optimizing belief network inference. rather than designing optimizations on an algorithm by algorithm basis, we argue that one should use an unoptimized algorithm to generate a Q-DAG, a compiled graphical representation of the belief network, and then optimize the Q-DAG and its evaluator instead. We present a set of Q-DAG optimizations that supplant optimizations designed for traditional inference algorithms, including zero compression, network pruning and caching. We show that our Q-DAG optimizations require time linear in the Q-DAG size, and significantly simplify the process of designing algorithms for optimizing belief network inference.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1533",
        "title": "Model Reduction Techniques for Computing Approximately Optimal Solutions for Markov Decision Processes",
        "authors": [
            "Thomas L. Dean",
            "Robert Givan",
            "Sonia Leach"
        ],
        "abstract": "We present a method for solving implicit (factored) Markov decision processes (MDPs) with very large state spaces. We introduce a property of state space partitions which we call epsilon-homogeneity.  Intuitively, an epsilon-homogeneous partition groups together states that behave approximately the same under all or some subset of policies. Borrowing from recent work on model minimization in computer-aided software verification, we present an algorithm that takes a factored representation of an MDP and an 0<=epsilon<=1 and computes a factored epsilon-homogeneous partition of the state space.  This partition defines a family of related MDPs - those MDPs with state space equal to the blocks of the partition, and transition probabilities \"approximately\" like those of any (original MDP) state in the source block. To formally study such families of MDPs, we introduce the new notion of a \"bounded parameter MDP\" (BMDP), which is a family of (traditional) MDPs defined by specifying upper and lower bounds on the transition probabilities and rewards. We describe algorithms that operate on BMDPs to find policies that are approximately optimal with respect to the original MDP.  In combination, our method for reducing a large implicit MDP to a possibly much smaller BMDP using an epsilon-homogeneous partition, and our methods for selecting actions in BMDPs constitute a new approach for analyzing large implicit MDPs. Among its advantages, this new approach provides insight into existing algorithms to solving implicit MDPs, provides useful connections to work in automata theory and model minimization, and suggests methods, which involve varying epsilon, to trade time and space (specifically in terms of the size of the corresponding state space) for solution quality.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1534",
        "title": "A Scheme for Approximating Probabilistic Inference",
        "authors": [
            "Rina Dechter",
            "Irina Rish"
        ],
        "abstract": "This paper describes a class of probabilistic approximation algorithms based on bucket elimination which offer adjustable levels of accuracy and efficiency.  We analyze the approximation for several tasks:  finding the most probable explanation, belief updating and finding the maximum a posteriori hypothesis.  We identify regions of completeness and provide preliminary empirical evaluation on randomly generated networks.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1535",
        "title": "Myopic Value of Information in Influence Diagrams",
        "authors": [
            "Soren L. Dittmer",
            "Finn Verner Jensen"
        ],
        "abstract": "We present a method for calculation of myopic value of information in influence diagrams (Howard & Matheson, 1981) based on the strong junction tree framework (Jensen, Jensen & Dittmer, 1994).  The difference in instantiation order in the influence diagrams is reflected in the corresponding junction trees by the order in which the chance nodes are marginalized.  This order of marginalization can be changed by table expansion and in effect the same junction tree with expanded tables may be used for calculating the expected utility for scenarios with different instantiation order.  We also compare our method to the classic method of modeling different instantiation orders in the same influence diagram.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1536",
        "title": "Limitations of Skeptical Default Reasoning",
        "authors": [
            "Jens Doerpmund"
        ],
        "abstract": "Poole has shown that nonmonotonic logics do not handle the lottery paradox correctly.  In this paper we will show that Pollock's theory of defeasible reasoning fails for the same reason: defeasible reasoning is incompatible with the skeptical notion of derivability.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1537",
        "title": "Decision-making Under Ordinal Preferences and Comparative Uncertainty",
        "authors": [
            "Didier Dubois",
            "Helene Fargier",
            "Henri Prade"
        ],
        "abstract": "This paper investigates the problem of finding a preference relation on a set of acts from the knowledge of an ordering on events (subsets of states of the world) describing the decision-maker (DM)s uncertainty and an ordering of consequences of acts, describing the DMs preferences.  However, contrary to classical approaches to decision theory, we try to do it without resorting to any numerical representation of utility nor uncertainty, and without even using any qualitative scale on which both uncertainty and preference could be mapped.  It is shown that although many axioms of Savage theory can be preserved and despite the intuitive appeal of the method for constructing a preference over acts, the approach is inconsistent with a probabilistic representation of uncertainty, but leads to the kind of uncertainty theory encountered in non-monotonic reasoning (especially preferential and rational inference), closely related to possibility theory.  Moreover the method turns out to be either very little decisive or to lead to very risky decisions, although its basic principles look sound.  This paper raises the question of the very possibility of purely symbolic approaches to Savage-like decision-making under uncertainty and obtains preliminary negative results.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1538",
        "title": "Sequential Update of Bayesian Network Structure",
        "authors": [
            "Nir Friedman",
            "Moises Goldszmidt"
        ],
        "abstract": "There is an obvious need for improving the performance and accuracy of a Bayesian network as new data is observed.  Because of errors in model construction and changes in the dynamics of the domains, we cannot afford to ignore the information in new data.  While sequential update of parameters for a fixed structure can be accomplished using standard techniques, sequential update of network structure is still an open problem.  In this paper, we investigate sequential update of Bayesian networks were both parameters and structure are expected to change.  We introduce a new approach that allows for the flexible manipulation of the tradeoff between the quality of the learned networks and the amount of information that is maintained about past observations.  We formally describe our approach including the necessary modifications to the scoring functions for learning Bayesian networks, evaluate its effectiveness through an empirical study, and extend it to the case of missing data.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1540",
        "title": "The Complexity of Plan Existence and Evaluation in Probabilistic Domains",
        "authors": [
            "Judy Goldsmith",
            "Michael L. Littman",
            "Martin Mundhenk"
        ],
        "abstract": "We examine the computational complexity of testing and finding small plans in probabilistic planning domains with succinct representations.  We find that many problems of interest are complete for a variety of complexity classes: NP, co-NP, PP, NP^PP, co-NP^PP, and PSPACE.  Of these, the probabilistic classes PP and NP^PP are likely to be of special interest in the field of uncertainty in artificial intelligence and are deserving of additional study.  These results suggest a fruitful direction of future algorithmic development.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1541",
        "title": "Algorithm Portfolio Design: Theory vs. Practice",
        "authors": [
            "Carla P. Gomes",
            "Bart Selman"
        ],
        "abstract": "Stochastic algorithms are among the best for solving computationally hard search and reasoning problems. The runtime of such procedures is characterized by a random variable. Different algorithms give rise to different probability distributions. One can take advantage of such differences by combining several algorithms into a portfolio, and running them in parallel or interleaving them on a single processor. We provide a detailed evaluation of the portfolio approach on distributions of hard combinatorial search problems.  We show under what conditions the protfolio approach can have a dramatic computational advantage over the best traditional methods.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1542",
        "title": "Learning Bayesian Nets that Perform Well",
        "authors": [
            "Russell Greiner",
            "Adam J. Grove",
            "Dale Schuurmans"
        ],
        "abstract": "A Bayesian net (BN) is more than a succinct way to encode a probabilistic distribution; it also corresponds to a function used to answer queries.  A BN can therefore be evaluated by the accuracy of the answers it returns.  Many algorithms for learning BNs, however, attempt to optimize another criterion (usually likelihood, possibly augmented with a regularizing term), which is independent of the distribution of queries that are posed.  This paper takes the \"performance criteria\" seriously, and considers the challenge of computing the BN whose performance - read \"accuracy over the distribution of queries\" - is optimal.  We show that many aspects of this learning task are more difficult than the corresponding subtasks in the standard model.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1543",
        "title": "Probability Update: Conditioning vs. Cross-Entropy",
        "authors": [
            "Adam J. Grove",
            "Joseph Y. Halpern"
        ],
        "abstract": "Conditioning is the generally agreed-upon method for updating probability distributions when one learns that an event is certainly true.  But it has been argued that we need other rules, in particular the rule of cross-entropy minimization, to handle updates that involve uncertain information.  In this paper we re-examine such a case: van Fraassen's Judy Benjamin problem, which in essence asks how one might update given the value of a conditional probability.  We argue that -- contrary to the suggestions in the literature -- it is possible to use simple conditionalization in this case, and thereby obtain answers that agree fully with intuition.  This contrasts with proposals such as cross-entropy, which are easier to apply but can give unsatisfactory answers.  Based on the lessons from this example, we speculate on some general philosophical issues concerning probability update. \n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1544",
        "title": "Problem-Focused Incremental Elicitation of Multi-Attribute Utility Models",
        "authors": [
            "Vu A. Ha",
            "Peter Haddawy"
        ],
        "abstract": "Decision theory has become widely accepted in the AI community as a useful framework for planning and decision making.  Applying the framework typically requires elicitation of some form of probability and utility information.  While much work in AI has focused on providing representations and tools for elicitation of probabilities, relatively little work has addressed the elicitation of utility models.  This imbalance is not particularly justified considering that probability models are relatively stable across problem instances, while utility models may be different for each instance.  Spending large amounts of time on elicitation can be undesirable for interactive systems used in low-stakes decision making and in time-critical decision making.  In this paper we investigate the issues of reasoning with incomplete utility models.  We identify patterns of problem instances where plans can be proved to be suboptimal if the (unknown) utility function satisfies certain conditions.  We present an approach to planning and decision making that performs the utility elicitation incrementally and in a way that is informed by the domain model.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1546",
        "title": "Inference with Idempotent Valuations",
        "authors": [
            "Luis D. Hernandez",
            "Serafin Moral"
        ],
        "abstract": "Valuation based systems verifying an idempotent property are studied.  A partial order is defined between the valuations giving them a lattice structure.  Then, two different strategies are introduced to represent valuations: as infimum of the most informative valuations or as supremum of the least informative ones.  It is studied how to carry out computations with both representations in an efficient way.  The particular cases of finite sets and convex polytopes are considered.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1547",
        "title": "Perception, Attention, and Resources: A Decision-Theoretic Approach to Graphics Rendering",
        "authors": [
            "Eric J. Horvitz",
            "Jed Lengyel"
        ],
        "abstract": "We describe work to control graphics rendering under limited computational resources by taking a decision-theoretic perspective on perceptual costs and computational savings of approximations.  The work extends earlier work on the control of rendering by introducing methods and models for computing the expected cost associated with degradations of scene components.  The expected cost is computed by considering the perceptual cost of degradations and a probability distribution over the attentional focus of viewers.  We review the critical literature describing findings on visual search and attention, discuss the implications of the findings, and introduce models of expected perceptual cost. Finally, we discuss policies that harness information about the expected cost of scene components.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1548",
        "title": "Time-Critical Reasoning: Representations and Application",
        "authors": [
            "Eric J. Horvitz",
            "Adam Seiver"
        ],
        "abstract": "We review the problem of time-critical action and discuss a reformulation that shifts knowledge acquisition from the assessment of complex temporal probabilistic dependencies to the direct assessment of time-dependent utilities over key outcomes of interest.  We dwell on a class of decision problems characterized by the centrality of diagnosing and reacting in a timely manner to pathological processes. We motivate key ideas in the context of trauma-care triage and transportation decisions.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1549",
        "title": "Learning Belief Networks in Domains with Recursively Embedded Pseudo Independent Submodels",
        "authors": [
            "Jun Hu",
            "Yang Xiang"
        ],
        "abstract": "A pseudo independent (PI) model is a probabilistic domain model (PDM) where proper subsets of a set of collectively dependent variables display marginal independence.  PI models cannot be learned correctly by many algorithms that rely on a single link search. Earlier work on learning PI models has suggested a straightforward multi-link search algorithm.  However, when a domain contains recursively embedded PI submodels, it may escape the detection of such an algorithm.  In this paper, we propose an improved algorithm that ensures the learning of all embedded PI submodels whose sizes are upper bounded by a predetermined parameter.  We show that this improved learning capability only increases the complexity slightly beyond that of the previous algorithm.  The performance of the new algorithm is demonstrated through experiment. \n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1550",
        "title": "Relational Bayesian Networks",
        "authors": [
            "Manfred Jaeger"
        ],
        "abstract": "A new method is developed to represent probabilistic relations on multiple random events.  Where previously knowledge bases containing probabilistic rules were used for this purpose, here a probability distribution over the relations is directly represented by a Bayesian network.  By using a powerful way of specifying conditional probability distributions in these networks, the resulting formalism is more expressive than the previous ones.  Particularly, it provides for constraints on equalities of events, and it allows to define complex, nested combination functions. \n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1551",
        "title": "Composition of Probability Measures on Finite Spaces",
        "authors": [
            "Radim Jirousek"
        ],
        "abstract": "Decomposable models and Bayesian networks can be defined as sequences of oligo-dimensional probability measures connected with operators of composition.  The preliminary results suggest that the probabilistic models allowing for effective computational procedures are represented by sequences possessing a special property; we shall call them perfect sequences.  The paper lays down the elementary foundation necessary for further study of iterative application of operators of composition.  We believe to develop a technique describing several graph models in a unifying way.  We are convinced that practically all theoretical results and procedures connected with decomposable models and Bayesian networks can be translated into the terminology introduced in this paper.  For example, complexity of computational procedures in these models is closely dependent on possibility to change the ordering of oligo-dimensional measures defining the model.  Therefore, in this paper, lot of attention is paid to possibility to change ordering of the operators of composition.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1553",
        "title": "Nested Junction Trees",
        "authors": [
            "Uffe Kj\u00e6rulff"
        ],
        "abstract": "The efficiency of inference in both the Hugin and, most notably, the Shafer-Shenoy architectures can be improved by exploiting the independence relations induced by the incoming messages of a clique.  That is, the message to be sent from a clique can be computed via a factorization of the clique potential in the form of a junction tree.  In this paper we show that by exploiting such nested junction trees in the computation of messages both space and time costs of the conventional propagation methods may be reduced.  The paper presents a structured way of exploiting the nested junction trees technique to achieve such reductions.  The usefulness of the method is emphasized through a thorough empirical evaluation involving ten large real-world Bayesian networks and the Hugin inference algorithm.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1554",
        "title": "Object-Oriented Bayesian Networks",
        "authors": [
            "Daphne Koller",
            "Avi Pfeffer"
        ],
        "abstract": "Bayesian networks provide a modeling language and associated inference algorithm for stochastic domains.  They have been successfully applied in a variety of medium-scale applications.  However, when faced with a large complex domain, the task of modeling using Bayesian networks begins to resemble the task of programming using logical circuits.  In this paper, we describe an object-oriented Bayesian network (OOBN) language, which allows complex domains to be described in terms of inter-related objects.  We use a Bayesian network fragment to describe the probabilistic relations between the attributes of an object.  These attributes can themselves be objects, providing a natural framework for encoding part-of hierarchies.  Classes are used to provide a reusable probabilistic model which can be applied to multiple similar objects.  Classes also support inheritance of model fragments from a class to a subclass, allowing the common aspects of related classes to be defined only once.  Our language has clear declarative semantics: an OOBN can be interpreted as a stochastic functional program, so that it uniquely specifies a probabilistic model.  We provide an inference algorithm for OOBNs, and show that much of the structural information encoded by an OOBN--particularly the encapsulation of variables within an object and the reuse of model fragments in different contexts--can also be used to speed up the inference process.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1555",
        "title": "Nonuniform Dynamic Discretization in Hybrid Networks",
        "authors": [
            "Alexander V. Kozlov",
            "Daphne Koller"
        ],
        "abstract": "We consider probabilistic inference in general hybrid networks, which include continuous and discrete variables in an arbitrary topology.  We reexamine the question of variable discretization in a hybrid network aiming at minimizing the information loss induced by the discretization.  We show that a nonuniform partition across all variables as opposed to uniform partition of each variable separately reduces the size of the data structures needed to represent a continuous function.  We also provide a simple but efficient procedure for nonuniform partition.  To represent a nonuniform discretization in the computer memory, we introduce a new data structure, which we call a Binary Split Partition (BSP) tree.  We show that BSP trees can be an exponential factor smaller than the data structures in the standard uniform discretization in multiple dimensions and show how the BSP trees can be used in the standard join tree algorithm.  We show that the accuracy of the inference process can be significantly improved by adjusting discretization with evidence.  We construct an iterative anytime algorithm that gradually improves the quality of the discretization and the accuracy of the answer on a query.  We provide empirical evidence that the algorithm converges.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1556",
        "title": "Probabilistic Acceptance",
        "authors": [
            "Henry E. Kyburg Jr"
        ],
        "abstract": "The idea of fully accepting statements when the evidence has rendered them probable enough faces a number of difficulties.  We leave the interpretation of probability largely open, but attempt to suggest a contextual approach to full belief.  We show that the difficulties of probabilistic acceptance are not as severe as they are sometimes painted, and that though there are oddities associated with probabilistic acceptance they are in some instances less awkward than the difficulties associated with other nonmonotonic formalisms.  We show that the structure at which we arrive provides a natural home for statistical inference.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1557",
        "title": "Network Fragments: Representing Knowledge for Constructing Probabilistic Models",
        "authors": [
            "Kathryn Blackmond Laskey",
            "Suzanne M. Mahoney"
        ],
        "abstract": "In most current applications of belief networks, domain knowledge is represented by a single belief network that applies to all problem instances in the domain.  In more complex domains, problem-specific models must be constructed from a knowledge base encoding probabilistic relationships in the domain.  Most work in knowledge-based model construction takes the rule as the basic unit of knowledge.  We present a knowledge representation framework that permits the knowledge base designer to specify knowledge in larger semantically meaningful units which we call network fragments.  Our framework provides for representation of asymmetric independence and canonical intercausal interaction.  We discuss the combination of network fragments to form problem-specific models to reason about particular problem instances.  The framework is illustrated using examples from the domain of military situation awareness.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1558",
        "title": "Computational Advantages of Relevance Reasoning in Bayesian Belief Networks",
        "authors": [
            "Yan Lin",
            "Marek J. Druzdzel"
        ],
        "abstract": "This paper introduces a computational framework for reasoning in Bayesian belief networks that derives significant advantages from focused inference and relevance reasoning.  This framework is based on d -separation and other simple and computationally efficient techniques for pruning irrelevant parts of a network.  Our main contribution is a technique that we call relevance-based decomposition.  Relevance-based decomposition approaches belief updating in large networks by focusing on their parts and decomposing them into partially overlapping subnetworks.  This makes reasoning in some intractable networks possible and, in addition, often results in significant speedup, as the total time taken to update all subnetworks is in practice often considerably less than the time taken to update the network as a whole.  We report results of empirical tests that demonstrate practical significance of our approach.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1560",
        "title": "A Target Classification Decision Aid",
        "authors": [
            "Todd Michael Mansell"
        ],
        "abstract": "A submarine's sonar team is responsible for detecting, localising and classifying targets using information provided by the platform's sensor suite.  The information used to make these assessments is typically uncertain and/or incomplete and is likely to require a measure of confidence in its reliability.  Moreover, improvements in sensor and communication technology are resulting in increased amounts of on-platform and off-platform information available for evaluation. This proliferation of imprecise information increases the risk of overwhelming the operator. To assist the task of localisation and classification a concept demonstration decision aid (Horizon), based on evidential reasoning, has been developed. Horizon is an information fusion software package for representing and fusing imprecise information about the state of the world, expressed across suitable frames of reference. The Horizon software is currently at prototype stage. \n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1561",
        "title": "Structure and Parameter Learning for Causal Independence and Causal Interaction Models",
        "authors": [
            "Christopher Meek",
            "David Heckerman"
        ],
        "abstract": "This paper discusses causal independence models and a generalization of these models called causal interaction models. Causal interaction models are models that have independent mechanisms where a mechanism can have several causes. In addition to introducing several particular types of causal interaction models, we show how we can apply the Bayesian approach to learning causal interaction models obtaining approximate posterior distributions for the models and obtain MAP and ML estimates for the parameters. We illustrate the approach with a simulation study of learning model posteriors.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1562",
        "title": "Support and Plausibility Degrees in Generalized Functional Models",
        "authors": [
            "Paul-Andre Monney"
        ],
        "abstract": "By discussing several examples, the theory of generalized functional models is shown to be very natural for modeling some situations of reasoning under uncertainty.  A generalized functional model is a pair (f, P) where f is a function describing the interactions between a parameter variable, an observation variable and a random source, and P is a probability distribution for the random source.  Unlike traditional functional models, generalized functional models do not require that there is only one value of the parameter variable that is compatible with an observation and a realization of the random source.  As a consequence, the results of the analysis of a generalized functional model are not expressed in terms of probability distributions but rather by support and plausibility functions.  The analysis of a generalized functional model is very logical and is inspired from ideas already put forward by R.A. Fisher in his theory of fiducial probability.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1563",
        "title": "The Cognitive Processing of Causal Knowledge",
        "authors": [
            "Scott B. Morris",
            "Doug Cork",
            "Richard E. Neapolitan"
        ],
        "abstract": "There is a brief description of the probabilistic causal graph model for representing, reasoning with, and learning causal structure using Bayesian networks. It is then argued that this model is closely related to how humans reason with and learn causal structure. It is shown that studies in psychology on discounting (reasoning concerning how the presence of one cause of an effect makes another cause less probable) support the hypothesis that humans reach the same judgments as algorithms for doing inference in Bayesian networks.  Next, it is shown how studies by Piaget indicate that humans learn causal structure by observing the same independencies and dependencies as those used by certain algorithms for learning the structure of a Bayesian network. Based on this indication, a subjective definition of causality is forwarded. Finally, methods for further testing the accuracy of these claims are discussed.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1564",
        "title": "Representing Aggregate Belief through the Competitive Equilibrium of a Securities Market",
        "authors": [
            "David M. Pennock",
            "Michael P. Wellman"
        ],
        "abstract": "We consider the problem of belief aggregation: given a group of individual agents with probabilistic beliefs over a set of uncertain events, formulate a sensible consensus or aggregate probability distribution over these events.  Researchers have proposed many aggregation methods, although on the question of which is best the general consensus is that there is no consensus.  We develop a market-based approach to this problem, where agents bet on uncertain events by buying or selling securities contingent on their outcomes.  Each agent acts in the market so as to maximize expected utility at given securities prices, limited in its activity only by its own risk aversion.  The equilibrium prices of goods in this market represent aggregate beliefs. For agents with constant risk aversion, we demonstrate that the aggregate probability exhibits several desirable properties, and is related to independently motivated techniques. We argue that the market-based approach provides a plausible mechanism for belief aggregation in multiagent systems, as it directly addresses self-motivated agent incentives for participation and for truthfulness, and can provide a decision-theoretic foundation for the \"expert weights\" often employed in centralized pooling techniques.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1565",
        "title": "Learning Bayesian Networks from Incomplete Databases",
        "authors": [
            "Marco Ramoni",
            "Paola Sebastiani"
        ],
        "abstract": "Bayesian approaches to learn the graphical structure of Bayesian Belief Networks (BBNs) from databases share the assumption that the database is complete, that is, no entry is reported as unknown.  Attempts to relax this assumption involve the use of expensive iterative methods to discriminate among different structures.  This paper introduces a deterministic method to learn the graphical structure of a BBN from a possibly incomplete database.  Experimental evaluations show a significant robustness of this method and a remarkable independence of its execution time from the number of missing data.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1567",
        "title": "Cost-Sharing in Bayesian Knowledge Bases",
        "authors": [
            "Solomon Eyal Shimony",
            "Carmel Domshlak",
            "Eugene Santos Jr"
        ],
        "abstract": "Bayesian knowledge bases (BKBs) are a generalization of Bayes networks and weighted proof graphs (WAODAGs), that allow cycles in the causal graph.  Reasoning in BKBs requires finding the most probable inferences consistent with the evidence.  The cost-sharing heuristic for finding least-cost explanations in WAODAGs was presented and shown to be effective by Charniak and Husain.  However, the cycles in BKBs would make the definition of cost-sharing cyclic as well, if applied directly to BKBs.  By treating the defining equations of cost-sharing as a system of equations, one can properly define an admissible cost-sharing heuristic for BKBs.  Empirical evaluation shows that cost-sharing improves performance significantly when applied to BKBs.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1569",
        "title": "Sequential Thresholds: Context Sensitive Default Extensions",
        "authors": [
            "Choh Man Teng"
        ],
        "abstract": "Default logic encounters some conceptual difficulties in representing common sense reasoning tasks.  We argue that we should not try to formulate modular default rules that are presumed to work in all or most circumstances.  We need to take into account the importance of the context which is continuously evolving during the reasoning process. Sequential thresholding is a quantitative counterpart of default logic which makes explicit the role context plays in the construction of a non-monotonic extension.  We present a semantic characterization of generic non-monotonic reasoning, as well as the instantiations pertaining to default logic and sequential thresholding.  This provides a link between the two mechanisms as well as a way to integrate the two that can be beneficial to both. \n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1570",
        "title": "On Stable Multi-Agent Behavior in Face of Uncertainty",
        "authors": [
            "Moshe Tennenholtz"
        ],
        "abstract": "A stable joint plan should guarantee the achievement of a designer's goal in a multi-agent environment, while ensuring that deviations from the prescribed plan would be detected. We present a computational framework where stable joint plans can be studied, as well as several basic results about the representation, verification and synthesis of stable joint plans.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1573",
        "title": "Region-Based Approximations for Planning in Stochastic Domains",
        "authors": [
            "Nevin Lianwen Zhang",
            "Wenju Liu"
        ],
        "abstract": "This paper is concerned with planning in stochastic domains by means of partially observable Markov decision processes (POMDPs).  POMDPs are difficult to solve.  This paper identifies a  subclass of POMDPs called region observable POMDPs, which are easier to solve and can be used to approximate general POMDPs to arbitrary accuracy.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1574",
        "title": "Independence of Causal Influence and Clique Tree Propagation",
        "authors": [
            "Nevin Lianwen Zhang",
            "Li Yan"
        ],
        "abstract": "This paper explores the role of independence of causal influence (ICI) in Bayesian network inference. ICI allows one to factorize a conditional probability table into smaller pieces.  We describe a method for exploiting the factorization in clique tree propagation (CTP) - the state-of-the-art exact inference algorithm for Bayesian networks.  We also present empirical results showing that the resulting algorithm is significantly more efficient than the combination of CTP and previous techniques for exploiting ICI.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1575",
        "title": "Fast Value Iteration for Goal-Directed Markov Decision Processes",
        "authors": [
            "Nevin Lianwen Zhang",
            "Weihong Zhang"
        ],
        "abstract": "Planning problems where effects of actions are non-deterministic can be modeled as Markov decision processes.  Planning problems are usually goal-directed.  This paper proposes several techniques for exploiting the goal-directedness to accelerate value iteration, a standard algorithm for solving Markov decision processes.  Empirical studies have shown that the techniques can bring about significant speedups.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.2056",
        "title": "Complexity distribution of agent policies",
        "authors": [
            "Jose Hernandez-Orallo"
        ],
        "abstract": "We analyse the complexity of environments according to the policies that need to be used to achieve high performance. The performance results for a population of policies leads to a distribution that is examined in terms of policy complexity and analysed through several diagrams and indicators. The notion of environment response curve is also introduced, by inverting the performance results into an ability scale. We apply all these concepts, diagrams and indicators to a minimalistic environment class, agent-populated elementary cellular automata, showing how the difficulty, discriminating power and ranges (previous to normalisation) may vary for several environments.\n    ",
        "submission_date": "2013-02-08T00:00:00",
        "last_modified_date": "2013-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.2465",
        "title": "RIO: Minimizing User Interaction in Debugging of Knowledge Bases",
        "authors": [
            "Patrick Rodler",
            "Kostyantyn Shchekotykhin",
            "Philipp Fleiss",
            "Gerhard Friedrich"
        ],
        "abstract": "The best currently known interactive debugging systems rely upon some meta-information in terms of fault probabilities in order to improve their efficiency. However, misleading meta information might result in a dramatic decrease of the performance and its assessment is only possible a-posteriori. Consequently, as long as the actual fault is unknown, there is always some risk of suboptimal interactions. In this work we present a reinforcement learning strategy that continuously adapts its behavior depending on the performance achieved and minimizes the risk of using low-quality meta information. Therefore, this method is suitable for application scenarios where reliable prior fault estimates are difficult to obtain. Using diverse real-world knowledge bases, we show that the proposed interactive query strategy is scalable, features decent reaction time, and outperforms both entropy-based and no-risk strategies on average w.r.t. required amount of user interaction.\n    ",
        "submission_date": "2013-02-11T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3549",
        "title": "An Algorithm for Finding Minimum d-Separating Sets in Belief Networks",
        "authors": [
            "Silvia Acid",
            "Luis M. de Campos"
        ],
        "abstract": "The criterion commonly used in directed acyclic graphs (dags) for testing graphical independence is the well-known d-separation criterion.  It allows us to build graphical representations of dependency models (usually probabilistic dependency models) in the form of belief networks, which make easy interpretation and management of independence relationships possible, without reference to numerical parameters (conditional probabilities).  In this paper, we study the following combinatorial problem: finding the minimum d-separating set for two nodes in a dag.  This set would represent the minimum information (in the sense of minimum number of variables) necessary to prevent these two nodes from influencing each other.  The solution to this basic problem and some of its extensions can be useful in several ways, as we shall see later.  Our solution is based on a two-step process: first, we reduce the original problem to the simpler one of finding a minimum separating set in an undirected graph, and second, we develop an algorithm for solving it.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3550",
        "title": "Constraining Influence Diagram Structure by Generative Planning: An Application to the Optimization of Oil Spill Response",
        "authors": [
            "John Mark Agosta"
        ],
        "abstract": "This paper works through the optimization of a real world planning problem, with a combination of a generative planning tool and an influence diagram solver.  The problem is taken from an existing application in the domain of oil spill emergency response.  The planning agent manages constraints that order sets of feasible equipment employment actions.  This is mapped at an intermediate level of abstraction onto an influence diagram. In addition, the planner can apply a surveillance operator that determines observability of the state---the unknown trajectory of the oil.  The uncertain world state and the objective function properties are part of the influence diagram structure, but not represented in the planning agent domain.  By exploiting this structure under the constraints generated by the planning agent, the influence diagram solution complexity simplifies considerably, and an optimum solution to the employment problem based on the objective function is found.  Finding this optimum is equivalent to the simultaneous evaluation of a range of plans.  This result is an example of bounded optimality, within the limitations of this hybrid generative planner and influence diagram architecture.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3551",
        "title": "Inference Using Message Propagation and Topology Transformation in Vector Gaussian Continuous Networks",
        "authors": [
            "Satnam Alag",
            "Alice M. Agogino"
        ],
        "abstract": "We extend Gaussian networks - directed acyclic graphs that encode probabilistic relationships between variables - to its vector form.  Vector Gaussian continuous networks consist of composite nodes representing multivariates, that take continuous values.  These vector or composite nodes can represent correlations between parents, as opposed to conventional univariate nodes.  We derive rules for inference in these networks based on two methods: message propagation and topology transformation.  These two approaches lead to the development of algorithms, that can be implemented in either a centralized or a decentralized manner.  The domain of application of these networks are monitoring and estimation problems.  This new representation along with the rules for inference developed here can be used to derive current Bayesian algorithms such as the Kalman filter, and provide a rich foundation to develop new algorithms.  We illustrate this process by deriving the decentralized form of the Kalman filter.  This work unifies concepts from artificial intelligence and modern control theory.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3552",
        "title": "A Structurally and Temporally Extended Bayesian Belief Network Model: Definitions, Properties, and Modeling Techniques",
        "authors": [
            "Constantin F. Aliferis",
            "Gregory F. Cooper"
        ],
        "abstract": "We developed the language of Modifiable Temporal Belief Networks (MTBNs) as a structural and temporal extension of Bayesian Belief Networks (BNs) to facilitate normative temporal and causal modeling under uncertainty.  In this paper we present definitions of the model, its components, and its fundamental properties.  We also discuss how to represent various types of temporal knowledge, with an emphasis on hybrid temporal-explicit time modeling, dynamic structures, avoiding causal temporal inconsistencies, and dealing with models that involve simultaneously actions (decisions) and causal and non-causal associations.  We examine the relationships among BNs, Modifiable Belief Networks, and MTBNs with a single temporal granularity, and suggest areas of application suitable to each one of them.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3553",
        "title": "An Alternative Markov Property for Chain Graphs",
        "authors": [
            "Steen A. Andersson",
            "David Madigan",
            "Michael D. Perlman"
        ],
        "abstract": "Graphical Markov models use graphs, either undirected, directed, or mixed, to represent possible dependences among statistical variables.  Applications of undirected graphs (UDGs) include models for spatial dependence and image analysis, while acyclic directed graphs (ADGs), which are especially convenient for statistical analysis, arise in such fields as genetics and psychometrics and as models for expert systems and Bayesian belief networks.  Lauritzen, Wermuth and Frydenberg (LWF) introduced a Markov property for chain graphs, which are mixed graphs that can be used to represent simultaneously both causal and associative dependencies and which include both UDGs and ADGs as special cases.  In this paper an alternative Markov property (AMP) for chain graphs is introduced, which in some ways is a more direct extension of the ADG Markov property than is the LWF property for chain graph.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3554",
        "title": "Plan Development using Local Probabilistic Models",
        "authors": [
            "Ella M. Atkins",
            "Edmund H. Durfee",
            "Kang G. Shin"
        ],
        "abstract": "Approximate models of world state transitions are necessary when building plans for complex systems operating in dynamic environments.  External event probabilities can depend on state feature values as well as time spent in that particular state.  We assign temporally -dependent probability functions to state transitions.  These functions are used to locally compute state probabilities, which are then used to select highly probable goal paths and eliminate improbable states.  This probabilistic model has been implemented in the Cooperative Intelligent Real-time Control Architecture (CIRCA), which combines an AI planner with a separate real-time system such that plans are developed, scheduled, and executed with real-time guarantees.  We present flight simulation tests that demonstrate how our probabilistic model may improve CIRCA performance.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3555",
        "title": "Entailment in Probability of Thresholded Generalizations",
        "authors": [
            "Donald Bamber"
        ],
        "abstract": "A nonmonotonic logic of thresholded generalizations is presented.  Given propositions A and B from a language L and a positive integer k, the thresholded generalization A=>B{k} means that the conditional probability P(B|A) falls short of one by no more than c*d^k.  A two-level probability structure is defined.  At the lower level, a model is defined to be a probability function on L. At the upper level, there is a probability distribution over models.  A definition is given of what it means for a collection of thresholded generalizations to entail another thresholded generalization.  This nonmonotonic entailment relation, called \"entailment in probability\", has the feature that its conclusions are \"probabilistically trustworthy\" meaning that, given true premises, it is improbable that an entailed conclusion would be false.  A procedure is presented for ascertaining  whether any given collection of premises entails any given conclusion.  It is shown that entailment in probability is closely related to Goldszmidt and Pearl's System-Z^+, thereby demonstrating that the conclusions of System-Z^+ are probabilistically trustworthy.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3557",
        "title": "Approximations for Decision Making in the Dempster-Shafer Theory of Evidence",
        "authors": [
            "Mathias Bauer"
        ],
        "abstract": "The computational complexity of reasoning within the Dempster-Shafer theory of evidence is one of the main points of criticism this formalism has to face.  To overcome this difficulty various approximation algorithms have been suggested that aim at reducing the number of focal elements in the belief functions involved.  Besides introducing a new algorithm using this method, this paper describes an empirical study that examines the appropriateness of these approximation procedures in decision making situations.  It presents the empirical findings and discusses the various tradeoffs that have to be taken into account when actually applying one of these methods.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3559",
        "title": "Coping with the Limitations of Rational Inference in the Framework of Possibility Theory",
        "authors": [
            "Salem Benferhat",
            "Didier Dubois",
            "Henri Prade"
        ],
        "abstract": "Possibility theory offers a framework where both Lehmann's \"preferential inference\" and the more productive (but less cautious) \"rational closure inference\" can be represented. However, there are situations where the second inference does not provide expected results either because it cannot produce them, or even provide counter-intuitive conclusions.  This state of facts is not due to the principle of selecting a unique ordering of interpretations (which can be encoded by one possibility distribution), but rather to the absence of constraints expressing pieces of knowledge we have implicitly in mind.  It is advocated in this paper that constraints induced by independence information can help finding the right ordering of interpretations.  In particular, independence constraints can be systematically assumed with respect to formulas composed of literals which do not appear in the conditional knowledge base, or for default rules with respect to situations which are \"normal\" according to the other default rules in the base.  The notion of independence which is used can be easily expressed in the qualitative setting of possibility theory.  Moreover, when a counter-intuitive plausible conclusion of a set of defaults, is in its rational closure, but not in its preferential closure, it is always possible to repair the set of defaults so as to produce the desired conclusion.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3560",
        "title": "Arguing for Decisions: A Qualitative Model of Decision Making",
        "authors": [
            "Blai Bonet",
            "Hector Geffner"
        ],
        "abstract": "We develop a qualitative model of decision making with two aims: to describe how people make simple decisions and to enable computer programs to do the same.  Current approaches based on Planning or Decisions Theory either ignore uncertainty and tradeoffs, or provide languages and algorithms that are too complex for this task.  The proposed model provides a language based on rules, a semantics based on high probabilities and lexicographical preferences, and a transparent decision procedure where reasons for and against decisions interact.  The model is no substitude for Decision Theory, yet for decisions that people find easy to explain it may provide an appealing alternative.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3562",
        "title": "Context-Specific Independence in Bayesian Networks",
        "authors": [
            "Craig Boutilier",
            "Nir Friedman",
            "Moises Goldszmidt",
            "Daphne Koller"
        ],
        "abstract": "Bayesian networks provide a language for qualitatively representing the conditional independence properties of a distribution.  This allows a natural and compact representation of the distribution, eases knowledge acquisition, and supports effective inference algorithms.  It is well-known, however, that there are certain independencies that we cannot capture qualitatively within the Bayesian network structure: independencies that hold only in certain contexts, i.e., given a specific assignment of values to certain variables.  In this paper, we propose a formal notion of context-specific independence (CSI), based on regularities in the conditional probability tables (CPTs) at a node.  We present a technique, analogous to (and based on) d-separation, for determining when such independence holds in a given network.  We then focus on a particular qualitative representation scheme - tree-structured CPTs - for capturing CSI.  We suggest ways in which this representation can be used to support effective inference algorithms.  In particular, we present a structural decomposition of the resulting network which can improve the performance of clustering algorithms, and an alternative algorithm based on cutset conditioning.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3563",
        "title": "Decision-Theoretic Troubleshooting: A Framework for Repair and Experiment",
        "authors": [
            "John S. Breese",
            "David Heckerman"
        ],
        "abstract": "We develop and extend existing decision-theoretic methods for troubleshooting a nonfunctioning device. Traditionally, diagnosis with Bayesian networks has focused on belief updating---determining the probabilities of various faults given current observations. In this paper, we extend this paradigm to include taking actions. In particular, we consider three classes of actions: (1) we can make observations regarding the behavior of a device and infer likely faults as in traditional diagnosis, (2) we can repair a component and then observe the behavior of the device to infer likely faults, and (3) we can change the configuration of the device, observe its new behavior, and infer the likelihood of faults. Analysis of latter two classes of troubleshooting actions requires incorporating notions of persistence into the belief-network formalism used for probabilistic inference.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2015-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3564",
        "title": "Tail Sensitivity Analysis in Bayesian Networks",
        "authors": [
            "Enrique F. Castillo",
            "Cristina Solares",
            "Patricia Gomez"
        ],
        "abstract": "The paper presents an efficient method for simulating the tails of a target variable Z=h(X) which depends on a set of basic variables X=(X_1, ..., X_n).  To this aim, variables X_i, i=1, ..., n are sequentially simulated in such a manner that Z=h(x_1, ..., x_i-1, X_i, ..., X_n) is guaranteed to be in the tail of Z.  When this method is difficult to apply, an alternative method is proposed, which leads to a low rejection proportion of sample values, when compared with the Monte Carlo method.  Both methods are shown to be very useful to perform a sensitivity analysis of Bayesian networks, when very large confidence intervals for the marginal/conditional probabilities are required, as in reliability or risk analysis.  The methods are shown to behave best when all scores coincide.  The required modifications for this to occur are discussed.  The methods are illustrated with several examples and one example of application to a real case is used to illustrate the whole process.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3565",
        "title": "Decision-Analytic Approaches to Operational Decision Making: Application and Observation",
        "authors": [
            "Tom Chavez"
        ],
        "abstract": "Decision analysis (DA) and the rich set of tools developed by researchers in decision making under uncertainty show great potential to penetrate the technological content of the products and services delivered by firms in a variety of industries as well as the business processes used to deliver those products and services to market.  In this paper I describe work in progress at Sun Microsystems in the application of decision-analytic methods to Operational Decision Making (ODM) in its World-Wide Operations (WWOPS) Business Management Group.  Working with membersof product engineering, marketing, and sales, operations planners from WWOPS have begun to use a decision-analytic framework called SCRAM (Supply Communication/Risk Assessment and Management) to structure and solve problems in product planning, tracking, and transition.  Concepts such as information value provide a powerful method of managing huge information sets and thereby enable managers to focus attention on factors that matter most for their business.  Finally, our process-oriented introduction of decision-analytic methods to Sun managers has led to a focused effort to develop decision support software based on methods from decision making under uncertainty.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3566",
        "title": "Learning Equivalence Classes of Bayesian Networks Structures",
        "authors": [
            "David Maxwell Chickering"
        ],
        "abstract": "Approaches to learning Bayesian networks from data typically combine a scoring function with a heuristic search procedure.  Given a Bayesian network structure, many of the scoring functions derived in the literature return a score for the entire equivalence class to which the structure belongs.  When using such a scoring function, it is appropriate for the heuristic search algorithm to search over equivalence classes of Bayesian networks as opposed to individual structures.  We present the general formulation of a search space for which the states of the search correspond to equivalence classes of structures.  Using this space, any one of a number of heuristic search algorithms can easily be applied.  We compare greedy search performance in the proposed search space to greedy search performance in a search space for which the states correspond to individual Bayesian network structures.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3568",
        "title": "Independence with Lower and Upper Probabilities",
        "authors": [
            "Lonnie Chrisman"
        ],
        "abstract": "It is shown that the ability of the interval probability representation to capture epistemological independence is severely limited.  Two events are epistemologically independent if knowledge of the first event does not alter belief (i.e., probability bounds) about the second.  However, independence in this form can only exist in a 2-monotone probability function in degenerate cases i.e., if the prior bounds are either point probabilities or entirely vacuous. Additional limitations are characterized for other classes of lower probabilities as well.  It is argued that these phenomena are simply a matter of interpretation.  They appear to be limitations when one interprets probability bounds as a measure of epistemological indeterminacy (i.e., uncertainty arising from a lack of knowledge), but are exactly as one would expect when probability intervals are interpreted as representations of ontological indeterminacy (indeterminacy introduced by structural approximations).  The ontological interpretation is introduced and discussed.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3569",
        "title": "Propagation of 2-Monotone Lower Probabilities on an Undirected Graph",
        "authors": [
            "Lonnie Chrisman"
        ],
        "abstract": "Lower and upper probabilities, also known as Choquet capacities, are widely used as a convenient representation for sets of probability distributions.  This paper presents a graphical decomposition and exact propagation algorithm for computing marginal posteriors of 2-monotone lower probabilities (equivalently, 2-alternating upper probabilities).\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3570",
        "title": "Quasi-Bayesian Strategies for Efficient Plan Generation: Application to the Planning to Observe Problem",
        "authors": [
            "Fabio Gagliardi Cozman",
            "Eric Krotkov"
        ],
        "abstract": "Quasi-Bayesian theory uses convex sets of probability distributions and expected loss to represent preferences about plans.  The theory focuses on decision robustness, i.e., the extent to which plans are affected by deviations in subjective assessments of probability.  The present work presents solutions for plan generation when robustness of probability assessments must be included: plans contain information about the robustness of certain actions.  The surprising result is that some problems can be solved faster in the Quasi-Bayesian framework than within usual Bayesian theory.  We investigate this on the planning to observe problem, i.e., an agent must decide whether to take new observations or not.  The fundamental question is: How, and how much, to search for a \"best\" plan, based on the robustness of probability assessments?  Plan generation algorithms are derived in the context of material classification with an acoustic robotic probe.  A package that constructs Quasi-Bayesian plans is available through anonymous ftp.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3571",
        "title": "Some Experiments with Real-Time Decision Algorithms",
        "authors": [
            "Bruce D'Ambrosio",
            "Scott Burgess"
        ],
        "abstract": "Real-time Decision algorithms are a class of incremental resource-bounded [Horvitz, 89] or anytime [Dean, 93] algorithms for evaluating influence diagrams.  We present a test domain for real-time decision algorithms, and the results of experiments with several Real-time Decision Algorithms in this domain.  The results demonstrate high performance for two algorithms, a decision-evaluation variant of Incremental Probabilisitic Inference [D'Ambrosio 93] and a variant of an algorithm suggested by Goldszmidt, [Goldszmidt, 95], PK-reduced.  We discuss the implications of these experimental results and explore the broader applicability of these algorithms.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3572",
        "title": "Bucket Elimination: A Unifying Framework for Several Probabilistic Inference",
        "authors": [
            "Rina Dechter"
        ],
        "abstract": "Probabilistic inference algorithms for finding the most probable explanation, the maximum aposteriori hypothesis, and the maximum expected utility and for updating belief are reformulated as an elimination--type algorithm called bucket elimination.  This emphasizes the principle common to many of the algorithms appearing in that literature and clarifies their relationship to nonserial dynamic programming algorithms.  We also present a general way of combining conditioning and elimination within this framework.  Bounds on complexity are given for all the algorithms as a function of the problem's structure.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3573",
        "title": "Topological Parameters for Time-Space Tradeoff",
        "authors": [
            "Rina Dechter"
        ],
        "abstract": "In this paper we propose a family of algorithms combining tree-clustering with conditioning that trade space for time.  Such algorithms are useful for reasoning in probabilistic and deterministic networks as well as for accomplishing optimization tasks. By analyzing the problem structure it will be possible to select from a spectrum the algorithm that best meets a given time-space specification. \n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3574",
        "title": "Sound Abstraction of Probabilistic Actions in The Constraint Mass Assignment Framework",
        "authors": [
            "AnHai Doan",
            "Peter Haddawy"
        ],
        "abstract": "This paper provides a formal and practical framework for sound abstraction of probabilistic actions.  We start by precisely defining the concept of sound abstraction within the context of finite-horizon planning (where each plan is a finite sequence of actions).  Next we show that such abstraction cannot be performed within the traditional probabilistic action representation, which models a world with a single probability distribution over the state space.  We then present the constraint mass assignment representation, which models the world with a set of probability distributions and is a generalization of mass assignment representations.  Within this framework, we present sound abstraction procedures for three types of action abstraction.  We end the paper with discussions and related work on sound and approximate abstraction.  We give pointers to papers in which we discuss other sound abstraction-related issues, including applications, estimating loss due to abstraction, and automatically generating abstraction hierarchies.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3575",
        "title": "Belief Revision with Uncertain Inputs in the Possibilistic Setting",
        "authors": [
            "Didier Dubois",
            "Henri Prade"
        ],
        "abstract": "This paper discusses belief revision under uncertain inputs in the framework of possibility theory.  Revision can be based on two possible definitions of the conditioning operation, one based on min operator which requires a purely ordinal scale only, and another based on product, for which a richer structure is needed, and which is a particular case of Dempster's rule of conditioning.  Besides, revision under uncertain inputs can be understood in two different ways depending on whether the input is viewed, or not, as a constraint to enforce.  Moreover, it is shown that M.A. Williams' transmutations, originally defined in the setting of Spohn's functions, can be captured in this framework, as well as Boutilier's natural revision.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3576",
        "title": "An Evaluation of Structural Parameters for Probabilistic Reasoning: Results on Benchmark Circuits",
        "authors": [
            "Yousri El Fattah",
            "Rina Dechter"
        ],
        "abstract": "Many algorithms for processing probabilistic networks are dependent on the topological properties of the problem's structure.  Such algorithms (e.g., clustering, conditioning) are effective only if the problem has a sparse graph captured by parameters such as tree width and cycle-cut set size.  In this paper we initiate a study to determine the potential of structure-based algorithms in real-life applications.  We analyze empirically the structural properties of problems coming from the circuit diagnosis domain.  Specifically, we locate those properties that capture the effectiveness of clustering and conditioning as well as of a family of conditioning+clustering algorithms designed to gradually trade space for time. We perform our analysis on 11 benchmark circuits widely used in the testing community. We also report on the effect of ordering heuristics on tree-clustering and show that, on our benchmarks, the well-known max-cardinality ordering is substantially inferior to an ordering called min-degree. \n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3577",
        "title": "Learning Bayesian Networks with Local Structure",
        "authors": [
            "Nir Friedman",
            "Moises Goldszmidt"
        ],
        "abstract": "In this paper we examine a novel addition to the known methods for learning Bayesian networks from data that improves the quality of the learned networks.  Our approach explicitly represents and learns the local structure in the conditional probability tables (CPTs), that quantify these networks.  This increases the space of possible models, enabling the representation of CPTs with a variable number of parameters that depends on the learned local structures.  The resulting learning procedure is capable of inducing models that better emulate the real complexity of the interactions present in the data.  We describe the theoretical foundations and practical aspects of learning local structures, as well as an empirical evaluation of the proposed method.  This evaluation indicates that learning curves characterizing the procedure that exploits the local structure converge faster than these of the standard procedure.  Our results also show that networks learned with local structure tend to be more complex (in terms of arcs), yet require less parameters.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3578",
        "title": "A Qualitative Markov Assumption and its Implications for Belief Change",
        "authors": [
            "Nir Friedman",
            "Joseph Y. Halpern"
        ],
        "abstract": "The study of belief change has been an active area in philosophy and AI.  In recent years two special cases of belief change, belief revision and belief update, have been studied in detail.  Roughly, revision treats a surprising observation as a sign that previous beliefs were wrong, while update treats a surprising observation as an indication that the world has changed.  In general, we would expect that an agent making an observation may both want to revise some earlier beliefs and assume that some change has occurred in the world.  We define a novel approach to belief change that allows us to do this, by applying ideas from probability theory in a qualitative setting.  The key idea is to use a qualitative Markov assumption, which says that state transitions are independent.  We show that a recent approach to modeling qualitative uncertainty using plausibility measures allows us to make such a qualitative Markov assumption in a relatively straightforward way, and show how the Markov assumption can be used to provide an attractive belief-change model.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3581",
        "title": "Theoretical Foundations for Abstraction-Based Probabilistic Planning",
        "authors": [
            "Vu A. Ha",
            "Peter Haddawy"
        ],
        "abstract": "Modeling worlds and actions under uncertainty is one of the central problems in the framework of decision-theoretic planning.  The representation must be general enough to capture real-world problems but at the same time it must provide a basis upon which theoretical results can be derived.  The central notion in the framework we propose here is that of the affine-operator, which serves as a tool for constructing (convex) sets of probability distributions, and which can be considered as a generalization of belief functions and interval mass assignments.  Uncertainty in the state of the worlds is modeled with sets of probability distributions, represented by affine-trees while actions are defined as tree-manipulators.  A small set of key properties of the affine-operator is presented, forming the basis for most existing operator-based definitions of probabilistic action projection and action abstraction.  We derive and prove correct three projection rules, which vividly illustrate the precision-complexity tradeoff in plan projection.  Finally, we show how the three types of action abstraction identified by Haddawy and Doan are manifested in the present framework.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3582",
        "title": "Why Is Diagnosis Using Belief Networks Insensitive to Imprecision In Probabilities?",
        "authors": [
            "Max Henrion",
            "Malcolm Pradhan",
            "Brendan del Favero",
            "Kurt Huang",
            "Gregory M. Provan",
            "Paul O'Rorke"
        ],
        "abstract": "Recent research has found that diagnostic performance with Bayesian belief networks is often surprisingly insensitive to imprecision in the numerical probabilities.  For example, the authors have recently completed an extensive study in which they applied random noise to the numerical probabilities in a set of belief networks for medical diagnosis, subsets of the CPCS network, a subset of the QMR (Quick Medical Reference) focused on liver and bile diseases.  The diagnostic performance in terms of the average probabilities assigned to the actual diseases showed small sensitivity even to large amounts of noise.  In this paper, we summarize the findings of this study and discuss possible explanations of this low sensitivity.  One reason is that the criterion for performance is average probability of the true hypotheses, rather than average error in probability, which is insensitive to symmetric noise distributions.  But, we show that even asymmetric, logodds-normal noise has modest effects.  A second reason is that the gold-standard posterior probabilities are often near zero or one, and are little disturbed by noise.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3583",
        "title": "Flexible Policy Construction by Information Refinement",
        "authors": [
            "Michael C. Horsch",
            "David L. Poole"
        ],
        "abstract": "We report on work towards flexible algorithms for solving decision problems represented as influence diagrams.  An algorithm is given to construct a tree structure for each decision node in an influence diagram.  Each tree represents a decision function and is constructed incrementally.  The improvements to the tree converge to the optimal decision function (neglecting computational costs) and the asymptotic behaviour is only a constant factor worse than dynamic programming techniques, counting the number of Bayesian network queries.  Empirical results show how expected utility increases with the size of the tree and the number of Bayesian net calculations.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3584",
        "title": "Efficient Search-Based Inference for Noisy-OR Belief Networks: TopEpsilon",
        "authors": [
            "Kurt Huang",
            "Max Henrion"
        ],
        "abstract": "Inference algorithms for arbitrary belief networks are impractical for large, complex belief networks.  Inference algorithms for specialized classes of belief networks have been shown to be more efficient.  In this paper, we present a search-based algorithm for approximate inference on arbitrary, noisy-OR belief networks, generalizing earlier work on search-based inference for two-level, noisy-OR belief networks.  Initial experimental results appear promising.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3585",
        "title": "A Probabilistic Model For Sensor Validation",
        "authors": [
            "Pablo H. Ibarguengoytia",
            "Luis Enrique Sucar",
            "Sunil Vadera"
        ],
        "abstract": "The validation of data from sensors has become an important issue in the operation and control of modern industrial plants.  One approach is to use knowledge based techniques to detect inconsistencies in measured data.  This article presents a probabilistic model for the detection of such inconsistencies.  Based on probability propagation, this method is able to find the existence of a possible fault among the set of sensors.  That is, if an error exists, many sensors present an apparent fault due to the propagation from the sensor(s) with a real fault.  So the fault detection mechanism can only tell if a sensor has a potential fault, but it can not tell if the fault is real or apparent.  So the central problem is to develop a theory, and then an algorithm, for distinguishing real and apparent faults, given that one or more sensors can fail at the same time.  This article then, presents an approach based on two levels: (i) probabilistic reasoning, to detect a potential fault, and (ii) constraint management, to distinguish the real fault from the apparent ones.  The proposed approach is exemplified by applying it to a power plant model.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3586",
        "title": "Computing Upper and Lower Bounds on Likelihoods in Intractable Networks",
        "authors": [
            "Tommi S. Jaakkola",
            "Michael I. Jordan"
        ],
        "abstract": "We present deterministic techniques for computing upper and lower bounds on marginal probabilities in sigmoid and noisy-OR networks.  These techniques become useful when the size of the network (or clique size) precludes exact computations.  We illustrate the tightness of the bounds by numerical experiments.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3587",
        "title": "MIDAS - An Influence Diagram for Management of Mildew in Winter Wheat",
        "authors": [
            "Allan Leck Jensen",
            "Finn Verner Jensen"
        ],
        "abstract": "We present a prototype of a decision support system for management of the fungal disease mildew in winter wheat.  The prototype is based on an influence diagram which is used to determine the optimal time and dose of mildew treatments.  This involves multiple decision opportunities over time, stochasticity, inaccurate information and incomplete knowledge.  The paper describes the practical and theoretical problems encountered during the construction of the influence diagram, and also the experience with the prototype.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3588",
        "title": "Computational Complexity Reduction for BN2O Networks Using Similarity of States",
        "authors": [
            "Alexander V. Kozlov",
            "Jaswinder Pal Singh"
        ],
        "abstract": "Although probabilistic inference in a general Bayesian belief network is an NP-hard problem, computation time for inference can be reduced in most practical cases by exploiting domain knowledge and by making approximations in the knowledge representation.  In this paper we introduce the property of similarity of states and a new method for approximate knowledge representation and inference which is based on this property.  We define two or more states of a node to be similar when the ratio of their probabilities, the likelihood ratio, does not depend on the instantiations of the other nodes in the network.  We show that the similarity of states exposes redundancies in the joint probability distribution which can be exploited to reduce the computation time of probabilistic inference in networks with multiple similar states, and that the computational complexity in the networks with exponentially many similar states might be polynomial.  We demonstrate our ideas on the example of a BN2O network -- a two layer network often used in diagnostic problems -- by reducing it to a very close network with multiple similar states.  We show that the answers to practical queries converge very fast to the answers obtained with the original network.  The maximum error is as low as 5% for models that require only 10% of the computation time needed by the original BN2O model.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3589",
        "title": "Uncertain Inferences and Uncertain Conclusions",
        "authors": [
            "Henry E. Kyburg Jr"
        ],
        "abstract": "Uncertainty may be taken to characterize inferences, their conclusions, their premises or all three.  Under some treatments of uncertainty, the inferences itself is never characterized by uncertainty.  We explore both the significance of uncertainty in the premises and in the conclusion of an argument that involves uncertainty.  We argue that for uncertainty to characterize the conclusion of an inference is natural, but that there is an interplay between uncertainty in the premises and uncertainty in the procedure of argument itself.  We show that it is possible in principle to incorporate all uncertainty in the premises, rendering uncertainty arguments deductively valid.  But we then argue (1) that this does not reflect human argument, (2) that it is computationally costly, and (3) that the gain in simplicity obtained by allowing uncertainty inference can sometimes outweigh the loss of flexibility it entails.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3591",
        "title": "Network Engineering for Complex Belief Networks",
        "authors": [
            "Suzanne M. Mahoney",
            "Kathryn Blackmond Laskey"
        ],
        "abstract": "Like any large system development effort, the construction of a complex belief network model requires systems engineering to manage the design and construction process.  We propose a rapid prototyping approach to network engineering.  We describe criteria for identifying network modules and the use of \"stubs\" to represent not-yet-constructed modules.  We propose an object oriented representation for belief networks which captures the semantics of the problem in addition to conditional independencies and probabilities.  Methods for evaluating complex belief network models are discussed.  The ideas are illustrated with examples from a large belief network construction problem in the military intelligence domain.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3592",
        "title": "Probabilistic Disjunctive Logic Programming",
        "authors": [
            "Liem Ngo"
        ],
        "abstract": "In this paper we propose a framework for combining Disjunctive Logic Programming and Poole's Probabilistic Horn Abduction.  We use the concept of hypothesis to specify the probability structure.  We consider the case in which probabilistic information is not available.  Instead of using probability intervals, we allow for the specification of the probabilities of disjunctions.  Because minimal models are used as characteristic models in disjunctive logic programming, we apply the principle of indifference on the set of minimal models to derive default probability values.  We define the concepts of explanation and partial explanation of a formula, and use them to determine the default probability distribution(s) induced by a program.  An algorithm for calculating the default probability of a goal is presented.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3594",
        "title": "Geometric Implications of the Naive Bayes Assumption",
        "authors": [
            "Mark Alan Peot"
        ],
        "abstract": "A naive (or Idiot) Bayes network is a network with a single hypothesis node and several observations that are conditionally independent given the hypothesis.  We recently surveyed a number of members of the UAI community and discovered a general lack of understanding of the implications of the Naive Bayes assumption on the kinds of problems that can be solved by these networks.  It has long been recognized [Minsky 61] that if observations are binary, the decision surfaces in these networks are hyperplanes.  We extend this result (hyperplane separability) to Naive Bayes networks with m-ary observations.  In addition, we illustrate the effect of observation-observation dependencies on decision surfaces.  Finally, we discuss the implications of these results on knowledge acquisition and research in learning.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3595",
        "title": "Identifying Independencies in Causal Graphs with Feedback",
        "authors": [
            "Judea Pearl",
            "Rina Dechter"
        ],
        "abstract": "We show that the d -separation criterion constitutes a valid test for conditional independence relationships that are induced by feedback systems involving discrete variables.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3596",
        "title": "A Graph-Theoretic Analysis of Information Value",
        "authors": [
            "Kim-Leng Poh",
            "Eric J. Horvitz"
        ],
        "abstract": "We derive qualitative relationships about the informational relevance of variables in graphical decision models based on a consideration of the topology of the models.  Specifically, we identify dominance relations for the expected value of information on chance variables in terms of their position and relationships in influence diagrams.  The qualitative relationships can be harnessed to generate nonnumerical procedures for ordering uncertain variables in a decision model by their informational relevance.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3597",
        "title": "A Framework for Decision-Theoretic Planning I: Combining the Situation Calculus, Conditional Plans, Probability and Utility",
        "authors": [
            "David L. Poole"
        ],
        "abstract": "This paper shows how we can combine logical representations of actions and decision theory in such a manner that seems natural for both.  In particular we assume an axiomatization of the domain in terms of situation calculus, using what is essentially Reiter's solution to the frame problem, in terms of the completion of the axioms defining the state change.  Uncertainty is handled in terms of the independent choice logic, which allows for independent choices and a logic program that gives the consequences of the choices.  As part of the consequences are a specification of the utility of (final) states.  The robot adopts robot plans, similar to the GOLOG programming language.  Within this logic, we can define the expected utility of a conditional plan, based on the axiomatization of the actions, the uncertainty and the utility.  The ?planning' problem is to find the plan with the highest expected utility.  This is related to recent structured representations for POMDPs; here we use stochastic situation calculus rules to specify the state transition function and the reward/value function.  Finally we show that with stochastic frame axioms, actions representations in probabilistic STRIPS are exponentially larger than using the representation proposed here.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3598",
        "title": "Optimal Monte Carlo Estimation of Belief Network Inference",
        "authors": [
            "Malcolm Pradhan",
            "Paul Dagum"
        ],
        "abstract": "We present two Monte Carlo sampling algorithms for probabilistic inference that guarantee polynomial-time convergence for a larger class of network than current sampling algorithms provide.  These new methods are variants of the known likelihood weighting algorithm.  We use of recent advances in the theory of optimal stopping rules for Monte Carlo simulation to obtain an inference approximation with relative error epsilon and a small failure probability delta.  We present an empirical evaluation of the algorithms which demonstrates their improved performance.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3599",
        "title": "A Discovery Algorithm for Directed Cyclic Graphs",
        "authors": [
            "Thomas S. Richardson"
        ],
        "abstract": "Directed acyclic graphs have been used fruitfully to represent causal strucures (Pearl 1988).  However, in the social sciences and elsewhere models are often used which correspond both causally and statistically to directed graphs with directed cycles (Spirtes 1995).  Pearl (1993) discussed predicting the effects of intervention in models of this kind, so-called linear non-recursive structural equation models.  This raises the question of whether it is possible to make inferences about causal structure with cycles, form sample data.  In particular do there exist general, informative, feasible and reliable precedures for inferring causal structure from conditional independence relations among variables in a sample generated by an unknown causal structure?  In this paper I present a discovery algorithm that is correct in the large sample limit, given commonly (but often implicitly) made plausible assumptions, and which provides information about the existence or non-existence of causal pathways from one variable to another.  The algorithm is polynomial on sparse graphs.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3600",
        "title": "A Polynomial-Time Algorithm for Deciding Markov Equivalence of Directed Cyclic Graphical Models",
        "authors": [
            "Thomas S. Richardson"
        ],
        "abstract": "Although the concept of d-separation was originally defined for directed acyclic graphs (see Pearl 1988), there is a natural extension of he concept to directed cyclic graphs.  When exactly the same set of d-separation relations hold in two directed graphs, no matter whether respectively cyclic or acyclic, we say that they are Markov equivalent.  In other words, when two directed cyclic graphs are Markov equivalent, the set of distributions that satisfy a natural extension of the Global Directed Markov condition (Lauritzen et al. 1990) is exactly the same for each graph.  There is an obvious exponential (in the number of vertices) time algorithm for deciding Markov equivalence of two directed cyclic graphs; simply chech all of the d-separation relations in each graph.  In this paper I state a theorem that gives necessary and sufficient conditions for the Markov equivalence of two directed cyclic graphs, where each of the conditions can be checked in polynomial time.  Hence, the theorem can be easily adapted into a polynomial time algorithm for deciding the Markov equivalence of two directed cyclic graphs.  Although space prohibits inclusion of correctness proofs, they are fully described in Richardson (1994b).\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3601",
        "title": "Coherent Knowledge Processing at Maximum Entropy by SPIRIT",
        "authors": [
            "Wilhelm Roedder",
            "Carl-Heinz Meyer"
        ],
        "abstract": "SPIRIT is an expert system shell for probabilistic knowledge bases.  Knowledge acquisition is performed by processing facts and rules on discrete variables in a rich syntax.  The shell generates a probability distribution which respects all acquired facts and rules and which maximizes entropy.  The user-friendly devices of SPIRIT to define variables, formulate rules and create the knowledge base are revealed in detail.  Inductive learning is possible.  Medium sized applications show the power of the system.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3602",
        "title": "Sample-and-Accumulate Algorithms for Belief Updating in Bayes Networks",
        "authors": [
            "Eugene Santos Jr.",
            "Solomon Eyal Shimony",
            "Edward Williams"
        ],
        "abstract": "Belief updating in Bayes nets, a well known computationally hard problem, has recently been approximated by several deterministic algorithms, and by various randomized approximation algorithms.  Deterministic algorithms usually provide probability bounds, but have an exponential runtime.  Some randomized schemes have a polynomial runtime, but provide only probability estimates.  We present randomized algorithms that enumerate high-probability partial instantiations, resulting in probability bounds.  Some of these algorithms are also sampling algorithms.  Specifically, we introduce and evaluate a variant of backward sampling, both as a sampling algorithm and as a randomized enumeration algorithm.  We also relax the implicit assumption used by both sampling and accumulation algorithms, that query nodes must be instantiated in all the samples.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3603",
        "title": "A Measure of Decision Flexibility",
        "authors": [
            "Ross D. Shachter",
            "Marvin Mandelbaum"
        ],
        "abstract": "We propose a decision-analytical approach to comparing the flexibility of decision situations from the perspective of a decision-maker who exhibits constant risk-aversion over a monetary value model.  Our approach is simple yet seems to be consistent with a variety of flexibility concepts, including robust and adaptive alternatives.  We try to compensate within the model for uncertainty that was not anticipated or not modeled.  This approach not only allows one to compare the flexibility of plans, but also guides the search for new, more flexible alternatives.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3604",
        "title": "Binary Join Trees",
        "authors": [
            "Prakash P. Shenoy"
        ],
        "abstract": "The main goal of this paper is to describe a data structure called binary join trees that are useful in computing multiple marginals efficiently using the Shenoy-Shafer architecture. We define binary join trees, describe their utility, and sketch a procedure for constructing them.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3605",
        "title": "Efficient Enumeration of Instantiations in Bayesian Networks",
        "authors": [
            "Sampath Srinivas",
            "Pandurang Nayak"
        ],
        "abstract": "Over the past several years Bayesian networks have been applied to a wide variety of problems.  A central problem in applying Bayesian networks is that of finding one or more of the most probable instantiations of a network.  In this paper we develop an efficient algorithm that incrementally enumerates the instantiations of a Bayesian network in decreasing order of probability.  Such enumeration algorithms are applicable in a variety of applications ranging from medical expert systems to model-based diagnosis.  Fundamentally, our algorithm is simply performing a lazy enumeration of the sorted list of all instantiations of the network.  This insight leads to a very concise algorithm statement which is both easily understood and implemented.  We show that for singly connected networks, our algorithm generates the next instantiation in time polynomial in the size of the network.  The algorithm extends to arbitrary Bayesian networks using standard conditioning techniques.  We empirically evaluate the enumeration algorithm and demonstrate its practicality.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3606",
        "title": "On Separation Criterion and Recovery Algorithm for Chain Graphs",
        "authors": [
            "Milan Studeny"
        ],
        "abstract": "Chain graphs give a natural unifying point of view on Markov and Bayesian networks and enlarge the potential of graphical models for description of conditional independence structures.  In the paper a direct graphical separation criterion for chain graphs, called c-separation, which generalizes the d-separation criterion for Bayesian networks is introduced (recalled).  It is equivalent to the classic moralization criterion for chain graphs and complete in sense that for every chain graph there exists a probability distribution satisfying exactly conditional independencies derivable from the chain graph by the c-separation criterion.  Every class of Markov equivalent chain graphs can be uniquely described by a natural representative, called the largest chain graph.  A recovery algorithm, which on basis of the (conditional) dependency model induced by an unknown chain graph finds the corresponding largest chain graph, is presented.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3607",
        "title": "Possible World Partition Sequences: A Unifying Framework for Uncertain Reasoning",
        "authors": [
            "Choh Man Teng"
        ],
        "abstract": "When we work with information from multiple sources, the formalism each employs to handle uncertainty may not be uniform.  In order to be able to combine these knowledge bases of different formats, we need to first establish a common basis for characterizing and evaluating the different formalisms, and provide a semantics for the combined mechanism.  A common framework can provide an infrastructure for building an integrated system, and is essential if we are to understand its behavior.  We present a unifying framework based on an ordered partition of possible worlds called partition sequences, which corresponds to our intuitive notion of biasing towards certain possible scenarios when we are uncertain of the actual situation. We show that some of the existing formalisms, namely, default logic, autoepistemic logic, probabilistic conditioning and thresholding (generalized conditioning), and possibility theory can be incorporated into this general framework.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3608",
        "title": "Supply Restoration in Power Distribution Systems - A Case Study in Integrating Model-Based Diagnosis and Repair Planning",
        "authors": [
            "Sylvie Thiebaux",
            "Marie-Odile Cordier",
            "Olivier Jehl",
            "Jean-Paul Krivine"
        ],
        "abstract": "Integrating diagnosis and repair is particularly crucial when gaining sufficient information to discriminate between several candidate diagnoses requires carrying out some repair actions.  A typical case is supply restoration in a faulty power distribution system.  This problem, which is a major concern for electricity distributors, features partial observability, and stochastic repair actions which are more elaborate than simple replacement of components.  This paper analyses the difficulties in applying existing work on integrating model-based diagnosis and repair and on planning in partially observable stochastic domains to this real-world problem, and describes the pragmatic approach we have retained so far.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3609",
        "title": "Real Time Estimation of Bayesian Networks",
        "authors": [
            "Robert L. Welch"
        ],
        "abstract": "For real time evaluation of a Bayesian network when there is not sufficient time to obtain an exact solution, a guaranteed response time, approximate solution is required.  It is shown that nontraditional methods utilizing estimators based on an archive of trial solutions and genetic search can provide an approximate solution that is considerably superior to the traditional Monte Carlo simulation methods.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3610",
        "title": "Testing Implication of Probabilistic Dependencies",
        "authors": [
            "Michael S. K. M. Wong"
        ],
        "abstract": "Axiomatization has been widely used for testing logical implications.  This paper suggests a non-axiomatic method, the chase, to test if a new dependency follows from a given set of probabilistic dependencies. Although the chase computation may require exponential time in some cases, this technique is a powerful tool for establishing nontrivial theoretical results. More importantly, this approach  provides valuable insight into the intriguing connection between relational databases and probabilistic reasoning systems.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3611",
        "title": "Optimal Factory Scheduling using Stochastic Dominance A*",
        "authors": [
            "Peter R. Wurman",
            "Michael P. Wellman"
        ],
        "abstract": "We examine a standard factory scheduling problem with stochastic processing and setup times, minimizing the expectation of the weighted number of tardy jobs.  Because the costs of operators in the schedule are stochastic and sequence dependent, standard dynamic programming algorithms such as A* may fail to find the optimal schedule.  The SDA* (Stochastic Dominance A*) algorithm remedies this difficulty by relaxing the pruning condition.  We present an improved state-space search formulation for these problems and discuss the conditions under which stochastic scheduling problems can be solved optimally using SDA*.  In empirical testing on randomly generated problems, we found that in 70%, the expected cost of the optimal stochastic solution is lower than that of the solution derived using a deterministic approximation, with comparable search effort.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3612",
        "title": "Critical Remarks on Single Link Search in Learning Belief Networks",
        "authors": [
            "Yang Xiang",
            "Michael S. K. M. Wong",
            "N. Cercone"
        ],
        "abstract": "In learning belief networks, the single link lookahead search is widely adopted to reduce the search space.  We show that there exists a class of probabilistic domain models which displays a special pattern of dependency.  We analyze the behavior of several learning algorithms using different scoring metrics such as the entropy, conditional independence, minimal description length and Bayesian metrics.  We demonstrate that single link lookahead search procedures (employed in these algorithms) cannot learn these models correctly.  Thus, when the underlying domain model actually belongs to this class, the use of a single link search procedure will result in learning of an incorrect model.  This may lead to inference errors when the model is used.  Our analysis suggests that if the prior knowledge about a domain does not rule out the possible existence of these models, a multi-link lookahead search or other heuristics should be used for the learning process.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3831",
        "title": "Quantum Entanglement in Concept Combinations",
        "authors": [
            "Diederik Aerts",
            "Sandro Sozzo"
        ],
        "abstract": "Research in the application of quantum structures to cognitive science confirms that these structures quite systematically appear in the dynamics of concepts and their combinations and quantum-based models faithfully represent experimental data of situations where classical approaches are problematical. In this paper, we analyze the data we collected in an experiment on a specific conceptual combination, showing that Bell's inequalities are violated in the experiment. We present a new refined entanglement scheme to model these data within standard quantum theory rules, where 'entangled measurements and entangled evolutions' occur, in addition to the expected 'entangled states', and present a full quantum representation in complex Hilbert space of the data. This stronger form of entanglement in measurements and evolutions might have relevant applications in the foundations of quantum theory, as well as in the interpretation of nonlocality tests. It could indeed explain some non-negligible 'anomalies' identified in EPR-Bell experiments.\n    ",
        "submission_date": "2013-02-15T00:00:00",
        "last_modified_date": "2013-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4381",
        "title": "Reasoning about Independence in Probabilistic Models of Relational Data",
        "authors": [
            "Marc Maier",
            "Katerina Marazopoulou",
            "David Jensen"
        ],
        "abstract": "We extend the theory of d-separation to cases in which data instances are not independent and identically distributed. We show that applying the rules of d-separation directly to the structure of probabilistic models of relational data inaccurately infers conditional independence. We introduce relational d-separation, a theory for deriving conditional independence facts from relational models. We provide a new representation, the abstract ground graph, that enables a sound, complete, and computationally efficient method for answering d-separation queries about relational models, and we present empirical results that demonstrate effectiveness.\n    ",
        "submission_date": "2013-02-18T00:00:00",
        "last_modified_date": "2014-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4421",
        "title": "Towards a theory of good SAT representations",
        "authors": [
            "Matthew Gwynne",
            "Oliver Kullmann"
        ],
        "abstract": "We aim at providing a foundation of a theory of \"good\" SAT representations F of boolean functions f. We argue that the hierarchy UC_k of unit-refutation complete clause-sets of level k, introduced by the authors, provides the most basic target classes, that is, F in UC_k is to be achieved for k as small as feasible. If F does not contain new variables, i.e., F is equivalent (as a CNF) to f, then F in UC_1 is similar to \"achieving (generalised) arc consistency\" known from the literature (it is somewhat weaker, but theoretically much nicer to handle). We show that for polysize representations of boolean functions in this sense, the hierarchy UC_k is strict. The boolean functions for these separations are \"doped\" minimally unsatisfiable clause-sets of deficiency 1; these functions have been introduced in [Sloan, Soerenyi, Turan, 2007], and we generalise their construction and show a correspondence to a strengthened notion of irredundant sub-clause-sets. Turning from lower bounds to upper bounds, we believe that many common CNF representations fit into the UC_k scheme, and we give some basic tools to construct representations in UC_1 with new variables, based on the Tseitin translation. Note that regarding new variables the UC_1-representations are stronger than mere \"arc consistency\", since the new variables are not excluded from consideration.\n    ",
        "submission_date": "2013-02-18T00:00:00",
        "last_modified_date": "2013-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4475",
        "title": "In Love With a Robot: the Dawn of Machine-To-Machine Marketing",
        "authors": [
            "Emil Kotomin"
        ],
        "abstract": "The article looks at mass market artificial intelligence tools in the context of their ever-growing sophistication, availability and market penetration. The subject is especially relevant today for these exact reasons - if a few years ago AI was the subject of high tech research and science fiction novels, today, we increasingly rely on cloud robotics to cater to our daily needs - to trade stock, predict weather, manage diaries, find friends and buy presents online.\n    ",
        "submission_date": "2013-02-18T00:00:00",
        "last_modified_date": "2016-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4928",
        "title": "Graphical Models for Preference and Utility",
        "authors": [
            "Fahiem Bacchus",
            "Adam J. Grove"
        ],
        "abstract": "Probabilistic independence can dramatically simplify the task of eliciting, representing, and computing with probabilities in large domains.  A key technique in achieving these benefits is the idea of graphical modeling.  We survey existing notions of independence for utility functions in a multi-attribute space, and suggest that these can be used to achieve similar advantages.  Our new results concern conditional additive independence, which we show always has a perfect representation as separation in an undirected graph (a Markov network).  Conditional additive independencies entail a particular functional for the utility function that is analogous to a product decomposition of a probability function, and confers analogous benefits.  This functional form has been utilized in the Bayesian network and influence diagram literature, but generally without an explanation in terms of independence.  The functional form yields a decomposition of the utility function that can greatly speed up expected utility calculations, particularly when the utility graph has a similar topology to the probabilistic network being used.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4929",
        "title": "Counterfactuals and Policy Analysis in Structural Models",
        "authors": [
            "Alexander Balke",
            "Judea Pearl"
        ],
        "abstract": "Evaluation of counterfactual queries (e.g., \"If A were true, would C have been true?\") is important to fault diagnosis, planning, determination of liability, and policy analysis.  We present a method of revaluating counterfactuals when the underlying causal model is represented by structural models - a nonlinear generalization of the simultaneous equations models commonly used in econometrics and social sciences.  This new method provides a coherent means for evaluating policies involving the control of variables which, prior to enacting the policy were influenced by other variables in the system.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4930",
        "title": "Belief Functions and Default Reasoning",
        "authors": [
            "Salem Benferhat",
            "Alessandro Saffiotti",
            "Philippe Smets"
        ],
        "abstract": "We present a new approach to dealing with default information based on the theory of belief functions.  Our semantic structures, inspired by Adams' epsilon-semantics, are epsilon-belief assignments, where values committed to focal elements are either close to 0 or close to 1.  We define two systems based on these structures, and relate them to other non-monotonic systems presented in the literature.  We show that our second system correctly addresses the well-known problems of specificity, irrelevance, blocking of inheritance, ambiguity, and redundancy.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4931",
        "title": "An Algebraic Semantics for Possibilistic Logic",
        "authors": [
            "Luca Boldrin",
            "Claudio Sossai"
        ],
        "abstract": "The first contribution of this paper is the presentation of a Pavelka - like formulation of possibilistic logic in which the language is naturally enriched by two connectives which represent negation (eg) and a new type of conjunction (otimes).  The space of truth values for this logic is the lattice of possibility functions, that, from an algebraic point of view, forms a quantal.  A second contribution comes from the understanding of the new conjunction as the combination of tokens of information coming from different sources, which makes our language \"dynamic\".  A Gentzen calculus is presented, which is proved sound and complete with respect to the given semantics. The problem of truth functionality is discussed in this context.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4932",
        "title": "Automating Computer Bottleneck Detection with Belief Nets",
        "authors": [
            "John S. Breese",
            "Russ Blake"
        ],
        "abstract": "We describe an application of belief networks to the diagnosis of bottlenecks in computer systems.  The technique relies on a high-level functional model of the interaction between application workloads, the Windows NT operating system, and system hardware.  Given a workload description, the model predicts the values of observable system counters available from the Windows NT performance monitoring tool.  Uncertainty in workloads, predictions, and counter values are characterized with Gaussian distributions. During diagnostic inference, we use observed performance monitor values to find the most probable assignment to the workload parameters.  In this paper we provide some background on automated bottleneck detection, describe the structure of the system model, and discuss empirical procedures for model calibration and verification.  Part of the calibration process includes generating a dataset to estimate a multivariate Gaussian error model.  Initial results in diagnosing bottlenecks are presented.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4933",
        "title": "Chain Graphs for Learning",
        "authors": [
            "Wray L. Buntine"
        ],
        "abstract": "Chain graphs combine directed and undirected graphs and their underlying mathematics combines properties of the two.  This paper gives a simplified definition of chain graphs based on a hierarchical combination of Bayesian (directed) and Markov (undirected) networks.  Examples of a chain graph are multivariate feed-forward networks, clustering with conditional interaction between variables, and forms of Bayes classifiers.  Chain graphs are then extended using the notation of plates so that samples and data analysis problems can be represented in a graphical model as well.  Implications for learning are discussed in the conclusion.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4934",
        "title": "Error Estimation in Approximate Bayesian Belief Network Inference",
        "authors": [
            "Enrique F. Castillo",
            "Remco R. Bouckaert",
            "Jose M. Sarabia",
            "Cristina Solares"
        ],
        "abstract": "We can perform inference in Bayesian belief networks by enumerating instantiations with high probability thus approximating the marginals.  In this paper, we present a method for determining the fraction of instantiations that has to be considered such that the absolute error in the marginals does not exceed a predefined value.  The method is based on extreme value theory. Essentially, the proposed method uses the reversed generalized Pareto distribution to model probabilities of instantiations below a given threshold. Based on this distribution, an estimate of the maximal absolute error if instantiations with probability smaller than u are disregarded can be made.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4935",
        "title": "Generating the Structure of a Fuzzy Rule under Uncertainty",
        "authors": [
            "Juan Luis Castro",
            "Jose Manuel Zurita"
        ],
        "abstract": "The aim of this paper is to present a method for identifying the structure of a rule in a fuzzy model.  For this purpose, an ATMS shall be used (Zurita 1994). An algorithm obtaining the identification of the structure will be suggested (Castro 1995).  The minimal structure of the rule (with respect to the number of variables that must appear in the rule) will be found by this algorithm.  Furthermore, the identification parameters shall be obtained  simultaneously.  The proposed method shall be applied for classification to an example.  The {em Iris Plant Database} shall be learnt for all three kinds of plants.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4936",
        "title": "Practical Model-Based Diagnosis with Qualitative Possibilistic Uncertainty",
        "authors": [
            "Didier Cayrac",
            "Didier Dubois",
            "Henri Prade"
        ],
        "abstract": "An approach to fault isolation that exploits vastly incomplete models is presented. It relies on separate descriptions of each component behavior, together with the links between them, which enables focusing of the reasoning to the relevant part of the system. As normal observations do not need explanation, the behavior of the components is limited to anomaly propagation.  Diagnostic solutions are disorders (fault modes or abnormal signatures) that are consistent with the observations, as well as abductive explanations.  An ordinal representation of uncertainty based on possibility theory provides a simple exception-tolerant description of the component behaviors. We can for instance distinguish between effects that are more or less certainly present (or absent) and effects that are more or less certainly present (or absent) when a given anomaly is present.  A realistic example illustrates the benefits of this approach.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4937",
        "title": "Decision Flexibility",
        "authors": [
            "Tom Chavez",
            "Ross D. Shachter"
        ],
        "abstract": "The development of new methods and representations for temporal decision-making requires a principled basis for characterizing and measuring the flexibility of decision strategies in the face of uncertainty. Our goal in this paper is to provide a framework - not a theory - for observing how decision policies behave in the face of informational perturbations, to gain clues as to how they might behave in the face of unanticipated, possibly unarticulated uncertainties.  To this end, we find it beneficial to distinguish between two types of uncertainty: \"Small World\" and \"Large World\" uncertainty. The first type can be resolved by posing an unambiguous question to a \"clairvoyant,\" and is anchored on some well-defined aspect of a decision frame. The second type is more troublesome, yet it is often of greater interest when we address the issue of flexibility; this type of uncertainty can be resolved only by consulting a \"psychic.\"  We next observe that one approach to flexibility used in the economics literature is already implicitly accounted for in the Maximum Expected Utility (MEU) principle from decision theory. Though simple, the observation establishes the context for a more illuminating notion of flexibility, what we term flexibility with respect to information revelation. We show how to perform flexibility analysis of a static (i.e., single period) decision problem using a simple example, and we observe that the most flexible alternative thus identified is not necessarily the MEU alternative.  We extend our analysis for a dynamic (i.e., multi-period) model, and we demonstrate how to calculate the value of flexibility for decision strategies that allow downstream revision of an upstream commitment decision.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4938",
        "title": "A Transformational Characterization of Equivalent Bayesian Network Structures",
        "authors": [
            "David Maxwell Chickering"
        ],
        "abstract": "We present a simple characterization of equivalent Bayesian network structures based on local transformations.  The significance of the characterization is twofold.  First, we are able to easily prove several new invariant properties of theoretical interest for equivalent structures.  Second, we use the characterization to derive an efficient algorithm that identifies all of the compelled edges in a structure.  Compelled edge identification is of particular importance for learning Bayesian network structures from data because these edges indicate causal relationships when certain assumptions hold.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4939",
        "title": "Conditioning Methods for Exact and Approximate Inference in Causal Networks",
        "authors": [
            "Adnan Darwiche"
        ],
        "abstract": "We present two algorithms for exact and approximate inference in causal networks.  The first algorithm, dynamic conditioning, is a refinement of cutset conditioning that has linear complexity on some networks for which cutset conditioning is exponential.  The second algorithm, B-conditioning, is an algorithm for approximate inference that allows one to trade-off the quality of approximations with the computation time. We also present some experimental results illustrating the properties of the proposed algorithms.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4940",
        "title": "Independence Concepts for Convex Sets of Probabilities",
        "authors": [
            "Luis M. de Campos",
            "Serafin Moral"
        ],
        "abstract": "In this paper we study different concepts of independence for convex sets of probabilities. There will be two basic ideas for independence.  The first is irrelevance.  Two variables are independent when a change on the knowledge about one variable does not affect the other.  The second one is factorization. Two variables are independent when the joint convex set of probabilities can be decomposed on the product of marginal convex sets. In the case of the Theory of Probability, these two starting points give rise to the same definition.  In the case of convex sets of probabilities, the resulting concepts will be strongly related, but they will not be equivalent.  As application of the concept of independence, we shall consider the problem of building  a global convex set from marginal convex sets of probabilities.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4941",
        "title": "Clustering Without (Thinking About) Triangulation",
        "authors": [
            "Denise L. Draper"
        ],
        "abstract": "The undirected technique for evaluating belief networks [Jensen, ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4942",
        "title": "Implementation of Continuous Bayesian Networks Using Sums of Weighted Gaussians",
        "authors": [
            "Eric Driver",
            "Darryl Morrell"
        ],
        "abstract": "Bayesian networks provide a method of representing conditional independence between random variables and computing the probability distributions associated with these random variables.  In this paper, we extend Bayesian network structures to compute probability density functions for continuous random variables.  We make this extension by approximating prior and conditional densities using sums of weighted Gaussian distributions and then finding the propagation rules for updating the densities in terms of these weights.  We present a simple example that illustrates the Bayesian network for continuous variables; this example shows the effect of the network structure and approximation errors on the computation of densities for variables in the network.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4943",
        "title": "Elicitation of Probabilities for Belief Networks: Combining Qualitative and Quantitative Information",
        "authors": [
            "Marek J. Druzdzel",
            "Linda C. van der Gaag"
        ],
        "abstract": "Although the usefulness of belief networks for reasoning under uncertainty is widely accepted, obtaining numerical probabilities that they require is still perceived a major obstacle.  Often not enough statistical data is available to allow for reliable probability estimation.  Available information may not be directly amenable for encoding in the network.  Finally, domain experts may be reluctant to provide numerical probabilities.  In this paper, we propose a method for elicitation of probabilities from a domain expert that is non-invasive and accommodates whatever probabilistic information the expert is willing to state.  We express all available information, whether qualitative or quantitative in nature, in a canonical form consisting of (in) equalities expressing constraints on the hyperspace of possible joint probability distributions.  We then use this canonical form to derive second-order probability distributions over the desired probabilities.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4944",
        "title": "Numerical Representations of Acceptance",
        "authors": [
            "Didier Dubois",
            "Henri Prade"
        ],
        "abstract": "Accepting a proposition means that our confidence in this proposition is strictly greater than the confidence in its negation.  This paper investigates the subclass of uncertainty measures, expressing confidence, that capture the idea of acceptance, what we call acceptance functions.  Due to the monotonicity property of confidence measures, the acceptance of a proposition entails the acceptance of any of its logical consequences.  In agreement with the idea that a belief set (in the sense of Gardenfors) must be closed under logical consequence, it is also required that the separate acceptance o two propositions entail the acceptance of their conjunction.  Necessity (and possibility) measures agree with this view of acceptance while probability and belief functions generally do not.  General properties of acceptance functions are estabilished.  The motivation behind this work is the investigation of a setting for belief revision more general than the one proposed by Alchourron, Gardenfors and Makinson, in connection with the notion of conditioning.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4945",
        "title": "Fraud/Uncollectible Debt Detection Using a Bayesian Network Based Learning System: A Rare Binary Outcome with Mixed Data Structures",
        "authors": [
            "Kazuo J. Ezawa",
            "Til Schuermann"
        ],
        "abstract": "The fraud/uncollectible debt problem in the telecommunications industry presents two technical challenges:  the detection and the treatment of the account given the detection.  In this paper, we focus on the first problem of detection using Bayesian network models, and we briefly discuss the application of a normative expert system for the treatment at the end.  We apply Bayesian network models to the problem of fraud/uncollectible debt detection for telecommunication services.  In addition to being quite successful at predicting rare event outcomes, it is able to handle a mixture of categorical and continuous data.  We present a performance comparison using linear and non-linear discriminant analysis, classification and regression trees, and Bayesian network models\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4946",
        "title": "A Constraint Satisfaction Approach to Decision under Uncertainty",
        "authors": [
            "Helene Fargier",
            "Jerome Lang",
            "Roger Martin-Clouaire",
            "Thomas Schiex"
        ],
        "abstract": "The Constraint Satisfaction Problem (CSP) framework offers a simple and sound basis for representing and solving simple decision problems, without uncertainty.  This paper is devoted to an extension of the CSP framework enabling us to deal with some decisions problems under uncertainty.  This extension relies on a differentiation between the agent-controllable decision variables and the uncontrollable parameters whose values depend on the occurrence of uncertain events. The uncertainty on the values of the parameters is assumed to be given under the form of a probability distribution. Two algorithms are given, for computing respectively decisions solving the problem with a maximal probability, and conditional decisions mapping the largest possible amount of possible cases to actual decisions.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4947",
        "title": "Plausibility Measures: A User's Guide",
        "authors": [
            "Nir Friedman",
            "Joseph Y. Halpern"
        ],
        "abstract": "We examine a new approach to modeling uncertainty based on plausibility measures, where a plausibility measure just associates with an event its plausibility, an element is some partially ordered set.  This approach is easily seen to generalize other approaches to modeling uncertainty, such as probability measures, belief functions, and possibility measures.  The lack of structure in a plausibility measure makes it easy for us to add structure on an \"as needed\" basis, letting us examine what is required to ensure that a plausibility measure has certain properties of interest.  This gives us insight into the essential features of the properties in question, while allowing us to prove general results that apply to many approaches to reasoning about uncertainty.  Plausibility measures have already proved useful in analyzing default reasoning.  In this paper, we examine their \"algebraic properties,\" analogues to the use of + and * in probability theory.  An understanding of such properties will be essential if plausibility measures are to be used in practice as a representation tool.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4948",
        "title": "Testing Identifiability of Causal Effects",
        "authors": [
            "David Galles",
            "Judea Pearl"
        ],
        "abstract": "This paper concerns the probabilistic evaluation of the effects of actions in the presence of unmeasured variables.  We show that the identification of causal effect between a singleton variable X and a set of variables Y can be accomplished systematically, in time polynomial in the number of variables in the graph.  When the causal effect is identifiable, a closed-form expression can be obtained for the probability that the action will achieve a specified goal, or a set of goals.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4949",
        "title": "A Characterization of the Dirichlet Distribution with Application to Learning Bayesian Networks",
        "authors": [
            "Dan Geiger",
            "David Heckerman"
        ],
        "abstract": "We provide a new characterization of the Dirichlet distribution.  This characterization implies that under assumptions made by several previous authors for learning belief networks, a Dirichlet prior on the parameters is inevitable.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4950",
        "title": "Fast Belief Update Using Order-of-Magnitude Probabilities",
        "authors": [
            "Moises Goldszmidt"
        ],
        "abstract": "We present an algorithm, called Predict, for updating beliefs in causal networks quantified with order-of-magnitude probabilities.  The algorithm takes advantage of both the structure and the quantification of the network and presents a polynomial asymptotic complexity.  Predict exhibits a conservative behavior in that it is always sound but not always complete.  We provide sufficient conditions for completeness and present algorithms for testing these conditions and for computing a complete set of plausible values.  We propose Predict as an efficient method to estimate probabilistic values and illustrate its use in conjunction with two known algorithms for probabilistic inference.  Finally, we describe an application of Predict to plan evaluation, present experimental results, and discuss issues regarding its use with conditional logics of belief, and in the characterization of irrelevance.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4951",
        "title": "Transforming Prioritized Defaults and Specificity into Parallel Defaults",
        "authors": [
            "Benjamin N. Grosof"
        ],
        "abstract": "We show how to transform any set of prioritized propositional defaults into an equivalent set of parallel (i.e., unprioritized) defaults, in circumscription.  We give an algorithm to implement the transform.  We show how to use the transform algorithm as a generator of a whole family of inferencing algorithms for circumscription.  The method is to employ the transform algorithm as a front end to any inferencing algorithm, e.g., one of the previously available, that handles the parallel (empty) case of prioritization.  Our algorithms provide not just coverage of a new expressive class, but also alternatives to previous algorithms for implementing the previously covered class (?layered?) of prioritization.  In particular, we give a new query-answering algorithm for prioritized cirumscription which is sound and complete for the full expressive class of unrestricted finite prioritization partial orders, for propositional defaults (or minimized predicates).  By contrast, previous algorithms required that the prioritization partial order be layered, i.e., structured similar to the system of rank in the military.  Our algorithm enables, for the first time, the implementation of the most useful class of prioritization: non-layered prioritization partial orders.  Default inheritance, for example, typically requires non-layered prioritization to represent specificity adequately.  Our algorithm enables not only the implementation of default inheritance (and specificity) within prioritized circumscription, but also the extension and combination of default inheritance with other kinds of prioritized default reasoning, e.g.: with stratified logic programs with negation-as-failure.  Such logic programs are previously known to be representable equivalently as layered-priority predicate circumscriptions.  Worst-case, the transform increases the number of defaults exponentially.  We discuss how inferencing is practically implementable nevertheless in two kinds of situations: general expressiveness but small numbers of defaults, or expressive special cases with larger numbers of defaults.  One such expressive special case is non-?top-heaviness? of the prioritization partial order.  In addition to its direct implementation, the transform can also be exploited analytically to generate special case algorithms, e.g., a tractable transform for a class within default inheritance (detailed in another, forthcoming paper).  We discuss other aspects of the significance of the fundamental result.  One can view the transform as reducing n degrees of partially ordered belief confidence to just 2 degrees of confidence: for-sure and (unprioritized) default.  Ordinary, parallel default reasoning, e.g., in parallel circumscription or Poole's Theorist, can be viewed in these terms as reducing 2 degrees of confidence to just 1 degree of confidence: that of the non-monotonic theory's conclusions.  The expressive reduction's computational complexity suggests that prioritization is valuable for its expressive conciseness, just as defaults are for theirs.  For Reiter's Default Logic and Poole's Theorist, the transform implies how to extend those formalisms so as to equip them with a concept of prioritization that is exactly equivalent to that in circumscription.  This provides an interesting alternative to Brewka's approach to equipping them with prioritization-type precedence.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4952",
        "title": "Efficient Decision-Theoretic Planning: Techniques and Empirical Analysis",
        "authors": [
            "Peter Haddawy",
            "AnHai Doan",
            "Richard Goodwin"
        ],
        "abstract": "This paper discusses techniques for performing efficient decision-theoretic planning.  We give an overview of the DRIPS decision-theoretic refinement planning system, which uses abstraction to efficiently identify optimal plans.  We present techniques for automatically generating search control information, which can significantly improve the planner's performance.  We evaluate the efficiency of DRIPS both with and without the search control rules on a complex medical planning problem and compare its performance to that of a branch-and-bound decision tree algorithm.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4953",
        "title": "Fuzzy Logic and Probability",
        "authors": [
            "Petr Hajek",
            "Lluis Godo",
            "Francesc Esteva"
        ],
        "abstract": "In this paper we deal with a new approach to probabilistic reasoning in a logical framework.  Nearly almost all logics of probability that have been proposed in the literature are based on classical two-valued logic.  After making clear the differences between fuzzy logic and probability theory, here we propose a {em fuzzy} logic of probability for which completeness results (in a probabilistic sense) are provided. The main idea behind this approach is that probability values of crisp propositions can be understood as truth-values of some suitable fuzzy propositions associated to the crisp ones.  Moreover, suggestions and examples of how to extend the formalism to cope with conditional probabilities and with other uncertainty formalisms are also provided.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4954",
        "title": "Probabilistic Temporal Reasoning with Endogenous Change",
        "authors": [
            "Steve Hanks",
            "David Madigan",
            "Jonathan Gavrin"
        ],
        "abstract": "This paper presents a probabilistic model for reasoning about the state of a system as it changes over time, both due to exogenous and endogenous influences.  Our target domain is a class of medical prediction problems that are neither so urgent as to preclude careful diagnosis nor progress so slowly as to allow arbitrary testing and treatment options.  In these domains there is typically enough time to gather information about the patient's state and consider alternative diagnoses and treatments, but the temporal interaction between the timing of tests, treatments, and the course of the disease must also be considered.  Our approach is to elicit a qualitative structural model of the patient from a human expert---the model identifies important attributes, the way in which exogenous changes affect attribute values, and the way in which the patient's condition changes endogenously.  We then elicit probabilistic information to capture the expert's uncertainty about the effects of tests and treatments and the nature and timing of endogenous state changes.  This paper describes the model in the context of a problem in treating vehicle accident trauma, and suggests a method for solving the model based on the technique of sequential imputation.  A complementary goal of this work is to understand and synthesize a disparate collection of research efforts all using the name ?probabilistic temporal reasoning.?  This paper analyzes related work and points out essential differences between our proposed model and other approaches in the literature.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4955",
        "title": "Toward a Characterization of Uncertainty Measure for the Dempster-Shafer Theory",
        "authors": [
            "David Harmanec"
        ],
        "abstract": "This is a working paper summarizing results of an ongoing research project whose aim is to uniquely characterize the uncertainty measure for the Dempster-Shafer Theory.  A set of intuitive axiomatic requirements is presented, some of their implications are shown, and the proof is given of the minimality of recently proposed measure AU among all measures satisfying the proposed requirements.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4956",
        "title": "A Definition and Graphical Representation for Causality",
        "authors": [
            "David Heckerman",
            "Ross D. Shachter"
        ],
        "abstract": "We present a precise definition of cause and effect in terms of a fundamental notion called unresponsiveness. Our definition is based on Savage's (1954) formulation of decision theory and departs from the traditional view of causation in that our causal assertions are made relative to a set of decisions. An important consequence of this departure is that we can reason about cause locally, not requiring a causal explanation for every dependency. Such local reasoning can be beneficial because it may not be necessary to determine whether a particular dependency is causal to make a decision. Also in this paper, we examine the graphical encoding of causal relationships. We show that influence diagrams in canonical form are an accurate and efficient representation of causal relationships. In addition, we establish a correspondence between canonical form and Pearl's causal theory.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4957",
        "title": "Learning Bayesian Networks: A Unification for Discrete and Gaussian Domains",
        "authors": [
            "David Heckerman",
            "Dan Geiger"
        ],
        "abstract": "We examine Bayesian methods for learning Bayesian networks from a combination of prior knowledge and statistical data. In particular, we unify the approaches we presented at last year's conference for discrete and Gaussian domains. We derive a general Bayesian scoring metric, appropriate for both domains. We then use this metric in combination with well-known statistical facts about the Dirichlet and normal--Wishart distributions to derive our metrics for discrete and Gaussian domains.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2021-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4958",
        "title": "A Bayesian Approach to Learning Causal Networks",
        "authors": [
            "David Heckerman"
        ],
        "abstract": "Whereas acausal Bayesian networks represent probabilistic independence, causal Bayesian networks represent causal relationships. In this paper, we examine Bayesian methods for learning both types of networks. Bayesian methods for learning acausal networks are fairly well developed. These methods often employ assumptions to facilitate the construction of priors, including the assumptions of parameter independence, parameter modularity, and likelihood equivalence. We show that although these assumptions also can be appropriate for learning causal networks, we need additional assumptions in order to learn causal networks. We introduce two sufficient assumptions, called {em mechanism independence} and {em component independence}. We show that these new assumptions, when combined with parameter independence, parameter modularity, and likelihood equivalence, allow us to apply methods for learning acausal networks to learn causal networks.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4959",
        "title": "Display of Information for Time-Critical Decision Making",
        "authors": [
            "Eric J. Horvitz",
            "Matthew Barry"
        ],
        "abstract": "We describe methods for managing the complexity of information displayed to people responsible for making high-stakes, time-critical decisions.  The techniques provide tools for real-time control of the configuration and quantity of information displayed to a user, and a methodology for designing flexible human-computer interfaces for monitoring applications.  After defining a prototypical set of display decision problems, we introduce the expected value of revealed information (EVRI) and the related measure of expected value of displayed information (EVDI).  We describe how these measures can be used to enhance computer displays used for monitoring complex systems.  We motivate the presentation by discussing our efforts to employ decision-theoretic control of displays for a time-critical monitoring application at the NASA Mission Control Center in Houston.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4960",
        "title": "Reasoning, Metareasoning, and Mathematical Truth: Studies of Theorem Proving under Limited Resources",
        "authors": [
            "Eric J. Horvitz",
            "Adrian Klein"
        ],
        "abstract": "In earlier work, we introduced flexible inference and decision-theoretic metareasoning to address the intractability of normative inference.  Here, rather than pursuing the task of computing beliefs and actions with decision models composed of distinctions about uncertain events, we examine methods for inferring beliefs about mathematical truth before an automated theorem prover completes a proof.  We employ a Bayesian analysis to update belief in truth, given theorem-proving progress, and show how decision-theoretic methods can be used to determine the value of continuing to deliberate versus taking immediate action in time-critical situations.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4961",
        "title": "Improved Sampling for Diagnostic Reasoning in Bayesian Networks",
        "authors": [
            "Mark Hulme"
        ],
        "abstract": "Bayesian networks offer great potential for use in automating large scale diagnostic reasoning tasks.  Gibbs sampling is the main technique used to perform diagnostic reasoning in large richly interconnected Bayesian networks.  Unfortunately Gibbs sampling can take an excessive time to generate a representative sample.  In this paper we describe and test a number of heuristic strategies for improving sampling in noisy-or Bayesian networks.  The strategies include Monte Carlo Markov chain sampling techniques other than Gibbs sampling.  Emphasis is put on strategies that can be implemented in distributed systems.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4962",
        "title": "Cautious Propagation in Bayesian Networks",
        "authors": [
            "Finn Verner Jensen"
        ],
        "abstract": "Consider the situation where some evidence e has been entered to a Bayesian network. When performing conflict analysis, sensitivity analysis, or when answering questions like \"What if the finding on X had been y instead of x?\" you need probabilities P (e'| h), where e' is a subset of e, and h is a configuration of a (possibly empty) set of variables.  Cautious propagation is a modification of HUGIN propagation into a Shafer-Shenoy-like architecture.  It is less efficient than HUGIN propagation; however, it provides easy access to P (e'| h) for a great deal of relevant subsets e'.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4963",
        "title": "Information/Relevance Influence Diagrams",
        "authors": [
            "Ali Jenzarli"
        ],
        "abstract": "In this paper we extend the influence diagram (ID) representation for decisions under uncertainty.  In the standard ID, arrows into a decision node are only informational; they do not represent constraints on what the decision maker can do.  We can represent such constraints only indirectly, using arrows to the children of the decision and sometimes adding more variables to the influence diagram, thus making the ID more complicated.  Users of influence diagrams often want to represent constraints by arrows into decision nodes.  We represent constraints on decisions by allowing relevance arrows into decision nodes.  We call the resulting representation information/relevance influence diagrams (IRIDs).  Information/relevance influence diagrams allow for direct representation and specification of constrained decisions.  We use a combination of stochastic dynamic programming and Gibbs sampling to solve IRIDs.  This method is especially useful when exact methods for solving IDs fail.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4965",
        "title": "Stochastic Simulation Algorithms for Dynamic Probabilistic Networks",
        "authors": [
            "Keiji Kanazawa",
            "Daphne Koller",
            "Stuart Russell"
        ],
        "abstract": "Stochastic simulation algorithms such as likelihood weighting often give fast, accurate approximations to posterior probabilities in probabilistic networks, and are the methods of choice for very large networks.  Unfortunately, the special characteristics of dynamic probabilistic networks (DPNs), which are used to represent stochastic temporal processes, mean that standard simulation algorithms perform very poorly.  In essence, the simulation trials diverge further and further from reality as the process is observed over time.  In this paper, we present simulation algorithms that use the evidence observed at each time step to push the set of trials back towards reality.  The first algorithm, \"evidence reversal\" (ER) restructures each time slice of the DPN so that the evidence nodes for the slice become ancestors of the state variables.  The second algorithm, called \"survival of the fittest\" sampling (SOF), \"repopulates\" the set of trials at each time step using a stochastic reproduction rate weighted by the likelihood of the evidence according to each trial.  We compare the performance of each algorithm with likelihood weighting on the original network, and also investigate the benefits of combining the ER and SOF methods.  The ER/SOF combination appears to maintain bounded error independent of the number of time steps in the simulation.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4966",
        "title": "Probabilistic Exploration in Planning while Learning",
        "authors": [
            "Grigoris I. Karakoulas"
        ],
        "abstract": "Sequential decision tasks with incomplete information are characterized by the exploration problem; namely the trade-off between further exploration for learning more about the environment and immediate exploitation of the accrued information for decision-making.  Within artificial intelligence, there has been an increasing interest in studying planning-while-learning algorithms for these decision tasks. In this paper we focus on the exploration problem in reinforcement learning and Q-learning in particular.  The existing exploration strategies for Q-learning are of a heuristic nature and they exhibit limited scaleability in tasks with large (or infinite) state and action spaces. Efficient experimentation is needed for resolving uncertainties when possible plans are compared (i.e. exploration).  The experimentation should be sufficient for selecting with statistical significance a locally optimal plan (i.e. exploitation).  For this purpose, we develop a probabilistic hill-climbing algorithm that uses a statistical selection procedure to decide how much exploration is needed for selecting a plan which is, with arbitrarily high probability, arbitrarily close to a locally optimal one. Due to its generality the algorithm can be employed for the exploration strategy of robust Q-learning. An experiment on a relatively complex control task shows that the proposed exploration strategy performs better than a typical exploration strategy. \n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4967",
        "title": "On the Detection of Conflicts in Diagnostic Bayesian Networks Using Abstraction",
        "authors": [
            "Young-Gyun Kim",
            "Marco Valtorta"
        ],
        "abstract": "An important issue in the use of expert systems is the so-called brittleness problem.  Expert systems model only a limited part of the world.  While the explicit management of uncertainty in expert systems itigates the brittleness problem, it is still possible for a system to be used, unwittingly, in ways that the system is not prepared to address.  Such a situation may be detected by the method of straw models, first presented by Jensen et al. [1990] and later generalized and justified by Laskey [1991].  We describe an algorithm, which we have implemented, that takes as input an annotated diagnostic Bayesian network (the base model) and constructs, without assistance, a bipartite network to be used as a straw model.  We show that in some cases this straw model is better that the independent straw model of Jensen et al., the only other straw model for which a construction algorithm has been designed and implemented.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4968",
        "title": "HUGS: Combining Exact Inference and Gibbs Sampling in Junction Trees",
        "authors": [
            "Uffe Kj\u00e6rulff"
        ],
        "abstract": "  Dawid, Kjaerulff and Lauritzen (1994) provided a preliminary description of a hybrid between Monte-Carlo sampling methods and exact local computations in junction trees. Utilizing the strengths of both methods, such hybrid inference methods has the potential of expanding the class of problems which can be solved under bounded resources as well as solving problems which otherwise resist exact solutions. The paper provides a detailed description of a particular instance of such a hybrid scheme; namely, combination of exact inference and Gibbs sampling in discrete Bayesian networks. We argue that this combination calls for an extension of the usual message passing scheme of ordinary junction trees.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4969",
        "title": "Sensitivities: An Alternative to Conditional Probabilities for Bayesian Belief Networks",
        "authors": [
            "Alexander V. Kozlov",
            "Jaswinder Pal Singh"
        ],
        "abstract": "We show an alternative way of representing a Bayesian belief network by sensitivities and probability distributions.  This representation is equivalent to the traditional representation by conditional probabilities, but makes dependencies between nodes apparent and intuitively easy to understand.  We also propose a QR matrix representation for the sensitivities and/or conditional probabilities which is more efficient, in both memory requirements and computational speed, than the traditional representation for computer-based implementations of probabilistic inference.  We use sensitivities to show that for a certain class of binary networks, the computation time for approximate probabilistic inference with any positive upper bound on the error of the result is independent of the size of the network.  Finally, as an alternative to traditional algorithms that use conditional probabilities, we describe an exact algorithm for probabilistic inference that uses the QR-representation for sensitivities and updates probability distributions of nodes in a network according to messages from the neighbors.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4970",
        "title": "Is There a Role for Qualitative Risk Assessment?",
        "authors": [
            "Paul J. Krause",
            "John Fox",
            "Philip Judson"
        ],
        "abstract": "Classically, risk is characterized by a point value probability indicating the likelihood of occurrence of an adverse effect.  However, there are domains where the attainability of objective numerical risk characterizations is increasingly being questioned. This paper reviews the arguments in favour of extending classical techniques of risk assessment to incorporate meaningful qualitative and weak quantitative risk characterizations.  A technique in which linguistic uncertainty terms are defined in terms of patterns of argument is then proposed.  The technique is demonstrated using a prototype computer-based system for predicting the carcinogenic risk due to novel chemical compounds.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4971",
        "title": "On the Complexity of Solving Markov Decision Problems",
        "authors": [
            "Michael L. Littman",
            "Thomas L. Dean",
            "Leslie Pack Kaelbling"
        ],
        "abstract": "Markov decision problems (MDPs) provide the foundations for a number of problems of interest to AI researchers studying automated planning and reinforcement learning.  In this paper, we summarize results regarding the complexity of solving MDPs and the running time of MDP solution algorithms.  We argue that, although MDPs can be solved efficiently in theory, more study is needed to reveal practical algorithms for solving large problems quickly.  To encourage future research, we sketch some alternative methods of analysis that rely on the structure of MDPs.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4972",
        "title": "Causal Inference and Causal Explanation with Background Knowledge",
        "authors": [
            "Christopher Meek"
        ],
        "abstract": "This paper presents correct algorithms for answering the following two questions; (i) Does there exist a causal explanation consistent with a set of background knowledge which explains all of the observed independence facts in a sample? (ii) Given that there is such a causal explanation what are the causal relationships common to every such causal explanation?\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4973",
        "title": "Strong Completeness and Faithfulness in Bayesian Networks",
        "authors": [
            "Christopher Meek"
        ],
        "abstract": "A completeness result for d-separation applied to discrete Bayesian networks is presented and it is shown that in a strong measure-theoretic sense almost all discrete distributions for a given network structure are faithful; i.e. the independence facts true of the distribution are all and only those entailed by the network structure.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4974",
        "title": "A Theoretical Framework for Context-Sensitive Temporal Probability Model Construction with Application to Plan Projection",
        "authors": [
            "Liem Ngo",
            "Peter Haddawy",
            "James Helwig"
        ],
        "abstract": "We define a context-sensitive temporal probability logic for representing classes of discrete-time temporal Bayesian networks.  Context constraints allow inference to be focused on only the relevant portions of the probabilistic knowledge.  We provide a declarative semantics for our language.  We present a Bayesian network construction algorithm whose generated networks give sound and complete answers to queries.  We use related concepts in logic programming to justify our approach.  We have implemented a Bayesian network construction algorithm for a subset of the theory and demonstrate it's application to the problem of evaluating the effectiveness of treatments for acute cardiac conditions.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4975",
        "title": "Refining Reasoning in Qualitative Probabilistic Networks",
        "authors": [
            "Simon Parsons"
        ],
        "abstract": "In recent years there has been a spate of papers describing systems for probabilisitic reasoning which do not use numerical probabilities.  In some cases the simple set of values used by these systems make it impossible to predict how a probability will change or which hypothesis is most likely given certain evidence.  This paper concentrates on such situations, and suggests a number of ways in which they may be resolved by refining the representation.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4976",
        "title": "On the Testability of Causal Models with Latent and Instrumental Variables",
        "authors": [
            "Judea Pearl"
        ],
        "abstract": "Certain causal models involving unmeasured variables induce no independence constraints among the observed variables but imply, nevertheless, inequality contraints on the observed distribution.  This paper derives a general formula for such instrumental variables, that is, exogenous variables that directly affect some variables but not all.  With the help of this formula, it is possible to test whether a model involving instrumental variables may account for the data, or, conversely, whether a given variables can be deemed instrumental.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4977",
        "title": "Probabilistic Evaluation of Sequential Plans from Causal Models with Hidden Variables",
        "authors": [
            "Judea Pearl",
            "James M. Robins"
        ],
        "abstract": "The paper concerns the probabilistic evaluation of plans in the presence of unmeasured variables, each plan consisting of several concurrent or sequential actions.  We establish a graphical criterion for recognizing when the effects of a given plan can be predicted from passive observations on measured variables only.  When the criterion is satisfied, a closed-form expression is provided for the probability that the plan will achieve a specified goal.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4978",
        "title": "Exploiting the Rule Structure for Decision Making within the Independent Choice Logic",
        "authors": [
            "David L. Poole"
        ],
        "abstract": "This paper introduces the independent choice logic, and in particular the \"single agent with nature\" instance of the independent choice logic, namely ICLdt.  This is a logical framework for decision making uncertainty that extends both logic programming and stochastic models such as influence diagrams.  This paper shows how the representation of a decision problem within the independent choice logic can be exploited to cut down the combinatorics of dynamic programming.  One of the main problems with influence diagram evaluation techniques is the need to optimise a decision for all values of the 'parents' of a decision variable.  In this paper we show how the rule based nature of the ICLdt can be exploited so that we only make distinctions in the values of the information available for a decision that will make a difference to utility.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4979",
        "title": "Abstraction in Belief Networks: The Role of Intermediate States in Diagnostic Reasoning",
        "authors": [
            "Gregory M. Provan"
        ],
        "abstract": "Bayesian belief networks are bing increasingly used as a knowledge representation for diagnostic reasoning.  One simple method for conducting diagnostic reasoning is to represent system faults and observations only.  In this paper, we investigate how having intermediate nodes-nodes other than fault and observation nodes affects the diagnostic performance of a Bayesian belief network.  We conducted a series of experiments on a set of real belief networks for medical diagnosis in liver and bile disease.  We compared the effects on diagnostic performance of a two-level network consisting just of disease and finding nodes with that of a network which models intermediate pathophysiological disease states as well.  We provide some theoretical evidence for differences observed between the abstracted two-level network and the full network.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4980",
        "title": "Accounting for Context in Plan Recognition, with Application to Traffic Monitoring",
        "authors": [
            "David V. Pynadath",
            "Michael P. Wellman"
        ],
        "abstract": "Typical approaches to plan recognition start from a representation of an agent's possible plans, and reason evidentially from observations of the agent's actions to assess the plausibility of the various candidates.  A more expansive view of the task (consistent with some prior work) accounts for the context in which the plan was generated, the mental state and planning process of the agent, and consequences of the agent's actions in the world.  We present a general Bayesian framework encompassing this view, and focus on how context can be exploited in plan recognition.  We demonstrate the approach on a problem in traffic monitoring, where the objective is to induce the plan of the driver from observation of vehicle movements.  Starting from a model of how the driver generates plans, we show how the highway context can appropriately influence the recognizer's interpretation of observed driver behavior.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4981",
        "title": "A New Pruning Method for Solving Decision Trees and Game Trees",
        "authors": [
            "Prakash P. Shenoy"
        ],
        "abstract": "The main goal of this paper is to describe a new pruning method for solving decision trees and game trees.  The pruning method for decision trees suggests a slight variant of decision trees that we call scenario trees.  In scenario trees, we do not need a conditional probability for each edge emanating from a chance node. Instead, we require a joint probability for each path from the root node to a leaf node. We compare the pruning method to the traditional rollback method for decision trees and game trees.  For problems that require Bayesian revision of probabilities, a scenario tree representation with the pruning method is more efficient than a decision tree representation with the rollback method. For game trees, the pruning method is more efficient than the rollback method.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4982",
        "title": "Directed Cyclic Graphical Representations of Feedback Models",
        "authors": [
            "Peter L. Spirtes"
        ],
        "abstract": "The use of directed acyclic graphs (DAGs) to represent conditional independence relations among random variables has proved fruitful in a variety of ways.  Recursive structural equation models are one kind of DAG model.  However, non-recursive structural equation models of the kinds used to model economic processes are naturally represented by directed cyclic graphs with independent errors, a characterization of conditional independence errors, a characterization of conditional independence constraints is obtained, and it is shown that the result generalizes in a natural way to systems in which the error variables or noises are statistically dependent.  For non-linear systems with independent errors a sufficient condition for conditional independence of variables in associated distributions is obtained.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4983",
        "title": "Causal Inference in the Presence of Latent Variables and Selection Bias",
        "authors": [
            "Peter L. Spirtes",
            "Christopher Meek",
            "Thomas S. Richardson"
        ],
        "abstract": "We show that there is a general, informative and reliable procedure for discovering causal relations when, for all the investigator knows, both latent variables and selection bias may be at work. Given information about conditional independence and dependence relations between measured variables, even when latent variables and selection bias may be present, there are sufficient conditions for reliably concluding that there is a causal path from one variable to another, and sufficient conditions for reliably concluding when no such causal path exists.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4984",
        "title": "Modeling Failure Priors and Persistence in Model-Based Diagnosis",
        "authors": [
            "Sampath Srinivas"
        ],
        "abstract": "Probabilistic model-based diagnosis computes the posterior probabilities of failure of components from the prior probabilities of component failure and observations of system behavior.  One problem with this method is that such priors are almost never directly available.  One of the reasons is that the prior probability estimates include an implicit notion of a time interval over which they are specified -- for example, if the probability of failure of a component is 0.05, is this over the period of a day or is this over a week?  A second problem facing probabilistic model-based diagnosis is the modeling of persistence.  Say we have an observation about a system at time t_1 and then another observation at a later time t_2.  To compute posterior probabilities that take into account both the observations, we need some model of how the state of the system changes from time t_1 to t_2. In this paper, we address these problems using techniques from Reliability theory.  We show how to compute the failure prior of a component from an empirical measure of its reliability -- the Mean Time Between Failure (MTBF).  We also develop a scheme to model persistence when handling multiple time tagged observations.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4985",
        "title": "A Polynomial Algorithm for Computing the Optimal Repair Strategy in a System with Independent Component Failures",
        "authors": [
            "Sampath Srinivas"
        ],
        "abstract": "The goal of diagnosis is to compute good repair strategies in response to anomalous system behavior. In a decision theoretic framework, a good repair strategy has low expected cost.  In a general formulation of the problem, the computation of the optimal (lowest expected cost) repair strategy for a system with multiple faults is intractable.  In this paper, we consider an interesting and natural restriction on the behavior of the system being diagnosed: (a) the system exhibits faulty behavior if and only if one or more components is malfunctioning. (b) The failures of the system components are independent.  Given this restriction on system behavior, we develop a polynomial time algorithm for computing the optimal repair strategy.  We then go on to introduce a system hierarchy and the notion of inspecting (testing) components before repair.  We develop a linear time algorithm for computing an optimal repair strategy for the hierarchical system which includes both repair and inspection.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4986",
        "title": "Exploiting System Hierarchy to Compute Repair Plans in Probabilistic Model-based Diagnosis",
        "authors": [
            "Sampath Srinivas",
            "Eric J. Horvitz"
        ],
        "abstract": "The goal of model-based diagnosis is to isolate causes of anomalous system behavior and recommend inexpensive repair actions in response.  In general, precomputing optimal repair policies is intractable.  To date, investigators addressing this problem have explored approximations that either impose restrictions on the system model (such as a single fault assumption) or compute an immediate best action with limited lookahead. In this paper, we develop a formulation of repair in model-based diagnosis and a repair algorithm that computes optimal sequences of actions. This optimal approach is costly but can be applied to precompute an optimal repair strategy for compact systems. We show how we can exploit a hierarchical system specification to make this approach tractable for large systems.  When introducing hierarchy, we also consider the tradeoff between simply replacing a component and decomposing it to repair its subcomponents. The hierarchical repair algorithm is suitable for off-line precomputation of an optimal repair strategy.  A modification of the algorithm takes advantage of an iterative deepening scheme to trade off inference time and the quality of the computed strategy.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4987",
        "title": "Path Planning under Time-Dependent Uncertainty",
        "authors": [
            "Michael P. Wellman",
            "Matthew Ford",
            "Kenneth Larson"
        ],
        "abstract": "Standard algorithms for finding the shortest path in a graph require that the cost of a path be additive in edge costs, and typically assume that costs are deterministic.  We consider the problem of uncertain edge costs, with potential probabilistic dependencies among the costs.  Although these dependencies violate the standard dynamic-programming decomposition, we identify a weaker stochastic consistency condition that justifies a generalized dynamic-programming approach based on stochastic dominance.  We present a revised path-planning algorithm and prove that it produces optimal paths under time-dependent uncertain costs.  We test the algorithm by applying it to a model of stochastic bus networks, and present empirical performance results comparing it to some alternatives.  Finally, we consider extensions of these concepts to a more general class of problems of heuristic search under uncertainty.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4988",
        "title": "Defaults and Infinitesimals: Defeasible Inference by Nonarchimedean Entropy-Maximization",
        "authors": [
            "Emil Weydert"
        ],
        "abstract": "We develop a new semantics for defeasible inference based on extended probability measures allowed to take infinitesimal values, on the interpretation of defaults as generalized conditional probability constraints and on a preferred-model implementation of entropy maximization.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4989",
        "title": "An Order of Magnitude Calculus",
        "authors": [
            "Nic Wilson"
        ],
        "abstract": "This paper develops a simple calculus for order of magnitude reasoning.  A semantics is given with soundness and completeness results.  Order of magnitude probability functions are easily defined and turn out to be equivalent to kappa functions, which are slight generalizations of Spohn's Natural Conditional Functions.  The calculus also gives rise to an order of magnitude decision theory, which can be used to justify an amended version of Pearl's decision theory for kappa functions, although the latter is weaker and less expressive.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4990",
        "title": "A Method for Implementing a Probabilistic Model as a Relational Database",
        "authors": [
            "Michael S. K. M. Wong",
            "C. J. Butz",
            "Yang Xiang"
        ],
        "abstract": "This paper discusses a method for implementing a probabilistic inference system based on an extended relational data model.  This model provides a unified approach for a variety of applications such as dynamic programming, solving sparse linear equations, and constraint propagation. In this framework, the probability model is represented as a generalized relational database.  Subsequent probabilistic requests can be processed as standard relational queries.  Conventional database management systems can be easily adopted for implementing such an approximate reasoning system.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4991",
        "title": "Optimization of Inter-Subnet Belief Updating in Multiply Sectioned Bayesian Networks",
        "authors": [
            "Yang Xiang"
        ],
        "abstract": "Recent developments show that Multiply Sectioned Bayesian Networks (MSBNs) can be used for diagnosis of natural systems as well as for model-based diagnosis of artificial systems.  They can be applied to single-agent oriented reasoning systems as well as multi-agent distributed probabilistic reasoning systems.  Belief propagation between a pair of subnets plays a central role in maintenance of global consistency in a MSBN.  This paper studies the operation UpdateBelief, presented originally with MSBNs, for inter-subnet propagation.  We analyze how the operation achieves its intended functionality, which provides hints as for how its efficiency can be improved.  We then define two new versions of UpdateBelief that reduce the computation time for inter-subnet propagation.  One of them is optimal in the sense that the minimum amount of computation for coordinating multi-linkage belief propagation is required.  The optimization problem is solved through the solution of a graph-theoretic problem: the minimum weight open tour in a tree.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4992",
        "title": "Generating Explanations for Evidential Reasoning",
        "authors": [
            "Hong Xu",
            "Philippe Smets"
        ],
        "abstract": "In this paper, we present two methods to provide explanations for reasoning with belief functions in the valuation-based systems.  One approach, inspired by Strat's method, is based on sensitivity analysis, but its computation is simpler thus easier to implement than Strat's.  The other one is to examine the impact of evidence on the conclusion based on the measure of the information content in the evidence.  We show the property of additivity for the pieces of evidence that are conditional independent within the context of the valuation-based systems.  We will give an example to show how these approaches are applied in an evidential network. \n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4993",
        "title": "Inference with Causal Independence in the CPSC Network",
        "authors": [
            "Nevin Lianwen Zhang"
        ],
        "abstract": "This paper reports experiments with the causal independence inference algorithm proposed by Zhang and Poole (1994b) on the CPSC network created by Pradhan et al. (1994).  It is found that the algorithm is able to answer 420 of the 422 possible zero-observation queries, 94 of 100 randomly generated five-observation queries, 87 of 100 randomly generated ten-observation queries, and 69 of 100 randomly generated twenty-observation queries.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.5215",
        "title": "Development Of Ontology-Based Intelligent System For Software Testing",
        "authors": [
            "A. Anandaraj",
            "P. Kalaivani",
            "V. Rameshkumar"
        ],
        "abstract": "Software testing is a prime factor in software industry. Besides knowing the importance of testing, only limited time is allocated for teaching it. It will be more efficient if testing is taught simultaneously with programming foundations. This integrated learning of testing techniques and programming allows the programmers to perform in a better way and this leads to the improvement of the performance of the industry progress. In this paper, a technique named ontology is introduced, it first defines the various testing process in hierarchy and define relationships among them, to share and reuse the knowledge that is captured, secondly metadata is created by natural language processing and finally, the application use ontologies to support test management, it act as knowledge base for multiple environment with the integrated teaching of programming foundation and testing concepts. Keywords: Meta Data, Ontology, Software Testing, Integration, Programming Foundations.\n    ",
        "submission_date": "2013-02-21T00:00:00",
        "last_modified_date": "2013-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.5417",
        "title": "An Ontology Construction Approach for the Domain Of Poultry Science Using Protege",
        "authors": [
            "P. Kalaivani",
            "A. Anandaraj",
            "K. Raja"
        ],
        "abstract": "The information retrieval systems that are present nowadays are mainly based on full text matching of keywords or topic based classification. This matching of keywords often returns a large number of irrelevant information and this does not meet the users query requirement. In order to solve this problem and to enhance the search using semantic environment, a technique named ontology is implemented for the field of poultry in this paper. Ontology is an emerging technique in the current field of research in semantic environment. This paper constructs ontology using the tool named Protege version 4.0 and this also generates Resource Description Framework schema and XML scripts for using poultry ontology in web.\n    ",
        "submission_date": "2013-02-21T00:00:00",
        "last_modified_date": "2013-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.5824",
        "title": "Measuring Visual Complexity of Cluster-Based Visualizations",
        "authors": [
            "B. Duffy",
            "A. Dasgupta",
            "R. Kosara",
            "S. Walton",
            "M. Chen"
        ],
        "abstract": "Handling visual complexity is a challenging problem in visualization owing to the subjectiveness of its definition and the difficulty in devising generalizable quantitative metrics. In this paper we address this challenge by measuring the visual complexity of two common forms of cluster-based visualizations: scatter plots and parallel coordinatess. We conceptualize visual complexity as a form of visual uncertainty, which is a measure of the degree of difficulty for humans to interpret a visual representation correctly. We propose an algorithm for estimating visual complexity for the aforementioned visualizations using Allen's interval algebra. We first establish a set of primitive 2-cluster cases in scatter plots and another set for parallel coordinatess based on symmetric isomorphism. We confirm that both are the minimal sets and verify the correctness of their members computationally. We score the uncertainty of each primitive case based on its topological properties, including the existence of overlapping regions, splitting regions and meeting points or edges. We compare a few optional scoring schemes against a set of subjective scores by humans, and identify the one that is the most consistent with the subjective scores. Finally, we extend the 2-cluster measure to k-cluster measure as a general purpose estimator of visual complexity for these two forms of cluster-based visualization.\n    ",
        "submission_date": "2013-02-23T00:00:00",
        "last_modified_date": "2013-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6214",
        "title": "Modification of conceptual clustering algorithm Cobweb for numerical data using fuzzy membership function",
        "authors": [
            "A.V. Korobeynikov",
            "I.I. Islamgaliev"
        ],
        "abstract": " Modification of a conceptual clustering algorithm Cobweb for the purpose of its application for numerical data is offered. Keywords: clustering, algorithm Cobweb, numerical data, fuzzy membership function.\n    ",
        "submission_date": "2013-02-25T00:00:00",
        "last_modified_date": "2013-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6442",
        "title": "A Modelling Approach Based on Fuzzy Agents",
        "authors": [
            "Alain-J\u00e9r\u00f4me Foug\u00e8res"
        ],
        "abstract": "Modelling of complex systems is mainly based on the decomposition of these systems in autonomous elements, and the identification and definitio9n of possible interactions between these elements. For this, the agent-based approach is a modelling solution often proposed. Complexity can also be due to external events or internal to systems, whose main characteristics are uncertainty, imprecision, or whose perception is subjective (i.e. interpreted). Insofar as fuzzy logic provides a solution for modelling uncertainty, the concept of fuzzy agent can model both the complexity and uncertainty. This paper focuses on introducing the concept of fuzzy agent: a classical architecture of agent is redefined according to a fuzzy perspective. A pedagogical illustration of fuzzy agentification of a smart watering system is then proposed.\n    ",
        "submission_date": "2013-02-26T00:00:00",
        "last_modified_date": "2013-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6595",
        "title": "Combining Multiple Time Series Models Through A Robust Weighted Mechanism",
        "authors": [
            "Ratnadip Adhikari",
            "R. K. Agrawal"
        ],
        "abstract": "Improvement of time series forecasting accuracy through combining multiple models is an important as well as a dynamic area of research. As a result, various forecasts combination methods have been developed in literature. However, most of them are based on simple linear ensemble strategies and hence ignore the possible relationships between two or more participating models. In this paper, we propose a robust weighted nonlinear ensemble technique which considers the individual forecasts from different models as well as the correlations among them while combining. The proposed ensemble is constructed using three well-known forecasting models and is tested for three real-world time series. A comparison is made among the proposed scheme and three other widely used linear combination methods, in terms of the obtained forecast errors. This comparison shows that our ensemble scheme provides significantly lower forecast errors than each individual model as well as each of the four linear combination methods.\n    ",
        "submission_date": "2013-02-26T00:00:00",
        "last_modified_date": "2013-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6602",
        "title": "Using Modified Partitioning Around Medoids Clustering Technique in Mobile Network Planning",
        "authors": [
            "Lamiaa Fattouh Ibrahim",
            "Manal Hamed Al Harbi"
        ],
        "abstract": "Every cellular network deployment requires planning and optimization in order to provide adequate coverage, capacity, and quality of service (QoS). Optimization mobile radio network planning is a very complex task, as many aspects must be taken into account. With the rapid development in mobile network we need effective network planning tool to satisfy the need of customers. However, deciding upon the optimum placement for the base stations (BS s) to achieve best services while reducing the cost is a complex task requiring vast computational resource. This paper introduces the spatial clustering to solve the Mobile Networking Planning problem. It addresses antenna placement problem or the cell planning problem, involves locating and configuring infrastructure for mobile networks by modified the original Partitioning Around Medoids PAM algorithm. M-PAM (Modified Partitioning Around Medoids) has been proposed to satisfy the requirements and constraints. PAM needs to specify number of clusters (k) before starting to search for the best locations of base stations. The M-PAM algorithm uses the radio network planning to determine k. We calculate for each cluster its coverage and capacity and determine if they satisfy the mobile requirements, if not we will increase (k) and reapply algorithms depending on two methods for clustering. Implementation of this algorithm to a real case study is presented. Experimental results and analysis indicate that the M-PAM algorithm when applying method two is effective in case of heavy load distribution, and leads to minimum number of base stations, which directly affected onto the cost of planning the network.\n    ",
        "submission_date": "2013-02-26T00:00:00",
        "last_modified_date": "2013-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6779",
        "title": "An Evaluation of an Algorithm for Inductive Learning of Bayesian Belief Networks Usin",
        "authors": [
            "Constantin F. Aliferis",
            "Gregory F. Cooper"
        ],
        "abstract": "Bayesian learning of belief networks (BLN) is a method for automatically constructing belief networks (BNs) from data using search and Bayesian scoring techniques.  K2 is a particular instantiation of the method that implements a greedy search strategy.  To evaluate the accuracy of K2, we randomly generated a number of BNs and for each of those we simulated data sets. K2 was then used to induce the generating BNs from the simulated data.  We examine the performance of the program, and the factors that influence it.  We also present a simple BN model, developed from our results, which predicts the accuracy of K2, when given various characteristics of the data set.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6780",
        "title": "Probabilistic Constraint Satisfaction with Non-Gaussian Noise",
        "authors": [
            "Russ B. Altman",
            "Cheng C. Chen",
            "William B. Poland",
            "Jaswinder Pal Singh"
        ],
        "abstract": "We have previously reported a Bayesian algorithm for determining the coordinates of points in three-dimensional space from uncertain constraints.  This method is useful in the determination of biological molecular structure.  It is limited, however, by the requirement that the uncertainty in the constraints be normally distributed. In this paper, we present an extension of the original algorithm that allows constraint uncertainty to be represented as a mixture of Gaussians, and thereby allows arbitrary constraint distributions.  We illustrate the performance of this algorithm on a problem drawn from the domain of molecular structure determination, in which a multicomponent constraint representation produces a much more accurate solution than the old single component mechanism.  The new mechanism uses mixture distributions to decompose the problem into a set of independent problems with unimodal constraint uncertainty.  The results of the unimodal subproblems are periodically recombined using Bayes' law, to avoid combinatorial explosion.  The new algorithm is particularly suited for parallel implementation.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6781",
        "title": "A Bayesian Method Reexamined",
        "authors": [
            "Derek D. Ayers"
        ],
        "abstract": "This paper examines the \"K2\" network scoring metric of Cooper and Herskovits.  It shows counterintuitive results from applying this metric to simple networks.  One family of noninformative priors is suggested for assigning equal scores to equivalent networks.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6782",
        "title": "Laplace's Method Approximations for Probabilistic Inference in Belief Networks with Continuous Variables",
        "authors": [
            "Adriano Azevedo-Filho",
            "Ross D. Shachter"
        ],
        "abstract": "Laplace's method, a family of asymptotic methods used to approximate integrals, is presented as a potential candidate for the tool box of techniques used for knowledge acquisition and probabilistic inference in belief networks with continuous variables.  This technique approximates posterior moments and marginal posterior distributions with reasonable accuracy [errors are O(n^-2) for posterior means] in many interesting cases. The method also seems promising for computing approximations for Bayes factors for use in the context of model selection, model uncertainty and mixtures of pdfs.  The limitations, regularity conditions and computational difficulties for the implementation of Laplace's method are comparable to those associated with the methods of maximum likelihood and posterior mode analysis.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6783",
        "title": "Generating New Beliefs From Old",
        "authors": [
            "Fahiem Bacchus",
            "Adam J. Grove",
            "Joseph Y. Halpern",
            "Daphne Koller"
        ],
        "abstract": "In previous work [BGHK92, BGHK93], we have studied the random-worlds approach -- a particular (and quite powerful) method for generating degrees of belief (i.e., subjective probabilities) from a knowledge base consisting of objective (first-order, statistical, and default) information.  But allowing a knowledge base to contain only objective information is sometimes limiting.  We occasionally wish to include information about degrees of belief in the knowledge base as well, because there are contexts in which old beliefs represent important information that should influence new beliefs.  In this paper, we describe three quite general techniques for extending a method that generates degrees of belief from objective information to one that can make use of degrees of belief as well. All of our techniques are bloused on well-known approaches, such as cross-entropy.  We discuss general connections between the techniques and in particular show that, although conceptually and technically quite different, all of the techniques give the same answer when applied to the random-worlds method.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6784",
        "title": "Counterfactual Probabilities: Computational Methods, Bounds and Applications",
        "authors": [
            "Alexander Balke",
            "Judea Pearl"
        ],
        "abstract": "Evaluation of counterfactual queries (e.g., \"If A were true, would C have been true?\") is important to fault diagnosis, planning, and determination of liability.  In this paper we present methods for computing the probabilities of such queries using the formulation proposed in [Balke and Pearl, 1994], where the antecedent of the query is interpreted as an external action that forces the proposition A to be true.  When a prior probability is available on the causal mechanisms governing the domain, counterfactual probabilities can be evaluated precisely.  However, when causal knowledge is specified as conditional probabilities on the observables, only bounds can computed.  This paper develops techniques for evaluating these bounds, and demonstrates their use in two applications: (1) the determination of treatment efficacy from studies in which subjects may choose their own treatment, and (2) the determination of liability in product-safety litigation.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6786",
        "title": "Modus Ponens Generating Function in the Class of ^-valuations of Plausibility",
        "authors": [
            "Ildar Z. Batyrshin"
        ],
        "abstract": "We discuss the problem of construction of inference procedures which can manipulate with uncertainties measured in ordinal scales and fulfill to the property of strict monotonicity of conclusion.  The class of A-valuations of plausibility is considered where operations based only on information about linear ordering of plausibility values are used.  In this class the modus ponens generating function fulfiling to the property of strict monotonicity of conclusions is introduced.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6787",
        "title": "Approximation Algorithms for the Loop Cutset Problem",
        "authors": [
            "Ann Becker",
            "Dan Geiger"
        ],
        "abstract": "We show how to find a small loop curser in a Bayesian network.  Finding such a loop cutset is the first step in the method of conditioning for inference.  Our algorithm for finding a loop cutset, called MGA, finds a loop cutset which is guaranteed in the worst case to contain less than twice the number of variables contained in a minimum loop cutset.  We test MGA on randomly generated graphs and find that the average ratio between the number of instances associated with the algorithms' output and the number of instances associated with a minimum solution is 1.22.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6788",
        "title": "Possibility and Necessity Functions over Non-classical Logics",
        "authors": [
            "Philippe Besnard",
            "Jerome Lang"
        ],
        "abstract": "We propose an integration of possibility theory into non-classical logics.  We obtain many formal results that generalize the case where possibility and necessity functions are based on classical logic.  We show how useful such an approach is by applying it to reasoning under uncertain and inconsistent information.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6789",
        "title": "Exploratory Model Building",
        "authors": [
            "Raj Bhatnagar"
        ],
        "abstract": "Some instances of creative thinking require an agent to build and test hypothetical theories.  Such a reasoner needs to explore the space of not only those situations that have occurred in the past, but also those that are rationally conceivable.  In this paper we present a formalism for exploring the space of conceivable situation-models for those domains in which the knowledge is primarily probabilistic in nature.  The formalism seeks to construct consistent, minimal, and desirable situation-descriptions by selecting suitable domain-attributes and dependency relationships from the available domain knowledge.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6791",
        "title": "Planning with External Events",
        "authors": [
            "Jim S. Blythe"
        ],
        "abstract": "I describe a planning methodology for domains with uncertainty in the form of external events that are not completely predictable.  The events are represented by enabling conditions and probabilities of occurrence.  The planner is goal-directed and backward chaining, but the subgoals are suggested by analyzing the probability of success of the partial plan rather than being simply the open conditions of the operators in the plan.  The partial plan is represented as a Bayesian belief net to compute its probability of success. Since calculating the probability of success of a plan can be very expensive I introduce two other techniques for computing it, one that uses Monte Carlo simulation to estimate it and one based on a Markov chain representation that uses knowledge about the dependencies between the predicates describing the domain.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6792",
        "title": "Properties of Bayesian Belief Network Learning Algorithms",
        "authors": [
            "Remco R. Bouckaert"
        ],
        "abstract": "Bayesian belief network learning algorithms have three basic components: a measure of a network structure and a database, a search heuristic that chooses network structures to be considered, and a method of estimating the probability tables from the database.  This paper contributes to all these three topics.  The behavior of the Bayesian measure of Cooper and Herskovits and a minimum description length (MDL) measure are compared with respect to their properties for both limiting size and finite size databases.  It is shown that the MDL measure has more desirable properties than the Bayesian measure when a distribution is to be learned.  It is shown that selecting belief networks with certain minimallity properties is NP-hard.  This result justifies the use of search heuristics instead of exact algorithms for choosing network structures to be considered.  In some cases, a collection of belief networks can be represented by a single belief network which leads to a new kind of probability table estimation called smoothing.  We argue that smoothing can be efficiently implemented by incorporating it in the search heuristic.  Experimental results suggest that for learning probabilities of belief networks smoothing is helpful.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6793",
        "title": "A Stratified Simulation Scheme for Inference in Bayesian Belief Networks",
        "authors": [
            "Remco R. Bouckaert"
        ],
        "abstract": "Simulation schemes for probabilistic inference in Bayesian belief networks offer many advantages over exact algorithms; for example, these schemes have a linear and thus predictable runtime while exact algorithms have exponential runtime.  Experiments have shown that likelihood weighting is one of the most promising simulation schemes.  In this paper, we present a new simulation scheme that generates samples more evenly spread in the sample space than the likelihood weighting scheme.  We show both theoretically and experimentally that the stratified scheme outperforms likelihood weighting in average runtime and error in estimates of beliefs.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6794",
        "title": "Efficient Estimation of the Value of Information in Monte Carlo Models",
        "authors": [
            "Tom Chavez",
            "Max Henrion"
        ],
        "abstract": "The expected value of information (EVI) is the most powerful measure of sensitivity to uncertainty in a decision model: it measures the potential of information to improve the decision, and hence measures the expected value of outcome.  Standard methods for computing EVI use discrete variables and are computationally intractable for models that contain more than a few variables.  Monte Carlo simulation provides the basis for more tractable evaluation of large predictive models with continuous and discrete variables, but so far computation of EVI in a Monte Carlo setting also has appeared impractical.  We introduce an approximate approach based on pre-posterior analysis for estimating EVI in Monte Carlo models.  Our method uses a linear approximation to the value function and multiple linear regression to estimate the linear model from the samples.  The approach is efficient and practical for extremely large models.  It allows easy estimation of EVI for perfect or partial information on individual variables or on combinations of variables.  We illustrate its implementation within Demos (a decision modeling system), and its application to a large model for crisis transportation planning.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6795",
        "title": "Symbolic Probabilitistic Inference in Large BN2O Networks",
        "authors": [
            "Bruce D'Ambrosio"
        ],
        "abstract": "A BN2O network is a two level belief net in which the parent interactions are modeled using the noisy-or interaction model. In this paper we discuss application of the SPI local expression language to efficient inference in large BN2O networks.  In particular, we show that there is significant structure, which can be exploited to improve over the Quickscore result.  We further describe how symbolic techniques can provide information which can significantly reduce the computation required for computing all cause posterior marginals. Finally, we present a novel approximation technique with preliminary experimental results.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6796",
        "title": "Action Networks: A Framework for Reasoning about Actions and Change under Uncertainty",
        "authors": [
            "Adnan Darwiche",
            "Moises Goldszmidt"
        ],
        "abstract": "This work proposes action networks as a semantically well-founded framework for reasoning about actions and change under uncertainty.  Action networks add two primitives to probabilistic causal networks: controllable variables and persistent variables.  Controllable variables allow the representation of actions as directly setting the value of specific events in the domain, subject to preconditions. Persistent variables provide a canonical model of persistence according to which both the state of a variable and the causal mechanism dictating its value persist over time unless intervened upon by an action (or its consequences).  Action networks also allow different methods for quantifying the uncertainty in causal relationships, which go beyond traditional probabilistic quantification.  This paper describes both recent results and work in progress.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6797",
        "title": "On the Relation between Kappa Calculus and Probabilistic Reasoning",
        "authors": [
            "Adnan Darwiche",
            "Moises Goldszmidt"
        ],
        "abstract": "We study the connection between kappa calculus and probabilistic reasoning in diagnosis applications.  Specifically, we abstract a probabilistic belief network for diagnosing faults into a kappa network and compare the ordering of faults computed using both methods. We show that, at least for the example examined, the ordering of faults coincide as long as all the causal relations in the original probabilistic network are taken into account.  We also provide a formal analysis of some network structures where the two methods will differ.  Both kappa rankings and infinitesimal probabilities have been used extensively to study default reasoning and belief revision. But little has been done on utilizing their connection as outlined above.  This is partly because the relation between kappa and probability calculi assumes that probabilities are arbitrarily close to one (or zero).  The experiments in this paper investigate this relation when this assumption is not satisfied. The reported results have important implications on the use of kappa rankings to enhance the knowledge engineering of uncertainty models.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6798",
        "title": "A Structured, Probabilistic Representation of Action",
        "authors": [
            "Ron Davidson",
            "Michael R. Fehling"
        ],
        "abstract": "When agents devise plans for execution in the real world, they face two important forms of uncertainty: they can never have complete knowledge about the state of the world, and they do not have complete control, as the effects of their actions are uncertain.  While most classical planning methods avoid explicit uncertainty reasoning, we believe that uncertainty should be explicitly represented and reasoned about.  We develop a probabilistic representation for states and actions, based on belief networks.  We define conditional belief nets (CBNs) to capture the probabilistic dependency of the effects of an action upon the state of the world.  We also use a CBN to represent the intrinsic relationships among entities in the environment, which persist from state to state.  We present a simple projection algorithm to construct the belief network of the state succeeding an action, using the environment CBN model to infer indirect effects.  We discuss how the qualitative aspects of belief networks and CBNs make them appropriate for the various stages of the problem solving process, from model construction to the design of planning algorithms.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6799",
        "title": "Integrating Planning and Execution in Stochastic Domains",
        "authors": [
            "Richard Dearden",
            "Craig Boutilier"
        ],
        "abstract": "We investigate planning in time-critical domains represented as Markov Decision Processes, showing that search based techniques can be a very powerful method for finding close to optimal plans.  To reduce the computational cost of planning in these domains, we execute actions as we construct the plan, and sacrifice optimality by searching to a fixed depth and using a heuristic function to estimate the value of states. Although this paper concentrates on the search algorithm, we also discuss ways of constructing heuristic functions suitable for this approach. Our results show that by interleaving search and execution, close to optimal policies can be found without the computational requirements of other approaches.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6800",
        "title": "Localized Partial Evaluation of Belief Networks",
        "authors": [
            "Denise L. Draper",
            "Steve Hanks"
        ],
        "abstract": "Most algorithms for propagating evidence through belief networks have been exact and exhaustive: they produce an exact (point-valued) marginal probability for every node in the network.  Often, however, an application will not need information about every n ode in the network nor will it need exact probabilities.  We present the localized partial evaluation (LPE) propagation algorithm, which computes interval bounds on the marginal probability of a specified query node by examining a subset of the nodes in the entire network.  Conceptually, LPE ignores parts of the network that are \"too far away\" from the queried node to have much impact on its value.  LPE has the \"anytime\" property of being able to produce better solutions (tighter intervals) given more time to consider more of the network.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6801",
        "title": "A Probabilistic Model of Action for Least-Commitment Planning with Information Gather",
        "authors": [
            "Denise L. Draper",
            "Steve Hanks",
            "Daniel Weld"
        ],
        "abstract": "AI planning algorithms have addressed the problem of generating sequences of operators that achieve some input goal, usually assuming that the planning agent has perfect control over and information about the world.  Relaxing these assumptions requires an extension to the action representation that allows reasoning both about the changes an action makes and the information it provides.  This paper presents an action representation that extends the deterministic STRIPS model, allowing actions to have both causal and informational effects, both of which can be context dependent and noisy.  We also demonstrate how a standard least-commitment planning algorithm can be extended to include informational actions and contingent execution.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6802",
        "title": "Some Properties of Joint Probability Distributions",
        "authors": [
            "Marek J. Druzdzel"
        ],
        "abstract": "Several Artificial Intelligence schemes for reasoning under uncertainty explore either explicitly or implicitly asymmetries among probabilities of various states of their uncertain domain models.  Even though the correct working of these schemes is practically contingent upon the existence of a small number of probable states, no formal justification has been proposed of why this should be the case.  This paper attempts to fill this apparent gap by studying asymmetries among probabilities of various states of uncertain models.  By rewriting the joint probability distribution over a model's variables into a product of individual variables' prior and conditional probability distributions, and applying central limit theorem to this product, we can demonstrate that the probabilities of individual states of the model can be expected to be drawn from highly skewed, log-normal distributions.  With sufficient asymmetry in individual prior and conditional probability distributions, a small fraction of states can be expected to cover a large portion of the total probability space with the remaining states having practically negligible probability.  Theoretical discussion is supplemented by simulation results and an illustrative real-world example.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6803",
        "title": "An Ordinal View of Independence with Application to Plausible Reasoning",
        "authors": [
            "Didier Dubois",
            "Luis Farinas del Cerro",
            "Andreas Herzig",
            "Henri Prade"
        ],
        "abstract": "An ordinal view of independence is studied in the framework of possibility theory.  We investigate three possible definitions of dependence, of increasing strength. One of them is the counterpart to the multiplication law in probability theory, and the two others are based on the notion of conditional possibility.  These two have enough expressive power to support the whole possibility theory, and a complete axiomatization is provided for the strongest one.  Moreover we show that weak independence is well-suited to the problems of belief change and plausible reasoning, especially to address the problem of blocking of property inheritance in exception-tolerant taxonomic reasoning.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6804",
        "title": "Penalty logic and its Link with Dempster-Shafer Theory",
        "authors": [
            "Florence Dupin de Saint-Cyr",
            "Jerome Lang",
            "Thomas Schiex"
        ],
        "abstract": "Penalty logic, introduced by Pinkas, associates to each formula of a knowledge base the price to pay if this formula is violated.  Penalties may be used as a criterion for selecting preferred consistent subsets in an inconsistent knowledge base, thus inducing a non-monotonic inference relation.  A precise formalization and the main properties of penalty logic and of its associated non-monotonic inference relation are given in the first part. We also show that penalty logic and Dempster-Shafer theory are related, especially in the infinitesimal case.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6805",
        "title": "Value of Evidence on Influence Diagrams",
        "authors": [
            "Kazuo J. Ezawa"
        ],
        "abstract": "In this paper, we introduce evidence propagation operations on influence diagrams and a concept of value of evidence, which measures the value of experimentation.  Evidence propagation operations are critical for the computation of the value of evidence, general update and inference operations in normative expert systems which are based on the influence diagram (generalized Bayesian network) paradigm.  The value of evidence allows us to compute directly an outcome sensitivity, a value of perfect information and a value of control which are used in decision analysis (the science of decision making under uncertainty).  More specifically, the outcome sensitivity is the maximum difference among the values of evidence, the value of perfect information is the expected value of the values of evidence, and the value of control is the optimal value of the values of evidence. We also discuss an implementation and a relative computational efficiency issues related to the value of evidence and the value of perfect information.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6806",
        "title": "Conditional Independence in Possibility Theory",
        "authors": [
            "Pascale Fonck"
        ],
        "abstract": "Possibilistic conditional independence is investigated: we propose a definition of this notion similar to the one used in probability theory.  The links between independence and non-interactivity are investigated, and properties of these relations are given.  The influence of the conjunction used to define a conditional measure of possibility is also highlighted: we examine three types of conjunctions: Lukasiewicz - like T-norms, product-like T-norms and the minimum operator.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6807",
        "title": "Backward Simulation in Bayesian Networks",
        "authors": [
            "Robert Fung",
            "Brendan del Favero"
        ],
        "abstract": "Backward simulation is an approximate inference technique for Bayesian belief networks.  It differs from existing simulation methods in that it starts simulation from the known evidence and works backward (i.e., contrary to the direction of the arcs).  The technique's focus on the evidence leads to improved convergence in situations where the posterior beliefs are dominated by the evidence rather than by the prior probabilities. Since this class of situations is large, the technique may make practical the application of approximate inference in Bayesian belief networks to many real-world problems.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6808",
        "title": "Learning Gaussian Networks",
        "authors": [
            "Dan Geiger",
            "David Heckerman"
        ],
        "abstract": "We describe algorithms for learning Bayesian networks from a combination of user knowledge and statistical data. The algorithms have two components: a scoring metric and a search procedure. The scoring metric takes a network structure, statistical data, and a user's prior knowledge, and returns a score proportional to the posterior probability of the network structure given the data. The search procedure generates networks for evaluation by the scoring metric. Previous work has concentrated on metrics for domains containing only discrete variables, under the assumption that data represents a multinomial sample. In this paper, we extend this work, developing scoring metrics for domains containing all continuous variables or a mixture of discrete and continuous variables, under the assumption that continuous data is sampled from a multivariate normal distribution. Our work extends traditional statistical approaches for identifying vanishing regression coefficients in that we identify two important assumptions, called event equivalence and parameter modularity, that when combined allow the construction of prior distributions for multivariate normal parameters from a single prior Bayesian network specified by a user.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2021-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6809",
        "title": "On Testing Whether an Embedded Bayesian Network Represents a Probability Model",
        "authors": [
            "Dan Geiger",
            "Azaria Paz",
            "Judea Pearl"
        ],
        "abstract": "Testing the validity of probabilistic models containing unmeasured (hidden) variables is shown to be a hard task.  We show that the task of testing whether models are structurally incompatible with the data at hand, requires an exponential number of independence evaluations, each of the form: \"X is conditionally independent of Y, given Z.\"  In contrast, a linear number of such evaluations is required to test a standard Bayesian network (one per vertex).  On the positive side, we show that if a network with hidden variables G has a tree skeleton, checking whether G represents a given probability model P requires the polynomial number of such independence evaluations.  Moreover, we provide an algorithm that efficiently constructs a tree-structured Bayesian network (with hidden variables) that represents P if such a network exists, and further recognizes when such a network does not exist.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6810",
        "title": "Epsilon-Safe Planning",
        "authors": [
            "Robert P. Goldman",
            "Mark S. Boddy"
        ],
        "abstract": "We introduce an approach to high-level conditional planning we call epsilon-safe planning.  This probabilistic approach commits us to planning to meet some specified goal with a probability of success of at least 1-epsilon for some user-supplied epsilon.  We describe several algorithms for epsilon-safe planning based on conditional planners. The two conditional planners we discuss are Peot and Smith's nonlinear conditional planner, CNLP, and our own linear conditional planner, PLINTH.  We present a straightforward extension to conditional planners for which computing the necessary probabilities is simple, employing a commonly-made but perhaps overly-strong independence assumption.  We also discuss a second approach to epsilon-safe planning which relaxes this independence assumption, involving the incremental construction of a probability dependence model in conjunction with the construction of the plan graph.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6811",
        "title": "Generating Bayesian Networks from Probability Logic Knowledge Bases",
        "authors": [
            "Peter Haddawy"
        ],
        "abstract": "We present a method for dynamically generating Bayesian networks from knowledge bases consisting of first-order probability logic sentences.  We present a subset of probability logic sufficient for representing the class of Bayesian networks with discrete-valued nodes.  We impose constraints on the form of the sentences that guarantee that the knowledge base contains all the probabilistic information necessary to generate a network.  We define the concept of d-separation for knowledge bases and prove that a knowledge base with independence conditions defined by d-separation is a complete specification of a probability distribution.  We present a network generation algorithm that, given an inference problem in the form of a query Q and a set of evidence E, generates a network to compute P(Q|E).  We prove the algorithm to be correct.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6812",
        "title": "Abstracting Probabilistic Actions",
        "authors": [
            "Peter Haddawy",
            "AnHai Doan"
        ],
        "abstract": "This paper discusses the problem of abstracting conditional probabilistic actions.  We identify two distinct types of abstraction: intra-action abstraction and inter-action abstraction.  We define what it means for the abstraction of an action to be correct and then derive two methods of intra-action abstraction and two methods of inter-action abstraction which are correct according to this criterion.  We illustrate the developed techniques by applying them to actions described with the temporal action representation used in the DRIPS decision-theoretic planner and we describe how the planner uses abstraction to reduce the complexity of planning.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6814",
        "title": "A New Look at Causal Independence",
        "authors": [
            "David Heckerman",
            "John S. Breese"
        ],
        "abstract": "Heckerman (1993) defined causal independence in terms of a set of temporal conditional independence statements. These statements formalized certain types of causal interaction where (1) the effect is independent of the order that causes are introduced and (2) the impact of a single cause on the effect does not depend on what other causes have previously been applied. In this paper, we introduce an equivalent a temporal characterization of causal independence based on a functional representation of the relationship between causes and the effect. In this representation, the interaction between causes and effect can be written as a nested decomposition of functions. Causal independence can be exploited by representing this decomposition in the belief network, resulting in representations that are more efficient for inference than general causal models. We present empirical results showing the benefits of a causal-independence representation for belief-network inference.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2015-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6815",
        "title": "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data",
        "authors": [
            "David Heckerman",
            "Dan Geiger",
            "David Maxwell Chickering"
        ],
        "abstract": "We describe algorithms for learning Bayesian networks from a combination of user knowledge and statistical data. The algorithms have two components: a scoring metric and a search procedure. The scoring metric takes a network structure, statistical data, and a user's prior knowledge, and returns a score proportional to the posterior probability of the network structure given the data. The search procedure generates networks for evaluation by the scoring metric. Our contributions are threefold. First, we identify two important properties of metrics, which we call event equivalence and parameter modularity. These properties have been mostly ignored, but when combined, greatly simplify the encoding of a user's prior knowledge. In particular, a user can express her knowledge-for the most part-as a single prior Bayesian network for the domain. Second, we describe local search and annealing algorithms to be used in conjunction with scoring metrics. In the special case where each node has at most one parent, we show that heuristic search can be replaced with a polynomial algorithm to identify the networks with the highest score. Third, we describe a methodology for evaluating Bayesian-network learning algorithms. We apply this approach to a comparison of metrics and search procedures.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6816",
        "title": "A Decision-Based View of Causality",
        "authors": [
            "David Heckerman",
            "Ross D. Shachter"
        ],
        "abstract": "Most traditional models of uncertainty have focused on the associational relationship among variables as captured by conditional dependence. In order to successfully manage intelligent systems for decision making, however, we must be able to predict the effects of actions. In this paper, we attempt to unite two branches of research that address such predictions: causal modeling and decision analysis. First, we provide a definition of causal dependence in decision-analytic terms, which we derive from consequences of causal dependence cited in the literature. Using this definition, we show how causal dependence can be represented within an influence diagram. In particular, we identify two inadequacies of an ordinary influence diagram as a representation for cause. We introduce a special class of influence diagrams, called causal influence diagrams, which corrects one of these problems, and identify situations where the other inadequacy can be eliminated. In addition, we describe the relationships between Howard Canonical Form and existing graphical representations of cause.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6817",
        "title": "Probabilistic Description Logics",
        "authors": [
            "Jochen Heinsohn"
        ],
        "abstract": "On the one hand, classical terminological knowledge representation excludes the possibility of handling uncertain concept descriptions involving, e.g., \"usually true\" concept properties, generalized quantifiers, or exceptions.  On the other hand, purely numerical approaches for handling uncertainty in general are unable to consider terminological knowledge.  This paper presents the language ACP which is a probabilistic extension of terminological logics and aims at closing the gap between the two areas of research.  We present the formal semantics underlying the language ALUP and introduce the probabilistic formalism that is based on classes of probabilities and is realized by means of probabilistic constraints.  Besides inferring implicitly existent probabilistic relationships, the constraints guarantee terminological and probabilistic consistency.  Altogether, the new language ALUP applies to domains where both term descriptions and uncertainty have to be handled.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6818",
        "title": "An Experimental Comparison of Numerical and Qualitative Probabilistic Reasoning",
        "authors": [
            "Max Henrion",
            "Gregory M. Provan",
            "Brendan del Favero",
            "Gillian Sanders"
        ],
        "abstract": "Qualitative and infinitesimal probability schemes are consistent with the axioms of probability theory, but avoid the need for precise numerical probabilities.  Using qualitative probabilities could substantially reduce the effort for knowledge engineering and improve the robustness of results. We examine experimentally how well infinitesimal probabilities (the kappa-calculus of Goldszmidt and Pearl) perform a diagnostic task - troubleshooting a car that will not start by comparison with a conventional numerical belief network.  We found the infinitesimal scheme to be as good as the numerical scheme in identifying the true fault.  The performance of the infinitesimal scheme worsens significantly for prior fault probabilities greater than 0.03.  These results suggest that infinitesimal probability methods may be of substantial practical value for machine diagnosis with small prior fault probabilities.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6819",
        "title": "An Alternative Proof Method for Possibilistic Logic and its Application to Terminological Logics",
        "authors": [
            "Bernhard Hollunder"
        ],
        "abstract": "Possibilistic logic, an extension of first-order logic, deals with uncertainty that can be estimated in terms of possibility and necessity measures.  Syntactically, this means that a first-order formula is equipped with a possibility degree or a necessity degree that expresses to what extent the formula is possibly or necessarily true.  Possibilistic resolution yields a calculus for possibilistic logic which respects the semantics developed for possibilistic logic.  A drawback, which possibilistic resolution inherits from classical resolution, is that it may not terminate if applied to formulas belonging to decidable fragments of first-order logic.  Therefore we propose an alternative proof method for possibilistic logic.  The main feature of this method is that it completely abstracts from a concrete calculus but uses as basic operation a test for classical entailment.  We then instantiate possibilistic logic with a terminological logic, which is a decidable subclass o f first-order logic but nevertheless much more expressive than propositional logic.  This yields an extension of terminological logics towards the representation of uncertain knowledge which is satisfactory from a semantic as well as algorithmic point of view.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6820",
        "title": "Possibilistic Conditioning and Propagation",
        "authors": [
            "Yen-Teh Hsia"
        ],
        "abstract": "We give an axiomatization of confidence transfer - a known conditioning scheme - from the perspective of expectation-based inference in the sense of Gardenfors and Makinson. Then, we use the notion of belief independence to \"filter out\" different proposal s of possibilistic conditioning rules, all are variations of confidence transfer.  Among the three rules that we consider, only Dempster's rule of conditioning passes the test of supporting the notion of belief independence.  With the use of this conditioning rule, we then show that we can use local computation for computing desired conditional marginal possibilities of the joint possibility satisfying the given constraints.  It turns out that our local computation scheme is already proposed by Shenoy.  However, our intuitions are completely different from that of Shenoy.  While Shenoy just defines a local computation scheme that fits his framework of valuation-based systems, we derive that local computation scheme from II(,8) = tI(,8 I a) * II(a) and appropriate independence assumptions, just like how the Bayesians derive their local computation scheme.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6821",
        "title": "The Automated Mapping of Plans for Plan Recognition",
        "authors": [
            "Marcus J. Huber",
            "Edmund H. Durfee",
            "Michael P. Wellman"
        ],
        "abstract": "To coordinate with other agents in its environment, an agent needs models of what the other agents are trying to do.  When communication is impossible or expensive, this information must be acquired indirectly via plan recognition.  Typical approaches to plan recognition start with a specification of the possible plans the other agents may be following, and develop special techniques for discriminating among the possibilities. Perhaps more desirable would be a uniform procedure for mapping plans to general structures supporting inference based on uncertain and incomplete observations.  In this paper, we describe a set of methods for converting plans represented in a flexible procedural language to observation models represented as probabilistic belief networks.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6822",
        "title": "A Logic for Default Reasoning About Probabilities",
        "authors": [
            "Manfred Jaeger"
        ],
        "abstract": "A logic is defined that allows to express information about statistical probabilities and about degrees of belief in specific propositions. By interpreting the two types of probabilities in one common probability space, the semantics given are well suited to model the influence of statistical information on the formation of subjective beliefs. Cross entropy minimization is a key element in these semantics, the use of which is justified by showing that the resulting logic exhibits some very reasonable properties.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6823",
        "title": "Optimal Junction Trees",
        "authors": [
            "Finn Verner Jensen",
            "Frank Jensen"
        ],
        "abstract": "The paper deals with optimality issues in connection with updating beliefs in networks. We address two processes: triangulation and construction of junction trees. In the first part, we give a simple algorithm for constructing an optimal junction tree from a triangulated network. In the second part, we argue that any exact method based on local calculations must either be less efficient than the junction tree method, or it has an optimality problem equivalent to that of triangulation.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6824",
        "title": "From Influence Diagrams to Junction Trees",
        "authors": [
            "Frank Jensen",
            "Finn Verner Jensen",
            "Soren L. Dittmer"
        ],
        "abstract": "We present an approach to the solution of decision problems formulated as influence diagrams.  This approach involves a special triangulation of the underlying graph, the construction of a junction tree with special properties, and a message passing algorithm operating on the junction tree for computation of expected utilities and optimal decision policies.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6825",
        "title": "Reduction of Computational Complexity in Bayesian Networks through Removal of Weak Dependencies",
        "authors": [
            "Uffe Kj\u00e6rulff"
        ],
        "abstract": "The paper presents a method for reducing the computational complexity of Bayesian networks through identification and removal of weak dependencies (removal of links from the (moralized) independence graph).  The removal of a small number of links may reduce the computational complexity dramatically, since several fill-ins and moral links may be rendered superfluous by the removal.  The method is described in terms of impact on the independence graph, the junction tree, and the potential functions associated with these.  An empirical evaluation of the method using large real-world networks demonstrates the applicability of the method.  Further, the method, which has been implemented in Hugin, complements the approximation method suggested by Jensen & Andersen (1990).\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6826",
        "title": "Using New Data to Refine a Bayesian Network",
        "authors": [
            "Wai Lam",
            "Fahiem Bacchus"
        ],
        "abstract": "We explore the issue of refining an existent Bayesian network structure using new data which might mention only a subset of the variables.  Most previous works have only considered the refinement of the network's conditional probability parameters, and have not addressed the issue of refining the network's structure.  We develop a new approach for refining the network's structure. Our approach is based on the Minimal Description Length (MDL) principle, and it employs an adapted version of a Bayesian network learning algorithm developed in our previous work.  One of the adaptations required is to modify the previous algorithm to account for the structure of the existent network.  The learning algorithm generates a partial network structure which can then be used to improve the existent network.  We also present experimental evidence demonstrating the effectiveness of our approach.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6827",
        "title": "Syntax-based Default Reasoning as Probabilistic Model-based Diagnosis",
        "authors": [
            "Jerome Lang"
        ],
        "abstract": "We view the syntax-based approaches to default reasoning as a model-based diagnosis problem, where each source giving a piece of information is considered as a component. It is formalized in the ATMS framework (each source corresponds to an assumption). We assume then that all sources are independent and \"fail\" with a very small probability. This leads to a probability assignment on the set of candidates, or equivalently on the set of consistent environments. This probability assignment induces a Dempster-Shafer belief function which measures the probability that a proposition can be deduced from the evidence. This belief function can be used in several different ways to define a non-monotonic consequence relation. We study and compare these consequence relations. The -case of prioritized knowledge bases is briefly considered.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6829",
        "title": "Fuzzy Geometric Relations to Represent Hierarchical Spatial Information",
        "authors": [
            "Stephane Lapointe",
            "Rene Proulx"
        ],
        "abstract": "A model to represent spatial information is presented in this paper.  It is based on fuzzy constraints represented as fuzzy geometric relations that can be hierarchically structured. The concept of spatial template is introduced to capture the idea of interrelated objects in two-dimensional space.  The representation model is used to specify imprecise or vague information consisting in relative locations and orientations of template objects.  It is shown in this paper how a template represented by this model can be matched against a crisp situation to recognize a particular instance of this template.  Furthermore, the proximity measure (fuzzy measure) between the instance and the template is worked out - this measure can be interpreted as a degree of similarity.  In this context, template recognition can be viewed as a case of fuzzy pattern recognition.  The results of this work have been implemented and applied to a complex military problem from which this work originated.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6830",
        "title": "Constructing Belief Networks to Evaluate Plans",
        "authors": [
            "Paul E. Lehner",
            "Christopher Elsaesser",
            "Scott A. Musman"
        ],
        "abstract": "This paper examines the problem of constructing belief networks to evaluate plans produced by an knowledge-based planner.  Techniques are presented for handling various types of complicating plan features.  These include plans with context-dependent consequences, indirect consequences, actions with preconditions that must be true during the execution of an action, contingencies, multiple levels of abstraction multiple execution agents with partially-ordered and temporally overlapping actions, and plans which reference specific times and time durations.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6831",
        "title": "Operator Selection While Planning Under Uncertainty",
        "authors": [
            "Todd Michael Mansell",
            "Grahame Smith"
        ],
        "abstract": "This paper describes the best first search strategy used by U-Plan (Mansell 1993a), a planning system that constructs quantitatively ranked plans given an incomplete description of an uncertain environment.  U-Plan uses uncertain and incomplete evidence de scribing the environment, characterizes it using a Dempster-Shafer interval, and generates a set of possible world states.  Plan construction takes place in an abstraction hierarchy where strategic decisions are made before tactical decisions.  Search through this abstraction hierarchy is guided by a quantitative measure (expected fulfillment) based on decision theory.  The search strategy is best first with the provision to update expected fulfillment and review previous decisions in the light of planning developments. U-Plan generates multiple plans for multiple possible worlds, and attempts to use existing plans for new world situations.  A super-plan is then constructed, based on merging the set of plans and appropriately timed knowledge acquisition operators, which are used to decide between plan alternatives during plan execution.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6832",
        "title": "Model-Based Diagnosis with Qualitative Temporal Uncertainty",
        "authors": [
            "Wolfgang Nejdl",
            "Johann Gamper"
        ],
        "abstract": "In this paper we describe a framework for model-based diagnosis of dynamic systems, which extends previous work in this field by using and expressing temporal uncertainty in the form of qualitative interval relations a la Allen. Based on a logical framework extended by qualitative and quantitative temporal constraints we show how to describe behavioral models (both consistency- and abductive-based), discuss how to use abstract observations and show how abstract temporal diagnoses are computed.  This yields an expressive framework, which allows the representation of complex temporal behavior allowing us to represent temporal uncertainty.  Due to its abstraction capabilities computation is made independent of the number of observations and time points in a temporal setting.  An example of hepatitis diagnosis is used throughout the paper.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6833",
        "title": "Incremental Dynamic Construction of Layered Polytree Networks",
        "authors": [
            "Keung-Chi Ng",
            "Tod S. Levitt"
        ],
        "abstract": "Certain classes of problems, including perceptual data understanding, robotics, discovery, and learning, can be represented as incremental, dynamically constructed belief networks. These automatically constructed networks can be dynamically extended and modified as evidence of new individuals becomes available.  The main result of this paper is the incremental extension of the singly connected polytree network in such a way that the network retains its singly connected polytree structure after the changes.  The algorithm is deterministic and is guaranteed to have a complexity of single node addition that is at most of order proportional to the number of nodes (or size) of the network.  Additional speed-up can be achieved by maintaining the path information.  Despite its incremental and dynamic nature, the algorithm can also be used for probabilistic inference in belief networks in a fashion similar to other exact inference algorithms.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6835",
        "title": "A Probabilistic Calculus of Actions",
        "authors": [
            "Judea Pearl"
        ],
        "abstract": "We present a symbolic machinery that admits both probabilistic and causal information about a given domain and produces probabilistic statements about the effect of actions and the impact of observations.  The calculus admits two types of conditioning operators: ordinary Bayes conditioning, P(y|X = x), which represents the observation X = x, and causal conditioning, P(y|do(X = x)), read the probability of Y = y conditioned on holding X constant (at x) by deliberate action. Given a mixture of such observational and causal sentences, together with the topology of the causal graph, the calculus derives new conditional probabilities of both types, thus enabling one to quantify the effects of actions (and policies) from partially specified knowledge bases, such as Bayesian networks in which some conditional probabilities may not be available.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6836",
        "title": "Robust Planning in Uncertain Environments",
        "authors": [
            "Stephen G. Pimentel",
            "Lawrence M. Brem"
        ],
        "abstract": "This paper describes a novel approach to planning which takes advantage of decision theory to greatly improve robustness in an uncertain environment.  We present an algorithm which computes conditional plans of maximum expected utility.  This algorithm relies on a representation of the search space as an AND/OR tree and employs a depth-limit to control computation costs.  A numeric robustness factor, which parameterizes the utility function, allows the user to modulate the degree of risk-aversion employed by the planner.  Via a look-ahead search, the planning algorithm seeks to find an optimal plan using expected utility as its optimization criterion.  We present experimental results obtained by applying our algorithm to a non-deterministic extension of the blocks world domain.  Our results demonstrate that the robustness factor governs the degree of risk embodied in the conditional plans computed by our algorithm.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6837",
        "title": "Anytime Decision Making with Imprecise Probabilities",
        "authors": [
            "Michael Pittarelli"
        ],
        "abstract": "This paper examines methods of decision making that are able to accommodate limitations on both the form in which uncertainty pertaining to a decision problem can be realistically represented and the amount of computing time available before a decision must be made.  The methods are anytime algorithms in the sense of Boddy and Dean 1991.  Techniques are presented for use with Frisch and Haddawy's [1992] anytime deduction system, with an anytime adaptation of Nilsson's [1986] probabilistic logic, and with a probabilistic database model.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6839",
        "title": "Knowledge Engineering for Large Belief Networks",
        "authors": [
            "Malcolm Pradhan",
            "Gregory M. Provan",
            "Blackford Middleton",
            "Max Henrion"
        ],
        "abstract": "We present several techniques for knowledge engineering of large belief networks (BNs) based on the our experiences with a network derived from a large medical knowledge base.  The noisyMAX, a generalization of the noisy-OR gate, is used to model causal in dependence in a BN with multi-valued variables.  We describe the use of leak probabilities to enforce the closed-world assumption in our model. We present Netview, a visualization tool based on causal independence and the use of leak probabilities.  The Netview software allows knowledge engineers to dynamically view sub-networks for knowledge engineering, and it provides version control for editing a BN.  Netview generates sub-networks in which leak probabilities are dynamically updated to reflect the missing portions of the network.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6840",
        "title": "Solving Asymmetric Decision Problems with Influence Diagrams",
        "authors": [
            "Runping Qi",
            "Nevin Lianwen Zhang",
            "David L. Poole"
        ],
        "abstract": "While influence diagrams have many advantages as a representation framework for Bayesian decision problems, they have a serious drawback in handling asymmetric decision problems.  To be represented in an influence diagram, an asymmetric decision problem must be symmetrized.  A considerable amount of unnecessary computation may be involved when a symmetrized influence diagram is evaluated by conventional algorithms.  In this paper we present an approach for avoiding such unnecessary computation in influence diagram evaluation.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6841",
        "title": "Belief Maintenance in Bayesian Networks",
        "authors": [
            "Marco Ramoni",
            "Alberto Riva"
        ],
        "abstract": "Bayesian Belief Networks (BBNs) are a powerful formalism for reasoning under uncertainty but bear some severe limitations: they require a large amount of information before any reasoning process can start, they have limited contradiction handling capabilities, and their ability to provide explanations for their conclusion is still controversial.  There exists a class of reasoning systems, called Truth Maintenance Systems (TMSs), which are able to deal with partially specified knowledge, to provide well-founded explanation for their conclusions, and to detect and handle contradictions. TMSs incorporating measure of uncertainty are called Belief Maintenance Systems (BMSs).  This paper describes how a BMS based on probabilistic logic can be applied to BBNs, thus introducing a new class of BBNs, called Ignorant Belief Networks, able to incrementally deal with partially specified conditional dependencies, to provide explanations, and to detect and handle contradictions.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6842",
        "title": "Belief Updating by Enumerating High-Probability Independence-Based Assignments",
        "authors": [
            "Eugene Santos Jr.",
            "Solomon Eyal Shimony"
        ],
        "abstract": "Independence-based (IB) assignments to Bayesian belief networks were originally proposed as abductive explanations.  IB assignments assign fewer variables in abductive explanations than do schemes assigning values to all evidentially supported variables.  We use IB assignments to approximate marginal probabilities in Bayesian belief networks.  Recent work in belief updating for Bayes networks attempts to approximate posterior probabilities by finding a small number of the highest probability complete (or perhaps evidentially supported) assignments.  Under certain assumptions, the probability mass in the union of these assignments is sufficient to obtain a good approximation.  Such methods are especially useful for highly-connected networks, where the maximum clique size or the cutset size make the standard algorithms intractable.  Since IB assignments contain fewer assigned variables, the probability mass in each assignment is greater than in the respective complete assignment.  Thus, fewer IB assignments are sufficient, and a good approximation can be obtained more efficiently.  IB assignments can be used for efficiently approximating posterior node probabilities even in cases which do not obey the rather strict skewness assumptions used in previous research.  Two algorithms for finding the high probability IB assignments are suggested: one by doing a best-first heuristic search, and another by special-purpose integer linear programming. Experimental results show that this approach is feasible for highly connected belief networks.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6843",
        "title": "Global Conditioning for Probabilistic Inference in Belief Networks",
        "authors": [
            "Ross D. Shachter",
            "Stig K. Andersen",
            "Peter Szolovits"
        ],
        "abstract": "In this paper we propose a new approach to probabilistic inference on belief networks, global conditioning, which is a simple generalization of Pearl's (1986b) method of loopcutset conditioning.  We show that global conditioning, as well as loop-cutset conditioning, can be thought of as a special case of the method of Lauritzen and Spiegelhalter (1988) as refined by Jensen et al (199Oa; 1990b).  Nonetheless, this approach provides new opportunities for parallel processing and, in the case of sequential processing, a tradeoff of time for memory.  We also show how a hybrid method (Suermondt and others 1990) combining loop-cutset conditioning with Jensen's method can be viewed within our framework.  By exploring the relationships between these methods, we develop a unifying framework in which the advantages of each approach can be combined successfully.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6844",
        "title": "Belief Induced by the Partial Knowledge of the Probabilities",
        "authors": [
            "Philippe Smets"
        ],
        "abstract": "We construct the belief function that quantifies the agent, beliefs about which event of Q will occurred when he knows that the event is selected by a chance set-up and that the probability function associated to the chance set up is only partially known.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6845",
        "title": "Ignorance and the Expressiveness of Single- and Set-Valued Probability Models of Belief",
        "authors": [
            "Paul Snow"
        ],
        "abstract": "Over time, there have hen refinements in the way that probability distributions are used for representing beliefs.  Models which rely on single probability distributions depict a complete ordering among the propositions of interest, yet human beliefs are sometimes not completely ordered.  Non-singleton sets of probability distributions can represent partially ordered beliefs.  Convex sets are particularly convenient and expressive, but it is known that there are reasonable patterns of belief whose faithful representation require less restrictive sets.  The present paper shows that prior ignorance about three or more exclusive alternatives and the emergence of partially ordered beliefs when evidence is obtained defy representation by any single set of distributions, but yield to a representation baud on several uts.  The partial order is shown to be a partial qualitative probability which shares some intuitively appealing attributes with probability distributions.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6846",
        "title": "A Probabilistic Approach to Hierarchical Model-based Diagnosis",
        "authors": [
            "Sampath Srinivas"
        ],
        "abstract": "Model-based diagnosis reasons backwards from a functional schematic of a system to isolate faults given observations of anomalous behavior.  We develop a fully probabilistic approach to model based diagnosis and extend it to support hierarchical models.  Our scheme translates the functional schematic into a Bayesian network and diagnostic inference takes place in the Bayesian network.  A Bayesian network diagnostic inference algorithm is modified to take advantage of the hierarchy to give computational gains.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6847",
        "title": "Semigraphoids Are Two-Antecedental Approximations of Stochastic Conditional Independence Models",
        "authors": [
            "Milan Studeny"
        ],
        "abstract": "The semigraphoid closure of every couple of CI-statements (GI=conditional independence) is a stochastic CI-model.  As a consequence of this result it is shown that every probabilistically sound inference rule for CI-model, having at most two antecedents, is derivable from the semigraphoid inference rules.  This justifies the use of semigraphoids as approximations of stochastic CI-models in probabilistic reasoning.  The list of all 19 potential dominant elements of the mentioned semigraphoid closure is given as a byproduct.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6848",
        "title": "Exceptional Subclasses in Qualitative Probability",
        "authors": [
            "Sek-Wah Tan"
        ],
        "abstract": "System Z+ [Goldszmidt and Pearl, 1991, Goldszmidt, 1992] is a formalism for reasoning with normality defaults of the form \"typically if phi then + (with strength cf)\" where 6 is a positive integer.  The system has a critical shortcoming in that it does not sanction inheritance across exceptional subclasses.  In this paper we propose an extension to System Z+ that rectifies this shortcoming by extracting additional conditions between worlds from the defaults database.  We show that the additional constraints do not change the notion of the consistency of a database.  We also make comparisons with competing default reasoning systems.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6849",
        "title": "A Defect in Dempster-Shafer Theory",
        "authors": [
            "Pei Wang"
        ],
        "abstract": "By analyzing the relationships among chance, weight of evidence and degree of beliefwe show that the assertion \"probability functions are special cases of belief functions\" and the assertion \"Dempster's rule can be used to combine belief functions based on distinct bodies of evidence\" together lead to an inconsistency in Dempster-Shafer theory.  To solve this problem, we must reject some fundamental postulates of the theory.  We introduce a new approach for uncertainty management that shares many intuitive ideas with D-S theory, while avoiding this problem.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6850",
        "title": "State-space Abstraction for Anytime Evaluation of Probabilistic Networks",
        "authors": [
            "Michael P. Wellman",
            "Chao-Lin Liu"
        ],
        "abstract": "One important factor determining the computational complexity of evaluating a probabilistic network is the cardinality of the state spaces of the nodes.  By varying the granularity of the state spaces, one can trade off accuracy in the result for computational efficiency.  We present an anytime procedure for approximate evaluation of probabilistic networks based on this idea.  On application to some simple networks, the procedure exhibits a smooth improvement in approximation quality as computation time increases. This suggests that state-space abstraction is one more useful control parameter for designing real-time probabilistic reasoners.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6851",
        "title": "General Belief Measures",
        "authors": [
            "Emil Weydert"
        ],
        "abstract": "Probability measures by themselves, are known to be inappropriate for modeling the dynamics of plain belief and their excessively strong measurability constraints make them unsuitable for some representational tasks, e.g. in the context of firstorder knowledge. In this paper, we are therefore going to look for possible alternatives and extensions. We begin by delimiting the general area of interest, proposing a minimal list of assumptions to be satisfied by any reasonable quasi-probabilistic valuation concept. Within this framework, we investigate two particularly interesting kinds of quasi-measures which are not or much less affected by the traditional problems. * Ranking measures, which generalize Spohn-type and possibility measures. * Cumulative measures, which combine the probabilistic and the ranking philosophy, allowing thereby a fine-grained account of static and dynamic belief.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6852",
        "title": "Generating Graphoids from Generalised Conditional Probability",
        "authors": [
            "Nic Wilson"
        ],
        "abstract": "We take a general approach to uncertainty on product spaces, and give sufficient conditions for the independence structures of uncertainty measures to satisfy graphoid properties.  Since these conditions are arguably more intuitive than some of the graphoid properties, they can be viewed as explanations why probability and certain other formalisms generate graphoids.  The conditions include a sufficient condition for the Intersection property which can still apply even if there is a strong logical relations hip between the variables.  We indicate how these results can be used to produce theories of qualitative conditional probability which are semi-graphoids and graphoids.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6853",
        "title": "On Axiomatization of Probabilistic Conditional Independencies",
        "authors": [
            "Michael S. K. M. Wong",
            "Z. W. Wang"
        ],
        "abstract": "This paper studies the connection between probabilistic conditional independence in uncertain reasoning and data dependency in relational databases.  As a demonstration of the usefulness of this preliminary investigation, an alternate proof is presented for refuting the conjecture suggested by Pearl and Paz that probabilistic conditional independencies have a complete axiomatization.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6854",
        "title": "Evidential Reasoning with Conditional Belief Functions",
        "authors": [
            "Hong Xu",
            "Philippe Smets"
        ],
        "abstract": "In the existing evidential networks with belief functions, the relations among the variables are always represented by joint belief functions on the product space of the involved variables.  In this paper, we use conditional belief functions to represent such relations in the network and show some relations of these two kinds of representations. We also present a propagation algorithm for such networks. By analyzing the properties of some special evidential networks with conditional belief functions, we show that the reasoning process can be simplified in such kinds of networks.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6855",
        "title": "Inter-causal Independence and Heterogeneous Factorization",
        "authors": [
            "Nevin Lianwen Zhang",
            "David L Poole"
        ],
        "abstract": "It is well known that conditional independence can be used to factorize a joint probability into a multiplication of conditional probabilities.  This paper proposes a constructive definition of inter-causal independence, which can be used to further factorize a conditional probability.  An inference algorithm is developed, which makes use of both conditional independence and inter-causal independence to reduce inference complexity in Bayesian networks.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.7251",
        "title": "Modeling Stable Matching Problems with Answer Set Programming",
        "authors": [
            "Sofie De Clercq",
            "Steven Schockaert",
            "Martine De Cock",
            "Ann Now\u00e9"
        ],
        "abstract": "The Stable Marriage Problem (SMP) is a well-known matching problem first introduced and solved by Gale and Shapley (1962). Several variants and extensions to this problem have since been investigated to cover a wider set of applications. Each time a new variant is considered, however, a new algorithm needs to be developed and implemented. As an alternative, in this paper we propose an encoding of the SMP using Answer Set Programming (ASP). Our encoding can easily be extended and adapted to the needs of specific applications. As an illustration we show how stable matchings can be found when individuals may designate unacceptable partners and ties between preferences are allowed. Subsequently, we show how our ASP based encoding naturally allows us to select specific stable matchings which are optimal according to a given criterion. Each time, we can rely on generic and efficient off-the-shelf answer set solvers to find (optimal) stable matchings.\n    ",
        "submission_date": "2013-02-28T00:00:00",
        "last_modified_date": "2013-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.0213",
        "title": "The Semantic Web takes Wing: Programming Ontologies with Tawny-OWL",
        "authors": [
            "Phillip Lord"
        ],
        "abstract": "The Tawny-OWL library provides a fully-programmatic environment for ontology   building; it enables the use of a rich set of tools for ontology   development, by recasting development as a form of programming. It is built  in Clojure - a modern Lisp dialect, and is backed by the OWL API. Used  simply, it has a similar syntax to OWL Manchester syntax, but it provides  arbitrary extensibility and abstraction. It builds on existing facilities  for Clojure, which provides a rich and modern programming tool chain, for  versioning, distributed development, build, testing and continuous  integration. In this paper, we describe the library, this environment and  the its potential implications for the ontology development process.\n    ",
        "submission_date": "2013-03-01T00:00:00",
        "last_modified_date": "2013-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.0787",
        "title": "Restricted Manipulation in Iterative Voting: Convergence and Condorcet Efficiency",
        "authors": [
            "Umberto Grandi",
            "Andrea Loreggia",
            "Francesca Rossi",
            "Kristen Brent Venable",
            "Toby Walsh"
        ],
        "abstract": "In collective decision making, where a voting rule is used to take a collective decision among a group of agents, manipulation by one or more agents is usually considered negative behavior to be avoided, or at least to be made computationally difficult for the agents to perform. However, there are scenarios in which a restricted form of manipulation can instead be beneficial. In this paper we consider the iterative version of several voting rules, where at each step one agent is allowed to manipulate by modifying his ballot according to a set of restricted manipulation moves which are computationally easy and require little information to be performed. We prove convergence of iterative voting rules when restricted manipulation is allowed, and we present experiments showing that most iterative voting rules have a higher Condorcet efficiency than their non-iterative version.\n    ",
        "submission_date": "2013-03-04T00:00:00",
        "last_modified_date": "2013-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1454",
        "title": "Causality in Bayesian Belief Networks",
        "authors": [
            "Marek J. Druzdzel",
            "Herbert A. Simon"
        ],
        "abstract": "We address the problem of causal interpretation of the graphical structure of Bayesian belief networks (BBNs).  We review the concept of causality explicated in the domain of structural equations models and show that it is applicable to BBNs.  In this view, which we call mechanism-based, causality is defined within models and causal asymmetries arise when mechanisms are placed in the context of a system.  We lay the link between structural equations models and BBNs models and formulate the conditions under which the latter can be given causal interpretation.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1455",
        "title": "From Conditional Oughts to Qualitative Decision Theory",
        "authors": [
            "Judea Pearl"
        ],
        "abstract": "The primary theme of this investigation is a decision theoretic account of conditional ought statements (e.g., \"You ought to do A, if C\") that rectifies glaring deficiencies in classical deontic logic.  The resulting account forms a sound basis for qualitative decision theory, thus providing a framework for qualitative planning under uncertainty.  In particular, we show that adding causal relationships (in the form of a single graph) as part of an epistemic state is sufficient to facilitate the analysis of action sequences, their consequences, their interaction with observations, their expected utilities and, hence, the synthesis of plans and strategies under uncertainty.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1456",
        "title": "A Probabilistic Algorithm for Calculating Structure: Borrowing from Simulated Annealing",
        "authors": [
            "Russ B. Altman"
        ],
        "abstract": "We have developed a general Bayesian algorithm for determining the coordinates of points in a three-dimensional space.  The algorithm takes as input a set of probabilistic constraints on the coordinates of the points, and an a priori distribution for each point location.  The output is a maximum-likelihood estimate of the location of each point.  We use the extended, iterated Kalman filter, and add a search heuristic for optimizing its solution under nonlinear conditions.  This heuristic is based on the same principle as the simulated annealing heuristic for other optimization problems.  Our method uses any probabilistic constraints that can be expressed as a function of the point coordinates (for example, distance, angles, dihedral angles, and planarity).  It assumes that all constraints have Gaussian noise.  In this paper, we describe the algorithm and show its performance on a set of synthetic data to illustrate its convergence properties, and its applicability to domains such ng molecular structure determination.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1457",
        "title": "A Study of Scaling Issues in Bayesian Belief Networks for Ship Classification",
        "authors": [
            "Scott A. Musman",
            "L. W. Chang"
        ],
        "abstract": "The problems associated with scaling involve active and challenging research topics in the area of artificial intelligence.  The purpose is to solve real world problems by means of AI technologies, in cases where the complexity of representation of the real world problem is potentially combinatorial.  In this paper, we present a novel approach to cope with the scaling issues in Bayesian belief networks for ship classification.  The proposed approach divides the conceptual model of a complex ship classification problem into a set of small modules that work together to solve the classification problem while preserving the functionality of the original model.  The possible ways of explaining sensor returns (e.g., the evidence) for some features, such as portholes along the length of a ship, are sometimes combinatorial.  Thus, using an exhaustive approach, which entails the enumeration of all possible explanations, is impractical for larger problems.  We present a network structure (referred to as Sequential Decomposition, SD) in which each observation is associated with a set of legitimate outcomes which are consistent with the explanation of each observed piece of evidence.  The results show that the SD approach allows one to represent feature-observation relations in a manageable way and achieve the same explanatory power as an exhaustive approach.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1458",
        "title": "Tradeoffs in Constructing and Evaluating Temporal Influence Diagrams",
        "authors": [
            "Gregory M. Provan"
        ],
        "abstract": "This paper addresses the tradeoffs which need to be considered in reasoning using probabilistic network representations, such as Influence Diagrams (IDs). In particular, we examine the tradeoffs entailed in using Temporal Influence Diagrams (TIDs) which adequately capture the temporal evolution of a dynamic system without prohibitive data and computational requirements.  Three approaches for TID construction which make different tradeoffs are examined: (1) tailoring the network at each time interval to the data available (rather then just copying the original Bayes Network for all time intervals); (2) modeling the evolution of a parsimonious subset of variables (rather than all variables); and (3) model selection approaches, which seek to minimize some measure of the predictive accuracy of the model without introducing too many parameters, which might cause \"overfitting\" of the model. Methods of evaluating the accuracy/efficiency of the tradeoffs are proposed.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1459",
        "title": "End-User Construction of Influence Diagrams for Bayesian Statistics",
        "authors": [
            "Harold P. Lehmann",
            "Ross D. Shachter"
        ],
        "abstract": "Influence diagrams are ideal knowledge representations for Bayesian statistical models. However, these diagrams are difficult for end users to interpret and to manipulate.  We present a user-based architecture that enables end users to create and to manipulate the knowledge representation.  We use the problem of physicians' interpretation of two-arm parallel randomized clinical trials (TAPRCT) to illustrate the architecture and its use. There are three primary data structures.  Elements of statistical models are encoded as subgraphs of a restricted class of influence diagram.  The interpretations of those elements are mapped into users' language in a domain-specific, user-based semantic interface, called a patient-flow diagram, in the TAPRCT problem.  Pennitted transformations of the statistical model that maintain the semantic relationships of the model are encoded in a metadata-state diagram, called the cohort-state diagram, in the TAPRCT problem.  The algorithm that runs the system uses modular actions called construction steps.  This framework has been implemented in a system called THOMAS, that allows physicians to interpret the data reported from a TAPRCT.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1460",
        "title": "On Considering Uncertainty and Alternatives in Low-Level Vision",
        "authors": [
            "Steven M. LaValle",
            "Seth A. Hutchinson"
        ],
        "abstract": "In this paper we address the uncertainty issues involved in the low-level vision task of image segmentation.  Researchers in computer vision have worked extensively on this problem, in which the goal is to partition (or segment) an image into regions that are homogeneous or uniform in some sense.  This segmentation is often utilized by some higher level process, such as an object recognition system. We show that by considering uncertainty in a Bayesian formalism, we can use statistical image models to build an approximate representation of a probability distribution over a space of alternative segmentations.  We give detailed descriptions of the various levels of uncertainty associated with this problem, discuss the interaction of prior and posterior distributions, and provide the operations for constructing this representation.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1461",
        "title": "Forecasting Sleep Apnea with Dynamic Network Models",
        "authors": [
            "Paul Dagum",
            "Adam Galper"
        ],
        "abstract": "Dynamic network models (DNMs) are belief networks for temporal reasoning.  The DNM methodology combines techniques from time series analysis and probabilistic reasoning to provide (1) a knowledge representation that integrates noncontemporaneous and contemporaneous dependencies and (2) methods for iteratively refining these dependencies in response to the effects of exogenous influences.  We use belief-network inference algorithms to perform forecasting, control, and discrete event simulation on DNMs.  The belief network formulation allows us to move beyond the traditional assumptions of linearity in the relationships among time-dependent variables and of normality in their probability distributions.  We demonstrate the DNM methodology on an important forecasting problem in medicine.  We conclude with a discussion of how the methodology addresses several limitations found in traditional time series analyses.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1462",
        "title": "Normative Engineering Risk Management Systems",
        "authors": [
            "Peter J. Regan"
        ],
        "abstract": "This paper describes a normative system design that incorporates diagnosis, dynamic evolution, decision making, and information gathering.  A single influence diagram demonstrates the design's coherence, yet each activity is more effectively modeled and evaluated separately.  Application to offshore oil platforms illustrates the design.  For this application, the normative system is embedded in a real-time expert system.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1463",
        "title": "Diagnosis of Multiple Faults: A Sensitivity Analysis",
        "authors": [
            "David Heckerman",
            "Michael Shwe"
        ],
        "abstract": "We compare the diagnostic accuracy of three diagnostic inference models: the simple Bayes model, the multimembership Bayes model, which is isomorphic to the parallel combination function in the certainty-factor model, and a model that incorporates the noisy OR-gate interaction. The comparison is done on 20 clinicopathological conference (CPC) cases from the American Journal of Medicine-challenging cases describing actual patients often with multiple disorders. We find that the distributions produced by the noisy OR model agree most closely with the gold-standard diagnoses, although substantial differences exist between the distributions and the diagnoses. In addition, we find that the multimembership Bayes model tends to significantly overestimate the posterior probabilities of diseases, whereas the simple Bayes model tends to significantly underestimate the posterior probabilities. Our results suggest that additional work to refine the noisy OR model for internal medicine will be worthwhile.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1464",
        "title": "Additive Belief-Network Models",
        "authors": [
            "Paul Dagum",
            "Adam Galper"
        ],
        "abstract": "The inherent intractability of probabilistic inference has hindered the application of belief networks to large domains.  Noisy OR-gates [30] and probabilistic similarity networks [18, 17] escape the complexity of inference by restricting model expressiveness.  Recent work in the application of belief-network models to time-series analysis and forecasting [9, 10] has given rise to the additive belief network model (ABNM).  We (1) discuss the nature and implications of the approximations made by an additive decomposition of a belief network, (2) show greater efficiency in the induction of additive models when available data are scarce, (3) generalize probabilistic inference algorithms to exploit the additive decomposition of ABNMs, (4) show greater efficiency of inference, and (5) compare results on inference with a simple additive belief network.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1465",
        "title": "Parameter Adjustment in Bayes Networks. The generalized noisy OR-gate",
        "authors": [
            "Francisco Javier Diez"
        ],
        "abstract": "Spiegelhalter and Lauritzen [15] studied sequential learning in Bayesian networks and proposed three models for the representation of conditional probabilities.  A forth model, shown here, assumes that the parameter distribution is given by a product of Gaussian functions and updates them from the _ and _r messages of evidence propagation.  We also generalize the noisy OR-gate for multivalued variables, develop the algorithm to compute probability in time proportional to the number of parents (even in networks with loops) and apply the learning model to this gate.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1466",
        "title": "A fuzzy relation-based extension of Reggia's relational model for diagnosis handling uncertain and incomplete information",
        "authors": [
            "Didier Dubois",
            "Henri Prade"
        ],
        "abstract": "Relational models for diagnosis are based on a direct description of the association between disorders and manifestations.  This type of model has been specially used and developed by Reggia and his co-workers in the late eighties as a basic starting point for approaching diagnosis problems.  The paper proposes a new relational model which includes Reggia's model as a particular case and which allows for a more expressive representation of the observations and of the manifestations associated with disorders. The model distinguishes, i) between manifestations which are certainly absent and those which are not (yet) observed, and ii) between manifestations which cannot be caused by a given disorder and manifestations for which we do not know if they can or cannot be caused by this disorder.  This new model, which can handle uncertainty in a non-probabilistic way, is based on possibility theory and so-called twofold fuzzy sets, previously introduced by the authors.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1467",
        "title": "Dialectic Reasoning with Inconsistent Information",
        "authors": [
            "Morten Elvang-G\u00f8ransson",
            "Paul J. Krause",
            "John Fox"
        ],
        "abstract": "  From an inconsistent database non-trivial arguments may be constructed both for a proposition, and for the contrary of that proposition. Therefore, inconsistency in a logical database causes uncertainty about which conclusions to accept. This kind of uncertainty is called logical uncertainty. We define a concept of \"acceptability\", which induces a means for differentiating arguments. The more acceptable an argument, the more confident we are in it. A specific interest is to use the acceptability classes to assign linguistic qualifiers to propositions, such that the qualifier assigned to a propositions reflects its logical uncertainty. A more general interest is to understand how classes of acceptability can be defined for arguments constructed from an inconsistent database, and how this notion of acceptability can be devised to reflect different criteria. Whilst concentrating on the aspects of assigning linguistic qualifiers to propositions, we also indicate the more general significance of the notion of acceptability.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1468",
        "title": "Causal Independence for Knowledge Acquisition and Inference",
        "authors": [
            "David Heckerman"
        ],
        "abstract": "I introduce a temporal belief-network representation of causal independence that a knowledge engineer can use to elicit probabilistic models. Like the current, atemporal belief-network representation of causal independence, the new representation makes knowledge acquisition tractable. Unlike the atemproal representation, however, the temporal representation can simplify inference, and does not require the use of unobservable variables. The representation is less general than is the atemporal representation, but appears to be useful for many practical applications.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1469",
        "title": "Utility-Based Abstraction and Categorization",
        "authors": [
            "Eric J. Horvitz",
            "Adrian Klein"
        ],
        "abstract": "We take a utility-based approach to categorization.  We construct generalizations about events and actions by considering losses associated with failing to distinguish among detailed distinctions in a decision model.  The utility-based methods transform detailed states of the world into more abstract categories comprised of disjunctions of the states. We show how we can cluster distinctions into groups of distinctions at progressively higher levels of abstraction, and describe rules for decision making with the abstractions. The techniques introduce a utility-based perspective on the nature of concepts, and provide a means of simplifying decision models used in automated reasoning systems. We demonstrate the techniques by describing the capabilities and output of TUBA, a program for utility-based abstraction.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1470",
        "title": "Sensitivity Analysis for Probability Assessments in Bayesian Networks",
        "authors": [
            "Kathryn Blackmond Laskey"
        ],
        "abstract": "When eliciting probability models from experts, knowledge engineers may compare the results of the model with expert judgment on test scenarios, then adjust model parameters to bring the behavior of the model more in line with the expert's intuition.  This paper presents a methodology for analytic computation of sensitivity values to measure the impact of small changes in a network parameter on a target probability value or distribution.  These values can be used to guide knowledge elicitation.  They can also be used in a gradient descent algorithm to estimate parameter values that maximize a measure of goodness-of-fit to both local and holistic probability assessments.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1471",
        "title": "Causal Modeling",
        "authors": [
            "John F. Lemmer"
        ],
        "abstract": "Causal Models are like Dependency Graphs and Belief Nets in that they provide a structure and a set of assumptions from which a joint distribution can, in principle, be computed.  Unlike Dependency Graphs, Causal Models are models of hierarchical and/or parallel processes, rather than models of distributions (partially) known to a model builder through some sort of gestalt.  As such, Causal Models are more modular, easier to build, more intuitive, and easier to understand than Dependency Graph Models.  Causal Models are formally defined and Dependency Graph Models are shown to be a special case of them.  Algorithms supporting inference are presented.  Parsimonious methods for eliciting dependent probabilities are presented.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1472",
        "title": "Some Complexity Considerations in the Combination of Belief Networks",
        "authors": [
            "Izhar Matzkevich",
            "Bruce Abramson"
        ],
        "abstract": "One topic that is likely to attract an increasing amount of attention within the Knowledge-base systems research community is the coordination of information provided by multiple experts.  We envision a situation in which several experts independently encode information as belief networks.  A potential user must then coordinate the conclusions and recommendations of these networks to derive some sort of consensus.  One approach to such a consensus is the fusion of the contributed networks into a single, consensus model prior to the consideration of any case-specific data (specific observations, test results).  This approach requires two types of combination procedures, one for probabilities, and one for graphs.  Since the combination of probabilities is relatively well understood, the key barriers to this approach lie in the realm of graph theory.  This paper provides formal definitions of some of the operations necessary to effect the necessary graphical combinations, and provides complexity analyses of these procedures.  The paper's key result is that most of these operations are NP-hard, and its primary message is that the derivation of ?good? consensus networks must be done heuristically.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1473",
        "title": "Deriving a Minimal I-map of a Belief Network Relative to a Target Ordering of its Nodes",
        "authors": [
            "Izhar Matzkevich",
            "Bruce Abramson"
        ],
        "abstract": "This paper identifies and solves a new optimization problem: Given a belief network (BN) and a target ordering on its variables, how can we efficiently derive its minimal I-map whose arcs are consistent with the target ordering?  We present three solutions to this problem, all of which lead to directed acyclic graphs based on the original BN's recursive basis relative to the specified ordering (such a DAG is sometimes termed the boundary DAG drawn from the given BN relative to the said ordering [5]).  Along the way, we also uncover an important general principal about arc reversals: when reordering a BN according to some target ordering, (while attempting to minimize the number of arcs generated), the sequence of arc reversals should follow the topological ordering induced by the original belief network's arcs to as great an extent as possible.  These results promise to have a significant impact on the derivation of consensus models, as well as on other algorithms that require the reconfiguration and/or combination of BN's.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1474",
        "title": "Probabilistic Conceptual Network: A Belief Representation Scheme for Utility-Based Categorization",
        "authors": [
            "Kim-Leng Poh",
            "Michael R. Fehling"
        ],
        "abstract": "Probabilistic conceptual network is a knowledge representation scheme designed for reasoning about concepts and categorical abstractions in utility-based categorization.  The scheme combines the formalisms of abstraction and inheritance hierarchies from artificial intelligence, and probabilistic networks from decision analysis.  It provides a common framework for representing conceptual knowledge, hierarchical knowledge, and uncertainty.  It facilitates dynamic construction of categorization decision models at varying levels of abstraction.  The scheme is applied to an automated machining problem for reasoning about the state of the machine at varying levels of abstraction in support of actions for maintaining competitiveness of the plant.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1475",
        "title": "Reasoning about the Value of Decision-Model Refinement: Methods and Application",
        "authors": [
            "Kim-Leng Poh",
            "Eric J. Horvitz"
        ],
        "abstract": "We investigate the value of extending the completeness of a decision model along different dimensions of refinement.  Specifically, we analyze the expected value of quantitative, conceptual, and structural refinement of decision models.  We illustrate the key dimensions of refinement with examples.  The analyses of value of model refinement can be used to focus the attention of an analyst or an automated reasoning system on extensions of a decision model associated with the greatest expected value.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1476",
        "title": "Mixtures of Gaussians and Minimum Relative Entropy Techniques for Modeling Continuous Uncertainties",
        "authors": [
            "William B. Poland",
            "Ross D. Shachter"
        ],
        "abstract": "Problems of probabilistic inference and decision making under uncertainty commonly involve continuous random variables.  Often these are discretized to a few points, to simplify assessments and computations.  An alternative approximation is to fit analytically tractable continuous probability distributions.  This approach has potential simplicity and accuracy advantages, especially if variables can be transformed first.  This paper shows how a minimum relative entropy criterion can drive both transformation and fitting, illustrating with a power and logarithm family of transformations and mixtures of Gaussian (normal) distributions, which allow use of efficient influence diagram methods. The fitting procedure in this case is the well-known EM algorithm.  The selection of the number of components in a fitted mixture distribution is automated with an objective that trades off accuracy and computational cost.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1477",
        "title": "Valuation Networks and Conditional Independence",
        "authors": [
            "Prakash P. Shenoy"
        ],
        "abstract": "Valuation networks have been proposed as graphical representations of valuation-based systems (VBSs).  The VBS framework is able to capture many uncertainty calculi including probability theory, Dempster-Shafer's belief-function theory, Spohn's epistemic belief theory, and Zadeh's possibility theory.  In this paper, we show how valuation networks encode conditional independence relations.  For the probabilistic case, the class of probability models encoded by valuation networks includes undirected graph models, directed acyclic graph models, directed balloon graph models, and recursive causal graph models.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1478",
        "title": "Relevant Explanations: Allowing Disjunctive Assignments",
        "authors": [
            "Solomon Eyal Shimony"
        ],
        "abstract": "Relevance-based explanation is a scheme in which partial assignments to Bayesian belief network variables are explanations (abductive conclusions).  We allow variables to remain unassigned in explanations as long as they are irrelevant to the explanation, where irrelevance is defined in terms of statistical independence.  When multiple-valued variables exist in the system, especially when subsets of values correspond to natural types of events, the over specification problem, alleviated by independence-based explanation, resurfaces.  As a solution to that, as well as for addressing the question of explanation specificity, it is desirable to collapse such a subset of values into a single value on the fly.  The equivalent method, which is adopted here, is to generalize the notion of assignments to allow disjunctive assignments.   We proceed to define generalized independence based explanations as maximum posterior probability independence based generalized assignments (GIB-MAPs).  GIB assignments are shown to have certain properties that ease the design of algorithms for computing GIB-MAPs. One such algorithm is discussed here, as well as suggestions for how other algorithms may be adapted to compute GIB-MAPs. GIB-MAP explanations still suffer from instability, a problem which may be addressed using ?approximate? conditional independence as a condition for irrelevance.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1479",
        "title": "A Generalization of the Noisy-Or Model",
        "authors": [
            "Sampath Srinivas"
        ],
        "abstract": "The Noisy-Or model is convenient for describing a class of uncertain relationships in Bayesian networks [Pearl 1988]. Pearl describes the Noisy-Or model for Boolean variables.  Here we generalize the model to nary input and output variables and to arbitrary functions other than the Boolean OR function.  This generalization is a useful modeling aid for construction of Bayesian networks.  We illustrate with some examples including digital circuit diagnosis and network reliability analysis.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1480",
        "title": "Using First-Order Probability Logic for the Construction of Bayesian Networks",
        "authors": [
            "Fahiem Bacchus"
        ],
        "abstract": "We present a mechanism for constructing graphical models, specifically Bayesian networks, from a knowledge base of general probabilistic information.  The unique feature of our approach is that it uses a powerful first-order probabilistic logic for expressing the general knowledge base.  This logic allows for the representation of a wide range of logical and probabilistic information. The model construction procedure we propose uses notions from direct inference to identify pieces of local statistical information from the knowledge base that are most appropriate to the particular event we want to reason about.  These pieces are composed to generate a joint probability distribution specified as a Bayesian network.  Although there are fundamental difficulties in dealing with fully general knowledge, our procedure is practical for quite rich knowledge bases and it supports the construction of a far wider range of networks than allowed for by current template technology.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1481",
        "title": "Representing and Reasoning With Probabilistic Knowledge: A Bayesian Approach",
        "authors": [
            "Marie desJardins"
        ],
        "abstract": "PAGODA (Probabilistic Autonomous Goal-Directed Agent) is a model for autonomous learning in probabilistic domains [desJardins, 1992] that incorporates innovative techniques for using the agent's existing knowledge to guide and constrain the learning process and for representing, reasoning with, and learning probabilistic knowledge.  This paper describes the probabilistic representation and inference mechanism used in PAGODA.  PAGODA forms theories about the effects of its actions and the world state on the environment over time.  These theories are represented as conditional probability distributions.  A restriction is imposed on the structure of the theories that allows the inference mechanism to find a unique predicted distribution for any action and world state description.  These restricted theories are called uniquely predictive theories.  The inference mechanism, Probability Combination using Independence (PCI), uses minimal independence assumptions to combine the probabilities in a theory to make probabilistic predictions.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1482",
        "title": "Graph-Grammar Assistance for Automated Generation of Influence Diagrams",
        "authors": [
            "John W. Egar",
            "Mark A. Musen"
        ],
        "abstract": "One of the most difficult aspects of modeling complex dilemmas in decision-analytic terms is composing a diagram of relevance relations from a set of domain concepts. Decision models in domains such as medicine, however, exhibit certain prototypical patterns that can guide the modeling process.  Medical concepts can be classified according to semantic types that have characteristic positions and typical roles in an influence-diagram model.  We have developed a graph-grammar production system that uses such inherent interrelationships among medical terms to facilitate the modeling of medical decisions.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1483",
        "title": "Using Causal Information and Local Measures to Learn Bayesian Networks",
        "authors": [
            "Wai Lam",
            "Fahiem Bacchus"
        ],
        "abstract": "In previous work we developed a method of learning Bayesian Network models from raw data.  This method relies on the well known minimal description length (MDL) principle. The MDL principle is particularly well suited to this task as it allows us to tradeoff, in a principled way, the accuracy of the learned network against its practical usefulness.  In this paper we present some new results that have arisen from our work. In particular, we present a new local way of computing the description length.  This allows us to make significant improvements in our search algorithm.  In addition, we modify our algorithm so that it can take into account partial domain information that might be provided by a domain expert.  The local computation of description length also opens the door for local refinement of an existent network.  The feasibility of our approach is demonstrated by experiments involving networks of a practical size.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1484",
        "title": "Minimal Assumption Distribution Propagation in Belief Networks",
        "authors": [
            "Ron Musick"
        ],
        "abstract": "As belief networks are used to model increasingly complex situations, the need to automatically construct them from large databases will become paramount.  This paper concentrates on solving a part of the belief network induction problem: that of learning the quantitative structure (the conditional probabilities), given the qualitative structure.  In particular, a theory is presented that shows how to propagate inference distributions in a belief network, with the only assumption being that the given qualitative structure is correct.  Most inference algorithms must make at least this assumption.  The theory is based on four network transformations that are sufficient for any inference in a belief network.  Furthermore, the claim is made that contrary to popular belief, error will not necessarily grow as the inference chain grows.  Instead, for QBN belief nets induced from large enough samples, the error is more likely to decrease as the size of the inference chain increases.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1485",
        "title": "An Algorithm for the Construction of Bayesian Network Structures from Data",
        "authors": [
            "Moninder Singh",
            "Marco Valtorta"
        ],
        "abstract": "Previous algorithms for the construction of Bayesian belief network structures from data have been either highly dependent on conditional independence (CI) tests, or have required an ordering on the nodes to be supplied by the user.  We present an algorithm that integrates these two approaches - CI tests are used to generate an ordering on the nodes from the database which is then used to recover the underlying Bayesian network structure using a non CI based method.  Results of preliminary evaluation of the algorithm on two networks (ALARM and LED) are presented.  We also discuss some algorithm performance issues and open problems.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1486",
        "title": "A Construction of Bayesian Networks from Databases Based on an MDL Principle",
        "authors": [
            "Joe Suzuki"
        ],
        "abstract": "This paper addresses learning stochastic rules especially on an inter-attribute relation based on a Minimum Description Length (MDL) principle with a finite number of examples, assuming an application to the design of intelligent relational database systems.  The stochastic rule in this paper consists of a model giving the structure like the dependencies of a Bayesian Belief Network (BBN) and some stochastic parameters each indicating a conditional probability of an attribute value given the state determined by the other attributes' values in the same record.  Especially, we propose the extended version of the algorithm of Chow and Liu in that our learning algorithm selects the model in the range where the dependencies among the attributes are represented by some general plural number of trees.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1487",
        "title": "Knowledge-Based Decision Model Construction for Hierarchical Diagnosis: A Preliminary Report",
        "authors": [
            "Soe-Tsyr Yuan"
        ],
        "abstract": "Numerous methods for probabilistic reasoning in large, complex belief or decision networks are currently being developed.  There has been little research on automating the dynamic, incremental construction of decision models.  A uniform value-driven method of decision model construction is proposed for the hierarchical complete diagnosis. Hierarchical complete diagnostic reasoning is formulated as a stochastic process and modeled using influence diagrams.  Given observations, this method creates decision models in order to obtain the best actions sequentially for locating and repairing a fault at minimum cost.  This method construct decision models incrementally, interleaving probe actions with model construction and evaluation.  The method treats meta-level and baselevel tasks uniformly.  That is, the method takes a decision-theoretic look at the control of search in causal pathways and structural hierarchies.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1488",
        "title": "A Synthesis of Logical and Probabilistic Reasoning for Program Understanding and Debugging",
        "authors": [
            "Lisa J. Burnell",
            "Eric J. Horvitz"
        ],
        "abstract": "We describe the integration of logical and uncertain reasoning methods to identify the likely source and location of software problems.  To date, software engineers have had few tools for identifying the sources of error in complex software packages.  We describe a method for diagnosing software problems through combining logical and uncertain reasoning analyses.  Our preliminary results suggest that such methods can be of value in directing the attention of software engineers to paths of an algorithm that have the highest likelihood of harboring a programming error.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1489",
        "title": "An Implementation of a Method for Computing the Uncertainty in Inferred Probabilities in Belief Networks",
        "authors": [
            "Peter Che",
            "Richard E. Neapolitan",
            "James Kenevan",
            "Martha Evens"
        ],
        "abstract": "In recent years the belief network has been used increasingly to model systems in Al that must perform uncertain inference.  The development of efficient algorithms for probabilistic inference in belief networks has been a focus of much research in AI. Efficient algorithms for certain classes of belief networks have been developed, but the problem of reporting the uncertainty in inferred probabilities has received little attention. A system should not only be capable of reporting the values of inferred probabilities and/or the favorable choices of a decision; it should report the range of possible error in the inferred probabilities and/or choices.  Two methods have been developed and implemented for determining the variance in inferred probabilities in belief networks. These methods, the Approximate Propagation Method and the Monte Carlo Integration Method are discussed and compared in this paper.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1490",
        "title": "Incremental Probabilistic Inference",
        "authors": [
            "Bruce D'Ambrosio"
        ],
        "abstract": "Propositional representation services such as truth maintenance systems offer powerful support for incremental, interleaved, problem-model construction and evaluation. Probabilistic inference systems, in contrast, have lagged behind in supporting this incrementality typically demanded by problem solvers.  The problem, we argue, is that the basic task of probabilistic inference is typically formulated at too large a grain-size. We show how a system built around a smaller grain-size inference task can have the desired incrementality and serve as the basis for a low-level (propositional) probabilistic representation service.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1491",
        "title": "Deliberation Scheduling for Time-Critical Sequential Decision Making",
        "authors": [
            "Thomas L. Dean",
            "Leslie Pack Kaelbling",
            "Jak Kirman",
            "Ann Nicholson"
        ],
        "abstract": "We describe a method for time-critical decision making involving sequential tasks and stochastic processes.  The method employs several iterative refinement routines for solving different aspects of the decision making problem.  This paper concentrates on the meta-level control problem of deliberation scheduling, allocating computational resources to these routines.  We provide different models corresponding to optimization problems that capture the different circumstances and computational strategies for decision making under time constraints.  We consider precursor models in which all decision making is performed prior to execution and recurrent models in which decision making is performed in parallel with execution, accounting for the states observed during execution and anticipating future states.  We describe algorithms for precursor and recurrent models and provide the results of our empirical investigations to date.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1492",
        "title": "Intercausal Reasoning with Uninstantiated Ancestor Nodes",
        "authors": [
            "Marek J. Druzdzel",
            "Max Henrion"
        ],
        "abstract": "Intercausal reasoning is a common inference pattern involving probabilistic dependence of causes of an observed common effect.  The sign of this dependence is captured by a qualitative property called product synergy.  The current definition of product synergy is insufficient for intercausal reasoning where there are additional uninstantiated causes of the common effect.  We propose a new definition of product synergy and prove its adequacy for intercausal reasoning with direct and indirect evidence for the common effect.  The new definition is based on a new property matrix half positive semi-definiteness, a weakened form of matrix positive semi-definiteness.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1493",
        "title": "Inference Algorithms for Similarity Networks",
        "authors": [
            "Dan Geiger",
            "David Heckerman"
        ],
        "abstract": "We examine two types of similarity networks each based on a distinct notion of relevance. For both types of similarity networks we present an efficient inference algorithm that works under the assumption that every event has a nonzero probability of occurrence. Another inference algorithm is developed for type 1 similarity networks that works under no restriction, albeit less efficiently.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1494",
        "title": "Two Procedures for Compiling Influence Diagrams",
        "authors": [
            "Paul E. Lehner",
            "Azar Sadigh"
        ],
        "abstract": "Two algorithms are presented for \"compiling\" influence diagrams into a set of simple decision rules.  These decision rules define simple-to-execute, complete, consistent, and near-optimal decision procedures.  These compilation algorithms can be used to derive decision procedures for human teams solving time constrained decision problems.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1495",
        "title": "An efficient approach for finding the MPE in belief networks",
        "authors": [
            "Zhaoyu Li",
            "Bruce D'Ambrosio"
        ],
        "abstract": "Given a belief network with evidence, the task of finding the I most probable explanations (MPE) in the belief network is that of identifying and ordering the I most probable instantiations of the non-evidence nodes of the belief network.  Although many approaches have been proposed for solving this problem, most work only for restricted topologies (i.e., singly connected belief networks).  In this paper, we will present a new approach for finding I MPEs in an arbitrary belief network.  First, we will present an algorithm for finding the MPE in a belief network.  Then, we will present a linear time algorithm for finding the next MPE after finding the first MPE.  And finally, we will discuss the problem of finding the MPE for a subset of variables of a belief network, and show that the problem can be efficiently solved by this approach.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1496",
        "title": "A Method for Planning Given Uncertain and Incomplete Information",
        "authors": [
            "Todd Michael Mansell"
        ],
        "abstract": "This paper describes ongoing research into planning in an uncertain environment.  In particular, it introduces U-Plan, a planning system that constructs quantitatively ranked plans given an incomplete description of the state of the world.  U-Plan uses a DempsterShafer interval to characterise uncertain and incomplete information about the state of the world.  The planner takes as input what is known about the world, and constructs a number of possible initial states with representations at different abstraction levels.  A plan is constructed for the initial state with the greatest support, and this plan is tested to see if it will work for other possible initial states.  All, part, or none of the existing plans may be used in the generation of the plans for the remaining possible worlds.  Planning takes place in an abstraction hierarchy where strategic decisions are made before tactical decisions.  A super-plan is then constructed, based on merging the set of plans and the appropriately timed acquisition of essential knowledge, which is used to decide between plan alternatives.  U-Plan usually produces a super-plan in less time than a classical planner would take to produce a set of plans, one for each possible world.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1497",
        "title": "The use of conflicts in searching Bayesian networks",
        "authors": [
            "David L. Poole"
        ],
        "abstract": "This paper discusses how conflicts (as used by the consistency-based diagnosis community) can be adapted to be used in a search-based algorithm for computing prior and posterior probabilities in discrete Bayesian Networks.  This is an \"anytime\" algorithm, that at any stage can estimate the probabilities and give an error bound. Whereas the most popular Bayesian net algorithms exploit the structure of the network for efficiency, we exploit probability distributions for efficiency; this algorithm is most suited to the case with extreme probabilities.  This paper presents a solution to the inefficiencies found in naive algorithms, and shows how the tools of the consistency-based diagnosis community (namely conflicts) can be used effectively to improve the efficiency.  Empirical results with networks having tens of thousands of nodes are presented.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1498",
        "title": "GALGO: A Genetic ALGOrithm Decision Support Tool for Complex Uncertain Systems Modeled with Bayesian Belief Networks",
        "authors": [
            "Carlos Rojas-Guzman",
            "Mark A. Kramer"
        ],
        "abstract": "Bayesian belief networks can be used to represent and to reason about complex systems with uncertain, incomplete and conflicting information.  Belief networks are graphs encoding and quantifying probabilistic dependence and conditional independence among variables.  One type of reasoning of interest in diagnosis is called abductive inference (determination of the global most probable system description given the values of any partial subset of variables).  In some cases, abductive inference can be performed with exact algorithms using distributed network computations but it is an NP-hard problem and complexity increases drastically with the presence of undirected cycles, number of discrete states per variable, and number of variables in the network.  This paper describes an approximate method based on genetic algorithms to perform abductive inference in large, multiply connected networks for which complexity is a concern when using most exact methods and for which systematic search methods are not feasible.  The theoretical adequacy of the method is discussed and preliminary experimental results are presented.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1499",
        "title": "Using Tree-Decomposable Structures to Approximate Belief Networks",
        "authors": [
            "Sumit Sarkar"
        ],
        "abstract": "Tree structures have been shown to provide an efficient framework for propagating beliefs [Pearl,1986].  This paper studies the problem of finding an optimal approximating tree.  The star decomposition scheme for sets of three binary variables [Lazarsfeld,1966; Pearl,1986] is shown to enhance the class of probability distributions that can support tree structures; such structures are called tree-decomposable structures.  The logarithm scoring rule is found to be an appropriate optimality criterion to evaluate different tree-decomposable structures.  Characteristics of such structures closest to the actual belief network are identified using the logarithm rule, and greedy and exact techniques are developed to find the optimal approximation.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1500",
        "title": "Using Potential Influence Diagrams for Probabilistic Inference and Decision Making",
        "authors": [
            "Ross D. Shachter",
            "Pierre Ndilikilikesha"
        ],
        "abstract": "The potential influence diagram is a generalization of the standard \"conditional\" influence diagram, a directed network representation for probabilistic inference and decision analysis [Ndilikilikesha, 1991].  It allows efficient inference calculations corresponding exactly to those on undirected graphs. In this paper, we explore the relationship between potential and conditional influence diagrams and provide insight into the properties of the potential influence diagram.  In particular, we show how to convert a potential influence diagram into a conditional influence diagram, and how to view the potential influence diagram operations in terms of the conditional influence diagram.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1501",
        "title": "Deciding Morality of Graphs is NP-complete",
        "authors": [
            "Tom S. Verma",
            "Judea Pearl"
        ],
        "abstract": "In order to find a causal explanation for data presented in the form of covariance and concentration matrices it is necessary to decide if the graph formed by such associations is a projection of a directed acyclic graph (dag).  We show that the general problem of deciding whether such a dag exists is NP-complete.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1502",
        "title": "Incremental computation of the value of perfect information in stepwise-decomposable influence diagrams",
        "authors": [
            "Nevin Lianwen Zhang",
            "Runping Qi",
            "David L. Poole"
        ],
        "abstract": "To determine the value of perfect information in an influence diagram, one needs first to modify the diagram to reflect the change in information availability, and then to compute the optimal expected values of both the original diagram and the modified diagram.  The value of perfect information is the difference between the two optimal expected values. This paper is about how to speed up the computation of the optimal expected value of the modified diagram by making use of the intermediate computation results obtained when computing the optimal expected value of the original diagram.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1503",
        "title": "Argumentative inference in uncertain and inconsistent knowledge bases",
        "authors": [
            "Salem Benferhat",
            "Didier Dubois",
            "Henri Prade"
        ],
        "abstract": "This paper presents and discusses several methods for reasoning from inconsistent knowledge bases.  A so-called argumentative-consequence relation taking into account the existence of consistent arguments in favor of a conclusion and the absence of consistent arguments in favor of its contrary, is particularly investigated.  Flat knowledge bases, i.e. without any priority between their elements, as well as prioritized ones where some elements are considered as more strongly entrenched than others are studied under different consequence relations.  Lastly a paraconsistent-like treatment of prioritized knowledge bases is proposed, where both the level of entrenchment and the level of paraconsistency attached to a formula are propagated.  The priority levels are handled in the framework of possibility theory.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1504",
        "title": "Argument Calculus and Networks",
        "authors": [
            "Adnan Darwiche"
        ],
        "abstract": "A major reason behind the success of probability calculus is that it possesses a number of valuable tools, which are based on the notion of probabilistic independence.  In this paper, I identify a notion of logical independence that makes some of these tools available to a class of propositional databases, called argument databases.  Specifically, I suggest a graphical representation of argument databases, called argument networks, which resemble Bayesian networks.  I also suggest an algorithm for reasoning with argument networks, which resembles a basic algorithm for reasoning with Bayesian networks.  Finally, I show that argument networks have several applications: Nonmonotonic reasoning, truth maintenance, and diagnosis.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1505",
        "title": "Argumentation as a General Framework for Uncertain Reasoning",
        "authors": [
            "John Fox",
            "Paul J. Krause",
            "Morten Elvang-G\u00f8ransson"
        ],
        "abstract": "Argumentation is the process of constructing arguments about propositions, and the assignment of statements of confidence to those propositions based on the nature and relative strength of their supporting arguments.  The process is modelled as a labelled deductive system, in which propositions are doubly labelled with the grounds on which they are based and a representation of the confidence attached to the argument.  Argument construction is captured by a generalized argument consequence relation based on the ^,--fragment of minimal logic.  Arguments can be aggregated by a variety of numeric and symbolic flattening functions.  This approach appears to shed light on the common logical structure of a variety of quantitative, qualitative and defeasible uncertainty calculi.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1506",
        "title": "On reasoning in networks with qualitative uncertainty",
        "authors": [
            "Simon Parsons",
            "E. H. Mamdani"
        ],
        "abstract": "In this paper some initial work towards a new approach to qualitative reasoning under uncertainty is presented.  This method is not only applicable to qualitative probabilistic reasoning, as is the case with other methods, but also allows the qualitative propagation within networks of values based upon possibility theory and Dempster-Shafer evidence theory.  The method is applied to two simple networks from which a large class of directed graphs may be constructed.  The results of this analysis are used to compare the qualitative behaviour of the three major quantitative uncertainty handling formalisms, and to demonstrate that the qualitative integration of the formalisms is possible under certain assumptions.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1507",
        "title": "Qualitative Measures of Ambiguity",
        "authors": [
            "Michael S. K. M. Wong",
            "Z. W. Wang"
        ],
        "abstract": "This paper introduces a qualitative measure of ambiguity and analyses its relationship with other measures of uncertainty.  Probability measures relative likelihoods, while ambiguity measures vagueness surrounding those judgments.  Ambiguity is an important representation of uncertain knowledge.  It deals with a different, type of uncertainty modeled by subjective probability or belief.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1508",
        "title": "A Bayesian Variant of Shafer's Commonalities For Modelling Unforeseen Events",
        "authors": [
            "Robert F. Bordley"
        ],
        "abstract": "Shafer's theory of belief and the Bayesian theory of probability are two alternative and mutually inconsistent approaches toward modelling uncertainty in artificial intelligence. To help reduce the conflict between these two approaches, this paper reexamines expected utility theory-from which Bayesian probability theory is derived.  Expected utility theory requires the decision maker to assign a utility to each decision conditioned on every possible event that might occur.  But frequently the decision maker cannot foresee all the events that might occur, i.e., one of the possible events is the occurrence of an unforeseen event.  So once we acknowledge the existence of unforeseen events, we need to develop some way of assigning utilities to decisions conditioned on unforeseen events.  The commonsensical solution to this problem is to assign similar utilities to events which are similar.  Implementing this commonsensical solution is equivalent to replacing Bayesian subjective probabilities over the space of foreseen and unforeseen events by random set theory probabilities over the space of foreseen events.  This leads to an expected utility principle in which normalized variants of Shafer's commonalities play the role of subjective probabilities.  Hence allowing for unforeseen events in decision analysis causes Bayesian probability theory  to become much more similar to Shaferian theory.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1509",
        "title": "The Probability of a Possibility: Adding Uncertainty to Default Rules",
        "authors": [
            "Craig Boutilier"
        ],
        "abstract": "We present a semantics for adding uncertainty to conditional logics for default reasoning and belief revision.  We are able to treat conditional sentences as statements of conditional probability, and express rules for revision such as \"If A were believed, then B would be believed to degree p.\"  This method of revision extends conditionalization by allowing meaningful revision by sentences whose probability is zero.  This is achieved through the use of counterfactual probabilities.  Thus, our system accounts for the best properties of qualitative methods of update (in particular, the AGM theory of revision) and probabilistic methods.  We also show how our system can be viewed as a unification of probability theory and possibility theory, highlighting their orthogonality and providing a means for expressing the probability of a possibility.  We also demonstrate the connection to Lewis's method of imaging.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1510",
        "title": "Possibilistic decreasing persistence",
        "authors": [
            "Dimiter Driankov",
            "Jerome Lang"
        ],
        "abstract": "A key issue in the handling of temporal data is the treatment of persistence; in most approaches it consists in inferring defeasible confusions by extrapolating from the actual knowledge of the history of the world; we propose here a gradual modelling of persistence, following the idea that persistence is decreasing (the further we are from the last time point where a fluent is known to be true, the less certainly true the fluent is); it is based on possibility theory, which has strong relations with other well-known ordering-based approaches to nonmonotonic reasoning. We compare our approach with Dean and Kanazawa's probabilistic projection.  We give a formal modelling of the decreasing persistence problem.  Lastly, we show how to infer nonmonotonic conclusions using the principle of decreasing persistence.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1511",
        "title": "Discounting and Combination Operations in Evidential Reasoning",
        "authors": [
            "Jiwen W. Guan",
            "David A. Bell"
        ],
        "abstract": "Evidential reasoning is now a leading topic in Artificial Intelligence.  Evidence is represented by a variety of evidential functions.  Evidential reasoning is carried out by certain kinds of fundamental operation on these functions.  This paper discusses two of the basic operations on evidential functions, the discount operation and the well-known orthogonal sum operation.  We show that the discount operation is not commutative with the orthogonal sum operation, and derive expressions for the two operations applied to the various evidential function.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1512",
        "title": "Probabilistic Assumption-Based Reasoning",
        "authors": [
            "Jurg Kohlas",
            "Paul-Andre Monney"
        ],
        "abstract": "The classical propositional assumption-based model is extended to incorporate probabilities for the assumptions.  Then it is placed into the framework of evidence theory.  Several authors like Laskey, Lehner (1989) and Provan (1990) already proposed a similar point of view, but the first paper is not as much concerned with mathematical foundations, and Provan's paper develops into a different direction.  Here we thoroughly develop and present the mathematical foundations of this theory, together with computational methods adapted from Reiter, De Kleer (1987) and Inoue (1992).  Finally, recently proposed techniques for computing degrees of support are presented.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1513",
        "title": "Partially Specified Belief Functions",
        "authors": [
            "Serafin Moral",
            "Luis M. de Campos"
        ],
        "abstract": "This paper presents a procedure to determine a complete belief function from the known values of belief for some of the subsets of the frame of discerment.  The method is based on the principle of minimum commitment and a new principle called the focusing principle.  This additional principle is based on the idea that belief is specified for the most relevant sets: the focal elements.  The resulting procedure is compared with existing methods of building complete belief functions: the minimum specificity principle and the least commitment principle.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1514",
        "title": "Jeffrey's rule of conditioning generalized to belief functions",
        "authors": [
            "Philippe Smets"
        ],
        "abstract": "Jeffrey's rule of conditioning has been proposed in order to revise a probability measure by another probability function.  We generalize it within the framework of the models based on belief functions.  We show that several forms of Jeffrey's conditionings can be defined that correspond to the geometrical rule of conditioning and to Dempster's rule of conditioning, respectively.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1515",
        "title": "Inference with Possibilistic Evidence",
        "authors": [
            "Fengming Song",
            "Ping Liang"
        ],
        "abstract": "In this paper, the concept of possibilistic evidence which is a possibility distribution as well as a body of evidence is proposed over an infinite universe of discourse.  The inference with possibilistic evidence is investigated based on a unified inference framework maintaining both the compatibility of concepts and the consistency of the probability logic.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1516",
        "title": "Constructing Lower Probabilities",
        "authors": [
            "Carl G. Wagner",
            "Bruce Tonn"
        ],
        "abstract": "An elaboration of Dempster's method of constructing belief functions suggests a broadly applicable strategy for constructing lower probabilities under a variety of evidentiary constraints.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1517",
        "title": "Belief Revision in Probability Theory",
        "authors": [
            "Pei Wang"
        ],
        "abstract": "In a probability-based reasoning system, Bayes' theorem and its variations are often used to revise the system's beliefs.  However, if the explicit conditions and the implicit conditions of probability assignments `me properly distinguished, it follows that Bayes' theorem is not a generally applicable revision rule.  Upon properly distinguishing belief revision from belief updating, we see that Jeffrey's rule and its variations are not revision rules, either.  Without these distinctions, the limitation of the Bayesian approach is often ignored or underestimated.  Revision, in its general form, cannot be done in the Bayesian approach, because a probability distribution function alone does not contain the information needed by the operation.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1518",
        "title": "The Assumptions Behind Dempster's Rule",
        "authors": [
            "Nic Wilson"
        ],
        "abstract": "This paper examines the concept of a combination rule for belief functions.  It is shown that two fairly simple and apparently reasonable assumptions determine Dempster's rule, giving a new justification for it.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1519",
        "title": "A Belief-Function Based Decision Support System",
        "authors": [
            "Hong Xu",
            "Yen-Teh Hsia",
            "Philippe Smets"
        ],
        "abstract": "In this paper, we present a decision support system based on belief functions and the pignistic transformation.  The system is an integration of an evidential system for belief function propagation and a valuation-based system for Bayesian decision analysis.  The two subsystems are connected through the pignistic transformation.  The system takes as inputs the user's \"gut feelings\" about a situation and suggests what, if any, are to be tested and in what order, and it does so with a user friendly interface.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1700",
        "title": "K-Nearest Neighbour algorithm coupled with logistic regression in medical case-based reasoning systems. Application to prediction of access to the renal transplant waiting list in Brittany",
        "authors": [
            "Boris Campillo-Gimenez",
            "Wassim Jouini",
            "Sahar Bayat",
            "Marc Cuggia"
        ],
        "abstract": "Introduction. Case Based Reasoning (CBR) is an emerg- ing decision making paradigm in medical research where new cases are solved relying on previously solved similar cases. Usually, a database of solved cases is provided, and every case is described through a set of attributes (inputs) and a label (output). Extracting useful information from this database can help the CBR system providing more reliable results on the yet to be solved cases. Objective. For that purpose we suggest a general frame- work where a CBR system, viz. K-Nearest Neighbor (K-NN) algorithm, is combined with various information obtained from a Logistic Regression (LR) model. Methods. LR is applied, on the case database, to assign weights to the attributes as well as the solved cases. Thus, five possible decision making systems based on K-NN and/or LR were identified: a standalone K-NN, a standalone LR and three soft K-NN algorithms that rely on the weights based on the results of the LR. The evaluation of the described approaches is performed in the field of renal transplant access waiting list. Results and conclusion. The results show that our suggested approach, where the K-NN algorithm relies on both weighted attributes and cases, can efficiently deal with non relevant attributes, whereas the four other approaches suffer from this kind of noisy setups. The robustness of this approach suggests interesting perspectives for medical problem solving tools using CBR methodology.\n    ",
        "submission_date": "2013-03-07T00:00:00",
        "last_modified_date": "2013-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.2013",
        "title": "Computing as compression: the SP theory of intelligence",
        "authors": [
            "J Gerard Wolff"
        ],
        "abstract": "This paper provides an overview of the SP theory of intelligence and its central idea that artificial intelligence, mainstream computing, and much of human perception and cognition, may be understood as information compression.\n",
        "submission_date": "2013-03-08T00:00:00",
        "last_modified_date": "2013-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.2430",
        "title": "Quantum and Concept Combination, Entangled Measurements and Prototype Theory",
        "authors": [
            "Diederik Aerts"
        ],
        "abstract": "We analyze the meaning of the violation of the marginal probability law for situations of correlation measurements where entanglement is identified. We show that for quantum theory applied to the cognitive realm such a violation does not lead to the type of problems commonly believed to occur in situations of quantum theory applied to the physical realm. We briefly situate our quantum approach for modeling concepts and their combinations with respect to the notions of 'extension' and 'intension' in theories of meaning, and in existing concept theories.\n    ",
        "submission_date": "2013-03-11T00:00:00",
        "last_modified_date": "2013-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.2860",
        "title": "Fairness in Academic Course Timetabling",
        "authors": [
            "Moritz M\u00fchlenthaler",
            "Rolf Wanka"
        ],
        "abstract": "We consider the problem of creating fair course timetables in the setting of a university. Our motivation is to improve the overall satisfaction of individuals concerned (students, teachers, etc.) by providing a fair timetable to them. The central idea is that undesirable arrangements in the course timetable, i.e., violations of soft constraints, should be distributed in a fair way among the individuals. We propose two formulations for the fair course timetabling problem that are based on max-min fairness and Jain's fairness index, respectively. Furthermore, we present and experimentally evaluate an optimization algorithm based on simulated annealing for solving max-min fair course timetabling problems. The new contribution is concerned with measuring the energy difference between two timetables, i.e., how much worse a timetable is compared to another timetable with respect to max-min fairness. We introduce three different energy difference measures and evaluate their impact on the overall algorithm performance. The second proposed problem formulation focuses on the tradeoff between fairness and the total amount of soft constraint violations. Our experimental evaluation shows that the known best solutions to the ITC2007 curriculum-based course timetabling instances are quite fair with respect to Jain's fairness index. However, the experiments also show that the fairness can be improved further for only a rather small increase in the total amount of soft constraint violations.\n    ",
        "submission_date": "2013-03-12T00:00:00",
        "last_modified_date": "2013-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.2912",
        "title": "Integrated Pre-Processing for Bayesian Nonlinear System Identification with Gaussian Processes",
        "authors": [
            "Roger Frigola",
            "Carl Edward Rasmussen"
        ],
        "abstract": "We introduce GP-FNARX: a new model for nonlinear system identification based on a nonlinear autoregressive exogenous model (NARX) with filtered regressors (F) where the nonlinear regression problem is tackled using sparse Gaussian processes (GP). We integrate data pre-processing with system identification into a fully automated procedure that goes from raw data to an identified model. Both pre-processing parameters and GP hyper-parameters are tuned by maximizing the marginal likelihood of the probabilistic model. We obtain a Bayesian model of the system's dynamics which is able to report its uncertainty in regions where the data is scarce. The automated approach, the modeling of uncertainty and its relatively low computational cost make of GP-FNARX a good candidate for applications in robotics and adaptive control.\n    ",
        "submission_date": "2013-03-12T00:00:00",
        "last_modified_date": "2013-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.3163",
        "title": "A Greedy Approximation of Bayesian Reinforcement Learning with Probably Optimistic Transition Model",
        "authors": [
            "Kenji Kawaguchi",
            "Mauricio Araya"
        ],
        "abstract": "Bayesian Reinforcement Learning (RL) is capable of not only incorporating domain knowledge, but also solving the exploration-exploitation dilemma in a natural way. As Bayesian RL is intractable except for special cases, previous work has proposed several approximation methods. However, these methods are usually too sensitive to parameter values, and finding an acceptable parameter setting is practically impossible in many applications. In this paper, we propose a new algorithm that greedily approximates Bayesian RL to achieve robustness in parameter space. We show that for a desired learning behavior, our proposed algorithm has a polynomial sample complexity that is lower than those of existing algorithms. We also demonstrate that the proposed algorithm naturally outperforms other existing algorithms when the prior distributions are not significantly misleading. On the other hand, the proposed algorithm cannot handle greatly misspecified priors as well as the other algorithms can. This is a natural consequence of the fact that the proposed algorithm is greedier than the other algorithms. Accordingly, we discuss a way to select an appropriate algorithm for different tasks based on the algorithms' greediness. We also introduce a new way of simplifying Bayesian planning, based on which future work would be able to derive new algorithms.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.4017",
        "title": "Separating Topology and Geometry in Space Planning",
        "authors": [
            "Benachir Medjdoub",
            "Bernard Yannou"
        ],
        "abstract": "We are dealing with the problem of space layout planning here. We present an architectural conceptual CAD approach. Starting with design specifications in terms of constraints over spaces, a specific enumeration heuristics leads to a complete set of consistent conceptual design solutions named topological solutions. These topological solutions which do not presume any precise definitive dimension correspond to the sketching step that an architect carries out from the Design specifications on a preliminary design phase in architecture.\n    ",
        "submission_date": "2013-03-16T00:00:00",
        "last_modified_date": "2013-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.4183",
        "title": "Generating extrema approximation of analytically incomputable functions through usage of parallel computer aided genetic algorithms",
        "authors": [
            "Lukasz Swierczewski"
        ],
        "abstract": "This paper presents capabilities of using genetic algorithms to find approximations of function extrema, which cannot be found using analytic ways. To enhance effectiveness of calculations, algorithm has been parallelized using OpenMP library. We gained much increase in speed on platforms using multithreaded processors with shared memory free access. During analysis we used different modifications of genetic operator, using them we obtained varied evolution process of potential solutions. Results allow to choose best methods among many applied in genetic algorithms and observation of acceleration on Yorkfield, Bloomfield, Westmere-EX and most recent Sandy Bridge cores.\n    ",
        "submission_date": "2013-03-18T00:00:00",
        "last_modified_date": "2013-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.4431",
        "title": "Generalized Thompson Sampling for Sequential Decision-Making and Causal Inference",
        "authors": [
            "Pedro A. Ortega",
            "Daniel A. Braun"
        ],
        "abstract": "Recently, it has been shown how sampling actions from the predictive distribution over the optimal action-sometimes called Thompson sampling-can be applied to solve sequential adaptive control problems, when the optimal policy is known for each possible environment. The predictive distribution can then be constructed by a Bayesian superposition of the optimal policies weighted by their posterior probability that is updated by Bayesian inference and causal calculus. Here we discuss three important features of this approach. First, we discuss in how far such Thompson sampling can be regarded as a natural consequence of the Bayesian modeling of policy uncertainty. Second, we show how Thompson sampling can be used to study interactions between multiple adaptive agents, thus, opening up an avenue of game-theoretic analysis. Third, we show how Thompson sampling can be applied to infer causal relationships when interacting with an environment in a sequential fashion. In summary, our results suggest that Thompson sampling might not merely be a useful heuristic, but a principled method to address problems of adaptive sequential decision-making and causal inference.\n    ",
        "submission_date": "2013-03-18T00:00:00",
        "last_modified_date": "2013-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5132",
        "title": "Discovering Semantic Spatial and Spatio-Temporal Outliers from Moving Object Trajectories",
        "authors": [
            "Vitor Cunha Fontes",
            "Vania Bogorny"
        ],
        "abstract": "Several algorithms have been proposed for discovering patterns from trajectories of moving objects, but only a few have concentrated on outlier detection. Existing approaches, in general, discover spatial outliers, and do not provide any further analysis of the patterns. In this paper we introduce semantic spatial and spatio-temporal outliers and propose a new algorithm for trajectory outlier detection. Semantic outliers are computed between regions of interest, where objects have similar movement intention, and there exist standard paths which connect the regions. We show with experiments on real data that the method finds semantic outliers from trajectory data that are not discovered by similar approaches.\n    ",
        "submission_date": "2013-03-21T00:00:00",
        "last_modified_date": "2013-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5177",
        "title": "Model Based Framework for Estimating Mutation Rate of Hepatitis C Virus in Egypt",
        "authors": [
            "Nabila Shikoun",
            "Mohamed El Nahas",
            "Samar Kassim"
        ],
        "abstract": "Hepatitis C virus (HCV) is a widely spread disease all over the world. HCV has very high mutation rate that makes it resistant to antibodies. Modeling HCV to identify the virus mutation process is essential to its detection and predicting its evolution. This paper presents a model based framework for estimating mutation rate of HCV in two steps. Firstly profile hidden Markov model (PHMM) architecture was builder to select the sequences which represents sequence per year. Secondly mutation rate was calculated by using pair-wise distance method between sequences. A pilot study is conducted on NS5B zone of HCV dataset of genotype 4 subtype a (HCV4a) in Egypt.\n    ",
        "submission_date": "2013-03-21T00:00:00",
        "last_modified_date": "2013-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5391",
        "title": "RES - a Relative Method for Evidential Reasoning",
        "authors": [
            "Zhi An",
            "David A. Bell",
            "John G. Hughes"
        ],
        "abstract": "In this paper we describe a novel method for evidential reasoning [1].  It involves modelling the process of evidential reasoning in three steps, namely, evidence structure construction, evidence accumulation, and decision making.  The proposed method, called RES, is novel in that evidence strength is associated with an evidential support relationship (an argument) between a pair of statements and such strength is carried by comparison between arguments.  This is in contrast to the onventional approaches, where evidence strength is represented numerically and is associated with a statement.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5392",
        "title": "Optimizing Causal Orderings for Generating DAGs from Data",
        "authors": [
            "Remco R. Bouckaert"
        ],
        "abstract": "An algorithm for generating the structure of a directed acyclic graph from data using the notion of causal input lists is presented.  The algorithm manipulates the ordering of the variables with operations which very much resemble arc reversal.  Operations are only applied if the DAG after the operation represents at least the independencies represented by the DAG before the operation until no more arcs can be removed from the DAG.  The resulting DAG is a minimal l-map.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5393",
        "title": "Modal Logics for Qualitative Possibility and Beliefs",
        "authors": [
            "Craig Boutilier"
        ],
        "abstract": "Possibilistic logic has been proposed as a numerical formalism for reasoning with uncertainty.  There has been interest in developing qualitative accounts of possibility, as well as an explanation of the relationship between possibility and modal logics.  We present two modal logics that can be used to represent and reason with qualitative statements of possibility and necessity.  Within this modal framework, we are able to identify interesting relationships between possibilistic logic, beliefs and conditionals.  In particular, the most natural conditional definable via possibilistic means for default reasoning is identical to Pearl's conditional for e-semantics.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5394",
        "title": "Structural Controllability and Observability in Influence Diagrams",
        "authors": [
            "Brian Y. Chan",
            "Ross D. Shachter"
        ],
        "abstract": "Influence diagram is a graphical representation of belief networks with uncertainty.  This article studies the structural properties of a probabilistic model in an influence diagram. In particular, structural controllability theorems and structural observability theorems are developed and algorithms are formulated.  Controllability and observability are fundamental concepts in dynamic systems (Luenberger 1979).  Controllability corresponds to the ability to control a system while observability analyzes the inferability of its variables.  Both properties can be determined by the ranks of the system matrices. Structural controllability and observability, on the other hand, analyze the property of a system with its structure only, without the specific knowledge of the values of its elements (tin 1974, Shields and Pearson 1976).  The structural analysis explores the connection between the structure of a model and the functional dependence among its elements.  It is useful in comprehending problem and formulating solution by challenging the underlying intuitions and detecting inconsistency in a model.  This type of qualitative reasoning can sometimes provide insight even when there is insufficient numerical information in a model.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5395",
        "title": "Lattice-Based Graded Logic: a Multimodal Approach",
        "authors": [
            "Philippe Chatalic",
            "Christine Froidevaux"
        ],
        "abstract": "Experts do not always feel very, comfortable when they have to give precise numerical estimations of certainty degrees.  In this paper we present a qualitative approach which allows for attaching partially ordered symbolic grades to logical formulas.  Uncertain information is expressed by means of parameterized modal operators.  We propose a semantics for this multimodal logic and give a sound and complete axiomatization.  We study the links with related approaches and suggest how this framework might be used to manage both uncertain and incomplere knowledge.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5396",
        "title": "Dynamic Network Models for Forecasting",
        "authors": [
            "Paul Dagum",
            "Adam Galper",
            "Eric J. Horvitz"
        ],
        "abstract": "We have developed a probabilistic forecasting methodology through a synthesis of belief network models and classical time-series analysis.  We present the dynamic network model (DNM) and describe methods for constructing, refining, and performing inference with this representation of temporal probabilistic knowledge.  The DNM representation extends static belief-network models to more general dynamic forecasting models by integrating and iteratively refining contemporaneous and time-lagged dependencies.  We discuss key concepts in terms of a model for forecasting U.S. car sales in Japan.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5397",
        "title": "Reformulating Inference Problems Through Selective Conditioning",
        "authors": [
            "Paul Dagum",
            "Eric J. Horvitz"
        ],
        "abstract": "We describe how we selectively reformulate portions of a belief network that pose difficulties for solution with a stochastic-simulation algorithm.  With employ the selective conditioning approach to target specific nodes in a belief network for decomposition, based on the contribution the nodes make to the tractability of stochastic simulation.  We review previous work on BNRAS algorithms- randomized approximation algorithms for probabilistic inference.  We show how selective conditioning can be employed to reformulate a single BNRAS problem into multiple tractable BNRAS simulation problems.  We discuss how we can use another simulation algorithm-logic sampling-to solve a component of the inference problem that provides a means for knitting the solutions of individual subproblems into a final result.  Finally, we analyze tradeoffs among the computational subtasks associated with the selective conditioning approach to reformulation.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5398",
        "title": "Entropy and Belief Networks",
        "authors": [
            "Norman C. Dalkey"
        ],
        "abstract": "The product expansion of conditional probabilities for belief nets is not maximum entropy.  This appears to deny a desirable kind of assurance for the model.  However, a kind of guarantee that is almost as strong as maximum entropy can be derived. Surprisingly, a variant model also exhibits the guarantee, and for many cases obtains a higher performance score than the product expansion.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5399",
        "title": "Parallelizing Probabilistic Inference: Some Early Explorations",
        "authors": [
            "Bruce D'Ambrosio",
            "Tony Fountain",
            "Zhaoyu Li"
        ],
        "abstract": "We report on an experimental investigation into opportunities for parallelism in beliefnet inference.  Specifically, we report on a study performed of the available parallelism, on hypercube style machines, of a set of randomly generated belief nets, using factoring (SPI) style inference algorithms. Our results indicate that substantial speedup is available, but that it is available only through parallelization of individual conformal product operations, and depends critically on finding an appropriate factoring.  We find negligible opportunity for parallelism at the topological, or clustering tree, level.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5400",
        "title": "Objection-Based Causal Networks",
        "authors": [
            "Adnan Darwiche"
        ],
        "abstract": "This paper introduces the notion of objection-based causal networks which resemble probabilistic causal networks except that they are quantified using objections.  An objection is a logical sentence and denotes a condition under which a, causal dependency does not exist.  Objection-based causal networks enjoy almost all the properties that make probabilistic causal networks popular, with the added advantage that objections are, arguably more intuitive than probabilities.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5401",
        "title": "A Symbolic Approach to Reasoning with Linguistic Quantifiers",
        "authors": [
            "Didier Dubois",
            "Henri Prade",
            "Lluis Godo",
            "Ramon Lopez de Mantaras"
        ],
        "abstract": "This paper investigates the possibility of performing automated reasoning in probabilistic logic when probabilities are expressed by means of linguistic quantifiers.  Each linguistic term is expressed as a prescribed interval of proportions.  Then instead of propagating numbers, qualitative terms are propagated in accordance with the numerical interpretation of these terms.  The quantified syllogism, modelling the chaining of probabilistic rules, is studied in this context.  It is shown that a qualitative counterpart of this syllogism makes sense, and is relatively independent of the threshold defining the linguistically meaningful intervals, provided that these threshold values remain in accordance with the intuition.  The inference power is less than that of a full-fledged probabilistic con-quaint propagation device but better corresponds to what could be thought of as commonsense probabilistic reasoning.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5402",
        "title": "Possibilistic Assumption based Truth Maintenance System, Validation in a Data Fusion Application",
        "authors": [
            "Francesco Fulvio Monai",
            "Thomas Chehire"
        ],
        "abstract": "Data fusion allows the elaboration and the evaluation of a situation synthesized from low level informations provided by different kinds of sensors.  The fusion of the collected data will result in fewer and higher level informations more easily assessed by a human operator and that will assist him effectively in his decision process.  In this paper we present the suitability and the advantages of using a Possibilistic Assumption based Truth Maintenance System (n-ATMS) in a data fusion military application.  We first describe the problem, the needed knowledge representation formalisms and problem solving paradigms.  Then we remind the reader of the basic concepts of ATMSs, Possibilistic Logic and 11-ATMSs.  Finally we detail the solution to the given data fusion problem and conclude with the results and comparison with a non-possibilistic solution.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5404",
        "title": "Knowledge Integration for Conditional Probability Assessments",
        "authors": [
            "Angelo Gilio",
            "Fulvio Spezzaferri"
        ],
        "abstract": "In the probabilistic approach to uncertainty management the input knowledge is usually represented by means of some probability distributions.  In this paper we assume that the input knowledge is given by two discrete conditional probability distributions, represented by two stochastic matrices P and Q.  The consistency of the knowledge base is analyzed.  Coherence conditions and explicit formulas for the extension to marginal distributions are obtained in some special cases.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5405",
        "title": "Integrating Model Construction and Evaluation",
        "authors": [
            "Robert P. Goldman",
            "John S. Breese"
        ],
        "abstract": "To date, most probabilistic reasoning systems have relied on a fixed belief network constructed at design time.  The network is used by an application program as a representation of (in)dependencies in the domain.  Probabilistic inference algorithms operate over the network to answer queries.  Recognizing the inflexibility of fixed models has led researchers to develop automated network construction procedures that use an expressive knowledge base to generate a network that can answer a query.  Although more flexible than fixed model approaches, these construction procedures separate construction and evaluation into distinct phases.  In this paper we develop an approach to combining incremental construction and evaluation of a partial probability model.  The combined method holds promise for improved methods for control of model construction based on a trade-off between fidelity of results and cost of construction.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5406",
        "title": "Reasoning With Qualitative Probabilities Can Be Tractable",
        "authors": [
            "Moises Goldszmidt",
            "Judea Pearl"
        ],
        "abstract": "We recently described a formalism for reasoning with if-then rules that re expressed with different levels of firmness [18].  The formalism interprets these rules as extreme conditional probability statements, specifying orders of magnitude of disbelief, which impose constraints over possible rankings of worlds.  It was shown that, once we compute a priority function Z+ on the rules, the degree to which a given query is confirmed or denied can be computed in O(log n`) propositional satisfiability tests, where n is the number of rules in the knowledge base.  In this paper, we show that computing Z+ requires O(n2 X log n) satisfiability tests, not an exponential number as was conjectured in [18], which reduces to polynomial complexity in the case of Horn expressions.  We also show how reasoning with imprecise observations can be incorporated in our formalism and how the popular notions of belief revision and epistemic entrenchment are embodied naturally and tractably.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5407",
        "title": "A computational scheme for Reasoning in Dynamic Probabilistic Networks",
        "authors": [
            "Uffe Kj\u00e6rulff"
        ],
        "abstract": "A computational scheme for reasoning about dynamic systems using (causal) probabilistic networks is presented.  The scheme is based on the framework of Lauritzen and Spiegelhalter (1988), and may be viewed as a generalization of the inference methods of classical time-series analysis in the sense that it allows description of non-linear, multivariate dynamic systems with complex conditional independence structures.  Further, the scheme provides a method for efficient backward smoothing and possibilities for efficient, approximate forecasting methods.  The scheme has been implemented on top of the HUGIN shell.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5408",
        "title": "The Dynamic of Belief in the Transferable Belief Model and Specialization-Generalization Matrices",
        "authors": [
            "Frank Klawonn",
            "Philippe Smets"
        ],
        "abstract": "The fundamental updating process in the transferable belief model is related to the concept of specialization and can be described by a specialization matrix.  The degree of belief in the truth of a proposition is a degree of justified support.  The Principle of Minimal Commitment implies that one should never give more support to the truth of a proposition than justified.  We show that Dempster's rule of conditioning corresponds essentially to the least committed specialization, and that Dempster's rule of combination results essentially from commutativity requirements.  The concept of generalization, dual to thc concept of specialization, is described.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5409",
        "title": "A Note on the Measure of Discord",
        "authors": [
            "George J. Klir",
            "Behzad Parviz"
        ],
        "abstract": "A new entropy-like measure as well as a new measure of total uncertainty pertaining to the Dempster-Shafer theory are introduced.  It is argued that these measures are better justified than any of the previously proposed candidates.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5410",
        "title": "Semantics for Probabilistic Inference",
        "authors": [
            "Henry E. Kyburg Jr"
        ],
        "abstract": "A number of writers(Joseph Halpern and Fahiem Bacchus among them) have offered semantics for formal languages in which inferences concerning probabilities can be made.  Our concern is different. This paper provides a formalization of nonmonotonic inferences in which the conclusion is supported only to a certain degree.  Such inferences are clearly 'invalid' since they must allow the falsity of a conclusion even when the premises are true.  Nevertheless, such inferences can be characterized both syntactically and semantically.  The 'premises' of probabilistic arguments are sets of statements (as in a database or knowledge base), the conclusions categorical statements in the language. We provide standards for both this form of inference, for which high probability is required, and for an inference in which the conclusion is qualified by an intermediate interval of support.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5411",
        "title": "Some Problems for Convex Bayesians",
        "authors": [
            "Henry E. Kyburg Jr.",
            "Michael Pittarelli"
        ],
        "abstract": "We discuss problems for convex Bayesian decision making and uncertainty representation.  These include the inability to accommodate various natural and useful constraints and the possibility of an analog of the classical Dutch Book being made against an agent behaving in accordance with convex Bayesian prescriptions.  A more general set-based Bayesianism may be as tractable and would avoid the difficulties we raise.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5412",
        "title": "Bayesian Meta-Reasoning: Determining Model Adequacy from Within a Small World",
        "authors": [
            "Kathryn Blackmond Laskey"
        ],
        "abstract": "This paper presents a Bayesian framework for assessing the adequacy of a model without the necessity of explicitly enumerating a specific alternate model.  A test statistic is developed for tracking the performance of the model across repeated problem instances. Asymptotic methods are used to derive an approximate distribution for the test statistic. When the model is rejected, the individual components of the test statistic can be used to guide search for an alternate model.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5413",
        "title": "The Bounded Bayesian",
        "authors": [
            "Kathryn Blackmond Laskey"
        ],
        "abstract": "The ideal Bayesian agent reasons from a global probability model, but real agents are restricted to simplified models which they know to be adequate only in restricted circumstances.  Very little formal theory has been developed to help fallibly rational agents manage the process of constructing and revising small world models.  The goal of this paper is to present a theoretical framework for analyzing model management approaches.  For a probability forecasting problem, a search process over small world models is analyzed as an approximation to a larger-world model which the agent cannot explicitly enumerate or compute.  Conditions are given under which the sequence of small-world models converges to the larger-world probabilities.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5414",
        "title": "Representing Context-Sensitive Knowledge in a Network Formalism: A Preliminary Report",
        "authors": [
            "Tze-Yun Leong"
        ],
        "abstract": "Automated decision making is often complicated by the complexity of the knowledge involved.  Much of this complexity arises from the context sensitive variations of the underlying phenomena.  We propose a framework for representing descriptive, context-sensitive knowledge.  Our approach attempts to integrate categorical and uncertain knowledge in a network formalism.  This paper outlines the basic representation constructs, examines their expressiveness and efficiency, and discusses the potential applications of the framework.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5415",
        "title": "A Probabilistic Network of Predicates",
        "authors": [
            "Dekang Lin"
        ],
        "abstract": "Bayesian networks are directed acyclic graphs representing independence relationships among a set of random variables.  A random variable can be regarded as a set of exhaustive and mutually exclusive propositions.  We argue that there are several drawbacks resulting from the propositional nature and acyclic structure of Bayesian networks.  To remedy these shortcomings, we propose a probabilistic network where nodes represent unary predicates and which may contain directed cycles.  The proposed representation allows us to represent domain knowledge in a single static network even though we cannot determine the instantiations of the predicates before hand.  The ability to deal with cycles also enables us to handle cyclic causal tendencies and to recognize recursive plans.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5416",
        "title": "Representing Heuristic Knowledge in D-S Theory",
        "authors": [
            "Weiru Liu",
            "John G. Hughes",
            "Michael F. McTear"
        ],
        "abstract": "The Dempster-Shafer theory of evidence has been used intensively to deal with uncertainty in knowledge-based systems.  However the representation of uncertain relationships between evidence and hypothesis groups (heuristic knowledge) is still a major research problem.  This paper presents an approach to representing such heuristic knowledge by evidential mappings which are defined on the basis of mass functions.  The relationships between evidential mappings and multi valued mappings, as well as between evidential mappings and Bayesian multi- valued causal link models in Bayesian theory are discussed. Following this the detailed procedures for constructing evidential mappings for any set of heuristic rules are introduced.  Several situations of belief propagation are discussed.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5417",
        "title": "The Topological Fusion of Bayes Nets",
        "authors": [
            "Izhar Matzkevich",
            "Bruce Abramson"
        ],
        "abstract": "Bayes nets are relatively recent innovations.  As a result, most of their theoretical development has focused on the simplest class of single-author models.  The introduction of more sophisticated multiple-author settings raises a variety of interesting questions. One such question involves the nature of compromise and consensus.  Posterior compromises let each model process all data to arrive at an independent response, and then split the difference.  Prior compromises, on the other hand, force compromise to be reached on all points before data is observed.  This paper introduces prior compromises in a Bayes net setting.  It outlines the problem and develops an efficient algorithm for fusing two directed acyclic graphs into a single, consensus structure, which may then be used as the basis of a prior compromise.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5418",
        "title": "Calculating Uncertainty Intervals From Conditional Convex Sets of Probabilities",
        "authors": [
            "Serafin Moral"
        ],
        "abstract": "In Moral, Campos (1991) and Cano, Moral, Verdegay-Lopez (1991) a new method of conditioning convex sets of probabilities has been proposed.  The result of it is a convex set of non-necessarily normalized probability distributions.  The normalizing factor of each probability distribution is interpreted as the possibility assigned to it by the conditioning information.  From this, it is deduced that the natural value for the conditional probability of an event is a possibility distribution.  The aim of this paper is to study methods of transforming this possibility distribution into a probability (or uncertainty) interval.  These methods will be based on the use of Sugeno and Choquet integrals.  Their behaviour will be compared in basis to some selected examples.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5419",
        "title": "Sensor Validation Using Dynamic Belief Networks",
        "authors": [
            "Ann Nicholson",
            "J. M. Brady"
        ],
        "abstract": "The trajectory of a robot is monitored in a restricted dynamic environment using light beam sensor data.  We have a Dynamic Belief Network (DBN), based on a discrete model of the domain, which provides discrete monitoring analogous to conventional quantitative filter techniques.  Sensor observations are added to the basic DBN in the form of specific evidence.  However, sensor data is often partially or totally incorrect.  We show how the basic DBN, which infers only an impossible combination of evidence, may be modified to handle specific types of incorrect data which may occur in the domain.  We then present an extension to the DBN, the addition of an invalidating node, which models the status of the sensor as working or defective.  This node provides a qualitative explanation of inconsistent data: it is caused by a defective sensor.  The connection of successive instances of the invalidating node models the status of a sensor over time, allowing the DBN to handle both persistent and intermittent faults.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5420",
        "title": "Empirical Probabilities in Monadic Deductive Databases",
        "authors": [
            "Raymond T. Ng",
            "V. S. Subrahmanian"
        ],
        "abstract": "We address the problem of supporting empirical probabilities in monadic logic databases. Though the semantics of multivalued logic programs has been studied extensively, the treatment of probabilities as results of statistical findings has not been studied in logic programming/deductive databases.  We develop a model-theoretic characterization of logic databases that facilitates such a treatment.  We present an algorithm for checking consistency of such databases and prove its total correctness.  We develop a sound and complete query processing procedure for handling queries to such databases.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5421",
        "title": "aHUGIN: A System Creating Adaptive Causal Probabilistic Networks",
        "authors": [
            "Kristian G. Olesen",
            "Steffen L. Lauritzen",
            "Finn Verner Jensen"
        ],
        "abstract": "The paper describes aHUGIN, a tool for creating adaptive systems.  aHUGIN is an extension of the HUGIN shell, and is based on the methods reported by Spiegelhalter and Lauritzen (1990a).  The adaptive systems resulting from aHUGIN are able to adjust the C011ditional probabilities in the model.  A short analysis of the adaptation task is given and the features of aHUGIN are described.  Finally a session with experiments is reported and the results are discussed.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5422",
        "title": "MESA: Maximum Entropy by Simulated Annealing",
        "authors": [
            "Gerhard Paa\u00df"
        ],
        "abstract": "Probabilistic reasoning systems combine different probabilistic rules and probabilistic facts to arrive at the desired probability values of consequences.  In this paper we describe the MESA-algorithm (Maximum Entropy by Simulated Annealing) that derives a joint distribution of variables or propositions.  It takes into account the reliability of probability values and can resolve conflicts between contradictory statements.  The joint distribution is represented in terms of marginal distributions and therefore allows to process large inference networks and to determine desired probability values with high precision.  The procedure derives a maximum entropy distribution subject to the given constraints.  It can be applied to inference networks of arbitrary topology and may be extended into a number of directions.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5423",
        "title": "Decision Methods for Adaptive Task-Sharing in Associate Systems",
        "authors": [
            "Thomas S. Paterson",
            "Michael R. Fehling"
        ],
        "abstract": "This paper describes some results of research on associate systems: knowledge-based systems that flexibly and adaptively support their human users in carrying out complex, time-dependent problem-solving tasks under uncertainty.  Based on principles derived from decision theory and decision analysis, a problem-solving approach is presented which can overcome many of the limitations of traditional expert-systems.  This approach implements an explicit model of the human user's problem-solving capabilities as an integral element in the overall problem solving architecture.  This integrated model, represented as an influence diagram, is the basis for achieving adaptive task sharing behavior between the associate system and the human user.  This associate system model has been applied toward ongoing research on a Mars Rover Manager's Associate (MRMA).  MRMA's role would be to manage a small fleet of robotic rovers on the Martian surface.  The paper describes results for a specific scenario where MRMA examines the benefits and costs of consulting human experts on Earth to assist a Mars rover with a complex resource management decision.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5424",
        "title": "Modeling Uncertain Temporal Evolutions in Model-Based Diagnosis",
        "authors": [
            "Luigi Portinale"
        ],
        "abstract": "Although the notion of diagnostic problem has been extensively investigated in the context of static systems, in most practical applications the behavior of the modeled system is significantly variable during time.  The goal of the paper is to propose a novel approach to the modeling of uncertainty about temporal evolutions of time-varying systems and a characterization of model-based temporal diagnosis.  Since in most real world cases knowledge about the temporal evolution of the system to be diagnosed is uncertain, we consider the case when probabilistic temporal knowledge is available for each component of the system and we choose to model it by means of Markov chains.  In fact, we aim at exploiting the statistical assumptions underlying reliability theory in the context of the diagnosis of timevarying systems.  We finally show how to exploit Markov chain theory in order to discard, in the diagnostic process, very unlikely diagnoses.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5425",
        "title": "Guess-And-Verify Heuristics for Reducing Uncertainties in Expert Classification Systems",
        "authors": [
            "Yuping Qiu",
            "Louis Anthony Cox Jr.",
            "Lawrence Davis"
        ],
        "abstract": "An expert classification system having statistical information about the prior probabilities of the different classes should be able to use this knowledge to reduce the amount of additional information that it must collect, e.g., through questions, in order to make a correct classification.  This paper examines how best to use such prior information and additional information-collection opportunities to reduce uncertainty about the class to which a case belongs, thus minimizing the average cost or effort required to correctly classify new cases.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5426",
        "title": "R&D Analyst: An Interactive Approach to Normative Decision System Model Construction",
        "authors": [
            "Peter J. Regan",
            "Samuel Holtzman"
        ],
        "abstract": "This paper describes the architecture of R&D Analyst, a commercial intelligent decision system for evaluating corporate research and development projects and portfolios.  In analyzing projects, R&D Analyst interactively guides a user in constructing an influence diagram model for an individual research project.  The system's interactive approach can be clearly explained from a blackboard system perspective.  The opportunistic reasoning emphasis of blackboard systems satisfies the flexibility requirements of model construction, thereby suggesting that a similar architecture would be valuable for developing normative decision systems in other domains.  Current research is aimed at extending the system architecture to explicitly consider of sequential decisions involving limited temporal, financial, and physical resources.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5427",
        "title": "Possibilistic Constraint Satisfaction Problems or \"How to handle soft constraints?\"",
        "authors": [
            "Thomas Schiex"
        ],
        "abstract": "Many AI synthesis problems such as planning or scheduling may be modelized as constraint satisfaction problems (CSP).  A CSP is typically defined as the problem of finding any consistent labeling for a fixed set of variables satisfying all given constraints between these variables.  However, for many real tasks such as job-shop scheduling, time-table scheduling, design?, all these constraints have not the same significance and have not to be necessarily satisfied.  A first distinction can be made between hard constraints, which every solution should satisfy and soft constraints, whose satisfaction has not to be certain.  In this paper, we formalize the notion of possibilistic constraint satisfaction problems that allows the modeling of uncertainly satisfied constraints.  We use a possibility distribution over labelings to represent respective possibilities of each labeling.  Necessity-valued constraints allow a simple expression of the respective certainty degrees of each constraint.  The main advantage of our approach is its integration in the CSP technical framework.  Most classical techniques, such as Backtracking (BT), arcconsistency enforcing (AC) or Forward Checking have been extended to handle possibilistics CSP and are effectively implemented.  The utility of our approach is demonstrated on a simple design problem.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5428",
        "title": "Decision Making Using Probabilistic Inference Methods",
        "authors": [
            "Ross D. Shachter",
            "Mark Alan Peot"
        ],
        "abstract": "The analysis of decision making under uncertainty is closely related to the analysis of probabilistic inference.  Indeed, much of the research into efficient methods for probabilistic inference in expert systems has been motivated by the fundamental normative arguments of decision theory.  In this paper we show how the developments underlying those efficient methods can be applied immediately to decision problems.  In addition to general approaches which need know nothing about the actual probabilistic inference method, we suggest some simple modifications to the clustering family of algorithms in order to efficiently incorporate decision making capabilities.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5429",
        "title": "Conditional Independence in Uncertainty Theories",
        "authors": [
            "Prakash P. Shenoy"
        ],
        "abstract": "This paper introduces the notions of independence and conditional independence in valuation-based systems (VBS).  VBS is an axiomatic framework capable of representing many different uncertainty calculi.  We define independence and conditional independence in terms of factorization of the joint valuation.  The definitions of independence and conditional independence in VBS generalize the corresponding definitions in probability theory.  Our definitions apply not only to probability theory, but also to Dempster-Shafer's belief-function theory, Spohn's epistemic-belief theory, and Zadeh's possibility theory.  In fact, they apply to any uncertainty calculi that fit in the framework of valuation-based systems.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5430",
        "title": "The Nature of the Unnormalized Beliefs Encountered in the Transferable Belief Model",
        "authors": [
            "Philippe Smets"
        ],
        "abstract": "Within the transferable belief model, positive basic belief masses can be allocated to the empty set, leading to unnormalized belief functions.  The nature of these unnormalized beliefs is analyzed.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5431",
        "title": "Intuitions about Ordered Beliefs Leading to Probabilistic Models",
        "authors": [
            "Paul Snow"
        ],
        "abstract": "The general use of subjective probabilities to model belief has been justified using many axiomatic schemes.  For example, ?consistent betting behavior' arguments are well-known.  To those not already convinced of the unique fitness and generality of probability models, such justifications are often unconvincing.  The present paper explores another rationale for probability models.  ?Qualitative probability,' which is known to provide stringent constraints on belief representation schemes, is derived from five simple assumptions about relationships among beliefs.  While counterparts of familiar rationality concepts such as transitivity, dominance, and consistency are used, the betting context is avoided.  The gap between qualitative probability and probability proper can be bridged by any of several additional assumptions.  The discussion here relies on results common in the recent  AI literature, introducing a sixth simple assumption.  The narrative emphasizes models based on unique complete orderings, but the rationale extends easily to motivate set-valued representations of partial orderings as well.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5432",
        "title": "Expressing Relational and Temporal Knowledge in Visual Probabilistic Networks",
        "authors": [
            "Luis Enrique Sucar",
            "Duncan F. Gillies"
        ],
        "abstract": "Bayesian networks have been used extensively in diagnostic tasks such as medicine, where they represent the dependency relations between a set of symptoms and a set of diseases.  A criticism of this type of knowledge representation is that it is restricted to this kind of task, and that it cannot cope with the knowledge required in other artificial intelligence applications.  For example, in computer vision, we require the ability to model complex knowledge, including temporal and relational factors.  In this paper we extend Bayesian networks to model relational and temporal knowledge for high-level vision.  These extended networks have a simple structure which permits us to propagate probability efficiently.  We have applied them to the domain of endoscopy, illustrating how the general modelling principles can be used in specific cases.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5433",
        "title": "A Fuzzy Logic Approach to Target Tracking",
        "authors": [
            "Chin-Wang Tao",
            "Wiley E. Thompson"
        ],
        "abstract": "This paper discusses a target tracking problem in which no dynamic mathematical model is explicitly assumed.  A nonlinear filter based on the fuzzy If-then rules is developed.  A comparison with a Kalman filter is made, and empirical results show that the performance of the fuzzy filter is better.  Intensive simulations suggest that theoretical justification of the empirical results is possible.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5434",
        "title": "Towards Precision of Probabilistic Bounds Propagation",
        "authors": [
            "Helmut Thone",
            "Ulrich Guntzer",
            "Werner Kiessling"
        ],
        "abstract": "The DUCK-calculus presented here is a recent approach to cope with probabilistic uncertainty in a sound and efficient way.  Uncertain rules with bounds for probabilities and explicit conditional independences can be maintained incrementally.  The basic inference mechanism relies on local bounds propagation, implementable by deductive databases with a bottom-up fixpoint evaluation.  In situations, where no precise bounds are deducible, it can be combined with simple operations research techniques on a local scope.  In particular, we provide new precise analytical bounds for probabilistic entailment.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5435",
        "title": "An Algorithm for Deciding if a Set of Observed Independencies Has a Causal Explanation",
        "authors": [
            "Tom S. Verma",
            "Judea Pearl"
        ],
        "abstract": "In a previous paper [Pearl and Verma, 1991] we presented an algorithm for extracting causal influences from independence information, where a causal influence was defined as the existence of a directed arc in all minimal causal models consistent with the data.  In this paper we address the question of deciding whether there exists a causal model that explains ALL the observed dependencies and independencies.  Formally, given a list M of conditional independence statements, it is required to decide whether there exists a directed acyclic graph (dag) D that is perfectly consistent with M, namely, every statement in M, and no other, is reflected via dseparation in D.  We present and analyze an effective algorithm that tests for the existence of such a day, and produces one, if it exists.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5436",
        "title": "Generalizing Jeffrey Conditionalization",
        "authors": [
            "Carl G. Wagner"
        ],
        "abstract": "Jeffrey's rule has been generalized by Wagner to the case in which new evidence bounds the possible revisions of a prior probability below by a Dempsterian lower probability. Classical probability kinematics arises within this generalization as the special case in which the evidentiary focal elements of the bounding lower probability are pairwise disjoint.  We discuss a twofold extension of this generalization, first allowing the lower bound to be any two-monotone capacity and then allowing the prior to be a lower envelope.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5437",
        "title": "Interval Structure: A Framework for Representing Uncertain Information",
        "authors": [
            "Michael S. K. M. Wong",
            "L. S. Wang",
            "Y. Y. Yao"
        ],
        "abstract": "In this paper, a unified framework for representing uncertain information based on the notion of an interval structure is proposed.  It is shown that the lower and upper approximations of the rough-set model, the lower and upper bounds of incidence calculus, and the belief and plausibility functions all obey the axioms of an interval structure.  An interval structure can be used to synthesize the decision rules provided by the experts.  An efficient algorithm to find the desirable set of rules is developed from a set of sound and complete inference axioms.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5438",
        "title": "Exploring Localization in Bayesian Networks for Large Expert Systems",
        "authors": [
            "Yang Xiang",
            "David L. Poole",
            "Michael P. Beddoes"
        ],
        "abstract": "Current Bayesian net representations do not consider structure in the domain and include all variables in a homogeneous network.  At any time, a human reasoner in a large domain may direct his attention to only one of a number of natural subdomains, i.e., there is ?localization' of queries and evidence.  In such a case, propagating evidence through a homogeneous network is inefficient since the entire network has to be updated each time. This paper presents multiply sectioned Bayesian networks that enable a (localization preserving) representation of natural subdomains by separate Bayesian subnets.  The subnets are transformed into a set of permanent junction trees such that evidential reasoning takes place at only one of them at a time.  Probabilities obtained are identical to those that would be obtained from the homogeneous network.  We discuss attention shift to a different junction tree and propagation of previously acquired evidence.  Although the overall system can be large, computational requirements are governed by the size of only one junction tree.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5439",
        "title": "A Decision Calculus for Belief Functions in Valuation-Based Systems",
        "authors": [
            "Hong Xu"
        ],
        "abstract": "Valuation-based system (VBS) provides a general framework for representing knowledge and drawing inferences under uncertainty.  Recent studies have shown that the semantics of VBS can represent and solve Bayesian decision problems (Shenoy, 1991a).  The purpose of this paper is to propose a decision calculus for Dempster-Shafer (D-S) theory in the framework of VBS.  The proposed calculus uses a weighting factor whose role is similar to the probabilistic interpretation of an assumption that disambiguates decision problems represented with belief functions (Strat 1990).  It will be shown that with the presented calculus, if the decision problems are represented in the valuation network properly, we can solve the problems by using fusion algorithm (Shenoy 1991a).  It will also be shown the presented decision calculus can be reduced to the calculus for Bayesian probability theory when probabilities, instead of belief functions, are given.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5440",
        "title": "Sidestepping the Triangulation Problem in Bayesian Net Computations",
        "authors": [
            "Nevin Lianwen Zhang",
            "David L. Poole"
        ],
        "abstract": "This paper presents a new approach for computing posterior probabilities in Bayesian nets, which sidesteps the triangulation problem.  The current state of art is the clique tree propagation approach.  When the underlying graph of a Bayesian net is triangulated, this approach arranges its cliques into a tree and computes posterior probabilities by appropriately passing around messages in that tree.  The computation in each clique is simply direct marginalization.  When the underlying graph is not triangulated, one has to first triangulated it by adding edges.  Referred to as the triangulation problem, the problem of finding an optimal or even a ?good? triangulation proves to be difficult.  In this paper, we propose to first decompose a Bayesian net into smaller components by making use of Tarjan's algorithm for decomposing an undirected graph at all its minimal complete separators.  Then, the components are arranged into a tree and posterior probabilities are computed by appropriately passing around messages in that tree.  The computation in each component is carried out by repeating the whole procedure from the beginning.  Thus the triangulation problem is sidestepped.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5659",
        "title": "Viterbi training in PRISM",
        "authors": [
            "Taisuke Sato",
            "Keiichi Kubota"
        ],
        "abstract": "VT (Viterbi training), or hard EM, is an efficient way of parameter learning for probabilistic models with hidden variables. Given an observation $y$, it searches for a state of hidden variables $x$ that maximizes $p(x,y \\mid \\theta)$ by coordinate ascent on parameters $\\theta$ and $x$. In this paper we introduce VT to PRISM, a logic-based probabilistic modeling system for generative models. VT improves PRISM in three ways. First VT in PRISM converges faster than EM in PRISM due to the VT's termination condition. Second, parameters learned by VT often show good prediction performance compared to those learned by EM. We conducted two parsing experiments with probabilistic grammars while learning parameters by a variety of inference methods, i.e.\\ VT, EM, MAP and VB. The result is that VT achieved the best parsing accuracy among them in both experiments. Also we conducted a similar experiment for classification tasks where a hidden variable is not a prediction target unlike probabilistic grammars. We found that in such a case VT does not necessarily yield superior performance. Third since VT always deals with a single probability of a single explanation, Viterbi explanation, the exclusiveness condition that is imposed on PRISM programs is no more required if we learn parameters by VT.\n",
        "submission_date": "2013-03-22T00:00:00",
        "last_modified_date": "2013-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5703",
        "title": "ARCO1: An Application of Belief Networks to the Oil Market",
        "authors": [
            "Bruce Abramson"
        ],
        "abstract": "Belief networks are a new, potentially important, class of knowledge-based models. ARCO1, currently under development at the Atlantic Richfield Company (ARCO) and the University of Southern California (USC), is the most advanced reported implementation of these models in a financial forecasting setting.  ARCO1's underlying belief network models the variables believed to have an impact on the crude oil market.  A pictorial market model-developed on a MAC II- facilitates consensus among the members of the forecasting team.  The system forecasts crude oil prices via Monte Carlo analyses of the network. Several different models of the oil market have been developed; the system's ability to be updated quickly highlights its flexibility.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5704",
        "title": "\"Conditional Inter-Causally Independent\" Node Distributions, a Property of \"Noisy-Or\" Models",
        "authors": [
            "John Mark Agosta"
        ],
        "abstract": "This paper examines the interdependence generated between two parent nodes with a common instantiated child node, such as two hypotheses sharing common evidence. The relation so generated has been termed \"intercausal.\" It is shown by construction that inter-causal independence is possible for binary distributions at one state of evidence. For such \"CICI\" distributions, the two measures of inter-causal effect, \"multiplicative synergy\" and \"additive synergy\" are equal. The well known \"noisy-or\" model is an example of such a distribution. This introduces novel semantics for the noisy-or, as a model of the degree of conflict among competing hypotheses of a common observation.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5705",
        "title": "Combining Multiple-Valued Logics in Modular Expert Systems",
        "authors": [
            "Jaume Agust\u00ed-Cullell",
            "Francesc Esteva",
            "Pere Garcia",
            "Lluis Godo",
            "Carles Sierra"
        ],
        "abstract": "The way experts manage uncertainty usually changes depending on the task they are performing.  This fact has lead us to consider the problem of communicating modules (task implementations) in a large and structured knowledge based system when modules have different uncertainty calculi.  In this paper, the analysis of the communication problem is made assuming that (i) each uncertainty calculus is an inference mechanism defining an entailment relation, and therefore the communication is considered to be inference-preserving, and (ii) we restrict ourselves to the case which the different uncertainty calculi are given by a class of truth functional Multiple-valued Logics.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5706",
        "title": "Constraint Propagation with Imprecise Conditional Probabilities",
        "authors": [
            "Stephane Amarger",
            "Didier Dubois",
            "Henri Prade"
        ],
        "abstract": "An approach to reasoning with default rules where the proportion of exceptions, or more generally the probability of encountering an exception, can be at least roughly assessed is presented.  It is based on local uncertainty propagation rules which provide the best bracketing of a conditional probability of interest from the knowledge of the bracketing of some other conditional probabilities.  A procedure that uses two such propagation rules repeatedly is proposed in order to estimate any simple conditional probability of interest from the available knowledge.  The iterative procedure, that does not require independence assumptions, looks promising with respect to the linear programming method.  Improved bounds for conditional probabilities are given when independence assumptions hold.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5707",
        "title": "Bayesian Networks Aplied to Therapy Monitoring",
        "authors": [
            "Carlo Berzuini",
            "David J. Spiegelhalter",
            "Riccardo Bellazzi"
        ],
        "abstract": "We propose a general Bayesian network model for application in a wide class of problems of therapy monitoring.  We discuss the use of stochastic simulation as a computational approach to inference on the proposed class of models.  As an illustration we present an application to the monitoring of cytotoxic chemotherapy in breast cancer.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5708",
        "title": "Some Properties of Plausible Reasoning",
        "authors": [
            "Wray L. Buntine"
        ],
        "abstract": "This paper presents a plausible reasoning system to illustrate some broad issues in knowledge representation: dualities between different reasoning forms, the difficulty of unifying complementary reasoning styles, and the approximate nature of plausible reasoning.  These issues have a common underlying theme: there should be an underlying belief calculus of which the many different reasoning forms are special cases, sometimes approximate.  The system presented allows reasoning about defaults, likelihood, necessity and possibility in a manner similar to the earlier work of Adams.  The system is based on the belief calculus of subjective Bayesian probability which itself is based on a few simple assumptions about how belief should be manipulated.  Approximations, semantics, consistency and consequence results are presented for the system.  While this puts these often discussed plausible reasoning forms on a probabilistic footing, useful application to practical problems remains an issue.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5709",
        "title": "Theory Refinement on Bayesian Networks",
        "authors": [
            "Wray L. Buntine"
        ],
        "abstract": "Theory refinement is the task of updating a domain theory in the light of new cases, to be done automatically or with some expert assistance.  The problem of theory refinement under uncertainty is reviewed here in the context of Bayesian statistics, a theory of belief revision.  The problem is reduced to an incremental learning task as follows: the learning system is initially primed with a partial theory supplied by a domain expert, and thereafter maintains its own internal representation of alternative theories which is able to be interrogated by the domain expert and able to be incrementally refined from data. Algorithms for refinement of Bayesian networks are presented to illustrate what is meant by \"partial theory\", \"alternative theory representation\", etc.  The algorithms are an incremental variant of batch learning algorithms from the literature so can work well in batch and incremental mode.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5710",
        "title": "Combination of Upper and Lower Probabilities",
        "authors": [
            "Jose E. Cano",
            "Serafin Moral",
            "Juan F. Verdegay-Lopez"
        ],
        "abstract": "In this paper, we consider several types of information and methods of combination associated with incomplete probabilistic systems.  We discriminate between 'a priori' and evidential information.  The former one is a description of the whole population, the latest is a restriction based on observations for a particular case.  Then, we propose different combination methods for each one of them.  We also consider conditioning as the heterogeneous combination of 'a priori' and evidential information.  The evidential information is represented as a convex set of likelihood functions.  These will have an associated possibility distribution with behavior according to classical Possibility Theory.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5711",
        "title": "A Probabilistic Analysis of Marker-Passing Techniques for Plan-Recognition",
        "authors": [
            "Glenn Carroll",
            "Eugene Charniak"
        ],
        "abstract": "Useless paths are a chronic problem for marker-passing techniques.  We use a probabilistic analysis to justify a method for quickly identifying and rejecting useless paths.  Using the same analysis, we identify key conditions and assumptions necessary for marker-passing to perform well.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5712",
        "title": "Symbolic Probabilistic Inference with Continuous Variables",
        "authors": [
            "Kuo-Chu Chang",
            "Robert Fung"
        ],
        "abstract": "Research on Symbolic Probabilistic Inference (SPI) [2, 3] has provided an algorithm for resolving general queries in Bayesian networks.  SPI applies the concept of dependency directed backward search to probabilistic inference, and is incremental with respect to both queries and observations.  Unlike traditional Bayesian network inferencing algorithms, SPI algorithm is goal directed, performing only those calculations that are required to respond to queries.  Research to date on SPI applies to Bayesian networks with discrete-valued variables and does not address variables with continuous values.  In this papers, we extend the SPI algorithm to handle Bayesian networks made up of continuous variables where the relationships between the variables are restricted to be ?linear gaussian?.  We call this variation of the SPI algorithm, SPI Continuous (SPIC). SPIC modifies the three basic SPI operations: multiplication, summation, and substitution.  However, SPIC retains the framework of the SPI algorithm, namely building the search tree and recursive query mechanism and therefore retains the goal-directed and incrementality features of SPI.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5713",
        "title": "Symbolic Probabilistic Inference with Evidence Potential",
        "authors": [
            "Kuo-Chu Chang",
            "Robert Fung"
        ],
        "abstract": "Recent research on the Symbolic Probabilistic Inference (SPI) algorithm[2] has focused attention on the importance of resolving general queries in Bayesian networks.  SPI applies the concept of dependency-directed backward search to probabilistic inference, and is incremental with respect to both queries and observations.  In response to this research we have extended the evidence potential algorithm [3] with the same features. We call the extension symbolic evidence potential inference (SEPI).  SEPI like SPI can handle generic queries and is incremental with respect to queries and observations.  While in SPI, operations are done on a search tree constructed from the nodes of the original network, in SEPI, a clique-tree structure obtained from the evidence potential algorithm [3] is the basic framework for recursive query processing.  In this paper, we describe the systematic query and caching procedure of SEPI. SEPI begins with finding a clique tree from a Bayesian network-the standard procedure of the evidence potential algorithm.  With the clique tree, various probability distributions are computed and stored in each clique.  This is the ?pre-processing? step of SEPI.  Once this step is done, the query can then be computed.  To process a query, a recursive process similar to the SPI algorithm is used.  The queries are directed to the root clique and decomposed into queries for the clique's subtrees until a particular query can be answered at the clique at which it is directed.  The algorithm and the computation are simple.  The SEPI algorithm will be presented in this paper along with several examples.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5714",
        "title": "A Bayesian Method for Constructing Bayesian Belief Networks from Databases",
        "authors": [
            "Gregory F. Cooper",
            "Edward H. Herskovits"
        ],
        "abstract": "This paper presents a Bayesian method for constructing Bayesian belief networks from a database of cases.  Potential applications include computer-assisted hypothesis testing, automated scientific discovery, and automated construction of probabilistic expert systems.  Results are presented of a preliminary evaluation of an algorithm for constructing a belief network from a database of cases.  We relate the methods in this paper to previous work, and we discuss open problems.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5715",
        "title": "Local Expression Languages for Probabilistic Dependence: a Preliminary Report",
        "authors": [
            "Bruce D'Ambrosio"
        ],
        "abstract": "We present a generalization of the local expression language used in the Symbolic Probabilistic Inference (SPI) approach to inference in belief nets [1l, [8].  The local expression language in SPI is the language in which the dependence of a node on its antecedents is described.  The original language represented the dependence as a single monolithic conditional probability distribution.  The extended language provides a set of operators (*, +, and -) which can be used to specify methods for combining partial conditional distributions.  As one instance of the utility of this extension, we show how this extended language can be used to capture the semantics, representational advantages, and inferential complexity advantages of the \"noisy or\" relationship.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5716",
        "title": "Symbolic Decision Theory and Autonomous Systems",
        "authors": [
            "John Fox",
            "Paul J. Krause"
        ],
        "abstract": "The ability to reason under uncertainty and with incomplete information is a fundamental requirement of decision support technology.  In this paper we argue that the concentration on theoretical techniques for the evaluation and selection of decision options has distracted attention from many of the wider issues in decision making.  Although numerical methods of reasoning under uncertainty have strong theoretical foundations, they are representationally weak and only deal with a small part of the decision process. Knowledge based systems, on the other hand, offer greater flexibility but have not been accompanied by a clear decision theory.  We describe here work which is under way towards providing a theoretical framework for symbolic decision procedures.  A central proposal is an extended form of inference which we call argumentation; reasoning for and against decision options from generalised domain theories.  The approach has been successfully used in several decision support applications, but it is argued that a comprehensive decision theory must cover autonomous decision making, where the agent can formulate questions as well as take decisions.  A major theoretical challenge for this theory is to capture the idea of reflection to permit decision agents to reason about their goals, what they believe and why, and what they need to know or do in order to achieve their goals.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5717",
        "title": "A Reason Maintenace System Dealing with Vague Data",
        "authors": [
            "B. Fringuelli",
            "S. Marcugini",
            "A. Milani",
            "S. Rivoira"
        ],
        "abstract": "A reason maintenance system which extends an ATMS through Mukaidono's fuzzy logic is described.  It supports a problem solver in situations affected by incomplete information and vague data, by allowing nonmonotonic inferences and the revision of previous conclusions when contradictions are detected.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5718",
        "title": "Advances in Probabilistic Reasoning",
        "authors": [
            "Dan Geiger",
            "David Heckerman"
        ],
        "abstract": "This paper discuses multiple Bayesian networks representation paradigms for encoding asymmetric independence assertions. We offer three contributions: (1) an inference mechanism that makes explicit use of asymmetric independence to speed up computations, (2) a simplified definition of similarity networks and extensions of their theory, and (3) a generalized representation scheme that encodes more types of asymmetric independence assertions than do similarity networks.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5719",
        "title": "Probability Estimation in Face of Irrelevant Information",
        "authors": [
            "Adam J. Grove",
            "Daphne Koller"
        ],
        "abstract": "In this paper, we consider one aspect of the problem of applying decision theory to the design of agents that learn how to make decisions under uncertainty.  This aspect concerns how an agent can estimate probabilities for the possible states of the world, given that it only makes limited observations before committing to a decision.  We show that the naive application of statistical tools can be improved upon if the agent can determine which of his observations are truly relevant to the estimation problem at hand. We give a framework in which such determinations can be made, and define an estimation procedure to use them.  Our framework also suggests several extensions, which show how additional knowledge can be used to improve tile estimation procedure still further.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5720",
        "title": "An Approximate Nonmyopic Computation for Value of Information",
        "authors": [
            "David Heckerman",
            "Eric J. Horvitz",
            "Blackford Middleton"
        ],
        "abstract": "Value-of-information analyses provide a straightforward means for selecting the best next observation to make, and for determining whether it is better to gather additional information or to act immediately. Determining the next best test to perform, given a state of uncertainty about the world, requires a consideration of the value of making all possible sequences of observations. In practice, decision analysts and expert-system designers have avoided the intractability of exact computation of the value of information by relying on a myopic approximation. Myopic analyses are based on the assumption that only one additional test will be performed, even when there is an opportunity to make a large number of observations. We present a nonmyopic approximation for value of information that bypasses the traditional myopic analyses by exploiting the statistical properties of large samples.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5721",
        "title": "Search-based Methods to Bound Diagnostic Probabilities in Very Large Belief Nets",
        "authors": [
            "Max Henrion"
        ],
        "abstract": "Since exact probabilistic inference is intractable in general for large multiply connected belief nets, approximate methods are required.  A promising approach is to use heuristic search among hypotheses (instantiations of the network) to find the most probable ones, as in the TopN algorithm.  Search is based on the relative probabilities of hypotheses which are efficient to compute.  Given upper and lower bounds on the relative probability of partial hypotheses, it is possible to obtain bounds on the absolute probabilities of hypotheses.  Best-first search aimed at reducing the maximum error progressively narrows the bounds as more hypotheses are examined.  Here, qualitative probabilistic analysis is employed to obtain bounds on the relative probability of partial hypotheses for the BN20 class of networks networks and a generalization replacing the noisy OR assumption by negative synergy.  The approach is illustrated by application to a very large belief network, QMR-BN, which is a reformulation of the Internist-1 system for diagnosis in internal medicine.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5722",
        "title": "Time-Dependent Utility and Action Under Uncertainty",
        "authors": [
            "Eric J. Horvitz",
            "Geoffrey Rutledge"
        ],
        "abstract": "We discuss representing and reasoning with knowledge about the time-dependent utility of an agent's actions.  Time-dependent utility plays a crucial role in the interaction between computation and action under bounded resources.  We present a semantics for time-dependent utility and describe the use of time-dependent information in decision contexts.  We illustrate our discussion with examples of time-pressured reasoning in Protos, a system constructed to explore the ideal control of inference by reasoners with limit abilities.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5723",
        "title": "Non-monotonic Reasoning and the Reversibility of Belief Change",
        "authors": [
            "Daniel Hunter"
        ],
        "abstract": "Traditional approaches to non-monotonic reasoning fail to satisfy a number of plausible axioms for belief revision and suffer from conceptual difficulties as well.  Recent work on ranked preferential models (RPMs) promises to overcome some of these difficulties. Here we show that RPMs are not adequate to handle iterated belief change.  Specifically, we show that RPMs do not always allow for the reversibility of belief change.  This result indicates the need for numerical strengths of belief.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5724",
        "title": "Belief and Surprise - A Belief-Function Formulation",
        "authors": [
            "Yen-Teh Hsia"
        ],
        "abstract": "We motivate and describe a theory of belief in this paper.  This theory is developed with the following view of human belief in mind.  Consider the belief that an event E will occur (or has occurred or is occurring).  An agent either entertains this belief or does not entertain this belief (i.e., there is no \"grade\" in entertaining the belief).  If the agent chooses to exercise \"the will to believe\" and entertain this belief, he/she/it is entitled to a degree of confidence c (1 > c > 0) in doing so.  Adopting this view of human belief, we conjecture that whenever an agent entertains the belief that E will occur with c degree of confidence, the agent will be surprised (to the extent c) upon realizing that E did not occur.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5725",
        "title": "Evidential Reasoning in a Categorial Perspective: Conjunction and Disjunction of Belief Functions",
        "authors": [
            "Robert Kennes"
        ],
        "abstract": "The categorial approach to evidential reasoning can be seen as a combination of the probability kinematics approach of Richard Jeffrey (1965) and the maximum (cross-) entropy inference approach of E. T. Jaynes (1957).  As a consequence of that viewpoint, it is well known that category theory provides natural definitions for logical connectives. In particular, disjunction and conjunction are modelled by general categorial constructions known as products and coproducts.  In this paper, I focus mainly on Dempster-Shafer theory of belief functions for which I introduce a category I call Dempster?s category.  I prove the existence of and give explicit formulas for conjunction and disjunction in the subcategory of separable belief functions.  In Dempster?s category, the new defined conjunction can be seen as the most cautious conjunction of beliefs, and thus no assumption about distinctness (of the sources) of beliefs is needed as opposed to Dempster?s rule of combination, which calls for distinctness (of the sources) of beliefs.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5726",
        "title": "Reasoning with Mass Distributions",
        "authors": [
            "Rudolf Kruse",
            "Detlef Nauck",
            "Frank Klawonn"
        ],
        "abstract": "The concept of movable evidence masses that flow from supersets to subsets as specified by experts represents a suitable framework for reasoning under uncertainty.  The mass flow is controlled by specialization matrices.  New evidence is integrated into the frame of discernment by conditioning or revision (Dempster's rule of conditioning), for which special specialization matrices exist.  Even some aspects of non-monotonic reasoning can be represented by certain specialization matrices.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5727",
        "title": "A Logic of Graded Possibility and Certainty Coping with Partial Inconsistency",
        "authors": [
            "Jerome Lang",
            "Didier Dubois",
            "Henri Prade"
        ],
        "abstract": "A semantics is given to possibilistic logic, a logic that handles weighted classical logic formulae, and where weights are interpreted as lower bounds on degrees of certainty or possibility, in the sense of Zadeh's possibility theory.  The proposed semantics is based on fuzzy sets of interpretations.  It is tolerant to partial inconsistency.  Satisfiability is extended from interpretations to fuzzy sets of interpretations, each fuzzy set representing a possibility distribution describing what is known about the state of the world.  A possibilistic knowledge base is then viewed as a set of possibility distributions that satisfy it.  The refutation method of automated deduction in possibilistic logic, based on previously introduced generalized resolution principle is proved to be sound and complete with respect to the proposed semantics, including the case of partial inconsistency.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5728",
        "title": "Conflict and Surprise: Heuristics for Model Revision",
        "authors": [
            "Kathryn Blackmond Laskey"
        ],
        "abstract": "Any probabilistic model of a problem is based on assumptions which, if violated, invalidate the model.  Users of probability based decision aids need to be alerted when cases arise that are not covered by the aid's model.  Diagnosis of model failure is also necessary to control dynamic model construction and revision.  This paper presents a set of decision theoretically motivated heuristics for diagnosing situations in which a model is likely to provide an inadequate representation of the process being modeled.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5729",
        "title": "Reasoning under Uncertainty: Some Monte Carlo Results",
        "authors": [
            "Paul E. Lehner",
            "Azar Sadigh"
        ],
        "abstract": "A series of monte carlo studies were performed to compare the behavior of some alternative procedures for reasoning under uncertainty.  The behavior of several Bayesian, linear model and default reasoning procedures were examined in the context of increasing levels of calibration error.  The most interesting result is that Bayesian procedures tended to output more extreme posterior belief values (posterior beliefs near 0.0 or 1.0) than other techniques, but the linear models were relatively less likely to output strong support for an erroneous conclusion.  Also, accounting for the probabilistic dependencies between evidence items was important for both Bayesian and linear updating procedures.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5730",
        "title": "Representation Requirements for Supporting Decision Model Formulation",
        "authors": [
            "Tze-Yun Leong"
        ],
        "abstract": "This paper outlines a methodology for analyzing the representational support for knowledge-based decision-modeling in a broad domain.  A relevant set of inference patterns and knowledge types are identified.  By comparing the analysis results to existing representations, some insights are gained into a design approach for integrating categorical and uncertain knowledge in a context sensitive manner.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5731",
        "title": "A Language for Planning with Statistics",
        "authors": [
            "Nathaniel G. Martin",
            "James F. Allen"
        ],
        "abstract": "When a planner must decide whether it has enough evidence to make a decision based on probability, it faces the sample size problem.  Current planners using probabilities need not deal with this problem because they do not generate their probabilities from observations.  This paper presents an event based language in which the planner's probabilities are calculated from the binomial random variable generated by the observed ratio of one type of event to another.  Such probabilities are subject to error, so the planner must introspect about their validity.  Inferences about the probability of these events can be made using statistics.  Inferences about the validity of the approximations can be made using interval estimation.  Interval estimation allows the planner to avoid making choices that are only weakly supported by the planner's evidence.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5732",
        "title": "A Modification to Evidential Probability",
        "authors": [
            "B\u00fclent Murtezao\u011flu",
            "Henry E. Kyburg Jr"
        ],
        "abstract": "Selecting the right reference class and the right interval when faced with conflicting candidates and no possibility of establishing subset style dominance has been a problem for Kyburg's Evidential Probability system.  Various methods have been proposed by Loui and Kyburg to solve this problem in a way that is both intuitively appealing and justifiable within Kyburg's framework.  The scheme proposed in this paper leads to stronger statistical assertions without sacrificing too much of the intuitive appeal of Kyburg's latest proposal.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5733",
        "title": "Investigation of Variances in Belief Networks",
        "authors": [
            "Richard E. Neapolitan",
            "James Kenevan"
        ],
        "abstract": "The belief network is a well-known graphical structure for representing independences in a joint probability distribution.  The methods, which perform probabilistic inference in belief networks, often treat the conditional probabilities which are stored in the network as certain values.  However, if one takes either a subjectivistic or a limiting frequency approach to probability, one can never be certain of probability values.  An algorithm should not only be capable of reporting the probabilities of the alternatives of remaining nodes when other nodes are instantiated; it should also be capable of reporting the uncertainty in these probabilities relative to the uncertainty in the probabilities which are stored in the network.  In this paper a method for determining the variances in inferred probabilities is obtained under the assumption that a posterior distribution on the uncertainty variables can be approximated by the prior distribution.  It is shown that this assumption is plausible if their is a reasonable amount of confidence in the probabilities which are stored in the network.  Furthermore in this paper, a surprising upper bound for the prior variances in the probabilities of the alternatives of all nodes is obtained in the case where the probability distributions of the probabilities of the alternatives are beta distributions.  It is shown that the prior variance in the probability at an alternative of a node is bounded above by the largest variance in an element of the conditional probability distribution for that node.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5734",
        "title": "A Sensitivity Analysis of Pathfinder: A Follow-up Study",
        "authors": [
            "Keung-Chi Ng",
            "Bruce Abramson"
        ],
        "abstract": "At last year?s Uncertainty in AI Conference, we reported the results of a sensitivity analysis study of Pathfinder.  Our findings were quite unexpected-slight variations to Pathfinder?s parameters appeared to lead to substantial degradations in system performance.  A careful look at our first analysis, together with the valuable feedback provided by the participants of last year?s conference, led us to conduct a follow-up study.  Our follow-up differs from our initial study in two ways: (i) the probabilities 0.0 and 1.0 remained unchanged, and (ii) the variations to the probabilities that are close to both ends (0.0 or 1.0) were less than the ones close to the middle (0.5).  The results of the follow-up study look more reasonable-slight variations to Pathfinder?s parameters now have little effect on its performance.  Taken together, these two sets of results suggest a viable extension of a common decision analytic sensitivity analysis to the larger, more complex settings generally encountered in artificial intelligence.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5735",
        "title": "Non-monotonic Negation in Probabilistic Deductive Databases",
        "authors": [
            "Raymond T. Ng",
            "V. S. Subrahmanian"
        ],
        "abstract": "In this paper we study the uses and the semantics of non-monotonic negation in probabilistic deductive data bases.  Based on the stable semantics for classical logic programming, we introduce the notion of stable formula, functions.  We show that stable formula, functions are minimal fixpoints of operators associated with probabilistic deductive databases with negation.  Furthermore, since a. probabilistic deductive database may not necessarily have a stable formula function, we provide a stable class semantics for such databases.  Finally, we demonstrate that the proposed semantics can handle default reasoning naturally in the context of probabilistic deduction.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5736",
        "title": "Management of Uncertainty in the Multi-Level Monitoring and Diagnosis of the Time of Flight Scintillation Array",
        "authors": [
            "Robert K. Paasch",
            "Alice M. Agogino"
        ],
        "abstract": "We present a general architecture for the monitoring and diagnosis of large scale sensor-based systems with real time diagnostic constraints.  This architecture is multileveled, combining a single monitoring level based on statistical methods with two model based diagnostic levels.  At each level, sources of uncertainty are identified, and integrated methodologies for uncertainty management are developed.  The general architecture was applied to the monitoring and diagnosis of a specific nuclear physics detector at Lawrence Berkeley National Laboratory that contained approximately 5000 components and produced over 500 channels of output data.  The general architecture is scalable, and work is ongoing to apply it to detector systems one and two orders of magnitude more complex.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5737",
        "title": "Integrating Probabilistic Rules into Neural Networks: A Stochastic EM Learning Algorithm",
        "authors": [
            "Gerhard Paass"
        ],
        "abstract": "The EM-algorithm is a general procedure to get maximum likelihood estimates if part of the observations on the variables of a network are missing.  In this paper a stochastic version of the algorithm is adapted to probabilistic neural networks describing the associative dependency of variables.  These networks have a probability distribution, which is a special case of the distribution generated by probabilistic inference networks. Hence both types of networks can be combined allowing to integrate probabilistic rules as well as unspecified associations in a sound way.  The resulting network may have a number of interesting features including cycles of probabilistic rules, hidden 'unobservable' variables, and uncertain and contradictory evidence.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5738",
        "title": "Representing Bayesian Networks within Probabilistic Horn Abduction",
        "authors": [
            "David L. Poole"
        ],
        "abstract": "This paper presents a simple framework for Horn clause abduction, with probabilities associated with hypotheses.  It is shown how this representation can represent any probabilistic knowledge representable in a Bayesian belief network.  The main contributions are in finding a relationship between logical and probabilistic notions of evidential reasoning.  This can be used as a basis for a new way to implement Bayesian Networks that allows for approximations to the value of the posterior probabilities, and also points to a way that Bayesian networks can be extended beyond a propositional language.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5739",
        "title": "Dynamic Network Updating Techniques For Diagnostic Reasoning",
        "authors": [
            "Gregory M. Provan"
        ],
        "abstract": "A new probabilistic network construction system, DYNASTY, is proposed for diagnostic reasoning given variables whose probabilities change over time.  Diagnostic reasoning is formulated as a sequential stochastic process, and is modeled using influence diagrams. Given a set O of observations, DYNASTY creates an influence diagram in order to devise the best action given O. Sensitivity analyses are conducted to determine if the best network has been created, given the uncertainty in network parameters and topology. DYNASTY uses an equivalence class approach to provide decision thresholds for the sensitivity analysis.  This equivalence-class approach to diagnostic reasoning differentiates diagnoses only if the required actions are different.  A set of network-topology updating algorithms are proposed for dynamically updating the network when necessary.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5740",
        "title": "High Level Path Planning with Uncertainty",
        "authors": [
            "Runping Qi",
            "David L. Poole"
        ],
        "abstract": "For high level path planning, environments are usually modeled as distance graphs, and path planning problems are reduced to computing the shortest path in distance graphs. One major drawback of this modeling is the inability to model uncertainties, which are often encountered in practice.  In this paper, a new tool, called U-yraph, is proposed for environment modeling.  A U-graph is an extension of distance graphs with the ability to handle a kind of uncertainty.  By modeling an uncertain environment as a U-graph, and a navigation problem as a Markovian decision process, we can precisely define a new optimality criterion for navigation plans, and more importantly, we can come up with a general algorithm for computing optimal plans for navigation tasks.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5741",
        "title": "Formal Model of Uncertainty for Possibilistic Rules",
        "authors": [
            "Arthur Ramer"
        ],
        "abstract": "Given a universe of discourse X-a domain of possible outcomes-an experiment may consist of selecting one of its elements, subject to the operation of chance, or of observing the elements, subject to imprecision.  A priori uncertainty about the actual result of the experiment may be quantified, representing either the likelihood of the choice of :r_X or the degree to which any such X would be suitable as a description of the outcome.  The former case corresponds to a probability distribution, while the latter gives a possibility assignment on X.  The study of such assignments and their properties falls within the purview of possibility theory [DP88, Y80, Z783.  It, like probability theory, assigns values between 0 and 1 to express likelihoods of outcomes. Here, however, the similarity ends.  Possibility theory uses the maximum and minimum functions to combine uncertainties, whereas probability theory uses the plus and times operations.  This leads to very dissimilar theories in terms of analytical framework, even though they share several semantic concepts.  One of the shared concepts consists of expressing quantitatively the uncertainty associated with a given distribution.  In probability theory its value corresponds to the gain of information that would result from conducting an experiment and ascertaining an actual result.  This gain of information can equally well be viewed as a decrease in uncertainty about the outcome of an experiment. In this case the standard measure of information, and thus uncertainty, is Shannon entropy [AD75, G77].  It enjoys several advantages-it is characterized uniquely by a few, very natural properties, and it can be conveniently used in decision processes.  This application is based on the principle of maximum entropy; it has become a popular method of relating decisions to uncertainty.  This paper demonstrates that an equally integrated theory can be built on the foundation of possibility theory.  We first show how to define measures of in formation and uncertainty for possibility assignments.  Next we construct an information-based metric on the space of all possibility distributions defined on a given domain.  It allows us to capture the notion of proximity in information content among the distributions.  Lastly, we show that all the above constructions can be carried out for continuous distributions-possibility assignments on arbitrary measurable domains.  We consider this step very significant-finite domains of discourse are but approximations of the real-life infinite domains.  If possibility theory is to represent real world situations, it must handle continuous distributions both directly and through finite approximations.  In the last section we discuss a principle of maximum uncertainty for possibility distributions.  We show how such a principle could be formalized as an inference rule.  We also suggest it could be derived as a consequence of simple assumptions about combining information.  We would like to mention that possibility assignments can be viewed as fuzzy sets and that every fuzzy set gives rise to an assignment of possibilities.  This correspondence has far reaching consequences in logic and in control theory.  Our treatment here is independent of any special interpretation; in particular we speak of possibility distributions and possibility measures, defining them as measurable mappings into the interval [0, 1].  Our presentation is intended as a self-contained, albeit terse summary.  Topics discussed were selected with care, to demonstrate both the completeness and a certain elegance of the theory.  Proofs are not included; we only offer illustrative examples.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5742",
        "title": "Deliberation and its Role in the Formation of Intentions",
        "authors": [
            "Anand S. Rao",
            "Michael P. Georgeff"
        ],
        "abstract": "Deliberation plays an important role in the design of rational agents embedded in the real-world.  In particular, deliberation leads to the formation of intentions, i.e., plans of action that the agent is committed to achieving.  In this paper, we present a branching time possible-worlds model for representing and reasoning about, beliefs, goals, intentions, time, actions, probabilities, and payoffs.  We compare this possible-worlds approach with the more traditional decision tree representation and provide a transformation from decision trees to possible worlds.  Finally, we illustrate how an agent can perform deliberation using a decision-tree representation and then use a possible-worlds model to form and reason about his intentions.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5743",
        "title": "Handling Uncertainty during Plan Recognition in Task-Oriented Consultation Systems",
        "authors": [
            "Bhavani Raskutti",
            "Ingrid Zukerman"
        ],
        "abstract": "During interactions with human consultants, people are used to providing partial and/or inaccurate information, and still be understood and assisted.  We attempt to emulate this capability of human consultants; in computer consultation systems.  In this paper, we present a mechanism for handling uncertainty in plan recognition during task-oriented consultations.  The uncertainty arises while choosing an appropriate interpretation of a user?s statements among many possible interpretations.  Our mechanism handles this uncertainty by using probability theory to assess the probabilities of the interpretations, and complements this assessment by taking into account the information content of the interpretations.  The information content of an interpretation is a measure of how well defined an interpretation is in terms of the actions to be performed on the basis of the interpretation.  This measure is used to guide the inference process towards interpretations with a higher information content.  The information content for an interpretation depends on the specificity and the strength of the inferences in it, where the strength of an inference depends on the reliability of the information on which the inference is based.  Our mechanism has been developed for use in task-oriented consultation systems.  The domain that we have chosen for exploration is that of a travel agency.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5744",
        "title": "Truth as Utility: A Conceptual Synthesis",
        "authors": [
            "Enrique H. Ruspini"
        ],
        "abstract": "This paper introduces conceptual relations that synthesize utilitarian and logical concepts, extending the logics of preference of Rescher.  We define first, in the context of a possible worlds model, constraint-dependent measures that quantify the relative quality of alternative solutions of reasoning problems or the relative desirability of various policies in control, decision, and planning problems.  We show that these measures may be interpreted as truth values in a multi valued logic and propose mechanisms for the representation of complex constraints as combinations of simpler restrictions.  These extended logical operations permit also the combination and aggregation of goal-specific quality measures into global measures of utility.  We identify also relations that represent differential preferences between alternative solutions and relate them to the previously defined desirability measures.  Extending conventional modal logic formulations, we introduce structures for the representation of ignorance about the utility of alternative solutions.  Finally, we examine relations between these concepts and similarity based semantic models of fuzzy logic.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5745",
        "title": "Pulcinella: A General Tool for Propagating Uncertainty in Valuation Networks",
        "authors": [
            "Alessandro Saffiotti",
            "Elisabeth Umkehrer"
        ],
        "abstract": "We present PULCinella and its use in comparing uncertainty theories.  PULCinella is a general tool for Propagating Uncertainty based on the Local Computation technique of Shafer and Shenoy.  It may be specialized to different uncertainty theories: at the moment, Pulcinella can propagate probabilities, belief functions, Boolean values, and possibilities.  Moreover, Pulcinella allows the user to easily define his own specializations.  To illustrate Pulcinella, we analyze two examples by using each of the four theories above.  In the first one, we mainly focus on intrinsic differences between theories.  In the second one, we take a knowledge engineer viewpoint, and check the adequacy of each theory to a given problem.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5746",
        "title": "Structuring Bodies of Evidence",
        "authors": [
            "Sandra Sandri"
        ],
        "abstract": "In this article we present two ways of structuring bodies of evidence, which allow us to reduce the complexity of the operations usually performed in the framework of evidence theory.  The first structure just partitions the focal elements in a body of evidence by their cardinality.  With this structure we are able to reduce the complexity on the calculation of the belief functions Bel, Pl, and Q.  The other structure proposed here, the Hierarchical Trees, permits us to reduce the complexity of the calculation of Bel, Pl, and Q, as well as of the Dempster's rule of combination in relation to the brute-force algorithm.  Both these structures do not require the generation of all the subsets of the reference domain.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5747",
        "title": "On the Generation of Alternative Explanations with Implications for Belief Revision",
        "authors": [
            "Eugene Santos Jr"
        ],
        "abstract": "In general, the best explanation for a given observation makes no promises on how good it is with respect to other alternative explanations.  A major deficiency of message-passing schemes for belief revision in Bayesian networks is their inability to generate alternatives beyond the second best.  In this paper, we present a general approach based on linear constraint systems that naturally generates alternative explanations in an orderly and highly efficient manner.  This approach is then applied to cost-based abduction problems as well as belief revision in Bayesian net works.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5748",
        "title": "Completing Knowledge by Competing Hierarchies",
        "authors": [
            "Kerstin Schill",
            "Ernst Poppel",
            "Christoph Zetzsche"
        ],
        "abstract": "A control strategy for expert systems is presented which is based on Shafer's Belief theory and the combination rule of Dempster.  In contrast to well known strategies it is not sequentially and hypotheses-driven, but parallel and self organizing, determined by the concept of information gain.  The information gain, calculated as the maximal difference between the actual evidence distribution in the knowledge base and the potential evidence determines each consultation step.  Hierarchically structured knowledge is an important representation form and experts even use several hierarchies in parallel for constituting their knowledge.  Hence the control strategy is applied to a layered set of distinct hierarchies.  Depending on the actual data one of these hierarchies is chosen by the control strategy for the next step in the reasoning process.  Provided the actual data are well matched to the structure of one hierarchy, this hierarchy remains selected for a longer consultation time.  If no good match can be achieved, a switch from the actual hierarchy to a competing one will result, very similar to the phenomenon of restructuring in problem solving tasks.  Up to now the control strategy is restricted to multi hierarchical knowledge bases with disjunct hierarchies.  It is implemented in the expert system IBIG (inference by information gain), being presently applied to acquired speech disorders (aphasia).\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5749",
        "title": "A Graph-Based Inference Method for Conditional Independence",
        "authors": [
            "Ross D. Shachter"
        ],
        "abstract": "The graphoid axioms for conditional independence, originally described by Dawid [1979], are fundamental to probabilistic reasoning [Pearl, 19881.  Such axioms provide a mechanism for manipulating conditional independence assertions without resorting to their numerical definition.  This paper explores a representation for independence statements using multiple undirected graphs and some simple graphical transformations. The independence statements derivable in this system are equivalent to those obtainable by the graphoid axioms.  Therefore, this is a purely graphical proof technique for conditional independence.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5750",
        "title": "A Fusion Algorithm for Solving Bayesian Decision Problems",
        "authors": [
            "Prakash P. Shenoy"
        ],
        "abstract": "This paper proposes a new method for solving Bayesian decision problems.  The method consists of representing a Bayesian decision problem as a valuation-based system and applying a fusion algorithm for solving it.  The fusion algorithm is a hybrid of local computational methods for computation of marginals of joint probability distributions and the local computational methods for discrete optimization problems.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5751",
        "title": "Algorithms for Irrelevance-Based Partial MAPs",
        "authors": [
            "Solomon Eyal Shimony"
        ],
        "abstract": "Irrelevance-based partial MAPs are useful constructs for domain-independent explanation using belief networks.  We look at two definitions for such partial MAPs, and prove important properties that are useful in designing algorithms for computing them effectively.  We make use of these properties in modifying our standard MAP best-first algorithm, so as to handle irrelevance-based partial MAPs.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5752",
        "title": "About Updating",
        "authors": [
            "Philippe Smets"
        ],
        "abstract": "Survey of several forms of updating, with a practical illustrative example.  We study several updating (conditioning) schemes that emerge naturally from a common scenarion to provide some insights into their meaning.  Updating is a subtle operation and there is no single method, no single 'good' rule.  The choice of the appropriate rule must always be given due consideration.  Planchet (1989) presents a mathematical survey of many rules.  We focus on the practical meaning of these rules.  After summarizing the several rules for conditioning, we present an illustrative example in which the various forms of conditioning can be explained.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5753",
        "title": "Compressed Constraints in Probabilistic Logic and Their Revision",
        "authors": [
            "Paul Snow"
        ],
        "abstract": "In probabilistic logic entailments, even moderate size problems can yield linear constraint systems with so many variables that exact methods are impractical.  This difficulty can be remedied in many cases of interest by introducing a three valued logic (true, false, and \"don't care\").  The three-valued approach allows the construction of \"compressed\" constraint systems which have the same solution sets as their two-valued counterparts, but which may involve dramatically fewer variables.  Techniques to calculate point estimates for the posterior probabilities of entailed sentences are discussed.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5754",
        "title": "Detecting Causal Relations in the Presence of Unmeasured Variables",
        "authors": [
            "Peter L. Spirtes"
        ],
        "abstract": "The presence of latent variables can greatly complicate inferences about causal relations between measured variables from statistical data.  In many cases, the presence of latent variables makes it impossible to determine for two measured variables A and B, whether A causes B, B causes A, or there is some common cause.  In this paper I present several theorems that state conditions under which it is possible to reliably infer the causal relation between two measured variables, regardless of whether latent variables are acting or not.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5755",
        "title": "A Method for Integrating Utility Analysis into an Expert System for Design Evaluation",
        "authors": [
            "Deborah L. Thurston",
            "Yun Qi Tian"
        ],
        "abstract": "In mechanical design, there is often unavoidable uncertainty in estimates of design performance.  Evaluation of design alternatives requires consideration of the impact of this uncertainty.  Expert heuristics embody assumptions regarding the designer's attitude towards risk and uncertainty that might be reasonable in most cases but inaccurate in others.  We present a technique to allow designers to incorporate their own unique attitude towards uncertainty as opposed to those assumed by the domain expert's rules. The general approach is to eliminate aspects of heuristic rules which directly or indirectly include assumptions regarding the user's attitude towards risk, and replace them with explicit, user-specified probabilistic multi attribute utility and probability distribution functions.  We illustrate the method in a system for material selection for automobile bumpers.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5756",
        "title": "From Relational Databases to Belief Networks",
        "authors": [
            "Wilson X. Wen"
        ],
        "abstract": "The relationship between belief networks and relational databases is examined.  Based on this analysis, a method to construct belief networks automatically from statistical relational data is proposed.  A comparison between our method and other methods shows that our method has several advantages when generalization or prediction is deeded.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5757",
        "title": "A Monte-Carlo Algorithm for Dempster-Shafer Belief",
        "authors": [
            "Nic Wilson"
        ],
        "abstract": "A very computationally-efficient Monte-Carlo algorithm for the calculation of Dempster-Shafer belief is described.  If Bel is the combination using Dempster's Rule of belief functions Bel, ..., Bel,7, then, for subset b of the frame C), Bel(b) can be calculated in time linear in 1(31 and m (given that the weight of conflict is bounded).  The algorithm can also be used to improve the complexity of the Shenoy-Shafer algorithms on Markov trees, and be generalised to calculate Dempster-Shafer Belief over other logics.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5758",
        "title": "Compatibility of Quantitative and Qualitative Representations of Belief",
        "authors": [
            "Michael S. K. M. Wong",
            "Y. Y. Yao",
            "P. Lingras"
        ],
        "abstract": "The compatibility of quantitative and qualitative representations of beliefs was studied extensively in probability theory.  It is only recently that this important topic is considered in the context of belief functions.  In this paper, the compatibility of various quantitative belief measures and qualitative belief structures is investigated.  Four classes of belief measures considered are: the probability function, the monotonic belief function, Shafer's belief function, and Smets' generalized belief function.  The analysis of their individual compatibility with different belief structures not only provides a sound b<msis for these quantitative measures, but also alleviates some of the difficulties in the acquisition and interpretation of numeric belief numbers.  It is shown that the structure of qualitative probability is compatible with monotonic belief functions.  Moreover, a belief structure slightly weaker than that of qualitative belief is compatible with Smets' generalized belief functions.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5759",
        "title": "An Efficient Implementation of Belief Function Propagation",
        "authors": [
            "Hong Xu"
        ],
        "abstract": "The local computation technique (Shafer et al. 1987, Shafer and Shenoy 1988, Shenoy and Shafer 1986) is used for propagating belief functions in so called a Markov Tree.  In this paper, we describe an efficient implementation of belief function propagation on the basis of the local computation technique.  The presented method avoids all the redundant computations in the propagation process, and so makes the computational complexity decrease with respect to other existing implementations (Hsia and Shenoy 1989, Zarley et al. 1988).  We also give a combined algorithm for both propagation and re-propagation which makes the re-propagation process more efficient when one or more of the prior belief functions is changed.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5760",
        "title": "A Non-Numeric Approach to Multi-Criteria/Multi-Expert Aggregation Based on Approximate Reasoning",
        "authors": [
            "Ronald R. Yager"
        ],
        "abstract": "We describe a technique that can be used for the fusion of multiple sources of information as well as for the evaluation and selection of alternatives under multi-criteria. Three important properties contribute to the uniqueness of the technique introduced.  The first is the ability to do all necessary operations and aggregations with information that is of a nonnumeric linguistic nature.  This facility greatly reduces the burden on the providers of information, the experts.  A second characterizing feature is the ability assign, again linguistically, differing importance to the criteria or in the case of information fusion to the individual sources of information.  A third significant feature of the approach is its ability to be used as method to find a consensus of the opinion of multiple experts on the issue of concern.  The techniques used in this approach are base on ideas developed from the theory of approximate reasoning.  We illustrate the approach with a problem of project selection.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5761",
        "title": "Why Do We Need Foundations for Modelling Uncertainties?",
        "authors": [
            "Henry E. Kyburg Jr"
        ],
        "abstract": "Surely we want solid foundations. What kind of castle can we build on sand? What is the point of devoting effort to balconies and minarets, if the foundation may be so weak as to allow the structure to collapse of its own weight? We want our foundations set on bedrock, designed to last for generations. Who would want an architect who cannot certify the soundness of the foundations of his buildings?\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5919",
        "title": "Heart Disease Prediction System using Associative Classification and Genetic Algorithm",
        "authors": [
            "M.Akhil Jabbar",
            "B L Deekshatulu",
            "Priti Chandra"
        ],
        "abstract": "Associative classification is a recent and rewarding technique which integrates association rule mining and classification to a model for prediction and achieves maximum accuracy. Associative classifiers are especially fit to applications where maximum accuracy is desired to a model for prediction. There are many domains such as medical where the maximum accuracy of the model is desired. Heart disease is a single largest cause of death in developed countries and one of the main contributors to disease burden in developing countries. Mortality data from the registrar general of India shows that heart disease are a major cause of death in India, and in Andhra Pradesh coronary heart disease cause about 30%of deaths in rural areas. Hence there is a need to develop a decision support system for predicting heart disease of a patient. In this paper we propose efficient associative classification algorithm using genetic approach for heart disease prediction. The main motivation for using genetic algorithm in the discovery of high level prediction rules is that the discovered rules are highly comprehensible, having high predictive accuracy and of high interestingness values. Experimental Results show that most of the classifier rules help in the best prediction of heart disease which even helps doctors in their diagnosis decisions.\n    ",
        "submission_date": "2013-03-24T00:00:00",
        "last_modified_date": "2013-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5929",
        "title": "DLOLIS-A: Description Logic based Text Ontology Learning",
        "authors": [
            "Sourish Dasgupta",
            "Ankur Padia",
            "Kushal Shah",
            "Rupali KaPatel",
            "Prasenjit Majumder"
        ],
        "abstract": "Ontology Learning has been the subject of intensive study for the past decade. Researchers in this field have been motivated by the possibility of automatically building a knowledge base on top of text documents so as to support reasoning based knowledge extraction. While most works in this field have been primarily statistical (known as light-weight Ontology Learning) not much attempt has been made in axiomatic Ontology Learning (called heavy-weight Ontology Learning) from Natural Language text documents. Heavy-weight Ontology Learning supports more precise formal logic-based reasoning when compared to statistical ontology learning. In this paper we have proposed a sound Ontology Learning tool DLOL_(IS-A) that maps English language IS-A sentences into their equivalent Description Logic (DL) expressions in order to automatically generate a consistent pair of T-box and A-box thereby forming both regular (definitional form) and generalized (axiomatic form) DL ontology. The current scope of the paper is strictly limited to IS-A sentences that exclude the possible structures of: (i) implicative IS-A sentences, and (ii) \"Wh\" IS-A questions. Other linguistic nuances that arise out of pragmatics and epistemic of IS-A sentences are beyond the scope of this present work. We have adopted Gold Standard based Ontology Learning evaluation on chosen IS-A rich Wikipedia documents.\n    ",
        "submission_date": "2013-03-24T00:00:00",
        "last_modified_date": "2013-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.6932",
        "title": "Bipolar Fuzzy Soft sets and its applications in decision making problem",
        "authors": [
            "Muhammad Aslam",
            "Saleem Abdullah",
            "Kifayat ullah"
        ],
        "abstract": "In this article, we combine the concept of a bipolar fuzzy set and a soft set. We introduce the notion of bipolar fuzzy soft set and study fundamental properties. We study basic operations on bipolar fuzzy soft set. We define exdended union, intersection of two bipolar fuzzy soft set. We also give an application of bipolar fuzzy soft set into decision making problem. We give a general algorithm to solve decision making problems by using bipolar fuzzy soft set.\n    ",
        "submission_date": "2013-03-23T00:00:00",
        "last_modified_date": "2013-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.7032",
        "title": "A Massively Parallel Associative Memory Based on Sparse Neural Networks",
        "authors": [
            "Zhe Yao",
            "Vincent Gripon",
            "Michael G. Rabbat"
        ],
        "abstract": "Associative memories store content in such a way that the content can be later retrieved by presenting the memory with a small portion of the content, rather than presenting the memory with an address as in more traditional memories. Associative memories are used as building blocks for algorithms within database engines, anomaly detection systems, compression algorithms, and face recognition systems. A classical example of an associative memory is the Hopfield neural network. Recently, Gripon and Berrou have introduced an alternative construction which builds on ideas from the theory of error correcting codes and which greatly outperforms the Hopfield network in capacity, diversity, and efficiency. In this paper we implement a variation of the Gripon-Berrou associative memory on a general purpose graphical processing unit (GPU). The work of Gripon and Berrou proposes two retrieval rules, sum-of-sum and sum-of-max. The sum-of-sum rule uses only matrix-vector multiplication and is easily implemented on the GPU. The sum-of-max rule is much less straightforward to implement because it involves non-linear operations. However, the sum-of-max rule gives significantly better retrieval error rates. We propose a hybrid rule tailored for implementation on a GPU which achieves a 880-fold speedup without sacrificing any accuracy.\n    ",
        "submission_date": "2013-03-28T00:00:00",
        "last_modified_date": "2013-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.7137",
        "title": "Discrete Optimization of Statistical Sample Sizes in Simulation by Using the Hierarchical Bootstrap Method",
        "authors": [
            "A. Andronov",
            "M. Fioshin"
        ],
        "abstract": "The Bootstrap method application in simulation supposes that value of random variables are not generated during the simulation process but extracted from available sample populations. In the case of Hierarchical Bootstrap the function of interest is calculated recurrently using the calculation tree. In the present paper we consider the optimization of sample sizes in each vertex of the calculation tree. The dynamic programming method is used for this aim. Proposed method allows to decrease a variance of system characteristic estimators.\n    ",
        "submission_date": "2013-03-28T00:00:00",
        "last_modified_date": "2013-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.7200",
        "title": "Design for a Darwinian Brain: Part 1. Philosophy and Neuroscience",
        "authors": [
            "Chrisantha Fernando"
        ],
        "abstract": "Physical symbol systems are needed for open-ended cognition. A good way to understand physical symbol systems is by comparison of thought to chemistry. Both have systematicity, productivity and compositionality. The state of the art in cognitive architectures for open-ended cognition is critically assessed. I conclude that a cognitive architecture that evolves symbol structures in the brain is a promising candidate to explain open-ended cognition. Part 2 of the paper presents such a cognitive architecture.\n    ",
        "submission_date": "2013-03-28T00:00:00",
        "last_modified_date": "2013-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.7201",
        "title": "Design for a Darwinian Brain: Part 2. Cognitive Architecture",
        "authors": [
            "Chrisantha Fernando",
            "Vera Vasas"
        ],
        "abstract": "The accumulation of adaptations in an open-ended manner during lifetime learning is a holy grail in reinforcement learning, intrinsic motivation, artificial curiosity, and developmental robotics. We present a specification for a cognitive architecture that is capable of specifying an unlimited range of behaviors. We then give examples of how it can stochastically explore an interesting space of adjacent possible behaviors. There are two main novelties; the first is a proper definition of the fitness of self-generated games such that interesting games are expected to evolve. The second is a modular and evolvable behavior language that has systematicity, productivity, and compositionality, i.e. it is a physical symbol system. A part of the architecture has already been implemented on a humanoid robot.\n    ",
        "submission_date": "2013-03-28T00:00:00",
        "last_modified_date": "2013-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.7430",
        "title": "Introducing Nominals to the Combined Query Answering Approaches for EL",
        "authors": [
            "Giorgio Stefanoni",
            "Boris Motik",
            "Ian Horrocks"
        ],
        "abstract": "So-called combined approaches answer a conjunctive query over a description logic ontology in three steps: first, they materialise certain consequences of the ontology and the data; second, they evaluate the query over the data; and third, they filter the result of the second phase to eliminate unsound answers. Such approaches were developed for various members of the DL-Lite and the EL families of languages, but none of them can handle ontologies containing nominals. In our work, we bridge this gap and present a combined query answering approach for ELHO---a logic that contains all features of the OWL 2 EL standard apart from transitive roles and complex role inclusions. This extension is nontrivial because nominals require equality reasoning, which introduces complexity into the first and the third step. Our empirical evaluation suggests that our technique is suitable for practical application, and so it provides a practical basis for conjunctive query answering in a large fragment of OWL 2 EL.\n    ",
        "submission_date": "2013-03-29T00:00:00",
        "last_modified_date": "2013-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.7445",
        "title": "Agent-based modeling of a price information trading business",
        "authors": [
            "Saad Ahmad Khan",
            "Ladislau Boloni"
        ],
        "abstract": "We describe an agent-based simulation of a fictional (but feasible) information trading business. The Gas Price Information Trader (GPIT) buys information about real-time gas prices in a metropolitan area from drivers and resells the information to drivers who need to refuel their vehicles.\n",
        "submission_date": "2013-03-29T00:00:00",
        "last_modified_date": "2013-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.0100",
        "title": "Entanglement Zoo I: Foundational and Structural Aspects",
        "authors": [
            "Diederik Aerts",
            "Sandro Sozzo"
        ],
        "abstract": "We put forward a general classification for a structural description of the entanglement present in compound entities experimentally violating Bell's inequalities, making use of a new entanglement scheme that we developed recently. Our scheme, although different from the traditional one, is completely compatible with standard quantum theory, and enables quantum modeling in complex Hilbert space for different types of situations. Namely, situations where entangled states and product measurements appear ('customary quantum modeling'), and situations where states and measurements and evolutions between measurements are entangled ('nonlocal box modeling', 'nonlocal non-marginal box modeling'). The role played by Tsirelson's bound and marginal distribution law is emphasized. Specific quantum models are worked out in detail in complex Hilbert space within this new entanglement scheme.\n    ",
        "submission_date": "2013-03-30T00:00:00",
        "last_modified_date": "2013-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.0102",
        "title": "Entanglement Zoo II: Examples in Physics and Cognition",
        "authors": [
            "Diederik Aerts",
            "Sandro Sozzo"
        ],
        "abstract": "We have recently presented a general scheme enabling quantum modeling of different types of situations that violate Bell's inequalities. In this paper, we specify this scheme for a combination of two concepts. We work out a quantum Hilbert space model where 'entangled measurements' occur in addition to the expected 'entanglement between the component concepts', or 'state entanglement'. We extend this result to a macroscopic physical entity, the 'connected vessels of water', which maximally violates Bell's inequalities. We enlighten the structural and conceptual analogies between the cognitive and physical situations which are both examples of a nonlocal non-marginal box modeling in our classification.\n    ",
        "submission_date": "2013-03-30T00:00:00",
        "last_modified_date": "2013-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.0145",
        "title": "Phase Transition and Network Structure in Realistic SAT Problems",
        "authors": [
            "Soumya C. Kambhampati",
            "Thomas Liu"
        ],
        "abstract": "A fundamental question in Computer Science is understanding when a specific class of problems go from being computationally easy to hard. Because of its generality and applications, the problem of Boolean Satisfiability (aka SAT) is often used as a vehicle for investigating this question. A signal result from these studies is that the hardness of SAT problems exhibits a dramatic easy-to-hard phase transition with respect to the problem constrainedness. Past studies have however focused mostly on SAT instances generated using uniform random distributions, where all constraints are independently generated, and the problem variables are all considered of equal importance. These assumptions are unfortunately not satisfied by most real problems. Our project aims for a deeper understanding of hardness of SAT problems that arise in practice. We study two key questions: (i) How does easy-to-hard transition change with more realistic distributions that capture neighborhood sensitivity and rich-get-richer aspects of real problems and (ii) Can these changes be explained in terms of the network properties (such as node centrality and small-worldness) of the clausal networks of the SAT problems. Our results, based on extensive empirical studies and network analyses, provide important structural and computational insights into realistic SAT problems. Our extensive empirical studies show that SAT instances from realistic distributions do exhibit phase transition, but the transition occurs sooner (at lower values of constrainedness) than the instances from uniform random distribution. We show that this behavior can be explained in terms of their clausal network properties such as eigenvector centrality and small-worldness (measured indirectly in terms of the clustering coefficients and average node distance).\n    ",
        "submission_date": "2013-03-30T00:00:00",
        "last_modified_date": "2013-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.0620",
        "title": "Disjunctive Logic Programs versus Normal Logic Programs",
        "authors": [
            "Heng Zhang",
            "Yan Zhang"
        ],
        "abstract": "This paper focuses on the expressive power of disjunctive and normal logic programs under the stable model semantics over finite, infinite, or arbitrary structures. A translation from disjunctive logic programs into normal logic programs is proposed and then proved to be sound over infinite structures. The equivalence of expressive power of two kinds of logic programs over arbitrary structures is shown to coincide with that over finite structures, and coincide with whether or not NP is closed under complement. Over finite structures, the intranslatability from disjunctive logic programs to normal logic programs is also proved if arities of auxiliary predicates and functions are bounded in a certain way.\n    ",
        "submission_date": "2013-04-02T00:00:00",
        "last_modified_date": "2013-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.0715",
        "title": "A cookbook of translating English to Xapi",
        "authors": [
            "Ladislau B\u00f6l\u00f6ni"
        ],
        "abstract": "The Xapagy cognitive architecture had been designed to perform narrative reasoning: to model and mimic the activities performed by humans when witnessing, reading, recalling, narrating and talking about stories. Xapagy communicates with the outside world using Xapi, a simplified, \"pidgin\" language which is strongly tied to the internal representation model (instances, scenes and verb instances) and reasoning techniques (shadows and headless shadows). While not fully a semantic equivalent of natural language, Xapi can represent a wide range of complex stories. We illustrate the representation technique used in Xapi through examples taken from folk physics, folk psychology as well as some more unusual literary examples. We argue that while the Xapi model represents a conceptual shift from the English representation, the mapping is logical and consistent, and a trained knowledge engineer can translate between English and Xapi at near-native speed.\n    ",
        "submission_date": "2013-03-31T00:00:00",
        "last_modified_date": "2013-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.0806",
        "title": "IFP-Intuitionistic fuzzy soft set theory and its applications",
        "authors": [
            "Faruk Karaaslan",
            "Naim Cagman",
            "Saban Yilmaz"
        ],
        "abstract": "In this work, we present definition of intuitionistic fuzzy parameterized (IFP) intuitionistic fuzzy soft set and its operations. Then we define IFP-aggregation operator to form IFP-intuitionistic fuzzy soft-decision-making method which allows constructing more efficient decision processes.\n    ",
        "submission_date": "2013-04-02T00:00:00",
        "last_modified_date": "2016-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.0844",
        "title": "Coalitional Manipulation for Schulze's Rule",
        "authors": [
            "Serge Gaspers",
            "Thomas Kalinowski",
            "Nina Narodytska",
            "Toby Walsh"
        ],
        "abstract": "Schulze's rule is used in the elections of a large number of organizations including Wikimedia and Debian. Part of the reason for its popularity is the large number of axiomatic properties, like monotonicity and Condorcet consistency, which it satisfies. We identify a potential shortcoming of Schulze's rule: it is computationally vulnerable to manipulation. In particular, we prove that computing an unweighted coalitional manipulation (UCM) is polynomial for any number of manipulators. This result holds for both the unique winner and the co-winner versions of UCM. This resolves an open question stated by Parkes and Xia (2012). We also prove that computing a weighted coalitional manipulation (WCM) is polynomial for a bounded number of candidates. Finally, we discuss the relation between the unique winner UCM problem and the co-winner UCM problem and argue that they have substantially different necessary and sufficient conditions for the existence of a successful manipulation.\n    ",
        "submission_date": "2013-04-03T00:00:00",
        "last_modified_date": "2013-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.0897",
        "title": "Duality in STRIPS planning",
        "authors": [
            "Martin Suda"
        ],
        "abstract": "We describe a duality mapping between STRIPS planning tasks. By exchanging the initial and goal conditions, taking their respective complements, and swapping for every action its precondition and delete list, one obtains for every STRIPS task its dual version, which has a solution if and only if the original does. This is proved by showing that the described transformation essentially turns progression (forward search) into regression (backward search) and vice versa.\n",
        "submission_date": "2013-04-03T00:00:00",
        "last_modified_date": "2013-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.0913",
        "title": "Predicting Network Attacks Using Ontology-Driven Inference",
        "authors": [
            "Ahmad Salahi",
            "Morteza Ansarinia"
        ],
        "abstract": "Graph knowledge models and ontologies are very powerful modeling and re asoning tools. We propose an effective approach to model network attacks and attack prediction which plays important roles in security management. The goals of this study are: First we model network attacks, their prerequisites and consequences using knowledge representation methods in order to provide description logic reasoning and inference over attack domain concepts. And secondly, we propose an ontology-based system which predicts potential attacks using inference and observing information which provided by sensory inputs. We generate our ontology and evaluate corresponding methods using CAPEC, CWE, and CVE hierarchical datasets. Results from experiments show significant capability improvements comparing to traditional hierarchical and relational models. Proposed method also reduces false alarms and improves intrusion detection effectiveness.\n    ",
        "submission_date": "2013-04-03T00:00:00",
        "last_modified_date": "2013-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1081",
        "title": "Exploiting Functional Dependencies in Qualitative Probabilistic Reasoning",
        "authors": [
            "Michael P. Wellman"
        ],
        "abstract": "Functional dependencies restrict the potential interactions among variables connected in a probabilistic network.  This restriction can be exploited in qualitative probabilistic reasoning by introducing deterministic variables and modifying the inference rules to produce stronger conclusions in the presence of functional relations.  I describe how to accomplish these modifications in qualitative probabilistic networks by exhibiting the update procedures for graphical transformations involving probabilistic and deterministic variables and combinations.  A simple example demonstrates that the augmented scheme can reduce qualitative ambiguity that would arise without the special treatment of functional dependency.  Analysis of qualitative synergy reveals that new higher-order relations are required to reason effectively about synergistic interactions among deterministic variables.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1082",
        "title": "Qualitative Propagation and Scenario-based Explanation of Probabilistic Reasoning",
        "authors": [
            "Max Henrion",
            "Marek J. Druzdzel"
        ],
        "abstract": "Comprehensible explanations of probabilistic reasoning are a prerequisite for wider acceptance of Bayesian methods in expert systems and decision support systems.  A study of human reasoning under uncertainty suggests two different strategies for explaining probabilistic reasoning: The first, qualitative belief propagation, traces the qualitative effect of evidence through a belief network from one variable to the next.  This propagation algorithm is an alternative to the graph reduction algorithms of Wellman (1988) for inference in qualitative probabilistic networks.  It is based on a qualitative analysis of intercausal reasoning, which is a generalization of Pearl's \"explaining away\", and an alternative to Wellman's definition of qualitative synergy.  The other, Scenario-based reasoning, involves the generation of alternative causal \"stories\" accounting for the evidence.  Comparing a few of the most probable scenarios provides an approximate way to explain the results of probabilistic reasoning.  Both schemes employ causal as well as probabilistic knowledge.  Probabilities may be presented as phrases and/or numbers.  Users can control the style, abstraction and completeness of explanations.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1083",
        "title": "Managing Uncertainty in Rule Based Cognitive Models",
        "authors": [
            "Thomas R. Shultz"
        ],
        "abstract": "An experiment replicated and extended recent findings on psychologically realistic ways of modeling propagation of uncertainty in rule based reasoning.  Within a single production rule, the antecedent evidence can be summarized by taking the maximum of disjunctively connected antecedents and the minimum of conjunctively connected antecedents.  The maximum certainty factor attached to each of the rule's conclusions can be sealed down by multiplication with this summarized antecedent certainty. Heckerman's modified certainty factor technique can be used to combine certainties for common conclusions across production rules.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1084",
        "title": "Context-Dependent Similarity",
        "authors": [
            "Yizong Cheng"
        ],
        "abstract": "Attribute weighting and differential weighting, two major mechanisms for computing context-dependent similarity or dissimilarity measures are studied and compared.  A dissimilarity measure based on subset size in the context is proposed and its metrization and application are given.  It is also shown that while all attribute weighting dissimilarity measures are metrics differential weighting dissimilarity measures are usually non-metric.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1085",
        "title": "Similarity Networks for the Construction of Multiple-Faults Belief Networks",
        "authors": [
            "David Heckerman"
        ],
        "abstract": "A similarity network is a tool for constructing belief networks for the diagnosis of a single fault. In this paper, we examine modifications to the similarity-network representation that facilitate the construction of belief networks for the diagnosis of multiple coexisting faults.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1086",
        "title": "Integrating Probabilistic, Taxonomic and Causal Knowledge in Abductive Diagnosis",
        "authors": [
            "Dekang Lin",
            "Randy Goebel"
        ],
        "abstract": "We propose an abductive diagnosis theory that integrates probabilistic, causal and taxonomic knowledge.  Probabilistic knowledge allows us to select the most likely explanation; causal knowledge allows us to make reasonable independence assumptions; taxonomic knowledge allows causation to be modeled at different levels of detail, and allows observations be described in different levels of precision.  Unlike most other approaches where a causal explanation is a hypothesis that one or more causative events occurred, we define an explanation of a set of observations to be an occurrence of a chain of causation events.  These causation events constitute a scenario where all the observations are true.  We show that the probabilities of the scenarios can be computed from the conditional probabilities of the causation events.  Abductive reasoning is inherently complex even if only modest expressive power is allowed.  However, our abduction algorithm is exponential only in the number of observations to be explained, and is polynomial in the size of the knowledge base.  This contrasts with many other abduction procedures that are exponential in the size of the knowledge base.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1087",
        "title": "What is an Optimal Diagnosis?",
        "authors": [
            "David L. Poole",
            "Gregory M. Provan"
        ],
        "abstract": "Within diagnostic reasoning there have been a number of proposed definitions of a diagnosis, and thus of the most likely diagnosis, including most probable posterior hypothesis, most probable interpretation, most probable covering hypothesis, etc.  Most of these approaches assume that the most likely diagnosis must be computed, and that a definition of what should be computed can be made a priori, independent of what the diagnosis is used for.  We argue that the diagnostic problem, as currently posed, is incomplete: it does not consider how the diagnosis is to be used, or the utility associated with the treatment of the abnormalities.  In this paper we analyze several well-known definitions of diagnosis, showing that the different definitions of the most likely diagnosis have different qualitative meanings, even given the same input data.  We argue that the most appropriate definition of (optimal) diagnosis needs to take into account the utility of outcomes and what the diagnosis is used for.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1088",
        "title": "Kutato: An Entropy-Driven System for Construction of Probabilistic Expert Systems from Databases",
        "authors": [
            "Edward H. Herskovits",
            "Gregory F. Cooper"
        ],
        "abstract": "Kutato is a system that takes as input a database of cases and produces a belief network that captures many of the dependence relations represented by those data.  This system incorporates a module for determining the entropy of a belief network and a module for constructing belief networks based on entropy calculations.  Kutato constructs an initial belief network in which all variables in the database are assumed to be marginally independent.  The entropy of this belief network is calculated, and that arc is added that minimizes the entropy of the resulting belief network.  Conditional probabilities for an arc are obtained directly from the database.  This process continues until an entropy-based threshold is reached.  We have tested the system by generating databases from networks using the probabilistic logic-sampling method, and then using those databases as input to Kutato.  The system consistently reproduces the original belief networks with high fidelity.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1089",
        "title": "Ideal Reformulation of Belief Networks",
        "authors": [
            "John S. Breese",
            "Eric J. Horvitz"
        ],
        "abstract": "The intelligent reformulation or restructuring of a belief network can greatly increase the efficiency of inference.  However, time expended for reformulation is not available for performing inference.  Thus, under time pressure, there is a tradeoff between the time dedicated to reformulating the network and the time applied to the implementation of a solution.  We investigate this partition of resources into time applied to reformulation and time used for inference.  We shall describe first general principles for computing the ideal partition of resources under uncertainty.  These principles have applicability to a wide variety of problems that can be divided into interdependent phases of problem solving. After, we shall present results of our empirical study of the problem of determining the ideal amount of time to devote to searching for clusters in belief networks.  In this work, we acquired and made use of probability distributions that characterize (1) the performance of alternative heuristic search methods for reformulating a network instance into a set of cliques, and (2) the time for executing inference procedures on various belief networks.  Given a preference model describing the value of a solution as a function of the delay required for its computation, the system selects an ideal time to devote to reformulation.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1090",
        "title": "Computationally-Optimal Real-Resource Strategies",
        "authors": [
            "David Einav",
            "Michael R. Fehling"
        ],
        "abstract": "This paper focuses on managing the cost of deliberation before action.  In many problems, the overall quality of the solution reflects costs incurred and resources consumed in deliberation as well as the cost and benefit of execution, when both the resource consumption in deliberation phase, and the costs in deliberation and execution are uncertain and may be described by probability distribution functions.  A feasible (in terms of resource consumption) strategy that minimizes the expected total cost is termed computationally-optimal.  For a situation with several independent, uninterruptible methods to solve the problem, we develop a pseudopolynomial-time algorithm to construct generate-and-test computationally optimal strategy.  We show this strategy-construction problem to be NP-complete, and apply Bellman's Optimality Principle to solve it efficiently.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1091",
        "title": "Problem Formulation as the Reduction of a Decision Model",
        "authors": [
            "David Heckerman",
            "Eric J. Horvitz"
        ],
        "abstract": "In this paper, we extend the QMRDT probabilistic model for the domain of internal medicine to include decisions about treatments. In addition, we describe how we can use the comprehensive decision model to construct a simpler decision model for a specific patient. In so doing, we transform the task of problem formulation to that of narrowing of a larger problem.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1092",
        "title": "Dynamic Construction of Belief Networks",
        "authors": [
            "Robert P. Goldman",
            "Eugene Charniak"
        ],
        "abstract": "We describe a method for incrementally constructing belief networks.  We have developed a network-construction language similar to a forward-chaining language using data dependencies, but with additional features for specifying distributions.  Using this language, we can define parameterized classes of probabilistic models.  These parameterized models make it possible to apply probabilistic reasoning to problems for which it is impractical to have a single large static model.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1093",
        "title": "A New Algorithm for Finding MAP Assignments to Belief Networks",
        "authors": [
            "Solomon Eyal Shimony",
            "Eugene Charniak"
        ],
        "abstract": "We present a new algorithm for finding maximum a-posterior) (MAP) assignments of values to belief networks.  The belief network is compiled into a network consisting only of nodes with boolean (i.e. only 0 or 1) conditional probabilities.  The MAP assignment is then found using a best-first search on the resulting network.  We argue that, as one would anticipate, the algorithm is exponential for the general case, but only linear in the size of the network for poly trees.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1094",
        "title": "Reducing Uncertainty in Navigation and Exploration",
        "authors": [
            "K. Bayse",
            "M. Lejter",
            "Keiji Kanazawa"
        ],
        "abstract": "A significant problem in designing mobile robot control systems involves coping with the uncertainty that arises in moving about in an unknown or partially unknown environment and relying on noisy or ambiguous sensor data to acquire knowledge about that environment. We describe a control system that chooses what activity to engage in next on the basis of expectations about how the information re- turned as a result of a given activity will improve 2 its knowledge about the spatial layout of its environment. Certain of the higher-level components of the control system are specified in terms of probabilistic decision models whose output is used to mediate the behavior of lower-level control components responsible for movement and sensing.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1095",
        "title": "Ergo: A Graphical Environment for Constructing Bayesian",
        "authors": [
            "Ingo Beinlich",
            "Edward H. Herskovits"
        ],
        "abstract": "We describe an environment that considerably simplifies the process of generating Bayesian belief networks. The system has been implemented on readily available, inexpensive hardware, and provides clarity and high performance. We present an introduction to Bayesian belief networks, discuss algorithms for inference with these networks, and delineate the classes of problems that can be solved with this paradigm. We then describe the hardware and software that constitute the system, and illustrate Ergo's use with several example\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1096",
        "title": "Decision Making with Interval Influence Diagrams",
        "authors": [
            "John S. Breese",
            "Kenneth W. Fertig"
        ],
        "abstract": "In previous work (Fertig and Breese, 1989; Fertig and Breese, 1990) we defined a mechanism for performing probabilistic reasoning in influence diagrams using interval rather than point-valued probabilities.  In this paper we extend these procedures to incorporate decision nodes and interval-valued value functions in the diagram.  We derive the procedures for chance node removal (calculating expected value) and decision node removal (optimization) in influence diagrams where lower bounds on probabilities are stored at each chance node and interval bounds are stored on the value function associated with the diagram's value node.  The output of the algorithm are a set of admissible alternatives for each decision variable and a set of bounds on expected value based on the imprecision in the input.  The procedure can be viewed as an approximation to a full e-dimensional sensitivity analysis where n are the number of imprecise probability distributions in the input.  We show the transformations are optimal and sound.  The performance of the algorithm on an influence diagrams is investigated and compared to an exact algorithm.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1097",
        "title": "A Randomized Approximation Algorithm of Logic Sampling",
        "authors": [
            "R. Martin Chavez",
            "Gregory F. Cooper"
        ],
        "abstract": "In recent years, researchers in decision analysis and artificial intelligence (AI) have used Bayesian belief networks to build models of expert opinion. Using standard methods drawn from the theory of computational complexity, workers in the field have shown that the problem of exact probabilistic inference on belief networks almost certainly requires exponential computation in the worst ease [3]. We have previously described a randomized approximation scheme, called BN-RAS, for computation on belief networks [ 1, 2, 4]. We gave precise analytic bounds on the convergence of BN-RAS and showed how to trade running time for accuracy in the evaluation of posterior marginal probabilities. We now extend our previous results and demonstrate the generality of our framework by applying similar mathematical techniques to the analysis of convergence for logic sampling [7], an alternative simulation algorithm for probabilistic inference.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1099",
        "title": "Time, Chance, and Action",
        "authors": [
            "Peter Haddawy"
        ],
        "abstract": "To operate intelligently in the world, an agent must reason about its actions.  The consequences of an action are a function of both the state of the world and the action itself.  Many aspects of the world are inherently stochastic, so a representation for reasoning about actions must be able to express chances of world states as well as indeterminacy in the effects of actions and other events.  This paper presents a propositional temporal probability logic for representing and reasoning about actions. The logic can represent the probability that facts hold and events occur at various times. It can represent the probability that actions and other events affect the future.  It can represent concurrent actions and conditions that hold or change during execution of an action.  The model of probability relates probabilities over time.  The logical language integrates both modal and probabilistic constructs and can thus represent and distinguish between possibility, probability, and truth.  Several examples illustrating the use of the logic are given.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1100",
        "title": "A Dynamic Approach to Probabilistic Inference",
        "authors": [
            "Michael C. Horsch",
            "David L. Poole"
        ],
        "abstract": "In this paper we present a framework for dynamically constructing Bayesian networks. We introduce the notion of a background knowledge base of schemata, which is a collection of parameterized conditional probability statements. These schemata explicitly separate the general knowledge of properties an individual may have from the specific knowledge of particular individuals that may have these properties. Knowledge of individuals can be combined with this background knowledge to create Bayesian networks, which can then be used in any propagation scheme. We discuss the theory and assumptions necessary for the implementation of dynamic Bayesian networks, and indicate where our approach may be useful.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1101",
        "title": "Approximations in Bayesian Belief Universe for Knowledge Based Systems",
        "authors": [
            "Frank Jensen",
            "S. K. Anderson"
        ],
        "abstract": "When expert systems based on causal probabilistic networks (CPNs) reach a certain size and complexity, the \"combinatorial explosion monster\" tends to be present. We propose an approximation scheme that identifies rarely occurring cases and excludes these from being processed as ordinary cases in a CPN-based expert system. Depending on the topology and the probability distributions of the CPN, the numbers (representing probabilities of state combinations) in the underlying numerical representation can become very small. Annihilating these numbers and utilizing the resulting sparseness through data structuring techniques often results in several orders of magnitude of improvement in the consumption of computer resources. Bounds on the errors introduced into a CPN-based expert system through approximations are established. Finally, reports on empirical studies of applying the approximation scheme to a real-world CPN are given.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1102",
        "title": "Robust Inference Policies",
        "authors": [
            "Paul E. Lehner"
        ],
        "abstract": "A series of monte carlo studies were performed to assess the extent to which different inference procedures robustly output reasonable belief values in the context of increasing levels of judgmental imprecision. It was found that, when compared to an equal-weights linear model, the Bayesian procedures are more likely to deduce strong support for a hypothesis. But, the Bayesian procedures are also more likely to strongly support the wrong hypothesis. Bayesian techniques are more powerful, but are also more error prone.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1103",
        "title": "Minimum Error Tree Decomposition",
        "authors": [
            "L. Liu",
            "Y. Ma",
            "D. Wilkins",
            "Z. Bian",
            "X. Ying"
        ],
        "abstract": "This paper describes a generalization of previous methods for constructing tree-structured belief network with hidden variables. The major new feature of the described method is the ability to produce a tree decomposition even when there are errors in the correlation data among the input variables. This is an important extension of existing methods since the correlational coefficients usually cannot be measured with precision. The technique involves using a greedy search algorithm that locally minimizes an error function.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1104",
        "title": "A Polynomial Time Algorithm for Finding Bayesian Probabilities from Marginal Constraints",
        "authors": [
            "J. W. Miller",
            "R. M. Goodman"
        ],
        "abstract": "A method of calculating probability values from a system of marginal constraints is presented. Previous systems for finding the probability of a single attribute have either made an independence assumption concerning the evidence or have required, in the worst case, time exponential in the number of attributes of the system. In this paper a closed form solution to the probability of an attribute given the evidence is found. The closed form solution, however does not enforce the (non-linear) constraint that all terms in the underlying distribution be positive. The equation requires O(r^3) steps to evaluate, where r is the number of independent marginal constraints describing the system at the time of evaluation. Furthermore, a marginal constraint may be exchanged with a new constraint, and a new solution calculated in O(r^2) steps. This method is appropriate for calculating probabilities in a real time expert system\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1105",
        "title": "Computation of Variances in Causal Networks",
        "authors": [
            "Richard E. Neapolitan",
            "James Kenevan"
        ],
        "abstract": "The causal (belief) network is a well-known graphical structure for representing independencies in a joint probability distribution. The exact methods and the approximation methods, which perform probabilistic inference in causal networks, often treat the conditional probabilities which are stored in the network as certain values. However, if one takes either a subjectivistic or a limiting frequency approach to probability, one can never be certain of probability values. An algorithm for probabilistic inference should not only be capable of reporting the inferred probabilities; it should also be capable of reporting the uncertainty in these probabilities relative to the uncertainty in the probabilities which are stored in the network. In section 2 of this paper a method is given for determining the prior variances of the probabilities of all the nodes. Section 3 contains an approximation method for determining the variances in inferred probabilities.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1106",
        "title": "A Sensitivity Analysis of Pathfinder",
        "authors": [
            "Keung-Chi Ng",
            "Bruce Abramson"
        ],
        "abstract": "Knowledge elicitation is one of the major bottlenecks in expert system design. Systems based on Bayes nets require two types of information--network structure and parameters (or probabilities). Both must be elicited from the domain expert. In general, parameters have greater opacity than structure, and more time is spent in their refinement than in any other phase of elicitation. Thus, it is important to determine the point of diminishing returns, beyond which further refinements will promise little (if any) improvement. Sensitivity analyses address precisely this issue--the sensitivity of a model to the precision of its parameters. In this paper, we report the results of a sensitivity analysis of Pathfinder, a Bayes net based system for diagnosing pathologies of the lymph system. This analysis is intended to shed some light on the relative importance of structure and parameters to system performance, as well as the sensitivity of a system based on a Bayes net to noise in its assessed parameters.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1107",
        "title": "IDEAL: A Software Package for Analysis of Influence Diagrams",
        "authors": [
            "Sampath Srinivas",
            "John S. Breese"
        ],
        "abstract": "IDEAL (Influence Diagram Evaluation and Analysis in Lisp) is a software environment for creation and evaluation of belief networks and influence diagrams. IDEAL is primarily a research tool and provides an implementation of many of the latest developments in belief network and influence diagram evaluation in a unified framework. This paper describes IDEAL and some lessons learned during its development.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1108",
        "title": "On the Equivalence of Causal Models",
        "authors": [
            "Tom S. Verma",
            "Judea Pearl"
        ],
        "abstract": "Scientists often use directed acyclic graphs (days) to model the qualitative structure of causal theories, allowing the parameters to be estimated from observational data.  Two causal models are equivalent if there is no experiment which could distinguish one from the other.  A canonical representation for causal models is presented which yields an efficient graphical criterion for deciding equivalence, and provides a theoretical basis for extracting causal structures from empirical data.  This representation is then extended to the more general case of an embedded causal model, that is, a dag in which only a subset of the variables are observable.  The canonical representation presented here yields an efficient algorithm for determining when two embedded causal models reflect the same dependency information.  This algorithm leads to a model theoretic definition of causation in terms of statistical dependencies.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1109",
        "title": "Application of Confidence Intervals to the Autonomous Acquisition of High-level Spatial Knowledge",
        "authors": [
            "Lambert E. Wixson"
        ],
        "abstract": "Objects in the world usually appear in context, participating in spatial relationships and interactions that are predictable and expected. Knowledge of these contexts can be used in the task of using a mobile camera to search for a specified object in a room. We call this the object search task. This paper is concerned with representing this knowledge in a manner facilitating its application to object search while at the same time lending itself to autonomous learning by a robot. The ability for the robot to learn such knowledge without supervision is crucial due to the vast number of possible relationships that can exist for any given set of objects. Moreover, since a robot will not have an infinite amount of time to learn, it must be able to determine an order in which to look for possible relationships so as to maximize the rate at which new knowledge is gained. In effect, there must be a \"focus of interest\" operator that allows the robot to choose which examples are likely to convey the most new information and should be examined first. This paper demonstrates how a representation based on statistical confidence intervals allows the construction of a system that achieves the above goals. An algorithm, based on the Highest Impact First heuristic, is presented as a means for providing a \"focus of interest\" with which to control the learning process, and examples are given.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1110",
        "title": "Directed Reduction Algorithms and Decomposable Graphs",
        "authors": [
            "Ross D. Shachter",
            "Stig K. Andersen",
            "Kim-Leng Poh"
        ],
        "abstract": "In recent years, there have been intense research efforts to develop efficient methods for probabilistic inference in probabilistic influence diagrams or belief networks.  Many people have concluded that the best methods are those based on undirected graph structures, and that those methods are inherently superior to those based on node reduction operations on the influence diagram.  We show here that these two approaches are essentially the same, since they are explicitly or implicity building and operating on the same underlying graphical structures.  In this paper we examine those graphical structures and show how this insight can lead to an improved class of directed reduction methods.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1111",
        "title": "Optimal Decomposition of Belief Networks",
        "authors": [
            "Wilson X. Wen"
        ],
        "abstract": "In this paper, optimum decomposition of belief networks is discussed. Some methods of decomposition are examined and a new method - the method of Minimum Total Number of States (MTNS) - is proposed.  The problem of optimum belief network decomposition under our framework, as under all the other frameworks, is shown to be NP-hard. According to the computational complexity analysis, an algorithm of belief network decomposition is proposed in (Wee, 1990a) based on simulated annealing.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1112",
        "title": "Pruning Bayesian Networks for Efficient Computation",
        "authors": [
            "Michelle Baker",
            "Terrance E. Boult"
        ],
        "abstract": "This paper analyzes the circumstances under which Bayesian networks can be pruned in order to reduce computational complexity without altering the computation for variables of interest.  Given a problem instance which consists of a query and evidence for a set of nodes in the network, it is possible to delete portions of the network which do not participate in the computation for the query.  Savings in computational complexity can be large when the original network is not singly connected.  Results analogous to those described in this paper have been derived before [Geiger, Verma, and Pearl 89, Shachter 88] but the implications for reducing complexity of the computations in Bayesian networks have not been stated explicitly.  We show how a preprocessing step can be used to prune a Bayesian network prior to using standard algorithms to solve a given problem instance.  We also show how our results can be used in a parallel distributed implementation in order to achieve greater savings.  We define a computationally equivalent subgraph of a Bayesian network.  The algorithm developed in [Geiger, Verma, and Pearl 89] is modified to construct the subgraphs described in this paper with O(e) complexity, where e is the number of edges in the Bayesian network.  Finally, we define a minimal computationally equivalent subgraph and prove that the subgraphs described are minimal.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1113",
        "title": "On Heuristics for Finding Loop Cutsets in Multiply-Connected Belief Networks",
        "authors": [
            "Jonathan Stillman"
        ],
        "abstract": "We introduce a new heuristic algorithm for the problem of finding minimum size loop cutsets in multiply connected belief networks.  We compare this algorithm to that proposed in [Suemmondt and Cooper, 1988].  We provide lower bounds on the performance of these algorithms with respect to one another and with respect to optimal. We demonstrate that no heuristic algorithm for this problem cam be guaranteed to produce loop cutsets within a constant difference from optimal.  We discuss experimental results based on randomly generated networks, and discuss future work and open questions.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1114",
        "title": "A Combination of Cutset Conditioning with Clique-Tree Propagation in the Pathfinder System",
        "authors": [
            "Jaap Suermondt",
            "Gregory F. Cooper",
            "David Heckerman"
        ],
        "abstract": "Cutset conditioning and clique-tree propagation are two popular methods for performing exact probabilistic inference in Bayesian belief networks.  Cutset conditioning is based on decomposition of a subset of network nodes, whereas clique-tree propagation depends on aggregation of nodes.  We describe a means to combine cutset conditioning and clique- tree propagation in an approach called aggregation after decomposition (AD).  We discuss the application of the AD method in the Pathfinder system, a medical expert system that offers assistance with diagnosis in hematopathology.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1115",
        "title": "Possibility as Similarity: the Semantics of Fuzzy Logic",
        "authors": [
            "Enrique H. Ruspini"
        ],
        "abstract": "This paper addresses fundamental issues on the nature of the concepts and structures of fuzzy logic, focusing, in particular, on the conceptual and functional differences that exist between probabilistic and possibilistic approaches.  A semantic model provides the basic framework to define possibilistic structures and concepts by means of a function that quantifies proximity, closeness, or resemblance between pairs of possible worlds.  The resulting model is a natural extension, based on multiple conceivability relations, of the modal logic concepts of necessity and possibility.  By contrast, chance-oriented probabilistic concepts and structures rely on measures of set extension that quantify the proportion of possible worlds where a proposition is true.  Resemblance between possible worlds is quantified by a generalized similarity relation: a function that assigns a number between O and 1 to every pair of possible worlds.  Using this similarity relation, which is a form of numerical complement of a classic metric or distance, it is possible to define and interpret the major constructs and methods of fuzzy logic: conditional and unconditioned possibility and necessity distributions and the generalized modus ponens of Zadeh.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1116",
        "title": "Integrating Case-Based and Rule-Based Reasoning: the Possibilistic Connection",
        "authors": [
            "Soumitra Dutta",
            "Piero P. Bonissone"
        ],
        "abstract": "Rule based reasoning (RBR) and case based reasoning (CBR) have emerged as two important and complementary reasoning methodologies in artificial intelligence (Al).  For problem solving in complex, real world situations, it is useful to integrate RBR and CBR. This paper presents an approach to achieve a compact and seamless integration of RBR and CBR within the base architecture of rules.  The paper focuses on the possibilistic nature of the approximate reasoning methodology common to both CBR and RBR.  In CBR, the concept of similarity is casted as the complement of the distance between cases. In RBR the transitivity of similarity is the basis for the approximate deductions based on the generalized modus ponens.  It is shown that the integration of CBR and RBR is possible without altering the inference engine of RBR.  This integration is illustrated in the financial domain of mergers and acquisitions.  These ideas have been implemented in a prototype system called MARS.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1117",
        "title": "Credibility Discounting in the Theory of Approximate Reasoning",
        "authors": [
            "Ronald R. Yager"
        ],
        "abstract": "We are concerned with the problem of introducing credibility type information into reasoning systems.  The concept of credibility allows us to discount information provided by agents.  An important characteristic of this kind of procedure is that a complete lack of credibility rather than resulting in the negation of the information provided results in the nullification of the information provided.  We suggest a representational scheme for credibility qualification in the theory of approximate reasoning.  We discuss the concept of relative credibility.  By this idea we mean to indicate situations in which the credibility of a piece of evidence is determined by its compatibility with higher priority evidence. This situation leads to structures very much in the spirit of nonmonotonic reasoning.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1118",
        "title": "Updating with Belief Functions, Ordinal Conditioning Functions and Possibility Measures",
        "authors": [
            "Didier Dubois",
            "Henri Prade"
        ],
        "abstract": "This paper discusses how a measure of uncertainty representing a state of knowledge can be updated when a new information, which may be pervaded with uncertainty, becomes available.  This problem is considered in various framework, namely: Shafer's evidence theory, Zadeh's possibility theory, Spohn's theory of epistemic states.  In the two first cases, analogues of Jeffrey's rule of conditioning are introduced and discussed.  The relations between Spohn's model and possibility theory are emphasized and Spohn's updating rule is contrasted with the Jeffrey-like rule of conditioning in possibility theory. Recent results by Shenoy on the combination of ordinal conditional functions are reinterpreted in the language of possibility theory.  It is shown that Shenoy's combination rule has a well-known possibilistic counterpart.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1119",
        "title": "A New Approach to Updating Beliefs",
        "authors": [
            "Ronald Fagin",
            "Joseph Y. Halpern"
        ],
        "abstract": "We define a new notion of conditional belief, which plays the same role for Dempster-Shafer belief functions as conditional probability does for probability functions.  Our definition is different from the standard definition given by Dempster, and avoids many of the well-known problems of that definition.  Just as the conditional probability Pr (lB) is a probability function which is the result of conditioning on B being true, so too our conditional belief function Bel (lB) is a belief function which is the result of conditioning on B being true.  We define the conditional belief as the lower envelope (that is, the inf) of a family of conditional probability functions, and provide a closed form expression for it.  An alternate way of understanding our definition of conditional belief is provided by considering ideas from an earlier paper [Fagin and Halpern, 1989], where we connect belief functions with inner measures.  In particular, we show here how to extend the definition of conditional probability to non measurable sets, in order to get notions of inner and outer conditional probabilities, which can be viewed as best approximations to the true conditional probability, given our lack of information.  Our definition of conditional belief turns out to be an exact analogue of our definition of inner conditional probability.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1120",
        "title": "The Transferable Belief Model and Other Interpretations of Dempster-Shafer's Model",
        "authors": [
            "Philippe Smets"
        ],
        "abstract": "Dempster-Shafer's model aims at quantifying degrees of belief But there are so many interpretations of Dempster-Shafer's theory in the literature that it seems useful to present the various contenders in order to clarify their respective positions.  We shall successively consider the classical probability model, the upper and lower probabilities model, Dempster's model, the transferable belief model, the evidentiary value model, the provability or necessity model.  None of these models has received the qualification of Dempster-Shafer.  In fact the transferable belief model is our interpretation not of Dempster's work but of Shafer's work as presented in his book (Shafer 1976, Smets 1988). It is a ?purified' form of Dempster-Shafer's model in which any connection with probability concept has been deleted.  Any model for belief has at least two components: one static that describes our state of belief, the other dynamic that explains how to update our belief given new pieces of information.  We insist on the fact that both components must be considered in order to study these models.  Too many authors restrict themselves to the static component and conclude that Dempster-Shafer theory is the same as some other theory.  But once the dynamic component is considered, these conclusions break down.  Any comparison based only on the static component is too restricted.  The dynamic component must also be considered as the originality of the models based on belief functions lies in its dynamic component.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1121",
        "title": "Valuation-Based Systems for Discrete Optimization",
        "authors": [
            "Prakash P. Shenoy",
            "Glenn Shafer"
        ],
        "abstract": "This paper describes valuation-based systems for representing and solving discrete optimization problems.  In valuation-based systems, we represent information in an optimization problem using variables, sample spaces of variables, a set of values, and functions that map sample spaces of sets of variables to the set of values.  The functions, called valuations, represent the factors of an objective function.  Solving the optimization problem involves using two operations called combination and marginalization. Combination tells us how to combine the factors of the joint objective function. Marginalization is either maximization or minimization.  Solving an optimization problem can be simply described as finding the marginal of the joint objective function for the empty set.  We state some simple axioms that combination and marginalization need to satisfy to enable us to solve an optimization problem using local computation. For optimization problems, the solution method of valuation-based systems reduces to non-serial dynamic programming.  Thus our solution method for VBS can be regarded as an abstract description of dynamic programming.  And our axioms can be viewed as conditions that permit the use of dynamic programming.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1122",
        "title": "Computational Aspects of the Mobius Transform",
        "authors": [
            "Robert Kennes",
            "Philippe Smets"
        ],
        "abstract": "In this paper we associate with every (directed) graph G a transformation called the Mobius transformation of the graph G.  The Mobius transformation of the graph (O) is of major significance for Dempster-Shafer theory of evidence.  However, because it is computationally very heavy, the Mobius transformation together with Dempster's rule of combination is a major obstacle to the use of Dempster-Shafer theory for handling uncertainty in expert systems.  The major contribution of this paper is the discovery of the 'fast Mobius transformations' of (O).  These 'fast Mobius transformations' are the fastest algorithms for computing the Mobius transformation of (O).  As an easy but useful application, we provide, via the commonality function, an algorithm for computing Dempster's rule of combination which is much faster than the usual one.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1123",
        "title": "Using Dempster-Shafer Theory in Knowledge Representation",
        "authors": [
            "Alessandro Saffiotti"
        ],
        "abstract": "In this paper, we suggest marrying Dempster-Shafer (DS) theory with Knowledge Representation (KR).  Born out of this marriage is the definition of \"Dempster-Shafer Belief Bases\", abstract data types representing uncertain knowledge that use DS theory for representing strength of belief about our knowledge, and the linguistic structures of an arbitrary KR system for representing the knowledge itself.  A formal result guarantees that both the properties of the given KR system and of DS theory are preserved.  The general model is exemplified by defining DS Belief Bases where First Order Logic and (an extension of) KRYPTON are used as KR systems.  The implementation problem is also touched upon.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1124",
        "title": "A Hierarchical Approach to Designing Approximate Reasoning-Based Controllers for Dynamic Physical Systems",
        "authors": [
            "Hamid R. Berenji",
            "Yung-Yaw Chen",
            "Chuen-Chien Lee",
            "Jyh-Shing Jang",
            "S. Murugesan"
        ],
        "abstract": "This paper presents a new technique for the design of approximate reasoning based controllers for dynamic physical systems with interacting goals.  In this approach, goals are achieved based on a hierarchy defined by a control knowledge base and remain highly interactive during the execution of the control task.  The approach has been implemented in a rule-based computer program which is used in conjunction with a prototype hardware system to solve the cart-pole balancing problem in real-time.  It provides a complementary approach to the conventional analytical control methodology, and is of substantial use where a precise mathematical model of the process being controlled is not available.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1125",
        "title": "Evidence Combination and Reasoning and Its Application to Real-World Problem-Solving",
        "authors": [
            "L. W. Chang",
            "Rangasami L. Kashyap"
        ],
        "abstract": "In this paper a new mathematical procedure is presented for combining different pieces of evidence which are represented in the interval form to reflect our knowledge about the truth of a hypothesis. Evidences may be correlated to each other (dependent evidences) or conflicting in supports (conflicting evidences). First, assuming independent evidences, we propose a methodology to construct combination rules which obey a set of essential properties. The method is based on a geometric model. We compare results obtained from Dempster-Shafer's rule and the proposed combination rules with both conflicting and non-conflicting data and show that the values generated by proposed combining rules are in tune with our intuition in both cases. Secondly, in the case that evidences are known to be dependent, we consider extensions of the rules derived for handling conflicting evidence. The performance of proposed rules are shown by different examples. The results show that the proposed rules reasonably make decision under dependent evidences\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1126",
        "title": "On Some Equivalence Relations between Incidence Calculus and Dempster-Shafer Theory of Evidence",
        "authors": [
            "F. Correa da Silva",
            "Alan Bundy"
        ],
        "abstract": "Incidence Calculus and Dempster-Shafer Theory of Evidence are both theories to describe agents' degrees of belief in propositions, thus being appropriate to represent uncertainty in reasoning systems. This paper presents a straightforward equivalence proof between some special cases of these theories.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1127",
        "title": "Using Belief Functions for Uncertainty Management and Knowledge Acquisition: An Expert Application",
        "authors": [
            "Mary McLeish",
            "P. Yao",
            "T. Stirtzinger"
        ],
        "abstract": "This paper describes recent work on an ongoing project in medical diagnosis at the University of Guelph. A domain on which experts are not very good at pinpointing a single disease outcome is explored. On-line medical data is available over a relatively short period of time. Belief Functions (Dempster-Shafer theory) are first extracted from data and then modified with expert opinions. Several methods for doing this are compared and results show that one formulation statistically outperforms the others, including a method suggested by Shafer. Expert opinions and statistically derived information about dependencies among symptoms are also compared. The benefits of using uncertainty management techniques as methods for knowledge acquisition from data are discussed.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1128",
        "title": "An Architecture for Probabilistic Concept-Based Information Retrieval",
        "authors": [
            "Robert Fung",
            "S. L. Crawford",
            "Lee A. Appelbaum",
            "Richard M. Tong"
        ],
        "abstract": "While concept-based methods for information retrieval can provide improved performance over more conventional techniques, they require large amounts of effort to acquire the concepts and their qualitative and quantitative relationships. This paper discusses an architecture for probabilistic concept-based information retrieval which addresses the knowledge acquisition problem. The architecture makes use of the probabilistic networks technology for representing and reasoning about concepts and includes a knowledge acquisition component which partially automates the construction of concept knowledge bases from data. We describe two experiments that apply the architecture to the task of retrieving documents about terrorism from a set of documents from the Reuters news service. The experiments provide positive evidence that the architecture design is feasible and that there are advantages to concept-based methods.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1129",
        "title": "Amplitude-Based Approach to Evidence Accumulation",
        "authors": [
            "A. J. Hanson"
        ],
        "abstract": "We point out the need to use probability amplitudes rather than probabilities to model evidence accumulation in decision processes involving real physical sensors. Optical information processing systems are given as typical examples of systems that naturally gather evidence in this manner. We derive a new, amplitude-based generalization of the Hough transform technique used for object recognition in machine vision. We argue that one should use complex Hough accumulators and square their magnitudes to get a proper probabilistic interpretation of the likelihood that an object is present. Finally, we suggest that probability amplitudes may have natural applications in connectionist models, as well as in formulating knowledge-based reasoning problems.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1130",
        "title": "A Probabilistic Reasoning Environment",
        "authors": [
            "Kathryn Blackmond Laskey"
        ],
        "abstract": "A framework is presented for a computational theory of probabilistic argument. The Probabilistic Reasoning Environment encodes knowledge at three levels. At the deepest level are a set of schemata encoding the system's domain knowledge. This knowledge is used to build a set of second-level arguments, which are structured for efficient recapture of the knowledge used to construct them. Finally, at the top level is a Bayesian network constructed from the arguments. The system is designed to facilitate not just propagation of beliefs and assimilation of evidence, but also the dynamic process of constructing a belief network, evaluating its adequacy, and revising it when necessary.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1131",
        "title": "On Non-monotonic Conditional Reasoning",
        "authors": [
            "Hung-Trung Nguyen"
        ],
        "abstract": "This note is concerned with a formal analysis of the problem of non-monotonic reasoning in intelligent systems, especially when the uncertainty is taken into account in a quantitative way. A firm connection between logic and probability is established by introducing conditioning notions by means of formal structures that do not rely on quantitative measures. The associated conditional logic, compatible with conditional probability evaluations, is non-monotonic relative to additional evidence. Computational aspects of conditional probability logic are mentioned. The importance of this development lies on its role to provide a conceptual basis for various forms of evidence combination and on its significance to unify multi-valued and non-monotonic logics\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1132",
        "title": "Decisions with Limited Observations over a Finite Product Space: the Klir Effect",
        "authors": [
            "Michael Pittarelli"
        ],
        "abstract": "Probability estimation by maximum entropy reconstruction of an initial relative frequency estimate from its projection onto a hypergraph model of the approximate conditional independence relations exhibited by it is investigated. The results of this study suggest that use of this estimation technique may improve the quality of decisions that must be made on the basis of limited observations over a decomposable finite product space.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1133",
        "title": "Fine-Grained Decision-Theoretic Search Control",
        "authors": [
            "Stuart Russell"
        ],
        "abstract": "Decision-theoretic control of search has previously used as its basic unit. of computation the generation and evaluation of a complete set of successors. Although this simplifies analysis, it results in some lost opportunities for pruning and satisficing. This paper therefore extends the analysis of the value of computation to cover individual successor evaluations. The analytic techniques used may prove useful for control of reasoning in more general settings. A formula is developed for the expected value of a node, k of whose n successors have been evaluated. This formula is used to estimate the value of expanding further successors, using a general formula for the value of a computation in game-playing developed in earlier work. We exhibit an improved version of the MGSS* algorithm, giving empirical results for the game of Othello.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1134",
        "title": "Rules, Belief Functions and Default Logic",
        "authors": [
            "Nic Wilson"
        ],
        "abstract": "This paper describes a natural framework for rules, based on belief functions, which includes a repre- sentation of numerical rules, default rules and rules allowing and rules not allowing contraposition. In particular it justifies the use of the Dempster-Shafer Theory for representing a particular class of rules, Belief calculated being a lower probability given certain independence assumptions on an underlying space. It shows how a belief function framework can be generalised to other logics, including a general Monte-Carlo algorithm for calculating belief, and how a version of Reiter's Default Logic can be seen as a limiting case of a belief function formalism.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1135",
        "title": "Combination of Evidence Using the Principle of Minimum Information Gain",
        "authors": [
            "Michael S. K. M. Wong",
            "P. Lingras"
        ],
        "abstract": "One of the most important aspects in any treatment of uncertain information is the rule of combination for updating the degrees of uncertainty. The theory of belief functions uses the Dempster rule to combine two belief functions defined by independent bodies of evidence. However, with limited dependency information about the accumulated belief the Dempster rule may lead to unsatisfactory results. The present study suggests a method to determine the accumulated belief based on the premise that the information gain from the combination process should be minimum. This method provides a mechanism that is equivalent to the Bayes rule when all the conditional probabilities are available and to the Dempster rule when the normalization constant is equal to one. The proposed principle of minimum information gain is shown to be equivalent to the maximum entropy formalism, a special case of the principle of minimum cross-entropy. The application of this principle results in a monotonic increase in belief with accumulation of consistent evidence. The suggested approach may provide a more reasonable criterion for identifying conflicts among various bodies of evidence.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1136",
        "title": "Probabilistic Evaluation of Candidates and Symptom Clustering for Multidisorder Diagnosis",
        "authors": [
            "Thomas D. Wu"
        ],
        "abstract": "This paper derives a formula for computing the conditional probability of a set of candidates, where a candidate is a set of disorders that explain a given set of positive findings.  Such candidate sets are produced by a recent method for multidisorder diagnosis called symptom clustering.  A symptom clustering represents a set of candidates compactly as a cartesian product of differential diagnoses.  By evaluating the probability of a candidate set, then, a large set of candidates can be validated or pruned simultaneously.  The probability of a candidate set is then specialized to obtain the probability of a single candidate.  Unlike earlier results, the equation derived here allows the specification of positive, negative, and unknown symptoms and does not make assumptions about disorders not in the candidate.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1137",
        "title": "Extending Term Subsumption systems for Uncertainty Management",
        "authors": [
            "John Yen",
            "Piero P. Bonissone"
        ],
        "abstract": "A major difficulty in developing and maintaining very large knowledge bases originates from the variety of forms in which knowledge is made available to the KB builder. The objective of this research is to bring together two complementary knowledge representation schemes: term subsumption languages, which represent and reason about defining characteristics of concepts, and proximate reasoning models, which deal with uncertain knowledge and data in expert systems. Previous works in this area have primarily focused on probabilistic inheritance. In this paper, we address two other important issues regarding the integration of term subsumption-based systems and approximate reasoning models. First, we outline a general architecture that specifies the interactions between the deductive reasoner of a term subsumption system and an approximate reasoner. Second, we generalize the semantics of terminological language so that terminological knowledge can be used to make plausible inferences. The architecture, combined with the generalized semantics, forms the foundation of a synergistic tight integration of term subsumption systems and approximate reasoning models.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1138",
        "title": "Refinement and Coarsening of Bayesian Networks",
        "authors": [
            "Kuo-Chu Chang",
            "Robert Fung"
        ],
        "abstract": "In almost all situation assessment problems, it is useful to dynamically contract and expand the states under consideration as assessment proceeds.  Contraction is most often used to combine similar events or low probability events together in order to reduce computation.  Expansion is most often used to make distinctions of interest which have significant probability in order to improve the quality of the assessment.  Although other uncertainty calculi, notably Dempster-Shafer [Shafer, 1976], have addressed these operations, there has not yet been any approach of refining and coarsening state spaces for the Bayesian Network technology.  This paper presents two operations for refining and coarsening the state space in Bayesian Networks.  We also discuss their practical implications for knowledge acquisition.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1139",
        "title": "Second Order Probabilities for Uncertain and Conflicting Evidence",
        "authors": [
            "Gerhard Paa\u00df"
        ],
        "abstract": "In this paper the elicitation of probabilities from human experts is considered as a measurement process, which may be disturbed by random 'measurement noise'.  Using Bayesian concepts a second order probability distribution is derived reflecting the uncertainty of the input probabilities.  The algorithm is based on an approximate sample representation of the basic probabilities.  This sample is continuously modified by a stochastic simulation procedure, the Metropolis algorithm, such that the sequence of successive samples corresponds to the desired posterior distribution.  The procedure is able to combine inconsistent probabilities according to their reliability and is applicable to general inference networks with arbitrary structure.  Dempster-Shafer probability mass functions may be included using specific measurement distributions.  The properties of the approach are demonstrated by numerical experiments.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1140",
        "title": "Computing Probability Intervals Under Independency Constraints",
        "authors": [
            "Linda C. van der Gaag"
        ],
        "abstract": "Many AI researchers argue that probability theory is only capable of dealing with uncertainty in situations where a full specification of a joint probability distribution is available, and conclude that it is not suitable for application in knowledge-based systems. Probability intervals, however, constitute a means for expressing incompleteness of information.  We present a method for computing such probability intervals for probabilities of interest from a partial specification of a joint probability distribution.  Our method improves on earlier approaches by allowing for independency relationships between statistical variables to be exploited.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1141",
        "title": "An Empirical Analysis of Likelihood-Weighting Simulation on a Large, Multiply-Connected Belief Network",
        "authors": [
            "Michael Shwe",
            "Gregory F. Cooper"
        ],
        "abstract": "We analyzed the convergence properties of likelihood- weighting algorithms on a two-level, multiply connected, belief-network representation of the QMR knowledge base of internal medicine. Specifically, on two difficult diagnostic cases, we examined the effects of Markov blanket scoring, importance sampling, demonstrating that the Markov blanket scoring and self-importance sampling significantly improve the convergence of the simulation on our model.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1142",
        "title": "Towards a Normative Theory of Scientific Evidence",
        "authors": [
            "David Sher"
        ],
        "abstract": "A scientific reasoning system makes decisions using objective evidence in the form of independent experimental trials, propositional axioms, and constraints on the probabilities of events. As a first step towards this goal, we propose a system that derives probability intervals from objective evidence in those forms. Our reasoning system can manage uncertainty about data and rules in a rule based expert system. We expect that our system will be particularly applicable to diagnosis and analysis in domains with a wealth of experimental evidence such as medicine. We discuss limitations of this solution and propose future directions for this research. This work can be considered a generalization of Nilsson's \"probabilistic logic\" [Nil86] to intervals and experimental observations.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1143",
        "title": "A Model for Non-Monotonic Reasoning Using Dempster's Rule",
        "authors": [
            "Mary McLeish"
        ],
        "abstract": "Considerable attention has been given to the problem of non-monotonic reasoning in a belief function framework.  Earlier work (M. Ginsberg) proposed solutions introducing meta-rules which recognized conditional independencies in a probabilistic sense.  More recently an e-calculus formulation of default reasoning (J. Pearl) shows that the application of Dempster's rule to a non-monotonic situation produces erroneous results. This paper presents a new belief function interpretation of the problem which combines the rules in a way which is more compatible with probabilistic results and respects conditions of independence necessary for the application of Dempster's combination rule. A new general framework for combining conflicting evidence is also proposed in which the normalization factor becomes modified.  This produces more intuitively acceptable results.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1144",
        "title": "Default Reasoning and the Transferable Belief Model",
        "authors": [
            "Philippe Smets",
            "Yen-Teh Hsia"
        ],
        "abstract": "Inappropriate use of Dempster's rule of combination has led some authors to reject the Dempster-Shafer model, arguing that it leads to supposedly unacceptable conclusions when defaults are involved.  A most classic example is about the penguin Tweety.  This paper will successively present: the origin of the miss-management of the Tweety example; two types of default; the correct solution for both types based on the transferable belief model (our interpretation of the Dempster-Shafer model (Shafer 1976, Smets 1988)); Except when explicitly stated, all belief functions used in this paper are simple support functions, i.e. belief functions for which only one proposition (the focus) of the frame of discernment receives a positive basic belief mass with the remaining mass being given to the tautology.  Each belief function will be described by its focus and the weight of the focus (e.g. m(A)=.9).  Computation of the basic belief masses are always performed by vacuously extending each belief function to the product space built from all variables involved, combining them on that space by Dempster's rule of combination, and projecting the result to the space corresponding to each individual variable.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1145",
        "title": "Separable and transitive graphoids",
        "authors": [
            "Dan Geiger",
            "David Heckerman"
        ],
        "abstract": "We examine three probabilistic formulations of the sentence a and b are totally unrelated with respect to a given set of variables U. First, two variables a and b are totally independent if they are independent given any value of any subset of the variables in U. Second, two variables are totally uncoupled if U can be partitioned into two marginally independent sets containing a and b respectively. Third, two variables are totally disconnected if the corresponding nodes are disconnected in every belief network representation. We explore the relationship between these three formulations of unrelatedness and explain their relevance to the process of acquiring probabilistic knowledge from human experts.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1146",
        "title": "Analysis in HUGIN of Data Conflict",
        "authors": [
            "Bo Chamberlain",
            "Finn Verner Jensen",
            "Frank Jensen",
            "Torsten Nordahl"
        ],
        "abstract": "After a brief introduction to causal probabilistic networks and the HUGIN approach, the problem of conflicting data is discussed.  A measure of conflict is defined, and it is used in the medical diagnostic system MUNIN.  Finally, it is discussed how to distinguish between conflicting data and a rare case.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1402",
        "title": "Computing Datalog Rewritings beyond Horn Ontologies",
        "authors": [
            "Bernardo Cuenca Grau",
            "Boris Motik",
            "Giorgos Stoilos",
            "Ian Horrocks"
        ],
        "abstract": "Rewriting-based approaches for answering queries over an OWL 2 DL ontology have so far been developed mainly for Horn fragments of OWL 2 DL. In this paper, we study the possibilities of answering queries over non-Horn ontologies using datalog rewritings. We prove that this is impossible in general even for very simple ontology languages, and even if PTIME = NP. Furthermore, we present a resolution-based procedure for $\\SHI$ ontologies that, in case it terminates, produces a datalog rewriting of the ontology. Our procedure necessarily terminates on DL-Lite_{bool}^H ontologies---an extension of OWL 2 QL with transitive roles and Boolean connectives.\n    ",
        "submission_date": "2013-04-04T00:00:00",
        "last_modified_date": "2013-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1491",
        "title": "Lp : A Logic for Statistical Information",
        "authors": [
            "Fahiem Bacchus"
        ],
        "abstract": "This extended abstract presents a logic, called Lp, that is capable of representing and reasoning with a wide variety of both qualitative and quantitative statistical information. The advantage of this logical formalism is that it offers a declarative representation of statistical knowledge; knowledge represented in this manner can be used for a variety of reasoning tasks.  The logic differs from previous work in probability logics in that it uses a probability distribution over the domain of discourse, whereas most previous work (e.g., Nilsson [2], Scott et al. [3], Gaifinan [4], Fagin et al. [5]) has investigated the attachment of probabilities to the sentences of the logic (also, see Halpern [6] and Bacchus [7] for further discussion of the differences).  The logic Lp possesses some further important features.  First, Lp is a superset of first order logic, hence it can represent ordinary logical assertions.  This means that Lp provides a mechanism for integrating statistical information and reasoning about uncertainty into systems based solely on logic.  Second, Lp possesses transparent semantics, based on sets and probabilities of those sets.  Hence, knowledge represented in Lp can be understood in terms of the simple primative concepts of sets and probabilities.  And finally, the there is a sound proof theory that has wide coverage (the proof theory is complete for certain classes of models).  The proof theory captures a sufficient range of valid inferences to subsume most previous probabilistic uncertainty reasoning systems.  For example, the linear constraints like those generated by Nilsson's probabilistic entailment [2] can be generated by the proof theory, and the Bayesian inference underlying belief nets [8] can be performed. In addition, the proof theory integrates quantitative and qualitative reasoning as well as statistical and logical reasoning.  In the next section we briefly examine previous work in probability logics, comparing it to Lp.  Then we present some of the varieties of statistical information that Lp is capable of expressing.  After this we present, briefly, the syntax, semantics, and proof theory of the logic.  We conclude with a few examples of knowledge representation and reasoning in Lp, pointing out the advantages of the declarative representation offered by Lp.  We close with a brief discussion of probabilities as degrees of belief, indicating how such probabilities can be generated from statistical knowledge encoded in Lp.  The reader who is interested in a more complete treatment should consult Bacchus [7].\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1492",
        "title": "Map Learning with Indistinguishable Locations",
        "authors": [
            "Kenneth Basye",
            "Thomas L. Dean"
        ],
        "abstract": "Nearly all spatial reasoning problems involve uncertainty of one sort or another. Uncertainty arises due to the inaccuracies of sensors used in measuring distances and angles.  We refer to this as directional uncertainty.  Uncertainty also arises in combining spatial information when one location is mistakenly identified with another.  We refer to this as recognition uncertainty.  Most problems in constructing spatial representations (maps) for the purpose of navigation involve both directional and recognition uncertainty. In this paper, we show that a particular class of spatial reasoning problems involving the construction of representations of large-scale space can be solved efficiently even in the presence of directional and recognition uncertainty.  We pay particular attention to the problems that arise due to recognition uncertainty.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1493",
        "title": "Temporal Reasoning with Probabilities",
        "authors": [
            "Carlo Berzuini",
            "Riccardo Bellazzi",
            "Silvana Quaglini"
        ],
        "abstract": "In this paper we explore representations of temporal knowledge based upon the formalism of Causal Probabilistic Networks (CPNs).  Two different ?continuous-time? representations are proposed.  In the first, the CPN includes variables representing ?event-occurrence times?, possibly on different time scales, and variables representing the ?state? of the system at these times.  In the second, the CPN describes the influences between random variables with values in () representing dates, i.e. time-points associated with the occurrence of relevant events.  However, structuring a system of inter-related dates as a network where all links commit to a single specific notion of cause and effect is in general far from trivial and leads to severe difficulties.  We claim that we should recognize explicitly different kinds of relation between dates, such as ?cause?, ?inhibition?, ?competition?, etc., and propose a method whereby these relations are coherently embedded in a CPN using additional auxiliary nodes corresponding to \"instrumental\" variables.  Also discussed, though not covered in detail, is the topic concerning how the quantitative specifications to be inserted in a temporal CPN can be learned from specific data.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1494",
        "title": "Now that I Have a Good Theory of Uncertainty, What Else Do I Need?",
        "authors": [
            "Piero P. Bonissone"
        ],
        "abstract": "Rather than discussing the isolated merits of a nominative theory of uncertainty, this paper focuses on a class of problems, referred to as Dynamic Classification Problem (DCP), which requires the integration of many theories, including a prescriptive theory of uncertainty.  We start by analyzing the Dynamic Classification Problem and by defining its induced requirements on a supporting (plausible) reasoning system.  We provide a summary of the underlying theory (based on the semantics of many-valed logics) and illustrate the constraints imposed upon it to ensure the modularity and computational performance required by the applications.  We describe the technologies used for knowledge engineering (such as object-based simulator to exercise requirements, and development tools to build the Knowledge Base and functionally validate it).  We emphasize the difference between development environment and run-time system, describe the rule cross-compiler, and the real-time inference engine with meta-reasoning capabilities.  Finally, we illustrate how our proposed technology satisfies the pop's requirements and analyze some of the lessons reamed from its applications to situation assessment problems for Pilot's Associate and Submarine Commander Associate.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1495",
        "title": "Uncertainty and Incompleteness",
        "authors": [
            "Piero P. Bonissone",
            "David A. Cyrluk",
            "James W. Goodwin",
            "Jonathan Stillman"
        ],
        "abstract": "Two major difficulties in using default logics are their intractability and the problem of selecting among multiple extensions.  We propose an approach to these problems based on integrating nommonotonic reasoning with plausible reasoning based on triangular norms.  A previously proposed system for reasoning with uncertainty (RUM) performs uncertain monotonic inferences on an acyclic graph.  We have extended RUM to allow nommonotonic inferences and cycles within nonmonotonic rules.  By restricting the size and complexity of the nommonotonic cycles we can still perform efficient inferences. Uncertainty measures provide a basis for deciding among multiple defaults.  Different algorithms and heuristics for finding the optimal defaults are discussed.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1496",
        "title": "BaRT: A Bayesian Reasoning Tool for Knowledge Based Systems",
        "authors": [
            "Lashon B. Booker",
            "Naveen Hota",
            "Connie Loggia Ramsey"
        ],
        "abstract": "As the technology for building knowledge based systems has matured, important lessons have been learned about the relationship between the architecture of a system and the nature of the problems it is intended to solve.  We are implementing a knowledge engineering tool called BART that is designed with these lessons in mind.  BART is a Bayesian reasoning tool that makes belief networks and other probabilistic techniques available to knowledge engineers building classificatory problem solvers.  BART has already been used to develop a decision aid for classifying ship images, and it is currently being used to manage uncertainty in systems concerned with analyzing intelligence reports.  This paper discusses how state-of-the-art probabilistic methods fit naturally into a knowledge based approach to classificatory problem solving, and describes the current capabilities of BART.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1497",
        "title": "Plan Recognition in Stories and in Life",
        "authors": [
            "Eugene Charniak",
            "Robert P. Goldman"
        ],
        "abstract": "Plan recognition does not work the same way in stories and in \"real life\" (people tend to jump to conclusions more in stories).  We present a theory of this, for the particular case of how objects in stories (or in life) influence plan recognition decisions.  We provide a Bayesian network formalization of a simple first-order theory of plans, and show how a particular network parameter seems to govern the difference between \"life-like\" and \"story-like\" response.  We then show why this parameter would be influenced (in the desired way) by a model of speaker (or author) topic selection which assumes that facts in stories are typically \"relevant\".\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1498",
        "title": "An Empirical Evaluation of a Randomized Algorithm for Probabilistic Inference",
        "authors": [
            "R. Martin Chavez",
            "Gregory F. Cooper"
        ],
        "abstract": "In recent years, researchers in decision analysis and artificial intelligence (Al) have used Bayesian belief networks to build models of expert opinion.  Using standard methods drawn from the theory of computational complexity, workers in the field have shown that the problem of probabilistic inference in belief networks is difficult and almost certainly intractable.  K N ET, a software environment for constructing knowledge-based systems within the axiomatic framework of decision theory, contains a randomized approximation scheme for probabilistic inference.  The algorithm can, in many circumstances, perform efficient approximate inference in large and richly interconnected models of medical diagnosis.  Unlike previously described stochastic algorithms for probabilistic inference, the randomized approximation scheme computes a priori bounds on running time by analyzing the structure and contents of the belief network.  In this article, we describe a randomized algorithm for probabilistic inference and analyze its performance mathematically.  Then, we devote the major portion of the paper to a discussion of the algorithm's empirical behavior.  The results indicate that the generation of good trials (that is, trials whose distribution closely matches the true distribution), rather than the computation of numerous mediocre trials, dominates the performance of stochastic simulation.  Key words: probabilistic inference, belief networks, stochastic simulation, computational complexity theory, randomized algorithms.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1499",
        "title": "Decision Making \"Biases\" and Support for Assumption-Based Higher-Order Reasoning",
        "authors": [
            "Marvin S. Cohen"
        ],
        "abstract": "Unaided human decision making appears to systematically violate consistency constraints imposed by normative theories; these biases in turn appear to justify the application of formal decision-analytic models. It is argued that both claims are wrong. In particular, we will argue that the \"confirmation bias\" is premised on an overly narrow view of how conflicting evidence is and ought to be handled. Effective decision aiding should focus on supporting the contral processes by means of which knowledge is extended into novel situations and in which assumptions are adopted, utilized, and revised. The Non- Monotonic Probabilist represents initial work toward such an aid.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1500",
        "title": "Automated Reasoning Using Possibilistic Logic: Semantics, Belief Revision and Variable Certainty Weights",
        "authors": [
            "Didier Dubois",
            "Jerome Lang",
            "Henri Prade"
        ],
        "abstract": "In this paper an approach to automated deduction under uncertainty,based on possibilistic logic, is proposed ; for that purpose we deal with clauses weighted by a degree which is a lower bound of a necessity or a possibility measure, according to the nature of the uncertainty. Two resolution rules are used for coping with the different situations, and the refutation method can be generalized. Besides the lower bounds are allowed to be functions of variables involved in the clause, which gives hypothetical reasoning capabilities. The relation between our approach and the idea of minimizing abnormality is briefly discussed. In case where only lower bounds of necessity measures are involved, a semantics is proposed, in which the completeness of the extended resolution principle is proved. Moreover deduction from a partially inconsistent knowledge base can be managed in this approach and displays some form of non-monotonicity.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1501",
        "title": "How Much More Probable is \"Much More Probable\"? Verbal Expressions for Probability Updates",
        "authors": [
            "Christopher Elsaesser",
            "Max Henrion"
        ],
        "abstract": "Bayesian inference systems should be able to explain their reasoning to users, translating from numerical to natural language.  Previous empirical work has investigated the correspondence between absolute probabilities and linguistic phrases.  This study extends that work to the correspondence between changes in probabilities (updates) and relative probability phrases, such as \"much more likely\" or \"a little less likely.\"  Subjects selected such phrases to best describe numerical probability updates.  We examined three hypotheses about the correspondence, and found the most descriptively accurate of these three to be that each such phrase corresponds to a fixed difference in probability (rather than fixed ratio of probabilities or of odds).  The empirically derived phrase selection function uses eight phrases and achieved a 72% accuracy in correspondence with the subjects' actual usage.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1502",
        "title": "Positive and Negative Explanations of Uncertain Reasoning in the Framework of Possibility Theory",
        "authors": [
            "Henri Farrency",
            "Henri Prade"
        ],
        "abstract": "This paper presents an approach for developing the explanation capabilities of rule-based expert systems managing imprecise and uncertain knowledge. The treatment of uncertainty takes place in the framework of possibility theory where the available information concerning the value of a logical or numerical variable is represented by a possibility distribution which restricts its more or less possible values. We first discuss different kinds of queries asking for explanations before focusing on the two following types : i) how, a particular possibility distribution is obtained (emphasizing the main reasons only) ; ii) why in a computed possibility distribution, a particular value has received a possibility degree which is so high, so low or so contrary to the expectation. The approach is based on the exploitation of equations in max-min algebra. This formalism includes the limit case of certain and precise information.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1503",
        "title": "Interval Influence Diagrams",
        "authors": [
            "Kenneth W. Fertig",
            "John S. Breese"
        ],
        "abstract": "We describe a mechanism for performing probabilistic reasoning in influence diagrams using interval rather than point valued probabilities.  We derive the procedures for node removal (corresponding to conditional expectation) and arc reversal (corresponding to Bayesian conditioning) in influence diagrams where lower bounds on probabilities are stored at each node.  The resulting bounds for the transformed diagram are shown to be optimal within the class of constraints on probability distributions that can be expressed exclusively as lower bounds on the component probabilities of the diagram.  Sequences of these operations can be performed to answer probabilistic queries with indeterminacies in the input and for performing sensitivity analysis on an influence diagram.  The storage requirements and computational complexity of this approach are comparable to those for point-valued probabilistic inference mechanisms, making the approach attractive for performing sensitivity analysis and where probability information is not available. Limited empirical data on an implementation of the methodology are provided.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1504",
        "title": "Weighing and Integrating Evidence for Stochastic Simulation in Bayesian Networks",
        "authors": [
            "Robert Fung",
            "Kuo-Chu Chang"
        ],
        "abstract": "Stochastic simulation approaches perform probabilistic inference in Bayesian networks by estimating the probability of an event based on the frequency that the event occurs in a set of simulation trials.  This paper describes the evidence weighting mechanism, for augmenting the logic sampling stochastic simulation algorithm [Henrion, 1986]. Evidence weighting modifies the logic sampling algorithm by weighting each simulation trial by the likelihood of a network's evidence given the sampled state node values for that trial.  We also describe an enhancement to the basic algorithm which uses the evidential integration technique [Chin and Cooper, 1987].  A comparison of the basic evidence weighting mechanism with the Markov blanket algorithm [Pearl, 1987], the logic sampling algorithm, and the evidence integration algorithm is presented.  The comparison is aided by analyzing the performance of the algorithms in a simple example network.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1505",
        "title": "d-Separation: From Theorems to Algorithms",
        "authors": [
            "Dan Geiger",
            "Tom S. Verma",
            "Judea Pearl"
        ],
        "abstract": "An efficient algorithm is developed that identifies all independencies implied by the topology of a Bayesian network.  Its correctness and maximality stems from the soundness and completeness of d-separation with respect to probability theory.  The algorithm runs in time O (l E l) where E is the number of edges in the network.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1506",
        "title": "The Effects of Perfect and Sample Information on Fuzzy Utilities in Decision-Making",
        "authors": [
            "Maria Angeles Gil",
            "Pramod Jain"
        ],
        "abstract": "In this paper, we first consider a Bayesian framework and model the \"utility function\" in terms of fuzzy random variables. On the basis of this model, we define the \"prior (fuzzy) expected utility\" associated with each action, and the corresponding \"posterior (fuzzy) expected utility given sample information from a random experiment\". The aim of this paper is to analyze how sample information can affect the expected utility. In this way, by using some fuzzy preference relations, we conclude that sample information allows a decision maker to increase the expected utility on the average. The upper bound on the value of the expected utility is when the decision maker has perfect information. Applications of this work to the field of artificial intelligence are presented through two examples.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1507",
        "title": "Deciding Consistency of Databases Containing Defeasible and Strict Information",
        "authors": [
            "Moises Goldszmidt",
            "Judea Pearl"
        ],
        "abstract": "We propose a norm of consistency for a mixed set of defeasible and strict sentences, based on a probabilistic semantics.  This norm establishes a clear distinction between knowledge bases depicting exceptions and those containing outright contradictions.  We then define a notion of entailment based also on probabilistic considerations and provide a characterization of the relation between consistency and entailment. We derive necessary and sufficient conditions for consistency, and provide a simple decision procedure for testing consistency and deciding whether a sentence is entailed by a database.  Finally, it is shown that if al1 sentences are Horn clauses, consistency and entailment can be tested in polynomial time. \n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1508",
        "title": "The Relationship between Knowledge, Belief and Certainty",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "abstract": "We consider the relation between knowledge and certainty, where a fact is known if it is true at all worlds an agent considers possible and is certain if it holds with probability 1. We identify certainty with probabilistic belief. We show that if we assume one fixed probability assignment, then the logic KD45, which has been identified as perhaps the most appropriate for belief, provides a complete axiomatization for reasoning about certainty. Just as an agent may believe a fact although phi is false, he may be certain that a fact phi, is true although phi is false. However, it is easy to see that an agent can have such false (probabilistic) beliefs only at a set of worlds of probability 0. If we restrict attention to structures where all worlds have positive probability, then S5 provides a complete axiomatization. If we consider a more general setting, where there might be a different probability assignment at each world, then by placing appropriate conditions on the support of the probability function (the set of worlds which have non-zero probability), we can capture many other well-known modal logics, such as T and S4. Finally, we consider which axioms characterize structures satisfying Miller's principle.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1509",
        "title": "Heuristic Search as Evidential Reasoning",
        "authors": [
            "Othar Hansson",
            "Andy Mayer"
        ],
        "abstract": "BPS, the Bayesian Problem Solver, applies probabilistic inference and decision-theoretic control to flexible, resource-constrained problem-solving. This paper focuses on the Bayesian inference mechanism in BPS, and contrasts it with those of traditional heuristic search techniques. By performing sound inference, BPS can outperform traditional techniques with significantly less computational effort. Empirical tests on the Eight Puzzle show that after only a few hundred node expansions, BPS makes better decisions than does the best existing algorithm after several million node expansions\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1510",
        "title": "The Compilation of Decision Models",
        "authors": [
            "David Heckerman",
            "John S. Breese",
            "Eric J. Horvitz"
        ],
        "abstract": "We introduce and analyze the problem of the compilation of decision models from a decision-theoretic perspective. The techniques described allow us to evaluate various configurations of compiled knowledge given the nature of evidential relationships in a domain, the utilities associated with alternative actions, the costs of run-time delays, and the costs of memory. We describe procedures for selecting a subset of the total observations available to be incorporated into a compiled situation-action mapping, in the context of a binary decision with conditional independence of evidence. The methods allow us to incrementally select the best pieces of evidence to add to the set of compiled knowledge in an engineering setting. After presenting several approaches to compilation, we exercise one of the methods to provide insight into the relationship between the distribution over weights of evidence and the preferred degree of compilation.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1511",
        "title": "A Tractable Inference Algorithm for Diagnosing Multiple Diseases",
        "authors": [
            "David Heckerman"
        ],
        "abstract": "We examine a probabilistic model for the diagnosis of multiple diseases. In the model, diseases and findings are represented as binary variables. Also, diseases are marginally independent, features are conditionally independent given disease instances, and diseases interact to produce findings via a noisy OR-gate. An algorithm for computing the posterior probability of each disease, given a set of observed findings, called quickscore, is presented. The time complexity of the algorithm is O(nm-2m+), where n is the number of diseases, m+ is the number of positive findings and m- is the number of negative findings. Although the time complexity of quickscore i5 exponential in the number of positive findings, the algorithm is useful in practice because the number of observed positive findings is usually far less than the number of diseases under consideration. Performance results for quickscore applied to a probabilistic version of Quick Medical Reference (QMR) are provided.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2022-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1512",
        "title": "Bounded Conditioning: Flexible Inference for Decisions under Scarce Resources",
        "authors": [
            "Eric J. Horvitz",
            "Jaap Suermondt",
            "Gregory F. Cooper"
        ],
        "abstract": "We introduce a graceful approach to probabilistic inference called bounded conditioning. Bounded conditioning monotonically refines the bounds on posterior probabilities in a belief network with computation, and converges on final probabilities of interest with the allocation of a complete resource fraction. The approach allows a reasoner to exchange arbitrary quantities of computational resource for incremental gains in inference quality. As such, bounded conditioning holds promise as a useful inference technique for reasoning under the general conditions of uncertain and varying reasoning resources. The algorithm solves a probabilistic bounding problem in complex belief networks by breaking the problem into a set of mutually exclusive, tractable subproblems and ordering their solution by the expected effect that each subproblem will have on the final answer. We introduce the algorithm, discuss its characterization, and present its performance on several belief networks, including a complex model for reasoning about problems in intensive-care medicine.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1513",
        "title": "Hierarchical Evidence Accumulation in the Pseiki System and Experiments in Model-Driven Mobile Robot Navigation",
        "authors": [
            "A. C. Kak",
            "K. M. Andress",
            "C. Lopez-Abadia",
            "M. S. Carroll",
            "J. R. Lewis"
        ],
        "abstract": "In this paper, we will review the process of evidence accumulation in the PSEIKI system for expectation-driven interpretation of images of 3-D scenes.  Expectations are presented to PSEIKI as a geometrical hierarchy of abstractions.  PSEIKI's job is then to construct abstraction hierarchies in the perceived image taking cues from the abstraction hierarchies in the expectations.  The Dempster-Shafer formalism is used for associating belief values with the different possible labels for the constructed abstractions in the perceived image.  This system has been used successfully for autonomous navigation of a mobile robot in indoor environments.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1514",
        "title": "A Decision-Theoretic Model for Using Scientific Data",
        "authors": [
            "Harold P. Lehmann"
        ],
        "abstract": "Many Artificial Intelligence systems depend on the agent's updating its beliefs about the world on the basis of experience.  Experiments constitute one type of experience, so scientific methodology offers a natural environment for examining the issues attendant to using this class of evidence.  This paper presents a framework which structures the process of using scientific data from research reports for the purpose of making decisions, using decision analysis as the basis for the structure and using medical research as the general scientific domain.  The structure extends the basic influence diagram for updating belief in an object domain parameter of interest by expanding the parameter into four parts: those of the patient, the population, the study sample, and the effective study sample.  The structure uses biases to perform the transformation of one parameter into another, so that, for instance, selection biases, in concert with the population parameter, yield the study sample parameter.  The influence diagram structure provides decision theoretic justification for practices of good clinical research such as randomized assignment and blindfolding of care providers.  The model covers most research designs used in medicine: case-control studies, cohort studies, and controlled clinical trials, and provides an architecture to separate clearly between statistical knowledge and domain knowledge.  The proposed general model can be the basis for clinical epidemiological advisory systems, when coupled with heuristic pruning of irrelevant biases; of statistical workstations, when the computational machinery for calculation of posterior distributions is added; and of meta-analytic reviews, when multiple studies may impact on a single population parameter.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1515",
        "title": "When Should a Decision Maker Ignore the Advice of a Decision Aid?",
        "authors": [
            "Paul E. Lehner",
            "Theresa M. Mullin",
            "Marvin S. Cohen"
        ],
        "abstract": "This paper argues that the principal difference between decision aids and most other types of information systems is the greater reliance of decision aids on fallible algorithms--algorithms that sometimes generate incorrect advice.  It is shown that interactive problem solving with a decision aid that is based on a fallible algorithm can easily result in aided performance which is poorer than unaided performance, even if the algorithm, by itself, performs significantly better than the unaided decision maker.  This suggests that unless certain conditions are satisfied, using a decision aid as an aid is counterproductive.  Some conditions under which a decision aid is best used as an aid are derived.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1516",
        "title": "Inference Policies",
        "authors": [
            "Paul E. Lehner"
        ],
        "abstract": "It is suggested that an AI inference system should reflect an inference policy that is tailored to the domain of problems to which it is applied -- and furthermore that an inference policy need not conform to any general theory of rational inference or induction.  We note, for instance, that Bayesian reasoning about the probabilistic characteristics of an inference domain may result in the specification of an nonBayesian procedure for reasoning within the inference domain.  In this paper, the idea of an inference policy is explored in some detail.  To support this exploration, the characteristics of some standard and nonstandard inference policies are examined.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1518",
        "title": "Defeasible Decisions: What the Proposal is and isn't",
        "authors": [
            "Ronald P. Loui"
        ],
        "abstract": "In two recent papers, I have proposed a description of decision analysis that differs from the Bayesian picture painted by Savage, Jeffrey and other classic authors.  Response to this view has been either overly enthusiastic or unduly pessimistic.  In this paper I try to place the idea in its proper place, which must be somewhere in between. Looking at decision analysis as defeasible reasoning produces a framework in which planning and decision theory can be integrated, but work on the details has barely begun. It also produces a framework in which the meta-decision regress can be stopped in a reasonable way, but it does not allow us to ignore meta-level decisions.  The heuristics for producing arguments that I have presented are only supposed to be suggestive; but they are not open to the egregious errors about which some have worried.  And though the idea is familiar to those who have studied heuristic search, it is somewhat richer because the control of dialectic is more interesting than the deepening of search.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1519",
        "title": "Experiments Using Belief Functions and Weights of Evidence incorporating Statistical Data and Expert Opinions",
        "authors": [
            "Mary McLeish",
            "P. Yao",
            "M. Cecile",
            "T. Stirtzinger"
        ],
        "abstract": "This paper presents some ideas and results of using uncertainty management methods in the presence of data in preference to other statistical and machine learning methods. A medical domain is used as a test-bed with data available from a large hospital database system which collects symptom and outcome information about patients. Data is often missing, of many variable types and sample sizes for particular outcomes is not large. Uncertainty management methods are useful for such domains and have the added advantage of allowing for expert modification of belief values originally obtained from data. Methodological considerations for using belief functions on statistical data are dealt with in some detail. Expert opinions are Incorporated at various levels of the project development and results are reported on an application to liver disease diagnosis. Recent results contrasting the use of weights of evidence and logistic regression on another medical domain are also presented.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1520",
        "title": "Shootout-89: A Comparative Evaluation of Knowledge-based Systems that Forecast Severe Weather",
        "authors": [
            "W. R. Moninger",
            "J. A. Flueck",
            "C. Lusk",
            "W. F. Roberts"
        ],
        "abstract": "During the summer of 1989, the Forecast Systems Laboratory of the National Oceanic and Atmospheric Administration sponsored an evaluation of artificial intelligence-based systems that forecast severe convective storms.  The evaluation experiment, called Shootout-89, took place in Boulder, and focussed on storms over the northeastern Colorado foothills and plains (Moninger, et al., 1990).  Six systems participated in Shootout-89.  These included traditional expert systems, an analogy-based system, and a system developed using methods from the cognitive science/judgment analysis tradition. Each day of the exercise, the systems generated 2 to 9 hour forecasts of the probabilities of occurrence of: non significant weather, significant weather, and severe weather, in each of four regions in northeastern Colorado.  A verification coordinator working at the Denver Weather Service Forecast Office gathered ground-truth data from a network of observers.  Systems were evaluated on the basis of several measures of forecast skill, and on other metrics such as timeliness, ease of learning, and ease of use.  Systems were generally easy to operate, however the various systems required substantially different levels of meteorological expertise on the part of their users--reflecting the various operational environments for which the systems had been designed.  Systems varied in their statistical behavior, but on this difficult forecast problem, the systems generally showed a skill approximately equal to that of persistence forecasts and climatological (historical frequency) forecasts.  The two systems that appeared best able to discriminate significant from non significant weather events were traditional expert systems.  Both of these systems required the operator to make relatively sophisticated meteorological judgments.  We are unable, based on only one summer's worth of data, to determine the extent to which the greater skill of the two systems was due to the content of their knowledge bases, or to the subjective judgments of the operator.  A follow-on experiment, Shootout-91, is currently being planned.  Interested potential participants are encouraged to contact the author at the address above.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1521",
        "title": "Conditioning on Disjunctive Knowledge: Defaults and Probabilities",
        "authors": [
            "Eric Neufeld",
            "J. D. Horton"
        ],
        "abstract": "Many writers have observed that default logics appear to contain the \"lottery paradox\" of probability theory.  This arises when a default \"proof by contradiction\" lets us conclude that a typical X is not a Y where Y is an unusual subclass of X. We show that there is a similar problem with default \"proof by cases\" and construct a setting where we might draw a different conclusion knowing a disjunction than we would knowing any particular disjunct.  Though Reiter's original formalism is capable of representing this distinction, other approaches are not.  To represent and reason about this case, default logicians must specify how a \"typical\" individual is selected. The problem is closely related to Simpson's paradox of probability theory.  If we accept a simple probabilistic account of defaults based on the notion that one proposition may favour or increase belief in another, the \"multiple extension problem\" for both conjunctive and disjunctive knowledge vanishes. \n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1522",
        "title": "Maximum Uncertainty Procedures for Interval-Valued Probability Distributions",
        "authors": [
            "Michael Pittarelli"
        ],
        "abstract": "Measures of uncertainty and divergence are introduced for interval-valued probability distributions and are shown to have desirable mathematical properties. A maximum uncertainty inference procedure for marginal interval distributions is presented. A technique for reconstruction of interval distributions from projections is developed based on this inference procedure\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1523",
        "title": "A Logical Interpretation of Dempster-Shafer Theory, with Application to Visual Recognition",
        "authors": [
            "Gregory M. Provan"
        ],
        "abstract": "We formulate Dempster Shafer Belief functions in terms of Propositional Logic using the implicit notion of provability underlying Dempster Shafer Theory.  Given a set of propositional clauses, assigning weights to certain propositional literals enables the Belief functions to be explicitly computed using Network Reliability techniques.  Also, the logical procedure corresponding to updating Belief functions using Dempster's Rule of Combination is shown.  This analysis formalizes the implementation of Belief functions within an Assumption-based Truth Maintenance System (ATMS).  We describe the extension of an ATMS-based visual recognition system, VICTORS, with this logical formulation of Dempster Shafer theory.  Without Dempster Shafer theory, VICTORS computes all possible visual interpretations (i.e. all logical models) without determining the best interpretation(s).  Incorporating Dempster Shafer theory enables optimal visual interpretations to be computed and a logical semantics to be maintained.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1524",
        "title": "Strategies for Generating Micro Explanations for Bayesian Belief Networks",
        "authors": [
            "Peter Sember",
            "Ingrid Zukerman"
        ],
        "abstract": "Bayesian Belief Networks have been largely overlooked by Expert Systems practitioners on the grounds that they do not correspond to the human inference mechanism. In this paper, we introduce an explanation mechanism designed to generate intuitive yet probabilistically sound explanations of inferences drawn by a Bayesian Belief Network. In particular, our mechanism accounts for the results obtained due to changes in the causal and the evidential support of a node.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1525",
        "title": "Evidence Absorption and Propagation through Evidence Reversals",
        "authors": [
            "Ross D. Shachter"
        ],
        "abstract": "The arc reversal/node reduction approach to probabilistic inference is extended to include the case of instantiated evidence by an operation called \"evidence reversal.\"  This not only provides a technique for computing posterior joint distributions on general belief networks, but also provides insight into the methods of Pearl [1986b] and Lauritzen and Spiegelhalter [1988].  Although it is well understood that the latter two algorithms are closely related, in fact all three algorithms are identical whenever the belief network is a forest.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1526",
        "title": "Simulation Approaches to General Probabilistic Inference on Belief Networks",
        "authors": [
            "Ross D. Shachter",
            "Mark Alan Peot"
        ],
        "abstract": "A number of algorithms have been developed to solve probabilistic inference problems on belief networks.  These algorithms can be divided into two main groups: exact techniques which exploit the conditional independence revealed when the graph structure is relatively sparse, and probabilistic sampling techniques which exploit the \"conductance\" of an embedded Markov chain when the conditional probabilities have non-extreme values.  In this paper, we investigate a family of \"forward\" Monte Carlo sampling techniques similar to Logic Sampling [Henrion, 1988] which appear to perform well even in some multiply connected networks with extreme conditional probabilities, and thus would be generally applicable.  We consider several enhancements which reduce the posterior variance using this approach and propose a framework and criteria for choosing when to use those enhancements.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1527",
        "title": "Decision under Uncertainty",
        "authors": [
            "Philippe Smets"
        ],
        "abstract": "We derive axiomatically the probability function that should be used to make decisions given any form of underlying uncertainty.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1528",
        "title": "Freedom: A Measure of Second-order Uncertainty for Intervalic Probability Schemes",
        "authors": [
            "Michael Smithson"
        ],
        "abstract": "This paper discusses a new measure that is adaptable to certain intervalic probability frameworks, possibility theory, and belief theory. As such, it has the potential for wide use in knowledge engineering, expert systems, and related problems in the human sciences. This measure (denoted here by F) has been introduced in Smithson (1988) and is more formally discussed in Smithson (1989a)o Here, I propose to outline the conceptual basis for F and compare its properties with other measures of second-order uncertainty. I will argue that F is an indicator of nonspecificity or alternatively, of freedom, as distinguished from either ambiguity or vagueness.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1529",
        "title": "Assessment, Criticism and Improvement of Imprecise Subjective Probabilities for a Medical Expert System",
        "authors": [
            "David J. Spiegelhalter",
            "Rodney C. Franklin",
            "Kate Bull"
        ],
        "abstract": "Three paediatric cardiologists assessed nearly 1000 imprecise subjective conditional probabilities for a simple belief network representing congenital heart disease, and the quality of the assessments has been measured using prospective data on 200 babies. Quality has been assessed by a Brier scoring rule, which decomposes into terms measuring lack of discrimination and reliability.  The results are displayed for each of 27 diseases and 24 questions, and generally the assessments are reliable although there was a tendency for the probabilities to be too extreme.  The imprecision allows the judgements to be converted to implicit samples, and by combining with the observed data the probabilities naturally adapt with experience.  This appears to be a practical procedure even for reasonably large expert systems.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1530",
        "title": "Automated Construction of Sparse Bayesian Networks from Unstructured Probabilistic Models and Domain Information",
        "authors": [
            "Sampath Srinivas",
            "Stuart Russell",
            "Alice M. Agogino"
        ],
        "abstract": "An algorithm for automated construction of a sparse Bayesian network given an unstructured probabilistic model and causal domain information from an expert has been developed and implemented.  The goal is to obtain a network that explicitly reveals as much information regarding conditional independence as possible.  The network is built incrementally adding one node at a time.  The expert's information and a greedy heuristic that tries to keep the number of arcs added at each step to a minimum are used to guide the search for the next node to add.  The probabilistic model is a predicate that can answer queries about independencies in the domain.  In practice the model can be implemented in various ways.  For example, the model could be a statistical independence test operating on empirical data or a deductive prover operating on a set of independence statements about the domain.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1531",
        "title": "Making Decisions with Belief Functions",
        "authors": [
            "Thomas M. Strat"
        ],
        "abstract": "A primary motivation for reasoning under uncertainty is to derive decisions in the face of inconclusive evidence. However, Shafer's theory of belief functions, which explicitly represents the underconstrained nature of many reasoning problems, lacks a formal procedure for making decisions. Clearly, when sufficient information is not available, no theory can prescribe actions without making additional assumptions. Faced with this situation, some assumption must be made if a clearly superior choice is to emerge. In this paper we offer a probabilistic interpretation of a simple assumption that disambiguates decision problems represented with belief functions. We prove that it yields expected values identical to those obtained by a probabilistic analysis that makes the same assumption. In addition, we show how the decision analysis methodology frequently employed in probabilistic reasoning can be extended for use with belief functions.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1532",
        "title": "Efficient Parallel Estimation for Markov Random Fields",
        "authors": [
            "Michael J. Swain",
            "Lambert E. Wixson",
            "Paul B. Chou"
        ],
        "abstract": "We present a new, deterministic, distributed MAP estimation algorithm for Markov Random Fields called Local Highest Confidence First (Local HCF).  The algorithm has been applied to segmentation problems in computer vision and its performance compared with stochastic algorithms.  The experiments show that Local HCF finds better estimates than stochastic algorithms with much less computation.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1533",
        "title": "Comparing Expert Systems Built Using Different Uncertain Inference Systems",
        "authors": [
            "David S. Vaughan",
            "Bruce M. Perrin",
            "Robert M. Yadrick"
        ],
        "abstract": "This study compares the inherent intuitiveness or usability of the most prominent methods for managing uncertainty in expert systems, including those of EMYCIN, PROSPECTOR, Dempster-Shafer theory, fuzzy set theory, simplified probability theory (assuming marginal independence), and linear regression using probability estimates. Participants in the study gained experience in a simple, hypothetical problem domain through a series of learning trials.  They were then randomly assigned to develop an expert system using one of the six Uncertain Inference Systems (UISs) listed above. Performance of the resulting systems was then compared.  The results indicate that the systems based on the PROSPECTOR and EMYCIN models were significantly less accurate for certain types of problems compared to systems based on the other UISs. Possible reasons for these differences are discussed.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1534",
        "title": "Directed Cycles in Belief Networks",
        "authors": [
            "Wilson X. Wen"
        ],
        "abstract": "The most difficult task in probabilistic reasoning may be handling directed cycles in belief networks. To the best knowledge of this author, there is no serious discussion of this problem at all in the literature of probabilistic reasoning so far.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1535",
        "title": "Can Uncertainty Management be Realized in a Finite Totally Ordered Probability Algebra?",
        "authors": [
            "Yang Xiang",
            "Michael P. Beddoes",
            "David L Poole"
        ],
        "abstract": "In this paper, the feasibility of using finite totally ordered probability models under Alelinnas's Theory of Probabilistic Logic [Aleliunas, 1988] is investigated.  The general form of the probability algebra of these models is derived and the number of possible algebras with given size is deduced.  Based on this analysis, we discuss problems of denominator-indifference and ambiguity-generation that arise in reasoning by cases and abductive reasoning.  An example is given that illustrates how these problems arise.  The investigation shows that a finite probability model may be of very limited usage.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1536",
        "title": "Normalization and the Representation of Nonmonotonic Knowledge in the Theory of Evidence",
        "authors": [
            "Ronald R. Yager"
        ],
        "abstract": "We discuss the Dempster-Shafer theory of evidence. We introduce a concept of monotonicity which is related to the diminution of the range between belief and plausibility. We show that the accumulation of knowledge in this framework exhibits a nonmonotonic property. We show how the belief structure can be used to represent typical or commonsense knowledge.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1628",
        "title": "Pattern-Based Constraint Satisfaction and Logic Puzzles",
        "authors": [
            "Denis Berthier"
        ],
        "abstract": "Pattern-Based Constraint Satisfaction and Logic Puzzles develops a pure logic, pattern-based perspective of solving the finite Constraint Satisfaction Problem (CSP), with emphasis on finding the \"simplest\" solution. Different ways of reasoning with the constraints are formalised by various families of \"resolution rules\", each of them carrying its own notion of simplicity. A large part of the book illustrates the power of the approach by applying it to various popular logic puzzles. It provides a unified view of how to model and solve them, even though they involve very different types of constraints: obvious symmetric ones in Sudoku, non-symmetric but transitive ones (inequalities) in Futoshiki, topological and geometric ones in Map colouring, Numbrix and Hidato, and even much more complex non-binary arithmetic ones in Kakuro (or Cross Sums). It also shows that the most familiar techniques for these puzzles can indeed be understood as mere application-specific presentations of the general rules. Sudoku is used as the main example throughout the book, making it also an advanced level sequel to \"The Hidden Logic of Sudoku\" (another book by the same author), with: many examples of relationships among different rules and of exceptional situations; comparisons of the resolution potential of various families of rules; detailed statistics of puzzles hardness; analysis of extreme instances.\n    ",
        "submission_date": "2013-04-05T00:00:00",
        "last_modified_date": "2013-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1672",
        "title": "Simulated Car Racing Championship: Competition Software Manual",
        "authors": [
            "Daniele Loiacono",
            "Luigi Cardamone",
            "Pier Luca Lanzi"
        ],
        "abstract": "This manual describes the competition software for the Simulated Car Racing Championship, an international competition held at major conferences in the field of Evolutionary Computation and in the field of Computational Intelligence and Games. It provides an overview of the architecture, the instructions to install the software and to run the simple drivers provided in the package, the description of the sensors and the actuators.\n    ",
        "submission_date": "2013-04-05T00:00:00",
        "last_modified_date": "2013-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1684",
        "title": "Probability Aggregates in Probability Answer Set Programming",
        "authors": [
            "Emad Saad"
        ],
        "abstract": "Probability answer set programming is a declarative programming that has been shown effective for representing and reasoning about a variety of probability reasoning tasks. However, the lack of probability aggregates, e.g. {\\em expected values}, in the language of disjunctive hybrid probability logic programs (DHPP) disallows the natural and concise representation of many interesting problems. In this paper, we extend DHPP to allow arbitrary probability aggregates. We introduce two types of probability aggregates; a type that computes the expected value of a classical aggregate, e.g., the expected value of the minimum, and a type that computes the probability of a classical aggregate, e.g, the probability of sum of values. In addition, we define a probability answer set semantics for DHPP with arbitrary probability aggregates including monotone, antimonotone, and nonmonotone probability aggregates. We show that the proposed probability answer set semantics of DHPP subsumes both the original probability answer set semantics of DHPP and the classical answer set semantics of classical disjunctive logic programs with classical aggregates, and consequently subsumes the classical answer set semantics of the original disjunctive logic programs. We show that the proposed probability answer sets of DHPP with probability aggregates are minimal probability models and hence incomparable, which is an important property for nonmonotonic probability reasoning.\n    ",
        "submission_date": "2013-04-05T00:00:00",
        "last_modified_date": "2013-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1819",
        "title": "Model-based Bayesian Reinforcement Learning for Dialogue Management",
        "authors": [
            "Pierre Lison"
        ],
        "abstract": "Reinforcement learning methods are increasingly used to optimise dialogue policies from experience. Most current techniques are model-free: they directly estimate the utility of various actions, without explicit model of the interaction dynamics. In this paper, we investigate an alternative strategy grounded in model-based Bayesian reinforcement learning. Bayesian inference is used to maintain a posterior distribution over the model parameters, reflecting the model uncertainty. This parameter distribution is gradually refined as more data is collected and simultaneously used to plan the agent's actions. Within this learning framework, we carried out experiments with two alternative formalisations of the transition model, one encoded with standard multinomial distributions, and one structured with probabilistic rules. We demonstrate the potential of our approach with empirical results on a user simulator constructed from Wizard-of-Oz data in a human-robot interaction scenario. The results illustrate in particular the benefits of capturing prior domain knowledge with high-level rules.\n    ",
        "submission_date": "2013-04-05T00:00:00",
        "last_modified_date": "2013-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1827",
        "title": "Fuzzy Aggregates in Fuzzy Answer Set Programming",
        "authors": [
            "Emad Saad"
        ],
        "abstract": "Fuzzy answer set programming is a declarative framework for representing and reasoning about knowledge in fuzzy environments. However, the unavailability of fuzzy aggregates in disjunctive fuzzy logic programs, DFLP, with fuzzy answer set semantics prohibits the natural and concise representation of many interesting problems. In this paper, we extend DFLP to allow arbitrary fuzzy aggregates. We define fuzzy answer set semantics for DFLP with arbitrary fuzzy aggregates including monotone, antimonotone, and nonmonotone fuzzy aggregates. We show that the proposed fuzzy answer set semantics subsumes both the original fuzzy answer set semantics of DFLP and the classical answer set semantics of classical disjunctive logic programs with classical aggregates, and consequently subsumes the classical answer set semantics of classical disjunctive logic programs. We show that the proposed fuzzy answer sets of DFLP with fuzzy aggregates are minimal fuzzy models and hence incomparable, which is an important property for nonmonotonic fuzzy reasoning.\n    ",
        "submission_date": "2013-04-05T00:00:00",
        "last_modified_date": "2013-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2339",
        "title": "The structure of Bayes nets for vision recognition",
        "authors": [
            "John Mark Agosta"
        ],
        "abstract": "This paper is part of a study whose goal is to show the effciency of using Bayes networks to carry out model based vision calculations.  [Binford et al. 1987] Recognition proceeds by drawing up a network model from the object's geometric and functional description that predicts the appearance of an object.  Then this network is used to find the object within a photographic image.  Many existing and proposed techniques for vision recognition resemble the uncertainty calculations of a Bayes net.  In contrast, though, they lack a derivation from first principles, and tend to rely on arbitrary parameters that we hope to avoid by a network model.  The connectedness of the network depends on what independence considerations can be identified in the vision problem.  Greater independence leads to easier calculations, at the expense of the net's expressiveness.  Once this trade-off is made and the structure of the network is determined, it should be possible to tailor a solution technique for it.  This paper explores the use of a network with multiply connected paths, drawing on both techniques of belief networks [Pearl 86] and influence diagrams.  We then demonstrate how one formulation of a multiply connected network can be solved.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2340",
        "title": "Summary of A New Normative Theory of Probabilistic Logic",
        "authors": [
            "Romas Aleliunas"
        ],
        "abstract": "By probabilistic logic I mean a normative theory of belief that explains how a body of evidence affects one's degree of belief in a possible hypothesis.  A new axiomatization of such a theory is presented which avoids a finite additivity axiom, yet which retains many useful inference rules.  Many of the examples of this theory--its models do not use numerical probabilities.  Put another way, this article gives sharper answers to the two questions: ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2341",
        "title": "Probability Distributions Over Possible Worlds",
        "authors": [
            "Fahiem Bacchus"
        ],
        "abstract": "In Probabilistic Logic Nilsson uses the device of a probability distribution over a set of possible worlds to assign probabilities to the sentences of a logical language.  In his paper Nilsson concentrated on inference and associated computational issues.  This paper, on the other hand, examines the probabilistic semantics in more detail, particularly for the case of first-order languages, and attempts to explain some of the features and limitations of this form of probability logic.  It is pointed out that the device of assigning probabilities to logical sentences has certain expressive limitations.  In particular, statistical assertions are not easily expressed by such a device.  This leads to certain difficulties with attempts to give probabilistic semantics to default reasoning using probabilities assigned to logical sentences.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2342",
        "title": "Hierarchical Evidence and Belief Functions",
        "authors": [
            "Paul K. Black",
            "Kathryn Blackmond Laskey"
        ],
        "abstract": "Dempster/Shafer (D/S) theory has been advocated as a way of representing incompleteness of evidence in a system's knowledge base. Methods now exist for propagating beliefs through chains of inference.  This paper discusses how rules with attached beliefs, a common representation for knowledge in automated reasoning systems, can be transformed into the joint belief functions required by propagation algorithms.  A rule is taken as defining a conditional belief function on the consequent given the antecedents.  It is demonstrated by example that different joint belief functions may be consistent with a given set of rules.  Moreover, different representations of the same rules may yield different beliefs on the consequent hypotheses.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2343",
        "title": "Decision-Theoretic Control of Problem Solving: Principles and Architecture",
        "authors": [
            "John S. Breese",
            "Michael R. Fehling"
        ],
        "abstract": "This paper presents an approach to the design of autonomous, real-time systems operating in uncertain environments.  We address issues of problem solving and reflective control of reasoning under uncertainty in terms of two fundamental elements: l) a set of decision-theoretic models for selecting among alternative problem-solving methods and 2) a general computational architecture for resource-bounded problem solving.  The decisiontheoretic models provide a set of principles for choosing among alternative problem-solving methods based on their relative costs and benefits, where benefits are characterized in terms of the value of information provided by the output of a reasoning activity.  The output may be an estimate of some uncertain quantity or a recommendation for action.  The computational architecture, called Schemer-ll, provides for interleaving of and communication among various problem-solving subsystems. These subsystems provide alternative approaches to information gathering, belief refinement, solution construction, and solution execution.  In particular, the architecture provides a mechanism for interrupting the subsystems in response to critical events.  We provide a decision theoretic account for scheduling problem-solving elements and for critical-event-driven interruption of activities in an architecture such as Schemer-II.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2344",
        "title": "Induction and Uncertainty Management Techniques Applied to Veterinary Medical Diagnosis",
        "authors": [
            "M. Cecile",
            "Mary McLeish",
            "P. Pascoe",
            "W. Taylor"
        ],
        "abstract": "This paper discusses a project undertaken between the Departments of Computing Science, Statistics, and the College of Veterinary Medicine to design a medical diagnostic system.  On-line medical data has been collected in the hospital database system for several years.  A number of induction methods are being used to extract knowledge from the data in an attempt to improve upon simple diagnostic charts used by the clinicians. They also enhance the results of classical statistical methods - finding many more significant variables.  The second part of the paper describes an essentially Bayesian method of evidence combination using fuzzy events at an initial step.  Results are presented and comparisons are made with other methods.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2345",
        "title": "KNET: Integrating Hypermedia and Bayesian Modeling",
        "authors": [
            "R. Martin Chavez",
            "Gregory F. Cooper"
        ],
        "abstract": "KNET is a general-purpose shell for constructing expert systems based on belief networks and decision networks.  Such networks serve as graphical representations for decision models, in which the knowledge engineer must define clearly the alternatives, states, preferences, and relationships that constitute a decision basis. KNET contains a knowledge-engineering core written in Object Pascal and an interface that tightly integrates HyperCard, a hypertext authoring tool for the Apple Macintosh computer, into a novel expert-system architecture.  Hypertext and hypermedia have become increasingly important in the storage management, and retrieval of information.  In broad terms, hypermedia deliver heterogeneous bits of information in dynamic, extensively cross-referenced packages.  The resulting KNET system features a coherent probabilistic scheme for managing uncertainty, an objectoriented graphics editor for drawing and manipulating decision networks, and HyperCard's potential for quickly constructing flexible and friendly user interfaces.  We envision KNET as a useful prototyping tool for our ongoing research on a variety of Bayesian reasoning problems, including tractable representation, inference, and explanation.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2346",
        "title": "A Method for Using Belief Networks as Influence Diagrams",
        "authors": [
            "Gregory F. Cooper"
        ],
        "abstract": "This paper demonstrates a method for using belief-network algorithms to solve influence diagram problems. In particular, both exact and approximation belief-network algorithms may be applied to solve influence-diagram problems. More generally, knowing the relationship between belief-network and influence-diagram problems may be useful in the design and development of more efficient influence diagram algorithms.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2347",
        "title": "Process, Structure, and Modularity in Reasoning with Uncertainty",
        "authors": [
            "Bruce D'Ambrosio"
        ],
        "abstract": "Computational mechanisms for uncertainty management must support interactive and incremental problem formulation, inference, hypothesis testing, and decision making. However, most current uncertainty inference systems concentrate primarily on inference, and provide no support for the larger issues.  We present a computational approach to uncertainty management which provides direct support for the dynamic, incremental aspect of this task, while at the same time permitting direct representation of the structure of evidential relationships. At the same time, we show that this approach responds to the modularity concerns of Heckerman and Horvitz [Heck87].  This paper emphasizes examples of the capabilities of this approach.  Another paper [D'Am89] details the representations and algorithms involved.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2348",
        "title": "Probabilistic Causal Reasoning",
        "authors": [
            "Thomas L. Dean",
            "Keiji Kanazawa"
        ],
        "abstract": "Predicting the future is an important component of decision making.  In most situations, however, there is not enough information to make accurate predictions.  In this paper, we develop a theory of causal reasoning for predictive inference under uncertainty.  We emphasize a common type of prediction that involves reasoning about persistence: whether or not a proposition once made true remains true at some later time.  We provide a decision procedure with a polynomial-time algorithm for determining the probability of the possible consequences of a set events and initial conditions.  The integration of simple probability theory with temporal projection enables us to circumvent problems that nonmonotonic temporal reasoning schemes have in dealing with persistence.  The ideas in this paper have been implemented in a prototype system that refines a database of causal rules in the course of applying those rules to construct and carry out plans in a manufacturing domain.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2349",
        "title": "Modeling uncertain and vague knowledge in possibility and evidence theories",
        "authors": [
            "Didier Dubois",
            "Henri Prade"
        ],
        "abstract": "This paper advocates the usefulness of new theories of uncertainty for the purpose of modeling some facets of uncertain knowledge, especially vagueness, in AI.  It can be viewed as a partial reply to Cheeseman's (among others) defense of probability.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2350",
        "title": "A Temporal Logic for Uncertain Events and An Outline of A Possible Implementation in An Extension of PROLOG",
        "authors": [
            "Soumitra Dutta"
        ],
        "abstract": "There is uncertainty associated with the occurrence of many events in real life. In this paper we develop a temporal logic to deal with such uncertain events and outline a possible implementation in an extension of PROLOG. Events are represented as fuzzy sets with the membership function giving the possibility of occurrence of the event in a given interval of time. The developed temporal logic is simple but powerful. It can determine effectively the various temporal relations between uncertain events or their combinations. PROLOG provides a uniform substrate on which to effectively implement such a temporal logic for uncertain events\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2351",
        "title": "Uncertainty Management for Fuzzy Decision Support Systems",
        "authors": [
            "Christoph F. Eick"
        ],
        "abstract": "A new approach for uncertainty management for fuzzy, rule based decision support systems is proposed: The domain expert's knowledge is expressed by a set of rules that frequently refer to vague and uncertain propositions. The certainty of propositions is represented using intervals [a, b] expressing that the proposition's probability is at least a and at most b. Methods and techniques for computing the overall certainty of fuzzy compound propositions that have been defined by using logical connectives 'and', 'or' and 'not' are introduced. Different inference schemas for applying fuzzy rules by using modus ponens are discussed. Different algorithms for combining evidence that has been received from different rules for the same proposition are provided. The relationship of the approach to other approaches is analyzed and its problems of knowledge acquisition and knowledge representation are discussed in some detail. The basic concepts of a rule-based programming language called PICASSO, for which the approach is a theoretical foundation, are outlined.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2352",
        "title": "Probability as a Modal Operator",
        "authors": [
            "Alan M. Frisch",
            "Peter Haddawy"
        ],
        "abstract": "This paper argues for a modal view of probability. The syntax and semantics of one particularly strong probability logic are discussed and some examples of the use of the logic are provided. We show that it is both natural and useful to think of probability as a modal operator. Contrary to popular belief in AI, a probability ranging between 0 and 1 represents a continuum between impossibility and necessity, not between simple falsity and truth. The present work provides a clear semantics for quantification into the scope of the probability operator and for higher-order probabilities. Probability logic is a language for expressing both probabilistic and logical concepts.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2353",
        "title": "Truth Maintenance Under Uncertainty",
        "authors": [
            "Li-Min Fu"
        ],
        "abstract": "This paper addresses the problem of resolving errors under uncertainty in a rule-based system. A new approach has been developed that reformulates this problem as a neural-network learning problem. The strength and the fundamental limitations of this approach are explored and discussed. The main result is that neural heuristics can be applied to solve some but not all problems in rule-based systems.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2354",
        "title": "Bayesian Assessment of a Connectionist Model for Fault Detection",
        "authors": [
            "Stephen I. Gallant"
        ],
        "abstract": "A previous paper [2] showed how to generate a linear discriminant network (LDN) that computes likely faults for a noisy fault detection problem by using a modification of the perceptron learning algorithm called the pocket algorithm. Here we compare the performance of this connectionist model with performance of the optimal Bayesian decision rule for the example that was previously described. We find that for this particular problem the connectionist model performs about 97% as well as the optimal Bayesian procedure. We then define a more general class of noisy single-pattern boolean (NSB) fault detection problems where each fault corresponds to a single :pattern of boolean instrument readings and instruments are independently noisy. This is equivalent to specifying that instrument readings are probabilistic but conditionally independent given any particular fault. We prove:\n",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2355",
        "title": "On the Logic of Causal Models",
        "authors": [
            "Dan Geiger",
            "Judea Pearl"
        ],
        "abstract": "This paper explores the role of Directed Acyclic Graphs (DAGs) as a representation of conditional independence relationships.  We show that DAGs offer polynomially sound and complete inference mechanisms for inferring conditional independence relationships from a given causal set of such relationships.  As a consequence, d-separation, a graphical criterion for identifying independencies in a DAG, is shown to uncover more valid independencies then any other criterion.  In addition, we employ the Armstrong property of conditional independence to show that the dependence relationships displayed by a DAG are inherently consistent, i.e. for every DAG D there exists some probability distribution P that embodies all the conditional independencies displayed in D and none other.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2356",
        "title": "The Optimality of Satisficing Solutions",
        "authors": [
            "Othar Hansson",
            "Andy Mayer"
        ],
        "abstract": "This paper addresses a prevailing assumption in single-agent heuristic search theory- that problem-solving algorithms should guarantee shortest-path solutions, which are typically called optimal. Optimality implies a metric for judging solution quality, where the optimal solution is the solution with the highest quality. When path-length is the metric, we will distinguish such solutions as p-optimal.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2357",
        "title": "An Empirical Comparison of Three Inference Methods",
        "authors": [
            "David Heckerman"
        ],
        "abstract": "In this paper, an empirical evaluation of three inference methods for uncertain reasoning is presented in the context of Pathfinder, a large expert system for the diagnosis of lymph-node pathology. The inference procedures evaluated are (1) Bayes' theorem, assuming evidence is conditionally independent given each hypothesis; (2) odds-likelihood updating, assuming evidence is conditionally independent given each hypothesis and given the negation of each hypothesis; and (3) a inference method related to the Dempster-Shafer theory of belief. Both expert-rating and decision-theoretic metrics are used to compare the diagnostic accuracy of the inference methods.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2023-01-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2358",
        "title": "Parallel Belief Revision",
        "authors": [
            "Daniel Hunter"
        ],
        "abstract": "This paper describes a formal system of belief revision developed by Wolfgang Spohn and shows that this system has a parallel implementation that can be derived from an influence diagram in a manner similar to that in which Bayesian networks are derived. The proof rests upon completeness results for an axiomatization of the notion of conditional independence, with the Spohn system being used as a semantics for the relation of conditional independence.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2359",
        "title": "Stochastic Sensitivity Analysis Using Fuzzy Influence Diagrams",
        "authors": [
            "Pramod Jain",
            "Alice M. Agogino"
        ],
        "abstract": "The practice of stochastic sensitivity analysis described in the decision analysis literature is a testimonial to the need for considering deviations from precise point estimates of uncertainty.  We propose the use of Bayesian fuzzy probabilities within an influence diagram computational scheme for performing sensitivity analysis during the solution of probabilistic inference and decision problems.  Unlike other parametric approaches, the proposed scheme does not require resolving the problem for the varying probability point estimates.  We claim that the solution to fuzzy influence diagrams provides as much information as the classical point estimate approach plus additional information concerning stochastic sensitivity.  An example based on diagnostic decision making in microcomputer assembly is used to illustrate this idea.  We claim that the solution to fuzzy influence diagrams provides as much information as the classical point estimate approach plus additional interval information that is useful for stochastic sensitivity analysis.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2360",
        "title": "A Representation of Uncertainty to Aid Insight into Decision Models",
        "authors": [
            "Holly B. Jimison"
        ],
        "abstract": "Many real world models can be characterized as weak, meaning that there is significant uncertainty in both the data input and inferences.  This lack of determinism makes it especially difficult for users of computer decision aids to understand and have confidence in the models.  This paper presents a representation for uncertainty and utilities that serves as a framework for graphical summary and computer-generated explanation of decision models.  The application described that tests the methodology is a computer decision aid designed to enhance the clinician-patient consultation process for patients with angina (chest pain due to lack of blood flow to the heart muscle).  The angina model is represented as a Bayesian decision network.  Additionally, the probabilities and utilities are treated as random variables with probability distributions on their range of possible values.  The initial distributions represent information on all patients with anginal symptoms, and the approach allows for rapid tailoring to more patientspecific distributions.  This framework provides a metric for judging the importance of each variable in the model dynamically.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2361",
        "title": "Rational Nonmonotonic Reasoning",
        "authors": [
            "Carl Kadie"
        ],
        "abstract": "Nonmonotonic reasoning is a pattern of reasoning that allows an agent to make and retract (tentative) conclusions from inconclusive evidence. This paper gives a possible-worlds interpretation of the nonmonotonic reasoning problem based on standard decision theory and the emerging probability logic. The system's central principle is that a tentative conclusion is a decision to make a bet, not an assertion of fact. The system is rational, and as sound as the proof theory of its underlying probability log.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2362",
        "title": "A Comparison of Decision Analysis and Expert Rules for Sequential Diagnosis",
        "authors": [
            "Jayant Kalagnanam",
            "Max Henrion"
        ],
        "abstract": "There has long been debate about the relative merits of decision theoretic methods and heuristic rule-based approaches for reasoning under uncertainty.  We report an experimental comparison of the performance of the two approaches to troubleshooting, specifically to test selection for fault diagnosis.  We use as experimental testbed the problem of diagnosing motorcycle engines.  The first approach employs heuristic test selection rules obtained from expert mechanics.  We compare it with the optimal decision analytic algorithm for test selection which employs estimated component failure probabilities and test costs.  The decision analytic algorithm was found to reduce the expected cost (i.e. time) to arrive at a diagnosis by an average of 14% relative to the expert rules.  Sensitivity analysis shows the results are quite robust to inaccuracy in the probability and cost estimates.  This difference suggests some interesting implications for knowledge acquisition.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2364",
        "title": "Probabilistic Inference and Probabilistic Reasoning",
        "authors": [
            "Henry E. Kyburg Jr"
        ],
        "abstract": "Uncertainty enters into human reasoning and inference in at least two ways. It is reasonable to suppose that there will be roles for these distinct uses of uncertainty also in automated reasoning.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2365",
        "title": "Probabilistic and Non-Monotonic Inference",
        "authors": [
            "Henry E. Kyburg Jr"
        ],
        "abstract": "(l) I have enough evidence to render the sentence S probable.  (la) So, relative to what I know, it is rational of me to believe S.  (2) Now that I have more evidence, S may no longer be probable.  (2a) So now, relative to what I know, it is not rational of me to believe S.  These seem a perfectly ordinary, common sense, pair of situations.  Generally and vaguely, I take them to embody what I shall call probabilistic inference.  This form of inference is clearly non-monotonic.  Relatively few people have taken this form of inference, based on high probability, to serve as a foundation for non-monotonic logic or for a logical or defeasible inference.  There are exceptions: Jane Nutter [16] thinks that sometimes probability has something to do with non-monotonic reasoning.  Judea Pearl [ 17] has recently been exploring the possibility.  There are any number of people whom one might call probability enthusiasts who feel that probability provides all the answers by itself, with no need of help from logic.  Cheeseman [1], Henrion [5] and others think it useful to look at a distribution of probabilities over a whole algebra of statements, to update that distribution in the light of new evidence, and to use the latest updated distribution of probability over the algebra as a basis for planning and decision making.  A slightly weaker form of this approach is captured by Nilsson [15], where one assumes certain probabilities for certain statements, and infers the probabilities, or constraints on the probabilities of other statement.  None of this corresponds to what I call probabilistic inference.  All of the inference that is taking place, either in Bayesian updating, or in probabilistic logic, is strictly deductive.  Deductive inference, particularly that concerned with the distribution of classical probabilities or chances, is of great importance.  But this is not to say that there is no important role for what earlier logicians have called \"ampliative\" or \"inductive\" or \"scientific\" inference, in which the conclusion goes beyond the premises, asserts more than do the premises.  This depends on what David Israel [6] has called \"real rules of inference\".  It is characteristic of any such logic or inference procedure that it can go wrong: that statements accepted at one point may be rejected at a later point.  Research underlying the results reported here has been partially supported by the Signals Warfare Center of the United States Army.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2366",
        "title": "Epistemological Relevance and Statistical Knowledge",
        "authors": [
            "Henry E. Kyburg Jr"
        ],
        "abstract": "For many years, at least since McCarthy and Hayes (1969), writers have lamented, and attempted to compensate for, the alleged fact that we often do not have adequate statistical knowledge for governing the uncertainty of belief, for making uncertain inferences, and the like. It is hardly ever spelled out what \"adequate statistical knowledge\" would be, if we had it, and how adequate statistical knowledge could be used to control and regulate epistemic uncertainty.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2368",
        "title": "Evidential Reasoning in a Network Usage Prediction Testbed",
        "authors": [
            "Ronald P. Loui"
        ],
        "abstract": "This paper reports on empirical work aimed at comparing evidential reasoning techniques.  While there is prima facie evidence for some conclusions, this i6 work in progress; the present focus is methodology, with the goal that subsequent results be meaningful.  The domain is a network of UNIX* cycle servers, and the task is to predict properties of the state of the network from partial descriptions of the state.  Actual data from the network are taken and used for blindfold testing in a betting game that allows abstention.  The focal technique has been Kyburg's method for reasoning with data of varying relevance to a particular query, though the aim is to be able eventually to compare various uncertainty calculi.  The conclusions are not novel, but are instructive. 1. All of the calculi performed better than human subjects, so unbiased access to sample experience is apparently of value.  2. Performance depends on metric: (a) when trials are repeated, net = gains - losses favors methods that place many bets, if the probability of placing a correct bet is sufficiently high; that is, it favors point-valued formalisms; (b) yield = gains/(gains + lossee) favors methods that bet only when sure to bet correctly; that is, it favors interval-valued formalisms.  3. Among the calculi, there were no clear winners or losers.  Methods are identified for eliminating the bias of the net as a performance criterion and for separating the calculi effectively: in both cases by posting odds for the betting game in the appropriate way.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2369",
        "title": "Justifying the Principle of Interval Constraints",
        "authors": [
            "Richard E. Neapolitan",
            "James Kenevan"
        ],
        "abstract": "When knowledge is obtained from a database, it is only possible to deduce confidence intervals for probability values. With confidence intervals replacing point values, the results in the set covering model include interval constraints for the probabilities of mutually exclusive and exhaustive explanations. The Principle of Interval Constraints ranks these explanations by determining the expected values of the probabilities based on distributions determined from the interval, constraints. This principle was developed using the Classical Approach to probability. This paper justifies the Principle of Interval Constraints with a more rigorous statement of the Classical Approach and by defending the concept of probabilities of probabilities.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2370",
        "title": "Probabilistic Semantics and Defaults",
        "authors": [
            "Eric Neufeld",
            "David L Poole"
        ],
        "abstract": "There is much interest in providing probabilistic semantics for defaults but most approaches seem to suffer from one of two problems: either they require numbers, a problem defaults were intended to avoid, or they generate peculiar side effects.  Rather than provide semantics for defaults, we address the problem defaults were intended to solve: that of reasoning under uncertainty where numeric probability distributions are not available.  We describe a non-numeric formalism called an inference graph based on standard probability theory, conditional independence and sentences of favouring where a favours b - favours(a, b) - p(a|b) > p(a).  The formalism seems to handle the examples from the nonmonotonic literature.  Most importantly, the sentences of our system can be verified by performing an appropriate experiment in the semantic domain.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2371",
        "title": "Decision Making with Linear Constraints on Probabilities",
        "authors": [
            "Michael Pittarelli"
        ],
        "abstract": "Techniques for decision making with knowledge of linear constraints on condition probabilities are examined. These constraints arise naturally in many situations: upper and lower condition probabilities are known; an ordering among the probabilities is determined; marginal probabilities or bounds on such probabilities are known, e.g., data are available in the form of a probabilistic database (Cavallo and Pittarelli, 1987a); etc. Standard situations of decision making under risk and uncertainty may also be characterized by linear constraints. Each of these types of information may be represented by a convex polyhedron of numerically determinate condition probabilities. A uniform approach to decision making under risk, uncertainty, and partial uncertainty based on a generalized version of a criterion of Hurwicz is proposed, Methods for processing marginal probabilities to improve decision making using any of the criteria discussed are presented.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2372",
        "title": "Maintenance in Probabilistic Knowledge-Based Systems",
        "authors": [
            "Thomas F. Reid",
            "Gregory S. Parnell"
        ],
        "abstract": "Recent developments using directed acyclical graphs (i.e., influence diagrams and Bayesian networks) for knowledge representation have lessened the problems of using probability in knowledge-based systems (KBS). Most current research involves the efficient propagation of new evidence, but little has been done concerning the maintenance of domain-specific knowledge, which includes the probabilistic information about the problem domain. By making use of conditional independencies represented in she graphs, however, probability assessments are required only for certain variables when the knowledge base is updated. The purpose of this study was to investigate, for those variables which require probability assessments, ways to reduce the amount of new knowledge required from the expert when updating probabilistic information in a probabilistic knowledge-based system. Three special cases (ignored outcome, split outcome, and assumed constraint outcome) were identified under which many of the original probabilities (those already in the knowledge-base) do not need to be reassessed when maintenance is required.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2373",
        "title": "A Linear Approximation Method for Probabilistic Inference",
        "authors": [
            "Ross D. Shachter"
        ],
        "abstract": "An approximation method is presented for probabilistic inference with continuous random variables.  These problems can arise in many practical problems, in particular where there are \"second order\" probabilities.  The approximation, based on the Gaussian influence diagram, iterates over linear approximations to the inference problem.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2374",
        "title": "An Axiomatic Framework for Bayesian and Belief-function Propagation",
        "authors": [
            "Prakash P. Shenoy",
            "Glenn Shafer"
        ],
        "abstract": "In this paper, we describe an abstract framework and axioms under which exact local computation of marginals is possible.  The primitive objects of the framework are variables and valuations.  The primitive operators of the framework are combination and marginalization.  These operate on valuations.  We state three axioms for these operators and we derive the possibility of local computation from the axioms.  Next, we describe a propagation scheme for computing marginals of a valuation when we have a factorization of the valuation on a hypertree.  Finally we show how the problem of computing marginals of joint probability distributions and joint belief functions fits the general framework.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2375",
        "title": "A General Non-Probabilistic Theory of Inductive Reasoning",
        "authors": [
            "Wolfgang Spohn"
        ],
        "abstract": "Probability theory, epistemically interpreted, provides an excellent, if not the best available account of inductive reasoning.  This is so because there are general and definite rules for the change of subjective probabilities through information or experience; induction and belief change are one and same topic, after all.  The most basic of these rules is simply to conditionalize with respect to the information received; and there are similar and more general rules.  1 Hence, a fundamental reason for the epistemological success of probability theory is that there at all exists a well-behaved concept of conditional probability.  Still, people have, and have reasons for, various concerns over probability theory.  One of these is my starting point: Intuitively, we have the notion of plain belief; we believe propositions2 to be true (or to be false or neither). Probability theory, however, offers no formal counterpart to this notion. Believing A is not the same as having probability 1 for A, because probability 1 is incorrigible3; but plain belief is clearly corrigible.  And believing A is not the same as giving A a probability larger than some 1 - c, because believing A and believing B is usually taken to be equivalent to believing A & B.4 Thus, it seems that the formal representation of plain belief has to take a non-probabilistic route.  Indeed, representing plain belief seems easy enough: simply represent an epistemic state by the set of all propositions believed true in it or, since I make the common assumption that plain belief is deductively closed, by the conjunction of all propositions believed true in it.  But this does not yet provide a theory of induction, i.e. an answer to the question how epistemic states so represented are changed tbrough information or experience.  There is a convincing partial answer: if the new information is compatible with the old epistemic state, then the new epistemic state is simply represented by the conjunction of the new information and the old beliefs.  This answer is partial because it does not cover the quite common case where the new information is incompatible with the old beliefs.  It is, however, important to complete the answer and to cover this case, too; otherwise, we would not represent plain belief as conigible.  The crucial problem is that there is no good completion.  When epistemic states are represented simply by the conjunction of all propositions believed true in it, the answer cannot be completed; and though there is a lot of fruitful work, no other representation of epistemic states has been proposed, as far as I know, which provides a complete solution to this problem.  In this paper, I want to suggest such a solution.  In [4], I have more fully argued that this is the only solution, if certain plausible desiderata are to be satisfied.  Here, in section 2, I will be content with formally defining and intuitively explaining my proposal.  I will compare my proposal with probability theory in section 3. It will turn out that the theory I am proposing is structurally homomorphic to probability theory in important respects and that it is thus equally easily implementable, but moreover computationally simpler.  Section 4 contains a very brief comparison with various kinds of logics, in particular conditional logic, with Shackle's functions of potential surprise and related theories, and with the Dempster - Shafer theory of belief functions.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2376",
        "title": "Generating Decision Structures and Causal Explanations for Decision Making",
        "authors": [
            "Spencer Star"
        ],
        "abstract": "This paper examines two related problems that are central to developing an autonomous decision-making agent, such as a robot.  Both problems require generating structured representafions from a database of unstructured declarative knowledge that includes many facts and rules that are irrelevant in the problem context.  The first problem is how to generate a well structured decision problem from such a database.  The second problem is how to generate, from the same database, a well-structured explanation of why some possible world occurred.  In this paper it is shown that the problem of generating the appropriate decision structure or explanation is intractable without introducing further constraints on the knowledge in the database.  The paper proposes that the problem search space can be constrained by adding knowledge to the database about causal relafions between events.  In order to determine the causal knowledge that would be most useful, causal theories for deterministic and indeterministic universes are proposed.  A program that uses some of these causal constraints has been used to generate explanations about faulty plans.  The program shows the expected increase in efficiency as the causal constraints are introduced.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2377",
        "title": "Updating Probabilities in Multiply-Connected Belief Networks",
        "authors": [
            "Jaap Suermondt",
            "Gregory F. Cooper"
        ],
        "abstract": "This paper focuses on probability updates in multiply-connected belief networks. Pearl has designed the method of conditioning, which enables us to apply his algorithm for belief updates in singly-connected networks to multiply-connected belief networks by selecting a loop-cutset for the network and instantiating these loop-cutset nodes. We discuss conditions that need to be satisfied by the selected nodes. We present a heuristic algorithm for finding a loop-cutset that satisfies these conditions.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2378",
        "title": "Handling uncertainty in a system for text-symbol context analysis",
        "authors": [
            "Bjornar Tessem",
            "Lars Johan Ersland"
        ],
        "abstract": "In pattern analysis, information regarding an object can often be drawn from its surroundings. This paper presents a method for handling uncertainty when using context of symbols and texts for analyzing technical drawings. The method is based on Dempster-Shafer theory and possibility theory.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2379",
        "title": "Causal Networks: Semantics and Expressiveness",
        "authors": [
            "Tom S. Verma",
            "Judea Pearl"
        ],
        "abstract": "Dependency knowledge of the form \"x is independent of y once z is known\" invariably obeys the four graphoid axioms, examples include probabilistic and database dependencies.  Often, such knowledge can be represented efficiently with graphical structures such as undirected graphs and directed acyclic graphs (DAGs).  In this paper we show that the graphical criterion called d-separation is a sound rule for reading independencies from any DAG based on a causal input list drawn from a graphoid.  The rule may be extended to cover DAGs that represent functional dependencies as well as conditional dependencies.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2380",
        "title": "MCE Reasoning in Recursive Causal Networks",
        "authors": [
            "Wilson X. Wen"
        ],
        "abstract": "A probabilistic method of reasoning under uncertainty is proposed based on the principle of Minimum Cross Entropy (MCE) and concept of Recursive Causal Model (RCM).  The dependency and correlations among the variables are described in a special language BNDL (Belief Networks Description Language).  Beliefs are propagated among the clauses of the BNDL programs representing the underlying probabilistic distributions. BNDL interpreters in both Prolog and C has been developed and the performance of the method is compared with those of the others.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2381",
        "title": "Nonmonotonic Reasoning via Possibility Theory",
        "authors": [
            "Ronald R. Yager"
        ],
        "abstract": "We introduce the operation of possibility qualification and show how. this modal-like operator can be used to represent \"typical\" or default knowledge in a theory of nonmonotonic reasoning. We investigate the representational power of this approach by looking at a number of prototypical problems from the nonmonotonic reasoning literature. In particular we look at the so called Yale shooting problem and its relation to priority in default reasoning.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2383",
        "title": "Generalizing the Dempster-Shafer Theory to Fuzzy Sets",
        "authors": [
            "John Yen"
        ],
        "abstract": "With the desire to apply the Dempster-Shafer theory to complex real world problems where the evidential strength is often imprecise and vague, several attempts have been made to generalize the theory.  However, the important concept in the D-S theory that the belief and plausibility functions are lower and upper probabilities is no longer preserved in these generalizations.  In this paper, we describe a generalized theory of evidence where the degree of belief in a fuzzy set is obtained by minimizing the probability of the fuzzy set under the constraints imposed by a basic probability assignment.  To formulate the probabilistic constraint of a fuzzy focal element, we decompose it into a set of consonant non-fuzzy focal elements.  By generalizing the compatibility relation to a possibility theory, we are able to justify our generalization to Dempster's rule based on possibility distribution.  Our generalization not only extends the application of the D-S theory but also illustrates a way that probability theory and fuzzy set theory can be combined to deal with different kinds of uncertain information in AI systems.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2384",
        "title": "Logical Fuzzy Optimization",
        "authors": [
            "Emad Saad"
        ],
        "abstract": "We present a logical framework to represent and reason about fuzzy optimization problems based on fuzzy answer set optimization programming. This is accomplished by allowing fuzzy optimization aggregates, e.g., minimum and maximum in the language of fuzzy answer set optimization programming to allow minimization or maximization of some desired criteria under fuzzy environments. We show the application of the proposed logical fuzzy optimization framework under the fuzzy answer set optimization programming to the fuzzy water allocation optimization problem.\n    ",
        "submission_date": "2013-04-05T00:00:00",
        "last_modified_date": "2013-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2418",
        "title": "Mod\u00e8le flou d'expression des pr\u00e9f\u00e9rences bas\u00e9 sur les CP-Nets",
        "authors": [
            "Hanene Rezgui",
            "Minyar Sassi-Hidri"
        ],
        "abstract": "This article addresses the problem of expressing preferences in flexible queries while basing on a combination of the fuzzy logic theory and Conditional Preference Networks or CP-Nets.\n    ",
        "submission_date": "2013-03-31T00:00:00",
        "last_modified_date": "2013-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2538",
        "title": "On Appropriate Selection of Fuzzy Aggregation Operators in Medical Decision Support System",
        "authors": [
            "K.M. Motahar Hossain",
            "Zahir Raihan",
            "M.M.A. Hashem"
        ],
        "abstract": "The Decision Support System (DSS) contains more than one antecedent and the degrees of strength of the antecedents need to be combined to determine the overall strength of the rule consequent. The membership values of the linguistic variables in Fuzzy have to be combined using an aggregation operator. But it is not feasible to predefine the form of aggregation operators in decision making. Instead, each rule should be found based on the feeling of the experts and on their actual decision pattern over the set of typical examples. Thus this work illustrates how the choice of aggregation operators is intended to mimic human decision making and can be selected and adjusted to fit empirical data, a series of test cases. Both parametrized and nonparametrized aggregation operators are adapted to fit empirical data. Moreover, they provided compensatory properties and, therefore, seemed to produce a better decision support system. To solve the problem, a threshold point from the output of the aggregation operators is chosen as the separation point between two classes. The best achieved accuracy is chosen as the appropriate aggregation operator. Thus a medical decision can be generated which is very close to a practitioner's guideline.\n    ",
        "submission_date": "2013-04-09T00:00:00",
        "last_modified_date": "2013-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2694",
        "title": "Symmetry-Aware Marginal Density Estimation",
        "authors": [
            "Mathias Niepert"
        ],
        "abstract": "The Rao-Blackwell theorem is utilized to analyze and improve the scalability of inference in large probabilistic models that exhibit symmetries. A novel marginal density estimator is introduced and shown both analytically and empirically to outperform standard estimators by several orders of magnitude. The developed theory and algorithms apply to a broad class of probabilistic models including statistical relational models considered not susceptible to lifted probabilistic inference.\n    ",
        "submission_date": "2013-04-09T00:00:00",
        "last_modified_date": "2013-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2711",
        "title": "Is Shafer General Bayes?",
        "authors": [
            "Paul K. Black"
        ],
        "abstract": "This paper examines the relationship between Shafer's belief functions and convex sets of probability distributions. Kyburg's (1986) result showed that belief function models form a subset of the class of closed convex probability distributions. This paper emphasizes the importance of Kyburg's result by looking at simple examples involving Bernoulli trials. Furthermore, it is shown that many convex sets of probability distributions generate the same belief function in the sense that they support the same lower and upper values. This has implications for a decision theoretic extension. Dempster's rule of combination is also compared with Bayes' rule of conditioning.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2712",
        "title": "Modifiable Combining Functions",
        "authors": [
            "Paul Cohen",
            "Glenn Shafer",
            "Prakash P. Shenoy"
        ],
        "abstract": "Modifiable combining functions are a synthesis of two common approaches to combining evidence. They offer many of the advantages of these approaches and avoid some disadvantages. Because they facilitate the acquisition, representation, explanation, and modification of knowledge about combinations of evidence, they are proposed as a tool for knowledge engineers who build systems that reason under uncertainty, not as a normative theory of evidence.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2713",
        "title": "Dempster-Shafer vs. Probabilistic Logic",
        "authors": [
            "Daniel Hunter"
        ],
        "abstract": "The combination of evidence in Dempster-Shafer theory is compared with the combination of evidence in probabilistic logic. Sufficient conditions are stated for these two methods to agree. It is then shown that these conditions are minimal in the sense that disagreement can occur when any one of them is removed. An example is given in which the traditional assumption of conditional independence of evidence on hypotheses holds and a uniform prior is assumed, but probabilistic logic and Dempster's rule give radically different results for the combination of two evidence events.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2714",
        "title": "Higher Order Probabilities",
        "authors": [
            "Henry E. Kyburg Jr"
        ],
        "abstract": "A number of writers have supposed that for the full specification of belief, higher order probabilities are required. Some have even supposed that there may be an unending sequence of higher order probabilities of probabilities of probabilities.... In the present paper we show that higher order probabilities can always be replaced by the marginal distributions of joint probability distributions. We consider both the case in which higher order probabilities are of the same sort as lower order probabilities and that in which higher order probabilities are distinct in character, as when lower order probabilities are construed as frequencies and higher order probabilities are construed as subjective degrees of belief. In neither case do higher order probabilities appear to offer any advantages, either conceptually or computationally.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2715",
        "title": "Belief in Belief Functions: An Examination of Shafer's Canonical Examples",
        "authors": [
            "Kathryn Blackmond Laskey"
        ],
        "abstract": "In the canonical examples underlying Shafer-Dempster theory, beliefs over the hypotheses of interest are derived from a probability model for a set of auxiliary hypotheses. Beliefs are derived via a compatibility relation connecting the auxiliary hypotheses to subsets of the primary hypotheses. A belief function differs from a Bayesian probability model in that one does not condition on those parts of the evidence for which no probabilities are specified. The significance of this difference in conditioning assumptions is illustrated with two examples giving rise to identical belief functions but different Bayesian probability distributions.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2716",
        "title": "Do We Need Higher-Order Probabilities and, If So, What Do They Mean?",
        "authors": [
            "Judea Pearl"
        ],
        "abstract": "The apparent failure of individual probabilistic expressions to distinguish uncertainty about truths from uncertainty about probabilistic assessments have prompted researchers to seek formalisms where the two types of uncertainties are given notational distinction. This paper demonstrates that the desired distinction is already a built-in feature of classical probabilistic models, thus, specialized notations are unnecessary.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2717",
        "title": "Bayesian Prediction for Artificial Intelligence",
        "authors": [
            "Matthew Self",
            "Peter Cheeseman"
        ],
        "abstract": "This paper shows that the common method used for making predictions under uncertainty in A1 and science is in error. This method is to use currently available data to select the best model from a given class of models-this process is called abduction-and then to use this model to make predictions about future data. The correct method requires averaging over all the models to make a prediction-we call this method transduction. Using transduction, an AI system will not give misleading results when basing predictions on small amounts of data, when no model is clearly best. For common classes of models we show that the optimal solution can be given in closed form.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2718",
        "title": "Can Evidence Be Combined in the Dempster-Shafer Theory",
        "authors": [
            "John Yen"
        ],
        "abstract": "Dempster's rule of combination has been the most controversial part of the Dempster-Shafer (D-S) theory. In particular, Zadeh has reached a conjecture on the noncombinability of evidence from a relational model of the D-S theory. In this paper, we will describe another relational model where D-S masses are represented as conditional granular distributions. By comparing it with Zadeh's relational model, we will show how Zadeh's conjecture on combinability does not affect the applicability of Dempster's rule in our model.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2719",
        "title": "An Interesting Uncertainty-Based Combinatoric Problem in Spare Parts Forecasting: The FRED System",
        "authors": [
            "John B. Bacon"
        ],
        "abstract": "The domain of spare parts forecasting is examined, and is found to present unique uncertainty based problems in the architectural design of a knowledge-based system. A mixture of different uncertainty paradigms is required for the solution, with an intriguing combinatoric problem arising from an uncertain choice of inference engines. Thus, uncertainty in the system is manifested in two different meta-levels. The different uncertainty paradigms and meta-levels must be integrated into a functioning whole. FRED is an example of a difficult real-world domain to which no existing uncertainty approach is completely appropriate. This paper discusses the architecture of FRED, highlighting: the points of uncertainty and other interesting features of the domain, the specific implications of those features on the system design (including the combinatoric explosions), their current implementation & future plans,and other problems and issues with the architecture.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2720",
        "title": "Bayesian Inference in Model-Based Machine Vision",
        "authors": [
            "Thomas O. Binford",
            "Tod S. Levitt",
            "Wallace B. Mann"
        ],
        "abstract": "This is a preliminary version of visual interpretation integrating multiple sensors in SUCCESSOR, an intelligent, model-based vision system. We pursue a thorough integration of hierarchical Bayesian inference with comprehensive physical representation of objects and their relations in a system for reasoning with geometry, surface materials and sensor models in machine vision. Bayesian inference provides a framework for accruing_ probabilities to rank order hypotheses.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2721",
        "title": "Using the Dempster-Shafer Scheme in a Diagnostic Expert System Shell",
        "authors": [
            "Gautam Biswas",
            "Teywansh S. Anand"
        ],
        "abstract": "This paper discusses an expert system shell that integrates rule-based reasoning and the Dempster-Shafer evidence combination scheme. Domain knowledge is stored as rules with associated belief functions. The reasoning component uses a combination of forward and backward inferencing mechanisms to allow interaction with users in a mixed-initiative format.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2722",
        "title": "Stochastic Simulation of Bayesian Belief Networks",
        "authors": [
            "Homer L. Chin",
            "Gregory F. Cooper"
        ],
        "abstract": "This paper examines Bayesian belief network inference using simulation as a method for computing the posterior probabilities of network variables. Specifically, it examines the use of a method described by Henrion, called logic sampling, and a method described by Pearl, called stochastic simulation. We first review the conditions under which logic sampling is computationally infeasible. Such cases motivated the development of the Pearl's stochastic simulation algorithm. We have found that this stochastic simulation algorithm, when applied to certain networks, leads to much slower than expected convergence to the true posterior probabilities. This behavior is a result of the tendency for local areas in the network to become fixed through many simulation cycles. The time required to obtain significant convergence can be made arbitrarily long by strengthening the probabilistic dependency between nodes. We propose the use of several forms of graph modification, such as graph pruning, arc reversal, and node reduction, in order to convert some networks into formats that are computationally more efficient for simulation.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2723",
        "title": "Temporal Reasoning About Uncertain Worlds",
        "authors": [
            "Steve Hanks"
        ],
        "abstract": "We present a program that manages a database of temporally scoped beliefs. The basic functionality of the system includes maintaining a network of constraints among time points, supporting a variety of fetches, mediating the application of causal rules, monitoring intervals of time for the addition of new facts, and managing data dependencies that keep the database consistent. At this level the system operates independent of any measure of belief or belief calculus. We provide an example of how an application program mi9ght use this functionality to implement a belief calculus.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2724",
        "title": "A Perspective on Confidence and Its Use in Focusing Attention During Knowledge Acquisition",
        "authors": [
            "David Heckerman",
            "Holly B. Jimison"
        ],
        "abstract": "We present a representation of partial confidence in belief and preference that is consistent with the tenets of decision-theory. The fundamental insight underlying the representation is that if a person is not completely confident in a probability or utility assessment, additional modeling of the assessment may improve decisions to which it is relevant. We show how a traditional decision-analytic approach can be used to balance the benefits of additional modeling with associated costs. The approach can be used during knowledge acquisition to focus the attention of a knowledge engineer or expert on parts of a decision model that deserve additional refinement.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2725",
        "title": "Practical Issues in Constructing a Bayes' Belief Network",
        "authors": [
            "Max Henrion"
        ],
        "abstract": "Bayes belief networks and influence diagrams are tools for constructing coherent probabilistic representations of uncertain knowledge. The process of constructing such a network to represent an expert's knowledge is used to illustrate a variety of techniques which can facilitate the process of structuring and quantifying uncertain relationships. These include some generalizations of the \"noisy OR gate\" concept. Sensitivity analysis of generic elements of Bayes' networks provides insight into when rough probability assessments are sufficient and when greater precision may be important.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2726",
        "title": "NAIVE: A Method for Representing Uncertainty and Temporal Relationships in an Automated Reasoner",
        "authors": [
            "Michael C. Higgins"
        ],
        "abstract": "This paper describes NAIVE, a low-level knowledge representation language and inferencing process. NAIVE has been designed for reasoning about nondeterministic dynamic systems like those found in medicine. Knowledge is represented in a graph structure consisting of nodes, which correspond to the variables describing the system of interest, and arcs, which correspond to the procedures used to infer the value of a variable from the values of other variables. The value of a variable can be determined at an instant in time, over a time interval or for a series of times. Information about the value of a variable is expressed as a probability density function which quantifies the likelihood of each possible value. The inferencing process uses these probability density functions to propagate uncertainty. NAIVE has been used to develop medical knowledge bases including over 100 variables.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2727",
        "title": "Objective Probability",
        "authors": [
            "Henry E. Kyburg Jr"
        ],
        "abstract": "A distinction is sometimes made between \"statistical\" and \"subjective\" probabilities. This is based on a distinction between \"unique\" events and \"repeatable\" events. We argue that this distinction is untenable, since all events are \"unique\" and all events belong to \"kinds\", and offer a conception of probability for A1 in which (1) all probabilities are based on -- possibly vague -- statistical knowledge, and (2) every statement in the language has a probability. This conception of probability can be applied to very rich languages.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2728",
        "title": "Coefficients of Relations for Probabilistic Reasoning",
        "authors": [
            "Silvio Ursic"
        ],
        "abstract": "Definitions and notations with historical references are given for some numerical coefficients commonly used to quantify relations among collections of objects for the purpose of expressing approximate knowledge and probabilistic reasoning.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2729",
        "title": "Satisfaction of Assumptions is a Weak Predictor of Performance",
        "authors": [
            "Ben P. Wise"
        ],
        "abstract": "This paper demonstrates a methodology for examining the accuracy of uncertain inference systems (UIS), after their parameters have been optimized, and does so for several common UIS's. This methodology may be used to test the accuracy when either the prior assumptions or updating formulae are not exactly satisfied. Surprisingly, these UIS's were revealed to be no more accurate on the average than a simple linear regression. Moreover, even on prior distributions which were deliberately biased so as give very good accuracy, they were less accurate than the simple probabilistic model which assumes marginal independence between inputs. This demonstrates that the importance of updating formulae can outweigh that of prior assumptions. Thus, when UIS's are judged by their final accuracy after optimization, we get completely different results than when they are judged by whether or not their prior assumptions are perfectly satisfied.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2730",
        "title": "Structuring Causal Tree Models with Continuous Variables",
        "authors": [
            "Lei Xu",
            "Judea Pearl"
        ],
        "abstract": "This paper considers the problem of invoking auxiliary, unobservable variables to facilitate the structuring of causal tree models for a given set of continuous variables. Paralleling the treatment of bi-valued variables in [Pearl 1986], we show that if a collection of coupled variables are governed by a joint normal distribution and a tree-structured representation exists, then both the topology and all internal relationships of the tree can be uncovered by observing pairwise dependencies among the observed variables (i.e., the leaves of the tree). Furthermore, the conditions for normally distributed variables are less restrictive than those governing bi-valued variables. The result extends the applications of causal tree models which were found useful in evidential reasoning tasks.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2731",
        "title": "Implementing Evidential Reasoning in Expert Systems",
        "authors": [
            "John Yen"
        ],
        "abstract": "The Dempster-Shafer theory has been extended recently for its application to expert systems. However, implementing the extended D-S reasoning model in rule-based systems greatly complicates the task of generating informative explanations. By implementing GERTIS, a prototype system for diagnosing rheumatoid arthritis, we show that two kinds of knowledge are essential for explanation generation: (l) taxonomic class relationships between hypotheses and (2) pointers to the rules that significantly contribute to belief in the hypothesis. As a result, the knowledge represented in GERTIS is richer and more complex than that of conventional rule-based systems. GERTIS not only demonstrates the feasibility of rule-based evidential-reasoning systems, but also suggests ways to generate better explanations, and to explicitly represent various useful relationships among hypotheses and rules.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2732",
        "title": "Decision Tree Induction Systems: A Bayesian Analysis",
        "authors": [
            "Wray L. Buntine"
        ],
        "abstract": "Decision tree induction systems are being used for knowledge acquisition in noisy domains. This paper develops a subjective Bayesian interpretation of the task tackled by these systems and the heuristic methods they use. It is argued that decision tree systems implicitly incorporate a prior belief that the simpler (in terms of decision tree complexity) of two hypotheses be preferred, all else being equal, and that they perform a greedy search of the space of decision rules to find one in which there is strong posterior belief. A number of improvements to these systems are then suggested.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2733",
        "title": "The Automatic Training of Rule Bases that Use Numerical Uncertainty Representations",
        "authors": [
            "Richard A. Caruana"
        ],
        "abstract": "The use of numerical uncertainty representations allows better modeling of some aspects of human evidential reasoning. It also makes knowledge acquisition and system development, test, and modification more difficult. We propose that where possible, the assignment and/or refinement of rule weights should be performed automatically. We present one approach to performing this training - numerical optimization - and report on the results of some preliminary tests in training rule bases. We also show that truth maintenance can be used to make training more efficient and ask some epistemological questions raised by training rule weights.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2734",
        "title": "The Inductive Logic of Information Systems",
        "authors": [
            "Norman C. Dalkey"
        ],
        "abstract": "An inductive logic can be formulated in which the elements are not propositions or probability distributions, but information systems. The logic is complete for information systems with binary hypotheses, i.e., it applies to all such systems. It is not complete for information systems with more than two hypotheses, but applies to a subset of such systems. The logic is inductive in that conclusions are more informative than premises. Inferences using the formalism have a strong justification in terms of the expected value of the derived information system.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2735",
        "title": "Automated Generation of Connectionist Expert Systems for Problems Involving Noise and Redundancy",
        "authors": [
            "Stephen I. Gallant"
        ],
        "abstract": "When creating an expert system, the most difficult and expensive task is constructing a knowledge base. This is particularly true if the problem involves noisy data and redundant measurements. This paper shows how to modify the MACIE process for generating connectionist expert systems from training examples so that it can accommodate noisy and redundant data. The basic idea is to dynamically generate appropriate training examples by constructing both a 'deep' model and a noise model for the underlying problem. The use of winner-take-all groups of variables is also discussed. These techniques are illustrated with a small example that would be very difficult for standard expert system approaches.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2736",
        "title": "The Recovery of Causal Poly-Trees from Statistical Data",
        "authors": [
            "George Rebane",
            "Judea Pearl"
        ],
        "abstract": "Poly-trees are singly connected causal networks in which variables may arise from multiple causes. This paper develops a method of recovering ply-trees from empirically measured probability distributions of pairs of variables. The method guarantees that, if the measured distributions are generated by a causal process structured as a ply-tree then the topological structure of such tree can be recovered precisely and, in addition, the causal directionality of the branches can be determined up to the maximum extent possible. The method also pinpoints the minimum (if any) external semantics required to determine the causal relationships among the variables considered.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2737",
        "title": "A Heuristic Bayesian Approach to Knowledge Acquisition: Application to Analysis of Tissue-Type Plasminogen Activator",
        "authors": [
            "Ross D. Shachter",
            "David M. Eddy",
            "Vic Hasselblad",
            "Robert Wolpert"
        ],
        "abstract": "This paper describes a heuristic Bayesian method for computing probability distributions from experimental data, based upon the multivariate normal form of the influence diagram. An example illustrates its use in medical technology assessment. This approach facilitates the integration of results from different studies, and permits a medical expert to make proper assessments without considerable statistical training.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2738",
        "title": "Theory-Based Inductive Learning: An Integration of Symbolic and Quantitative Methods",
        "authors": [
            "Spencer Star"
        ],
        "abstract": "The objective of this paper is to propose a method that will generate a causal explanation of observed events in an uncertain world and then make decisions based on that explanation. Feedback can cause the explanation and decisions to be modified. I call the method Theory-Based Inductive Learning (T-BIL). T-BIL integrates deductive learning, based on a technique called Explanation-Based Generalization (EBG) from the field of machine learning, with inductive learning methods from Bayesian decision theory. T-BIL takes as inputs (1) a decision problem involving a sequence of related decisions over time, (2) a training example of a solution to the decision problem in one period, and (3) the domain theory relevant to the decision problem. T-BIL uses these inputs to construct a probabilistic explanation of why the training example is an instance of a solution to one stage of the sequential decision problem. This explanation is then generalized to cover a more general class of instances and is used as the basis for making the next-stage decisions. As the outcomes of each decision are observed, the explanation is revised, which in turn affects the subsequent decisions. A detailed example is presented that uses T-BIL to solve a very general stochastic adaptive control problem for an autonomous mobile robot.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2739",
        "title": "Using T-Norm Based Uncertainty Calculi in a Naval Situation Assessment Application",
        "authors": [
            "Piero P. Bonissone"
        ],
        "abstract": "RUM (Reasoning with Uncertainty Module), is an integrated software tool based on a KEE, a frame system implemented in an object oriented language. RUM's architecture is composed of three layers: representation, inference, and control. The representation layer is based on frame-like data structures that capture the uncertainty information used in the inference layer and the uncertainty meta-information used in the control layer. The inference layer provides a selection of five T-norm based uncertainty calculi with which to perform the intersection, detachment, union, and pooling of information. The control layer uses the meta-information to select the appropriate calculus for each context and to resolve eventual ignorance or conflict in the information. This layer also provides a context mechanism that allows the system to focus on the relevant portion of the knowledge base, and an uncertain-belief revision system that incrementally updates the certainty values of well-formed formulae (wffs) in an acyclic directed deduction graph.  RUM has been tested and validated in a sequence of experiments in both naval and aerial situation assessment (SA), consisting of correlating reports and tracks, locating and classifying platforms, and identifying intents and threats. An example of naval situation assessment is illustrated. The testbed environment for developing these experiments has been provided by LOTTA, a symbolic simulator implemented in Flavors. This simulator maintains time-varying situations in a multi-player antagonistic game where players must make decisions in light of uncertain and incomplete data. RUM has been used to assist one of the LOTTA players to perform the SA task.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2740",
        "title": "A Study of Associative Evidential Reasoning",
        "authors": [
            "Yizong Cheng",
            "Rangasami L. Kashyap"
        ],
        "abstract": "Evidential reasoning is cast as the problem of simplifying the evidence-hypothesis relation and constructing combination formulas that possess certain testable properties. Important classes of evidence as identifiers, annihilators, and idempotents and their roles in determining binary operations on intervals of reals are discussed. The appropriate way of constructing formulas for combining evidence and their limitations, for instance, in robustness, are presented.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2741",
        "title": "A Measure-Free Approach to Conditioning",
        "authors": [
            "I. R. Goodman"
        ],
        "abstract": "In an earlier paper, a new theory of measurefree \"conditional\" objects was presented. In this paper, emphasis is placed upon the motivation of the theory. The central part of this motivation is established through an example involving a knowledge-based system. In order to evaluate combination of evidence for this system, using observed data, auxiliary at tribute and diagnosis variables, and inference rules connecting them, one must first choose an appropriate algebraic logic description pair (ALDP): a formal language or syntax followed by a compatible logic or semantic evaluation (or model). Three common choices- for this highly non-unique choice - are briefly discussed, the logics being Classical Logic, Fuzzy Logic, and Probability Logic. In all three,the key operator representing implication for the inference rules is interpreted as the often-used disjunction of a negation (b => a) = (b'v a), for any events a,b.\n",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2742",
        "title": "Convergent Deduction for Probabilistic Logic",
        "authors": [
            "Peter Haddawy",
            "Alan M. Frisch"
        ],
        "abstract": "This paper discusses the semantics and proof theory of Nilsson's probabilistic logic, outlining both the benefits of its well-defined model theory and the drawbacks of its proof theory. Within Nilsson's semantic framework, we derive a set of inference rules which are provably sound. The resulting proof system, in contrast to Nilsson's approach, has the important feature of convergence - that is, the inference process proceeds by computing increasingly narrow probability intervals which converge from above and below on the smallest entailed probability interval. Thus the procedure can be stopped at any time to yield partial information concerning the smallest entailed interval.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2744",
        "title": "A Knowledge Engineer's Comparison of Three Evidence Aggregation Methods",
        "authors": [
            "Donald H. Mitchell",
            "Steven A. Harp",
            "David K. Simkin"
        ],
        "abstract": "The comparisons of uncertainty calculi from the last two Uncertainty Workshops have all used theoretical probabilistic accuracy as the sole metric. While mathematical correctness is important, there are other factors which should be considered when developing reasoning systems. These other factors include, among other things, the error in uncertainty measures obtainable for the problem and the effect of this error on the performance of the resulting system.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2745",
        "title": "Towards Solving the Multiple Extension Problem: Combining Defaults and Probabilities",
        "authors": [
            "Eric Neufeld",
            "David L Poole"
        ],
        "abstract": "The multiple extension problem arises frequently in diagnostic and default inference. That is, we can often use any of a number of sets of defaults or possible hypotheses to explain observations or make Predictions. In default inference, some extensions seem to be simply wrong and we use qualitative techniques to weed out the unwanted ones. In the area of diagnosis, however, the multiple explanations may all seem reasonable, however improbable. Choosing among them is a matter of quantitative preference. Quantitative preference works well in diagnosis when knowledge is modelled causally. Here we suggest a framework that combines probabilities and defaults in a single unified framework that retains the semantics of diagnosis as construction of explanations from a fixed set of possible hypotheses. We can then compute probabilities incrementally as we construct explanations. Here we describe a branch and bound algorithm that maintains a set of all partial explanations while exploring a most promising one first. A most probable explanation is found first if explanations are partially ordered.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2746",
        "title": "Problem Structure and Evidential Reasoning",
        "authors": [
            "Richard M. Tong",
            "Lee A. Appelbaum"
        ],
        "abstract": "In our previous series of studies to investigate the role of evidential reasoning in the RUBRIC system for full-text document retrieval (Tong et al., 1985; Tong and Shapiro, 1985; Tong and Appelbaum, 1987), we identified the important role that problem structure plays in the overall performance of the system. In this paper, we focus on these structural elements (which we now call \"semantic structure\") and show how explicit consideration of their properties reduces what previously were seen as difficult evidential reasoning problems to more tractable questions.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2747",
        "title": "The Role of Calculi in Uncertain Inference Systems",
        "authors": [
            "Michael P. Wellman",
            "David Heckerman"
        ],
        "abstract": "Much of the controversy about methods for automated decision making has focused on specific calculi for combining beliefs or propagating uncertainty. We broaden the debate by (1) exploring the constellation of secondary tasks surrounding any primary decision problem, and (2) identifying knowledge engineering concerns that present additional representational tradeoffs. We argue on pragmatic grounds that the attempt to support all of these tasks within a single calculus is misguided. In the process, we note several uncertain reasoning objectives that conflict with the Bayesian ideal of complete specification of probabilities and utilities. In response, we advocate treating the uncertainty calculus as an object language for reasoning mechanisms that support the secondary tasks. Arguments against Bayesian decision theory are weakened when the calculus is relegated to this role. Architectures for uncertainty handling that take statements in the calculus as objects to be reasoned about offer the prospect of retaining normative status with respect to decision making while supporting the other tasks in uncertain reasoning.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2748",
        "title": "The Role of Tuning Uncertain Inference Systems",
        "authors": [
            "Ben P. Wise",
            "Bruce M. Perrin",
            "David S. Vaughan",
            "Robert M. Yadrick"
        ],
        "abstract": "This study examined the effects of \"tuning\" the parameters of the incremental function of MYCIN, the independent function of PROSPECTOR, a probability model that assumes independence, and a simple additive linear equation. me parameters of each of these models were optimized to provide solutions which most nearly approximated those from a full probability model for a large set of simple networks. Surprisingly, MYCIN, PROSPECTOR, and the linear equation performed equivalently; the independence model was clearly more accurate on the networks studied.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2750",
        "title": "Implementing a Bayesian Scheme for Revising Belief Commitments",
        "authors": [
            "Lashon B. Booker",
            "Naveen Hota",
            "Gavin Hemphill"
        ],
        "abstract": "Our previous work on classifying complex ship images [1,2] has evolved into an effort to develop software tools for building and solving generic classification problems. Managing the uncertainty associated with feature data and other evidence is an important issue in this endeavor. Bayesian techniques for managing uncertainty [7,12,13] have proven to be useful for managing several of the belief maintenance requirements of classification problem solving. One such requirement is the need to give qualitative explanations of what is believed. Pearl [11] addresses this need by computing what he calls a belief commitment-the most probable instantiation of all hypothesis variables given the evidence available. Before belief commitments can be computed, the straightforward implementation of Pearl's procedure involves finding an analytical solution to some often difficult optimization problems. We describe an efficient implementation of this procedure using tensor products that solves these problems enumeratively and avoids the need for case by case analysis. The procedure is thereby made more practical to use in the general case.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2751",
        "title": "Integrating Logical and Probabilistic Reasoning for Decision Making",
        "authors": [
            "John S. Breese",
            "Edison Tse"
        ],
        "abstract": "We describe a representation and a set of inference methods that combine logic programming techniques with probabilistic network representations for uncertainty (influence diagrams). The techniques emphasize the dynamic construction and solution of probabilistic and decision-theoretic models for complex and uncertain domains. Given a query, a logical proof is produced if possible; if not, an influence diagram based on the query and the knowledge of the decision domain is produced and subsequently solved. A uniform declarative, first-order, knowledge representation is combined with a set of integrated inference procedures for logical, probabilistic, and decision-theoretic reasoning.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2752",
        "title": "Compiling Fuzzy Logic Control Rules to Hardware Implementations",
        "authors": [
            "Stephen Chiu",
            "Masaki Togai"
        ],
        "abstract": "A major aspect of human reasoning involves the use of approximations. Particularly in situations where the decision-making process is under stringent time constraints, decisions are based largely on approximate, qualitative assessments of the situations. Our work is concerned with the application of approximate reasoning to real-time control. Because of the stringent processing speed requirements in such applications, hardware implementations of fuzzy logic inferencing are being pursued. We describe a programming environment for translating fuzzy control rules into hardware realizations. Two methods of hardware realizations are possible. The First is based on a special purpose chip for fuzzy inferencing. The second is based on a simple memory chip. The ability to directly translate a set of decision rules into hardware implementations is expected to make fuzzy control an increasingly practical approach to the control of complex systems.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2753",
        "title": "Steps Towards Programs that Manage Uncertainty",
        "authors": [
            "Paul Cohen"
        ],
        "abstract": "Reasoning under uncertainty in Al hats come to mean assessing the credibility of hypotheses inferred from evidence. But techniques for assessing credibility do not tell a problem solver what to do when it is uncertain. This is the focus of our current research. We have developed a medical expert system called MUM, for Managing Uncertainty in Medicine, that plans diagnostic sequences of questions, tests, and treatments. This paper describes the kinds of problems that MUM was designed to solve and gives a brief description of its architecture. More recently, we have built an empty version of MUM called MU, and used it to reimplement MUM and a small diagnostic system for plant pathology. The latter part of the paper describes the features of MU that make it appropriate for building expert systems that manage uncertainty.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2754",
        "title": "An Algorithm for Computing Probabilistic Propositions",
        "authors": [
            "Gregory F. Cooper"
        ],
        "abstract": "A method for computing probabilistic propositions is presented. It assumes the availability of a single external routine for computing the probability of one instantiated variable, given a conjunction of other instantiated variables. In particular, the method allows belief network algorithms to calculate general probabilistic propositions over nodes in the network. Although in the worst case the time complexity of the method is exponential in the size of a query, it is polynomial in the size of a number of common types of queries.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2755",
        "title": "Combining Symbolic and Numeric Approaches to Uncertainty Management",
        "authors": [
            "Bruce D'Ambrosio"
        ],
        "abstract": "A complete approach to reasoning under uncertainty requires support for incremental and interactive formulation and revision of, as well as reasoning with, models of the problem domain capable of representing our uncertainty. We present a hybrid reasoning scheme which combines symbolic and numeric methods for uncertainty management to provide efficient and effective support for each of these tasks. The hybrid is based on symbolic techniques adapted from Assumption-based Truth Maintenance systems (ATMS), combined with numeric methods adapted from the Dempster/Shafer theory of evidence, as extended in Baldwin's Support Logic Programming system. The hybridization is achieved by viewing an ATMS as a symbolic algebra system for uncertainty calculations. This technique has several major advantages over conventional methods for performing inference with numeric certainty estimates in addition to the ability to dynamically determine hypothesis spaces, including improved management of dependent and partially independent evidence, faster run-time evaluation of propositional certainties, the ability to query the certainty value of a proposition from multiple perspectives, and the ability to incrementally extend or revise domain models.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2756",
        "title": "Explanation of Probabilistic Inference for Decision Support Systems",
        "authors": [
            "Christopher Elsaesser"
        ],
        "abstract": "An automated explanation facility for Bayesian conditioning aimed at improving user acceptance of probability-based decision support systems has been developed. The domain-independent facility is based on an information processing perspective on reasoning about conditional evidence that accounts both for biased and normative inferences. Experimental results indicate that the facility is both acceptable to naive users and effective in improving understanding.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2758",
        "title": "Efficient Inference on Generalized Fault Diagrams",
        "authors": [
            "Ross D. Shachter",
            "Leonard Bertrand"
        ],
        "abstract": "The generalized fault diagram, a data structure for failure analysis based on the influence diagram, is defined. Unlike the fault tree, this structure allows for dependence among the basic events and replicated logical elements. A heuristic procedure is developed for efficient processing of these structures.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2759",
        "title": "Reasoning About Beliefs and Actions Under Computational Resource Constraints",
        "authors": [
            "Eric J. Horvitz"
        ],
        "abstract": "Although many investigators affirm a desire to build reasoning systems that behave consistently with the axiomatic basis defined by probability theory and utility theory, limited resources for engineering and computation can make a complete normative analysis impossible. We attempt to move discussion beyond the debate over the scope of problems that can be handled effectively to cases where it is clear that there are insufficient computational resources to perform an analysis deemed as complete. Under these conditions, we stress the importance of considering the expected costs and benefits of applying alternative approximation procedures and heuristics for computation and knowledge acquisition. We discuss how knowledge about the structure of user utility can be used to control value tradeoffs for tailoring inference to alternative contexts. We address the notion of real-time rationality, focusing on the application of knowledge about the expected timewise-refinement abilities of reasoning strategies to balance the benefits of additional computation with the costs of acting with a partial result. We discuss the benefits of applying decision theory to control the solution of difficult problems given limitations and uncertainty in reasoning resources.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2760",
        "title": "Advantages and a Limitation of Using LEG Nets in a Real-TIme Problem",
        "authors": [
            "Thomas Slack"
        ],
        "abstract": "After experimenting with a number of non-probabilistic methods for dealing with uncertainty many researchers reaffirm a preference for probability methods [1] [2], although this remains controversial. The importance of being able to form decisions from incomplete data in diagnostic problems has highlighted probabilistic methods [5] which compute posterior probabilities from prior distributions in a way similar to Bayes Rule, and thus are called Bayesian methods. This paper documents the use of a Bayesian method in a real time problem which is similar to medical diagnosis in that there is a need to form decisions and take some action without complete knowledge of conditions in the problem domain. This particular method has a limitation which is discussed.\n    ",
        "submission_date": "2013-03-28T00:00:00",
        "last_modified_date": "2013-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2797",
        "title": "Logical Fuzzy Preferences",
        "authors": [
            "Emad Saad"
        ],
        "abstract": "We present a unified logical framework for representing and reasoning about both quantitative and qualitative preferences in fuzzy answer set programming, called fuzzy answer set optimization programs. The proposed framework is vital to allow defining quantitative preferences over the possible outcomes of qualitative preferences. We show the application of fuzzy answer set optimization programs to the course scheduling with fuzzy preferences problem. To the best of our knowledge, this development is the first to consider a logical framework for reasoning about quantitative preferences, in general, and reasoning about both quantitative and qualitative preferences in particular.\n    ",
        "submission_date": "2013-04-05T00:00:00",
        "last_modified_date": "2013-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2799",
        "title": "Nested Aggregates in Answer Sets: An Application to a Priori Optimization",
        "authors": [
            "Emad Saad"
        ],
        "abstract": "We allow representing and reasoning in the presence of nested multiple aggregates over multiple variables and nested multiple aggregates over functions involving multiple variables in answer sets, precisely, in answer set optimization programming and in answer set programming. We show the applicability of the answer set optimization programming with nested multiple aggregates and the answer set programming with nested multiple aggregates to the Probabilistic Traveling Salesman Problem, a fundamental a priori optimization problem in Operation Research.\n    ",
        "submission_date": "2013-04-05T00:00:00",
        "last_modified_date": "2013-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3075",
        "title": "Application of Evidential Reasoning to Helicopter Flight Path Control",
        "authors": [
            "Shoshana Abel"
        ],
        "abstract": "This paper presents a methodology for research and development of the inferencing and knowledge representation aspects of an Expert System approach for performing reasoning under uncertainty in support of a real time vehicle guidance and navigation system. Such a system could be of major benefit for non-terrain following low altitude flight systems operating in foreign hostile environments such as might be experienced by NOE helicopter or similar mission craft. An innovative extension of the evidential reasoning methodology, termed the Sum-and-Lattice-Points Method, has been developed. The research and development effort presented in this paper consists of a formal mathematical development of the Sum-and-Lattice-Points Method, its formulation and representation in a parallel environment, prototype software development of the method within an expert system, and initial testing of the system within the confines of the vehicle guidance system.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3076",
        "title": "Knowledge Engineering Within A Generalized Bayesian Framework",
        "authors": [
            "Stephen W. Barth",
            "Steven W. Norton"
        ],
        "abstract": "During the ongoing debate over the representation of uncertainty in Artificial Intelligence, Cheeseman, Lemmer, Pearl, and others have argued that probability theory, and in particular the Bayesian theory, should be used as the basis for the inference mechanisms of Expert Systems dealing with uncertainty. In order to pursue the issue in a practical setting, sophisticated tools for knowledge engineering are needed that allow flexible and understandable interaction with the underlying knowledge representation schemes. This paper describes a Generalized Bayesian framework for building expert systems which function in uncertain domains, using algorithms proposed by Lemmer. It is neither rule-based nor frame-based, and requires a new system of knowledge engineering tools. The framework we describe provides a knowledge-based system architecture with an inference engine, explanation capability, and a unique aid for building consistent knowledge bases.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3077",
        "title": "Taxonomy, Structure, and Implementation of Evidential Reasoning",
        "authors": [
            "Moshe Ben-Bassat"
        ],
        "abstract": "The fundamental elements of evidential reasoning problems are described, followed by a discussion of the structure of various types of problems. Bayesian inference networks and state space formalism are used as the tool for problem representation.\n",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3078",
        "title": "Probabilistic Reasoning About Ship Images",
        "authors": [
            "Lashon B. Booker",
            "Naveen Hota"
        ],
        "abstract": "One of the most important aspects of current expert systems technology is the ability to make causal inferences about the impact of new evidence. When the domain knowledge and problem knowledge are uncertain and incomplete Bayesian reasoning has proven to be an effective way of forming such inferences [3,4,8]. While several reasoning schemes have been developed based on Bayes Rule, there has been very little work examining the comparative effectiveness of these schemes in a real application. This paper describes a knowledge based system for ship classification [1], originally developed using the PROSPECTOR updating method [2], that has been reimplemented to use the inference procedure developed by Pearl and Kim [4,5]. We discuss our reasons for making this change, the implementation of the new inference engine, and the comparative performance of the two versions of the system.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3079",
        "title": "Towards The Inductive Acquisition of Temporal Knowledge",
        "authors": [
            "Kaihu Chen"
        ],
        "abstract": "The ability to predict the future in a given domain can be acquired by discovering empirically from experience certain temporal patterns that tend to repeat unerringly. Previous works in time series analysis allow one to make quantitative predictions on the likely values of certain linear variables. Since certain types of knowledge are better expressed in symbolic forms, making qualitative predictions based on symbolic representations require a different approach. A domain independent methodology called TIM (Time based Inductive Machine) for discovering potentially uncertain temporal patterns from real time observations using the technique of inductive inference is described here.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3080",
        "title": "Some Extensions of Probabilistic Logic",
        "authors": [
            "Su-shing Chen"
        ],
        "abstract": "In [12], Nilsson proposed the probabilistic logic in which the truth values of logical propositions are probability values  between 0 and 1. It is applicable to any logical system for which the consistency of a finite set of propositions can be established. The probabilistic  inference scheme reduces to the ordinary logical inference when the probabilities of all propositions are either 0 or 1. This logic has the same  limitations of other probabilistic reasoning systems of the Bayesian approach. For common sense reasoning, consistency is not a very natural  assumption. We have some well known examples: {Dick is a Quaker, Quakers are pacifists, Republicans are not pacifists, Dick is a Republican}and  {Tweety is a bird, birds can fly, Tweety is a penguin}.  In this paper, we shall propose some extensions of the probabilistic logic. In the second  section, we shall consider the space of all interpretations, consistent or not. In terms of frames of discernment, the basic probability assignment  (bpa) and belief function can be defined. Dempster's combination rule is applicable. This extension of probabilistic logic is called the evidential  logic in [ 1]. For each proposition s, its belief function is represented by an interval [Spt(s), Pls(s)]. When all such intervals collapse to single  points, the evidential logic reduces to probabilistic logic (in the generalized version of not necessarily consistent interpretations). Certainly, we  get Nilsson's probabilistic logic by further restricting to consistent interpretations. In the third section, we shall give a probabilistic  interpretation of probabilistic logic in terms of multi-dimensional random variables. This interpretation brings the probabilistic logic into the  framework of probability theory. Let us consider a finite set S = {sl, s2, ..., Sn) of logical propositions. Each proposition may have true or false  values; and may be considered as a random variable. We have a probability distribution for each proposition. The e-dimensional random variable (sl,...,  Sn) may take values in the space of all interpretations of 2n binary vectors. We may compute absolute (marginal), conditional and joint probability  distributions. It turns out that the permissible probabilistic interpretation vector of Nilsson [12] consists of the joint probabilities of S.  Inconsistent interpretations will not appear, by setting their joint probabilities to be zeros. By summing appropriate joint probabilities, we get  probabilities of individual propositions or subsets of propositions. Since the Bayes formula and other techniques are valid for e-dimensional random  variables, the probabilistic logic is actually very close to the Bayesian inference schemes. In the last section, we shall consider a relaxation scheme  for probabilistic logic. In this system, not only new evidences will update the belief measures of a collection of propositions, but also constraint  satisfaction among these propositions in the relational network will revise these measures. This mechanism is similar to human reasoning which is an  evaluative process converging to the most satisfactory result. The main idea arises from the consistent labeling problem in computer vision. This  method is originally applied to scene analysis of line drawings. Later, it is applied to matching, constraint satisfaction and multi sensor fusion by  several authors [8], [16] (and see references cited there). Recently, this method is used in knowledge aggregation by Landy and Hummel [9].\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3081",
        "title": "Predicting The Performance of Minimax and Product in Game-Tree",
        "authors": [
            "Ping-Chung Chi",
            "Dana Nau"
        ],
        "abstract": "The discovery that the minimax decision rule performs poorly in some games has sparked interest in possible alternatives to minimax. Until recently, the only games in which minimax was known to perform poorly were games which were mainly of theoretical interest. However, this paper reports results showing poor performance of minimax in a more common game called kalah. For the kalah games tested, a non-minimax decision rule called the product rule performs significantly better than minimax.\n",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3082",
        "title": "Reasoning With Uncertain Knowledge",
        "authors": [
            "A. Julian Craddock",
            "Roger A. Browse"
        ],
        "abstract": "A model of knowledge representation is described in which propositional facts and the relationships among them can be supported by other facts. The set of knowledge which can be supported is called the set of cognitive units, each having associated descriptions of their explicit and implicit support structures, summarizing belief and reliability of belief. This summary is precise enough to be useful in a computational model while remaining descriptive of the underlying symbolic support structure. When a fact supports another supportive relationship between facts we call this meta-support. This facilitates reasoning about both the propositional knowledge. and the support structures underlying it.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3083",
        "title": "Models vs. Inductive Inference for Dealing With Probabilistic Knowledge",
        "authors": [
            "Norman C. Dalkey"
        ],
        "abstract": "Two different approaches to dealing with probabilistic knowledge are examined -models and inductive inference. Examples of the first are: influence diagrams [1], Bayesian networks [2], log-linear models [3, 4]. Examples of the second are: games-against nature [5, 6] varieties of maximum-entropy methods [7, 8, 9], and the author's min-score induction [10]. In the modeling approach, the basic issue is manageability, with respect to data elicitation and computation. Thus, it is assumed that the pertinent set of users in some sense knows the relevant probabilities, and the problem is to format that knowledge in a way that is convenient to input and store and that allows computation of the answers to current questions in an expeditious fashion. The basic issue for the inductive approach appears at first sight to be very different. In this approach it is presumed that the relevant probabilities are only partially known, and the problem is to extend that incomplete information in a reasonable way to answer current questions. Clearly, this approach requires that some form of induction be invoked. Of course, manageability is an important additional concern. Despite their seeming differences, the two approaches have a fair amount in common, especially with respect to the structural framework they employ. Roughly speaking, this framework involves identifying clusters of variables which strongly interact, establishing marginal probability distributions on the clusters, and extending the subdistributions to a more complete distribution, usually via a product formalism. The product extension is justified on the modeling approach in terms of assumed conditional independence; in the inductive approach the product form arises from an inductive rule.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3084",
        "title": "Towards a General-Purpose Belief Maintenance System",
        "authors": [
            "Brian Falkenhainer"
        ],
        "abstract": "There currently exists a gap between the theories proposed by the probability and uncertainty and the needs of Artificial  Intelligence research. These theories primarily address the needs of expert systems, using knowledge structures which must be pre-compiled and remain  static in structure during runtime. Many Al systems require the ability to dynamically add and remove parts of the current knowledge structure (e.g.,  in order to examine what the world would be like for different causal theories). This requires more flexibility than existing uncertainty systems  display. In addition, many Al researchers are only interested in using \"probabilities\" as a means of obtaining an ordering, rather than attempting to  derive an accurate probabilistic account of a situation. This indicates the need for systems which stress ease of use and don't require extensive  probability information when one cannot (or doesn't wish to) provide such information. This paper attempts to help reconcile the gap between  approaches to uncertainty and the needs of many AI systems by examining the control issues which arise, independent of a particular uncertainty  calculus. when one tries to satisfy these needs. Truth Maintenance Systems have been used extensively in problem solving tasks to help organize a set  of facts and detect inconsistencies in the believed state of the world. These systems maintain a set of true/false propositions and their associated  dependencies. However, situations often arise in which we are unsure of certain facts or in which the conclusions we can draw from available  information are somewhat uncertain. The non-monotonic TMS 12] was an attempt at reasoning when all the facts are not known, but it fails to take into  account degrees of belief and how available evidence can combine to strengthen a particular belief. This paper addresses the problem of probabilistic  reasoning as it applies to Truth Maintenance Systems. It describes a belief Maintenance System that manages a current set of beliefs in much the same  way that a TMS manages a set of true/false propositions. If the system knows that belief in fact is dependent in some way upon belief in fact2, then it  automatically modifies its belief in facts when new information causes a change in belief of fact2. It models the behavior of a TMS, replacing its  3-valued logic (true, false, unknown) with an infinite valued logic, in such a way as to reduce to a standard TMS if all statements are given in  absolute true/false terms. Belief Maintenance Systems can, therefore, be thought of as a generalization of Truth Maintenance Systems, whose possible  reasoning tasks are a superset of those for a TMS.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3085",
        "title": "Planning, Scheduling, and Uncertainty in the Sequence of Future Events",
        "authors": [
            "B. R. Fox",
            "Karl G. Kempf"
        ],
        "abstract": "Scheduling in the factory setting is compounded by computational complexity and temporal uncertainty. Together, these two factors guarantee that the process of constructing an optimal schedule will be costly and the chances of executing that schedule will be slight. Temporal uncertainty in the task execution time can be offset by several methods: eliminate uncertainty by careful engineering, restore certainty whenever it is lost, reduce the uncertainty by using more accurate sensors, and quantify and circumscribe the remaining uncertainty. Unfortunately, these methods focus exclusively on the sources of uncertainty and fail to apply knowledge of the tasks which are to be scheduled. A complete solution must adapt the schedule of activities to be performed according to the evolving state of the production world. The example of vision-directed assembly is presented to illustrate that the principle of least commitment, in the creation of a plan, in the representation of a schedule, and in the execution of a schedule, enables a robot to operate intelligently and efficiently, even in the presence of considerable uncertainty in the sequence of future events.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3086",
        "title": "Deriving And Combining Continuous Possibility Functions in the Framework of Evidential Reasoning",
        "authors": [
            "Pascal Fua"
        ],
        "abstract": "To develop an approach to utilizing continuous statistical information within the Dempster- Shafer framework, we combine methods proposed by Strat and by Shafero We first derive continuous possibility and mass functions from probability-density functions. Then we propose a rule for combining such evidence that is simpler and more efficiently computed than Dempster's rule. We discuss the relationship between Dempster's rule and our proposed rule for combining evidence over continuous frames.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3087",
        "title": "Non-Monotonicity in Probabilistic Reasoning",
        "authors": [
            "Benjamin N. Grosof"
        ],
        "abstract": "We start by defining an approach to non-monotonic probabilistic reasoning in terms of non-monotonic categorical (true-false) reasoning. We identify a type of non-monotonic probabilistic reasoning, akin to default inheritance, that is commonly found in practice, especially in \"evidential\" and \"Bayesian\" reasoning. We formulate this in terms of the Maximization of Conditional Independence (MCI), and identify a variety of applications for this sort of default. We propose a formalization using Pointwise Circumscription. We compare MCI to Maximum Entropy, another kind of non-monotonic principle, and conclude by raising a number of open questions\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3089",
        "title": "Flexible Interpretations: A Computational Model for Dynamic Uncertainty Assessment",
        "authors": [
            "Shohara L. Hardt"
        ],
        "abstract": "The investigations reported in this paper center on the process of dynamic uncertainty assessment during interpretation tasks in real domain. In particular, we are interested here in the nature of the control structure of computer programs that can support multiple interpretation and smooth transitions between them, in real time. Each step of the processing involves the interpretation of one input item and the appropriate re-establishment of the system's confidence of the correctness of its interpretation(s).\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3090",
        "title": "The Myth of Modularity in Rule-Based Systems",
        "authors": [
            "David Heckerman",
            "Eric J. Horvitz"
        ],
        "abstract": "In this paper, we examine the concept of modularity, an often cited advantage of the ruled-based representation methodology. We argue that the notion of modularity consists of two distinct concepts which we call syntactic modularity and semantic modularity. We argue that when reasoning under certainty, it is reasonable to regard the rule-based approach as both syntactically and semantically modular. However, we argue that in the case of plausible reasoning, rules are syntactically modular but are rarely semantically modular. To illustrate this point, we examine a particular approach for managing uncertainty in rule-based systems called the MYCIN certainty factor model. We formally define the concept of semantic modularity with respect to the certainty factor model and discuss logical consequences of the definition. We show that the assumption of semantic modularity imposes strong restrictions on rules in a knowledge base. We argue that such restrictions are rarely valid in practical applications. Finally, we suggest how the concept of semantic modularity can be relaxed in a manner that makes it appropriate for plausible reasoning.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3091",
        "title": "An Axiomatic Framework for Belief Updates",
        "authors": [
            "David Heckerman"
        ],
        "abstract": "In the 1940's, a physicist named Cox provided the first formal justification for the axioms of probability based on the subjective or Bayesian interpretation. He showed that if a measure of belief satisfies several fundamental properties, then the measure must be some monotonic transformation of a probability. In this paper, measures of change in belief or belief updates are examined. In the spirit of Cox, properties for a measure of change in belief are enumerated. It is shown that if a measure satisfies these properties, it must satisfy other restrictive conditions. For example, it is shown that belief updates in a probabilistic context must be equal to some monotonic transformation of a likelihood ratio. It is hoped that this formal explication of the belief update paradigm will facilitate critical discussion and useful extensions of the approach.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3092",
        "title": "Imprecise Meanings as a Cause of Uncertainty in Medical Knowledge-Based Systems",
        "authors": [
            "Steven J. Henkind"
        ],
        "abstract": "There has been a considerable amount of work on uncertainty in knowledge-based systems. This work has generally been concerned with uncertainty arising from the strength of inferences and the weight of evidence. In this paper we discuss another type of uncertainty: that which is due to imprecision in the underlying primitives used to represent the knowledge of the system. In particular, a given word may denote many similar but not identical entities. Such words are said to be lexically imprecise. Lexical imprecision has caused widespread problems in many areas. Unless this phenomenon is recognized and appropriately handled, it can degrade the performance of knowledge-based systems. In particular, it can lead to difficulties with the user interface, and with the inferencing processes of these systems. Some techniques are suggested for coping with this phenomenon.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3093",
        "title": "Evidence as Opinions of Experts",
        "authors": [
            "Robert Hummel",
            "Michael Landy"
        ],
        "abstract": "We describe a viewpoint on the Dempster/Shafer 'Theory of Evidence', and provide an interpretation which regards the combination formulas as statistics of the opinions of \"experts\". This is done by introducing spaces with binary operations that are simpler to interpret or simpler to implement than the standard combination formula, and showing that these spaces can be mapped homomorphically onto the Dempster/Shafer theory of evidence space. The experts in the space of \"opinions of experts\" combine information in a Bayesian fashion. We present alternative spaces for the combination of evidence suggested by this viewpoint.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3094",
        "title": "Decision Under Uncertainty in Diagnosis",
        "authors": [
            "Charles I. Kalme"
        ],
        "abstract": "This paper describes the incorporation of uncertainty in diagnostic reasoning based on the set covering model of Reggia et. al. extended to what in the Artificial Intelligence dichotomy between deep and compiled (shallow, surface) knowledge based diagnosis may be viewed as the generic form at the compiled end of the spectrum. A major undercurrent in this is advocating the need for a strong underlying model and an integrated set of support tools for carrying such a model in order to deal with uncertainty.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3095",
        "title": "Knowledge and Uncertainty",
        "authors": [
            "Henry E. Kyburg Jr"
        ],
        "abstract": "One purpose -- quite a few thinkers would say the main purpose -- of seeking knowledge about the world is to enhance our ability to make good decisions. An item of knowledge that can make no conceivable difference with regard to anything we might do would strike many as frivolous. Whether or not we want to be philosophical pragmatists in this strong sense with regard to everything we might want to enquire about, it seems a perfectly appropriate attitude to adopt toward artificial knowledge systems. If is granted that we are ultimately concerned with decisions, then some constraints are imposed on our measures of uncertainty at the level of decision making. If our measure of uncertainty is real-valued, then it isn't hard to show that it must satisfy the classical probability axioms. For example, if an act has a real-valued utility U(E) if the event E obtains, and the same real-valued utility if the denial of E obtains, so that U(E) = U(-E), then the expected utility of that act must be U(E), and that must be the same as the uncertainty-weighted average of the returns of the act, p-U(E) + q-U('E), where p and q represent the uncertainty of E and-E respectively. But then we must have p + q = 1.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3096",
        "title": "An Application of Non-Monotonic Probabilistic Reasoning to Air Force Threat Correlation",
        "authors": [
            "Kathryn Blackmond Laskey",
            "Marvin S. Cohen"
        ],
        "abstract": "Current approaches to expert systems' reasoning under uncertainty fail to capture the iterative revision process characteristic of intelligent human reasoning. This paper reports on a system, called the Non-monotonic Probabilist, or NMP (Cohen, et al., 1985). When its inferences result in substantial conflict, NMP examines and revises the assumptions underlying the inferences until conflict is reduced to acceptable levels. NMP has been implemented in a demonstration computer-based system, described below, which supports threat correlation and in-flight route replanning by Air Force pilots.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3097",
        "title": "Bayesian Inference for Radar Imagery Based Surveillance",
        "authors": [
            "Tod S. Levitt"
        ],
        "abstract": "We are interested in creating an automated or semi-automated system with the capability of taking a set of radar imagery, collection parameters and a priori map and other tactical data, and producing likely interpretations of the possible military situations given the available evidence. This paper is concerned with the problem of the interpretation and computation of certainty or belief in the conclusions reached by such a system.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3098",
        "title": "Evidential Reasoning in Parallel Hierarchical Vision Programs",
        "authors": [
            "Ze-Nian Li",
            "Leonard Uhr"
        ],
        "abstract": "This paper presents an efficient adaptation and application of the Dempster-Shafer theory of evidence, one that can be used effectively in a massively parallel hierarchical system for visual pattern perception. It describes the techniques used, and shows in an extended example how they serve to improve the system's performance as it applies a multiple-level set of processes.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3099",
        "title": "Computing Reference Classes",
        "authors": [
            "Ronald P. Loui"
        ],
        "abstract": "For any system with limited statistical knowledge, the combination of evidence and the interpretation of sampling information require the determination of the right reference class (or of an adequate one). The present note (1) discusses the use of reference classes in evidential reasoning, and (2) discusses implementations of Kyburg's rules for reference classes. This paper contributes the first frank discussion of how much of Kyburg's system is needed to be powerful, how much can be computed effectively, and how much is philosophical fat.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3100",
        "title": "An Uncertainty Management Calculus for Ordering Searches in Distributed Dynamic Databases",
        "authors": [
            "Uttam Mukhopadhyay"
        ],
        "abstract": "MINDS is a distributed system of cooperating query engines that customize, document retrieval for each user in a dynamic environment. It improves its performance and adapts to changing patterns of document distribution by observing system-user interactions and modifying the appropriate certainty factors, which act as search control parameters. It argued here that the uncertainty management calculus must account for temporal precedence, reliability of evidence, degree of support for a proposition, and saturation effects. The calculus presented here possesses these features. Some results obtained with this scheme are discussed.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3101",
        "title": "An Explanation Mechanism for Bayesian Inferencing Systems",
        "authors": [
            "Steven W. Norton"
        ],
        "abstract": "Explanation facilities are a particularly important feature of expert system frameworks. It is an area in which traditional rule-based expert system frameworks have had mixed results. While explanations about control are well handled, facilities are needed for generating better explanations concerning knowledge base content. This paper approaches the explanation problem by examining the effect an event has on a variable of interest within a symmetric Bayesian inferencing system. We argue that any effect measure operating in this context must satisfy certain properties. Such a measure is proposed. It forms the basis for an explanation facility which allows the user of the Generalized Bayesian Inferencing System to question the meaning of the knowledge base. That facility is described in detail.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3102",
        "title": "Distributed Revision of Belief Commitment in Multi-Hypothesis Interpretations",
        "authors": [
            "Judea Pearl"
        ],
        "abstract": "This paper extends the applications of belief-networks to include the revision of belief commitments, i.e., the categorical acceptance of a subset of hypotheses which, together, constitute the most satisfactory explanation of the evidence at hand. A coherent model of non-monotonic reasoning is established and distributed algorithms for belief revision are presented. We show that, in singly connected networks, the most satisfactory explanation can be found in linear time by a message-passing algorithm similar to the one used in belief updating. In multiply-connected networks, the problem may be exponentially hard but, if the network is sparse, topological considerations can be used to render the interpretation task tractable. In general, finding the most probable combination of hypotheses is no more complex than computing the degree of belief for any individual hypothesis. Applications to medical diagnosis are illustrated.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3103",
        "title": "Learning Link-Probabilities in Causal Trees",
        "authors": [
            "Igor Roizer",
            "Judea Pearl"
        ],
        "abstract": "A learning algorithm is presented which given the structure of a causal tree, will estimate its link probabilities by sequential measurements on the leaves only. Internal nodes of the tree represent conceptual (hidden) variables inaccessible to observation. The method described is incremental, local, efficient, and remains robust to measurement imprecisions.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3104",
        "title": "Approximate Deduction in Single Evidential Bodies",
        "authors": [
            "Enrique H. Ruspini"
        ],
        "abstract": "Results on approximate deduction in the context of the calculus of evidence of Dempster-Shafer and the theory of interval probabilities are reported. Approximate conditional knowledge about the truth of conditional propositions was assumed available and expressed as sets of possible values (actually numeric intervals) of conditional probabilities. Under different interpretations of this conditional knowledge, several formulas were produced to integrate unconditioned estimates (assumed given as sets of possible values of unconditioned probabilities) with conditional estimates. These formulas are discussed together with the computational characteristics of the methods derived from them. Of particular importance is one such evidence integration formulation, produced under a belief oriented interpretation, which incorporates both modus ponens and modus tollens inferential mechanisms, allows integration of conditioned and unconditioned knowledge without resorting to iterative or sequential approximations, and produces elementary mass distributions as outputs using similar distributions as inputs.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3105",
        "title": "The Rational and Computational Scope of Probabilistic Rule-Based Expert Systems",
        "authors": [
            "Shimon Schocken"
        ],
        "abstract": "Belief updating schemes in artificial intelligence may be viewed as three dimensional languages, consisting of a syntax (e.g. probabilities or certainty factors), a calculus (e.g. Bayesian or CF combination rules), and a semantics (i.e. cognitive interpretations of competing formalisms). This paper studies the rational scope of those languages on the syntax and calculus grounds. In particular, the paper presents an endomorphism theorem which highlights the limitations imposed by the conditional independence assumptions implicit in the CF calculus. Implications of the theorem to the relationship between the CF and the Bayesian languages and the Dempster-Shafer theory of evidence are presented. The paper concludes with a discussion of some implications on rule-based knowledge engineering in uncertain domains.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3106",
        "title": "A Causal Bayesian Model for the Diagnosis of Appendicitis",
        "authors": [
            "Stanley M. Schwartz",
            "Jonathan Baron",
            "John R. Clarke"
        ],
        "abstract": "The causal Bayesian approach is based on the assumption that effects (e.g., symptoms) that are not conditionally independent with respect to some causal agent (e.g., a disease) are conditionally independent with respect to some intermediate state caused by the agent, (e.g., a pathological condition). This paper describes the development of a causal Bayesian model for the diagnosis of appendicitis. The paper begins with a description of the standard Bayesian approach to reasoning about uncertainty and the major critiques it faces. The paper then lays the theoretical groundwork for the causal extension of the Bayesian approach, and details specific improvements we have developed. The paper then goes on to describe our knowledge engineering and implementation and the results of a test of the system. The paper concludes with a discussion of how the causal Bayesian approach deals with the criticisms of the standard Bayesian model and why it is superior to alternative approaches to reasoning about uncertainty popular in the Al community.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3107",
        "title": "A Backwards View for Assessment",
        "authors": [
            "Ross D. Shachter",
            "David Heckerman"
        ],
        "abstract": "Much artificial intelligence research focuses on the problem of deducing the validity of unobservable propositions or hypotheses from observable evidence.! Many of the knowledge representation techniques designed for this problem encode the relationship between evidence and hypothesis in a directed manner. Moreover, the direction in which evidence is stored is typically from evidence to hypothesis.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3108",
        "title": "DAVID: Influence Diagram Processing System for the Macintosh",
        "authors": [
            "Ross D. Shachter"
        ],
        "abstract": "Influence diagrams are a directed graph representation for uncertainties as probabilities. The graph distinguishes between those variables which are under the control of a decision maker (decisions, shown as rectangles) and those which are not (chances, shown as ovals), as well as explicitly denoting a goal for solution (value, shown as a rounded rectangle.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3109",
        "title": "Propagation of Belief Functions: A Distributed Approach",
        "authors": [
            "Prakash P. Shenoy",
            "Glenn Shafer",
            "Khaled Mellouli"
        ],
        "abstract": "In this paper, we describe a scheme for propagating belief functions in certain kinds of trees using only local computations. This scheme generalizes the computational scheme proposed by Shafer and Logan1 for diagnostic trees of the type studied by Gordon and Shortliffe, and the slightly more general scheme given by Shafer for hierarchical evidence. It also generalizes the scheme proposed by Pearl for Bayesian causal trees (see Shenoy and Shafer).  Pearl's causal trees and Gordon and Shortliffe's diagnostic trees are both ways of breaking the evidence that bears on a large problem down into smaller items of evidence that bear on smaller parts of the problem so that these smaller problems can be dealt with one at a time. This localization of effort is often essential in order to make the process of probability judgment feasible, both for the person who is making probability judgments and for the machine that is combining them. The basic structure for our scheme is a type of tree that generalizes both Pearl's and Gordon and Shortliffe's trees. Trees of this general type permit localized computation in Pearl's sense. They are based on qualitative judgments of conditional independence.  We believe that the scheme we describe here will prove useful in expert systems. It is now clear that the successful propagation of probabilities or certainty factors in expert systems requires much more structure than can be provided in a pure production-system framework. Bayesian schemes, on the other hand, often make unrealistic demands for structure. The propagation of belief functions in trees and more general networks stands on a middle ground where some sensible and useful things can be done.  We would like to emphasize that the basic idea of local computation for propagating probabilities is due to Judea Pearl. It is a very innovative idea; we do not believe that it can be found in the Bayesian literature prior to Pearl's work. We see our contribution as extending the usefulness of Pearl's idea by generalizing it from Bayesian probabilities to belief functions.  In the next section, we give a brief introduction to belief functions. The notions of qualitative independence for partitions and a qualitative Markov tree are introduced in Section III. Finally, in Section IV, we describe a scheme for propagating belief functions in qualitative Markov trees.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3110",
        "title": "Appropriate and Inappropriate Estimation Techniques",
        "authors": [
            "David Sher"
        ],
        "abstract": "Mode {also called MAP} estimation, mean estimation and median estimation are examined here to determine when they can be safely used to derive {posterior) cost minimizing estimates. (These are all Bayes procedures, using the mode. mean. or median of the posterior distribution). It is found that modal estimation only returns cost minimizing estimates when the cost function is 0-t. If the cost function is a function of distance then mean estimation only returns cost minimizing estimates when the cost function is squared distance from the true value and median estimation only returns cost minimizing estimates when the cost function ts the distance from the true value. Results are presented on the goodness or modal estimation with non 0-t cost functions\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3111",
        "title": "Estimating Uncertain Spatial Relationships in Robotics",
        "authors": [
            "Randall Smith",
            "Matthew Self",
            "Peter Cheeseman"
        ],
        "abstract": "In this paper, we describe a representation for spatial information, called the stochastic map, and associated procedures for building it, reading information from it, and revising it incrementally as new information is obtained. The map contains the estimates of relationships among objects in the map, and their uncertainties, given all the available information. The procedures provide a general solution to the problem of estimating uncertain relative spatial relationships. The estimates are probabilistic in nature, an advance over the previous, very conservative, worst-case approaches to the problem. Finally, the procedures are developed in the context of state-estimation and filtering theory, which provides a solid basis for numerous extensions.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3112",
        "title": "A VLSI Design and Implementation for a Real-Time Approximate Reasoning",
        "authors": [
            "Masaki Togai",
            "Hiroyuki Watanabe"
        ],
        "abstract": "The role of inferencing with uncertainty is becoming more important in rule-based expert systems (ES), since knowledge given by a human expert is often uncertain or imprecise. We have succeeded in designing a VLSI chip which can perform an entire inference process based on fuzzy logic. The design of the VLSI fuzzy inference engine emphasizes simplicity, extensibility, and efficiency (operational speed and layout area). It is fabricated in 2.5 um CMOS technology. The inference engine consists of three major components; a rule set memory, an inference processor, and a controller. In this implementation, a rule set memory is realized by a read only memory (ROM). The controller consists of two counters. In the inference processor, one data path is laid out for each rule. The number of the inference rule can be increased adding more data paths to the inference processor. All rules are executed in parallel, but each rule is processed serially. The logical structure of fuzzy inference proposed in the current paper maps nicely onto the VLSI structure. A two-phase nonoverlapping clocking scheme is used. Timing tests indicate that the inference engine can operate at approximately 20.8 MHz. This translates to an execution speed of approximately 80,000 Fuzzy Logical Inferences Per Second (FLIPS), and indicates that the inference engine is suitable for a demanding real-time application. The potential applications include decision-making in the area of command and control for intelligent robot systems, process control, missile and aircraft guidance, and other high performance machines.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3113",
        "title": "A General Purpose Inference Engine for Evidential Reasoning Research",
        "authors": [
            "Richard M. Tong",
            "Lee A. Appelbaum",
            "D. G. Shapiro"
        ],
        "abstract": "The purpose of this paper is to report on the most recent developments in our ongoing investigation of the representation and manipulation of uncertainty in automated reasoning systems. In our earlier studies (Tong and Shapiro, 1985) we described a series of experiments with RUBRIC (Tong et al., 1985), a system for full-text document retrieval, that generated some interesting insights into the effects of choosing among a class of scalar valued uncertainty calculi. [n order to extend these results we have begun a new series of experiments with a larger class of representations and calculi, and to help perform these experiments we have developed a general purpose inference engine.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3114",
        "title": "Generalizing Fuzzy Logic Probabilistic Inferences",
        "authors": [
            "Silvio Ursic"
        ],
        "abstract": "Linear representations for a subclass of boolean symmetric functions selected by a parity condition are shown to constitute a generalization of the linear constraints on probabilities introduced by Boole. These linear constraints are necessary to compute probabilities of events with relations between the. arbitrarily specified with propositional calculus boolean formulas.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3115",
        "title": "Qualitative Probabilistic Networks for Planning Under Uncertainty",
        "authors": [
            "Michael P. Wellman"
        ],
        "abstract": "Bayesian networks provide a probabilistic semantics for qualitative assertions about likelihood. A qualitative reasoner based on an algebra over these assertions can derive further conclusions about the influence of actions. While the conclusions are much weaker than those computed from complete probability distributions, they are still valuable for suggesting potential actions, eliminating obviously inferior plans, identifying important tradeoffs, and explaining probabilistic models.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3116",
        "title": "Experimentally Comparing Uncertain Inference Systems to Probability",
        "authors": [
            "Ben P. Wise"
        ],
        "abstract": "This paper examines the biases and performance of several uncertain inference systems: Mycin, a variant of Mycin. and a simplified version of probability using conditional independence assumptions. We present axiomatic arguments for using Minimum Cross Entropy inference as the best way to do uncertain inference. For Mycin and its variant we found special situations where its performance was very good, but also situations where performance was worse than random guessing, or where data was interpreted as having the opposite of its true import We have found that all three of these systems usually gave accurate results, and that the conditional independence assumptions gave the most robust results. We illustrate how the Importance of biases may be quantitatively assessed and ranked. Considerations of robustness might be a critical factor is selecting UlS's for a given application.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3117",
        "title": "Evaluation of Uncertain Inference Models I: PROSPECTOR",
        "authors": [
            "Robert M. Yadrick",
            "Bruce M. Perrin",
            "David S. Vaughan",
            "Peter D. Holden",
            "Karl G. Kempf"
        ],
        "abstract": "This paper examines the accuracy of the PROSPECTOR model for uncertain reasoning. PROSPECTOR's solutions for a large number of computer-generated inference networks were compared to those obtained from probability theory and minimum cross-entropy calculations. PROSPECTOR's answers were generally accurate for a restricted subset of problems that are consistent with its assumptions. However, even within this subset, we identified conditions under which PROSPECTOR's performance deteriorates.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3118",
        "title": "On Implementing Usual Values",
        "authors": [
            "Ronald R. Yager"
        ],
        "abstract": "In many cases commonsense knowledge consists of knowledge of what is usual. In this paper we develop a system for reasoning with usual information. This system is based upon the fact that these pieces of commonsense information involve both a probabilistic aspect and a granular aspect. We implement this system with the aid of possibility-probability granules.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3119",
        "title": "On the Combinality of Evidence in the Dempster-Shafer Theory",
        "authors": [
            "Lotfi Zadeh",
            "Anca Ralescu"
        ],
        "abstract": "In the current versions of the Dempster-Shafer theory, the only essential restriction on the validity of the rule of combination is that the sources of evidence must be statistically independent. Under this assumption, it is permissible to apply the Dempster-Shafer rule to two or mere distinct probability distributions.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3144",
        "title": "Logical Probability Preferences",
        "authors": [
            "Emad Saad"
        ],
        "abstract": "We present a unified logical framework for representing and reasoning about both probability quantitative and qualitative preferences in probability answer set programming, called probability answer set optimization programs. The proposed framework is vital to allow defining probability quantitative preferences over the possible outcomes of qualitative preferences. We show the application of probability answer set optimization programs to a variant of the well-known nurse restoring problem, called the nurse restoring with probability preferences problem. To the best of our knowledge, this development is the first to consider a logical framework for reasoning about probability quantitative preferences, in general, and reasoning about both probability quantitative and qualitative preferences in particular.\n    ",
        "submission_date": "2013-04-05T00:00:00",
        "last_modified_date": "2013-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3208",
        "title": "From Constraints to Resolution Rules, Part I: Conceptual Framework",
        "authors": [
            "Denis Berthier"
        ],
        "abstract": "Many real world problems naturally appear as constraints satisfaction problems (CSP), for which very efficient algorithms are known. Most of these involve the combination of two techniques: some direct propagation of constraints between variables (with the goal of reducing their sets of possible values) and some kind of structured search (depth-first, breadth-first,...). But when such blind search is not possible or not allowed or when one wants a 'constructive' or a 'pattern-based' solution, one must devise more complex propagation rules instead. In this case, one can introduce the notion of a candidate (a 'still possible' value for a variable). Here, we give this intuitive notion a well defined logical status, from which we can define the concepts of a resolution rule and a resolution theory. In order to keep our analysis as concrete as possible, we illustrate each definition with the well known Sudoku example. Part I proposes a general conceptual framework based on first order logic; with the introduction of chains and braids, Part II will give much deeper results.\n    ",
        "submission_date": "2013-04-11T00:00:00",
        "last_modified_date": "2013-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3210",
        "title": "From Constraints to Resolution Rules, Part II: chains, braids, confluence and T&E",
        "authors": [
            "Denis Berthier"
        ],
        "abstract": "In this Part II, we apply the general theory developed in Part I to a detailed analysis of the Constraint Satisfaction Problem (CSP). We show how specific types of resolution rules can be defined. In particular, we introduce the general notions of a chain and a braid. As in Part I, these notions are illustrated in detail with the Sudoku example - a problem known to be NP-complete and which is therefore typical of a broad class of hard problems. For Sudoku, we also show how far one can go in 'approximating' a CSP with a resolution theory and we give an empirical statistical analysis of how the various puzzles, corresponding to different sets of entries, can be classified along a natural scale of complexity. For any CSP, we also prove the confluence property of some Resolution Theories based on braids and we show how it can be used to define different resolution strategies. Finally, we prove that, in any CSP, braids have the same solving capacity as Trial-and-Error (T&E) with no guessing and we comment this result in the Sudoku case.\n    ",
        "submission_date": "2013-04-11T00:00:00",
        "last_modified_date": "2013-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3418",
        "title": "An Inequality Paradigm for Probabilistic Knowledge",
        "authors": [
            "Benjamin N. Grosof"
        ],
        "abstract": "We propose an inequality paradigm for probabilistic reasoning based on a logic of upper and lower bounds on conditional probabilities. We investigate a family of probabilistic logics, generalizing the work of Nilsson [14]. We develop a variety of logical notions for probabilistic reasoning, including soundness, completeness justification; and convergence: reduction of a theory to a simpler logical class. We argue that a bound view is especially useful for describing the semantics of probabilistic knowledge representation and for describing intermediate states of probabilistic inference and updating. We show that the Dempster-Shafer theory of evidence is formally identical to a special case of our generalized probabilistic logic. Our paradigm thus incorporates both Bayesian \"rule-based\" approaches and avowedly non-Bayesian \"evidential\" approaches such as MYCIN and DempsterShafer. We suggest how to integrate the two \"schools\", and explore some possibilities for novel synthesis of a variety of ideas in probabilistic reasoning.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3419",
        "title": "Probabilistic Interpretations for MYCIN's Certainty Factors",
        "authors": [
            "David Heckerman"
        ],
        "abstract": "This paper examines the quantities used by MYCIN to reason with uncertainty, called certainty factors. It is shown that the original definition of certainty factors is inconsistent with the functions used in MYCIN to combine the quantities. This inconsistency is used to argue for a redefinition of certainty factors in terms of the intuitively appealing desiderata associated with the combining functions. It is shown that this redefinition accommodates an unlimited number of probabilistic interpretations. These interpretations are shown to be monotonic transformations of the likelihood ratio p(EIH)/p(El H). The construction of these interpretations provides insight into the assumptions implicit in the certainty factor model. In particular, it is shown that if uncertainty is to be propagated through an inference network in accordance with the desiderata, evidence must be conditionally independent given the hypothesis and its negation and the inference network must have a tree structure. It is emphasized that assumptions implicit in the model are rarely true in practical applications. Methods for relaxing the assumptions are suggested.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3420",
        "title": "Uncertain Reasoning Using Maximum Entropy Inference",
        "authors": [
            "Daniel Hunter"
        ],
        "abstract": "The use of maximum entropy inference in reasoning with uncertain information is commonly justified by an information-theoretic argument. This paper discusses a possible objection to this information-theoretic justification and shows how it can be met. I then compare maximum entropy inference with certain other currently popular methods for uncertain reasoning. In making such a comparison, one must distinguish between static and dynamic theories of degrees of belief: a static theory concerns the consistency conditions for degrees of belief at a given time; whereas a dynamic theory concerns how one's degrees of belief should change in the light of new information. It is argued that maximum entropy is a dynamic theory and that a complete theory of uncertain reasoning can be gotten by combining maximum entropy inference with probability theory, which is a static theory. This total theory, I argue, is much better grounded than are other theories of uncertain reasoning.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3421",
        "title": "Independence and Bayesian Updating Methods",
        "authors": [
            "Rodney W. Johnson"
        ],
        "abstract": "Duda, Hart, and Nilsson have set forth a method for rule-based inference systems to use in updating the probabilities of hypotheses on the basis of multiple items of new evidence. Pednault, Zucker, and Muresan claimed to give conditions under which independence assumptions made by Duda et al. preclude updating-that is, prevent the evidence from altering the probabilities of the hypotheses. Glymour refutes Pednault et al.'s claim with a counterexample of a rather special form (one item of evidence is incompatible with all but one of the hypotheses); he raises, but leaves open, the question whether their result would be true with an added assumption to rule out such special cases. We show that their result does not hold even with the added assumption, but that it can nevertheless be largely salvaged. Namely, under the conditions assumed by Pednault et al., at most one of the items of evidence can alter the probability of any given hypothesis; thus, although updating is possible, multiple updating for any of the hypotheses is precluded.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3422",
        "title": "A Constraint Propagation Approach to Probabilistic Reasoning",
        "authors": [
            "Judea Pearl"
        ],
        "abstract": "The paper demonstrates that strict adherence to probability theory does not preclude the use of concurrent, self-activated constraint-propagation mechanisms for managing uncertainty. Maintaining local records of sources-of-belief allows both predictive and diagnostic inferences to be activated simultaneously and propagate harmoniously towards a stable equilibrium.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3423",
        "title": "Relative Entropy, Probabilistic Inference and AI",
        "authors": [
            "John E. Shore"
        ],
        "abstract": "Various properties of relative entropy have led to its widespread use in information theory. These properties suggest that relative entropy has a role to play in systems that attempt to perform inference in terms of probability distributions. In this paper, I will review some basic properties of relative entropy as well as its role in probabilistic inference. I will also mention briefly a few existing and potential applications of relative entropy to so-called artificial intelligence (AI).\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3424",
        "title": "Foundations of Probability Theory for AI - The Application of Algorithmic Probability to Problems in Artificial Intelligence",
        "authors": [
            "Ray Solomonoff"
        ],
        "abstract": "This paper covers two topics: first an introduction to Algorithmic Complexity Theory: how it defines probability, some of its characteristic properties and past successful applications. Second, we apply it to problems in A.I. - where it promises to give near optimum search procedures for two very broad classes of problems.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3425",
        "title": "Selecting Uncertainty Calculi and Granularity: An Experiment in Trading-Off Precision and Complexity",
        "authors": [
            "Piero P. Bonissone",
            "Keith S. Decker"
        ],
        "abstract": "The management of uncertainty in expert systems has usually been left to ad hoc representations and rules of combinations lacking either a sound theory or clear semantics. The objective of this paper is to establish a theoretical basis for defining the syntax and semantics of a small subset of calculi of uncertainty operating on a given term set of linguistic statements of likelihood. Each calculus is defined by specifying a negation, a conjunction and a disjunction operator. Families of Triangular norms and conorms constitute the most general representations of conjunction and disjunction operators. These families provide us with a formalism for defining an infinite number of different calculi of uncertainty. The term set will define the uncertainty granularity, i.e. the finest level of distinction among different quantifications of uncertainty. This granularity will limit the ability to differentiate between two similar operators. Therefore, only a small finite subset of the infinite number of calculi will produce notably different results. This result is illustrated by two experiments where nine and eleven different calculi of uncertainty are used with three term sets containing five, nine, and thirteen elements, respectively. Finally, the use of context dependent rule set is proposed to select the most appropriate calculus for any given situation. Such a rule set will be relatively small since it must only describe the selection policies for a small number of calculi (resulting from the analyzed trade-off between complexity and precision).\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3426",
        "title": "A Framework for Non-Monotonic Reasoning About Probabilistic Assumptions",
        "authors": [
            "Marvin S. Cohen"
        ],
        "abstract": "Attempts to replicate probabilistic reasoning in expert systems have typically overlooked a critical ingredient of that process. Probabilistic analysis typically requires extensive judgments regarding interdependencies among hypotheses and data, and regarding the appropriateness of various alternative models. The application of such models is often an iterative process, in which the plausibility of the results confirms or disconfirms the validity of assumptions made in building the model. In current expert systems, by contrast, probabilistic information is encapsulated within modular rules (involving, for example, \"certainty factors\"), and there is no mechanism for reviewing the overall form of the probability argument or the validity of the judgments entering into it.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3427",
        "title": "Metaprobability and Dempster-Shafer in Evidential Reasoning",
        "authors": [
            "Robert Fung",
            "Chee Yee Chong"
        ],
        "abstract": "Evidential reasoning in expert systems has often used ad-hoc uncertainty calculi. Although it is generally accepted that probability theory provides a firm theoretical foundation, researchers have found some problems with its use as a workable uncertainty calculus. Among these problems are representation of ignorance, consistency of probabilistic judgements, and adjustment of a priori judgements with experience. The application of metaprobability theory to evidential reasoning is a new approach to solving these problems.  Metaprobability theory can be viewed as a way to provide soft or hard constraints on beliefs in much the same manner as the Dempster-Shafer theory provides constraints on probability masses on subsets of the state space. Thus, we use the Dempster-Shafer theory, an alternative theory of evidential reasoning to illuminate metaprobability theory as a theory of evidential reasoning. The goal of this paper is to compare how metaprobability theory and Dempster-Shafer theory handle the adjustment of beliefs with evidence with respect to a particular thought experiment.  Sections 2 and 3 give brief descriptions of the metaprobability and Dempster-Shafer theories. Metaprobability theory deals with higher order probabilities applied to evidential reasoning. Dempster-Shafer theory is a generalization of probability theory which has evolved from a theory of upper and lower probabilities.  Section 4 describes a thought experiment and the metaprobability and DempsterShafer analysis of the experiment. The thought experiment focuses on forming beliefs about a population with 6 types of members {1, 2, 3, 4, 5, 6}. A type is uniquely defined by the values of three features: A, B, C. That is, if the three features of one member of the population were known then its type could be ascertained. Each of the three features has two possible values, (e.g. A can be either \"a0\" or \"al\"). Beliefs are formed from evidence accrued from two sensors: sensor A, and sensor B. Each sensor senses the corresponding defining feature. Sensor A reports that half of its observations are \"a0\" and half the observations are 'al'. Sensor B reports that half of its observations are ``b0,' and half are \"bl\". Based on these two pieces of evidence, what should be the beliefs on the distribution of types in the population? Note that the third feature is not observed by any sensor.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3428",
        "title": "Implementing Probabilistic Reasoning",
        "authors": [
            "Matthew L. Ginsberg"
        ],
        "abstract": "General problems in analyzing information in a probabilistic database are considered. The practical difficulties (and occasional advantages) of storing uncertain data, of using it conventional forward- or backward-chaining inference engines, and of working with a probabilistic version of resolution are discussed. The background for this paper is the incorporation of uncertain reasoning facilities in MRS, a general-purpose expert system building tool.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3429",
        "title": "Probability Judgement in Artificial Intelligence",
        "authors": [
            "Glenn Shafer"
        ],
        "abstract": "This paper is concerned with two theories of probability judgment: the Bayesian theory and the theory of belief functions. It illustrates these theories with some simple examples and discusses some of the issues that arise when we try to implement them in expert systems. The Bayesian theory is well known; its main ideas go back to the work of Thomas Bayes (1702-1761). The theory of belief functions, often called the Dempster-Shafer theory in the artificial intelligence community, is less well known, but it has even older antecedents; belief-function arguments appear in the work of George Hooper (16401723) and James Bernoulli (1654-1705). For elementary expositions of the theory of belief functions, see Shafer (1976, 1985).\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3430",
        "title": "A Framework for Comparing Uncertain Inference Systems to Probability",
        "authors": [
            "Ben P. Wise",
            "Max Henrion"
        ],
        "abstract": "Several different uncertain inference systems (UISs) have been developed for representing uncertainty in rule-based expert systems. Some of these, such as Mycin's Certainty Factors, Prospector, and Bayes' Networks were designed as approximations to probability, and others, such as Fuzzy Set Theory and DempsterShafer Belief Functions were not. How different are these UISs in practice, and does it matter which you use? When combining and propagating uncertain information, each UIS must, at least by implication, make certain assumptions about correlations not explicily specified. The maximum entropy principle with minimum cross-entropy updating, provides a way of making assumptions about the missing specification that minimizes the additional information assumed, and thus offers a standard against which the other UISs can be compared. We describe a framework for the experimental comparison of the performance of different UISs, and provide some illustrative results.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3431",
        "title": "Inductive Inference and the Representation of Uncertainty",
        "authors": [
            "Norman C. Dalkey"
        ],
        "abstract": "The form and justification of inductive inference rules depend strongly on the representation of uncertainty. This paper examines one generic representation, namely, incomplete information. The notion can be formalized by presuming that the relevant probabilities in a decision problem are known only to the extent that they belong to a class K of probability distributions. The concept is a generalization of a frequent suggestion that uncertainty be represented by intervals or ranges on probabilities. To make the representation useful for decision making, an inductive rule can be formulated which determines, in a well-defined manner, a best approximation to the unknown probability, given the set K. In addition, the knowledge set notion entails a natural procedure for updating -- modifying the set K given new evidence. Several non-intuitive consequences of updating emphasize the differences between inference with complete and inference with incomplete information.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3432",
        "title": "Machine Learning, Clustering, and Polymorphy",
        "authors": [
            "Stephen Jose Hanson",
            "Malcolm Bauer"
        ],
        "abstract": "This paper describes a machine induction program (WITT) that attempts to model human categorization. Properties of categories to which human subjects are sensitive includes best or prototypical members, relative contrasts between putative categories, and polymorphy (neither necessary or sufficient features). This approach represents an alternative to usual Artificial Intelligence approaches to generalization and conceptual clustering which tend to focus on necessary and sufficient feature rules, equivalence classes, and simple search and match schemes. WITT is shown to be more consistent with human categorization while potentially including results produced by more traditional clustering schemes. Applications of this approach in the domains of expert systems and information retrieval are also discussed.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3433",
        "title": "Induction, of and by Probability",
        "authors": [
            "Larry Rendell"
        ],
        "abstract": "This paper examines some methods and ideas underlying the author's successful probabilistic learning systems(PLS), which have proven uniquely effective and efficient in generalization learning or induction. While the emerging principles are generally applicable, this paper illustrates them in heuristic search, which demands noise management and incremental learning. In our approach, both task performance and learning are guided by probability. Probabilities are incrementally normalized and revised, and their errors are located and corrected.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3434",
        "title": "An Odds Ratio Based Inference Engine",
        "authors": [
            "David S. Vaughan",
            "Bruce M. Perrin",
            "Robert M. Yadrick",
            "Peter D. Holden",
            "Karl G. Kempf"
        ],
        "abstract": "Expert systems applications that involve uncertain inference can be represented by a multidimensional contingency table. These tables offer a general approach to inferring with uncertain evidence, because they can embody any form of association between any number of pieces of evidence and conclusions. (Simpler models may be required, however, if the number of pieces of evidence bearing on a conclusion is large.) This paper presents a method of using these tables to make uncertain inferences without assumptions of conditional independence among pieces of evidence or heuristic combining rules. As evidence is accumulated, new joint probabilities are calculated so as to maintain any dependencies among the pieces of evidence that are found in the contingency table. The new conditional probability of the conclusion is then calculated directly from these new joint probabilities and the conditional probabilities in the contingency table.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3435",
        "title": "A Framework for Control Strategies in Uncertain Inference Networks",
        "authors": [
            "Moshe Ben-Bassat",
            "Oded Maler"
        ],
        "abstract": "Control Strategies for hierarchical tree-like probabilistic inference networks are formulated and investigated. Strategies that utilize staged look-ahead and temporary focus on subgoals are formalized and refined using the Depth Vector concept that serves as a tool for defining the 'virtual tree' regarded by the control strategy. The concept is illustrated by four types of control strategies for three-level trees that are characterized according to their Depth Vector, and according to the way they consider intermediate nodes and the role that they let these nodes play. INFERENTI is a computerized inference system written in Prolog, which provides tools for exercising a variety of control strategies. The system also provides tools for simulating test data and for comparing the relative average performance under different strategies.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3436",
        "title": "Combining Uncertain Estimates",
        "authors": [
            "Henry Hamburger"
        ],
        "abstract": "In a real expert system, one may have unreliable, unconfident, conflicting estimates of the value for a particular parameter. It is important for decision making that the information present in this aggregate somehow find its way into use. We cast the problem of representing and combining uncertain estimates as selection of two kinds of functions, one to determine an estimate, the other its uncertainty. The paper includes a long list of properties that such functions should satisfy, and it presents one method that satisfies them.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3437",
        "title": "Confidence Factors, Empiricism and the Dempster-Shafer Theory of Evidence",
        "authors": [
            "John F. Lemmer"
        ],
        "abstract": "The issue of confidence factors in Knowledge Based Systems has become increasingly important and Dempster-Shafer (DS) theory has become increasingly popular as a basis for these factors. This paper discusses the need for an empirical lnterpretatlon of any theory of confidence factors applied to Knowledge Based Systems and describes an empirical lnterpretatlon of DS theory suggesting that the theory has been extensively misinterpreted. For the essentially syntactic DS theory, a model is developed based on sample spaces, the traditional semantic model of probability theory. This model is used to show that, if belief functions are based on reasonably accurate sampling or observation of a sample space, then the beliefs and upper probabilities as computed according to DS theory cannot be interpreted as frequency ratios. Since many proposed applications of DS theory use belief functions in situations with statistically derived evidence (Wesley [1]) and seem to appeal to statistical intuition to provide an lnterpretatlon of the results as has Garvey [2], it may be argued that DS theory has often been misapplied.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3438",
        "title": "Incidence Calculus: A Mechanism for Probabilistic Reasoning",
        "authors": [
            "Alan Bundy"
        ],
        "abstract": "Mechanisms for the automation of uncertainty are required for expert systems. Sometimes these mechanisms need to obey the properties of probabilistic reasoning. A purely numeric mechanism, like those proposed so far, cannot provide a probabilistic logic with truth functional connectives. We propose an alternative mechanism, Incidence Calculus, which is based on a representation of uncertainty using sets of points, which might represent situations, models or possible worlds. Incidence Calculus does provide a probabilistic logic with truth functional connectives.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3439",
        "title": "Evidential Confirmation as Transformed Probability",
        "authors": [
            "Benjamin N. Grosof"
        ],
        "abstract": "A considerable body of work in AI has been concerned with aggregating measures of confirmatory and disconfirmatory evidence for a common set of propositions. Claiming classical probability to be inadequate or inappropriate, several researchers have gone so far as to invent new formalisms and methods. We show how to represent two major such alternative approaches to evidential confirmation not only in terms of transformed (Bayesian) probability, but also in terms of each other. This unifies two of the leading approaches to confirmation theory, by showing that a revised MYCIN Certainty Factor method [12] is equivalent to a special case of Dempster-Shafer theory. It yields a well-understood axiomatic basis, i.e. conditional independence, to interpret previous work on quantitative confirmation theory. It substantially resolves the \"taxe-them-or-leave-them\" problem of priors: MYCIN had to leave them out, while PROSPECTOR had to have them in. It recasts some of confirmation theory's advantages in terms of the psychological accessibility of probabilistic information in different (transformed) formats. Finally, it helps to unify the representation of uncertain reasoning (see also [11]).\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3440",
        "title": "Interval-Based Decisions for Reasoning Systems",
        "authors": [
            "Ronald P. Loui"
        ],
        "abstract": "This essay looks at decision-making with interval-valued probability measures. Existing decision methods have either supplemented expected utility methods with additional criteria of optimality, or have attempted to supplement the interval-valued measures. We advocate a new approach, which makes the following questions moot: 1. which additional criteria to use, and 2. how wide intervals should be. In order to implement the approach, we need more epistemological information. Such information can be generated by a rule of acceptance with a parameter that allows various attitudes toward error, or can simply be declared. In sketch, the argument is: 1. probability intervals are useful and natural in All. systems; 2. wide intervals avoid error, but are useless in some risk sensitive decision-making; 3. one may obtain narrower intervals if one is less cautious; 4. if bodies of knowledge can be ordered by their caution, one should perform the decision analysis with the acceptable body of knowledge that is the most cautious, of those that are useful. The resulting behavior differs from that of a behavioral probabilist (a Bayesian) because in the proposal, 5. intervals based on successive bodies of knowledge are not always nested; 6. if the agent uses a probability for a particular decision, she need not commit to that probability for credence or future decision; and 7. there may be no acceptable body of knowledge that is useful; hence, sometimes no decision is mandated.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3441",
        "title": "Machine Generalization and Human Categorization: An Information-Theoretic View",
        "authors": [
            "James E. Corter",
            "Mark A. Gluck"
        ],
        "abstract": "In designing an intelligent system that must be able to explain its reasoning to a human user, or to provide generalizations that the human user finds reasonable, it may be useful to take into consideration psychological data on what types of concepts and categories people naturally use. The psychological literature on concept learning and categorization provides strong evidence that certain categories are more easily learned, recalled, and recognized than others. We show here how a measure of the informational value of a category predicts the results of several important categorization experiments better than standard alternative explanations. This suggests that information-based approaches to machine generalization may prove particularly useful and natural for human users of the systems.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3442",
        "title": "Exact Reasoning Under Uncertainty",
        "authors": [
            "Samuel Holtzman",
            "John S. Breese"
        ],
        "abstract": "This paper focuses on designing expert systems to support decision making in complex, uncertain environments. In this context, our research indicates that strictly probabilistic representations, which enable the use of decision-theoretic reasoning, are highly preferable to recently proposed alternatives (e.g., fuzzy set theory and Dempster-Shafer theory). Furthermore, we discuss the language of influence diagrams and a corresponding methodology -decision analysis -- that allows decision theory to be used effectively and efficiently as a decision-making aid. Finally, we use RACHEL, a system that helps infertile couples select medical treatments, to illustrate the methodology of decision analysis as basis for expert decision systems.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3443",
        "title": "The Estimation of Subjective Probabilities via Categorical Judgments of Uncertainty",
        "authors": [
            "Alf C. Zimmer"
        ],
        "abstract": "Theoretically as well as experimentally it is investigated how people represent their knowledge in order to make decisions or to share their knowledge with others. Experiment 1 probes into the ways how people 6ather information about the frequencies of events and how the requested response mode, that is, numerical vs. verbal estimates interferes with this knowledge. The least interference occurs if the subjects are allowed to give verbal responses. From this it is concluded that processing knowledge about uncertainty categorically, that is, by means of verbal expressions, imposes less mental work load on the decision matter than numerical processing.  Possibility theory is used as a framework for modeling the individual usage of verbal categories for grades of uncertainty. The 'elastic' constraints on the verbal expressions for every sing1e subject are determined in Experiment 2 by means of sequential calibration. In further experiments it is shown that the superiority of the verbal processing of knowledge about uncertainty guise generally reduces persistent biases reported in the literature: conservatism (Experiment 3) and neg1igence of regression (Experiment 4). The reanalysis of Hormann's data reveal that in verbal Judgments people exhibit sensitivity for base rates and are not prone to the conjunction fallacy. In a final experiment (5) about predictions in a real-life situation it turns out that in a numerical forecasting task subjects restricted themselves to those parts of their knowledge which are numerical. On the other hand subjects in a verbal forecasting task accessed verbally as well as numerically stated knowledge.  Forecasting is structurally related to the estimation of probabilities for rare events insofar as supporting and contradicting arguments have to be evaluated and the choice of the final Judgment has to be Justified according to the evidence brought forward. In order to assist people in such choice situations a formal model for the interactive checking of arguments has been developed. The model transforms the normal-language quantifiers used in the arguments into fuzzy numbers and evaluates the given train of arguments by means of fuzzy numerica1 operations. Ambiguities in the meanings of quantifiers are resolved interactively.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3444",
        "title": "A Cure for Pathological Behavior in Games that Use Minimax",
        "authors": [
            "Bruce Abramson"
        ],
        "abstract": "The traditional approach to choosing moves in game-playing programs is the minimax procedure. The general belief underlying its use is that increasing search depth improves play. Recent research has shown that given certain simplifying assumptions about a game tree's structure, this belief is erroneous: searching deeper decreases the probability of making a correct move. This phenomenon is called game tree pathology. Among these simplifying assumptions is uniform depth of win/loss (terminal) nodes, a condition which is not true for most real games. Analytic studies in [10] have shown that if every node in a pathological game tree is made terminal with probability exceeding a certain threshold, the resulting tree is nonpathological.  This paper considers a new evaluation function which recognizes increasing densities of forced wins at deeper levels in the tree. This property raises two points that strengthen the hypothesis that uniform win depth causes pathology. First, it proves mathematically that as search deepens, an evaluation function that does not explicitly check for certain forced win patterns becomes decreasingly likely to force wins. This failing predicts the pathological behavior of the original evaluation function. Second, it shows empirically that despite recognizing fewer mid-game wins than the theoretically predicted minimum, the new function is nonpathological.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3445",
        "title": "An Evaluation of Two Alternatives to Minimax",
        "authors": [
            "Dana Nau",
            "Paul Purdom",
            "Chun-Hung Tzeng"
        ],
        "abstract": "In the field of Artificial Intelligence, traditional approaches to choosing moves in games involve the we of the minimax algorithm. However, recent research results indicate that minimizing may not always be the best approach. In this paper we summarize the results of some measurements on several model games with several different evaluation functions. These measurements, which are presented in detail in [NPT], show that there are some new algorithms that can make significantly better use of evaluation function values than the minimax algorithm does.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3446",
        "title": "Intelligent Probabilistic Inference",
        "authors": [
            "Ross D. Shachter"
        ],
        "abstract": "The analysis of practical probabilistic models on the computer demands a convenient representation for the available knowledge and an efficient algorithm to perform inference. An appealing representation is the influence diagram, a network that makes explicit the random variables in a model and their probabilistic dependencies. Recent advances have developed solution procedures based on the influence diagram. In this paper, we examine the fundamental properties that underlie those techniques, and the information about the probabilistic structure that is available in the influence diagram representation.  The influence diagram is a convenient representation for computer processing while also being clear and non-mathematical. It displays probabilistic dependence precisely, in a way that is intuitive for decision makers and experts to understand and communicate. As a result, the same influence diagram can be used to build, assess and analyze a model, facilitating changes in the formulation and feedback from sensitivity analysis.  The goal in this paper is to determine arbitrary conditional probability distributions from a given probabilistic model. Given qualitative information about the dependence of the random variables in the model we can, for a specific conditional expression, specify precisely what quantitative information we need to be able to determine the desired conditional probability distribution. It is also shown how we can find that probability distribution by performing operations locally, that is, over subspaces of the joint distribution. In this way, we can exploit the conditional independence present in the model to avoid having to construct or manipulate the full joint distribution. These results are extended to include maximal processing when the information available is incomplete, and optimal decision making in an uncertain environment.  Influence diagrams as a computer-aided modeling tool were developed by Miller, Merkofer, and Howard [5] and extended by Howard and Matheson [2]. Good descriptions of how to use them in modeling are in Owen [7] and Howard and Matheson [2]. The notion of solving a decision problem through influence diagrams was examined by Olmsted [6] and such an algorithm was developed by Shachter [8]. The latter paper also shows how influence diagrams can be used to perform a variety of sensitivity analyses. This paper extends those results by developing a theory of the properties of the diagram that are used by the algorithm, and the information needed to solve arbitrary probability inference problems.  Section 2 develops the notation and the framework for the paper and the relationship between influence diagrams and joint probability distributions. The general probabilistic inference problem is posed in Section 3. In Section 4 the transformations on the diagram are developed and then put together into a solution procedure in Section 5. In Section 6, this procedure is used to calculate the information requirement to solve an inference problem and the maximal processing that can be performed with incomplete information. Section 7 contains a summary of results.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3448",
        "title": "Strong & Weak Methods: A Logical View of Uncertainty",
        "authors": [
            "John Fox"
        ],
        "abstract": "The last few years has seen a growing debate about techniques for managing uncertainty in AI systems. Unfortunately this debate has been cast as a rivalry between AI methods and classical probability based ones. Three arguments for extending the probability framework of uncertainty are presented, none of which imply a challenge to classical methods. These are (1) explicit representation of several types of uncertainty, specifically possibility and plausibility, as well as probability, (2) the use of weak methods for uncertainty management in problems which are poorly defined, and (3) symbolic representation of different uncertainty calculi and methods for choosing between them.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3450",
        "title": "Probabilistic Conflict Resolution in Hierarchical Hypothesis Spaces",
        "authors": [
            "Tod S. Levitt"
        ],
        "abstract": "Artificial intelligence applications such as industrial robotics, military surveillance, and hazardous environment clean-up, require situation understanding based on partial, uncertain, and ambiguous or erroneous evidence. It is necessary to evaluate the relative likelihood of multiple possible hypotheses of the (current) situation faced by the decision making program. Often, the evidence and hypotheses are hierarchical in nature. In image understanding tasks, for example, evidence begins with raw imagery, from which ambiguous features are extracted which have multiple possible aggregations providing evidential support for the presence of multiple hypothesis of objects and terrain, which in turn aggregate in multiple ways to provide partial evidence for different interpretations of the ambient scene. Information fusion for military situation understanding has a similar evidence/hypothesis hierarchy from multiple sensor through message level interpretations, and also provides evidence at multiple levels of the doctrinal hierarchy of military forces.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3451",
        "title": "Knowledge Structures and Evidential Reasoning in Decision Analysis",
        "authors": [
            "Gerald Shao-Hung Liu"
        ],
        "abstract": "The roles played by decision factors in making complex subject are decisions are characterized by how these factors affect the overall decision. Evidence that partially matches a factor is evaluated, and then effective computational rules are applied to these roles to form an appropriate aggregation of the evidence. The use of this technique supports the expression of deeper levels of causality, and may also preserve the cognitive structure of the decision maker better than the usual weighting methods, certainty-factor or other probabilistic models can.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3489",
        "title": "Logical Stochastic Optimization",
        "authors": [
            "Emad Saad"
        ],
        "abstract": "We present a logical framework to represent and reason about stochastic optimization problems based on probability answer set programming. This is established by allowing probability optimization aggregates, e.g., minimum and maximum in the language of probability answer set programming to allow minimization or maximization of some desired criteria under the probabilistic environments. We show the application of the proposed logical stochastic optimization framework under the probability answer set programming to two stages stochastic optimization problems with recourse.\n    ",
        "submission_date": "2013-04-06T00:00:00",
        "last_modified_date": "2013-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3762",
        "title": "Evolutionary Turing in the Context of Evolutionary Machines",
        "authors": [
            "Mark Burgin",
            "Eugene Eberbach"
        ],
        "abstract": "One of the roots of evolutionary computation was the idea of Turing about unorganized machines. The goal of this work is the development of foundations for evolutionary computations, connecting Turing's ideas and the contemporary state of art in evolutionary computations. To achieve this goal, we develop a general approach to evolutionary processes in the computational context, building mathematical models of computational systems, functioning of which is based on evolutionary processes, and studying properties of such systems. Operations with evolutionary machines are described and it is explored when definite classes of evolutionary machines are closed with respect to basic operations with these machines. We also study such properties as linguistic and functional equivalence of evolutionary machines and their classes, as well as computational power of evolutionary machines and their classes, comparing of evolutionary machines to conventional automata, such as finite automata or Turing machines.\n    ",
        "submission_date": "2013-04-13T00:00:00",
        "last_modified_date": "2013-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3763",
        "title": "An Improved ACS Algorithm for the Solutions of Larger TSP Problems",
        "authors": [
            "Md. Rakib Hassan",
            "Md. Kamrul Hasan",
            "M.M.A. Hashem"
        ],
        "abstract": "Solving large traveling salesman problem (TSP) in an efficient way is a challenging area for the researchers of computer science. This paper presents a modified version of the ant colony system (ACS) algorithm called Red-Black Ant Colony System (RB-ACS) for the solutions of TSP which is the most prominent member of the combinatorial optimization problem. RB-ACS uses the concept of ant colony system together with the parallel search of genetic algorithm for obtaining the optimal solutions quickly. In this paper, it is shown that the proposed RB-ACS algorithm yields significantly better performance than the existing best-known algorithms.\n    ",
        "submission_date": "2013-04-13T00:00:00",
        "last_modified_date": "2013-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3842",
        "title": "Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (2000)",
        "authors": [
            "Craig Boutilier",
            "Moises Goldszmidt"
        ],
        "abstract": "This is the Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence, which was held in San Francisco, CA, June 30 - July 3, 2000\n    ",
        "submission_date": "2013-04-13T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3843",
        "title": "Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (1999)",
        "authors": [
            "Kathryn Laskey",
            "Henri Prade"
        ],
        "abstract": "This is the Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, which was held in Stockholm Sweden, July 30 - August 1, 1999\n    ",
        "submission_date": "2013-04-13T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3844",
        "title": "Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (1998)",
        "authors": [
            "Gregory Cooper",
            "Serafin Moral"
        ],
        "abstract": "This is the Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, which was held in Madison, WI, July 24-26, 1998\n    ",
        "submission_date": "2013-04-13T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3846",
        "title": "Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (1997)",
        "authors": [
            "Dan Geiger",
            "Prakash Shenoy"
        ],
        "abstract": "This is the Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence, which was held in Providence, RI, August 1-3, 1997\n    ",
        "submission_date": "2013-04-13T00:00:00",
        "last_modified_date": "2013-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3847",
        "title": "Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence (1996)",
        "authors": [
            "Eric Horvitz",
            "Finn Jensen"
        ],
        "abstract": "This is the Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence, which was held in Portland, OR, August 1-4, 1996\n    ",
        "submission_date": "2013-04-13T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3848",
        "title": "Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (1995)",
        "authors": [
            "Philippe Besnard",
            "Steve Hanks"
        ],
        "abstract": "This is the Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, which was held in Montreal, QU, August 18-20, 1995\n    ",
        "submission_date": "2013-04-13T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3849",
        "title": "Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (1994)",
        "authors": [
            "Ramon Lopez de Mantaras",
            "David Poole"
        ],
        "abstract": "This is the Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, which was held in Seattle, WA, July 29-31, 1994\n    ",
        "submission_date": "2013-04-13T00:00:00",
        "last_modified_date": "2013-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3851",
        "title": "Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence (1993)",
        "authors": [
            "David Heckerman",
            "E. Mamdani"
        ],
        "abstract": "This is the Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, which was held in Washington, DC, July 9-11, 1993\n    ",
        "submission_date": "2013-04-13T00:00:00",
        "last_modified_date": "2013-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3852",
        "title": "Proceedings of the Eighth Conference on Uncertainty in Artificial Intelligence (1992)",
        "authors": [
            "Bruce D'Ambrosio",
            "Didier Dubois",
            "Philippe Smets",
            "Michael Wellman"
        ],
        "abstract": "This is the Proceedings of the Eighth Conference on Uncertainty in Artificial Intelligence, which was held in Stanford, CA, July 17-19, 1992\n    ",
        "submission_date": "2013-04-13T00:00:00",
        "last_modified_date": "2013-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3853",
        "title": "Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (1991)",
        "authors": [
            "Piero Bonissone",
            "Bruce D'Ambrosio",
            "Philippe Smets"
        ],
        "abstract": "This is the Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence, which was held in Los Angeles, CA, July 13-15, 1991\n    ",
        "submission_date": "2013-04-13T00:00:00",
        "last_modified_date": "2013-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3854",
        "title": "Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence (1990)",
        "authors": [
            "Piero Bonissone",
            "Max Henrion",
            "Laveen Kanal",
            "John Lemmer"
        ],
        "abstract": "This is the Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence, which was held in Cambridge, MA, Jul 27 - Jul 29, 1990\n    ",
        "submission_date": "2013-04-13T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3855",
        "title": "Proceedings of the Fifth Conference on Uncertainty in Artificial Intelligence (1989)",
        "authors": [
            "Max Henrion",
            "Laveen Kanal",
            "John Lemmer",
            "Ross Shachter"
        ],
        "abstract": "This is the Proceedings of the Fifth Conference on Uncertainty in Artificial Intelligence, which was held in Windsor, ON, August 18-20, 1989\n    ",
        "submission_date": "2013-04-13T00:00:00",
        "last_modified_date": "2013-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3856",
        "title": "Proceedings of the Fourth Conference on Uncertainty in Artificial Intelligence (1988)",
        "authors": [
            "Laveen Kanal",
            "John Lemmer",
            "Tod Levitt",
            "Ross Shachter"
        ],
        "abstract": "This is the Proceedings of the Fourth Conference on Uncertainty in Artificial Intelligence, which was held in Minneapolis, MN, July 10-12, 1988\n    ",
        "submission_date": "2013-04-13T00:00:00",
        "last_modified_date": "2013-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3857",
        "title": "Proceedings of the Third Conference on Uncertainty in Artificial Intelligence (1987)",
        "authors": [
            "Laveen Kanal",
            "John Lemmer",
            "Tod Levitt"
        ],
        "abstract": "This is the Proceedings of the Third Conference on Uncertainty in Artificial Intelligence, which was held in Seattle, WA, July 10-12, 1987\n    ",
        "submission_date": "2013-04-13T00:00:00",
        "last_modified_date": "2013-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3859",
        "title": "Proceedings of the Second Conference on Uncertainty in Artificial Intelligence (1986)",
        "authors": [
            "Laveen Kanal",
            "John Lemmer"
        ],
        "abstract": "This is the Proceedings of the Second Conference on Uncertainty in Artificial Intelligence, which was held in Philadelphia, PA, August 8-10, 1986\n    ",
        "submission_date": "2013-04-13T00:00:00",
        "last_modified_date": "2013-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3860",
        "title": "Justificatory and Explanatory Argumentation for Committing Agents",
        "authors": [
            "Ioan Alfred Letia",
            "Adrian Groza"
        ],
        "abstract": "In the interaction between agents we can have an explicative discourse, when communicating preferences or intentions, and a normative discourse, when considering normative knowledge. For justifying their actions our agents are endowed with a Justification and Explanation Logic (JEL), capable to cover both the justification for their commitments and explanations why they had to act in that way, due to the current situation in the environment. Social commitments are used to formalise justificatory and explanatory patterns. The combination of ex- planation, justification, and commitments\n    ",
        "submission_date": "2013-04-13T00:00:00",
        "last_modified_date": "2013-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3879",
        "title": "Automatic case acquisition from texts for process-oriented case-based reasoning",
        "authors": [
            "Valmi Dufour-Lussier",
            "Florence Le Ber",
            "Jean Lieber",
            "Emmanuel Nauer"
        ],
        "abstract": "This paper introduces a method for the automatic acquisition of a rich case representation from free text for process-oriented case-based reasoning. Case engineering is among the most complicated and costly tasks in implementing a case-based reasoning system. This is especially so for process-oriented case-based reasoning, where more expressive case representations are generally used and, in our opinion, actually required for satisfactory case adaptation. In this context, the ability to acquire cases automatically from procedural texts is a major step forward in order to reason on processes. We therefore detail a methodology that makes case acquisition from processes described as free text possible, with special attention given to assembly instruction texts. This methodology extends the techniques we used to extract actions from cooking recipes. We argue that techniques taken from natural language processing are required for this task, and that they give satisfactory results. An evaluation based on our implemented prototype extracting workflows from recipe texts is provided.\n    ",
        "submission_date": "2013-04-14T00:00:00",
        "last_modified_date": "2013-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3999",
        "title": "Off-policy Learning with Eligibility Traces: A Survey",
        "authors": [
            "Matthieu Geist",
            "Bruno Scherrer"
        ],
        "abstract": "In the framework of Markov Decision Processes, off-policy learning, that is the problem of learning a linear approximation of the value function of some fixed policy from one trajectory possibly generated by some other policy. We briefly review on-policy learning algorithms of the literature (gradient-based and least-squares-based), adopting a unified algorithmic view. Then, we highlight a systematic approach for adapting them to off-policy learning with eligibility traces. This leads to some known algorithms - off-policy LSTD(\\lambda), LSPE(\\lambda), TD(\\lambda), TDC/GQ(\\lambda) - and suggests new extensions - off-policy FPKF(\\lambda), BRM(\\lambda), gBRM(\\lambda), GTD2(\\lambda). We describe a comprehensive algorithmic derivation of all algorithms in a recursive and memory-efficent form, discuss their known convergence properties and illustrate their relative empirical behavior on Garnet problems. Our experiments suggest that the most standard algorithms on and off-policy LSTD(\\lambda)/LSPE(\\lambda) - and TD(\\lambda) if the feature space dimension is too large for a least-squares approach - perform the best.\n    ",
        "submission_date": "2013-04-15T00:00:00",
        "last_modified_date": "2013-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.4028",
        "title": "A Fuzzy Logic Based Certain Trust Model for E-Commerce",
        "authors": [
            "Kawser Wazed Nafi",
            "Tonny Shekha Kar",
            "Amjad Hossain",
            "M.M.A Hashem"
        ],
        "abstract": "Trustworthiness especially for service oriented system is very important topic now a day in IT field of the whole world. There are many successful E-commerce organizations presently run in the whole world, but E-commerce has not reached its full potential. The main reason behind this is lack of Trust of people in e-commerce. Again, proper models are still absent for calculating trust of different e-commerce organizations. Most of the present trust models are subjective and have failed to account vagueness and ambiguity of different domain. In this paper we have proposed a new fuzzy logic based Certain Trust model which considers these ambiguity and vagueness of different domain. Fuzzy Based Certain Trust Model depends on some certain values given by experts and developers. can be applied in a system like cloud computing, internet, website, e-commerce, etc. to ensure trustworthiness of these platforms. In this paper we show, although fuzzy works with uncertainties, proposed model works with some certain values. Some experimental results and validation of the model with linguistics terms are shown at the last part of the paper.\n    ",
        "submission_date": "2013-04-15T00:00:00",
        "last_modified_date": "2013-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.4182",
        "title": "Proceedings of the First Conference on Uncertainty in Artificial Intelligence (1985)",
        "authors": [
            "Laveen Kanal",
            "John Lemmer"
        ],
        "abstract": "This is the Proceedings of the First Conference on Uncertainty in Artificial Intelligence, which was held in Los Angeles, CA, July 10-12, 1985\n    ",
        "submission_date": "2013-04-15T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.4379",
        "title": "RockIt: Exploiting Parallelism and Symmetry for MAP Inference in Statistical Relational Models",
        "authors": [
            "Jan Noessner",
            "Mathias Niepert",
            "Heiner Stuckenschmidt"
        ],
        "abstract": "RockIt is a maximum a-posteriori (MAP) query engine for statistical relational models. MAP inference in graphical models is an optimization problem which can be compiled to integer linear programs (ILPs). We describe several advances in translating MAP queries to ILP instances and present the novel meta-algorithm cutting plane aggregation (CPA). CPA exploits local context-specific symmetries and bundles up sets of linear constraints. The resulting counting constraints lead to more compact ILPs and make the symmetry of the ground model more explicit to state-of-the-art ILP solvers. Moreover, RockIt parallelizes most parts of the MAP inference pipeline taking advantage of ubiquitous shared-memory multi-core architectures.\n",
        "submission_date": "2013-04-16T00:00:00",
        "last_modified_date": "2013-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.4415",
        "title": "Mining to Compact CNF Propositional Formulae",
        "authors": [
            "Said Jabbour",
            "Lakhdar Sais",
            "Yakoub Salhi"
        ],
        "abstract": "In this paper, we propose a first application of data mining techniques to propositional satisfiability. Our proposed Mining4SAT approach aims to discover and to exploit hidden structural knowledge for reducing the size of propositional formulae in conjunctive normal form (CNF). Mining4SAT combines both frequent itemset mining techniques and Tseitin's encoding for a compact representation of CNF formulae. The experiments of our Mining4SAT approach show interesting reductions of the sizes of many application instances taken from the last SAT competitions.\n    ",
        "submission_date": "2013-04-16T00:00:00",
        "last_modified_date": "2013-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.4925",
        "title": "h-approximation: History-Based Approximation of Possible World Semantics as ASP",
        "authors": [
            "Manfred Eppe",
            "Mehul Bhatt",
            "Frank Dylla"
        ],
        "abstract": "We propose an approximation of the Possible Worlds Semantics (PWS) for action planning. A corresponding planning system is implemented by a transformation of the action specification to an Answer-Set Program. A novelty is support for postdiction wrt. (a) the plan existence problem in our framework can be solved in NP, as compared to $\\Sigma_2^P$ for non-approximated PWS of Baral(2000); and (b) the planner generates optimal plans wrt. a minimal number of actions in $\\Delta_2^P$. We demo the planning system with standard problems, and illustrate its integration in a larger software framework for robot control in a smart home.\n    ",
        "submission_date": "2013-04-17T00:00:00",
        "last_modified_date": "2013-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.4965",
        "title": "Improvement/Extension of Modular Systems as Combinatorial Reengineering (Survey)",
        "authors": [
            "Mark Sh. Levin"
        ],
        "abstract": "The paper describes development (improvement/extension) approaches for composite (modular) systems (as combinatorial reengineering). The following system improvement/extension actions are considered: (a) improvement of systems component(s) (e.g., improvement of a system component, replacement of a system component); (b) improvement of system component interconnection (compatibility); (c) joint improvement improvement of system components(s) and their interconnection; (d) improvement of system structure (replacement of system part(s), addition of a system part, deletion of a system part, modification of system structure). The study of system improvement approaches involve some crucial issues: (i) scales for evaluation of system components and component compatibility (quantitative scale, ordinal scale, poset-like scale, scale based on interval multiset estimate), (ii) evaluation of integrated system quality, (iii) integration methods to obtain the integrated system quality. The system improvement/extension strategies can be examined as seleciton/combination of the improvement action(s) above and as modification of system structure. The strategies are based on combinatorial optimization problems (e.g., multicriteria selection, knapsack problem, multiple choice problem, combinatorial synthesis based on morphological clique problem, assignment/reassignment problem, graph recoloring problem, spanning problems, hotlink assignment). Here, heuristics are used. Various system improvement/extension strategies are presented including illustrative numerical examples.\n    ",
        "submission_date": "2013-04-17T00:00:00",
        "last_modified_date": "2013-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5051",
        "title": "Constraint Satisfaction over Generalized Staircase Constraints",
        "authors": [
            "Shubhadip Mitra",
            "Partha Dutta",
            "Arnab Bhattacharya"
        ],
        "abstract": "One of the key research interests in the area of Constraint Satisfaction Problem (CSP) is to identify tractable classes of constraints and develop efficient solutions for them. In this paper, we introduce generalized staircase (GS) constraints which is an important generalization of one such tractable class found in the literature, namely, staircase constraints. GS constraints are of two kinds, down staircase (DS) and up staircase (US). We first examine several properties of GS constraints, and then show that arc consistency is sufficient to determine a solution to a CSP over DS constraints. Further, we propose an optimal O(cd) time and space algorithm to compute arc consistency for GS constraints where c is the number of constraints and d is the size of the largest domain. Next, observing that arc consistency is not necessary for solving a DSCSP, we propose a more efficient algorithm for solving it. With regard to US constraints, arc consistency is not known to be sufficient to determine a solution, and therefore, methods such as path consistency or variable elimination are required. Since arc consistency acts as a subroutine for these existing methods, replacing it by our optimal O(cd) arc consistency algorithm produces a more efficient method for solving a USCSP.\n    ",
        "submission_date": "2013-04-18T00:00:00",
        "last_modified_date": "2013-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5159",
        "title": "Interactive POMDP Lite: Towards Practical Planning to Predict and Exploit Intentions for Interacting with Self-Interested Agents",
        "authors": [
            "Trong Nghia Hoang",
            "Kian Hsiang Low"
        ],
        "abstract": "A key challenge in non-cooperative multi-agent systems is that of developing efficient planning algorithms for intelligent agents to interact and perform effectively among boundedly rational, self-interested agents (e.g., humans). The practicality of existing works addressing this challenge is being undermined due to either the restrictive assumptions of the other agents' behavior, the failure in accounting for their rationality, or the prohibitively expensive cost of modeling and predicting their intentions. To boost the practicality of research in this field, we investigate how intention prediction can be efficiently exploited and made practical in planning, thereby leading to efficient intention-aware planning frameworks capable of predicting the intentions of other agents and acting optimally with respect to their predicted intentions. We show that the performance losses incurred by the resulting planning policies are linearly bounded by the error of intention prediction. Empirical evaluations through a series of stochastic games demonstrate that our policies can achieve better and more robust performance than the state-of-the-art algorithms.\n    ",
        "submission_date": "2013-04-18T00:00:00",
        "last_modified_date": "2013-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5449",
        "title": "Solving WCSP by Extraction of Minimal Unsatisfiable Cores",
        "authors": [
            "Christophe Lecoutre",
            "Nicolas Paris",
            "Olivier Roussel",
            "S\u00e9bastien Tabary"
        ],
        "abstract": "Usual techniques to solve WCSP are based on cost transfer operations coupled with a branch and bound algorithm. In this paper, we focus on an approach integrating extraction and relaxation of Minimal Unsatisfiable Cores in order to solve this problem. We decline our approach in two ways: an incomplete, greedy, algorithm and a complete one.\n    ",
        "submission_date": "2013-04-19T00:00:00",
        "last_modified_date": "2013-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5550",
        "title": "OntoRich - A Support Tool for Semi-Automatic Ontology Enrichment and Evaluation",
        "authors": [
            "Adrian Groza",
            "Gabriel Barbur",
            "Bogdan Blaga"
        ],
        "abstract": "This paper presents the OntoRich framework, a support tool for semi-automatic ontology enrichment and evaluation. The WordNet is used to extract candidates for dynamic ontology enrichment from RSS streams. With the integration of OpenNLP the system gains access to syntactic analysis of the RSS news. The enriched ontologies are evaluated against several qualitative metrics.\n    ",
        "submission_date": "2013-04-19T00:00:00",
        "last_modified_date": "2013-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5554",
        "title": "Enacting Social Argumentative Machines in Semantic Wikipedia",
        "authors": [
            "Adrian Groza",
            "Sergiu Indrie"
        ],
        "abstract": "This research advocates the idea of combining argumentation theory with the social web technology, aiming to enact large scale or mass argumentation. The proposed framework allows mass-collaborative editing of structured arguments in the style of semantic wikipedia. The long term goal is to apply the abstract machinery of argumentation theory to more practical applications based on human generated arguments, such as deliberative democracy, business negotiation, or self-care. The ARGNET system was developed based on ther Semantic MediaWiki framework and on the Argument Interchange Format (AIF) ontology.\n    ",
        "submission_date": "2013-04-19T00:00:00",
        "last_modified_date": "2013-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5705",
        "title": "A novice looks at emotional cognition",
        "authors": [
            "Rajendra K. Bera"
        ],
        "abstract": "Modeling emotional-cognition is in a nascent stage and therefore wide-open for new ideas and discussions. In this paper the author looks at the modeling problem by bringing in ideas from axiomatic mathematics, information theory, computer science, molecular biology, non-linear dynamical systems and quantum computing and explains how ideas from these disciplines may have applications in modeling emotional-cognition.\n    ",
        "submission_date": "2013-04-21T00:00:00",
        "last_modified_date": "2013-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5810",
        "title": "Exchanging OWL 2 QL Knowledge Bases",
        "authors": [
            "Marcelo Arenas",
            "Elena Botoeva",
            "Diego Calvanese",
            "Vladislav Ryzhikov"
        ],
        "abstract": "Knowledge base exchange is an important problem in the area of data exchange and knowledge representation, where one is interested in exchanging information between a source and a target knowledge base connected through a mapping. In this paper, we study this fundamental problem for knowledge bases and mappings expressed in OWL 2 QL, the profile of OWL 2 based on the description logic DL-Lite_R. More specifically, we consider the problem of computing universal solutions, identified as one of the most desirable translations to be materialized, and the problem of computing UCQ-representations, which optimally capture in a target TBox the information that can be extracted from a source TBox and a mapping by means of unions of conjunctive queries. For the former we provide a novel automata-theoretic technique, and complexity results that range from NP to EXPTIME, while for the latter we show NLOGSPACE-completeness.\n    ",
        "submission_date": "2013-04-21T00:00:00",
        "last_modified_date": "2013-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5863",
        "title": "Commonsense Reasoning and Large Network Analysis: A Computational Study of ConceptNet 4",
        "authors": [
            "Dimitrios I. Diochnos"
        ],
        "abstract": "In this report a computational study of ConceptNet 4 is performed using tools from the field of network analysis. Part I describes the process of extracting the data from the SQL database that is available online, as well as how the closure of the input among the assertions in the English language is computed. This part also performs a validation of the input as well as checks for the consistency of the entire database. Part II investigates the structural properties of ConceptNet 4. Different graphs are induced from the knowledge base by fixing different parameters. The degrees and the degree distributions are examined, the number and sizes of connected components, the transitivity and clustering coefficient, the cores, information related to shortest paths in the graphs, and cliques. Part III investigates non-overlapping, as well as overlapping communities that are found in ConceptNet 4. Finally, Part IV describes an investigation on rules.\n    ",
        "submission_date": "2013-04-22T00:00:00",
        "last_modified_date": "2013-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5892",
        "title": "A Social Welfare Optimal Sequential Allocation Procedure",
        "authors": [
            "Thomas Kalinowski",
            "Nina Nardoytska",
            "Toby Walsh"
        ],
        "abstract": "We consider a simple sequential allocation procedure for sharing indivisible items between agents in which agents take turns to pick items. Supposing additive utilities and independence between the agents, we show that the expected utility of each agent is computable in polynomial time. Using this result, we prove that the expected utilitarian social welfare is maximized when agents take alternate turns. We also argue that this mechanism remains optimal when agents behave strategically\n    ",
        "submission_date": "2013-04-22T00:00:00",
        "last_modified_date": "2013-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5897",
        "title": "Towards an Extension of the 2-tuple Linguistic Model to Deal With Unbalanced Linguistic Term sets",
        "authors": [
            "Mohammed-Amine Abchir",
            "Isis Truck"
        ],
        "abstract": "In the domain of Computing with words (CW), fuzzy linguistic approaches are known to be relevant in many decision-making problems. Indeed, they allow us to model the human reasoning in replacing words, assessments, preferences, choices, wishes... by ad hoc variables, such as fuzzy sets or more sophisticated variables.\n",
        "submission_date": "2013-04-22T00:00:00",
        "last_modified_date": "2013-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5961",
        "title": "Backdoors to Abduction",
        "authors": [
            "Andreas Pfandler",
            "Stefan R\u00fcmmele",
            "Stefan Szeider"
        ],
        "abstract": "Abductive reasoning (or Abduction, for short) is among the most fundamental AI reasoning methods, with a broad range of applications, including fault diagnosis, belief revision, and automated planning. Unfortunately, Abduction is of high computational complexity; even propositional Abduction is \\Sigma_2^P-complete and thus harder than NP and coNP. This complexity barrier rules out the existence of a polynomial transformation to propositional satisfiability (SAT). In this work we use structural properties of the Abduction instance to break this complexity barrier. We utilize the problem structure in terms of small backdoor sets. We present fixed-parameter tractable transformations from Abduction to SAT, which make the power of today's SAT solvers available to Abduction.\n    ",
        "submission_date": "2013-04-22T00:00:00",
        "last_modified_date": "2013-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5970",
        "title": "Three Generalizations of the FOCUS Constraint",
        "authors": [
            "Nina Narodytska",
            "Thierry Petit",
            "Mohamed Siala",
            "Toby Walsh"
        ],
        "abstract": "The FOCUS constraint expresses the notion that solutions are concentrated. In practice, this constraint suffers from the rigidity of its semantics. To tackle this issue, we propose three generalizations of the FOCUS constraint. We provide for each one a complete filtering algorithm as well as discussing decompositions.\n    ",
        "submission_date": "2013-04-22T00:00:00",
        "last_modified_date": "2013-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.6078",
        "title": "Automating the Dispute Resolution in Task Dependency Network",
        "authors": [
            "Ioan Alfred Letia",
            "Adrian Groza"
        ],
        "abstract": "When perturbation or unexpected events do occur, agents need protocols for repairing or reforming the supply chain. Unfortunate contingency could increase too much the cost of performance, while breaching the current contract may be more efficient. In our framework the principles of contract law are applied to set penalties: expectation damages, opportunity cost, reliance damages, and party design remedies, and they are introduced in the task dependency model\n    ",
        "submission_date": "2013-04-19T00:00:00",
        "last_modified_date": "2013-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.6174",
        "title": "How Hard Is It to Control an Election by Breaking Ties?",
        "authors": [
            "Nicholas Mattei",
            "Nina Narodytska",
            "Toby Walsh"
        ],
        "abstract": "We study the computational complexity of controlling the result of an election by breaking ties strategically. This problem is equivalent to the problem of deciding the winner of an election under parallel universes tie-breaking. When the chair of the election is only asked to break ties to choose between one of the co-winners, the problem is trivially easy. However, in multi-round elections, we prove that it can be NP-hard for the chair to compute how to break ties to ensure a given result. Additionally, we show that the form of the tie-breaking function can increase the opportunities for control. Indeed, we prove that it can be NP-hard to control an election by breaking ties even with a two-stage voting rule.\n    ",
        "submission_date": "2013-04-23T00:00:00",
        "last_modified_date": "2014-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.6442",
        "title": "Verification of Inconsistency-Aware Knowledge and Action Bases (Extended Version)",
        "authors": [
            "Diego Calvanese",
            "Evgeny Kharlamov",
            "Marco Montali",
            "Ario Santoso",
            "Dmitriy Zheleznyakov"
        ],
        "abstract": "Description Logic Knowledge and Action Bases (KABs) have been recently introduced as a mechanism that provides a semantically rich representation of the information on the domain of interest in terms of a DL KB and a set of actions to change such information over time, possibly introducing new objects. In this setting, decidability of verification of sophisticated temporal properties over KABs, expressed in a variant of first-order mu-calculus, has been shown. However, the established framework treats inconsistency in a simplistic way, by rejecting inconsistent states produced through action execution. We address this problem by showing how inconsistency handling based on the notion of repairs can be integrated into KABs, resorting to inconsistency-tolerant semantics. In this setting, we establish decidability and complexity of verification.\n    ",
        "submission_date": "2013-04-23T00:00:00",
        "last_modified_date": "2013-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.6551",
        "title": "Decision-Theoretic Troubleshooting: Hardness of Approximation",
        "authors": [
            "V\u00e1clav L\u00edn"
        ],
        "abstract": "Decision-theoretic troubleshooting is one of the areas to which Bayesian networks can be applied. Given a probabilistic model of a malfunctioning man-made device, the task is to construct a repair strategy with minimal expected cost. The problem has received considerable attention over the past two decades. Efficient solution algorithms have been found for simple cases, whereas other variants have been proven NP-complete. We study several variants of the problem found in literature, and prove that computing approximate troubleshooting strategies is NP-hard. In the proofs, we exploit a close connection to set-covering problems.\n    ",
        "submission_date": "2013-04-24T00:00:00",
        "last_modified_date": "2013-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.6810",
        "title": "Inference and learning in probabilistic logic programs using weighted Boolean formulas",
        "authors": [
            "Daan Fierens",
            "Guy Van den Broeck",
            "Joris Renkens",
            "Dimitar Shterionov",
            "Bernd Gutmann",
            "Ingo Thon",
            "Gerda Janssens",
            "Luc De Raedt"
        ],
        "abstract": "Probabilistic logic programs are logic programs in which some of the facts are annotated with probabilities. This paper investigates how classical inference and learning tasks known from the graphical model community can be tackled for probabilistic logic programs. Several such tasks such as computing the marginals given evidence and learning from (partial) interpretations have not really been addressed for probabilistic logic programs before.\n",
        "submission_date": "2013-04-25T00:00:00",
        "last_modified_date": "2013-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.7168",
        "title": "Non Deterministic Logic Programs",
        "authors": [
            "Emad Saad"
        ],
        "abstract": "Non deterministic applications arise in many domains, including, stochastic optimization, multi-objectives optimization, stochastic planning, contingent stochastic planning, reinforcement learning, reinforcement learning in partially observable Markov decision processes, and conditional planning. We present a logic programming framework called non deterministic logic programs, along with a declarative semantics and fixpoint semantics, to allow representing and reasoning about inherently non deterministic real-world applications. The language of non deterministic logic programs framework is extended with non-monotonic negation, and two alternative semantics are defined: the stable non deterministic model semantics and the well-founded non deterministic model semantics as well as their relationship is studied. These semantics subsume the deterministic stable model semantics and the deterministic well-founded semantics of deterministic normal logic programs, and they reduce to the semantics of deterministic definite logic programs without negation. We show the application of the non deterministic logic programs framework to a conditional planning problem.\n    ",
        "submission_date": "2013-04-26T00:00:00",
        "last_modified_date": "2013-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.7238",
        "title": "Solution of the Decision Making Problems using Fuzzy Soft Relations",
        "authors": [
            "Arindam Chaudhuri",
            "Kajal De",
            "Dipak Chatterjee"
        ],
        "abstract": "The Fuzzy Modeling has been applied in a wide variety of fields such as Engineering and Management Sciences and Social Sciences to solve a number Decision Making Problems which involve impreciseness, uncertainty and vagueness in data. In particular, applications of this Modeling technique in Decision Making Problems have remarkable significance. These problems have been tackled using various theories such as Probability theory, Fuzzy Set Theory, Rough Set Theory, Vague Set Theory, Approximate Reasoning Theory etc. which lack in parameterization of the tools due to which they could not be applied successfully to such problems. The concept of Soft Set has a promising potential for giving an optimal solution for these problems. With the motivation of this new concept, in this paper we define the concepts of Soft Relation and Fuzzy Soft Relation and then apply them to solve a number of Decision Making Problems. The advantages of Fuzzy Soft Relation compared to other paradigms are discussed. To the best of our knowledge this is the first work on the application of Fuzzy Soft Relation to the Decision Making Problems.\n    ",
        "submission_date": "2013-04-26T00:00:00",
        "last_modified_date": "2013-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.7239",
        "title": "Solution of System of Linear Equations - A Neuro-Fuzzy Approach",
        "authors": [
            "Arindam Chaudhuri",
            "Kajal De",
            "Dipak Chatterjee"
        ],
        "abstract": "Neuro-Fuzzy Modeling has been applied in a wide variety of fields such as Decision Making, Engineering and Management Sciences etc. In particular, applications of this Modeling technique in Decision Making by involving complex Systems of Linear Algebraic Equations have remarkable significance. In this Paper, we present Polak-Ribiere Conjugate Gradient based Neural Network with Fuzzy rules to solve System of Simultaneous Linear Algebraic Equations. This is achieved using Fuzzy Backpropagation Learning Rule. The implementation results show that the proposed Neuro-Fuzzy Network yields effective solutions for exactly determined, underdetermined and over-determined Systems of Linear Equations. This fact is demonstrated by the Computational Complexity analysis of the Neuro-Fuzzy Algorithm. The proposed Algorithm is simulated effectively using MATLAB software. To the best of our knowledge this is the first work of the Systems of Linear Algebraic Equations using Neuro-Fuzzy Modeling.\n    ",
        "submission_date": "2013-04-26T00:00:00",
        "last_modified_date": "2013-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.7843",
        "title": "A Hybrid Rule Based Fuzzy-Neural Expert System For Passive Network Monitoring",
        "authors": [
            "Azruddin Ahmad",
            "Gobithasan Rudrusamy",
            "Rahmat Budiarto",
            "Azman Samsudin",
            "Sureswaran Ramadass"
        ],
        "abstract": "An enhanced approach for network monitoring is to create a network monitoring tool that has artificial intelligence characteristics. There are a number of approaches available. One such approach is by the use of a combination of rule based, fuzzy logic and neural networks to create a hybrid ANFIS system. Such system will have a dual knowledge database approach. One containing membership function values to compare to and do deductive reasoning and another database with rules deductively formulated by an expert (a network administrator). The knowledge database will be updated continuously with newly acquired patterns. In short, the system will be composed of 2 parts, learning from data sets and fine-tuning the knowledge-base using neural network and the use of fuzzy logic in making decision based on the rules and membership functions inside the knowledge base. This paper will discuss the idea, steps and issues involved in creating such a system.\n    ",
        "submission_date": "2013-04-30T00:00:00",
        "last_modified_date": "2013-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.0187",
        "title": "A Community Based Algorithm for Large Scale Web Service Composition",
        "authors": [
            "Chantal Cherifi",
            "Yvan Rivierre",
            "Jean-Francois Santucci"
        ],
        "abstract": "Web service composition is the process of synthesizing a new composite service using a set of available Web services in order to satisfy a client request that cannot be treated by any available Web services. The Web services space is a dynamic environment characterized by a huge number of elements. Furthermore, many Web services are offering similar functionalities. In this paper we propose a model for Web service composition designed to address the scale effect and the redundancy issue. The Web services space is represented by a two-layered network architecture. A concrete similarity network layer organizes the Web services operations into communities of functionally similar operations. An abstract interaction network layer represents the composition relationships between the sets of communities. Composition synthesis is performed by a two-phased graph search algorithm. First, the interaction network is mined in order to discover abstract solutions to the request goal. Then, the abstract compositions are instantiated with concrete operations selected from the similarity network. This strategy allows an efficient exploration of the Web services space. Furthermore, operations grouped in a community can be easily substituted if necessary during the composition's synthesis's process.\n    ",
        "submission_date": "2013-05-01T00:00:00",
        "last_modified_date": "2013-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.0574",
        "title": "Extending Modern SAT Solvers for Enumerating All Models",
        "authors": [
            "Said Jabbour",
            "Lakhdar Sais",
            "Yakoub Salhi"
        ],
        "abstract": "In this paper, we address the problem of enumerating all models of a Boolean formula in conjunctive normal form (CNF). We propose an extension of CDCL-based SAT solvers to deal with this fundamental problem. Then, we provide an experimental evaluation of our proposed SAT model enumeration algorithms on both satisfiable SAT instances taken from the last SAT challenge and on instances from the SAT-based encoding of sequence mining problems.\n    ",
        "submission_date": "2013-05-02T00:00:00",
        "last_modified_date": "2013-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.1060",
        "title": "On Rational Closure in Description Logics of Typicality",
        "authors": [
            "Laura Giordano",
            "Valentina Gliozzi",
            "Nicola Olivetti",
            "Gian Luca Pozzato"
        ],
        "abstract": "We define the notion of rational closure in the context of Description Logics extended with a tipicality operator. We start from ALC+T, an extension of ALC with a typicality operator T: intuitively allowing to express concepts of the form T(C), meant to select the \"most normal\" instances of a concept C. The semantics we consider is based on rational model. But we further restrict the semantics to minimal models, that is to say, to models that minimise the rank of domain elements. We show that this semantics captures exactly a notion of rational closure which is a natural extension to Description Logics of Lehmann and Magidor's original one. We also extend the notion of rational closure to the Abox component. We provide an ExpTime algorithm for computing the rational closure of an Abox and we show that it is sound and complete with respect to the minimal model semantics.\n    ",
        "submission_date": "2013-05-05T00:00:00",
        "last_modified_date": "2013-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.1169",
        "title": "Multi-Objective AI Planning: Comparing Aggregation and Pareto Approaches",
        "authors": [
            "Mostepha Redouane Khouadjia",
            "Marc Schoenauer",
            "Vincent Vidal",
            "Johann Dr\u00e9o",
            "Pierre Sav\u00e9ant"
        ],
        "abstract": "Most real-world Planning problems are multi-objective, trying to minimize both the makespan of the solution plan, and some cost of the actions involved in the plan. But most, if not all existing approaches are based on single-objective planners, and use an aggregation of the objectives to remain in the single-objective context. Divide and Evolve (DaE) is an evolutionary planner that won the temporal deterministic satisficing track at the last International Planning Competitions (IPC). Like all Evolutionary Algorithms (EA), it can easily be turned into a Pareto-based Multi-Objective EA. It is however important to validate the resulting algorithm by comparing it with the aggregation approach: this is the goal of this paper. The comparative experiments on a recently proposed benchmark set that are reported here demonstrate the usefulness of going Pareto-based in AI Planning.\n    ",
        "submission_date": "2013-05-06T00:00:00",
        "last_modified_date": "2013-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.1655",
        "title": "A short note on estimating intelligence from user profiles in the context of universal psychometrics: prospects and caveats",
        "authors": [
            "Jose Hernandez-Orallo"
        ],
        "abstract": "There has been an increasing interest in inferring some personality traits from users and players in social networks and games, respectively. This goes beyond classical sentiment analysis, and also much further than customer profiling. The purpose here is to have a characterisation of users in terms of personality traits, such as openness, conscientiousness, extraversion, agreeableness, and neuroticism. While this is an incipient area of research, we ask the question of whether cognitive abilities, and intelligence in particular, are also measurable from user profiles. However, we pose the question as broadly as possible in terms of subjects, in the context of universal psychometrics, including humans, machines and hybrids. Namely, in this paper we analyse the following question: is it possible to measure the intelligence of humans and (non-human) bots in a social network or a game just from their user profiles, i.e., by observation, without the use of interactive tests, such as IQ tests, the Turing test or other more principled machine intelligence tests?\n    ",
        "submission_date": "2013-05-07T00:00:00",
        "last_modified_date": "2013-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.1679",
        "title": "High Level Pattern Classification via Tourist Walks in Networks",
        "authors": [
            "Thiago Christiano Silva",
            "Liang Zhao"
        ],
        "abstract": "Complex networks refer to large-scale graphs with nontrivial connection patterns. The salient and interesting features that the complex network study offer in comparison to graph theory are the emphasis on the dynamical properties of the networks and the ability of inherently uncovering pattern formation of the vertices. In this paper, we present a hybrid data classification technique combining a low level and a high level classifier. The low level term can be equipped with any traditional classification techniques, which realize the classification task considering only physical features (e.g., geometrical or statistical features) of the input data. On the other hand, the high level term has the ability of detecting data patterns with semantic meanings. In this way, the classification is realized by means of the extraction of the underlying network's features constructed from the input data. As a result, the high level classification process measures the compliance of the test instances with the pattern formation of the training data. Out of various high level perspectives that can be utilized to capture semantic meaning, we utilize the dynamical features that are generated from a tourist walker in a networked environment. Specifically, a weighted combination of transient and cycle lengths generated by the tourist walk is employed for that end. Interestingly, our study shows that the proposed technique is able to further improve the already optimized performance of traditional classification techniques.\n    ",
        "submission_date": "2013-05-07T00:00:00",
        "last_modified_date": "2013-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.1946",
        "title": "Semantic-based Anomalous Pattern Discovery in Moving Object Trajectories",
        "authors": [
            "Elena Camossi",
            "Paola Villa",
            "Luca Mazzola"
        ],
        "abstract": "In this work, we investigate a novel semantic approach for pattern discovery in trajectories that, relying on ontologies, enhances object movement information with event semantics. The approach can be applied to the detection of movement patterns and behaviors whenever the semantics of events occurring along the trajectory is, explicitly or implicitly, available. In particular, we tested it against an exacting case scenario in maritime surveillance, i.e., the discovery of suspicious container transportations.\n",
        "submission_date": "2013-05-08T00:00:00",
        "last_modified_date": "2013-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.1958",
        "title": "The Dynamically Extended Mind -- A Minimal Modeling Case Study",
        "authors": [
            "Tom Froese",
            "Carlos Gershenson",
            "David A. Rosenblueth"
        ],
        "abstract": "The extended mind hypothesis has stimulated much interest in cognitive science. However, its core claim, i.e. that the process of cognition can extend beyond the brain via the body and into the environment, has been heavily criticized. A prominent critique of this claim holds that when some part of the world is coupled to a cognitive system this does not necessarily entail that the part is also constitutive of that cognitive system. This critique is known as the \"coupling-constitution fallacy\". In this paper we respond to this reductionist challenge by using an evolutionary robotics approach to create a minimal model of two acoustically coupled agents. We demonstrate how the interaction process as a whole has properties that cannot be reduced to the contributions of the isolated agents. We also show that the neural dynamics of the coupled agents has formal properties that are inherently impossible for those neural networks in isolation. By keeping the complexity of the model to an absolute minimum, we are able to illustrate how the coupling-constitution fallacy is in fact based on an inadequate understanding of the constitutive role of nonlinear interactions in dynamical systems theory.\n    ",
        "submission_date": "2013-05-08T00:00:00",
        "last_modified_date": "2013-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.1961",
        "title": "An Improved Three-Weight Message-Passing Algorithm",
        "authors": [
            "Nate Derbinsky",
            "Jos\u00e9 Bento",
            "Veit Elser",
            "Jonathan S. Yedidia"
        ],
        "abstract": "We describe how the powerful \"Divide and Concur\" algorithm for constraint satisfaction can be derived as a special case of a message-passing version of the Alternating Direction Method of Multipliers (ADMM) algorithm for convex optimization, and introduce an improved message-passing algorithm based on ADMM/DC by introducing three distinct weights for messages, with \"certain\" and \"no opinion\" weights, as well as the standard weight used in ADMM/DC. The \"certain\" messages allow our improved algorithm to implement constraint propagation as a special case, while the \"no opinion\" messages speed convergence for some problems by making the algorithm focus only on active constraints. We describe how our three-weight version of ADMM/DC can give greatly improved performance for non-convex problems such as circle packing and solving large Sudoku puzzles, while retaining the exact performance of ADMM for convex problems. We also describe the advantages of our algorithm compared to other message-passing algorithms based upon belief propagation.\n    ",
        "submission_date": "2013-05-08T00:00:00",
        "last_modified_date": "2013-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.1991",
        "title": "On the universality of cognitive tests",
        "authors": [
            "David L. Dowe",
            "Jose Hernandez-Orallo"
        ],
        "abstract": "The analysis of the adaptive behaviour of many different kinds of systems such as humans, animals and machines, requires more general ways of assessing their cognitive abilities. This need is strengthened by increasingly more tasks being analysed for and completed by a wider diversity of systems, including swarms and hybrids. The notion of universal test has recently emerged in the context of machine intelligence evaluation as a way to define and use the same cognitive test for a variety of systems, using some principled tasks and adapting the interface to each particular subject. However, how far can universal tests be taken? This paper analyses this question in terms of subjects, environments, space-time resolution, rewards and interfaces. This leads to a number of findings, insights and caveats, according to several levels where universal tests may be progressively more difficult to conceive, implement and administer. One of the most significant contributions is given by the realisation that more universal tests are defined as maximisations of less universal tests for a variety of configurations. This means that universal tests must be necessarily adaptive.\n    ",
        "submission_date": "2013-05-09T00:00:00",
        "last_modified_date": "2013-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.2254",
        "title": "Programming with Personalized PageRank: A Locally Groundable First-Order Probabilistic Logic",
        "authors": [
            "William Yang Wang",
            "Kathryn Mazaitis",
            "William W. Cohen"
        ],
        "abstract": "In many probabilistic first-order representation systems, inference is performed by \"grounding\"---i.e., mapping it to a propositional representation, and then performing propositional inference. With a large database of facts, groundings can be very large, making inference and learning computationally expensive. Here we present a first-order probabilistic language which is well-suited to approximate \"local\" grounding: every query $Q$ can be approximately grounded with a small graph. The language is an extension of stochastic logic programs where inference is performed by a variant of personalized PageRank. Experimentally, we show that the approach performs well without weight learning on an entity resolution task; that supervised weight-learning improves accuracy; and that grounding time is independent of DB size. We also show that order-of-magnitude speedups are possible by parallelizing learning.\n    ",
        "submission_date": "2013-05-10T00:00:00",
        "last_modified_date": "2013-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.2265",
        "title": "Quality Measures of Parameter Tuning for Aggregated Multi-Objective Temporal Planning",
        "authors": [
            "Mostepha Redouane Khouadjia",
            "Marc Schoenauer",
            "Vincent Vidal",
            "Johann Dr\u00e9o",
            "Pierre Sav\u00e9ant"
        ],
        "abstract": "Parameter tuning is recognized today as a crucial ingredient when tackling an optimization problem. Several meta-optimization methods have been proposed to find the best parameter set for a given optimization algorithm and (set of) problem instances. When the objective of the optimization is some scalar quality of the solution given by the target algorithm, this quality is also used as the basis for the quality of parameter sets. But in the case of multi-objective optimization by aggregation, the set of solutions is given by several single-objective runs with different weights on the objectives, and it turns out that the hypervolume of the final population of each single-objective run might be a better indicator of the global performance of the aggregation method than the best fitness in its population. This paper discusses this issue on a case study in multi-objective temporal planning using the evolutionary planner DaE-YAHSP and the meta-optimizer ParamILS. The results clearly show how ParamILS makes a difference between both approaches, and demonstrate that indeed, in this context, using the hypervolume indicator as ParamILS target is the best choice. Other issues pertaining to parameter tuning in the proposed context are also discussed.\n    ",
        "submission_date": "2013-05-10T00:00:00",
        "last_modified_date": "2013-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.2415",
        "title": "Exponentiated Gradient LINUCB for Contextual Multi-Armed Bandits",
        "authors": [
            "Djallel Bouneffouf"
        ],
        "abstract": "We present Exponentiated Gradient LINUCB, an algorithm for con-textual multi-armed bandits. This algorithm uses Exponentiated Gradient to find the optimal exploration of the LINUCB. Within a deliberately designed offline simulation framework we conduct evaluations with real online event log data. The experimental results demonstrate that our algorithm outperforms surveyed algorithms.\n    ",
        "submission_date": "2013-05-10T00:00:00",
        "last_modified_date": "2013-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.2498",
        "title": "A Further Generalization of the Finite-Population Geiringer-like Theorem for POMDPs to Allow Recombination Over Arbitrary Set Covers",
        "authors": [
            "Boris Mitavskiy",
            "Jun He"
        ],
        "abstract": "A popular current research trend deals with expanding the Monte-Carlo tree search sampling methodologies to the environments with uncertainty and incomplete information. Recently a finite population version of Geiringer theorem with nonhomologous recombination has been adopted to the setting of Monte-Carlo tree search to cope with randomness and incomplete information by exploiting the entrinsic similarities within the state space of the problem. The only limitation of the new theorem is that the similarity relation was assumed to be an equivalence relation on the set of states. In the current paper we lift this \"curtain of limitation\" by allowing the similarity relation to be modeled in terms of an arbitrary set cover of the set of state-action pairs.\n    ",
        "submission_date": "2013-05-11T00:00:00",
        "last_modified_date": "2013-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.2561",
        "title": "Strategic Planning for Network Data Analysis",
        "authors": [
            "Kartik Talamadupula",
            "Octavian Udrea",
            "Anton Riabov",
            "Anand Ranganathan"
        ],
        "abstract": "As network traffic monitoring software for cybersecurity, malware detection, and other critical tasks becomes increasingly automated, the rate of alerts and supporting data gathered, as well as the complexity of the underlying model, regularly exceed human processing capabilities. Many of these applications require complex models and constituent rules in order to come up with decisions that influence the operation of entire systems. In this paper, we motivate the novel \"strategic planning\" problem -- one of gathering data from the world and applying the underlying model of the domain in order to come up with decisions that will monitor the system in an automated manner. We describe our use of automated planning methods to this problem, including the technique that we used to solve it in a manner that would scale to the demands of a real-time, real world scenario. We then present a PDDL model of one such application scenario related to network administration and monitoring, followed by a description of a novel integrated system that was built to accept generated plans and to continue the execution process. Finally, we present evaluations of two different automated planners and their different capabilities with our integrated system, both on a six-month window of network data, and using a simulator.\n    ",
        "submission_date": "2013-05-12T00:00:00",
        "last_modified_date": "2013-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.2724",
        "title": "Generalized Neutrosophic Soft Set",
        "authors": [
            "Said Broumi"
        ],
        "abstract": "In this paper we present a new concept called generalized neutrosophic soft set. This concept incorporates the beneficial properties of both generalized neutrosophic set introduced by A.A. Salama [7]and soft set techniques proposed by Molodtsov [4]. We also study some properties of this concept. Some definitions and operations have been introduced on generalized neutrosophic soft set. Finally we present an application of generalized neuutrosophic soft set in decision making problem.\n    ",
        "submission_date": "2013-05-13T00:00:00",
        "last_modified_date": "2013-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.3321",
        "title": "A Mining-Based Compression Approach for Constraint Satisfaction Problems",
        "authors": [
            "Said Jabbour",
            "Lakhdar Sais",
            "Yakoub Salhi"
        ],
        "abstract": "In this paper, we propose an extension of our Mining for SAT framework to Constraint satisfaction Problem (CSP). We consider n-ary extensional constraints (table constraints). Our approach aims to reduce the size of the CSP by exploiting the structure of the constraints graph and of its associated microstructure. More precisely, we apply itemset mining techniques to search for closed frequent itemsets on these two representation. Using Tseitin extension, we rewrite the whole CSP to another compressed CSP equivalent with respect to satisfiability. Our approach contrast with previous proposed approach by Katsirelos and Walsh, as we do not change the structure of the constraints.\n    ",
        "submission_date": "2013-05-14T00:00:00",
        "last_modified_date": "2013-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.4130",
        "title": "Belief Propagation for Linear Programming",
        "authors": [
            "Andrew Gelfand",
            "Jinwoo Shin",
            "Michael Chertkov"
        ],
        "abstract": "Belief Propagation (BP) is a popular, distributed heuristic for performing MAP computations in Graphical Models. BP can be interpreted, from a variational perspective, as minimizing the Bethe Free Energy (BFE). BP can also be used to solve a special class of Linear Programming (LP) problems. For this class of problems, MAP inference can be stated as an integer LP with an LP relaxation that coincides with minimization of the BFE at ``zero temperature\". We generalize these prior results and establish a tight characterization of the LP problems that can be formulated as an equivalent LP relaxation of MAP inference. Moreover, we suggest an efficient, iterative annealing BP algorithm for solving this broader class of LP problems. We demonstrate the algorithm's performance on a set of weighted matching problems by using it as a cutting plane method to solve a sequence of LPs tightened by adding ``blossom'' inequalities.\n    ",
        "submission_date": "2013-05-17T00:00:00",
        "last_modified_date": "2013-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.4744",
        "title": "The Doxastic Interpretation of Team Semantics",
        "authors": [
            "Pietro Galliani"
        ],
        "abstract": "We advance a doxastic interpretation for many of the logical connectives considered in Dependence Logic and in its extensions, and we argue that Team Semantics is a natural framework for reasoning about beliefs and belief updates.\n    ",
        "submission_date": "2013-05-21T00:00:00",
        "last_modified_date": "2013-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.4859",
        "title": "Extract ABox Modules for Efficient Ontology Querying",
        "authors": [
            "Jia Xu",
            "Patrick Shironoshita",
            "Ubbo Visser",
            "Nigel John",
            "Mansur Kabuka"
        ],
        "abstract": "The extraction of logically-independent fragments out of an ontology ABox can be useful for solving the tractability problem of querying ontologies with large ABoxes. In this paper, we propose a formal definition of an ABox module, such that it guarantees complete preservation of facts about a given set of individuals, and thus can be reasoned independently w.r.t. the ontology TBox. With ABox modules of this type, isolated or distributed (parallel) ABox reasoning becomes feasible, and more efficient data retrieval from ontology ABoxes can be attained. To compute such an ABox module, we present a theoretical approach and also an approximation for $\\mathcal{SHIQ}$ ontologies. Evaluation of the module approximation on different types of ontologies shows that, on average, extracted ABox modules are significantly smaller than the entire ABox, and the time for ontology reasoning based on ABox modules can be improved significantly.\n    ",
        "submission_date": "2013-05-21T00:00:00",
        "last_modified_date": "2014-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.4917",
        "title": "Note on Evaluation of Hierarchical Modular Systems",
        "authors": [
            "Mark Sh. Levin"
        ],
        "abstract": "This survey note describes a brief systemic view to approaches for evaluation of hierarchical composite (modular) systems. The list of considered issues involves the following: (i) basic assessment scales (quantitative scale, ordinal scale, multicriteria description, two kinds of poset-like scales), (ii) basic types of scale transformations problems, (iii) basic types of scale integration methods. Evaluation of the modular systems is considered as assessment of system components (and their compatibility) and integration of the obtained local estimates into the total system estimate(s). This process is based on the above-mentioned problems (i.e., scale transformation and integration). Illustrations of the assessment problems and evaluation approaches are presented (including numerical examples).\n    ",
        "submission_date": "2013-05-21T00:00:00",
        "last_modified_date": "2013-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.4955",
        "title": "A Data Mining Approach to Solve the Goal Scoring Problem",
        "authors": [
            "Renato Oliveira",
            "Paulo Adeodato",
            "Arthur Carvalho",
            "Icamaan Viegas",
            "Christian Diego",
            "Tsang Ing-Ren"
        ],
        "abstract": "In soccer, scoring goals is a fundamental objective which depends on many conditions and constraints. Considering the RoboCup soccer 2D-simulator, this paper presents a data mining-based decision system to identify the best time and direction to kick the ball towards the goal to maximize the overall chances of scoring during a simulated soccer match. Following the CRISP-DM methodology, data for modeling were extracted from matches of major international tournaments (10691 kicks), knowledge about soccer was embedded via transformation of variables and a Multilayer Perceptron was used to estimate the scoring chance. Experimental performance assessment to compare this approach against previous LDA-based approach was conducted from 100 matches. Several statistical metrics were used to analyze the performance of the system and the results showed an increase of 7.7% in the number of kicks, producing an overall increase of 78% in the number of goals scored.\n    ",
        "submission_date": "2013-05-21T00:00:00",
        "last_modified_date": "2013-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.4987",
        "title": "Robust Logistic Regression using Shift Parameters (Long Version)",
        "authors": [
            "Julie Tibshirani",
            "Christopher D. Manning"
        ],
        "abstract": "Annotation errors can significantly hurt classifier performance, yet datasets are only growing noisier with the increased use of Amazon Mechanical Turk and techniques like distant supervision that automatically generate labels. In this paper, we present a robust extension of logistic regression that incorporates the possibility of mislabelling directly into the objective. Our model can be trained through nearly the same means as logistic regression, and retains its efficiency on high-dimensional datasets. Through named entity recognition experiments, we demonstrate that our approach can provide a significant improvement over the standard model when annotation errors are present.\n    ",
        "submission_date": "2013-05-21T00:00:00",
        "last_modified_date": "2014-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.5030",
        "title": "Towards Rational Deployment of Multiple Heuristics in A*",
        "authors": [
            "David Tolpin",
            "Tal Beja",
            "Solomon Eyal Shimony",
            "Ariel Felner",
            "Erez Karpas"
        ],
        "abstract": "The obvious way to use several admissible heuristics in A* is to take their maximum. In this paper we aim to reduce the time spent on computing heuristics. We discuss Lazy A*, a variant of A* where heuristics are evaluated lazily: only when they are essential to a decision to be made in the A* search process. We present a new rational meta-reasoning based scheme, rational lazy A*, which decides whether to compute the more expensive heuristics at all, based on a myopic value of information estimate. Both methods are examined theoretically. Empirical evaluation on several domains supports the theoretical results, and shows that lazy A* and rational lazy A* are state-of-the-art heuristic combination methods.\n    ",
        "submission_date": "2013-05-22T00:00:00",
        "last_modified_date": "2013-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.5506",
        "title": "Introduction to Judea Pearl's Do-Calculus",
        "authors": [
            "Robert R. Tucci"
        ],
        "abstract": "This is a purely pedagogical paper with no new results. The goal of the paper is to give a fairly self-contained introduction to Judea Pearl's do-calculus, including proofs of his 3 rules.\n    ",
        "submission_date": "2013-04-26T00:00:00",
        "last_modified_date": "2013-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.5522",
        "title": "An Intelligent System to Detect, Avoid and Maintain Potholes: A Graph Theoretic Approach",
        "authors": [
            "Shreyas Balakuntala",
            "Sandeep Venkatesh"
        ],
        "abstract": "In this paper, we propose a conceptual framework where a centralized system, classifies the road based upon the level of damage. The centralized system also identifies the traffic intensity thereby prioritizing the roads that need quick action to be taken upon. Moreover, the system helps the driver to detect the level of damage to the road stretch and route the vehicle from an alternative path to its destination. The system sends a feedback to the concerned authorities for a quick response to the condition of the roads. The system we use comprises a laser sensor and pressure sensors in shock absorbers to detect and quantify the intensity of the pothole, a centralized server which maintains a database of locations of all the potholes which can be accessed by another unit inside the vehicle. A point to point connection device is also installed in vehicles so that, when a vehicle detects a pothole which is not in the database, all the vehicles within a range of 20 meters are warned about the pothole. The system computes a route with least number of potholes which is nearest to the desired destination . If the destination is unknown, then the system will check for potholes in the current road and displays the level of damage. The system is flexible enough that the destination can be added, removed or changed any time during the travel. The best possible route is suggested by the system upon the alteration. We prove that the algorithm returns an efficient path with least number of potholes.\n    ",
        "submission_date": "2013-05-23T00:00:00",
        "last_modified_date": "2013-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.5610",
        "title": "Integrating tabu search and VLSN search to develop enhanced algorithms: A case study using bipartite boolean quadratic programs",
        "authors": [
            "Fred Glover",
            "Tao Ye",
            "Abraham P. Punnen",
            "Gary Kochenberger"
        ],
        "abstract": "The bipartite boolean quadratic programming problem (BBQP) is a generalization of the well studied boolean quadratic programming problem. The model has a variety of real life applications; however, empirical studies of the model are not available in the literature, except in a few isolated instances. In this paper, we develop efficient heuristic algorithms based on tabu search, very large scale neighborhood (VLSN) search, and a hybrid algorithm that integrates the two. The computational study establishes that effective integration of simple tabu search with VLSN search results in superior outcomes, and suggests the value of such an integration in other settings. Complexity analysis and implementation details are provided along with conclusions drawn from experimental analysis. In addition, we obtain solutions better than the best previously known for almost all medium and large size benchmark instances.\n    ",
        "submission_date": "2013-05-24T00:00:00",
        "last_modified_date": "2013-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.5637",
        "title": "Algebraic Net Class Rewriting Systems, Syntax and Semantics for Knowledge Representation and Automated Problem Solving",
        "authors": [
            "Seppo Ilari Tirri"
        ],
        "abstract": "The intention of the present study is to establish general framework for automated problem solving by approaching the task universal algebraically introducing knowledge as realizations of generalized free algebra based nets, graphs with gluing forms connecting in- and out-edges to nodes. Nets are caused to undergo transformations in conceptual level by type wise differentiated intervening net rewriting systems dispersing problems to abstract parts, matching being determined by substitution relations. Achieved sets of conceptual nets constitute congruent classes. New results are obtained within construction of problem solving systems where solution algorithms are derived parallel with other candidates applied to the same net classes. By applying parallel transducer paths consisting of net rewriting systems to net classes congruent quotient algebras are established and the manifested class rewriting comprises all solution candidates whenever produced nets are in anticipated languages liable to acceptance of net automata.\n    ",
        "submission_date": "2013-05-24T00:00:00",
        "last_modified_date": "2013-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.5665",
        "title": "Validity of a clinical decision rule based alert system for drug dose adjustment in patients with renal failure intended to improve pharmacists' analysis of medication orders in hospitals",
        "authors": [
            "Boussadi Abdelali",
            "Caruba Thibaut",
            "Karras Alexandre",
            "Berdot Sarah",
            "Degoulet Patrice",
            "Durieux Pierre",
            "Sabatier Brigitte"
        ],
        "abstract": "Objective: The main objective of this study was to assess the diagnostic performances of an alert system integrated into the CPOE/EMR system for renally cleared drug dosing control. The generated alerts were compared with the daily routine practice of pharmacists as part of the analysis of medication orders. Materials and Methods: The pharmacists performed their analysis of medication orders as usual and were not aware of the alert system interventions that were not displayed for the purpose of the study neither to the physician nor to the pharmacist but kept with associate recommendations in a log file. A senior pharmacist analyzed the results of medication order analysis with and without the alert system. The unit of analysis was the drug prescription line. The primary study endpoints were the detection of drug-dose prescription errors and inter-rater reliability between the alert system and the pharmacists in the detection of drug dose error. Results: The alert system fired alerts in 8.41% (421/5006) of cases: 5.65% (283/5006) exceeds max daily dose alerts and 2.76% (138/5006) under dose alerts. The alert system and the pharmacists showed a relatively poor concordance: 0.106 (CI 95% [0.068, 0.144]). According to the senior pharmacist review, the alert system fired more appropriate alerts than pharmacists, and made fewer errors than pharmacists in analyzing drug dose prescriptions: 143 for the alert system and 261 for the pharmacists. Unlike the alert system, most diagnostic errors made by the pharmacists were false negatives. The pharmacists were not able to analyze a significant number (2097; 25.42%) of drug prescription lines because understaffing. Conclusion: This study strongly suggests that an alert system would be complementary to the pharmacists activity and contribute to drug prescription safety.\n    ",
        "submission_date": "2013-05-24T00:00:00",
        "last_modified_date": "2013-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.6037",
        "title": "Semi-bounded Rationality: A model for decision making",
        "authors": [
            "Tshilidzi Marwala"
        ],
        "abstract": "In this paper the theory of semi-bounded rationality is proposed as an extension of the theory of bounded rationality. In particular, it is proposed that a decision making process involves two components and these are the correlation machine, which estimates missing values, and the causal machine, which relates the cause to the effect. Rational decision making involves using information which is almost always imperfect and incomplete as well as some intelligent machine which if it is a human being is inconsistent to make decisions. In the theory of bounded rationality this decision is made irrespective of the fact that the information to be used is incomplete and imperfect and the human brain is inconsistent and thus this decision that is to be made is taken within the bounds of these limitations. In the theory of semi-bounded rationality, signal processing is used to filter noise and outliers in the information and the correlation machine is applied to complete the missing information and artificial intelligence is used to make more consistent decisions.\n    ",
        "submission_date": "2013-05-26T00:00:00",
        "last_modified_date": "2013-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.6187",
        "title": "Improved Branch-and-Bound for Low Autocorrelation Binary Sequences",
        "authors": [
            "S. D. Prestwich"
        ],
        "abstract": "The Low Autocorrelation Binary Sequence problem has applications in telecommunications, is of theoretical interest to physicists, and has inspired many optimisation researchers. Metaheuristics for the problem have progressed greatly in recent years but complete search has not progressed since a branch-and-bound method of 1996. In this paper we find four ways of improving branch-and-bound, leading to a tighter relaxation, faster convergence to optimality, and better empirical scalability.\n    ",
        "submission_date": "2013-05-27T00:00:00",
        "last_modified_date": "2013-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.6650",
        "title": "Active Sensing as Bayes-Optimal Sequential Decision Making",
        "authors": [
            "Sheeraz Ahmad",
            "Angela J. Yu"
        ],
        "abstract": "Sensory inference under conditions of uncertainty is a major problem in both machine learning and computational neuroscience. An important but poorly understood aspect of sensory processing is the role of active sensing. Here, we present a Bayes-optimal inference and control framework for active sensing, C-DAC (Context-Dependent Active Controller). Unlike previously proposed algorithms that optimize abstract statistical objectives such as information maximization (Infomax) [Butko & Movellan, 2010] or one-step look-ahead accuracy [Najemnik & Geisler, 2005], our active sensing model directly minimizes a combination of behavioral costs, such as temporal delay, response error, and effort. We simulate these algorithms on a simple visual search task to illustrate scenarios in which context-sensitivity is particularly beneficial and optimization with respect to generic statistical objectives particularly inadequate. Motivated by the geometric properties of the C-DAC policy, we present both parametric and non-parametric approximations, which retain context-sensitivity while significantly reducing computational complexity. These approximations enable us to investigate the more complex problem involving peripheral vision, and we notice that the difference between C-DAC and statistical policies becomes even more evident in this scenario.\n    ",
        "submission_date": "2013-05-28T00:00:00",
        "last_modified_date": "2013-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.7058",
        "title": "Towards an Ontology based integrated Framework for Semantic Web",
        "authors": [
            "Nora Y. Ibrahim",
            "Sahar A. Mokhtar",
            "Hany M. Harb"
        ],
        "abstract": "This Ontologies are widely used as a means for solving the information heterogeneity problems on the web because of their capability to provide explicit meaning to the information. They become an efficient tool for knowledge representation in a structured manner. There is always more than one ontology for the same domain. Furthermore, there is no standard method for building ontologies, and there are many ontology building tools using different ontology languages. Because of these reasons, interoperability between the ontologies is very low. Current ontology tools mostly use functions to build, edit and inference the ontology. Methods for merging heterogeneous domain ontologies are not included in most tools. This paper presents ontology merging methodology for building a single global ontology from heterogeneous eXtensible Markup Language (XML) data sources to capture and maintain all the knowledge which XML data sources can contain\n    ",
        "submission_date": "2013-05-30T00:00:00",
        "last_modified_date": "2013-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.7130",
        "title": "Memory Implementations - Current Alternatives",
        "authors": [
            "William Wilson",
            "Uwe Aickelin"
        ],
        "abstract": "Memory can be defined as the ability to retain and recall information in a diverse range of forms. It is a vital component of the way in which we as human beings operate on a day to day basis. Given a particular situation, decisions are made and actions undertaken in response to that situation based on our memory of related prior events and experiences. By utilising our memory we can anticipate the outcome of our chosen actions to avoid unexpected or unwanted events. In addition, as we subtly alter our actions and recognise altered outcomes we learn and create new memories, enabling us to improve the efficiency of our actions over time. However, as this process occurs so naturally in the subconscious its importance is often overlooked.\n    ",
        "submission_date": "2013-05-30T00:00:00",
        "last_modified_date": "2013-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.7145",
        "title": "Modelling and Analysing Cargo Screening Processes: A Project Outline",
        "authors": [
            "Peer-Olaf Siebers",
            "Uwe Aickelin",
            "David Menachof",
            "Galina Sherman",
            "Peter Zimmerman"
        ],
        "abstract": "The efficiency of current cargo screening processes at sea and air ports is unknown as no benchmarks exists against which they could be measured. Some manufacturer benchmarks exist for individual sensors but we have not found any benchmarks that take a holistic view of the screening procedures assessing a combination of sensors and also taking operator variability into account. Just adding up resources and manpower used is not an effective way for assessing systems where human decision-making and operator compliance to rules play a vital role. For such systems more advanced assessment methods need to be used, taking into account that the cargo screening process is of a dynamic and stochastic nature. Our project aim is to develop a decision support tool (cargo-screening system simulator) that will map the right technology and manpower to the right commodity-threat combination in order to maximize detection rates. In this paper we present a project outline and highlight the research challenges we have identified so far. In addition we introduce our first case study, where we investigate the cargo screening process at the ferry port in Calais.\n    ",
        "submission_date": "2013-05-30T00:00:00",
        "last_modified_date": "2013-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.7185",
        "title": "Collaborative ontology sharing and editing",
        "authors": [
            "Philippe A. Martin"
        ],
        "abstract": "This article first lists reasons why - in the long term or when creating a new knowledge base (KB) for general knowledge sharing purposes - collaboratively building a well-organized KB does/can provide more possibilities, with on the whole no more costs, than the mainstream approach where knowledge creation and re-use involves searching, merging and creating (semi-)independent (relatively small) ontologies or semi-formal documents. The article lists elements required to achieve this and describes the main one: a KB editing protocol that keeps the KB free of automatically/manually detected inconsistencies while not forcing them to discuss or agree on terminology and beliefs nor requiring a selection committee.\n    ",
        "submission_date": "2013-05-30T00:00:00",
        "last_modified_date": "2013-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.7254",
        "title": "Harmony search to solve the container storage problem with different container types",
        "authors": [
            "I. Ayachi",
            "R. Kammarti",
            "M.Ksouri",
            "P.Borne LACS",
            "ENIT",
            "Tunis-Belvedere Tunisie LAGIS",
            "Villeneuve d Ascq",
            "France"
        ],
        "abstract": "This paper presents an adaptation of the harmony search algorithm to solve the storage allocation problem for inbound and outbound containers. This problem is studied considering multiple container type (regular, open side, open top, tank, empty and refrigerated) which lets the situation more complicated, as various storage constraints appeared. The objective is to find an optimal container arrangement which respects their departure dates, and minimize the re-handle operations of containers. The performance of the proposed approach is verified comparing to the results generated by genetic algorithm and LIFO algorithm.\n    ",
        "submission_date": "2013-05-30T00:00:00",
        "last_modified_date": "2013-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.7345",
        "title": "Algebraic Properties of Qualitative Spatio-Temporal Calculi",
        "authors": [
            "Frank Dylla",
            "Till Mossakowski",
            "Thomas Schneider",
            "Diedrich Wolter"
        ],
        "abstract": "Qualitative spatial and temporal reasoning is based on so-called qualitative calculi. Algebraic properties of these calculi have several implications on reasoning algorithms. But what exactly is a qualitative calculus? And to which extent do the qualitative calculi proposed meet these demands? The literature provides various answers to the first question but only few facts about the second. In this paper we identify the minimal requirements to binary spatio-temporal calculi and we discuss the relevance of the according axioms for representation and reasoning. We also analyze existing qualitative calculi and provide a classification involving different notions of a relation algebra.\n    ",
        "submission_date": "2013-05-31T00:00:00",
        "last_modified_date": "2013-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.0095",
        "title": "Universal Induction with Varying Sets of Combinators",
        "authors": [
            "Alexey Potapov",
            "Sergey Rodionov"
        ],
        "abstract": "Universal induction is a crucial issue in AGI. Its practical applicability can be achieved by the choice of the reference machine or representation of algorithms agreed with the environment. This machine should be updatable for solving subsequent tasks more efficiently. We study this problem on an example of combinatory logic as the very simple Turing-complete reference machine, which enables modifying program representations by introducing different sets of primitive combinators. Genetic programming system is used to search for combinator expressions, which are easily decomposed into sub-expressions being recombined in crossover. Our experiments show that low-complexity induction or prediction tasks can be solved by the developed system (much more efficiently than using brute force); useful combinators can be revealed and included into the representation simplifying more difficult tasks. However, optimal sets of combinators depend on the specific task, so the reference machine should be adaptively chosen in coordination with the search engine.\n    ",
        "submission_date": "2013-06-01T00:00:00",
        "last_modified_date": "2013-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.0128",
        "title": "Towards Detection of Bottlenecks in Modular Systems",
        "authors": [
            "Mark Sh. Levin"
        ],
        "abstract": "The paper describes some basic approaches to detection of bottlenecks in composite (modular) systems. The following basic system bottlenecks detection problems are examined: (1) traditional quality management approaches (Pareto chart based method, multicriteria analysis as selection of Pareto-efficient points, and/or multicriteria ranking), (2) selection of critical system elements (critical components/modules, critical component interconnection), (3) selection of interconnected system components as composite system faults (via clique-based fusion), (4) critical elements (e.g., nodes) in networks, and (5) predictive detection of system bottlenecks (detection of system components based on forecasting of their parameters). Here, heuristic solving schemes are used. Numerical examples illustrate the approaches.\n    ",
        "submission_date": "2013-06-01T00:00:00",
        "last_modified_date": "2013-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.0539",
        "title": "On the Performance Bounds of some Policy Search Dynamic Programming Algorithms",
        "authors": [
            "Bruno Scherrer"
        ],
        "abstract": "We consider the infinite-horizon discounted optimal control problem formalized by Markov Decision Processes. We focus on Policy Search algorithms, that compute an approximately optimal policy by following the standard Policy Iteration (PI) scheme via an -approximate greedy operator (Kakade and Langford, 2002; Lazaric et al., 2010). We describe existing and a few new performance bounds for Direct Policy Iteration (DPI) (Lagoudakis and Parr, 2003; Fern et al., 2006; Lazaric et al., 2010) and Conservative Policy Iteration (CPI) (Kakade and Langford, 2002). By paying a particular attention to the concentrability constants involved in such guarantees, we notably argue that the guarantee of CPI is much better than that of DPI, but this comes at the cost of a relative--exponential in $\\frac{1}{\\epsilon}$-- increase of time complexity. We then describe an algorithm, Non-Stationary Direct Policy Iteration (NSDPI), that can either be seen as 1) a variation of Policy Search by Dynamic Programming by Bagnell et al. (2003) to the infinite horizon situation or 2) a simplified version of the Non-Stationary PI with growing period of Scherrer and Lesner (2012). We provide an analysis of this algorithm, that shows in particular that it enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a time complexity similar to that of DPI.\n    ",
        "submission_date": "2013-06-03T00:00:00",
        "last_modified_date": "2013-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.0665",
        "title": "Narrative based Postdictive Reasoning for Cognitive Robotics",
        "authors": [
            "Manfred Eppe",
            "Mehul Bhatt"
        ],
        "abstract": "Making sense of incomplete and conflicting narrative knowledge in the presence of abnormalities, unobservable processes, and other real world considerations is a challenge and crucial requirement for cognitive robotics systems. An added challenge, even when suitably specialised action languages and reasoning systems exist, is practical integration and application within large-scale robot control frameworks.\n",
        "submission_date": "2013-06-04T00:00:00",
        "last_modified_date": "2013-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.0751",
        "title": "First-Order Decomposition Trees",
        "authors": [
            "Nima Taghipour",
            "Jesse Davis",
            "Hendrik Blockeel"
        ],
        "abstract": "Lifting attempts to speed up probabilistic inference by exploiting symmetries in the model. Exact lifted inference methods, like their propositional counterparts, work by recursively decomposing the model and the problem. In the propositional case, there exist formal structures, such as decomposition trees (dtrees), that represent such a decomposition and allow us to determine the complexity of inference a priori. However, there is currently no equivalent structure nor analogous complexity results for lifted inference. In this paper, we introduce FO-dtrees, which upgrade propositional dtrees to the first-order level. We show how these trees can characterize a lifted inference solution for a probabilistic logical model (in terms of a sequence of lifted operations), and make a theoretical analysis of the complexity of lifted inference in terms of the novel notion of lifted width for the tree.\n    ",
        "submission_date": "2013-06-04T00:00:00",
        "last_modified_date": "2013-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.0963",
        "title": "Inferring Robot Task Plans from Human Team Meetings: A Generative Modeling Approach with Logic-Based Prior",
        "authors": [
            "Been Kim",
            "Caleb M. Chacha",
            "Julie Shah"
        ],
        "abstract": "We aim to reduce the burden of programming and deploying autonomous systems to work in concert with people in time-critical domains, such as military field operations and disaster response. Deployment plans for these operations are frequently negotiated on-the-fly by teams of human planners. A human operator then translates the agreed upon plan into machine instructions for the robots. We present an algorithm that reduces this translation burden by inferring the final plan from a processed form of the human team's planning conversation. Our approach combines probabilistic generative modeling with logical plan validation used to compute a highly structured prior over possible plans. This hybrid approach enables us to overcome the challenge of performing inference over the large solution space with only a small amount of noisy data from the team planning session. We validate the algorithm through human subject experimentation and show we are able to infer a human team's final plan with 83% accuracy on average. We also describe a robot demonstration in which two people plan and execute a first-response collaborative task with a PR2 robot. To the best of our knowledge, this is the first work that integrates a logical planning technique within a generative model to perform plan inference.\n    ",
        "submission_date": "2013-06-05T00:00:00",
        "last_modified_date": "2013-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.1031",
        "title": "LLAMA: Leveraging Learning to Automatically Manage Algorithms",
        "authors": [
            "Lars Kotthoff"
        ],
        "abstract": "Algorithm portfolio and selection approaches have achieved remarkable improvements over single solvers. However, the implementation of such systems is often highly customised and specific to the problem domain. This makes it difficult for researchers to explore different techniques for their specific problems. We present LLAMA, a modular and extensible toolkit implemented as an R package that facilitates the exploration of a range of different portfolio techniques on any problem domain. It implements the algorithm selection approaches most commonly used in the literature and leverages the extensive library of machine learning algorithms and techniques in R. We describe the current capabilities and limitations of the toolkit and illustrate its usage on a set of example SAT problems.\n    ",
        "submission_date": "2013-06-05T00:00:00",
        "last_modified_date": "2014-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.1034",
        "title": "ROTUNDE - A Smart Meeting Cinematography Initiative: Tools, Datasets, and Benchmarks for Cognitive Interpretation and Control",
        "authors": [
            "Mehul Bhatt",
            "Jakob Suchan",
            "Christian Freksa"
        ],
        "abstract": "We construe smart meeting cinematography with a focus on professional situations such as meetings and seminars, possibly conducted in a distributed manner across socio-spatially separated groups. The basic objective in smart meeting cinematography is to interpret professional interactions involving people, and automatically produce dynamic recordings of discussions, debates, presentations etc in the presence of multiple communication modalities. Typical modalities include gestures (e.g., raising one's hand for a question, applause), voice and interruption, electronic apparatus (e.g., pressing a button), movement (e.g., standing-up, moving around) etc. ROTUNDE, an instance of smart meeting cinematography concept, aims to: (a) develop functionality-driven benchmarks with respect to the interpretation and control capabilities of human-cinematographers, real-time video editors, surveillance personnel, and typical human performance in everyday situations; (b) Develop general tools for the commonsense cognitive interpretation of dynamic scenes from the viewpoint of visuo-spatial cognition centred perceptual narrativisation. Particular emphasis is placed on declarative representations and interfacing mechanisms that seamlessly integrate within large-scale cognitive (interaction) systems and companion technologies consisting of diverse AI sub-components. For instance, the envisaged tools would provide general capabilities for high-level commonsense reasoning about space, events, actions, change, and interaction.\n    ",
        "submission_date": "2013-06-05T00:00:00",
        "last_modified_date": "2013-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.1553",
        "title": "Direct Uncertainty Estimation in Reinforcement Learning",
        "authors": [
            "Sergey Rodionov",
            "Alexey Potapov",
            "Yurii Vinogradov"
        ],
        "abstract": "Optimal probabilistic approach in reinforcement learning is computationally infeasible. Its simplification consisting in neglecting difference between true environment and its model estimated using limited number of observations causes exploration vs exploitation problem. Uncertainty can be expressed in terms of a probability distribution over the space of environment models, and this uncertainty can be propagated to the action-value function via Bellman iterations, which are computationally insufficiently efficient though. We consider possibility of directly measuring uncertainty of the action-value function, and analyze sufficiency of this facilitated approach.\n    ",
        "submission_date": "2013-06-06T00:00:00",
        "last_modified_date": "2013-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.1557",
        "title": "Extending Universal Intelligence Models with Formal Notion of Representation",
        "authors": [
            "Alexey Potapov",
            "Sergey Rodionov"
        ],
        "abstract": "Solomonoff induction is known to be universal, but incomputable. Its approximations, namely, the Minimum Description (or Message) Length (MDL) principles, are adopted in practice in the efficient, but non-universal form. Recent attempts to bridge this gap leaded to development of the Representational MDL principle that originates from formal decomposition of the task of induction. In this paper, possible extension of the RMDL principle in the context of universal intelligence agents is considered, for which introduction of representations is shown to be an unavoidable meta-heuristic and a step toward efficient general intelligence. Hierarchical representations and model optimization with the use of information-theoretic interpretation of the adaptive resonance are also discussed.\n    ",
        "submission_date": "2013-06-06T00:00:00",
        "last_modified_date": "2013-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.1591",
        "title": "Autonomous search for a diffusive source in an unknown environment",
        "authors": [
            "Branko Ristic",
            "Alex Skvortsov",
            "Andrew Walker"
        ],
        "abstract": "The paper presents an approach to olfactory search for a diffusive emitting source of tracer (e.g. aerosol, gas) in an environment with unknown map of randomly placed and shaped obstacles.\n",
        "submission_date": "2013-06-07T00:00:00",
        "last_modified_date": "2013-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.2025",
        "title": "Flexibly-bounded Rationality and Marginalization of Irrationality Theories for Decision Making",
        "authors": [
            "Tshilidzi Marwala"
        ],
        "abstract": "In this paper the theory of flexibly-bounded rationality which is an extension to the theory of bounded rationality is revisited. Rational decision making involves using information which is almost always imperfect and incomplete together with some intelligent machine which if it is a human being is inconsistent to make decisions. In bounded rationality, this decision is made irrespective of the fact that the information to be used is incomplete and imperfect and that the human brain is inconsistent and thus this decision that is to be made is taken within the bounds of these limitations. In the theory of flexibly-bounded rationality, advanced information analysis is used, the correlation machine is applied to complete missing information and artificial intelligence is used to make more consistent decisions. Therefore flexibly-bounded rationality expands the bounds within which rationality is exercised. Because human decision making is essentially irrational, this paper proposes the theory of marginalization of irrationality in decision making to deal with the problem of satisficing in the presence of irrationality.\n    ",
        "submission_date": "2013-06-09T00:00:00",
        "last_modified_date": "2013-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.2268",
        "title": "Accomplishable Tasks in Knowledge Representation",
        "authors": [
            "Keehang Kwon",
            "Mi-Young Park"
        ],
        "abstract": "Knowledge Representation (KR) is traditionally based on the logic of facts, expressed in boolean logic. However, facts about an agent can also be seen as a set of accomplished tasks by the agent. This paper proposes a new approach to KR: the notion of task logical KR based on Computability Logic. This notion allows the user to represent both accomplished tasks and accomplishable tasks by the agent. This notion allows us to build sophisticated KRs about many interesting agents, which have not been supported by previous logical languages.\n    ",
        "submission_date": "2013-06-07T00:00:00",
        "last_modified_date": "2013-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.2295",
        "title": "Markov random fields factorization with context-specific independences",
        "authors": [
            "Alejandro Edera",
            "Facundo Bromberg",
            "Federico Schl\u00fcter"
        ],
        "abstract": "Markov random fields provide a compact representation of joint probability distributions by representing its independence properties in an undirected graph. The well-known Hammersley-Clifford theorem uses these conditional independences to factorize a Gibbs distribution into a set of factors. However, an important issue of using a graph to represent independences is that it cannot encode some types of independence relations, such as the context-specific independences (CSIs). They are a particular case of conditional independences that is true only for a certain assignment of its conditioning set; in contrast to conditional independences that must hold for all its assignments. This work presents a method for factorizing a Markov random field according to CSIs present in a distribution, and formally guarantees that this factorization is correct. This is presented in our main contribution, the context-specific Hammersley-Clifford theorem, a generalization to CSIs of the Hammersley-Clifford theorem that applies for conditional independences.\n    ",
        "submission_date": "2013-06-10T00:00:00",
        "last_modified_date": "2013-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.2558",
        "title": "The Effect of Biased Communications On Both Trusting and Suspicious Voters",
        "authors": [
            "William W. Cohen",
            "David P. Redlawsk",
            "Douglas Pierce"
        ],
        "abstract": "In recent studies of political decision-making, apparently anomalous behavior has been observed on the part of voters, in which negative information about a candidate strengthens, rather than weakens, a prior positive opinion about the candidate. This behavior appears to run counter to rational models of decision making, and it is sometimes interpreted as evidence of non-rational \"motivated reasoning\". We consider scenarios in which this effect arises in a model of rational decision making which includes the possibility of deceptive information. In particular, we will consider a model in which there are two classes of voters, which we will call trusting voters and suspicious voters, and two types of information sources, which we will call unbiased sources and biased sources. In our model, new data about a candidate can be efficiently incorporated by a trusting voter, and anomalous updates are impossible; however, anomalous updates can be made by suspicious voters, if the information source mistakenly plans for an audience of trusting voters, and if the partisan goals of the information source are known by the suspicious voter to be \"opposite\" to his own. Our model is based on a formalism introduced by the artificial intelligence community called \"multi-agent influence diagrams\", which generalize Bayesian networks to settings involving multiple agents with distinct goals.\n    ",
        "submission_date": "2013-06-11T00:00:00",
        "last_modified_date": "2013-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.2863",
        "title": "Random Drift Particle Swarm Optimization",
        "authors": [
            "Jun Sun",
            "Xiaojun Wu",
            "Vasile Palade",
            "Wei Fang",
            "Yuhui Shi"
        ],
        "abstract": "The random drift particle swarm optimization (RDPSO) algorithm, inspired by the free electron model in metal conductors placed in an external electric field, is presented, systematically analyzed and empirically studied in this paper. The free electron model considers that electrons have both a thermal and a drift motion in a conductor that is placed in an external electric field. The motivation of the RDPSO algorithm is described first, and the velocity equation of the particle is designed by simulating the thermal motion as well as the drift motion of the electrons, both of which lead the electrons to a location with minimum potential energy in the external electric field. Then, a comprehensive analysis of the algorithm is made, in order to provide a deep insight into how the RDPSO algorithm works. It involves a theoretical analysis and the simulation of the stochastic dynamical behavior of a single particle in the RDPSO algorithm. The search behavior of the algorithm itself is also investigated in detail, by analyzing the interaction between the particles. Some variants of the RDPSO algorithm are proposed by incorporating different random velocity components with different neighborhood topologies. Finally, empirical studies on the RDPSO algorithm are performed by using a set of benchmark functions from the CEC2005 benchmark suite. Based on the theoretical analysis of the particle's behavior, two methods of controlling the algorithmic parameters are employed, followed by an experimental analysis on how to select the parameter values, in order to obtain a good overall performance of the RDPSO algorithm and its variants in real-world applications. A further performance comparison between the RDPSO algorithms and other variants of PSO is made to prove the efficiency of the RDPSO algorithms.\n    ",
        "submission_date": "2013-06-12T00:00:00",
        "last_modified_date": "2013-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.2864",
        "title": "Finding Academic Experts on a MultiSensor Approach using Shannon's Entropy",
        "authors": [
            "Catarina Moreira",
            "Andreas Wichert"
        ],
        "abstract": "Expert finding is an information retrieval task concerned with the search for the most knowledgeable people, in some topic, with basis on documents describing peoples activities. The task involves taking a user query as input and returning a list of people sorted by their level of expertise regarding the user query. This paper introduces a novel approach for combining multiple estimators of expertise based on a multisensor data fusion framework together with the Dempster-Shafer theory of evidence and Shannon's entropy. More specifically, we defined three sensors which detect heterogeneous information derived from the textual contents, from the graph structure of the citation patterns for the community of experts, and from profile information about the academic experts. Given the evidences collected, each sensor may define different candidates as experts and consequently do not agree in a final ranking decision. To deal with these conflicts, we applied the Dempster-Shafer theory of evidence combined with Shannon's Entropy formula to fuse this information and come up with a more accurate and reliable final ranking list. Experiments made over two datasets of academic publications from the Computer Science domain attest for the adequacy of the proposed approach over the traditional state of the art approaches. We also made experiments against representative supervised state of the art algorithms. Results revealed that the proposed method achieved a similar performance when compared to these supervised techniques, confirming the capabilities of the proposed framework.\n    ",
        "submission_date": "2013-06-12T00:00:00",
        "last_modified_date": "2013-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.3317",
        "title": "Sparse Auto-Regressive: Robust Estimation of AR Parameters",
        "authors": [
            "Mohsen Joneidi"
        ],
        "abstract": "In this paper I present a new approach for regression of time series using their own samples. This is a celebrated problem known as Auto-Regression. Dealing with outlier or missed samples in a time series makes the problem of estimation difficult, so it should be robust against them. Moreover for coding purposes I will show that it is desired the residual of auto-regression be sparse. To these aims, I first assume a multivariate Gaussian prior on the residual and then obtain the estimation. Two simple simulations have been done on spectrum estimation and speech coding.\n    ",
        "submission_date": "2013-06-14T00:00:00",
        "last_modified_date": "2015-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.3542",
        "title": "Encoding Petri Nets in Answer Set Programming for Simulation Based Reasoning",
        "authors": [
            "Saadat Anwar",
            "Chitta Baral",
            "Katsumi Inoue"
        ],
        "abstract": "One of our long term research goals is to develop systems to answer realistic questions (e.g., some mentioned in textbooks) about biological pathways that a biologist may ask. To answer such questions we need formalisms that can model pathways, simulate their execution, model intervention to those pathways, and compare simulations under different circumstances. We found Petri Nets to be the starting point of a suitable formalism for the modeling and simulation needs. However, we need to make extensions to the Petri Net model and also reason with multiple simulation runs and parallel state evolutions. Towards that end Answer Set Programming (ASP) implementation of Petri Nets would allow us to do both. In this paper we show how ASP can be used to encode basic Petri Nets in an intuitive manner. We then show how we can modify this encoding to model several Petri Net extensions by making small changes. We then highlight some of the reasoning capabilities that we will use to accomplish our ultimate research goal.\n    ",
        "submission_date": "2013-06-15T00:00:00",
        "last_modified_date": "2013-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.3548",
        "title": "Encoding Higher Level Extensions of Petri Nets in Answer Set Programming",
        "authors": [
            "Saadat Anwar",
            "Chitta Baral",
            "Katsumi Inoue"
        ],
        "abstract": "Answering realistic questions about biological systems and pathways similar to the ones used by text books to test understanding of students about biological systems is one of our long term research goals. Often these questions require simulation based reasoning. To answer such questions, we need formalisms to build pathway models, add extensions, simulate, and reason with them. We chose Petri Nets and Answer Set Programming (ASP) as suitable formalisms, since Petri Net models are similar to biological pathway diagrams; and ASP provides easy extension and strong reasoning abilities. We found that certain aspects of biological pathways, such as locations and substance types, cannot be represented succinctly using regular Petri Nets. As a result, we need higher level constructs like colored tokens. In this paper, we show how Petri Nets with colored tokens can be encoded in ASP in an intuitive manner, how additional Petri Net extensions can be added by making small code changes, and how this work furthers our long term research goals. Our approach can be adapted to other domains with similar modeling needs.\n    ",
        "submission_date": "2013-06-15T00:00:00",
        "last_modified_date": "2013-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.3884",
        "title": "The Rise and Fall of Semantic Rule Updates Based on SE-Models",
        "authors": [
            "Martin Slota",
            "Jo\u00e3o Leite"
        ],
        "abstract": "Logic programs under the stable model semantics, or answer-set programs, provide an expressive rule-based knowledge representation framework, featuring a formal, declarative and well-understood semantics. However, handling the evolution of rule bases is still a largely open problem. The AGM framework for belief change was shown to give inappropriate results when directly applied to logic programs under a non-monotonic semantics such as the stable models. The approaches to address this issue, developed so far, proposed update semantics based on manipulating the syntactic structure of programs and rules.\n",
        "submission_date": "2013-06-17T00:00:00",
        "last_modified_date": "2013-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.3888",
        "title": "The SP theory of intelligence: an overview",
        "authors": [
            "J. Gerard Wolff"
        ],
        "abstract": "This article is an overview of the \"SP theory of intelligence\". The theory aims to simplify and integrate concepts across artificial intelligence, mainstream computing and human perception and cognition, with information compression as a unifying theme. It is conceived as a brain-like system that receives 'New' information and stores some or all of it in compressed form as 'Old' information. It is realised in the form of a computer model -- a first version of the SP machine. The concept of \"multiple alignment\" is a powerful central idea. Using heuristic techniques, the system builds multiple alignments that are 'good' in terms of information compression. For each multiple alignment, probabilities may be calculated. These provide the basis for calculating the probabilities of inferences. The system learns new structures from partial matches between patterns. Using heuristic techniques, the system searches for sets of structures that are 'good' in terms of information compression. These are normally ones that people judge to be 'natural', in accordance with the 'DONSVIC' principle -- the discovery of natural structures via information compression. The SP theory may be applied in several areas including 'computing', aspects of mathematics and logic, representation of knowledge, natural language processing, pattern recognition, several kinds of reasoning, information storage and retrieval, planning and problem solving, information compression, neuroscience, and human perception and cognition. Examples include the parsing and production of language including discontinuous dependencies in syntax, pattern recognition at multiple levels of abstraction and its integration with part-whole relations, nonmonotonic reasoning and reasoning with default values, reasoning in Bayesian networks including 'explaining away', causal diagnosis, and the solving of a geometric analogy problem.\n    ",
        "submission_date": "2013-06-13T00:00:00",
        "last_modified_date": "2015-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.3890",
        "title": "Big data and the SP theory of intelligence",
        "authors": [
            "J. Gerard Wolff"
        ],
        "abstract": "This article is about how the \"SP theory of intelligence\" and its realisation in the \"SP machine\" may, with advantage, be applied to the management and analysis of big data. The SP system -- introduced in the article and fully described elsewhere -- may help to overcome the problem of variety in big data: it has potential as \"a universal framework for the representation and processing of diverse kinds of knowledge\" (UFK), helping to reduce the diversity of formalisms and formats for knowledge and the different ways in which they are processed. It has strengths in the unsupervised learning or discovery of structure in data, in pattern recognition, in the parsing and production of natural language, in several kinds of reasoning, and more. It lends itself to the analysis of streaming data, helping to overcome the problem of velocity in big data. Central in the workings of the system is lossless compression of information: making big data smaller and reducing problems of storage and management. There is potential for substantial economies in the transmission of data, for big cuts in the use of energy in computing, for faster processing, and for smaller and lighter computers. The system provides a handle on the problem of veracity in big data, with potential to assist in the management of errors and uncertainties in data. It lends itself to the visualisation of knowledge structures and inferential processes. A high-parallel, open-source version of the SP machine would provide a means for researchers everywhere to explore what can be done with the system and to create new versions of it.\n    ",
        "submission_date": "2013-06-13T00:00:00",
        "last_modified_date": "2014-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.4411",
        "title": "Event-Object Reasoning with Curated Knowledge Bases: Deriving Missing Information",
        "authors": [
            "Chitta Baral",
            "Nguyen H. Vo"
        ],
        "abstract": "The broader goal of our research is to formulate answers to why and how questions with respect to knowledge bases, such as AURA. One issue we face when reasoning with many available knowledge bases is that at times needed information is missing. Examples of this include partially missing information about next sub-event, first sub-event, last sub-event, result of an event, input to an event, destination of an event, and raw material involved in an event. In many cases one can recover part of the missing knowledge through reasoning. In this paper we give a formal definition about how such missing information can be recovered and then give an ASP implementation of it. We then discuss the implication of this with respect to answering why and how questions.\n    ",
        "submission_date": "2013-06-19T00:00:00",
        "last_modified_date": "2013-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.4418",
        "title": "Structure Based Extended Resolution for Constraint Programming",
        "authors": [
            "Geoffrey Chu",
            "Peter J. Stuckey"
        ],
        "abstract": "Nogood learning is a powerful approach to reducing search in Constraint Programming (CP) solvers. The current state of the art, called Lazy Clause Generation (LCG), uses resolution to derive nogoods expressing the reasons for each search failure. Such nogoods can prune other parts of the search tree, producing exponential speedups on a wide variety of problems. Nogood learning solvers can be seen as resolution proof systems. The stronger the proof system, the faster it can solve a CP problem. It has recently been shown that the proof system used in LCG is at least as strong as general resolution. However, stronger proof systems such as \\emph{extended resolution} exist. Extended resolution allows for literals expressing arbitrary logical concepts over existing variables to be introduced and can allow exponentially smaller proofs than general resolution. The primary problem in using extended resolution is to figure out exactly which literals are useful to introduce. In this paper, we show that we can use the structural information contained in a CP model in order to introduce useful literals, and that this can translate into significant speedups on a range of problems.\n    ",
        "submission_date": "2013-06-19T00:00:00",
        "last_modified_date": "2013-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.4460",
        "title": "Implementing a Wall-In Building Placement in StarCraft with Declarative Programming",
        "authors": [
            "Michal Certicky"
        ],
        "abstract": "In real-time strategy games like StarCraft, skilled players often block the entrance to their base with buildings to prevent the opponent's units from getting inside. This technique, called \"walling-in\", is a vital part of player's skill set, allowing him to survive early aggression. However, current artificial players (bots) do not possess this skill, due to numerous inconveniences surfacing during its implementation in imperative languages like C++ or Java. In this text, written as a guide for bot programmers, we address the problem of finding an appropriate building placement that would block the entrance to player's base, and present a ready to use declarative solution employing the paradigm of answer set programming (ASP). We also encourage the readers to experiment with different declarative approaches to this problem.\n    ",
        "submission_date": "2013-06-19T00:00:00",
        "last_modified_date": "2013-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.4635",
        "title": "Towards Multistage Design of Modular Systems",
        "authors": [
            "Mark Sh. Levin"
        ],
        "abstract": "The paper describes multistage design of composite (modular) systems (i.e., design of a system trajectory). This design process consists of the following: (i) definition of a set of time/logical points; (ii) modular design of the system for each time/logical point (e.g., on the basis of combinatorial synthesis as hierarchical morphological design or multiple choice problem) to obtain several system solutions; (iii) selection of the system solution for each time/logical point while taking into account their quality and the quality of compatibility between neighbor selected system solutions (here, combinatorial synthesis is used as well). Mainly, the examined time/logical points are based on a time chain. In addition, two complicated cases are considered: (a) the examined logical points are based on a tree-like structure, (b) the examined logical points are based on a digraph. Numerical examples illustrate the approach.\n    ",
        "submission_date": "2013-06-19T00:00:00",
        "last_modified_date": "2013-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.4714",
        "title": "Penetration Testing == POMDP Solving?",
        "authors": [
            "Carlos Sarraute",
            "Olivier Buffet",
            "Joerg Hoffmann"
        ],
        "abstract": "Penetration Testing is a methodology for assessing network security, by generating and executing possible attacks. Doing so automatically allows for regular and systematic testing without a prohibitive amount of human labor. A key question then is how to generate the attacks. This is naturally formulated as a planning problem. Previous work (Lucangeli et al. 2010) used classical planning and hence ignores all the incomplete knowledge that characterizes hacking. More recent work (Sarraute et al. 2011) makes strong independence assumptions for the sake of scaling, and lacks a clear formal concept of what the attack planning problem actually is. Herein, we model that problem in terms of partially observable Markov decision processes (POMDP). This grounds penetration testing in a well-researched formalism, highlighting important aspects of this problem's nature. POMDPs allow to model information gathering as an integral part of the problem, thus providing for the first time a means to intelligently mix scanning actions with actual exploits.\n    ",
        "submission_date": "2013-06-19T00:00:00",
        "last_modified_date": "2013-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.4925",
        "title": "A Multi-Engine Approach to Answer Set Programming",
        "authors": [
            "Marco Maratea",
            "Luca Pulina",
            "Francesco Ricca"
        ],
        "abstract": "Answer Set Programming (ASP) is a truly-declarative programming paradigm proposed in the area of non-monotonic reasoning and logic programming, that has been recently employed in many applications. The development of efficient ASP systems is, thus, crucial. Having in mind the task of improving the solving methods for ASP, there are two usual ways to reach this goal: $(i)$ extending state-of-the-art techniques and ASP solvers, or $(ii)$ designing a new ASP solver from scratch. An alternative to these trends is to build on top of state-of-the-art solvers, and to apply machine learning techniques for choosing automatically the \"best\" available solver on a per-instance basis.\n",
        "submission_date": "2013-06-20T00:00:00",
        "last_modified_date": "2013-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.5053",
        "title": "Breaking Symmetry with Different Orderings",
        "authors": [
            "Nina Narodytska",
            "Toby Walsh"
        ],
        "abstract": "We can break symmetry by eliminating solutions within each symmetry class. For instance, the Lex-Leader method eliminates all but the smallest solution in the lexicographical ordering. Unfortunately, the Lex-Leader method is intractable in general. We prove that, under modest assumptions, we cannot reduce the worst case complexity of breaking symmetry by using other orderings on solutions. We also prove that a common type of symmetry, where rows and columns in a matrix of decision variables are interchangeable, is intractable to break when we use two promising alternatives to the lexicographical ordering: the Gray code ordering (which uses a different ordering on solutions), and the Snake-Lex ordering (which is a variant of the lexicographical ordering that re-orders the variables). Nevertheless, we show experimentally that using other orderings like the Gray code to break symmetry can be beneficial in practice as they may better align with the objective function and branching heuristic.\n    ",
        "submission_date": "2013-06-21T00:00:00",
        "last_modified_date": "2013-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.5070",
        "title": "3-SAT Problem A New Memetic-PSO Algorithm",
        "authors": [
            "Nasser Lotfi",
            "Jamshid Tamouk",
            "Mina Farmanbar"
        ],
        "abstract": "3-SAT problem is of great importance to many technical and scientific applications. This paper presents a new hybrid evolutionary algorithm for solving this satisfiability problem. 3-SAT problem has the huge search space and hence it is known as a NP-hard problem. So, deterministic approaches are not applicable in this context. Thereof, application of evolutionary processing approaches and especially PSO will be very effective for solving these kinds of problems. In this paper, we introduce a new evolutionary optimization technique based on PSO, Memetic algorithm and local search approaches. When some heuristics are mixed, their advantages are collected as well and we can reach to the better outcomes. Finally, we test our proposed algorithm over some benchmarks used by some another available algorithms. Obtained results show that our new method leads to the suitable results by the appropriate time. Thereby, it achieves a better result in compared with the existent approaches such as pure genetic algorithm and some verified types\n    ",
        "submission_date": "2013-06-21T00:00:00",
        "last_modified_date": "2013-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.5308",
        "title": "Cognitive Interpretation of Everyday Activities: Toward Perceptual Narrative Based Visuo-Spatial Scene Interpretation",
        "authors": [
            "Mehul Bhatt",
            "Jakob Suchan",
            "Carl Schultz"
        ],
        "abstract": "We position a narrative-centred computational model for high-level knowledge representation and reasoning in the context of a range of assistive technologies concerned with \"visuo-spatial perception and cognition\" tasks. Our proposed narrative model encompasses aspects such as \\emph{space, events, actions, change, and interaction} from the viewpoint of commonsense reasoning and learning in large-scale cognitive systems. The broad focus of this paper is on the domain of \"human-activity interpretation\" in smart environments, ambient intelligence etc. In the backdrop of a \"smart meeting cinematography\" domain, we position the proposed narrative model, preliminary work on perceptual narrativisation, and the immediate outlook on constructing general-purpose open-source tools for perceptual narrativisation.\n",
        "submission_date": "2013-06-22T00:00:00",
        "last_modified_date": "2013-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.5601",
        "title": "A Decomposition of the Max-min Fair Curriculum-based Course Timetabling Problem",
        "authors": [
            "Moritz M\u00fchlenthaler",
            "Rolf Wanka"
        ],
        "abstract": "We propose a decomposition of the max-min fair curriculum-based course timetabling (MMF-CB-CTT) problem. The decomposition models the room assignment subproblem as a generalized lexicographic bottleneck optimization problem (LBOP). We show that the generalized LBOP can be solved efficiently if the corresponding sum optimization problem can be solved efficiently. As a consequence, the room assignment subproblem of the MMF-CB-CTT problem can be solved efficiently. We use this insight to improve a previously proposed heuristic algorithm for the MMF-CB-CTT problem. Our experimental results indicate that using the new decomposition improves the performance of the algorithm on most of the 21 ITC2007 test instances with respect to the quality of the best solution found. Furthermore, we introduce a measure of the quality of a solution to a max-min fair optimization problem. This measure helps to overcome some limitations imposed by the qualitative nature of max-min fairness and aids the statistical evaluation of the performance of randomized algorithms for such problems. We use this measure to show that using the new decomposition the algorithm outperforms the original one on most instances with respect to the average solution quality.\n    ",
        "submission_date": "2013-06-24T00:00:00",
        "last_modified_date": "2013-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.5606",
        "title": "Proteus: A Hierarchical Portfolio of Solvers and Transformations",
        "authors": [
            "Barry Hurley",
            "Lars Kotthoff",
            "Yuri Malitsky",
            "Barry O'Sullivan"
        ],
        "abstract": "In recent years, portfolio approaches to solving SAT problems and CSPs have become increasingly common. There are also a number of different encodings for representing CSPs as SAT instances. In this paper, we leverage advances in both SAT and CSP solving to present a novel hierarchical portfolio-based approach to CSP solving, which we call Proteus, that does not rely purely on CSP solvers. Instead, it may decide that it is best to encode a CSP problem instance into SAT, selecting an appropriate encoding and a corresponding SAT solver. Our experimental evaluation used an instance of Proteus that involved four CSP solvers, three SAT encodings, and six SAT solvers, evaluated on the most challenging problem instances from the CSP solver competitions, involving global and intensional constraints. We show that significant performance improvements can be achieved by Proteus obtained by exploiting alternative view-points and solvers for combinatorial problem-solving.\n    ",
        "submission_date": "2013-06-24T00:00:00",
        "last_modified_date": "2014-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.5858",
        "title": "Distributed Heuristic Forward Search for Multi-Agent Systems",
        "authors": [
            "Raz Nissim",
            "Ronen Brafman"
        ],
        "abstract": "This paper describes a number of distributed forward search algorithms for solving multi-agent planning problems. We introduce a distributed formulation of non-optimal forward search, as well as an optimal version, MAD-A*. Our algorithms exploit the structure of multi-agent problems to not only distribute the work efficiently among different agents, but also to remove symmetries and reduce the overall workload. The algorithms ensure that private information is not shared among agents, yet computation is still efficient -- outperforming current state-of-the-art distributed planners, and in some cases even centralized search -- despite the fact that each agent has access only to partial information.\n    ",
        "submission_date": "2013-06-25T00:00:00",
        "last_modified_date": "2013-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.5884",
        "title": "Design of an Agent for Answering Back in Smart Phones",
        "authors": [
            "Sandeep Venkatesh",
            "Meera V Patil",
            "Nanditha Swamy"
        ],
        "abstract": "The objective of the paper is to design an agent which provides efficient response to the caller when a call goes unanswered in smartphones. The agent provides responses through text messages, email etc stating the most likely reason as to why the callee is unable to answer a call. Responses are composed taking into consideration the importance of the present call and the situation the callee is in at the moment like driving, sleeping, at work etc. The agent makes decisons in the compostion of response messages based on the patterns it has come across in the learning environment. Initially the user helps the agent to compose response messages. The agent associates this message to the percept it recieves with respect to the environment the callee is in. The user may thereafter either choose to make to response system automatic or choose to recieve suggestions from the agent for responses messages and confirm what is to be sent to the caller.\n    ",
        "submission_date": "2013-06-25T00:00:00",
        "last_modified_date": "2014-01-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.5960",
        "title": "Computation of Diet Composition for Patients Suffering from Kidney and Urinary Tract Diseases with the Fuzzy Genetic System",
        "authors": [
            "Sri Hartati",
            "Shofwatul 'Uyun"
        ],
        "abstract": "Determination of dietary food consumed a day for patients with diseases in general, greatly affect the health of the body and the healing process, is no exception for people with kidney disease and urinary tract. This paper presents the determination of diet composition in the form of food subtance for people with kidney and urinary tract diseases with a genetic fuzzy approach. This approach combines fuzzy logic and genetic algorithms, which utilizing fuzzy logic fuzzy tools and techniques to model the components of the genetic algorithm and adapting genetic algorithm control parameters, with the aim of improving system performance. The Mamdani fuzzy inference model and fuzzy rules based on population parameters and generation are used to determine the probability of crossover and mutation, and was using In this study, 400 food survey data along with their substances was used as test material. From the data, a varying amount of population is established. Each chromosome has 10 genes in which the value of each gene indicates the index number of foodstuffs in the database. The fuzzy genetic approach produces 10 best food substance and their compositions. The composition of these foods has nutritional value in accordance with the number of calories needed by people with kidney and urinary tract diseases by type of food.\n    ",
        "submission_date": "2013-06-25T00:00:00",
        "last_modified_date": "2013-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.5982",
        "title": "Activity Modeling in Smart Home using High Utility Pattern Mining over Data Streams",
        "authors": [
            "Menaka Gandhi.J",
            "K.S.Gayathri"
        ],
        "abstract": "Smart home technology is a better choice for the people to care about security, comfort and power saving as well. It is required to develop technologies that recognize the Activities of Daily Living (ADLs) of the residents at home and detect the abnormal behavior in the individual's patterns. Data mining techniques such as Frequent pattern mining (FPM), High Utility Pattern (HUP) Mining were used to find those activity patterns from the collected sensor data. But applying the above technique for Activity Recognition from the temporal sensor data stream is highly complex and challenging task. So, a new approach is proposed for activity recognition from sensor data stream which is achieved by constructing Frequent Pattern Stream tree (FPS - tree). FPS is a sliding window based approach to discover the recent activity patterns over time from data streams. The proposed work aims at identifying the frequent pattern of the user from the sensor data streams which are later modeled for activity recognition. The proposed FPM algorithm uses a data structure called Linked Sensor Data Stream (LSDS) for storing the sensor data stream information which increases the efficiency of frequent pattern mining algorithm through both space and time. The experimental results show the efficiency of the proposed algorithm and this FPM is further extended for applying for power efficiency using HUP to detect the high usage of power consumption of residents at smart home.\n    ",
        "submission_date": "2013-06-25T00:00:00",
        "last_modified_date": "2013-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.6302",
        "title": "Solving Relational MDPs with Exogenous Events and Additive Rewards",
        "authors": [
            "S. Joshi",
            "R. Khardon",
            "P. Tadepalli",
            "A. Raghavan",
            "A. Fern"
        ],
        "abstract": "We formalize a simple but natural subclass of service domains for relational planning problems with object-centered, independent exogenous events and additive rewards capturing, for example, problems in inventory control. Focusing on this subclass, we present a new symbolic planning algorithm which is the first algorithm that has explicit performance guarantees for relational MDPs with exogenous events. In particular, under some technical conditions, our planning algorithm provides a monotonic lower bound on the optimal value function. To support this algorithm we present novel evaluation and reduction techniques for generalized first order decision diagrams, a knowledge representation for real-valued functions over relational world states. Our planning algorithm uses a set of focus states, which serves as a training set, to simplify and approximate the symbolic solution, and can thus be seen to perform learning for planning. A preliminary experimental evaluation demonstrates the validity of our approach.\n    ",
        "submission_date": "2013-06-26T00:00:00",
        "last_modified_date": "2013-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.6375",
        "title": "Metaheuristics in Flood Disaster Management and Risk Assessment",
        "authors": [
            "Vena Pearl Bongolan",
            "Florencio C. Ballesteros Jr.",
            "Joyce Anne M. Banting",
            "Aina Marie Q. Olaes",
            "Charlymagne R. Aquino"
        ],
        "abstract": "A conceptual area is divided into units or barangays, each was allowed to evolve under a physical constraint. A risk assessment method was then used to identify the flood risk in each community using the following risk factors: the area's urbanized area ratio, literacy rate, mortality rate, poverty incidence, radio/TV penetration, and state of structural and non-structural measures. Vulnerability is defined as a weighted-sum of these components. A penalty was imposed for reduced vulnerability. Optimization comparison was done with MatLab's Genetic Algorithms and Simulated Annealing; results showed 'extreme' solutions and realistic designs, for simulated annealing and genetic algorithm, respectively.\n    ",
        "submission_date": "2013-06-26T00:00:00",
        "last_modified_date": "2013-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.6489",
        "title": "A Fuzzy Topsis Multiple-Attribute Decision Making for Scholarship Selection",
        "authors": [
            "Shofwatul 'Uyun",
            "Imam Riadi"
        ],
        "abstract": "As the education fees are becoming more expensive, more students apply for scholarships. Consequently, hundreds and even thousands of applications need to be handled by the sponsor. To solve the problems, some alternatives based on several attributes (criteria) need to be selected. In order to make a decision on such fuzzy problems, Fuzzy Multiple Attribute Decision Making (FMDAM) can be applied. In this study, Unified Modeling Language (UML) in FMADM with TOPSIS and Weighted Product (WP) methods is applied to select the candidates for academic and non-academic scholarships at Universitas Islam Negeri Sunan Kalijaga. Data used were a crisp and fuzzy data. The results show that TOPSIS and Weighted Product FMADM methods can be used to select the most suitable candidates to receive the scholarships since the preference values applied in this method can show applicants with the highest eligibility\n    ",
        "submission_date": "2013-06-27T00:00:00",
        "last_modified_date": "2013-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.6649",
        "title": "Measurements of collective machine intelligence",
        "authors": [
            "Michel Halmes"
        ],
        "abstract": "Independent from the still ongoing research in measuring individual intelligence, we anticipate and provide a framework for measuring collective intelligence. Collective intelligence refers to the idea that several individuals can collaborate in order to achieve high levels of intelligence. We present thus some ideas on how the intelligence of a group can be measured and simulate such tests. We will however focus here on groups of artificial intelligence agents (i.e., machines). We will explore how a group of agents is able to choose the appropriate problem and to specialize for a variety of tasks. This is a feature which is an important contributor to the increase of intelligence in a group (apart from the addition of more agents and the improvement due to common decision making). Our results reveal some interesting results about how (collective) intelligence can be modeled, about how collective intelligence tests can be designed and about the underlying dynamics of collective intelligence. As it will be useful for our simulations, we provide also some improvements of the threshold allocation model originally used in the area of swarm intelligence but further generalized here.\n    ",
        "submission_date": "2013-06-27T00:00:00",
        "last_modified_date": "2013-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.6802",
        "title": "Evaluation Measures for Hierarchical Classification: a unified view and novel approaches",
        "authors": [
            "Aris Kosmopoulos",
            "Ioannis Partalas",
            "Eric Gaussier",
            "Georgios Paliouras",
            "Ion Androutsopoulos"
        ],
        "abstract": "Hierarchical classification addresses the problem of classifying items into a hierarchy of classes. An important issue in hierarchical classification is the evaluation of different classification algorithms, which is complicated by the hierarchical relations among the classes. Several evaluation measures have been proposed for hierarchical classification using the hierarchy in different ways. This paper studies the problem of evaluation in hierarchical classification by analyzing and abstracting the key components of the existing performance measures. It also proposes two alternative generic views of hierarchical evaluation and introduces two corresponding novel measures. The proposed measures, along with the state-of-the art ones, are empirically tested on three large datasets from the domain of text classification. The empirical results illustrate the undesirable behavior of existing approaches and how the proposed methods overcome most of these methods across a range of cases.\n    ",
        "submission_date": "2013-06-28T00:00:00",
        "last_modified_date": "2013-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.6852",
        "title": "Axiomatic properties of inconsistency indices for pairwise comparisons",
        "authors": [
            "Matteo Brunelli",
            "Michele Fedrizzi"
        ],
        "abstract": "Pairwise comparisons are a well-known method for the representation of the subjective preferences of a decision maker. Evaluating their inconsistency has been a widely studied and discussed topic and several indices have been proposed in the literature to perform this task. Since an acceptable level of consistency is closely related with the reliability of preferences, a suitable choice of an inconsistency index is a crucial phase in decision making processes. The use of different methods for measuring consistency must be carefully evaluated, as it can affect the decision outcome in practical applications. In this paper, we present five axioms aimed at characterizing inconsistency indices. In addition, we prove that some of the indices proposed in the literature satisfy these axioms, while others do not, and therefore, in our view, they may fail to correctly evaluate inconsistency.\n    ",
        "submission_date": "2013-06-28T00:00:00",
        "last_modified_date": "2013-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.0060",
        "title": "Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs",
        "authors": [
            "Vikash K. Mansinghka",
            "Tejas D. Kulkarni",
            "Yura N. Perov",
            "Joshua B. Tenenbaum"
        ],
        "abstract": "The idea of computer vision as the Bayesian inverse problem to computer graphics has a long history and an appealing elegance, but it has proved difficult to directly implement. Instead, most vision tasks are approached via complex bottom-up processing pipelines. Here we show that it is possible to write short, simple probabilistic graphics programs that define flexible generative models and to automatically invert them to interpret real-world images. Generative probabilistic graphics programs consist of a stochastic scene generator, a renderer based on graphics software, a stochastic likelihood model linking the renderer's output and the data, and latent variables that adjust the fidelity of the renderer and the tolerance of the likelihood model. Representations and algorithms from computer graphics, originally designed to produce high-quality images, are instead used as the deterministic backbone for highly approximate and stochastic generative models. This formulation combines probabilistic programming, computer graphics, and approximate Bayesian computation, and depends only on general-purpose, automatic inference techniques. We describe two applications: reading sequences of degraded and adversarially obscured alphanumeric characters, and inferring 3D road models from vehicle-mounted camera images. Each of the probabilistic graphics programs we present relies on under 20 lines of probabilistic code, and supports accurate, approximately Bayesian inferences about ambiguous real-world images.\n    ",
        "submission_date": "2013-06-29T00:00:00",
        "last_modified_date": "2013-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.0339",
        "title": "Syntactic sensitive complexity for symbol-free sequence",
        "authors": [
            "Cheng-Yuan Liou",
            "Bo-Shiang Huang",
            "Daw-Ran Liou",
            "Alex A. Simak"
        ],
        "abstract": "This work uses the L-system to construct a tree structure for the text sequence and derives its complexity. It serves as a measure of structural complexity of the text. It is applied to anomaly detection in data transmission.\n    ",
        "submission_date": "2013-07-01T00:00:00",
        "last_modified_date": "2013-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.0845",
        "title": "The SP theory of intelligence: benefits and applications",
        "authors": [
            "J Gerard Wolff"
        ],
        "abstract": "This article describes existing and expected benefits of the \"SP theory of intelligence\", and some potential applications. The theory aims to simplify and integrate ideas across artificial intelligence, mainstream computing, and human perception and cognition, with information compression as a unifying theme. It combines conceptual simplicity with descriptive and explanatory power across several areas of computing and cognition. In the \"SP machine\" -- an expression of the SP theory which is currently realized in the form of a computer model -- there is potential for an overall simplification of computing systems, including software. The SP theory promises deeper insights and better solutions in several areas of application including, most notably, unsupervised learning, natural language processing, autonomous robots, computer vision, intelligent databases, software engineering, information compression, medical diagnosis and big data. There is also potential in areas such as the semantic web, bioinformatics, structuring of documents, the detection of computer viruses, data fusion, new kinds of computer, and the development of scientific theories. The theory promises seamless integration of structures and functions within and between different areas of application. The potential value, worldwide, of these benefits and applications is at least $190 billion each year. Further development would be facilitated by the creation of a high-parallel, open-source version of the SP machine, available to researchers everywhere.\n    ",
        "submission_date": "2013-06-13T00:00:00",
        "last_modified_date": "2013-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.1070",
        "title": "A Comparison of Non-stationary, Type-2 and Dual Surface Fuzzy Control",
        "authors": [
            "Naisan Benatar",
            "Uwe Aickelin",
            "Jonathan M. Garibaldi"
        ],
        "abstract": "Type-1 fuzzy logic has frequently been used in control systems. However this method is sometimes shown to be too restrictive and unable to adapt in the presence of uncertainty. In this paper we compare type-1 fuzzy control with several other fuzzy approaches under a range of uncertain conditions. Interval type-2 and non-stationary fuzzy controllers are compared, along with 'dual surface' type-2 control, named due to utilising both the lower and upper values produced from standard interval type-2 systems. We tune a type-1 controller, then derive the membership functions and footprints of uncertainty from the type-1 system and evaluate them using a simulated autonomous sailing problem with varying amounts of environmental uncertainty. We show that while these more sophisticated controllers can produce better performance than the type-1 controller, this is not guaranteed and that selection of Footprint of Uncertainty (FOU) size has a large effect on this relative performance.\n    ",
        "submission_date": "2013-07-03T00:00:00",
        "last_modified_date": "2013-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.1388",
        "title": "Introducing Memory and Association Mechanism into a Biologically Inspired Visual Model",
        "authors": [
            "Qiao Hong",
            "Li Yinlin",
            "Tang Tang",
            "Wang Peng"
        ],
        "abstract": "A famous biologically inspired hierarchical model firstly proposed by Riesenhuber and Poggio has been successfully applied to multiple visual recognition tasks. The model is able to achieve a set of position- and scale-tolerant recognition, which is a central problem in pattern recognition. In this paper, based on some other biological experimental results, we introduce the Memory and Association Mechanisms into the above biologically inspired model. The main motivations of the work are (a) to mimic the active memory and association mechanism and add the 'top down' adjustment to the above biologically inspired hierarchical model and (b) to build up an algorithm which can save the space and keep a good recognition performance. The new model is also applied to object recognition processes. The primary experimental results show that our method is efficient with much less memory requirement.\n    ",
        "submission_date": "2013-07-04T00:00:00",
        "last_modified_date": "2013-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.1482",
        "title": "Towards Combining HTN Planning and Geometric Task Planning",
        "authors": [
            "Lavindra de Silva",
            "Amit Kumar Pandey",
            "Mamoun Gharbi",
            "Rachid Alami"
        ],
        "abstract": "In this paper we present an interface between a symbolic planner and a geometric task planner, which is different to a standard trajectory planner in that the former is able to perform geometric reasoning on abstract entities---tasks. We believe that this approach facilitates a more principled interface to symbolic planning, while also leaving more room for the geometric planner to make independent decisions. We show how the two planners could be interfaced, and how their planning and backtracking could be interleaved. We also provide insights for a methodology for using the combined system, and experimental results to use as a benchmark with future extensions to both the combined system, as well as to the geometric task planner.\n    ",
        "submission_date": "2013-07-04T00:00:00",
        "last_modified_date": "2013-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.1568",
        "title": "Using MathML to Represent Units of Measurement for Improved Ontology Alignment",
        "authors": [
            "Chau Do",
            "Eric J. Pauwels"
        ],
        "abstract": "Ontologies provide a formal description of concepts and their relationships in a knowledge domain. The goal of ontology alignment is to identify semantically matching concepts and relationships across independently developed ontologies that purport to describe the same knowledge. In order to handle the widest possible class of ontologies, many alignment algorithms rely on terminological and structural meth- ods, but the often fuzzy nature of concepts complicates the matching process. However, one area that should provide clear matching solutions due to its mathematical nature, is units of measurement. Several on- tologies for units of measurement are available, but there has been no attempt to align them, notwithstanding the obvious importance for tech- nical interoperability. We propose a general strategy to map these (and similar) ontologies by introducing MathML to accurately capture the semantic description of concepts specified therein. We provide mapping results for three ontologies, and show that our approach improves on lexical comparisons.\n    ",
        "submission_date": "2013-07-05T00:00:00",
        "last_modified_date": "2013-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.1790",
        "title": "Lifting Structural Tractability to CSP with Global Constraints",
        "authors": [
            "Evgenij Thorstensen"
        ],
        "abstract": "A wide range of problems can be modelled as constraint satisfaction problems (CSPs), that is, a set of constraints that must be satisfied simultaneously. Constraints can either be represented extensionally, by explicitly listing allowed combinations of values, or implicitly, by special-purpose algorithms provided by a solver. Such implicitly represented constraints, known as global constraints, are widely used; indeed, they are one of the key reasons for the success of constraint programming in solving real-world problems.\n",
        "submission_date": "2013-07-06T00:00:00",
        "last_modified_date": "2013-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.1890",
        "title": "Solution of Rectangular Fuzzy Games by Principle of Dominance Using LR-type Trapezoidal Fuzzy Numbers",
        "authors": [
            "Arindam Chaudhuri"
        ],
        "abstract": "Fuzzy Set Theory has been applied in many fields such as Operations Research, Control Theory, and Management Sciences etc. In particular, an application of this theory in Managerial Decision Making Problems has a remarkable significance. In this Paper, we consider a solution of Rectangular Fuzzy game with pay-off as imprecise numbers instead of crisp numbers viz., interval and LR-type Trapezoidal Fuzzy Numbers. The solution of such Fuzzy games with pure strategies by minimax-maximin principle is discussed. The Algebraic Method to solve Fuzzy games without saddle point by using mixed strategies is also illustrated. Here, pay-off matrix is reduced to pay-off matrix by Dominance Method. This fact is illustrated by means of Numerical Example.\n    ",
        "submission_date": "2013-07-07T00:00:00",
        "last_modified_date": "2013-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.1891",
        "title": "A Comparative study of Transportation Problem under Probabilistic and Fuzzy Uncertainties",
        "authors": [
            "Arindam Chaudhuri",
            "Kajal De"
        ],
        "abstract": "Transportation Problem is an important aspect which has been widely studied in Operations Research domain. It has been studied to simulate different real life problems. In particular, application of this Problem in NP- Hard Problems has a remarkable significance. In this Paper, we present a comparative study of Transportation Problem through Probabilistic and Fuzzy Uncertainties. Fuzzy Logic is a computational paradigm that generalizes classical two-valued logic for reasoning under uncertainty. In order to achieve this, the notation of membership in a set needs to become a matter of degree. By doing this we accomplish two things viz., (i) ease of describing human knowledge involving vague concepts and (ii) enhanced ability to develop cost-effective solution to real-world problem. The multi-valued nature of Fuzzy Sets allows handling uncertain and vague information. It is a model-less approach and a clever disguise of Probability Theory. We give comparative simulation results of both approaches and discuss the Computational Complexity. To the best of our knowledge, this is the first work on comparative study of Transportation Problem using Probabilistic and Fuzzy Uncertainties.\n    ",
        "submission_date": "2013-07-07T00:00:00",
        "last_modified_date": "2013-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.1893",
        "title": "Trapezoidal Fuzzy Numbers for the Transportation Problem",
        "authors": [
            "Arindam Chaudhuri",
            "Kajal De",
            "Dipak Chatterjee",
            "Pabitra Mitra"
        ],
        "abstract": "Transportation Problem is an important problem which has been widely studied in Operations Research domain. It has been often used to simulate different real life problems. In particular, application of this Problem in NP Hard Problems has a remarkable significance. In this Paper, we present the closed, bounded and non empty feasible region of the transportation problem using fuzzy trapezoidal numbers which ensures the existence of an optimal solution to the balanced transportation problem. The multivalued nature of Fuzzy Sets allows handling of uncertainty and vagueness involved in the cost values of each cells in the transportation table. For finding the initial solution of the transportation problem we use the Fuzzy Vogel Approximation Method and for determining the optimality of the obtained solution Fuzzy Modified Distribution Method is used. The fuzzification of the cost of the transportation problem is discussed with the help of a numerical example. Finally, we discuss the computational complexity involved in the problem. To the best of our knowledge, this is the first work on obtaining the solution of the transportation problem using fuzzy trapezoidal numbers.\n    ",
        "submission_date": "2013-07-07T00:00:00",
        "last_modified_date": "2013-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.1895",
        "title": "Discovering Stock Price Prediction Rules of Bombay Stock Exchange Using Rough Fuzzy Multi Layer Perception Networks",
        "authors": [
            "Arindam Chaudhuri",
            "Kajal De",
            "Dipak Chatterjee"
        ],
        "abstract": "In India financial markets have existed for many years. A functionally accented, diverse, efficient and flexible financial system is vital to the national objective of creating a market driven, productive and competitive economy. Today markets of varying maturity exist in equity, debt, commodities and foreign exchange. In this work we attempt to generate prediction rules scheme for stock price movement at Bombay Stock Exchange using an important Soft Computing paradigm viz., Rough Fuzzy Multi Layer Perception. The use of Computational Intelligence Systems such as Neural Networks, Fuzzy Sets, Genetic Algorithms, etc. for Stock Market Predictions has been widely established. The process is to extract knowledge in the form of rules from daily stock movements. These rules can then be used to guide investors. To increase the efficiency of the prediction process, Rough Sets is used to discretize the data. The methodology uses a Genetic Algorithm to obtain a structured network suitable for both classification and rule extraction. The modular concept, based on divide and conquer strategy, provides accelerated training and a compact network suitable for generating a minimum number of rules with high certainty values. The concept of variable mutation operator is introduced for preserving the localized structure of the constituting Knowledge Based sub-networks, while they are integrated and evolved. Rough Set Dependency Rules are generated directly from the real valued attribute table containing Fuzzy membership values. The paradigm is thus used to develop a rule extraction algorithm. The extracted rules are compared with some of the related rule extraction techniques on the basis of some quantitative performance indices. The proposed methodology extracts rules which are less in number, are accurate, have high certainty factor and have low confusion with less computation time.\n    ",
        "submission_date": "2013-07-07T00:00:00",
        "last_modified_date": "2013-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.1900",
        "title": "Fuzzy Integer Linear Programming Mathematical Models for Examination Timetable Problem",
        "authors": [
            "Arindam Chaudhuri",
            "Kajal De"
        ],
        "abstract": "ETP is NP Hard combinatorial optimization problem. It has received tremendous research attention during the past few years given its wide use in universities. In this Paper, we develop three mathematical models for NSOU, Kolkata, India using FILP technique. To deal with impreciseness and vagueness we model various allocation variables through fuzzy numbers. The solution to the problem is obtained using Fuzzy number ranking method. Each feasible solution has fuzzy number obtained by Fuzzy objective function. The different FILP technique performance are demonstrated by experimental data generated through extensive simulation from NSOU, Kolkata, India in terms of its execution times. The proposed FILP models are compared with commonly used heuristic viz. ILP approach on experimental data which gives an idea about quality of heuristic. The techniques are also compared with different Artificial Intelligence based heuristics for ETP with respect to best and mean cost as well as execution time measures on Carter benchmark datasets to illustrate its effectiveness. FILP takes an appreciable amount of time to generate satisfactory solution in comparison to other heuristics. The formulation thus serves as good benchmark for other heuristics. The experimental study presented here focuses on producing a methodology that generalizes well over spectrum of techniques that generates significant results for one or more datasets. The performance of FILP model is finally compared to the best results cited in literature for Carter benchmarks to assess its potential. The problem can be further reduced by formulating with lesser number of allocation variables it without affecting optimality of solution obtained. FLIP model for ETP can also be adapted to solve other ETP as well as combinatorial optimization problems.\n    ",
        "submission_date": "2013-07-07T00:00:00",
        "last_modified_date": "2013-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.1903",
        "title": "Achieving greater Explanatory Power and Forecasting Accuracy with Non-uniform spread Fuzzy Linear Regression",
        "authors": [
            "Arindam Chaudhuri",
            "Kajal De"
        ],
        "abstract": "Fuzzy regression models have been applied to several Operations Research applications viz., forecasting and prediction. Earlier works on fuzzy regression analysis obtain crisp regression coefficients for eliminating the problem of increasing spreads for the estimated fuzzy responses as the magnitude of the independent variable increases. But they cannot deal with the problem of non-uniform spreads. In this work, a three-phase approach is discussed to construct the fuzzy regression model with non-uniform spreads to deal with this problem. The first phase constructs the membership functions of the least-squares estimates of regression coefficients based on extension principle to completely conserve the fuzziness of observations. They are then defuzzified by the centre of area method to obtain crisp regression coefficients in the second phase. Finally, the error terms of the method are determined by setting each estimated spread equal to its corresponding observed spread. The Tagaki-Sugeno inference system is used for improving the accuracy of forecasts. The simulation example demonstrates the strength of fuzzy linear regression model in terms of higher explanatory power and forecasting performance.\n    ",
        "submission_date": "2013-07-07T00:00:00",
        "last_modified_date": "2013-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.1905",
        "title": "A Dynamic Algorithm for the Longest Common Subsequence Problem using Ant Colony Optimization Technique",
        "authors": [
            "Arindam Chaudhuri"
        ],
        "abstract": "We present a dynamic algorithm for solving the Longest Common Subsequence Problem using Ant Colony Optimization Technique. The Ant Colony Optimization Technique has been applied to solve many problems in Optimization Theory, Machine Learning and Telecommunication Networks etc. In particular, application of this theory in NP-Hard Problems has a remarkable significance. Given two strings, the traditional technique for finding Longest Common Subsequence is based on Dynamic Programming which consists of creating a recurrence relation and filling a table of size . The proposed algorithm draws analogy with behavior of ant colonies function and this new computational paradigm is known as Ant System. It is a viable new approach to Stochastic Combinatorial Optimization. The main characteristics of this model are positive feedback, distributed computation, and the use of constructive greedy heuristic. Positive feedback accounts for rapid discovery of good solutions, distributed computation avoids premature convergence and greedy heuristic helps find acceptable solutions in minimum number of stages. We apply the proposed methodology to Longest Common Subsequence Problem and give the simulation results. The effectiveness of this approach is demonstrated by efficient Computational Complexity. To the best of our knowledge, this is the first Ant Colony Optimization Algorithm for Longest Common Subsequence Problem.\n    ",
        "submission_date": "2013-07-07T00:00:00",
        "last_modified_date": "2013-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.2200",
        "title": "Inconsistency and Accuracy of Heuristics with A* Search",
        "authors": [
            "Hang Dinh",
            "Hieu Dinh"
        ],
        "abstract": "Many studies in heuristic search suggest that the accuracy of the heuristic used has a positive impact on improving the performance of the search. In another direction, historical research perceives that the performance of heuristic search algorithms, such as A* and IDA*, can be improved by requiring the heuristics to be consistent -- a property satisfied by any perfect heuristic. However, a few recent studies show that inconsistent heuristics can also be used to achieve a large improvement in these heuristic search algorithms. These results leave us a natural question: which property of heuristics, accuracy or consistency/inconsistency, should we focus on when building heuristics? While there are studies on the heuristic accuracy with the assumption of consistency, no studies on both the inconsistency and the accuracy of heuristics are known to our knowledge.\n",
        "submission_date": "2013-07-08T00:00:00",
        "last_modified_date": "2013-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.2541",
        "title": "Geospatial Narratives and their Spatio-Temporal Dynamics: Commonsense Reasoning for High-level Analyses in Geographic Information Systems",
        "authors": [
            "Mehul Bhatt",
            "Jan Oliver Wallgruen"
        ],
        "abstract": "The modelling, analysis, and visualisation of dynamic geospatial phenomena has been identified as a key developmental challenge for next-generation Geographic Information Systems (GIS). In this context, the envisaged paradigmatic extensions to contemporary foundational GIS technology raises fundamental questions concerning the ontological, formal representational, and (analytical) computational methods that would underlie their spatial information theoretic underpinnings.\n",
        "submission_date": "2013-07-09T00:00:00",
        "last_modified_date": "2013-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.2704",
        "title": "Applications of repeat degree on coverings of neighborhoods",
        "authors": [
            "Hua Yao",
            "William Zhu"
        ],
        "abstract": "In covering based rough sets, the neighborhood of an element is the intersection of all the covering blocks containing the element. All the neighborhoods form a new covering called a covering of neighborhoods. In the course of studying under what condition a covering of neighborhoods is a partition, the concept of repeat degree is proposed, with the help of which the issue is addressed. This paper studies further the application of repeat degree on coverings of neighborhoods. First, we investigate under what condition a covering of neighborhoods is the reduct of the covering inducing it. As a preparation for addressing this issue, we give a necessary and sufficient condition for a subset of a set family to be the reduct of the set family. Then we study under what condition two coverings induce a same relation and a same covering of neighborhoods. Finally, we give the method of calculating the covering according to repeat degree.\n    ",
        "submission_date": "2013-07-10T00:00:00",
        "last_modified_date": "2013-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.2867",
        "title": "Tractable Combinations of Global Constraints",
        "authors": [
            "David A. Cohen",
            "Peter G. Jeavons",
            "Evgenij Thorstensen",
            "Stanislav \u017divn\u00fd"
        ],
        "abstract": "We study the complexity of constraint satisfaction problems involving global constraints, i.e., special-purpose constraints provided by a solver and represented implicitly by a parametrised algorithm. Such constraints are widely used; indeed, they are one of the key reasons for the success of constraint programming in solving real-world problems.\n",
        "submission_date": "2013-07-10T00:00:00",
        "last_modified_date": "2013-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.3040",
        "title": "Between Sense and Sensibility: Declarative narrativisation of mental models as a basis and benchmark for visuo-spatial cognition and computation focussed collaborative cognitive systems",
        "authors": [
            "Mehul Bhatt"
        ],
        "abstract": "What lies between `\\emph{sensing}' and `\\emph{sensibility}'? In other words, what kind of cognitive processes mediate sensing capability, and the formation of sensible impressions ---e.g., abstractions, analogies, hypotheses and theory formation, beliefs and their revision, argument formation--- in domain-specific problem solving, or in regular activities of everyday living, working and simply going around in the environment? How can knowledge and reasoning about such capabilities, as exhibited by humans in particular problem contexts, be used as a model and benchmark for the development of collaborative cognitive (interaction) systems concerned with human assistance, assurance, and empowerment?\n",
        "submission_date": "2013-07-11T00:00:00",
        "last_modified_date": "2014-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.3091",
        "title": "Artificial Intelligence MArkup Language: A Brief Tutorial",
        "authors": [
            "Maria das Gra\u00e7as Bruno Marietto",
            "Rafael Varago de Aguiar",
            "Gislene de Oliveira Barbosa",
            "Wagner Tanaka Botelho",
            "Edson Pimentel",
            "Robson dos Santos Fran\u00e7a",
            "Vera L\u00facia da Silva"
        ],
        "abstract": "The purpose of this paper is to serve as a reference guide for the development of chatterbots implemented with the AIML language. In order to achieve this, the main concepts in Pattern Recognition area are described because the AIML uses such theoretical framework in their syntactic and semantic structures. After that, AIML language is described and each AIML command/tag is followed by an application example. Also, the usage of AIML embedded tags for the handling of sequence dialogue limitations between humans and machines is shown. Finally, computer systems that assist in the design of chatterbots with the AIML language are classified and described.\n    ",
        "submission_date": "2013-07-11T00:00:00",
        "last_modified_date": "2013-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.3195",
        "title": "Action-based Character AI in Video-games with CogBots Architecture: A Preliminary Report",
        "authors": [
            "Davide Aversa",
            "Stavros Vassos"
        ],
        "abstract": "In this paper we propose an architecture for specifying the interaction of non-player characters (NPCs) in the game-world in a way that abstracts common tasks in four main conceptual components, namely perception, deliberation, control, action. We argue that this architecture, inspired by AI research on autonomous agents and robots, can offer a number of benefits in the form of abstraction, modularity, re-usability and higher degrees of personalization for the behavior of each NPC. We also show how this architecture can be used to tackle a simple scenario related to the navigation of NPCs under incomplete information about the obstacles that may obstruct the various way-points in the game, in a simple and effective way.\n    ",
        "submission_date": "2013-07-11T00:00:00",
        "last_modified_date": "2013-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.3435",
        "title": "On Nicod's Condition, Rules of Induction and the Raven Paradox",
        "authors": [
            "Hadi Mohasel Afshar",
            "Peter Sunehag"
        ],
        "abstract": "Philosophers writing about the ravens paradox often note that Nicod's Condition (NC) holds given some set of background information, and fails to hold against others, but rarely go any further. That is, it is usually not explored which background information makes NC true or false. The present paper aims to fill this gap. For us, \"(objective) background knowledge\" is restricted to information that can be expressed as probability events. Any other configuration is regarded as being subjective and a property of the a priori probability distribution. We study NC in two specific settings. In the first case, a complete description of some individuals is known, e.g. one knows of each of a group of individuals whether they are black and whether they are ravens. In the second case, the number of individuals having a particular property is given, e.g. one knows how many ravens or how many black things there are (in the relevant population). While some of the most famous answers to the paradox are measure-dependent, our discussion is not restricted to any particular probability measure. Our most interesting result is that in the second setting, NC violates a simple kind of inductive inference (namely projectability). Since relative to NC, this latter rule is more closely related to, and more directly justified by our intuitive notion of inductive reasoning, this tension makes a case against the plausibility of NC. In the end, we suggest that the informal representation of NC may seem to be intuitively plausible because it can easily be mistaken for reasoning by analogy.\n    ",
        "submission_date": "2013-07-12T00:00:00",
        "last_modified_date": "2013-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.3585",
        "title": "Improving MUC extraction thanks to local search",
        "authors": [
            "\u00c9ric Gr\u00e9goire",
            "Jean-Marie Lagniez",
            "Bertrand Mazure"
        ],
        "abstract": "ExtractingMUCs(MinimalUnsatisfiableCores)fromanunsatisfiable constraint network is a useful process when causes of unsatisfiability must be understood so that the network can be re-engineered and relaxed to become sat- isfiable. Despite bad worst-case computational complexity results, various MUC- finding approaches that appear tractable for many real-life instances have been proposed. Many of them are based on the successive identification of so-called transition constraints. In this respect, we show how local search can be used to possibly extract additional transition constraints at each main iteration step. The approach is shown to outperform a technique based on a form of model rotation imported from the SAT-related technology and that also exhibits additional transi- tion constraints. Our extensive computational experimentations show that this en- hancement also boosts the performance of state-of-the-art DC(WCORE)-like MUC extractors.\n    ",
        "submission_date": "2013-07-12T00:00:00",
        "last_modified_date": "2013-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.3964",
        "title": "Learning Markov networks with context-specific independences",
        "authors": [
            "Alejandro Edera",
            "Federico Schl\u00fcter",
            "Facundo Bromberg"
        ],
        "abstract": "Learning the Markov network structure from data is a problem that has received considerable attention in machine learning, and in many other application fields. This work focuses on a particular approach for this purpose called independence-based learning. Such approach guarantees the learning of the correct structure efficiently, whenever data is sufficient for representing the underlying distribution. However, an important issue of such approach is that the learned structures are encoded in an undirected graph. The problem with graphs is that they cannot encode some types of independence relations, such as the context-specific independences. They are a particular case of conditional independences that is true only for a certain assignment of its conditioning set, in contrast to conditional independences that must hold for all its assignments. In this work we present CSPC, an independence-based algorithm for learning structures that encode context-specific independences, and encoding them in a log-linear model, instead of a graph. The central idea of CSPC is combining the theoretical guarantees provided by the independence-based approach with the benefits of representing complex structures by using features in a log-linear model. We present experiments in a synthetic case, showing that CSPC is more accurate than the state-of-the-art IB algorithms when the underlying distribution contains CSIs.\n    ",
        "submission_date": "2013-07-15T00:00:00",
        "last_modified_date": "2013-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.4440",
        "title": "Parameterized Complexity Results for Plan Reuse",
        "authors": [
            "Ronald de Haan",
            "Anna Roub\u00ed\u010dkov\u00e1",
            "Stefan Szeider"
        ],
        "abstract": "Planning is a notoriously difficult computational problem of high worst-case complexity. Researchers have been investing significant efforts to develop heuristics or restrictions to make planning practically feasible. Case-based planning is a heuristic approach where one tries to reuse previous experience when solving similar problems in order to avoid some of the planning effort. Plan reuse may offer an interesting alternative to plan generation in some settings.\n",
        "submission_date": "2013-07-16T00:00:00",
        "last_modified_date": "2013-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.4689",
        "title": "DASH: Dynamic Approach for Switching Heuristics",
        "authors": [
            "Giovanni Di Liberto",
            "Serdar Kadioglu",
            "Kevin Leo",
            "Yuri Malitsky"
        ],
        "abstract": "Complete tree search is a highly effective method for tackling MIP problems, and over the years, a plethora of branching heuristics have been introduced to further refine the technique for varying problems. Recently, portfolio algorithms have taken the process a step further, trying to predict the best heuristic for each instance at hand. However, the motivation behind algorithm selection can be taken further still, and used to dynamically choose the most appropriate algorithm for each encountered subproblem. In this paper we identify a feature space that captures both the evolution of the problem in the branching tree and the similarity among subproblems of instances from the same MIP models. We show how to exploit these features to decide the best time to switch the branching heuristic and then show how such a system can be trained efficiently. Experiments on a highly heterogeneous collection of MIP instances show significant gains over the pure algorithm selection approach that for a given instance uses only a single heuristic throughout the search.\n    ",
        "submission_date": "2013-07-17T00:00:00",
        "last_modified_date": "2013-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.5322",
        "title": "Ontology alignment repair through modularization and confidence-based heuristics",
        "authors": [
            "Emanuel Santos",
            "Daniel Faria",
            "C\u00e1tia Pesquita",
            "Francisco Couto"
        ],
        "abstract": "Ontology Matching aims to find a set of semantic correspondences, called an alignment, between related ontologies. In recent years, there has been a growing interest in efficient and effective matching methods for large ontologies. However, most of the alignments produced for large ontologies are logically incoherent. It was only recently that the use of repair techniques to improve the quality of ontology alignments has been explored. In this paper we present a novel technique for detecting incoherent concepts based on ontology modularization, and a new repair algorithm that minimizes the incoherence of the resulting alignment and the number of matches removed from the input alignment. An implementation was done as part of a lightweight version of AgreementMaker system, a successful ontology matching platform, and evaluated using a set of four benchmark biomedical ontology matching tasks. Our results show that our implementation is efficient and produces better alignments with respect to their coherence and f-measure than the state of the art repairing tools. They also show that our implementation is a better alternative for producing coherent silver standard alignments.\n    ",
        "submission_date": "2013-07-19T00:00:00",
        "last_modified_date": "2013-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.5910",
        "title": "How to minimize the energy consumption in mobile ad-hoc networks",
        "authors": [
            "Abdellah Idrissi"
        ],
        "abstract": "In this work we are interested in the problem of energy management in Mobile Ad-hoc Network (MANET). The solving and optimization of MANET allow assisting the users to efficiently use their devices in order to minimize the batteries power consumption. In this framework, we propose a modelling of the MANET in form of a Constraint Optimization Problem called COMANET. Then, in the objective to minimize the consumption of batteries power, we present an approach based on an adaptation of the A star algorithm to the MANET problem called MANED. Finally, we expose some experimental results showing utility of this approach.\n    ",
        "submission_date": "2013-07-22T00:00:00",
        "last_modified_date": "2013-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.6023",
        "title": "The Use of Cuckoo Search in Estimating the Parameters of Software Reliability Growth Models",
        "authors": [
            "Dr. Najla Akram AL-Saati",
            "Marwa Abd-AlKareem"
        ],
        "abstract": "This work aims to investigate the reliability of software products as an important attribute of computer programs; it helps to decide the degree of trustworthiness a program has in accomplishing its specific functions. This is done using the Software Reliability Growth Models (SRGMs) through the estimation of their parameters. The parameters are estimated in this work based on the available failure data and with the search techniques of Swarm Intelligence, namely, the Cuckoo Search (CS) due to its efficiency, effectiveness and robustness. A number of SRGMs is studied, and the results are compared to Particle Swarm Optimization (PSO), Ant Colony Optimization (ACO) and extended ACO. Results show that CS outperformed both PSO and ACO in finding better parameters tested using identical datasets. It was sometimes outperformed by the extended ACO. Also in this work, the percentages of training data to testing data are investigated to show their impact on the results.\n    ",
        "submission_date": "2013-07-23T00:00:00",
        "last_modified_date": "2013-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.6291",
        "title": "A novel approach of solving the CNF-SAT problem",
        "authors": [
            "Xili Wang"
        ],
        "abstract": "In this paper, we discussed CNF-SAT problem (NP-Complete problem) and analysis two solutions that can solve the problem, the PL-Resolution algorithm and the WalkSAT algorithm. PL-Resolution is a sound and complete algorithm that can be used to determine satisfiability and unsatisfiability with certainty. WalkSAT can determine satisfiability if it finds a model, but it cannot guarantee to find a model even there exists one. However, WalkSAT is much faster than PL-Resolution, which makes WalkSAT more practical; and we have analysis the performance between these two algorithms, and the performance of WalkSAT is acceptable if the problem is not so hard.\n    ",
        "submission_date": "2013-07-24T00:00:00",
        "last_modified_date": "2013-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.6365",
        "title": "Time-Series Classification Through Histograms of Symbolic Polynomials",
        "authors": [
            "Josif Grabocka",
            "Martin Wistuba",
            "Lars Schmidt-Thieme"
        ],
        "abstract": "Time-series classification has attracted considerable research attention due to the various domains where time-series data are observed, ranging from medicine to econometrics. Traditionally, the focus of time-series classification has been on short time-series data composed of a unique pattern with intraclass pattern distortions and variations, while recently there have been attempts to focus on longer series composed of various local patterns. This study presents a novel method which can detect local patterns in long time-series via fitting local polynomial functions of arbitrary degrees. The coefficients of the polynomial functions are converted to symbolic words via equivolume discretizations of the coefficients' distributions. The symbolic polynomial words enable the detection of similar local patterns by assigning the same words to similar polynomials. Moreover, a histogram of the frequencies of the words is constructed from each time-series' bag of words. Each row of the histogram enables a new representation for the series and symbolize the existence of local patterns and their frequencies. Experimental evidence demonstrates outstanding results of our method compared to the state-of-art baselines, by exhibiting the best classification accuracies in all the datasets and having statistically significant improvements in the absolute majority of experiments.\n    ",
        "submission_date": "2013-07-24T00:00:00",
        "last_modified_date": "2013-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.7351",
        "title": "Knowledge Representation for Robots through Human-Robot Interaction",
        "authors": [
            "Emanuele Bastianelli",
            "Domenico Bloisi",
            "Roberto Capobianco",
            "Guglielmo Gemignani",
            "Luca Iocchi",
            "Daniele Nardi"
        ],
        "abstract": "The representation of the knowledge needed by a robot to perform complex tasks is restricted by the limitations of perception. One possible way of overcoming this situation and designing \"knowledgeable\" robots is to rely on the interaction with the user. We propose a multi-modal interaction framework that allows to effectively acquire knowledge about the environment where the robot operates. In particular, in this paper we present a rich representation framework that can be automatically built from the metric map annotated with the indications provided by the user. Such a representation, allows then the robot to ground complex referential expressions for motion commands and to devise topological navigation plans to achieve the target locations.\n    ",
        "submission_date": "2013-07-28T00:00:00",
        "last_modified_date": "2013-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.7466",
        "title": "Integration of 3D Object Recognition and Planning for Robotic Manipulation: A Preliminary Report",
        "authors": [
            "Damien Jade Duff",
            "Esra Erdem",
            "Volkan Patoglu"
        ],
        "abstract": "We investigate different approaches to integrating object recognition and planning in a tabletop manipulation domain with the set of objects used in the 2012 RoboCup@Work competition. Results of our preliminary experiments show that, with some approaches, close integration of perception and planning improves the quality of plans, as well as the computation times of feasible plans.\n    ",
        "submission_date": "2013-07-29T00:00:00",
        "last_modified_date": "2013-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.7494",
        "title": "ReAct! An Interactive Tool for Hybrid Planning in Robotics",
        "authors": [
            "Zeynep Dogmus",
            "Esra Erdem",
            "Volkan Patoglu"
        ],
        "abstract": "We present ReAct!, an interactive tool for high-level reasoning for cognitive robotic applications. ReAct! enables robotic researchers to describe robots' actions and change in dynamic domains, without having to know about the syntactic and semantic details of the underlying formalism in advance, and solve planning problems using state-of-the-art automated reasoners, without having to learn about their input/output language or usage. In particular, ReAct! can be used to represent sophisticated dynamic domains that feature concurrency, indirect effects of actions, and state/transition constraints. It allows for embedding externally defined calculations (e.g., checking for collision-free continuous trajectories) into representations of hybrid domains that require a tight integration of (discrete) high-level reasoning with (continuous) geometric reasoning. ReAct! also enables users to solve planning problems that involve complex goals. Such variety of utilities are useful for robotic researchers to work on interesting and challenging domains, ranging from service robotics to cognitive factories. ReAct! provides sample formalizations of some action domains (e.g., multi-agent path planning, Tower of Hanoi), as well as dynamic simulations of plans computed by a state-of-the-art automated reasoner (e.g., a SAT solver or an ASP solver).\n    ",
        "submission_date": "2013-07-29T00:00:00",
        "last_modified_date": "2013-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.7720",
        "title": "Herding the Crowd: Automated Planning for Crowdsourced Planning",
        "authors": [
            "Kartik Talamadupula",
            "Subbarao Kambhampati"
        ],
        "abstract": "There has been significant interest in crowdsourcing and human computation. One subclass of human computation applications are those directed at tasks that involve planning (e.g. travel planning) and scheduling (e.g. conference scheduling). Much of this work appears outside the traditional automated planning forums, and at the outset it is not clear whether automated planning has much of a role to play in these human computation systems. Interestingly however, work on these systems shows that even primitive forms of automated oversight of the human planner does help in significantly improving the effectiveness of the humans/crowd. In this paper, we will argue that the automated oversight used in these systems can be viewed as a primitive automated planner, and that there are several opportunities for more sophisticated automated planning in effectively steering crowdsourced planning. Straightforward adaptation of current planning technology is however hampered by the mismatch between the capabilities of human workers and automated planners. We identify two important challenges that need to be overcome before such adaptation of planning technology can occur: (i) interpreting the inputs of the human workers (and the requester) and (ii) steering or critiquing the plans being produced by the human workers armed only with incomplete domain and preference models. In this paper, we discuss approaches for handling these challenges, and characterize existing human computation systems in terms of the specific choices they make in handling these challenges.\n    ",
        "submission_date": "2013-07-29T00:00:00",
        "last_modified_date": "2013-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.7808",
        "title": "Automated Attack Planning",
        "authors": [
            "Carlos Sarraute"
        ],
        "abstract": "Penetration Testing is a methodology for assessing network security, by generating and executing possible attacks. Doing so automatically allows for regular and systematic testing. A key question then is how to automatically generate the attacks. A natural way to address this issue is as an attack planning problem. In this thesis, we are concerned with the specific context of regular automated pentesting, and use the term \"attack planning\" in that sense. The following three research directions are investigated.\n",
        "submission_date": "2013-07-30T00:00:00",
        "last_modified_date": "2013-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.7809",
        "title": "Les POMDP font de meilleurs hackers: Tenir compte de l'incertitude dans les tests de penetration",
        "authors": [
            "Carlos Sarraute",
            "Olivier Buffet",
            "Joerg Hoffmann"
        ],
        "abstract": "Penetration Testing is a methodology for assessing network security, by generating and executing possible hacking attacks. Doing so automatically allows for regular and systematic testing. A key question is how to generate the attacks. This is naturally formulated as planning under uncertainty, i.e., under incomplete knowledge about the network configuration. Previous work uses classical planning, and requires costly pre-processes reducing this uncertainty by extensive application of scanning methods. By contrast, we herein model the attack planning problem in terms of partially observable Markov decision processes (POMDP). This allows to reason about the knowledge available, and to intelligently employ scanning actions as part of the attack. As one would expect, this accurate solution does not scale. We devise a method that relies on POMDPs to find good attacks on individual machines, which are then composed into an attack on the network as a whole. This decomposition exploits network structure to the extent possible, making targeted approximations (only) where needed. Evaluating this method on a suitably adapted industrial test suite, we demonstrate its effectiveness in both runtime and solution quality.\n    ",
        "submission_date": "2013-07-30T00:00:00",
        "last_modified_date": "2013-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.8084",
        "title": "Combining Answer Set Programming and POMDPs for Knowledge Representation and Reasoning on Mobile Robots",
        "authors": [
            "Shiqi Zhang",
            "Mohan Sridharan"
        ],
        "abstract": "For widespread deployment in domains characterized by partial observability, non-deterministic actions and unforeseen changes, robots need to adapt sensing, processing and interaction with humans to the tasks at hand. While robots typically cannot process all sensor inputs or operate without substantial domain knowledge, it is a challenge to provide accurate domain knowledge and humans may not have the time and expertise to provide elaborate and accurate feedback. The architecture described in this paper combines declarative programming and probabilistic reasoning to address these challenges, enabling robots to: (a) represent and reason with incomplete domain knowledge, resolving ambiguities and revising existing knowledge using sensor inputs and minimal human feedback; and (b) probabilistically model the uncertainty in sensor input processing and navigation. Specifically, Answer Set Programming (ASP), a declarative programming paradigm, is combined with hierarchical partially observable Markov decision processes (POMDPs), using domain knowledge to revise probabilistic beliefs, and using positive and negative observations for early termination of tasks that can no longer be pursued. All algorithms are evaluated in simulation and on mobile robots locating target objects in indoor domains.\n    ",
        "submission_date": "2013-07-29T00:00:00",
        "last_modified_date": "2013-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.8182",
        "title": "POMDPs Make Better Hackers: Accounting for Uncertainty in Penetration Testing",
        "authors": [
            "Carlos Sarraute",
            "Olivier Buffet",
            "Joerg Hoffmann"
        ],
        "abstract": "Penetration Testing is a methodology for assessing network security, by generating and executing possible hacking attacks. Doing so automatically allows for regular and systematic testing. A key question is how to generate the attacks. This is naturally formulated as planning under uncertainty, i.e., under incomplete knowledge about the network configuration. Previous work uses classical planning, and requires costly pre-processes reducing this uncertainty by extensive application of scanning methods. By contrast, we herein model the attack planning problem in terms of partially observable Markov decision processes (POMDP). This allows to reason about the knowledge available, and to intelligently employ scanning actions as part of the attack. As one would expect, this accurate solution does not scale. We devise a method that relies on POMDPs to find good attacks on individual machines, which are then composed into an attack on the network as a whole. This decomposition exploits network structure to the extent possible, making targeted approximations (only) where needed. Evaluating this method on a suitably adapted industrial test suite, we demonstrate its effectiveness in both runtime and solution quality.\n    ",
        "submission_date": "2013-07-31T00:00:00",
        "last_modified_date": "2013-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.8279",
        "title": "Tracking Extrema in Dynamic Environment using Multi-Swarm Cellular PSO with Local Search",
        "authors": [
            "Somayeh Nabizadeh",
            "Alireza Rezvanian",
            "Mohammad Reza Meybodi"
        ],
        "abstract": "Many real-world phenomena can be modelled as dynamic optimization problems. In such cases, the environment problem changes dynamically and therefore, conventional methods are not capable of dealing with such problems. In this paper, a novel multi-swarm cellular particle swarm optimization algorithm is proposed by clustering and local search. In the proposed algorithm, the search space is partitioned into cells, while the particles identify changes in the search space and form clusters to create sub-swarms. Then a local search is applied to improve the solutions in the each cell. Simulation results for static standard benchmarks and dynamic environments show superiority of the proposed method over other alternative approaches.\n    ",
        "submission_date": "2013-07-31T00:00:00",
        "last_modified_date": "2013-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.0187",
        "title": "A Time and Space Efficient Junction Tree Architecture",
        "authors": [
            "Stephen Pasteris"
        ],
        "abstract": "The junction tree algorithm is a way of computing marginals of boolean multivariate probability distributions that factorise over sets of random variables. The junction tree algorithm first constructs a tree called a junction tree who's vertices are sets of random variables. The algorithm then performs a generalised version of belief propagation on the junction tree. The Shafer-Shenoy and Hugin architectures are two ways to perform this belief propagation that tradeoff time and space complexities in different ways: Hugin propagation is at least as fast as Shafer-Shenoy propagation and in the cases that we have large vertices of high degree is significantly faster. However, this speed increase comes at the cost of an increased space complexity. This paper first introduces a simple novel architecture, ARCH-1, which has the best of both worlds: the speed of Hugin propagation and the low space requirements of Shafer-Shenoy propagation. A more complicated novel architecture, ARCH-2, is then introduced which has, up to a factor only linear in the maximum cardinality of any vertex, time and space complexities at least as good as ARCH-1 and in the cases that we have large vertices of high degree is significantly faster than ARCH-1.\n    ",
        "submission_date": "2013-07-31T00:00:00",
        "last_modified_date": "2014-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.0227",
        "title": "An Enhanced Features Extractor for a Portfolio of Constraint Solvers",
        "authors": [
            "Roberto Amadini",
            "Maurizio Gabbrielli",
            "Jacopo Mauro"
        ],
        "abstract": "Recent research has shown that a single arbitrarily efficient solver can be significantly outperformed by a portfolio of possibly slower on-average solvers. The solver selection is usually done by means of (un)supervised learning techniques which exploit features extracted from the problem specification. In this paper we present an useful and flexible framework that is able to extract an extensive set of features from a Constraint (Satisfaction/Optimization) Problem defined in possibly different modeling languages: MiniZinc, FlatZinc or XCSP. We also report some empirical results showing that the performances that can be obtained using these features are effective and competitive with state of the art CSP portfolio techniques.\n    ",
        "submission_date": "2013-08-01T00:00:00",
        "last_modified_date": "2014-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.0299",
        "title": "Exact and Heuristic Methods for the Assembly Line Worker Assignment and Balancing Problem",
        "authors": [
            "Leonardo Borba",
            "Marcus Ritt"
        ],
        "abstract": "In traditional assembly lines, it is reasonable to assume that task execution times are the same for each worker. However, in sheltered work centres for disabled this assumption is not valid: some workers may execute some tasks considerably slower or even be incapable of executing them. Worker heterogeneity leads to a problem called the assembly line worker assignment and balancing problem (ALWABP). For a fixed number of workers the problem is to maximize the production rate of an assembly line by assigning workers to stations and tasks to workers, while satisfying precedence constraints between the tasks. This paper introduces new heuristic and exact methods to solve this problem. We present a new MIP model, propose a novel heuristic algorithm based on beam search, as well as a task-oriented branch-and-bound procedure which uses new reduction rules and lower bounds for solving the problem. Extensive computational tests on a large set of instances show that these methods are effective and improve over existing ones.\n    ",
        "submission_date": "2013-08-01T00:00:00",
        "last_modified_date": "2013-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.0356",
        "title": "Design and Development of an Expert System to Help Head of University Departments",
        "authors": [
            "Shervan Fekri-Ershad",
            "Hadi Tajalizadeh",
            "Shahram Jafari"
        ],
        "abstract": "One of the basic tasks which is responded for head of each university department, is employing lecturers based on some default factors such as experience, evidences, qualifies and etc. In this respect, to help the heads, some automatic systems have been proposed until now using machine learning methods, decision support systems (DSS) and etc. According to advantages and disadvantages of the previous methods, a full automatic system is designed in this paper using expert systems. The proposed system is included two main steps. In the first one, the human expert's knowledge is designed as decision trees. The second step is included an expert system which is evaluated using extracted rules of these decision trees. Also, to improve the quality of the proposed system, a majority voting algorithm is proposed as post processing step to choose the best lecturer which satisfied more expert's decision trees for each course. The results are shown that the designed system average accuracy is 78.88. Low computational complexity, simplicity to program and are some of other advantages of the proposed system.\n    ",
        "submission_date": "2013-08-01T00:00:00",
        "last_modified_date": "2013-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.0702",
        "title": "Universal Empathy and Ethical Bias for Artificial General Intelligence",
        "authors": [
            "Alexey Potapov",
            "Sergey Rodionov"
        ],
        "abstract": "Rational agents are usually built to maximize rewards. However, AGI agents can find undesirable ways of maximizing any prior reward function. Therefore value learning is crucial for safe AGI. We assume that generalized states of the world are valuable - not rewards themselves, and propose an extension of AIXI, in which rewards are used only to bootstrap hierarchical value learning. The modified AIXI agent is considered in the multi-agent environment, where other agents can be either humans or other \"mature\" agents, which values should be revealed and adopted by the \"infant\" AGI agent. General framework for designing such empathic agent with ethical bias is proposed also as an extension of the universal intelligence model. Moreover, we perform experiments in the simple Markov environment, which demonstrate feasibility of our approach to value learning in safe AGI.\n    ",
        "submission_date": "2013-08-03T00:00:00",
        "last_modified_date": "2013-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.0761",
        "title": "On estimating total time to solve SAT in distributed computing environments: Application to the SAT@home project",
        "authors": [
            "Alexander Semenov",
            "Oleg Zaikin"
        ],
        "abstract": "This paper proposes a method to estimate the total time required to solve SAT in distributed environments via partitioning approach. It is based on the observation that for some simple forms of problem partitioning one can use the Monte Carlo approach to estimate the time required to solve an original problem. The method proposed is based on an algorithm for searching for partitioning with an optimal solving time estimation. We applied this method to estimate the time required to perform logical cryptanalysis of the widely known stream ciphers A5/1 and Bivium. The paper also describes a volunteer computing project SAT@home aimed at solving hard combinatorial problems reduced to SAT. In this project during several months there were solved 10 problems of logical cryptanalysis of the A5/1 cipher thatcould not be solved using known rainbow tables.\n    ",
        "submission_date": "2013-08-04T00:00:00",
        "last_modified_date": "2013-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.0807",
        "title": "Stratified Labelings for Abstract Argumentation",
        "authors": [
            "Matthias Thimm",
            "Gabriele Kern-Isberner"
        ],
        "abstract": "We introduce stratified labelings as a novel semantical approach to abstract argumentation frameworks. Compared to standard labelings, stratified labelings provide a more fine-grained assessment of the controversiality of arguments using ranks instead of the usual labels in, out, and undecided. We relate the framework of stratified labelings to conditional logic and, in particular, to the System Z ranking functions.\n    ",
        "submission_date": "2013-08-04T00:00:00",
        "last_modified_date": "2013-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.1262",
        "title": "Pattern recognition issues on anisotropic smoothed particle hydrodynamics",
        "authors": [
            "Eraldo Pereira Marinho"
        ],
        "abstract": "This is a preliminary theoretical discussion on the computational requirements of the state of the art smoothed particle hydrodynamics (SPH) from the optics of pattern recognition and artificial intelligence. It is pointed out in the present paper that, when including anisotropy detection to improve resolution on shock layer, SPH is a very peculiar case of unsupervised machine learning. On the other hand, the free particle nature of SPH opens an opportunity for artificial intelligence to study particles as agents acting in a collaborative framework in which the timed outcomes of a fluid simulation forms a large knowledge base, which might be very attractive in computational astrophysics phenomenological problems like self-propagating star formation.\n    ",
        "submission_date": "2013-08-06T00:00:00",
        "last_modified_date": "2013-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.2116",
        "title": "MaLeS: A Framework for Automatic Tuning of Automated Theorem Provers",
        "authors": [
            "Daniel K\u00fchlwein",
            "Josef Urban"
        ],
        "abstract": "MaLeS is an automatic tuning framework for automated theorem provers. It provides solutions for both the strategy finding as well as the strategy scheduling problem. This paper describes the tool and the methods used in it, and evaluates its performance on three automated theorem provers: E, LEO-II and Satallax. An evaluation on a subset of the TPTP library problems shows that on average a MaLeS-tuned prover solves 8.67% more problems than the prover with its default settings.\n    ",
        "submission_date": "2013-08-09T00:00:00",
        "last_modified_date": "2014-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.2119",
        "title": "Deconstructing analogy",
        "authors": [
            "Mark Keane"
        ],
        "abstract": "Analogy has been shown to be important in many key cognitive abilities, including learning, problem solving, creativity and language change. For cognitive models of analogy, the fundamental computational question is how its inherent complexity (its NP-hardness) is solved by the human cognitive system. Indeed, different models of analogical processing can be categorized by the simplification strategies they adopt to make this computational problem more tractable. In this paper, I deconstruct several of these models in terms of the simplification-strategies they use; a deconstruction that provides some interesting perspectives on the relative differences between them. Later, I consider whether any of these computational simplifications reflect the actual strategies used by people and sketch a new cognitive model that tries to present a closer fit to the psychological evidence.\n    ",
        "submission_date": "2013-08-09T00:00:00",
        "last_modified_date": "2013-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.2124",
        "title": "Space as an invention of biological organisms",
        "authors": [
            "Alexander V. Terekhov",
            "J. Kevin O'Regan"
        ],
        "abstract": "The question of the nature of space around us has occupied thinkers since the dawn of humanity, with scientists and philosophers today implicitly assuming that space is something that exists objectively. Here we show that this does not have to be the case: the notion of space could emerge when biological organisms seek an economic representation of their sensorimotor flow. The emergence of spatial notions does not necessitate the existence of real physical space, but only requires the presence of sensorimotor invariants called `compensable' sensory changes. We show mathematically and then in simulations that na\u00efve agents making no assumptions about the existence of space are able to learn these invariants and to build the abstract notion that physicists call rigid displacement, which is independent of what is being displaced. Rigid displacements may underly perception of space as an unchanging medium within which objects are described by their relative positions. Our findings suggest that the question of the nature of space, currently exclusive to philosophy and physics, should also be addressed from the standpoint of neuroscience and artificial intelligence.\n    ",
        "submission_date": "2013-08-09T00:00:00",
        "last_modified_date": "2013-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.2234",
        "title": "Innovation networks",
        "authors": [
            "Petra Ahrweiler",
            "Mark T. Keane"
        ],
        "abstract": "This paper advances a framework for modeling the component interactions between cognitive and social aspects of scientific creativity and technological innovation. Specifically, it aims to characterize Innovation Networks; those networks that involve the interplay of people, ideas and organizations to create new, technologically feasible, commercially-realizable products, processes and organizational structures. The tri-partite framework captures networks of ideas (Concept Level), people (Individual Level) and social structures (Social-Organizational Level) and the interactions between these levels. At the concept level, new ideas are the nodes that are created and linked, kept open for further investigation or closed if solved by actors at the individual or organizational levels. At the individual level, the nodes are actors linked by shared worldviews (based on shared professional, educational, experiential backgrounds) who are the builders of the concept level. At the social-organizational level, the nodes are organizations linked by common efforts on a given project (e.g., a company-university collaboration) that by virtue of their intellectual property or rules of governance constrain the actions of individuals (at the Individual Level) or ideas (at the Concept Level). After describing this framework and its implications we paint a number of scenarios to flesh out how it can be applied.\n    ",
        "submission_date": "2013-08-09T00:00:00",
        "last_modified_date": "2013-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.2236",
        "title": "Surprise: Youve got some explaining to do",
        "authors": [
            "Meadhbh Foster",
            "Mark T. Keane"
        ],
        "abstract": "Why are some events more surprising than others? We propose that events that are more difficult to explain are those that are more surprising. The two experiments reported here test the impact of different event outcomes (Outcome-Type) and task demands (Task) on ratings of surprise for simple story scenarios. For the Outcome-Type variable, participants saw outcomes that were either known or less-known surprising outcomes for each scenario. For the Task variable, participants either answered comprehension questions or provided an explanation of the outcome. Outcome-Type reliably affected surprise judgments; known outcomes were rated as less surprising than less-known outcomes. Task also reliably affected surprise judgments; when people provided an explanation it lowered surprise judgments relative to simply answering comprehension questions. Both experiments thus provide evidence on this less-explored explanation aspect of surprise, specifically showing that ease of explanation is a key factor in determining the level of surprise experienced.\n    ",
        "submission_date": "2013-08-09T00:00:00",
        "last_modified_date": "2013-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.2240",
        "title": "Cognitive residues of similarity",
        "authors": [
            "Stephanie OToole",
            "Mark T. Keane"
        ],
        "abstract": "What are the cognitive after-effects of making a similarity judgement? What, cognitively, is left behind and what effect might these residues have on subsequent processing? In this paper, we probe for such after-effects using a visual search task, performed after a task in which pictures of real-world objects were compared. So, target objects were first presented in a comparison task (e.g., rate the similarity of this object to another) thus, presumably, modifying some of their features before asking people to visually search for the same object in complex scenes (with distractors and camouflaged backgrounds). As visual search is known to be influenced by the features of target objects, then any after-effects of the comparison task should be revealed in subsequent visual searches. Results showed that when people previously rated an object as being high on a scale (e.g., colour similarity or general similarity) then visual search is inhibited (slower RTs and more saccades in eye-tracking) relative to an object being rated as low in the same scale. There was also some evidence that different comparison tasks (e.g., compare on colour or compare on general similarity) have differential effects on visual search.\n    ",
        "submission_date": "2013-08-09T00:00:00",
        "last_modified_date": "2013-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.2309",
        "title": "Applying the Negative Selection Algorithm for Merger and Acquisition Target Identification",
        "authors": [
            "Satyakama Paul",
            "Andreas Janecek",
            "Fernando Buarque de Lima Neto",
            "Tshilidzi Marwala"
        ],
        "abstract": "In this paper, we propose a new methodology based on the Negative Selection Algorithm that belongs to the field of Computational Intelligence, specifically, Artificial Immune Systems to identify takeover targets. Although considerable research based on customary statistical techniques and some contemporary Computational Intelligence techniques have been devoted to identify takeover targets, most of the existing studies are based upon multiple previous mergers and acquisitions. Contrary to previous research, the novelty of this proposal lies in its ability to suggest takeover targets for novice firms that are at the beginning of their merger and acquisition spree. We first discuss the theoretical perspective and then provide a case study with details for practical implementation, both capitalizing from unique generalization capabilities of artificial immune systems algorithms.\n    ",
        "submission_date": "2013-08-10T00:00:00",
        "last_modified_date": "2013-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.2443",
        "title": "Fighting Sample Degeneracy and Impoverishment in Particle Filters: A Review of Intelligent Approaches",
        "authors": [
            "Tiancheng Li",
            "Shudong Sun",
            "Tariq P. Sattar",
            "Juan M. Corchado"
        ],
        "abstract": "During the last two decades there has been a growing interest in Particle Filtering (PF). However, PF suffers from two long-standing problems that are referred to as sample degeneracy and impoverishment. We are investigating methods that are particularly efficient at Particle Distribution Optimization (PDO) to fight sample degeneracy and impoverishment, with an emphasis on intelligence choices. These methods benefit from such methods as Markov Chain Monte Carlo methods, Mean-shift algorithms, artificial intelligence algorithms (e.g., Particle Swarm Optimization, Genetic Algorithm and Ant Colony Optimization), machine learning approaches (e.g., clustering, splitting and merging) and their hybrids, forming a coherent standpoint to enhance the particle filter. The working mechanism, interrelationship, pros and cons of these approaches are provided. In addition, Approaches that are effective for dealing with high-dimensionality are reviewed. While improving the filter performance in terms of accuracy, robustness and convergence, it is noted that advanced techniques employed in PF often causes additional computational requirement that will in turn sacrifice improvement obtained in real life filtering. This fact, hidden in pure simulations, deserves the attention of the users and designers of new filters.\n    ",
        "submission_date": "2013-08-12T00:00:00",
        "last_modified_date": "2014-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.2772",
        "title": "Extended Distributed Learning Automata:A New Method for Solving Stochastic Graph Optimization Problems",
        "authors": [
            "M.R.Mollakhalili Meybodi",
            "M.R.Meybodi"
        ],
        "abstract": "In this paper, a new structure of cooperative learning automata so-called extended learning automata (eDLA) is introduced. Based on the proposed structure, a new iterative randomized heuristic algorithm for finding optimal sub-graph in a stochastic edge-weighted graph through sampling is proposed. It has been shown that the proposed algorithm based on new networked-structure can be to solve the optimization problems on stochastic graph through less number of sampling in compare to standard sampling. Stochastic graphs are graphs in which the edges have an unknown distribution probability weights. Proposed algorithm uses an eDLA to find a policy that leads to an induced sub-graph that satisfies some restrictions such as minimum or maximum weight (length). At each stage of the proposed algorithm, eDLA determines which edges to be sampled. This eDLA-based proposed sampling method may result in decreasing unnecessary samples and hence decreasing the time that algorithm requires for finding the optimal sub-graph. It has been shown that proposed method converge to optimal solution, furthermore the probability of this convergence can be made arbitrarily close to 1 by using a sufficiently small learning rate. A new variance-aware threshold value was proposed that can be improving significantly convergence rate of the proposed eDLA-based algorithm. It has been shown that the proposed algorithm is competitive in terms of the quality of the solution\n    ",
        "submission_date": "2013-08-13T00:00:00",
        "last_modified_date": "2013-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.3309",
        "title": "Search-Space Characterization for Real-time Heuristic Search",
        "authors": [
            "Daniel Huntley",
            "Vadim Bulitko"
        ],
        "abstract": "Recent real-time heuristic search algorithms have demonstrated outstanding performance in video-game pathfinding. However, their applications have been thus far limited to that domain. We proceed with the aim of facilitating wider applications of real-time search by fostering a greater understanding of the performance of recent algorithms. We first introduce eight algorithm-independent complexity measures for search spaces and correlate their values with algorithm performance. The complexity measures are statistically shown to be significant predictors of algorithm performance across a set of commercial video-game maps. We then extend this analysis to a wider variety of search spaces in the first application of database-driven real-time search to domains outside of video-game pathfinding. In doing so, we gain insight into algorithm performance and possible enhancement as well as into search space complexity.\n    ",
        "submission_date": "2013-08-15T00:00:00",
        "last_modified_date": "2013-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.3784",
        "title": "Graph Colouring Problem Based on Discrete Imperialist Competitive Algorithm",
        "authors": [
            "Hojjat Emami",
            "Shahriar Lotfi"
        ],
        "abstract": "In graph theory, Graph Colouring Problem (GCP) is an assignment of colours to vertices of any given graph such that the colours on adjacent vertices are different. The GCP is known to be an optimization and NP-hard problem. Imperialist Competitive Algorithm (ICA) is a meta-heuristic optimization and stochastic search strategy which is inspired from socio-political phenomenon of imperialistic competition. The ICA contains two main operators: the assimilation and the imperialistic competition. The ICA has excellent capabilities such as high convergence rate and better global optimum achievement. In this research, a discrete version of ICA is proposed to deal with the solution of GCP. We call this algorithm as the DICA. The performance of the proposed method is compared with Genetic Algorithm (GA) on seven well-known graph colouring benchmarks. Experimental results demonstrate the superiority of the DICA for the benchmarks. This means DICA can produce optimal and valid solutions for different GCP instances.\n    ",
        "submission_date": "2013-08-17T00:00:00",
        "last_modified_date": "2013-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.3847",
        "title": "Exploiting Binary Floating-Point Representations for Constraint Propagation: The Complete Unabridged Version",
        "authors": [
            "Roberto Bagnara",
            "Matthieu Carlier",
            "Roberta Gori",
            "Arnaud Gotlieb"
        ],
        "abstract": "Floating-point computations are quickly finding their way in the design of safety- and mission-critical systems, despite the fact that designing floating-point algorithms is significantly more difficult than designing integer algorithms. For this reason, verification and validation of floating-point computations is a hot research topic. An important verification technique, especially in some industrial sectors, is testing. However, generating test data for floating-point intensive programs proved to be a challenging problem. Existing approaches usually resort to random or search-based test data generation, but without symbolic reasoning it is almost impossible to generate test inputs that execute complex paths controlled by floating-point computations. Moreover, as constraint solvers over the reals or the rationals do not natively support the handling of rounding errors, the need arises for efficient constraint solvers over floating-point domains. In this paper, we present and fully justify improved algorithms for the propagation of arithmetic IEEE 754 binary floating-point constraints. The key point of these algorithms is a generalization of an idea by B. Marre and C. Michel that exploits a property of the representation of floating-point numbers.\n    ",
        "submission_date": "2013-08-18T00:00:00",
        "last_modified_date": "2015-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.3900",
        "title": "Bat Algorithm: Literature Review and Applications",
        "authors": [
            "Xin-She Yang"
        ],
        "abstract": "Bat algorithm (BA) is a bio-inspired algorithm developed by Yang in 2010 and BA has been found to be very efficient. As a result, the literature has expanded significantly in the last 3 years. This paper provides a timely review of the bat algorithm and its new variants. A wide range of diverse applications and case studies are also reviewed and summarized briefly here. Further research topics are also discussed.\n    ",
        "submission_date": "2013-08-18T00:00:00",
        "last_modified_date": "2013-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.4008",
        "title": "A Literature Survey of Benchmark Functions For Global Optimization Problems",
        "authors": [
            "Momin Jamil",
            "Xin-She Yang"
        ],
        "abstract": "Test functions are important to validate and compare the performance of optimization algorithms. There have been many test or benchmark functions reported in the literature; however, there is no standard list or set of benchmark functions. Ideally, test functions should have diverse properties so that can be truly useful to test new algorithms in an unbiased way. For this purpose, we have reviewed and compiled a rich set of 175 benchmark functions for unconstrained optimization problems with diverse properties in terms of modality, separability, and valley landscape. This is by far the most complete set of functions so far in the literature, and tt can be expected this complete set of functions can be used for validation of new optimization in the future.\n    ",
        "submission_date": "2013-08-19T00:00:00",
        "last_modified_date": "2013-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.4761",
        "title": "Matching Demand with Supply in the Smart Grid using Agent-Based Multiunit Auction",
        "authors": [
            "Tri Kurniawan Wijaya",
            "Kate Larson",
            "Karl Aberer"
        ],
        "abstract": "Recent work has suggested reducing electricity generation cost by cutting the peak to average ratio (PAR) without reducing the total amount of the loads. However, most of these proposals rely on consumer's willingness to act. In this paper, we propose an approach to cut PAR explicitly from the supply side. The resulting cut loads are then distributed among consumers by the means of a multiunit auction which is done by an intelligent agent on behalf of the consumer. This approach is also in line with the future vision of the smart grid to have the demand side matched with the supply side. Experiments suggest that our approach reduces overall system cost and gives benefit to both consumers and the energy provider.\n    ",
        "submission_date": "2013-08-22T00:00:00",
        "last_modified_date": "2013-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.4846",
        "title": "POMDPs under Probabilistic Semantics",
        "authors": [
            "Krishnendu Chatterjee",
            "Martin Chmel\u00edk"
        ],
        "abstract": "We consider partially observable Markov decision processes (POMDPs) with limit-average payoff, where a reward value in the interval [0,1] is associated to every transition, and the payoff of an infinite path is the long-run average of the rewards. We consider two types of path constraints: (i) quantitative constraint defines the set of paths where the payoff is at least a given threshold {\\lambda} in (0, 1]; and (ii) qualitative constraint which is a special case of quantitative constraint with {\\lambda} = 1. We consider the computation of the almost-sure winning set, where the controller needs to ensure that the path constraint is satisfied with probability 1. Our main results for qualitative path constraint are as follows: (i) the problem of deciding the existence of a finite-memory controller is EXPTIME-complete; and (ii) the problem of deciding the existence of an infinite-memory controller is undecidable. For quantitative path constraint we show that the problem of deciding the existence of a finite-memory controller is undecidable.\n    ",
        "submission_date": "2013-08-22T00:00:00",
        "last_modified_date": "2013-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.4943",
        "title": "David Poole's Specificity Revised",
        "authors": [
            "Claus-Peter Wirth",
            "Frieder Stolzenburg"
        ],
        "abstract": "In the middle of the 1980s, David Poole introduced a semantical, model-theoretic notion of specificity to the artificial-intelligence community. Since then it has found further applications in non-monotonic reasoning, in particular in defeasible reasoning. Poole tried to approximate the intuitive human concept of specificity, which seems to be essential for reasoning in everyday life with its partial and inconsistent information. His notion, however, turns out to be intricate and problematic, which --- as we show --- can be overcome to some extent by a closer approximation of the intuitive human concept of specificity. Besides the intuitive advantages of our novel specificity ordering over Poole's specificity relation in the classical examples of the literature, we also report some hard mathematical facts: Contrary to what was claimed before, we show that Poole's relation is not transitive. The present means to decide our novel specificity relation, however, show only a slight improvement over the known ones for Poole's relation, and further work is needed in this aspect.\n    ",
        "submission_date": "2013-08-22T00:00:00",
        "last_modified_date": "2013-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.5046",
        "title": "The Fractal Dimension of SAT Formulas",
        "authors": [
            "C. Ans\u00f3tegui",
            "M. L. Bonet",
            "J. Gir\u00e1ldez-Cru",
            "J. Levy"
        ],
        "abstract": "Modern SAT solvers have experienced a remarkable progress on solving industrial instances. Most of the techniques have been developed after an intensive experimental testing process. Recently, there have been some attempts to analyze the structure of these formulas in terms of complex networks, with the long-term aim of explaining the success of these SAT solving techniques, and possibly improving them.\n",
        "submission_date": "2013-08-23T00:00:00",
        "last_modified_date": "2013-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.5136",
        "title": "Extending Similarity Measures of Interval Type-2 Fuzzy Sets to General Type-2 Fuzzy Sets",
        "authors": [
            "Josie McCulloch",
            "Christian Wagner",
            "Uwe Aickelin"
        ],
        "abstract": "Similarity measures provide one of the core tools that enable reasoning about fuzzy sets. While many types of similarity measures exist for type-1 and interval type-2 fuzzy sets, there are very few similarity measures that enable the comparison of general type-2 fuzzy sets. In this paper, we introduce a general method for extending existing interval type-2 similarity measures to similarity measures for general type-2 fuzzy sets. Specifically, we show how similarity measures for interval type-2 fuzzy sets can be employed in conjunction with the zSlices based general type-2 representation for fuzzy sets to provide measures of similarity which preserve all the common properties (i.e. reflexivity, symmetry, transitivity and overlapping) of the original interval type-2 similarity measure. We demonstrate examples of such extended fuzzy measures and provide comparisons between (different types of) interval and general type-2 fuzzy measures.\n    ",
        "submission_date": "2013-08-23T00:00:00",
        "last_modified_date": "2013-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.5137",
        "title": "Measuring the Directional Distance Between Fuzzy Sets",
        "authors": [
            "Josie McCulloch",
            "Christian Wagner",
            "Uwe Aickelin"
        ],
        "abstract": "The measure of distance between two fuzzy sets is a fundamental tool within fuzzy set theory. However, current distance measures within the literature do not account for the direction of change between fuzzy sets; a useful concept in a variety of applications, such as Computing With Words. In this paper, we highlight this utility and introduce a distance measure which takes the direction between sets into account. We provide details of its application for normal and non-normal, as well as convex and non-convex fuzzy sets. We demonstrate the new distance measure using real data from the MovieLens dataset and establish the benefits of measuring the direction between fuzzy sets.\n    ",
        "submission_date": "2013-08-23T00:00:00",
        "last_modified_date": "2013-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.5321",
        "title": "Evolution Theory of Self-Evolving Autonomous Problem Solving Systems",
        "authors": [
            "Seppo Ilari Tirri"
        ],
        "abstract": "The present study gives a mathematical framework for self-evolution within autonomous problem solving systems. Special attention is set on universal abstraction, thereof generation by net block homomorphism, consequently multiple order solving systems and the overall decidability of the set of the solutions. By overlapping presentation of nets new abstraction relation among nets is formulated alongside with consequent alphabetical net block renetting system proportional to normal forms of renetting systems regarding the operational power. A new structure in self-evolving problem solving is established via saturation by groups of equivalence relations and iterative closures of generated quotient transducer algebras over the whole evolution.\n    ",
        "submission_date": "2013-08-24T00:00:00",
        "last_modified_date": "2013-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.5374",
        "title": "Dynamic Reasoning Systems",
        "authors": [
            "Daniel G. Schwartz"
        ],
        "abstract": "A {\\it dynamic reasoning system} (DRS) is an adaptation of a conventional formal logical system that explicitly portrays reasoning as a temporal activity, with each extralogical input to the system and each inference rule application being viewed as occurring at a distinct time step. Every DRS incorporates some well-defined logic together with a controller that serves to guide the reasoning process in response to user inputs. Logics are generic, whereas controllers are application-specific. Every controller does, nonetheless, provide an algorithm for nonmonotonic belief revision. The general notion of a DRS comprises a framework within which one can formulate the logic and algorithms for a given application and prove that the algorithms are correct, i.e., that they serve to (i) derive all salient information and (ii) preserve the consistency of the belief set. This paper illustrates the idea with ordinary first-order predicate calculus, suitably modified for the present purpose, and two examples. The latter example revisits some classic nonmonotonic reasoning puzzles (Opus the Penguin, Nixon Diamond) and shows how these can be resolved in the context of a DRS, using an expanded version of first-order logic that incorporates typed predicate symbols. All concepts are rigorously defined and effectively computable, thereby providing the foundation for a future software implementation.\n    ",
        "submission_date": "2013-08-25T00:00:00",
        "last_modified_date": "2013-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.6206",
        "title": "The Partner Units Configuration Problem: Completing the Picture",
        "authors": [
            "Erich Christian Teppan",
            "Gerhard Friedrich"
        ],
        "abstract": "The partner units problem (PUP) is an acknowledged hard benchmark problem for the Logic Programming community with various industrial application fields like surveillance, electrical engineering, computer networks or railway safety systems. However, computational complexity remained widely unclear so far. In this paper we provide all missing complexity results making the PUP better exploitable for benchmark testing. Furthermore, we present QuickPup, a heuristic search algorithm for PUP instances which outperforms all state-of-the-art solving approaches and which is already in use in real world industrial configuration environments.\n    ",
        "submission_date": "2013-08-28T00:00:00",
        "last_modified_date": "2013-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.6292",
        "title": "Verification of Semantically-Enhanced Artifact Systems (Extended Version)",
        "authors": [
            "Babak Bagheri Hariri",
            "Diego Calvanese",
            "Marco Montali",
            "Ario Santoso",
            "Dmitry Solomakhin"
        ],
        "abstract": "Artifact-Centric systems have emerged in the last years as a suitable framework to model business-relevant entities, by combining their static and dynamic aspects. In particular, the Guard-Stage-Milestone (GSM) approach has been recently proposed to model artifacts and their lifecycle in a declarative way. In this paper, we enhance GSM with a Semantic Layer, constituted by a full-fledged OWL 2 QL ontology linked to the artifact information models through mapping specifications. The ontology provides a conceptual view of the domain under study, and allows one to understand the evolution of the artifact system at a higher level of abstraction. In this setting, we present a technique to specify temporal properties expressed over the Semantic Layer, and verify them according to the evolution in the underlying GSM model. This technique has been implemented in a tool that exploits state-of-the-art ontology-based data access technologies to manipulate the temporal properties according to the ontology and the mappings, and that relies on the GSMC model checker for verification.\n    ",
        "submission_date": "2013-08-28T00:00:00",
        "last_modified_date": "2013-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.6415",
        "title": "Learning-Based Procedural Content Generation",
        "authors": [
            "Jonathan Roberts",
            "Ke Chen"
        ],
        "abstract": "Procedural content generation (PCG) has recently become one of the hottest topics in computational intelligence and AI game researches. Among a variety of PCG techniques, search-based approaches overwhelmingly dominate PCG development at present. While SBPCG leads to promising results and successful applications, it poses a number of challenges ranging from representation to evaluation of the content being generated. In this paper, we present an alternative yet generic PCG framework, named learning-based procedure content generation (LBPCG), to provide potential solutions to several challenging problems in existing PCG techniques. By exploring and exploiting information gained in game development and public beta test via data-driven learning, our framework can generate robust content adaptable to end-user or target players on-line with minimal interruption to their experience. Furthermore, we develop enabling techniques to implement the various models required in our framework. For a proof of concept, we have developed a prototype based on the classic open source first-person shooter game, Quake. Simulation results suggest that our framework is promising in generating quality content.\n    ",
        "submission_date": "2013-08-29T00:00:00",
        "last_modified_date": "2013-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.6823",
        "title": "A Hypergraph-Partitioned Vertex Programming Approach for Large-scale Consensus Optimization",
        "authors": [
            "Hui Miao",
            "Xiangyang Liu",
            "Bert Huang",
            "Lise Getoor"
        ],
        "abstract": "In modern data science problems, techniques for extracting value from big data require performing large-scale optimization over heterogenous, irregularly structured data. Much of this data is best represented as multi-relational graphs, making vertex programming abstractions such as those of Pregel and GraphLab ideal fits for modern large-scale data analysis. In this paper, we describe a vertex-programming implementation of a popular consensus optimization technique known as the alternating direction of multipliers (ADMM). ADMM consensus optimization allows elegant solution of complex objectives such as inference in rich probabilistic models. We also introduce a novel hypergraph partitioning technique that improves over state-of-the-art partitioning techniques for vertex programming and significantly reduces the communication cost by reducing the number of replicated nodes up to an order of magnitude. We implemented our algorithm in GraphLab and measure scaling performance on a variety of realistic bipartite graph distributions and a large synthetic voter-opinion analysis application. In our experiments, we are able to achieve a 50% improvement in runtime over the current state-of-the-art GraphLab partitioning scheme.\n    ",
        "submission_date": "2013-08-30T00:00:00",
        "last_modified_date": "2013-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.0363",
        "title": "Sigma Point Belief Propagation",
        "authors": [
            "Florian Meyer",
            "Ondrej Hlinka",
            "Franz Hlawatsch"
        ],
        "abstract": "The sigma point (SP) filter, also known as unscented Kalman filter, is an attractive alternative to the extended Kalman filter and the particle filter. Here, we extend the SP filter to nonsequential Bayesian inference corresponding to loopy factor graphs. We propose sigma point belief propagation (SPBP) as a low-complexity approximation of the belief propagation (BP) message passing scheme. SPBP achieves approximate marginalizations of posterior distributions corresponding to (generally) loopy factor graphs. It is well suited for decentralized inference because of its low communication requirements. For a decentralized, dynamic sensor localization problem, we demonstrate that SPBP can outperform nonparametric (particle-based) BP while requiring significantly less computations and communications.\n    ",
        "submission_date": "2013-09-02T00:00:00",
        "last_modified_date": "2013-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.0659",
        "title": "Majority Rule for Belief Evolution in Social Networks",
        "authors": [
            "Yi Zhou"
        ],
        "abstract": "In this paper, we study how an agent's belief is affected by her neighbors in a social network. We first introduce a general framework, where every agent has an initial belief on a statement, and updates her belief according to her and her neighbors' current beliefs under some belief evolution functions, which, arguably, should satisfy some basic properties. Then, we focus on the majority rule belief evolution function, that is, an agent will (dis)believe the statement iff more than half of her neighbors (dis)believe it. We consider some fundamental issues about majority rule belief evolution, for instance, whether the belief evolution process will eventually converge. The answer is no in general. However, for random asynchronous belief evolution, this is indeed the case.\n    ",
        "submission_date": "2013-09-03T00:00:00",
        "last_modified_date": "2013-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.1226",
        "title": "Graded Causation and Defaults",
        "authors": [
            "Joseph Y. Halpern",
            "Christopher Hitchcock"
        ],
        "abstract": "Recent work in psychology and experimental philosophy has shown that judgments of actual causation are often influenced by consideration of defaults, typicality, and normality. A number of philosophers and computer scientists have also suggested that an appeal to such factors can help deal with problems facing existing accounts of actual causation. This paper develops a flexible formal framework for incorporating defaults, typicality, and normality into an account of actual causation. The resulting account takes actual causation to be both graded and comparative. We then show how our account would handle a number of standard cases.\n    ",
        "submission_date": "2013-09-05T00:00:00",
        "last_modified_date": "2013-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.1227",
        "title": "Compact Representations of Extended Causal Models",
        "authors": [
            "Joseph Y. Halpern",
            "Christopher Hitchcock"
        ],
        "abstract": "Judea Pearl was the first to propose a definition of actual causation using causal models. A number of authors have suggested that an adequate account of actual causation must appeal not only to causal structure, but also to considerations of normality. In earlier work, we provided a definition of actual causation using extended causal models, which include information about both causal structure and normality. Extended causal models are potentially very complex. In this paper, we show how it is possible to achieve a compact representation of extended causal models.\n    ",
        "submission_date": "2013-09-05T00:00:00",
        "last_modified_date": "2013-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.1228",
        "title": "Weighted regret-based likelihood: a new approach to describing uncertainty",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "abstract": "Recently, Halpern and Leung suggested representing uncertainty by a weighted set of probability measures, and suggested a way of making decisions based on this representation of uncertainty: maximizing weighted regret. Their paper does not answer an apparently simpler question: what it means, according to this representation of uncertainty, for an event E to be more likely than an event E'. In this paper, a notion of comparative likelihood when uncertainty is represented by a weighted set of probability measures is defined. It generalizes the ordering defined by probability (and by lower probability) in a natural way; a generalization of upper probability can also be defined. A complete axiomatic characterization of this notion of regret-based likelihood is given.\n    ",
        "submission_date": "2013-09-05T00:00:00",
        "last_modified_date": "2013-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.1973",
        "title": "Regret-Based Multi-Agent Coordination with Uncertain Task Rewards",
        "authors": [
            "Feng Wu",
            "Nicholas R. Jennings"
        ],
        "abstract": "Many multi-agent coordination problems can be represented as DCOPs. Motivated by task allocation in disaster response, we extend standard DCOP models to consider uncertain task rewards where the outcome of completing a task depends on its current state, which is randomly drawn from unknown distributions. The goal of solving this problem is to find a solution for all agents that minimizes the overall worst-case loss. This is a challenging problem for centralized algorithms because the search space grows exponentially with the number of agents and is nontrivial for standard DCOP algorithms we have. To address this, we propose a novel decentralized algorithm that incorporates Max-Sum with iterative constraint generation to solve the problem by passing messages among agents. By so doing, our approach scales well and can solve instances of the task allocation problem with hundreds of agents and tasks.\n    ",
        "submission_date": "2013-09-08T00:00:00",
        "last_modified_date": "2013-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.2351",
        "title": "Elementos de ingenier\u00eda de explotaci\u00f3n de la informaci\u00f3n aplicados a la investigaci\u00f3n tributaria fiscal",
        "authors": [
            "Rodrigo Lopez-Pablos"
        ],
        "abstract": "By introducing elements of information mining to tax analysis, by means of data mining software and advanced computational concepts of artificial intelligence, the problem of tax evader's crime against public property has been addressed. Through an empirical approach from a hypothetical case of use, induction algorithms, neural networks and bayesian networks are applied to determine the feasibility of its heuristic application by the tax public administrator. Different strategies are explored to facilitate the work of local and regional federal tax inspectors, considering their limited computational capabilities, but equally effective for those social scientist committed to handcrafting tax research.\n",
        "submission_date": "2013-09-10T00:00:00",
        "last_modified_date": "2013-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.2693",
        "title": "A Conflict-Based Path-Generation Heuristic for Evacuation Planning",
        "authors": [
            "Victor Pillac",
            "Pascal Van Henetenryck",
            "Caroline Even"
        ],
        "abstract": "Evacuation planning and scheduling is a critical aspect of disaster management and national security applications. This paper proposes a conflict-based path-generation approach for evacuation planning. Its key idea is to generate evacuation routes lazily for evacuated areas and to optimize the evacuation over these routes in a master problem. Each new path is generated to remedy conflicts in the evacuation and adds new columns and a new row in the master problem. The algorithm is applied to massive flood scenarios in the Hawkesbury-Nepean river (West Sydney, Australia) which require evacuating in the order of 70,000 persons. The proposed approach reduces the number of variables from 4,500,000 in a Mixed Integer Programming (MIP) formulation to 30,000 in the case study. With this approach, realistic evacuations scenarios can be solved near-optimally in real time, supporting both evacuation planning in strategic, tactical, and operational environments.\n    ",
        "submission_date": "2013-09-10T00:00:00",
        "last_modified_date": "2013-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.2747",
        "title": "Approximate Counting CSP Solutions Using Partition Function",
        "authors": [
            "Junping Zhou",
            "Weihua Su",
            "Minghao Yin"
        ],
        "abstract": "We propose a new approximate method for counting the number of the solutions for constraint satisfaction problem (CSP). The method derives from the partition function based on introducing the free energy and capturing the relationship of probabilities of variables and constraints, which requires the marginal probabilities. It firstly obtains the marginal probabilities using the belief propagation, and then computes the number of solutions according to the partition function. This allows us to directly plug the marginal probabilities into the partition function and efficiently count the number of solutions for CSP. The experimental results show that our method can solve both random problems and structural problems efficiently.\n    ",
        "submission_date": "2013-09-11T00:00:00",
        "last_modified_date": "2013-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.3039",
        "title": "How Relevant Are Chess Composition Conventions?",
        "authors": [
            "Azlan Iqbal"
        ],
        "abstract": "Composition conventions are guidelines used by human composers in composing chess problems. They are particularly significant in composition tournaments. Examples include, not having any check in the first move of the solution and not dressing up the board with unnecessary pieces. Conventions are often associated or even directly conflated with the overall aesthetics or beauty of a composition. Using an existing experimentally-validated computational aesthetics model for three-move mate problems, we analyzed sets of computer-generated compositions adhering to at least 2, 3 and 4 comparable conventions to test if simply conforming to more conventions had a positive effect on their aesthetics, as is generally believed by human composers. We found slight but statistically significant evidence that it does, but only to a point. We also analyzed human judge scores of 145 three-move mate problems composed by humans to see if they had any positive correlation with the computational aesthetic scores of those problems. We found that they did not. These seemingly conflicting findings suggest two main things. First, the right amount of adherence to composition conventions in a composition has a positive effect on its perceived aesthetics. Second, human judges either do not look at the same conventions related to aesthetics in the model used or emphasize others that have less to do with beauty as perceived by the majority of players, even though they may mistakenly consider their judgements beautiful in the traditional, non-esoteric sense. Human judges may also be relying significantly on personal tastes as we found no correlation between their individual scores either.\n    ",
        "submission_date": "2013-09-12T00:00:00",
        "last_modified_date": "2016-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.3242",
        "title": "Using memristor crossbar structure to implement a novel adaptive real time fuzzy modeling algorithm",
        "authors": [
            "Iman Esmaili Paeen Afrakoti",
            "Saeed Bagheri Shouraki",
            "Farnood Merrikhbayat"
        ],
        "abstract": "Although fuzzy techniques promise fast meanwhile accurate modeling and control abilities for complicated systems, different difficulties have been re-vealed in real situation implementations. Usually there is no escape of it-erative optimization based on crisp domain algorithms. Recently memristor structures appeared promising to implement neural network structures and fuzzy algorithms. In this paper a novel adaptive real-time fuzzy modeling algorithm is proposed which uses active learning method concept to mimic recent understandings of right brain processing techniques. The developed method is based on processing fuzzy numbers to provide the ability of being sensitive to each training data point to expand the knowledge tree leading to plasticity while used defuzzification technique guaranties enough stability. An outstanding characteristic of the proposed algorithm is its consistency to memristor crossbar hardware processing concepts. An analog implemen-tation of the proposed algorithm on memristor crossbars structure is also introduced in this paper. The effectiveness of the proposed algorithm in modeling and pattern recognition tasks is verified by means of computer simulations\n    ",
        "submission_date": "2013-09-12T00:00:00",
        "last_modified_date": "2013-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.3285",
        "title": "A tabu search algorithm with efficient diversification strategy for high school timetabling problem",
        "authors": [
            "Salman Hooshmand",
            "Mehdi Behshameh",
            "Omid Hamidi"
        ],
        "abstract": "The school timetabling problem can be described as scheduling a set of lessons (combination of classes, teachers, subjects and rooms) in a weekly timetable. This paper presents a novel way to generate timetables for high schools. The algorithm has three phases. Pre-scheduling, initial phase and optimization through tabu search. In the first phase, a graph based algorithm used to create groups of lessons to be scheduled simultaneously; then an initial solution is built by a sequential greedy heuristic. Finally, the solution is optimized using tabu search algorithm based on frequency based diversification. The algorithm has been tested on a set of real problems gathered from Iranian high schools. Experiments show that the proposed algorithm can effectively build acceptable timetables.\n    ",
        "submission_date": "2013-09-12T00:00:00",
        "last_modified_date": "2013-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.3611",
        "title": "Ultrametric Component Analysis with Application to Analysis of Text and of Emotion",
        "authors": [
            "Fionn Murtagh"
        ],
        "abstract": "We review the theory and practice of determining what parts of a data set are ultrametric. It is assumed that the data set, to begin with, is endowed with a metric, and we include discussion of how this can be brought about if a dissimilarity, only, holds. The basis for part of the metric-endowed data set being ultrametric is to consider triplets of the observables (vectors). We develop a novel consensus of hierarchical clusterings. We do this in order to have a framework (including visualization and supporting interpretation) for the parts of the data that are determined to be ultrametric. Furthermore a major objective is to determine locally ultrametric relationships as opposed to non-local ultrametric relationships. As part of this work, we also study a particular property of our ultrametricity coefficient, namely, it being a function of the difference of angles of the base angles of the isosceles triangle. This work is completed by a review of related work, on consensus hierarchies, and of a major new application, namely quantifying and interpreting the emotional content of narrative.\n    ",
        "submission_date": "2013-09-14T00:00:00",
        "last_modified_date": "2013-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.3917",
        "title": "Strategic Planning in Air Traffic Control as a Multi-objective Stochastic Optimization Problem",
        "authors": [
            "Ga\u00e9tan Marceau",
            "Pierre Sav\u00e9ant",
            "Marc Schoenauer"
        ],
        "abstract": "With the objective of handling the airspace sector congestion subject to continuously growing air traffic, we suggest to create a collaborative working plan during the strategic phase of air traffic control. The plan obtained via a new decision support tool presented in this article consists in a schedule for controllers, which specifies time of overflight on the different waypoints of the flight plans. In order to do it, we believe that the decision-support tool shall model directly the uncertainty at a trajectory level in order to propagate the uncertainty to the sector level. Then, the probability of congestion for any sector in the airspace can be computed. Since air traffic regulations and sector congestion are antagonist, we designed and implemented a multi-objective optimization algorithm for determining the best trade-off between these two criteria. The solution comes up as a set of alternatives for the multi-sector planner where the severity of the congestion cost is adjustable. In this paper, the Non-dominated Sorting Genetic Algorithm (NSGA-II) was used to solve an artificial benchmark problem involving 24 aircraft and 11 sectors, and is able to provide a good approximation of the Pareto front.\n    ",
        "submission_date": "2013-09-16T00:00:00",
        "last_modified_date": "2013-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.3921",
        "title": "Computational Methods for Probabilistic Inference of Sector Congestion in Air Traffic Management",
        "authors": [
            "Ga\u00e9tan Marceau",
            "Pierre Sav\u00e9ant",
            "Marc Schoenauer"
        ],
        "abstract": "This article addresses the issue of computing the expected cost functions from a probabilistic model of the air traffic flow and capacity management. The Clenshaw-Curtis quadrature is compared to Monte-Carlo algorithms defined specifically for this problem. By tailoring the algorithms to this model, we reduce the computational burden in order to simulate real instances. The study shows that the Monte-Carlo algorithm is more sensible to the amount of uncertainty in the system, but has the advantage to return a result with the associated accuracy on demand. The performances for both approaches are comparable for the computation of the expected cost of delay and the expected cost of congestion. Finally, this study shows some evidences that the simulation of the proposed probabilistic model is tractable for realistic instances.\n    ",
        "submission_date": "2013-09-16T00:00:00",
        "last_modified_date": "2013-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.4085",
        "title": "Multiobjective Tactical Planning under Uncertainty for Air Traffic Flow and Capacity Management",
        "authors": [
            "Ga\u00e9tan Marceau",
            "Pierre Sav\u00e9ant",
            "Marc Schoenauer"
        ],
        "abstract": "We investigate a method to deal with congestion of sectors and delays in the tactical phase of air traffic flow and capacity management. It relies on temporal objectives given for every point of the flight plans and shared among the controllers in order to create a collaborative environment. This would enhance the transition from the network view of the flow management to the local view of air traffic control. Uncertainty is modeled at the trajectory level with temporal information on the boundary points of the crossed sectors and then, we infer the probabilistic occupancy count. Therefore, we can model the accuracy of the trajectory prediction in the optimization process in order to fix some safety margins. On the one hand, more accurate is our prediction; more efficient will be the proposed solutions, because of the tighter safety margins. On the other hand, when uncertainty is not negligible, the proposed solutions will be more robust to disruptions. Furthermore, a multiobjective algorithm is used to find the tradeoff between the delays and congestion, which are antagonist in airspace with high traffic density. The flow management position can choose manually, or automatically with a preference-based algorithm, the adequate solution. This method is tested against two instances, one with 10 flights and 5 sectors and one with 300 flights and 16 sectors.\n    ",
        "submission_date": "2013-09-16T00:00:00",
        "last_modified_date": "2013-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.4408",
        "title": "Lambda Dependency-Based Compositional Semantics",
        "authors": [
            "Percy Liang"
        ],
        "abstract": "This short note presents a new formal language, lambda dependency-based compositional semantics (lambda DCS) for representing logical forms in semantic parsing. By eliminating variables and making existential quantification implicit, lambda DCS logical forms are generally more compact than those in lambda calculus.\n    ",
        "submission_date": "2013-09-17T00:00:00",
        "last_modified_date": "2013-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.4501",
        "title": "A fully automatic problem solver with human-style output",
        "authors": [
            "M. Ganesalingam",
            "W. T. Gowers"
        ],
        "abstract": "This paper describes a program that solves elementary mathematical problems, mostly in metric space theory, and presents solutions that are hard to distinguish from solutions that might be written by human mathematicians. The program is part of a more general project, which we also discuss.\n    ",
        "submission_date": "2013-09-17T00:00:00",
        "last_modified_date": "2013-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.4714",
        "title": "Temporal-Difference Learning to Assist Human Decision Making during the Control of an Artificial Limb",
        "authors": [
            "Ann L. Edwards",
            "Alexandra Kearney",
            "Michael Rory Dawson",
            "Richard S. Sutton",
            "Patrick M. Pilarski"
        ],
        "abstract": "In this work we explore the use of reinforcement learning (RL) to help with human decision making, combining state-of-the-art RL algorithms with an application to prosthetics. Managing human-machine interaction is a problem of considerable scope, and the simplification of human-robot interfaces is especially important in the domains of biomedical technology and rehabilitation medicine. For example, amputees who control artificial limbs are often required to quickly switch between a number of control actions or modes of operation in order to operate their devices. We suggest that by learning to anticipate (predict) a user's behaviour, artificial limbs could take on an active role in a human's control decisions so as to reduce the burden on their users. Recently, we showed that RL in the form of general value functions (GVFs) could be used to accurately detect a user's control intent prior to their explicit control choices. In the present work, we explore the use of temporal-difference learning and GVFs to predict when users will switch their control influence between the different motor functions of a robot arm. Experiments were performed using a multi-function robot arm that was controlled by muscle signals from a user's body (similar to conventional artificial limb control). Our approach was able to acquire and maintain forecasts about a user's switching decisions in real time. It also provides an intuitive and reward-free way for users to correct or reinforce the decisions made by the machine learning system. We expect that when a system is certain enough about its predictions, it can begin to take over switching decisions from the user to streamline control and potentially decrease the time and effort needed to complete tasks. This preliminary study therefore suggests a way to naturally integrate human- and machine-based decision making systems.\n    ",
        "submission_date": "2013-09-18T00:00:00",
        "last_modified_date": "2013-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.4962",
        "title": "HOL(y)Hammer: Online ATP Service for HOL Light",
        "authors": [
            "Cezary Kaliszyk",
            "Josef Urban"
        ],
        "abstract": "HOL(y)Hammer is an online AI/ATP service for formal (computer-understandable) mathematics encoded in the HOL Light system. The service allows its users to upload and automatically process an arbitrary formal development (project) based on HOL Light, and to attack arbitrary conjectures that use the concepts defined in some of the uploaded projects. For that, the service uses several automated reasoning systems combined with several premise selection methods trained on all the project proofs. The projects that are readily available on the server for such query answering include the recent versions of the Flyspeck, Multivariate Analysis and Complex Analysis libraries. The service runs on a 48-CPU server, currently employing in parallel for each task 7 AI/ATP combinations and 4 decision procedures that contribute to its overall performance. The system is also available for local installation by interested users, who can customize it for their own proof development. An Emacs interface allowing parallel asynchronous queries to the service is also provided. The overall structure of the service is outlined, problems that arise and their solutions are discussed, and an initial account of using the system is given.\n    ",
        "submission_date": "2013-09-19T00:00:00",
        "last_modified_date": "2013-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.5018",
        "title": "Semantic Advertising",
        "authors": [
            "Ben Zamanzadeh",
            "Naveen Ashish",
            "Cartic Ramakrishnan",
            "John Zimmerman"
        ],
        "abstract": "We present the concept of Semantic Advertising which we see as the future of online advertising. Semantic Advertising is online advertising powered by semantic technology which essentially enables us to represent and reason with concepts and the meaning of things. This paper aims to 1) Define semantic advertising, 2) Place it in the context of broader and more widely used concepts such as the Semantic Web and Semantic Search, 3) Provide a survey of work in related areas such as context matching, and 4) Provide a perspective on successful emerging technologies and areas of future work. We base our work on our experience as a company developing semantic technologies aimed at realizing the full potential of online advertising.\n    ",
        "submission_date": "2013-09-19T00:00:00",
        "last_modified_date": "2013-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.5110",
        "title": "An ant colony optimization algorithm for job shop scheduling problem",
        "authors": [
            "Edson Fl\u00f3rez",
            "Wilfredo G\u00f3mez",
            "Lola Bautista"
        ],
        "abstract": "The nature has inspired several metaheuristics, outstanding among these is Ant Colony Optimization (ACO), which have proved to be very effective and efficient in problems of high complexity (NP-hard) in combinatorial optimization. This paper describes the implementation of an ACO model algorithm known as Elitist Ant System (EAS), applied to a combinatorial optimization problem called Job Shop Scheduling Problem (JSSP). We propose a method that seeks to reduce delays designating the operation immediately available, but considering the operations that lack little to be available and have a greater amount of pheromone. The performance of the algorithm was evaluated for problems of JSSP reference, comparing the quality of the solutions obtained regarding the best known solution of the most effective methods. The solutions were of good quality and obtained with a remarkable efficiency by having to make a very low number of objective function evaluations.\n    ",
        "submission_date": "2013-09-19T00:00:00",
        "last_modified_date": "2013-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.5316",
        "title": "A modeling approach to design a software sensor and analyze agronomical features - Application to sap flow and grape quality relationship",
        "authors": [
            "Aur\u00e9lie Th\u00e9baut",
            "Thibault Scholash",
            "Brigitte Charnomordic",
            "Nadine Hilgert"
        ],
        "abstract": "This work proposes a framework using temporal data and domain knowledge in order to analyze complex agronomical features. The expertise is first formalized in an ontology, under the form of concepts and relationships between them, and then used in conjunction with raw data and mathematical models to design a software sensor. Next the software sensor outputs are put in relation to product quality, assessed by quantitative measurements. This requires the use of advanced data analysis methods, such as functional regression. The methodology is applied to a case study involving an experimental design in French vineyards. The temporal data consist of sap flow measurements, and the goal is to explain fruit quality (sugar concentration and weight), using vine's water courses through the various vine phenological stages. The results are discussed, as well as the method genericity and robustness.\n    ",
        "submission_date": "2013-09-20T00:00:00",
        "last_modified_date": "2013-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.5502",
        "title": "The multi-vehicle covering tour problem: building routes for urban patrolling",
        "authors": [
            "Washington Alves de Oliveira",
            "Antonio Carlos Moretti",
            "Ednei Felix Reis"
        ],
        "abstract": "In this paper we study a particular aspect of the urban community policing: routine patrol route planning. We seek routes that guarantee visibility, as this has a sizable impact on the community perceived safety, allowing quick emergency responses and providing surveillance of selected sites (e.g., hospitals, schools). The planning is restricted to the availability of vehicles and strives to achieve balanced routes. We study an adaptation of the model for the multi-vehicle covering tour problem, in which a set of locations must be visited, whereas another subset must be close enough to the planned routes. It constitutes an NP-complete integer programming problem. Suboptimal solutions are obtained with several heuristics, some adapted from the literature and others developed by us. We solve some adapted instances from TSPLIB and an instance with real data, the former being compared with results from literature, and latter being compared with empirical data.\n    ",
        "submission_date": "2013-09-21T00:00:00",
        "last_modified_date": "2016-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.5655",
        "title": "A new look at reweighted message passing",
        "authors": [
            "Vladimir Kolmogorov"
        ],
        "abstract": "We propose a new family of message passing techniques for MAP estimation in graphical models which we call {\\em Sequential Reweighted Message Passing} (SRMP). Special cases include well-known techniques such as {\\em Min-Sum Diffusion} (MSD) and a faster {\\em Sequential Tree-Reweighted Message Passing} (TRW-S). Importantly, our derivation is simpler than the original derivation of TRW-S, and does not involve a decomposition into trees. This allows easy generalizations. We present such a generalization for the case of higher-order graphical models, and test it on several real-world problems with promising results.\n    ",
        "submission_date": "2013-09-22T00:00:00",
        "last_modified_date": "2017-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.5984",
        "title": "An evolutionary approach to Function",
        "authors": [
            "Phillip Lord"
        ],
        "abstract": "Background: Understanding the distinction between function and role is vexing and difficult. While it appears to be useful, in practice this distinction is hard to apply, particularly within biology.\n",
        "submission_date": "2013-09-23T00:00:00",
        "last_modified_date": "2013-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6226",
        "title": "Automation of Mathematical Induction as part of the History of Logic",
        "authors": [
            "J Strother Moore",
            "Claus-Peter Wirth"
        ],
        "abstract": "We review the history of the automation of mathematical induction\n    ",
        "submission_date": "2013-09-24T00:00:00",
        "last_modified_date": "2014-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6297",
        "title": "Generating Explanations for Biomedical Queries",
        "authors": [
            "Esra Erdem",
            "Umut Oztok"
        ],
        "abstract": "We introduce novel mathematical models and algorithms to generate (shortest or k different) explanations for biomedical queries, using answer set programming. We implement these algorithms and integrate them in BIOQUERY-ASP. We illustrate the usefulness of these methods with some complex biomedical queries related to drug discovery, over the biomedical knowledge resources PHARMGKB, DRUGBANK, BIOGRID, CTD, SIDER, DISEASE ONTOLOGY and ORPHADATA. To appear in Theory and Practice of Logic Programming (TPLP).\n    ",
        "submission_date": "2013-09-24T00:00:00",
        "last_modified_date": "2013-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6433",
        "title": "A Fuzzy expert system for goalkeeper quality recognition",
        "authors": [
            "Mohammad Bazmara",
            "Shahram Jafari",
            "Fatemeh Pasand"
        ],
        "abstract": "Goalkeeper (GK) is an expert in soccer and goalkeeping is a complete professional job. In fact, achieving success seems impossible without a reliable GK. His effect in successes and failures is more dominant than other players. The most visible mistakes in a game are those of goalkeeper's. In this paper the expert fuzzy system is used as a suitable tool to study the quality of a goalkeeper and compare it with others. Previously done researches are used to find the goalkeepers' indexes in soccer. Soccer experts have found that a successful GK should have some qualifications. A new pattern is offered here which is called \"Soccer goalkeeper quality recognition using fuzzy expert systems\". This pattern has some important capabilities. Firstly, among some goalkeepers the one with the best quality for the main team arrange can be chosen. Secondly, the need to expert coaches for choosing a GK using their senses and experiences decreases a lot. Thirdly, in the survey of a GK, quantitative criteria can be included, and finally this pattern is simple and easy to understand.\n    ",
        "submission_date": "2013-09-25T00:00:00",
        "last_modified_date": "2013-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6816",
        "title": "Reasoning about Probabilities in Dynamic Systems using Goal Regression",
        "authors": [
            "Vaishak Belle",
            "Hector Levesque"
        ],
        "abstract": "Reasoning about degrees of belief in uncertain dynamic worlds is fundamental to many applications, such as robotics and planning, where actions modify state properties and sensors provide measurements, both of which are prone to noise. With the exception of limited cases such as Gaussian processes over linear phenomena, belief state evolution can be complex and hard to reason with in a general way. This paper proposes a framework with new results that allows the reduction of subjective probabilities after sensing and acting to questions about the initial state only. We build on an expressive probabilistic first-order logical account by Bacchus, Halpern and Levesque, resulting in a methodology that, in principle, can be coupled with a variety of existing inference solutions.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6817",
        "title": "Probabilistic Conditional Preference Networks",
        "authors": [
            "Damien Bigot",
            "Bruno Zanuttini",
            "Helene Fargier",
            "Jerome Mengin"
        ],
        "abstract": "In order to represent the preferences of a group of individuals, we introduce Probabilistic CP-nets (PCP-nets). PCP-nets provide a compact language for representing probability distributions over preference orderings. We argue that they are useful for aggregating preferences or modelling noisy preferences. Then we give efficient algorithms for the main reasoning problems, namely for computing the probability that a given outcome is preferred to another one, and the probability that a given outcome is optimal. As a by-product, we obtain an unexpected linear-time algorithm for checking dominance in a standard, tree-structured CP-net.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6822",
        "title": "Automorphism Groups of Graphical Models and Lifted Variational Inference",
        "authors": [
            "Hung Bui",
            "Tuyen Huynh",
            "Sebastian Riedel"
        ],
        "abstract": "Using the theory of group action, we first introduce the concept of the automorphism group of an exponential family or a graphical model, thus formalizing the general notion of symmetry of a probabilistic model. This automorphism group provides a precise mathematical framework for lifted inference in the general exponential family. Its group action partitions the set of random variables and feature functions into equivalent classes (called orbits) having identical marginals and expectations. Then the inference problem is effectively reduced to that of computing marginals or expectations for each class, thus avoiding the need to deal with each individual variable or feature. We demonstrate the usefulness of this general framework in lifting two classes of variational approximation for maximum a posteriori (MAP) inference: local linear programming (LP) relaxation and local LP relaxation with cycle constraints; the latter yields the first lifted variational inference algorithm that operates on a bound tighter than the local constraints.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6824",
        "title": "Learning Sparse Causal Models is not NP-hard",
        "authors": [
            "Tom Claassen",
            "Joris Mooij",
            "Tom Heskes"
        ],
        "abstract": "This paper shows that causal model discovery is not an NP-hard problem, in the sense that for sparse graphs bounded by node degree k the sound and complete causal model can be obtained in worst case order N^{2(k+2)} independence tests, even when latent variables and selection bias may be present. We present a modification of the well-known FCI algorithm that implements the method for an independence oracle, and suggest improvements for sample/real-world data versions. It does not contradict any known hardness results, and does not solve an NP-hard problem: it just proves that sparse causal discovery is perhaps more complicated, but not as hard as learning minimal Bayesian networks.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6825",
        "title": "Advances in Bayesian Network Learning using Integer Programming",
        "authors": [
            "Mark Bartlett",
            "James Cussens"
        ],
        "abstract": "We consider the problem of learning Bayesian networks (BNs) from complete discrete data. This problem of discrete optimisation is formulated as an integer program (IP). We describe the various steps we have taken to allow efficient solving of this IP. These are (i) efficient search for cutting planes, (ii) a fast greedy algorithm to find high-scoring (perhaps not optimal) BNs and (iii) tightening the linear relaxation of the IP. After relating this BN learning problem to set covering and the multidimensional 0-1 knapsack problem, we present our empirical results. These show improvements, sometimes dramatic, over earlier results.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2015-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6826",
        "title": "Qualitative Possibilistic Mixed-Observable MDPs",
        "authors": [
            "Nicolas Drougard",
            "Florent Teichteil-Konigsbuch",
            "Jean-Loup Farges",
            "Didier Dubois"
        ],
        "abstract": "Possibilistic and qualitative POMDPs (pi-POMDPs) are counterparts of POMDPs used to model situations where the agent's initial belief or observation probabilities are imprecise due to lack of past experiences or insufficient data collection. However, like probabilistic POMDPs, optimally solving pi-POMDPs is intractable: the finite belief state space exponentially grows with the number of system's states. In this paper, a possibilistic version of Mixed-Observable MDPs is presented to get around this issue: the complexity of solving pi-POMDPs, some state variables of which are fully observable, can be then dramatically reduced. A value iteration algorithm for this new formulation under infinite horizon is next proposed and the optimality of the returned policy (for a specified criterion) is shown assuming the existence of a \"stay\" action in some goal states. Experimental work finally shows that this possibilistic model outperforms probabilistic POMDPs commonly used in robotics, for a target recognition problem where the agent's observations are imprecise.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6827",
        "title": "Optimization With Parity Constraints: From Binary Codes to Discrete Integration",
        "authors": [
            "Stefano Ermon",
            "Carla P. Gomes",
            "Ashish Sabharwal",
            "Bart Selman"
        ],
        "abstract": "Many probabilistic inference tasks involve summations over exponentially large sets. Recently, it has been shown that these problems can be reduced to solving a polynomial number of MAP inference queries for a model augmented with randomly generated parity constraints. By exploiting a connection with max-likelihood decoding of binary codes, we show that these optimizations are computationally hard. Inspired by iterative message passing decoding algorithms, we propose an Integer Linear Programming (ILP) formulation for the problem, enhanced with new sparsification techniques to improve decoding performance. By solving the ILP through a sequence of LP relaxations, we get both lower and upper bounds on the partition function, which hold with high probability and are much tighter than those obtained with variational methods.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6828",
        "title": "Monte-Carlo Planning: Theoretically Fast Convergence Meets Practical Efficiency",
        "authors": [
            "Zohar Feldman",
            "Carmel Domshlak"
        ],
        "abstract": "Popular Monte-Carlo tree search (MCTS) algorithms for online planning, such as epsilon-greedy tree search and UCT, aim at rapidly identifying a reasonably good action, but provide rather poor worst-case guarantees on performance improvement over time. In contrast, a recently introduced MCTS algorithm BRUE guarantees exponential-rate improvement over time, yet it is not geared towards identifying reasonably good choices right at the go. We take a stand on the individual strengths of these two classes of algorithms, and show how they can be effectively connected. We then rationalize a principle of \"selective tree expansion\", and suggest a concrete implementation of this principle within MCTS. The resulting algorithm,s favorably compete with other MCTS algorithms under short planning times, while preserving the attractive convergence properties of BRUE.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6829",
        "title": "Bethe-ADMM for Tree Decomposition based Parallel MAP Inference",
        "authors": [
            "Qiang Fu",
            "Huahua Wang",
            "Arindam Banerjee"
        ],
        "abstract": "We consider the problem of maximum a posteriori (MAP) inference in discrete graphical models. We present a parallel MAP inference algorithm called Bethe-ADMM based on two ideas: tree-decomposition of the graph and the alternating direction method of multipliers (ADMM). However, unlike the standard ADMM, we use an inexact ADMM augmented with a Bethe-divergence based proximal function, which makes each subproblem in ADMM easy to solve in parallel using the sum-product algorithm. We rigorously prove global convergence of Bethe-ADMM. The proposed algorithm is extensively evaluated on both synthetic and real datasets to illustrate its effectiveness. Further, the parallel Bethe-ADMM is shown to scale almost linearly with increasing number of cores.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6832",
        "title": "Structured Message Passing",
        "authors": [
            "Vibhav Gogate",
            "Pedro Domingos"
        ],
        "abstract": "In this paper, we present structured message passing (SMP), a unifying framework for approximate inference algorithms that take advantage of structured representations such as algebraic decision diagrams and sparse hash tables. These representations can yield significant time and space savings over the conventional tabular representation when the message has several identical values (context-specific independence) or zeros (determinism) or both in its range. Therefore, in order to fully exploit the power of structured representations, we propose to artificially introduce context-specific independence and determinism in the messages. This yields a new class of powerful approximate inference algorithms which includes popular algorithms such as cluster-graph Belief propagation (BP), expectation propagation and particle BP as special cases. We show that our new algorithms introduce several interesting bias-variance trade-offs. We evaluate these trade-offs empirically and demonstrate that our new algorithms are more accurate and scalable than state-of-the-art techniques.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6836",
        "title": "Discovering Cyclic Causal Models with Latent Variables: A General SAT-Based Procedure",
        "authors": [
            "Antti Hyttinen",
            "Patrik O. Hoyer",
            "Frederick Eberhardt",
            "Matti Jarvisalo"
        ],
        "abstract": "We present a very general approach to learning the structure of causal models based on d-separation constraints, obtained from any given set of overlapping passive observational or experimental data sets. The procedure allows for both directed cycles (feedback loops) and the presence of latent variables. Our approach is based on a logical representation of causal pathways, which permits the integration of quite general background knowledge, and inference is performed using a Boolean satisfiability (SAT) solver. The procedure is complete in that it exhausts the available information on whether any given edge can be determined to be present or absent, and returns \"unknown\" otherwise. Many existing constraint-based causal discovery algorithms can be seen as special cases, tailored to circumstances in which one or more restricting assumptions apply. Simulations illustrate the effect of these assumptions on discovery and how the present algorithm scales.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6839",
        "title": "Solving Limited-Memory Influence Diagrams Using Branch-and-Bound Search",
        "authors": [
            "Arindam Khaled",
            "Eric A. Hansen",
            "Changhe Yuan"
        ],
        "abstract": "A limited-memory influence diagram (LIMID) generalizes a traditional influence diagram by relaxing the assumptions of regularity and no-forgetting, allowing a wider range of decision problems to be modeled. Algorithms for solving traditional influence diagrams are not easily generalized to solve LIMIDs, however, and only recently have exact algorithms for solving LIMIDs been developed. In this paper, we introduce an exact algorithm for solving LIMIDs that is based on branch-and-bound search. Our approach is related to the approach of solving an influence diagram by converting it to an equivalent decision tree, with the difference that the LIMID is converted to a much smaller decision graph that can be searched more efficiently.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6842",
        "title": "Causal Transportability of Experiments on Controllable Subsets of Variables: z-Transportability",
        "authors": [
            "Sanghack Lee",
            "Vasant Honavar"
        ],
        "abstract": "We introduce z-transportability, the problem of estimating the causal effect of a set of variables X on another set of variables Y in a target domain from experiments on any subset of controllable variables Z where Z is an arbitrary subset of observable variables V in a source domain. z-Transportability generalizes z-identifiability, the problem of estimating in a given domain the causal effect of X on Y from surrogate experiments on a set of variables Z such that Z is disjoint from X;. z-Transportability also generalizes transportability which requires that the causal effect of X on Y in the target domain be estimable from experiments on any subset of all observable variables in the source domain. We first generalize z-identifiability to allow cases where Z is not necessarily disjoint from X. Then, we establish a necessary and sufficient condition for z-transportability in terms of generalized z-identifiability and transportability. We provide a correct and complete algorithm that determines whether a causal effect is z-transportable; and if it is, produces a transport formula, that is, a recipe for estimating the causal effect of X on Y in the target domain using information elicited from the results of experimental manipulations of Z in the source domain and observational data from the target domain. Our results also show that do-calculus is complete for z-transportability.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6843",
        "title": "A Sound and Complete Algorithm for Learning Causal Models from Relational Data",
        "authors": [
            "Marc Maier",
            "Katerina Marazopoulou",
            "David Arbour",
            "David Jensen"
        ],
        "abstract": "The PC algorithm learns maximally oriented causal Bayesian networks. However, there is no equivalent complete algorithm for learning the structure of relational models, a more expressive generalization of Bayesian networks. Recent developments in the theory and representation of relational models support lifted reasoning about conditional independence. This enables a powerful constraint for orienting bivariate dependencies and forms the basis of a new algorithm for learning structure. We present the relational causal discovery (RCD) algorithm that learns causal relational models. We prove that RCD is sound and complete, and we present empirical results that demonstrate effectiveness.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6844",
        "title": "Evaluating Anytime Algorithms for Learning Optimal Bayesian Networks",
        "authors": [
            "Brandon Malone",
            "Changhe Yuan"
        ],
        "abstract": "Exact algorithms for learning Bayesian networks guarantee to find provably optimal networks. However, they may fail in difficult learning tasks due to limited time or memory. In this research we adapt several anytime heuristic search-based algorithms to learn Bayesian networks. These algorithms find high-quality solutions quickly, and continually improve the incumbent solution or prove its optimality before resources are exhausted. Empirical results show that the anytime window A* algorithm usually finds higher-quality, often optimal, networks more quickly than other approaches. The results also show that, surprisingly, while generating networks with few parents per variable are structurally simpler, they are harder to learn than complex generating networks with more parents per variable.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6845",
        "title": "On the Complexity of Strong and Epistemic Credal Networks",
        "authors": [
            "Denis D. Maua",
            "Cassio Polpo de Campos",
            "Alessio Benavoli",
            "Alessandro Antonucci"
        ],
        "abstract": "Credal networks are graph-based statistical models whose parameters take values in a set, instead of being sharply specified as in traditional statistical models (e.g., Bayesian networks). The computational complexity of inferences on such models depends on the irrelevance/independence concept adopted. In this paper, we study inferential complexity under the concepts of epistemic irrelevance and strong independence. We show that inferences under strong independence are NP-hard even in trees with ternary variables. We prove that under epistemic irrelevance the polynomial time complexity of inferences in credal trees is not likely to extend to more general models (e.g. singly connected networks). These results clearly distinguish networks that admit efficient inferences and those where inferences are most likely hard, and settle several open questions regarding computational complexity.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6846",
        "title": "Learning Periodic Human Behaviour Models from Sparse Data for Crowdsourcing Aid Delivery in Developing Countries",
        "authors": [
            "James McInerney",
            "Alex Rogers",
            "Nicholas R. Jennings"
        ],
        "abstract": "In many developing countries, half the population lives in rural locations, where access to essentials such as school materials, mosquito nets, and medical supplies is restricted. We propose an alternative method of distribution (to standard road delivery) in which the existing mobility habits of a local population are leveraged to deliver aid, which raises two technical challenges in the areas optimisation and learning. For optimisation, a standard Markov decision process applied to this problem is intractable, so we provide an exact formulation that takes advantage of the periodicities in human location behaviour. To learn such behaviour models from sparse data (i.e., cell tower observations), we develop a Bayesian model of human mobility. Using real cell tower data of the mobility behaviour of 50,000 individuals in Ivory Coast, we find that our model outperforms the state of the art approaches in mobility prediction by at least 25% (in held-out data likelihood). Furthermore, when incorporating mobility prediction with our MDP approach, we find a 81.3% reduction in total delivery time versus routine planning that minimises just the number of participants in the solution path.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6848",
        "title": "Tighter Linear Program Relaxations for High Order Graphical Models",
        "authors": [
            "Elad Mezuman",
            "Daniel Tarlow",
            "Amir Globerson",
            "Yair Weiss"
        ],
        "abstract": "Graphical models with High Order Potentials (HOPs) have received considerable interest in recent years. While there are a variety of approaches to inference in these models, nearly all of them amount to solving a linear program (LP) relaxation with unary consistency constraints between the HOP and the individual variables. In many cases, the resulting relaxations are loose, and in these cases the results of inference can be poor. It is thus desirable to look for more accurate ways of performing inference in these models. In this work, we study the LP relaxations that result from enforcing additional consistency constraints between the HOP and the rest of the model. We address theoretical questions about the strength of the resulting relaxations compared to the relaxations that arise in standard approaches, and we develop practical and efficient message passing algorithms for optimizing the LPs. Empirically, we show that the LPs with additional consistency constraints lead to more accurate inference on some challenging problems that include a combination of low order and high order terms.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6855",
        "title": "Evaluating computational models of explanation using human judgments",
        "authors": [
            "Michael Pacer",
            "Joseph Williams",
            "Xi Chen",
            "Tania Lombrozo",
            "Thomas Griffiths"
        ],
        "abstract": "We evaluate four computational models of explanation in Bayesian networks by comparing model predictions to human judgments. In two experiments, we present human participants with causal structures for which the models make divergent predictions and either solicit the best explanation for an observed event (Experiment 1) or have participants rate provided explanations for an observed event (Experiment 2). Across two versions of two causal structures and across both experiments, we find that the Causal Explanation Tree and Most Relevant Explanation models provide better fits to human data than either Most Probable Explanation or Explanation Tree models. We identify strengths and shortcomings of these models and what they can reveal about human explanation. We conclude by suggesting the value of pursuing computational and psychological investigations of explanation in parallel.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6856",
        "title": "Approximation of Lorenz-Optimal Solutions in Multiobjective Markov Decision Processes",
        "authors": [
            "Patrice Perny",
            "Paul Weng",
            "Judy Goldsmith",
            "Josiah Hanna"
        ],
        "abstract": "This paper is devoted to fair optimization in Multiobjective Markov Decision Processes (MOMDPs). A MOMDP is an extension of the MDP model for planning under uncertainty while trying to optimize several reward functions simultaneously. This applies to multiagent problems when rewards define individual utility functions, or in multicriteria problems when rewards refer to different features. In this setting, we study the determination of policies leading to Lorenz-non-dominated tradeoffs. Lorenz dominance is a refinement of Pareto dominance that was introduced in Social Choice for the measurement of inequalities. In this paper, we introduce methods to efficiently approximate the sets of Lorenz-non-dominated solutions of infinite-horizon, discounted MOMDPs. The approximations are polynomial-sized subsets of those solutions.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6857",
        "title": "Solution Methods for Constrained Markov Decision Process with Continuous Probability Modulation",
        "authors": [
            "Marek Petrik",
            "Dharmashankar Subramanian",
            "Janusz Marecki"
        ],
        "abstract": "We propose solution methods for previously-unsolved constrained MDPs in which actions can continuously modify the transition probabilities within some acceptable sets. While many methods have been proposed to solve regular MDPs with large state sets, there are few practical approaches for solving constrained MDPs with large action sets. In particular, we show that the continuous action sets can be replaced by their extreme points when the rewards are linear in the modulation. We also develop a tractable optimization formulation for concave reward functions and, surprisingly, also extend it to non- concave reward functions by using their concave envelopes. We evaluate the effectiveness of the approach on the problem of managing delinquencies in a portfolio of loans.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6864",
        "title": "Preference Elicitation For General Random Utility Models",
        "authors": [
            "Hossein Azari Soufiani",
            "David C. Parkes",
            "Lirong Xia"
        ],
        "abstract": "This paper discusses {General Random Utility Models (GRUMs)}. These are a class of parametric models that generate partial ranks over alternatives given attributes of agents and alternatives. We propose two preference elicitation scheme for GRUMs developed from principles in Bayesian experimental design, one for social choice and the other for personalized choice. We couple this with a general Monte-Carlo-Expectation-Maximization (MC-EM) based algorithm for MAP inference under GRUMs. We also prove uni-modality of the likelihood functions for a class of GRUMs. We examine the performance of various criteria by experimental studies, which show that the proposed elicitation scheme increases the precision of estimation.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6870",
        "title": "Dynamic Blocking and Collapsing for Gibbs Sampling",
        "authors": [
            "Deepak Venugopal",
            "Vibhav Gogate"
        ],
        "abstract": "In this paper, we investigate combining blocking and collapsing -- two widely used strategies for improving the accuracy of Gibbs sampling -- in the context of probabilistic graphical models (PGMs). We show that combining them is not straight-forward because collapsing (or eliminating variables) introduces new dependencies in the PGM and in computation-limited settings, this may adversely affect blocking. We therefore propose a principled approach for tackling this problem. Specifically, we develop two scoring functions, one each for blocking and collapsing, and formulate the problem of partitioning the variables in the PGM into blocked and collapsed subsets as simultaneously maximizing both scoring functions (i.e., a multi-objective optimization problem). We propose a dynamic, greedy algorithm for approximately solving this intractable optimization problem. Our dynamic algorithm periodically updates the partitioning into blocked and collapsed variables by leveraging correlation statistics gathered from the generated samples and enables rapid mixing by blocking together and collapsing highly correlated variables. We demonstrate experimentally the clear benefit of our dynamic approach: as more samples are drawn, our dynamic approach significantly outperforms static graph-based approaches by an order of magnitude in terms of accuracy.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6871",
        "title": "Bounded Approximate Symbolic Dynamic Programming for Hybrid MDPs",
        "authors": [
            "Luis Gustavo Vianna",
            "Scott Sanner",
            "Leliane Nunes de Barros"
        ],
        "abstract": "Recent advances in symbolic dynamic programming (SDP) combined with the extended algebraic decision diagram (XADD) data structure have provided exact solutions for mixed discrete and continuous (hybrid) MDPs with piecewise linear dynamics and continuous actions. Since XADD-based exact solutions may grow intractably large for many problems, we propose a bounded error compression technique for XADDs that involves the solution of a constrained bilinear saddle point problem. Fortuitously, we show that given the special structure of this problem, it can be expressed as a bilevel linear programming problem and solved to optimality in finite time via constraint generation, despite having an infinite set of constraints. This solution permits the use of efficient linear program solvers for XADD compression and enables a novel class of bounded approximate SDP algorithms for hybrid MDPs that empirically offers order-of-magnitude speedups over the exact solution in exchange for a small approximation error.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6872",
        "title": "On MAP Inference by MWSS on Perfect Graphs",
        "authors": [
            "Adrian Weller",
            "Tony S. Jebara"
        ],
        "abstract": "Finding the most likely (MAP) configuration of a Markov random field (MRF) is NP-hard in general. A promising, recent technique is to reduce the problem to finding a maximum weight stable set (MWSS) on a derived weighted graph, which if perfect, allows inference in polynomial time. We derive new results for this approach, including a general decomposition theorem for MRFs of any order and number of labels, extensions of results for binary pairwise models with submodular cost functions to higher order, and an exact characterization of which binary pairwise MRFs can be efficiently solved with this method. This defines the power of the approach on this class of models, improves our toolbox and expands the range of tractable models.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6989",
        "title": "Linear combination of one-step predictive information with an external reward in an episodic policy gradient setting: a critical analysis",
        "authors": [
            "Keyan Zahedi",
            "Georg Martius",
            "Nihat Ay"
        ],
        "abstract": "One of the main challenges in the field of embodied artificial intelligence is the open-ended autonomous learning of complex behaviours. Our approach is to use task-independent, information-driven intrinsic motivation(s) to support task-dependent learning. The work presented here is a preliminary step in which we investigate the predictive information (the mutual information of the past and future of the sensor stream) as an intrinsic drive, ideally supporting any kind of task acquisition. Previous experiments have shown that the predictive information (PI) is a good candidate to support autonomous, open-ended learning of complex behaviours, because a maximisation of the PI corresponds to an exploration of morphology- and environment-dependent behavioural regularities. The idea is that these regularities can then be exploited in order to solve any given task. Three different experiments are presented and their results lead to the conclusion that the linear combination of the one-step PI with an external reward function is not generally recommended in an episodic policy gradient setting. Only for hard tasks a great speed-up can be achieved at the cost of an asymptotic performance lost.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.7004",
        "title": "Calculation of Entailed Rank Constraints in Partially Non-Linear and Cyclic Models",
        "authors": [
            "Peter L. Spirtes"
        ],
        "abstract": "The Trek Separation Theorem (Sullivant et al. 2010) states necessary and sufficient conditions for a linear directed acyclic graphical model to entail for all possible values of its linear coefficients that the rank of various sub-matrices of the covariance matrix is less than or equal to n, for any given n. In this paper, I extend the Trek Separation Theorem in two ways: I prove that the same necessary and sufficient conditions apply even when the generating model is partially non-linear and contains some cycles. This justifies application of constraint-based causal search algorithms such as the BuildPureClusters algorithm (Silva et al. 2006) for discovering the causal structure of latent variable models to data generated by a wider class of causal models that may contain non-linear and cyclic relations among the latent variables.\n    ",
        "submission_date": "2013-09-17T00:00:00",
        "last_modified_date": "2013-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.7068",
        "title": "Investigation of commuting Hamiltonian in quantum Markov network",
        "authors": [
            "Farzad Ghafari Jouneghani",
            "Mohammad Babazadeh",
            "Rogayeh Bayramzadeh",
            "Hossein Movla"
        ],
        "abstract": "Graphical Models have various applications in science and engineering which include physics, bioinformatics, telecommunication and etc. Usage of graphical models needs complex computations in order to evaluation of marginal functions,so there are some powerful methods including mean field approximation, belief propagation algorithm and etc. Quantum graphical models have been recently developed in context of quantum information and computation, and quantum statistical physics, which is possible by generalization of classical probability theory to quantum theory. The main goal of this paper is preparing a primary generalization of Markov network, as a type of graphical models, to quantum case and applying in quantum statistical ",
        "submission_date": "2013-09-25T00:00:00",
        "last_modified_date": "2014-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.7145",
        "title": "Propagating Regular Counting Constraints",
        "authors": [
            "Nicolas Beldiceanu",
            "Pierre Flener",
            "Justin Pearson",
            "Pascal Van Hentenryck"
        ],
        "abstract": "Constraints over finite sequences of variables are ubiquitous in sequencing and timetabling. Moreover, the wide variety of such constraints in practical applications led to general modelling techniques and generic propagation algorithms, often based on deterministic finite automata (DFA) and their extensions. We consider counter-DFAs (cDFA), which provide concise models for regular counting constraints, that is constraints over the number of times a regular-language pattern occurs in a sequence. We show how to enforce domain consistency in polynomial time for atmost and atleast regular counting constraints based on the frequent case of a cDFA with only accepting states and a single counter that can be incremented by transitions. We also prove that the satisfaction of exact regular counting constraints is NP-hard and indicate that an incomplete algorithm for exact regular counting constraints is faster and provides more pruning than the existing propagator from [3]. Regular counting constraints are closely related to the CostRegular constraint but contribute both a natural abstraction and some computational advantages.\n    ",
        "submission_date": "2013-09-27T00:00:00",
        "last_modified_date": "2013-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.7173",
        "title": "Analysis of Optimization Techniques to Improve User Response Time of Web Applications and Their Implementation for MOODLE",
        "authors": [
            "Priyanka Manchanda"
        ],
        "abstract": "Analysis of seven optimization techniques grouped under three categories (hardware, back-end, and front-end) is done to study the reduction in average user response time for Modular Object Oriented Dynamic Learning Environment (Moodle), a Learning Management System which is scripted in PHP5, runs on Apache web server and utilizes MySQL database software. Before the implementation of these techniques, performance analysis of Moodle is performed for varying number of concurrent users. The results obtained for each optimization technique are then reported in a tabular format. The maximum reduction in end user response time was achieved for hardware optimization which requires Moodle server and database to be installed on solid state disk.\n    ",
        "submission_date": "2013-09-27T00:00:00",
        "last_modified_date": "2013-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.7971",
        "title": "Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence (2013)",
        "authors": [
            "Ann Nicholson",
            "Padhriac Smyth"
        ],
        "abstract": "This is the Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, which was held in Bellevue, WA, August 11-15, 2013\n    ",
        "submission_date": "2013-09-30T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.0602",
        "title": "Iterated Variable Neighborhood Search for the resource constrained multi-mode multi-project scheduling problem",
        "authors": [
            "Martin Josef Geiger"
        ],
        "abstract": "The resource constrained multi-mode multi-project scheduling problem (RCMMMPSP) is a notoriously difficult combinatorial optimization problem. For a given set of activities, feasible execution mode assignments and execution starting times must be found such that some optimization function, e.g. the makespan, is optimized. When determining an optimal (or at least feasible) assignment of decision variable values, a set of side constraints, such as resource availabilities, precedence constraints, etc., has to be respected.\n",
        "submission_date": "2013-10-02T00:00:00",
        "last_modified_date": "2013-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.0927",
        "title": "Learning Chordal Markov Networks by Constraint Satisfaction",
        "authors": [
            "Jukka Corander",
            "Tomi Janhunen",
            "Jussi Rintanen",
            "Henrik Nyman",
            "Johan Pensar"
        ],
        "abstract": "We investigate the problem of learning the structure of a Markov network from data. It is shown that the structure of such networks can be described in terms of constraints which enables the use of existing solver technology with optimization capabilities to compute optimal networks starting from initial scores computed from the data. To achieve efficient encodings, we develop a novel characterization of Markov network structure using a balancing condition on the separators between cliques forming the network. The resulting translations into propositional satisfiability and its extensions such as maximum satisfiability, satisfiability modulo theories, and answer set programming, enable us to prove optimal certain network structures which have been previously found by stochastic search.\n    ",
        "submission_date": "2013-10-03T00:00:00",
        "last_modified_date": "2013-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.1328",
        "title": "The Relevance of Proofs of the Rationality of Probability Theory to Automated Reasoning and Cognitive Models",
        "authors": [
            "Ernest Davis"
        ],
        "abstract": "A number of well-known theorems, such as Cox's theorem and de Finetti's theorem. prove that any model of reasoning with uncertain information that satisfies specified conditions of \"rationality\" must satisfy the axioms of probability theory. I argue here that these theorems do not in themselves demonstrate that probabilistic models are in fact suitable for any specific task in automated reasoning or plausible for cognitive models. First, the theorems only establish that there exists some probabilistic model; they do not establish that there exists a useful probabilistic model, i.e. one with a tractably small number of numerical parameters and a large number of independence assumptions. Second, there are in general many different probabilistic models for a given situation, many of which may be far more irrational, in the usual sense of the term, than a model that violates the axioms of probability theory. I illustrate this second point with an extended examples of two tasks of induction, of a similar structure, where the reasonable probabilistic models are very different.\n    ",
        "submission_date": "2013-10-04T00:00:00",
        "last_modified_date": "2013-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.1863",
        "title": "Empowerment -- an Introduction",
        "authors": [
            "Christoph Salge",
            "Cornelius Glackin",
            "Daniel Polani"
        ],
        "abstract": "This book chapter is an introduction to and an overview of the information-theoretic, task independent utility function \"Empowerment\", which is defined as the channel capacity between an agent's actions and an agent's sensors. It quantifies how much influence and control an agent has over the world it can perceive. This book chapter discusses the general idea behind empowerment as an intrinsic motivation and showcases several previous applications of empowerment to demonstrate how empowerment can be applied to different sensor-motor configuration, and how the same formalism can lead to different observed behaviors. Furthermore, we also present a fast approximation for empowerment in the continuous domain.\n    ",
        "submission_date": "2013-10-07T00:00:00",
        "last_modified_date": "2013-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.1947",
        "title": "Bayesian Optimization With Censored Response Data",
        "authors": [
            "Frank Hutter",
            "Holger Hoos",
            "Kevin Leyton-Brown"
        ],
        "abstract": "Bayesian optimization (BO) aims to minimize a given blackbox function using a model that is updated whenever new evidence about the function becomes available. Here, we address the problem of BO under partially right-censored response data, where in some evaluations we only obtain a lower bound on the function value. The ability to handle such response data allows us to adaptively censor costly function evaluations in minimization problems where the cost of a function evaluation corresponds to the function value. One important application giving rise to such censored data is the runtime-minimizing variant of the algorithm configuration problem: finding settings of a given parametric algorithm that minimize the runtime required for solving problem instances from a given distribution. We demonstrate that terminating slow algorithm runs prematurely and handling the resulting right-censored observations can substantially improve the state of the art in model-based algorithm configuration.\n    ",
        "submission_date": "2013-10-07T00:00:00",
        "last_modified_date": "2013-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.2089",
        "title": "Double four-bar crank-slider mechanism dynamic balancing by meta-heuristic algorithms",
        "authors": [
            "Habib Emdadi",
            "Mahsa Yazdanian",
            "Mir Mohammad Ettefagh",
            "Mohammad-Reza Feizi-Derakhshi"
        ],
        "abstract": "In this paper, a new method for dynamic balancing of double four-bar crank slider mechanism by meta- heuristic-based optimization algorithms is proposed. For this purpose, a proper objective function which is necessary for balancing of this mechanism and corresponding constraints has been obtained by dynamic modeling of the mechanism. Then PSO, ABC, BGA and HGAPSO algorithms have been applied for minimizing the defined cost function in optimization step. The optimization results have been studied completely by extracting the cost function, fitness, convergence speed and runtime values of applied algorithms. It has been shown that PSO and ABC are more efficient than BGA and HGAPSO in terms of convergence speed and result quality. Also, a laboratory scale experimental doublefour-bar crank-slider mechanism was provided for validating the proposed balancing method practically.\n    ",
        "submission_date": "2013-10-08T00:00:00",
        "last_modified_date": "2013-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.2298",
        "title": "SAT-based Preprocessing for MaxSAT (extended version)",
        "authors": [
            "Anton Belov",
            "Antonio Morgado",
            "Joao Marques-Silva"
        ],
        "abstract": "State-of-the-art algorithms for industrial instances of MaxSAT problem rely on iterative calls to a SAT solver. Preprocessing is crucial for the acceleration of SAT solving, and the key preprocessing techniques rely on the application of resolution and subsumption elimination. Additionally, satisfiability-preserving clause elimination procedures are often used. Since MaxSAT computation typically involves a large number of SAT calls, we are interested in whether an input instance to a MaxSAT problem can be preprocessed up-front, i.e. prior to running the MaxSAT solver, rather than (or, in addition to) during each iterative SAT solver call. The key requirement in this setting is that the preprocessing has to be sound, i.e. so that the solution can be reconstructed correctly and efficiently after the execution of a MaxSAT algorithm on the preprocessed instance. While, as we demonstrate in this paper, certain clause elimination procedures are sound for MaxSAT, it is well-known that this is not the case for resolution and subsumption elimination. In this paper we show how to adapt these preprocessing techniques to MaxSAT. To achieve this we recast the MaxSAT problem in a recently introduced labelled-CNF framework, and show that within the framework the preprocessing techniques can be applied soundly. Furthermore, we show that MaxSAT algorithms restated in the framework have a natural implementation on top of an incremental SAT solver. We evaluate the prototype implementation of a MaxSAT algorithm WMSU1 in this setting, demonstrate the effectiveness of preprocessing, and show overall improvement with respect to non-incremental versions of the algorithm on some classes of problems.\n    ",
        "submission_date": "2013-10-08T00:00:00",
        "last_modified_date": "2013-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.2350",
        "title": "The Generalized Traveling Salesman Problem solved with Ant Algorithms",
        "authors": [
            "Camelia-M. Pintea",
            "Petrica C. Pop",
            "Camelia Chira"
        ],
        "abstract": "A well known N P-hard problem called the Generalized Traveling Salesman Problem (GTSP) is considered. In GTSP the nodes of a complete undirected graph are partitioned into clusters. The objective is to find a minimum cost tour passing through exactly one node from each cluster. An exact exponential time algorithm and an effective meta-heuristic algorithm for the problem are presented. The meta-heuristic proposed is a modified Ant Colony System (ACS) algorithm called Reinforcing Ant Colony System (RACS) which introduces new correction rules in the ACS algorithm. Computational results are reported for many standard test problems. The proposed algorithm is competitive with the other already proposed heuristics for the GTSP in both solution quality and computational time.\n    ",
        "submission_date": "2013-10-09T00:00:00",
        "last_modified_date": "2013-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.2396",
        "title": "A necessary and sufficient condition for two relations to induce the same definable set family",
        "authors": [
            "Hua Yao",
            "William Zhu"
        ],
        "abstract": "In Pawlak rough sets, the structure of the definable set families is simple and clear, but in generalizing rough sets, the structure of the definable set families is a bit more complex. There has been much research work focusing on this topic. However, as a fundamental issue in relation based rough sets, under what condition two relations induce the same definable set family has not been discussed. In this paper, based on the concept of the closure of relations, we present a necessary and sufficient condition for two relations to induce the same definable set family.\n    ",
        "submission_date": "2013-10-09T00:00:00",
        "last_modified_date": "2013-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.2493",
        "title": "Combining Ontologies with Correspondences and Link Relations: The E-SHIQ Representation Framework",
        "authors": [
            "George A. Vouros",
            "Georgios Santipantakis"
        ],
        "abstract": "Combining knowledge and beliefs of autonomous peers in distributed settings, is a ma- jor challenge. In this paper we consider peers that combine ontologies and reason jointly with their coupled knowledge. Ontologies are within the SHIQ fragment of Description Logics. Although there are several representation frameworks for modular Description Log- ics, each one makes crucial assumptions concerning the subjectivity of peers' knowledge, the relation between the domains over which ontologies are interpreted, the expressivity of the constructors used for combining knowledge, and the way peers share their knowledge. However in settings where autonomous peers can evolve and extend their knowledge and beliefs independently from others, these assumptions may not hold. In this article, we moti- vate the need for a representation framework that allows peers to combine their knowledge in various ways, maintaining the subjectivity of their own knowledge and beliefs, and that reason collaboratively, constructing a tableau that is distributed among them, jointly. The paper presents the proposed E-SHIQ representation framework, the implementation of the E-SHIQ distributed tableau reasoner, and discusses the efficiency of this reasoner.\n    ",
        "submission_date": "2013-10-09T00:00:00",
        "last_modified_date": "2013-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.2743",
        "title": "Case Adaptation with Qualitative Algebras",
        "authors": [
            "Valmi Dufour-Lussier",
            "Florence Le Ber",
            "Jean Lieber",
            "Laura Martin"
        ],
        "abstract": "This paper proposes an approach for the adaptation of spatial or temporal cases in a case-based reasoning system. Qualitative algebras are used as spatial and temporal knowledge representation languages. The intuition behind this adaptation approach is to apply a substitution and then repair potential inconsistencies, thanks to belief revision on qualitative algebras. A temporal example from the cooking domain is given. (The paper on which this extended abstract is based was the recipient of the best paper award of the 2012 International Conference on Case-Based Reasoning.)\n    ",
        "submission_date": "2013-10-10T00:00:00",
        "last_modified_date": "2013-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.2797",
        "title": "Lemma Mining over HOL Light",
        "authors": [
            "Cezary Kaliszyk",
            "Josef Urban"
        ],
        "abstract": "Large formal mathematical libraries consist of millions of atomic inference steps that give rise to a corresponding number of proved statements (lemmas). Analogously to the informal mathematical practice, only a tiny fraction of such statements is named and re-used in later proofs by formal mathematicians. In this work, we suggest and implement criteria defining the estimated usefulness of the HOL Light lemmas for proving further theorems. We use these criteria to mine the large inference graph of all lemmas in the core HOL Light library, adding thousands of the best lemmas to the pool of named statements that can be re-used in later proofs. The usefulness of the new lemmas is then evaluated by comparing the performance of automated proving of the core HOL Light theorems with and without such added lemmas.\n    ",
        "submission_date": "2013-10-10T00:00:00",
        "last_modified_date": "2013-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.2805",
        "title": "MizAR 40 for Mizar 40",
        "authors": [
            "Cezary Kaliszyk",
            "Josef Urban"
        ],
        "abstract": "As a present to Mizar on its 40th anniversary, we develop an AI/ATP system that in 30 seconds of real time on a 14-CPU machine automatically proves 40% of the theorems in the latest official version of the Mizar Mathematical Library (MML). This is a considerable improvement over previous performance of large- theory AI/ATP methods measured on the whole MML. To achieve that, a large suite of AI/ATP methods is employed and further developed. We implement the most useful methods efficiently, to scale them to the 150000 formulas in MML. This reduces the training times over the corpus to 1-3 seconds, allowing a simple practical deployment of the methods in the online automated reasoning service for the Mizar users (MizAR).\n    ",
        "submission_date": "2013-10-10T00:00:00",
        "last_modified_date": "2013-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.2955",
        "title": "Spontaneous Analogy by Piggybacking on a Perceptual System",
        "authors": [
            "Marc Pickett",
            "David W. Aha"
        ],
        "abstract": "Most computational models of analogy assume they are given a delineated source domain and often a specified target domain. These systems do not address how analogs can be isolated from large domains and spontaneously retrieved from long-term memory, a process we call spontaneous analogy. We present a system that represents relational structures as feature bags. Using this representation, our system leverages perceptual algorithms to automatically create an ontology of relational structures and to efficiently retrieve analogs for new relational structures from long-term memory. We provide a demonstration of our approach that takes a set of unsegmented stories, constructs an ontology of analogical schemas (corresponding to plot devices), and uses this ontology to efficiently find analogs within new stories, yielding significant time-savings over linear analog retrieval at a small accuracy cost.\n    ",
        "submission_date": "2013-10-10T00:00:00",
        "last_modified_date": "2013-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.3174",
        "title": "Multi-Armed Bandits for Intelligent Tutoring Systems",
        "authors": [
            "Benjamin Clement",
            "Didier Roy",
            "Pierre-Yves Oudeyer",
            "Manuel Lopes"
        ],
        "abstract": "We present an approach to Intelligent Tutoring Systems which adaptively personalizes sequences of learning activities to maximize skills acquired by students, taking into account the limited time and motivational resources. At a given point in time, the system proposes to the students the activity which makes them progress faster. We introduce two algorithms that rely on the empirical estimation of the learning progress, RiARiT that uses information about the difficulty of each exercise and ZPDES that uses much less knowledge about the problem.\n",
        "submission_date": "2013-10-11T00:00:00",
        "last_modified_date": "2015-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.4086",
        "title": "A Computational Model of Two Cognitive Transitions Underlying Cultural Evolution",
        "authors": [
            "Liane Gabora",
            "Wei Wen Chia",
            "Hadi Firouzi"
        ],
        "abstract": "We tested the computational feasibility of the proposal that open-ended cultural evolution was made possible by two cognitive transitions: (1) onset of the capacity to chain thoughts together, followed by (2) onset of contextual focus (CF): the capacity to shift between a divergent mode of thought conducive to 'breaking out of a rut' and a convergent mode of thought conducive to minor modifications. These transitions were simulated in EVOC, an agent-based model of cultural evolution, in which the fitness of agents' actions increases as agents invent ideas for new actions, and imitate the fittest of their neighbors' actions. Both mean fitness and diversity of actions across the society increased with chaining, and even more so with CF, as hypothesized. CF was only effective when the fitness function changed, which supports its hypothesized role in generating and refining ideas.\n    ",
        "submission_date": "2013-10-15T00:00:00",
        "last_modified_date": "2013-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.4156",
        "title": "Validation Rules for Assessing and Improving SKOS Mapping Quality",
        "authors": [
            "Hong Sun",
            "Jos De Roo",
            "Marc Twagirumukiza",
            "Giovanni Mels",
            "Kristof Depraetere",
            "Boris De Vloed",
            "Dirk Colaert"
        ],
        "abstract": "The Simple Knowledge Organization System (SKOS) is popular for expressing controlled vocabularies, such as taxonomies, classifications, etc., for their use in Semantic Web applications. Using SKOS, concepts can be linked to other concepts and organized into hierarchies inside a single terminology system. Meanwhile, expressing mappings between concepts in different terminology systems is also possible. This paper discusses potential quality issues in using SKOS to express these terminology mappings. Problematic patterns are defined and corresponding rules are developed to automatically detect situations where the mappings either result in 'SKOS Vocabulary Hijacking' to the source vocabularies or cause conflicts. An example of using the rules to validate sample mappings between two clinical terminologies is given. The validation rules, expressed in N3 format, are available as open source.\n    ",
        "submission_date": "2013-10-15T00:00:00",
        "last_modified_date": "2013-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.4342",
        "title": "An Extensive Report on Cellular Automata Based Artificial Immune System for Strengthening Automated Protein Prediction",
        "authors": [
            "Pokkuluri Kiran Sree",
            "Inampudi Ramesh Babuhor",
            "SSSN Usha Devi N3"
        ],
        "abstract": "Artificial Immune System (AIS-MACA) a novel computational intelligence technique is can be used for strengthening the automated protein prediction system with more adaptability and incorporating more parallelism to the system. Most of the existing approaches are sequential which will classify the input into four major classes and these are designed for similar sequences. AIS-MACA is designed to identify ten classes from the sequences that share twilight zone similarity and identity with the training sequences with mixed and hybrid variations. This method also predicts three states (helix, strand, and coil) for the secondary structure. Our comprehensive design considers 10 feature selection methods and 4 classifiers to develop MACA (Multiple Attractor Cellular Automata) based classifiers that are build for each of the ten classes. We have tested the proposed classifier with twilight-zone and 1-high-similarity benchmark datasets with over three dozens of modern competing predictors shows that AIS-MACA provides the best overall accuracy that ranges between 80% and 89.8% depending on the dataset.\n    ",
        "submission_date": "2013-10-16T00:00:00",
        "last_modified_date": "2013-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.4986",
        "title": "Computing Preferred Extensions in Abstract Argumentation: a SAT-based Approach",
        "authors": [
            "Federico Cerutti",
            "Paul E. Dunne",
            "Massimiliano Giacomin",
            "Mauro Vallati"
        ],
        "abstract": "This paper presents a novel SAT-based approach for the computation of extensions in abstract argumentation, with focus on preferred semantics, and an empirical evaluation of its performances. The approach is based on the idea of reducing the problem of computing complete extensions to a SAT problem and then using a depth-first search method to derive preferred extensions. The proposed approach has been tested using two distinct SAT solvers and compared with three state-of-the-art systems for preferred extension computation. It turns out that the proposed approach delivers significantly better performances in the large majority of the considered cases.\n    ",
        "submission_date": "2013-10-18T00:00:00",
        "last_modified_date": "2013-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.5793",
        "title": "Intelligent City Traffic Management and Public Transportation System",
        "authors": [
            "Snehal Mulay",
            "Chinmay Dhekne",
            "Rucha Bapat",
            "Tanmay Budukh",
            "Soham Gadgil"
        ],
        "abstract": "Intelligent Transportation System in case of cities is controlling traffic congestion and regulating the traffic flow. This paper presents three modules that will help in managing city traffic issues and ultimately gives advanced development in transportation system. First module, Congestion Detection and Management will provide user real time information about congestion on the road towards his destination, Second module, Intelligent Public Transport System will provide user real time public transport information,i.e, local buses, and the third module, Signal Synchronization will help in controlling congestion at signals, with real time adjustments of signal timers according to the congestion. All the information that user is getting about the traffic or public transportation will be provided on users day to day device that is mobile through Android application or SMS. Moreover, communication can also be done via Website for Clients having internet access. And all these modules will be fully automated without any human intervention at server side.\n    ",
        "submission_date": "2013-10-22T00:00:00",
        "last_modified_date": "2013-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.6323",
        "title": "Logic in the Lab",
        "authors": [
            "Rineke Verbrugge"
        ],
        "abstract": "This file summarizes the plenary talk on laboratory experiments on logic at the TARK 2013 - 14th Conference on Theoretical Aspects of Rationality and Knowledge.\n    ",
        "submission_date": "2013-10-23T00:00:00",
        "last_modified_date": "2013-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.6429",
        "title": "Knowledge-Based Programs as Plans: Succinctness and the Complexity of Plan Existence",
        "authors": [
            "Jerome Lang",
            "Bruno Zanuttini"
        ],
        "abstract": "Knowledge-based programs (KBPs) are high-level protocols describing the course of action an agent should perform as a function of its knowledge. The use of KBPs for expressing action policies in AI planning has been surprisingly overlooked. Given that to each KBP corresponds an equivalent plan and vice versa, KBPs are typically more succinct than standard plans, but imply more on-line computation time. Here we make this argument formal, and prove that there exists an exponential succinctness gap between knowledge-based programs and standard plans. Then we address the complexity of plan existence. Some results trivially follow from results already known from the literature on planning under incomplete knowledge, but many were unknown so far.\n    ",
        "submission_date": "2013-10-23T00:00:00",
        "last_modified_date": "2013-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.6432",
        "title": "When is an Example a Counterexample?",
        "authors": [
            "Eric Pacuit",
            "Arthur Paul Pedersen",
            "Jan-Willem Romeijn"
        ],
        "abstract": "In this extended abstract, we carefully examine a purported counterexample to a postulate of iterated belief revision. We suggest that the example is better seen as a failure to apply the theory of belief revision in sufficient detail. The main contribution is conceptual aiming at the literature on the philosophical foundations of the AGM theory of belief revision [1]. Our discussion is centered around the observation that it is often unclear whether a specific example is a \"genuine\" counterexample to an abstract theory or a misapplication of that theory to a concrete case.\n    ",
        "submission_date": "2013-10-23T00:00:00",
        "last_modified_date": "2013-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.6775",
        "title": "Durkheim Project Data Analysis Report",
        "authors": [
            "Linas Vepstas"
        ],
        "abstract": "This report describes the suicidality prediction models created under the DARPA DCAPS program in association with the Durkheim Project [",
        "submission_date": "2013-10-24T00:00:00",
        "last_modified_date": "2013-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.7115",
        "title": "Studying a Chaotic Spiking Neural Model",
        "authors": [
            "Mohammad Alhawarat",
            "Waleed Nazih",
            "Mohammad Eldesouki"
        ],
        "abstract": "Dynamics of a chaotic spiking neuron model are being studied mathematically and experimentally. The Nonlinear Dynamic State neuron (NDS) is analysed to further understand the model and improve it. Chaos has many interesting properties such as sensitivity to initial conditions, space filling, control and synchronization. As suggested by biologists, these properties may be exploited and play vital role in carrying out computational tasks in human brain. The NDS model has some limitations; in thus paper the model is investigated to overcome some of these limitations in order to enhance the model. Therefore, the models parameters are tuned and the resulted dynamics are studied. Also, the discretization method of the model is considered. Moreover, a mathematical analysis is carried out to reveal the underlying dynamics of the model after tuning of its parameters. The results of the aforementioned methods revealed some facts regarding the NDS attractor and suggest the stabilization of a large number of unstable periodic orbits (UPOs) which might correspond to memories in phase space.\n    ",
        "submission_date": "2013-10-26T00:00:00",
        "last_modified_date": "2013-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.7367",
        "title": "Semantic Description of Web Services",
        "authors": [
            "Thabet Slimani"
        ],
        "abstract": "The tasks of semantic web service (discovery, selection, composition, and execution) are supposed to enable seamless interoperation between systems, whereby human intervention is kept at a minimum. In the field of Web service description research, the exploitation of descriptions of services through semantics is a better support for the life-cycle of Web services. The large number of developed ontologies, languages of representations, and integrated frameworks supporting the discovery, composition and invocation of services is a good indicator that research in the field of Semantic Web Services (SWS) has been considerably active. We provide in this paper a detailed classification of the approaches and solutions, indicating their core characteristics and objectives required and provide indicators for the interested reader to follow up further insights and details about these solutions and related software.\n    ",
        "submission_date": "2013-10-28T00:00:00",
        "last_modified_date": "2013-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.7442",
        "title": "Ranking basic belief assignments in decision making under uncertain environment",
        "authors": [
            "Yuxian Du",
            "Shiyu Chen",
            "Yong Hu",
            "Felix T.S. Chan",
            "Sankaran Mahadevan",
            "Yong Deng"
        ],
        "abstract": "Dempster-Shafer theory (D-S theory) is widely used in decision making under the uncertain environment. Ranking basic belief assignments (BBAs) now is an open issue. Existing evidence distance measures cannot rank the BBAs in the situations when the propositions have their own ranking order or their inherent measure of closeness. To address this issue, a new ranking evidence distance (RED) measure is proposed. Compared with the existing evidence distance measures including the Jousselme's distance and the distance between betting commitments, the proposed RED measure is much more general due to the fact that the order of the propositions in the systems is taken into consideration. If there is no order or no inherent measure of closeness in the propositions, our proposed RED measure is reduced to the existing evidence distance. Numerical examples show that the proposed RED measure is an efficient alternative to rank BBAs in decision making under uncertain environment.\n    ",
        "submission_date": "2013-10-28T00:00:00",
        "last_modified_date": "2013-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.7828",
        "title": "A Complete Parameterized Complexity Analysis of Bounded Planning",
        "authors": [
            "Christer Baeckstroem",
            "Peter Jonsson",
            "Sebastian Ordyniak",
            "Stefan Szeider"
        ],
        "abstract": "The propositional planning problem is a notoriously difficult computational problem, which remains hard even under strong syntactical and structural restrictions. Given its difficulty it becomes natural to study planning in the context of parameterized complexity. In this paper we continue the work initiated by Downey, Fellows and Stege on the parameterized complexity of planning with respect to the parameter \"length of the solution plan.\" We provide a complete classification of the parameterized complexity of the planning problem under two of the most prominent syntactical restrictions, i.e., the so called PUBS restrictions introduced by Baeckstroem and Nebel and restrictions on the number of preconditions and effects as introduced by Bylander. We also determine which of the considered fixed-parameter tractable problems admit a polynomial kernel and which don't.\n    ",
        "submission_date": "2013-10-29T00:00:00",
        "last_modified_date": "2013-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.8120",
        "title": "On the Tractability of Minimal Model Computation for Some CNF Theories",
        "authors": [
            "Fabrizio Angiulli",
            "Rachel Ben-Eliyahu-Zohary",
            "Fabio Fassetti",
            "Luigi Palopoli"
        ],
        "abstract": "Designing algorithms capable of efficiently constructing minimal models of CNFs is an important task in AI. This paper provides new results along this research line and presents new algorithms for performing minimal model finding and checking over positive propositional CNFs and model minimization over propositional CNFs. An algorithmic schema, called the Generalized Elimination Algorithm (GEA) is presented, that computes a minimal model of any positive CNF. The schema generalizes the Elimination Algorithm (EA) [BP97], which computes a minimal model of positive head-cycle-free (HCF) CNF theories. While the EA always runs in polynomial time in the size of the input HCF CNF, the complexity of the GEA depends on the complexity of the specific eliminating operator invoked therein, which may in general turn out to be exponential. Therefore, a specific eliminating operator is defined by which the GEA computes, in polynomial time, a minimal model for a class of CNF that strictly includes head-elementary-set-free (HEF) CNF theories [GLL06], which form, in their turn, a strict superset of HCF theories. Furthermore, in order to deal with the high complexity associated with recognizing HEF theories, an \"incomplete\" variant of the GEA (called IGEA) is proposed: the resulting schema, once instantiated with an appropriate elimination operator, always constructs a model of the input CNF, which is guaranteed to be minimal if the input theory is HEF. In the light of the above results, the main contribution of this work is the enlargement of the tractability frontier for the minimal model finding and checking and the model minimization problems.\n    ",
        "submission_date": "2013-10-30T00:00:00",
        "last_modified_date": "2013-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.8588",
        "title": "A Meta-heuristically Approach of the Spatial Assignment Problem of Human Resources in Multi-sites Enterprise",
        "authors": [
            "Tkatek Said",
            "Abdoun Otman",
            "Abouchabaka Jaafar",
            "Rafalia Najat"
        ],
        "abstract": "The aim of this work is to present a meta-heuristically approach of the spatial assignment problem of human resources in multi-sites enterprise. Usually, this problem consists to move employees from one site to another based on one or more criteria. Our goal in this new approach is to improve the quality of service and performance of all sites with maximizing an objective function under some managers imposed constraints. The formulation presented here of this problem coincides perfectly with a Combinatorial Optimization Problem (COP) which is in the most cases NP-hard to solve optimally. To avoid this difficulty, we have opted to use a meta-heuristic popular method, which is the genetic algorithm, to solve this problem in concrete cases. The results obtained have shown the effectiveness of our approach, which remains until now very costly in time. But the reduction of the time can be obtained by different ways that we plan to do in the next work.\n    ",
        "submission_date": "2013-09-22T00:00:00",
        "last_modified_date": "2013-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.8599",
        "title": "Information Compression, Intelligence, Computing, and Mathematics",
        "authors": [
            "J. Gerard Wolff"
        ],
        "abstract": "This paper presents evidence for the idea that much of artificial intelligence, human perception and cognition, mainstream computing, and mathematics, may be understood as compression of information via the matching and unification of patterns. This is the basis for the \"SP theory of intelligence\", outlined in the paper and fully described elsewhere. Relevant evidence may be seen: in empirical support for the SP theory; in some advantages of information compression (IC) in terms of biology and engineering; in our use of shorthands and ordinary words in language; in how we merge successive views of any one thing; in visual recognition; in binocular vision; in visual adaptation; in how we learn lexical and grammatical structures in language; and in perceptual constancies. IC via the matching and unification of patterns may be seen in both computing and mathematics: in IC via equations; in the matching and unification of names; in the reduction or removal of redundancy from unary numbers; in the workings of Post's Canonical System and the transition function in the Universal Turing Machine; in the way computers retrieve information from memory; in systems like Prolog; and in the query-by-example technique for information retrieval. The chunking-with-codes technique for IC may be seen in the use of named functions to avoid repetition of computer code. The schema-plus-correction technique may be seen in functions with parameters and in the use of classes in object-oriented programming. And the run-length coding technique may be seen in multiplication, in division, and in several other devices in mathematics and computing. The SP theory resolves the apparent paradox of \"decompression by compression\". And computing and cognition as IC is compatible with the uses of redundancy in such things as backup copies to safeguard data and understanding speech in a noisy environment.\n    ",
        "submission_date": "2013-10-31T00:00:00",
        "last_modified_date": "2015-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.0351",
        "title": "Rough matroids based on coverings",
        "authors": [
            "Bin Yang",
            "Hong Zhao",
            "William Zhu"
        ],
        "abstract": "The introduction of covering-based rough sets has made a substantial contribution to the classical rough sets. However, many vital problems in rough sets, including attribution reduction, are NP-hard and therefore the algorithms for solving them are usually greedy. Matroid, as a generalization of linear independence in vector spaces, it has a variety of applications in many fields such as algorithm design and combinatorial optimization. An excellent introduction to the topic of rough matroids is due to Zhu and Wang. On the basis of their work, we study the rough matroids based on coverings in this paper. First, we investigate some properties of the definable sets with respect to a covering. Specifically, it is interesting that the set of all definable sets with respect to a covering, equipped with the binary relation of inclusion $\\subseteq$, constructs a lattice. Second, we propose the rough matroids based on coverings, which are a generalization of the rough matroids based on relations. Finally, some properties of rough matroids based on coverings are explored. Moreover, an equivalent formulation of rough matroids based on coverings is presented. These interesting and important results exhibit many potential connections between rough sets and matroids.\n    ",
        "submission_date": "2013-11-02T00:00:00",
        "last_modified_date": "2013-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.0413",
        "title": "Information, Computation, Cognition. Agency-based Hierarchies of Levels",
        "authors": [
            "Gordana Dodig-Crnkovic"
        ],
        "abstract": "Nature can be seen as informational structure with computational dynamics (info-computationalism), where an (info-computational) agent is needed for the potential information of the world to actualize. Starting from the definition of information as the difference in one physical system that makes a difference in another physical system, which combines Bateson and Hewitt definitions, the argument is advanced for natural computation as a computational model of the dynamics of the physical world where information processing is constantly going on, on a variety of levels of organization. This setting helps elucidating the relationships between computation, information, agency and cognition, within the common conceptual framework, which has special relevance for biology and robotics.\n    ",
        "submission_date": "2013-11-02T00:00:00",
        "last_modified_date": "2013-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.0716",
        "title": "Artificial Intelligence in Humans",
        "authors": [
            "Michael Swan Laufer"
        ],
        "abstract": "In this paper, I put forward that in many instances, thinking mechanisms are equivalent to artificial intelligence modules programmed into the human mind.\n    ",
        "submission_date": "2013-10-30T00:00:00",
        "last_modified_date": "2013-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.0944",
        "title": "Connectivity for matroids based on rough sets",
        "authors": [
            "Bin Yang",
            "William Zhu"
        ],
        "abstract": "In mathematics and computer science, connectivity is one of the basic concepts of matroid theory: it asks for the minimum number of elements which need to be removed to disconnect the remaining nodes from each other. It is closely related to the theory of network flow problems. The connectivity of a matroid is an important measure of its robustness as a network. Therefore, it is very necessary to investigate the conditions under which a matroid is connected. In this paper, the connectivity for matroids is studied through relation-based rough sets. First, a symmetric and transitive relation is introduced from a general matroid and its properties are explored from the viewpoint of matroids. Moreover, through the relation introduced by a general matroid, an undirected graph is generalized. Specifically, the connection of the graph can be investigated by the relation-based rough sets. Second, we study the connectivity for matroids by means of relation-based rough sets and some conditions under which a general matroid is connected are presented. Finally, it is easy to prove that the connectivity for a general matroid with some special properties and its induced undirected graph is equivalent. These results show an important application of relation-based rough sets to matroids.\n    ",
        "submission_date": "2013-11-05T00:00:00",
        "last_modified_date": "2013-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.1632",
        "title": "Persistence, Change, and the Integration of Objects and Processes in the Framework of the General Formal Ontology",
        "authors": [
            "Heinrich Herre"
        ],
        "abstract": "In this paper we discuss various problems, associated to temporal phenomena. These problems include persistence and change, the integration of objects and processes, and truth-makers for temporal propositions. We propose an approach which interprets persistence as a phenomenon emanating from the activity of the mind, and which, additionally, postulates that persistence, finally, rests on personal identity. The General Formal Ontology (GFO) is a top level ontology being developed at the University of Leipzig. Top level ontologies can be roughly divided into 3D-ontologies, and 4D-ontologies. GFO is the only top level ontology, used in applications, which is a 4D-ontology admitting additionally 3D objects. Objects and processes are integrated in a natural way.\n    ",
        "submission_date": "2013-11-07T00:00:00",
        "last_modified_date": "2013-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.1935",
        "title": "Unsupervised learning human's activities by overexpressed recognized non-speech sounds",
        "authors": [
            "Serge Smidtas",
            "Magalie Peyrot"
        ],
        "abstract": "Human activity and environment produces sounds such as, at home, the noise produced by water, cough, or television. These sounds can be used to determine the activity in the environment. The objective is to monitor a person's activity or determine his environment using a single low cost microphone by sound analysis. The purpose is to adapt programs to the activity or environment or detect abnormal situations. Some patterns of over expressed repeatedly in the sequences of recognized sounds inter and intra environment allow to characterize activities such as the entrance of a person in the house, or a tv program watched. We first manually annotated 1500 sounds of daily life activity of old persons living at home recognized sounds. Then we inferred an ontology and enriched the database of annotation with a crowed sourced manual annotation of 7500 sounds to help with the annotation of the most frequent sounds. Using learning sound algorithms, we defined 50 types of the most frequent sounds. We used this set of recognizable sounds as a base to tag sounds and put tags on them. By using over expressed number of motifs of sequences of the tags, we were able to categorize using only a single low-cost microphone, complex activities of daily life of a persona at home as watching TV, entrance in the apartment of a person, or phone conversation including detecting unknown activities as repeated tasks performed by users.\n    ",
        "submission_date": "2013-11-08T00:00:00",
        "last_modified_date": "2013-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.2531",
        "title": "Motility at the origin of life: Its characterization and a model",
        "authors": [
            "Tom Froese",
            "Nathaniel Virgo",
            "Takashi Ikegami"
        ],
        "abstract": "Due to recent advances in synthetic biology and artificial life, the origin of life is currently a hot topic of research. We review the literature and argue that the two traditionally competing \"replicator-first\" and \"metabolism-first\" approaches are merging into one integrated theory of individuation and evolution. We contribute to the maturation of this more inclusive approach by highlighting some problematic assumptions that still lead to an impoverished conception of the phenomenon of life. In particular, we argue that the new consensus has so far failed to consider the relevance of intermediate timescales. We propose that an adequate theory of life must account for the fact that all living beings are situated in at least four distinct timescales, which are typically associated with metabolism, motility, development, and evolution. On this view, self-movement, adaptive behavior and morphological changes could have already been present at the origin of life. In order to illustrate this possibility we analyze a minimal model of life-like phenomena, namely of precarious, individuated, dissipative structures that can be found in simple reaction-diffusion systems. Based on our analysis we suggest that processes in intermediate timescales could have already been operative in prebiotic systems. They may have facilitated and constrained changes occurring in the faster- and slower-paced timescales of chemical self-individuation and evolution by natural selection, respectively.\n    ",
        "submission_date": "2013-11-11T00:00:00",
        "last_modified_date": "2013-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.2886",
        "title": "A Fuzzy AHP Approach for Supplier Selection Problem: A Case Study in a Gear Motor Company",
        "authors": [
            "Mustafa Batuhan Ayhan"
        ],
        "abstract": "Suuplier selection is one of the most important functions of a purchasing department. Since by deciding the best supplier, companies can save material costs and increase competitive ",
        "submission_date": "2013-10-09T00:00:00",
        "last_modified_date": "2013-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.2912",
        "title": "A Misanthropic Reinterpretation of the Chinese Room Problem",
        "authors": [
            "Michael S. Laufer"
        ],
        "abstract": "The chinese room problem asks if computers can think; I ask here if most humans can.\n    ",
        "submission_date": "2013-10-26T00:00:00",
        "last_modified_date": "2013-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.2914",
        "title": "A novel local search based on variable-focusing for random K-SAT",
        "authors": [
            "R\u00e9mi Lemoy",
            "Mikko Alava",
            "Erik Aurell"
        ],
        "abstract": "We introduce a new local search algorithm for satisfiability problems. Usual approaches focus uniformly on unsatisfied clauses. The new method works by picking uniformly random variables in unsatisfied clauses. A Variable-based Focused Metropolis Search (V-FMS) is then applied to random 3-SAT. We show that it is quite comparable in performance to the clause-based FMS. Consequences for algorithmic design are discussed.\n    ",
        "submission_date": "2013-10-09T00:00:00",
        "last_modified_date": "2013-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.3198",
        "title": "Sound, Complete and Minimal UCQ-Rewriting for Existential Rules",
        "authors": [
            "M\u00e9lanie K\u00f6nig",
            "Michel Lecl\u00e8re",
            "Marie-Laure Mugnier",
            "Micha\u00ebl Thomazo"
        ],
        "abstract": "We address the issue of Ontology-Based Data Access, with ontologies represented in the framework of existential rules, also known as Datalog+/-. A well-known approach involves rewriting the query using ontological knowledge. We focus here on the basic rewriting technique which consists of rewriting the initial query into a union of conjunctive queries. First, we study a generic breadth-first rewriting algorithm, which takes as input any rewriting operator, and define properties of rewriting operators that ensure the correctness of the algorithm. Then, we focus on piece-unifiers, which provide a rewriting operator with the desired properties. Finally, we propose an implementation of this framework and report some experiments.\n    ",
        "submission_date": "2013-11-13T00:00:00",
        "last_modified_date": "2013-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.3353",
        "title": "SUNNY: a Lazy Portfolio Approach for Constraint Solving",
        "authors": [
            "Roberto Amadini",
            "Maurizio Gabbrielli",
            "Jacopo Mauro"
        ],
        "abstract": "*** To appear in Theory and Practice of Logic Programming (TPLP) ***\n",
        "submission_date": "2013-11-14T00:00:00",
        "last_modified_date": "2014-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.3355",
        "title": "HINO: a BFO-aligned ontology representing human molecular interactions and pathways",
        "authors": [
            "Yongqun He",
            "Zoushuang Xiang"
        ],
        "abstract": "Many database resources, such as Reactome, collect manually annotated reactions, interactions, and pathways from peer-reviewed publications. The interactors (e.g., a protein), interactions, and pathways in these data resources are often represented as instances in using BioPAX, a standard pathway data exchange format. However, these interactions are better represented as classes (or universals) since they always occur given appropriate conditions. This study aims to represent various human interaction pathways and networks as classes via a formal ontology aligned with the Basic Formal Ontology (BFO). Towards this goal, the Human Interaction Network Ontology (HINO) was generated by extending the BFO-aligned Interaction Network Ontology (INO). All human pathways and associated processes and interactors listed in Reactome and represented in BioPAX were first converted to ontology classes by aligning them under INO. Related terms and associated relations and hierarchies from external ontologies (e.g., CHEBI and GO) were also retrieved and imported into HINO. HINO ontology terms were resolved in the linked ontology data server Ontobee. The RDF triples stored in the RDF triple store are queryable through a SPARQL program. Such an ontology system supports advanced pathway data integration and applications.\n    ",
        "submission_date": "2013-11-14T00:00:00",
        "last_modified_date": "2013-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.3800",
        "title": "Structural Weights in Ontology Matching",
        "authors": [
            "Mohammad Mehdi Keikha",
            "Mohammad Ali Nematbakhsh",
            "Behrouz Tork Ladani"
        ],
        "abstract": "Ontology matching finds correspondences between similar entities of different ontologies. Two ontologies may be similar in some aspects such as structure, semantic etc. Most ontology matching systems integrate multiple matchers to extract all the similarities that two ontologies may have. Thus, we face a major problem to aggregate different similarities. Some matching systems use experimental weights for aggregation of similarities among different matchers while others use machine learning approaches and optimization algorithms to find optimal weights to assign to different matchers. However, both approaches have their own deficiencies. In this paper, we will point out the problems and shortcomings of current similarity aggregation strategies. Then, we propose a new strategy, which enables us to utilize the structural information of ontologies to get weights of matchers, for the similarity aggregation task. For achieving this goal, we create a new Ontology Matching system which it uses three available matchers, namely GMO, ISub and VDoc. We have tested our similarity aggregation strategy on the OAEI 2012 data set. Experimental results show significant improvements in accuracies of several cases, especially in matching the classes of ontologies. We will compare the performance of our similarity aggregation strategy with other well-known strategies\n    ",
        "submission_date": "2013-11-15T00:00:00",
        "last_modified_date": "2013-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.3829",
        "title": "Planning based on classification by induction graph",
        "authors": [
            "Sofia Benbelkacem",
            "Baghdad Atmani",
            "Mohamed Benamina"
        ],
        "abstract": "In Artificial Intelligence, planning refers to an area of research that proposes to develop systems that can automatically generate a result set, in the form of an integrated decision-making system through a formal procedure, known as plan. Instead of resorting to the scheduling algorithms to generate plans, it is proposed to operate the automatic learning by decision tree to optimize time. In this paper, we propose to build a classification model by induction graph from a learning sample containing plans that have an associated set of descriptors whose values change depending on each plan. This model will then operate for classifying new cases by assigning the appropriate plan.\n    ",
        "submission_date": "2013-11-15T00:00:00",
        "last_modified_date": "2013-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.3959",
        "title": "Clustering Markov Decision Processes For Continual Transfer",
        "authors": [
            "M. M. Hassan Mahmud",
            "Majd Hawasly",
            "Benjamin Rosman",
            "Subramanian Ramamoorthy"
        ],
        "abstract": "We present algorithms to effectively represent a set of Markov decision processes (MDPs), whose optimal policies have already been learned, by a smaller source subset for lifelong, policy-reuse-based transfer learning in reinforcement learning. This is necessary when the number of previous tasks is large and the cost of measuring similarity counteracts the benefit of transfer. The source subset forms an `$\\epsilon$-net' over the original set of MDPs, in the sense that for each previous MDP $M_p$, there is a source $M^s$ whose optimal policy has $<\\epsilon$ regret in $M_p$. Our contributions are as follows. We present EXP-3-Transfer, a principled policy-reuse algorithm that optimally reuses a given source policy set when learning for a new MDP. We present a framework to cluster the previous MDPs to extract a source subset. The framework consists of (i) a distance $d_V$ over MDPs to measure policy-based similarity between MDPs; (ii) a cost function $g(\\cdot)$ that uses $d_V$ to measure how good a particular clustering is for generating useful source tasks for EXP-3-Transfer and (iii) a provably convergent algorithm, MHAV, for finding the optimal clustering. We validate our algorithms through experiments in a surveillance domain.\n    ",
        "submission_date": "2013-11-15T00:00:00",
        "last_modified_date": "2016-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.3982",
        "title": "Inferring Multilateral Relations from Dynamic Pairwise Interactions",
        "authors": [
            "Aaron Schein",
            "Juston Moore",
            "Hanna Wallach"
        ],
        "abstract": "Correlations between anomalous activity patterns can yield pertinent information about complex social processes: a significant deviation from normal behavior, exhibited simultaneously by multiple pairs of actors, provides evidence for some underlying relationship involving those pairs---i.e., a multilateral relation. We introduce a new nonparametric Bayesian latent variable model that explicitly captures correlations between anomalous interaction counts and uses these shared deviations from normal activity patterns to identify and characterize multilateral relations. We showcase our model's capabilities using the newly curated Global Database of Events, Location, and Tone, a dataset that has seen considerable interest in the social sciences and the popular press, but which has is largely unexplored by the machine learning community. We provide a detailed analysis of the latent structure inferred by our model and show that the multilateral relations correspond to major international events and long-term international relationships. These findings lead us to recommend our model for any data-driven analysis of interaction networks where dynamic interactions over the edges provide evidence for latent social structure.\n    ",
        "submission_date": "2013-11-15T00:00:00",
        "last_modified_date": "2013-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.4056",
        "title": "A generalized evidence distance",
        "authors": [
            "Hongming Mo",
            "Xiaoyan Su",
            "Yong Hu",
            "Yong Deng"
        ],
        "abstract": "Dempster-Shafer theory of evidence (D-S theory) is widely used in uncertain information process. The basic probability assignment(BPA) is a key element in D-S theory. How to measure the distance between two BPAs is an open issue. In this paper, a new method to measure the distance of two BPAs is proposed. The proposed method is a generalized of existing evidence distance. Numerical examples are illustrated that the proposed method can overcome the shortcomings of existing methods.\n    ",
        "submission_date": "2013-11-16T00:00:00",
        "last_modified_date": "2013-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.4064",
        "title": "Methods for Integrating Knowledge with the Three-Weight Optimization Algorithm for Hybrid Cognitive Processing",
        "authors": [
            "Nate Derbinsky",
            "Jos\u00e9 Bento",
            "Jonathan S. Yedidia"
        ],
        "abstract": "In this paper we consider optimization as an approach for quickly and flexibly developing hybrid cognitive capabilities that are efficient, scalable, and can exploit knowledge to improve solution speed and quality. In this context, we focus on the Three-Weight Algorithm, which aims to solve general optimization problems. We propose novel methods by which to integrate knowledge with this algorithm to improve expressiveness, efficiency, and scaling, and demonstrate these techniques on two example problems (Sudoku and circle packing).\n    ",
        "submission_date": "2013-11-16T00:00:00",
        "last_modified_date": "2013-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.4086",
        "title": "A hybrid decision support system : application on healthcare",
        "authors": [
            "Abdelhak Mansoul",
            "Baghdad Atmani",
            "Sofia Benbelkacem"
        ],
        "abstract": "Many systems based on knowledge, especially expert systems for medical decision support have been developed. Only systems are based on production rules, and cannot learn and evolve only by updating them. In addition, taking into account several criteria induces an exorbitant number of rules to be injected into the system. It becomes difficult to translate medical knowledge or a support decision as a simple rule. Moreover, reasoning based on generic cases became classic and can even reduce the range of possible solutions. To remedy that, we propose an approach based on using a multi-criteria decision guided by a case-based reasoning (CBR) approach.\n    ",
        "submission_date": "2013-11-16T00:00:00",
        "last_modified_date": "2013-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.4166",
        "title": "A Visibility Graph Averaging Aggregation Operator",
        "authors": [
            "Shiyu Chen",
            "Yong Hu",
            "Sankaran Mahadevan",
            "Yong Deng"
        ],
        "abstract": "The problem of aggregation is considerable importance in many disciplines. In this paper, a new type of operator called visibility graph averaging (VGA) aggregation operator is proposed. This proposed operator is based on the visibility graph which can convert a time series into a graph. The weights are obtained according to the importance of the data in the visibility graph. Finally, the VGA operator is used in the analysis of the TAIEX database to illustrate that it is practical and compared with the classic aggregation operators, it shows its advantage that it not only implements the aggregation of the data purely, but also conserves the time information, and meanwhile, the determination of the weights is more reasonable.\n    ",
        "submission_date": "2013-11-17T00:00:00",
        "last_modified_date": "2013-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.4319",
        "title": "Ranking Algorithms by Performance",
        "authors": [
            "Lars Kotthoff"
        ],
        "abstract": "A common way of doing algorithm selection is to train a machine learning model and predict the best algorithm from a portfolio to solve a particular problem. While this method has been highly successful, choosing only a single algorithm has inherent limitations -- if the choice was bad, no remedial action can be taken and parallelism cannot be exploited, to name but a few problems. In this paper, we investigate how to predict the ranking of the portfolio algorithms on a particular problem. This information can be used to choose the single best algorithm, but also to allocate resources to the algorithms according to their rank. We evaluate a range of approaches to predict the ranking of a set of algorithms on a problem. We furthermore introduce a framework for categorizing ranking predictions that allows to judge the expressiveness of the predictive output. Our experimental evaluation demonstrates on a range of data sets from the literature that it is beneficial to consider the relationship between algorithms when predicting rankings. We furthermore show that relatively naive approaches deliver rankings of good quality already.\n    ",
        "submission_date": "2013-11-18T00:00:00",
        "last_modified_date": "2013-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.4527",
        "title": "A message-passing algorithm for multi-agent trajectory planning",
        "authors": [
            "Jose Bento",
            "Nate Derbinsky",
            "Javier Alonso-Mora",
            "Jonathan Yedidia"
        ],
        "abstract": "We describe a novel approach for computing collision-free \\emph{global} trajectories for $p$ agents with specified initial and final configurations, based on an improved version of the alternating direction method of multipliers (ADMM). Compared with existing methods, our approach is naturally parallelizable and allows for incorporating different cost functionals with only minor adjustments. We apply our method to classical challenging instances and observe that its computational requirements scale well with $p$ for several cost functionals. We also show that a specialization of our algorithm can be used for {\\em local} motion planning by solving the problem of joint optimization in velocity space.\n    ",
        "submission_date": "2013-11-18T00:00:00",
        "last_modified_date": "2013-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.4564",
        "title": "Planning by case-based reasoning based on fuzzy logic",
        "authors": [
            "Baghdad Atmani",
            "Sofia Benbelkacem",
            "Mohamed Benamina"
        ],
        "abstract": "The treatment of complex systems often requires the manipulation of vague, imprecise and uncertain information. Indeed, the human being is competent in handling of such systems in a natural way. Instead of thinking in mathematical terms, humans describes the behavior of the system by language proposals. In order to represent this type of information, Zadeh proposed to model the mechanism of human thought by approximate reasoning based on linguistic variables. He introduced the theory of fuzzy sets in 1965, which provides an interface between language and digital worlds. In this paper, we propose a Boolean modeling of the fuzzy reasoning that we baptized Fuzzy-BML and uses the characteristics of induction graph classification. Fuzzy-BML is the process by which the retrieval phase of a CBR is modelled not in the conventional form of mathematical equations, but in the form of a database with membership functions of fuzzy rules.\n    ",
        "submission_date": "2013-11-18T00:00:00",
        "last_modified_date": "2013-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.4639",
        "title": "Post-Proceedings of the First International Workshop on Learning and Nonmonotonic Reasoning",
        "authors": [
            "Katsumi Inoue",
            "Chiaki Sakama"
        ],
        "abstract": "Knowledge Representation and Reasoning and Machine Learning are two important fields in AI. Nonmonotonic logic programming (NMLP) and Answer Set Programming (ASP) provide formal languages for representing and reasoning with commonsense knowledge and realize declarative problem solving in AI. On the other side, Inductive Logic Programming (ILP) realizes Machine Learning in logic programming, which provides a formal background to inductive learning and the techniques have been applied to the fields of relational learning and data mining. Generally speaking, NMLP and ASP realize nonmonotonic reasoning while lack the ability of learning. By contrast, ILP realizes inductive learning while most techniques have been developed under the classical monotonic logic. With this background, some researchers attempt to combine techniques in the context of nonmonotonic ILP. Such combination will introduce a learning mechanism to programs and would exploit new applications on the NMLP side, while on the ILP side it will extend the representation language and enable us to use existing solvers. Cross-fertilization between learning and nonmonotonic reasoning can also occur in such as the use of answer set solvers for ILP, speed-up learning while running answer set solvers, learning action theories, learning transition rules in dynamical systems, abductive learning, learning biological networks with inhibition, and applications involving default and negation. This workshop is the first attempt to provide an open forum for the identification of problems and discussion of possible collaborations among researchers with complementary expertise. The workshop was held on September 15th of 2013 in Corunna, Spain. This post-proceedings contains five technical papers (out of six accepted papers) and the abstract of the invited talk by Luc De Raedt.\n    ",
        "submission_date": "2013-11-19T00:00:00",
        "last_modified_date": "2013-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.4987",
        "title": "Analyzing Evolutionary Optimization in Noisy Environments",
        "authors": [
            "Chao Qian",
            "Yang Yu",
            "Zhi-Hua Zhou"
        ],
        "abstract": "Many optimization tasks have to be handled in noisy environments, where we cannot obtain the exact evaluation of a solution but only a noisy one. For noisy optimization tasks, evolutionary algorithms (EAs), a kind of stochastic metaheuristic search algorithm, have been widely and successfully applied. Previous work mainly focuses on empirical studying and designing EAs for noisy optimization, while, the theoretical counterpart has been little investigated. In this paper, we investigate a largely ignored question, i.e., whether an optimization problem will always become harder for EAs in a noisy environment. We prove that the answer is negative, with respect to the measurement of the expected running time. The result implies that, for optimization tasks that have already been quite hard to solve, the noise may not have a negative effect, and the easier a task the more negatively affected by the noise. On a representative problem where the noise has a strong negative effect, we examine two commonly employed mechanisms in EAs dealing with noise, the re-evaluation and the threshold selection strategies. The analysis discloses that the two strategies, however, both are not effective, i.e., they do not make the EA more noise tolerant. We then find that a small modification of the threshold selection allows it to be proven as an effective strategy for dealing with the noise in the problem.\n    ",
        "submission_date": "2013-11-20T00:00:00",
        "last_modified_date": "2013-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.5355",
        "title": "Dealing with the Fuzziness of Human Reasoning",
        "authors": [
            "Michael Gr. Voskoglou",
            "Igor Ya. Subbotin"
        ],
        "abstract": "Reasoning, the most important human brain operation, is charactrized by a degree fuzziness. In the present paper we construct a fuzzy model for the reasoning process giving through the calculation of the possibilities of all possible individuals' profiles a quantitative/qualitative view of their behaviour during the above process and we use the centroid defuzzification technique for measuring the reasoning skills. We also present a number of classroom experiments illustrating our results in practice.\n    ",
        "submission_date": "2013-11-21T00:00:00",
        "last_modified_date": "2013-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.5998",
        "title": "A brief network analysis of Artificial Intelligence publication",
        "authors": [
            "Yunpeng Li",
            "Jie Liu",
            "Yong Deng"
        ],
        "abstract": "In this paper, we present an illustration to the history of Artificial Intelligence(AI) with a statistical analysis of publish since 1940. We collected and mined through the IEEE publish data base to analysis the geological and chronological variance of the activeness of research in AI. The connections between different institutes are showed. The result shows that the leading community of AI research are mainly in the USA, China, the Europe and Japan. The key institutes, authors and the research hotspots are revealed. It is found that the research institutes in the fields like Data Mining, Computer Vision, Pattern Recognition and some other fields of Machine Learning are quite consistent, implying a strong interaction between the community of each field. It is also showed that the research of Electronic Engineering and Industrial or Commercial applications are very active in California. Japan is also publishing a lot of papers in robotics. Due to the limitation of data source, the result might be overly influenced by the number of published articles, which is to our best improved by applying network keynode analysis on the research community instead of merely count the number of publish.\n    ",
        "submission_date": "2013-11-23T00:00:00",
        "last_modified_date": "2013-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.6054",
        "title": "Q-learning optimization in a multi-agents system for image segmentation",
        "authors": [
            "Issam Qaffou",
            "Mohamed Sadgal",
            "Abdelaziz Elfazziki"
        ],
        "abstract": "To know which operators to apply and in which order, as well as attributing good values to their parameters is a challenge for users of computer vision. This paper proposes a solution to this problem as a multi-agent system modeled according to the Vowel approach and using the Q-learning algorithm to optimize its choice. An implementation is given to test and validate this method.\n    ",
        "submission_date": "2013-11-23T00:00:00",
        "last_modified_date": "2013-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.6591",
        "title": "On the Complexity and Approximation of Binary Evidence in Lifted Inference",
        "authors": [
            "Guy Van den Broeck",
            "Adnan Darwiche"
        ],
        "abstract": "Lifted inference algorithms exploit symmetries in probabilistic models to speed up inference. They show impressive performance when calculating unconditional probabilities in relational models, but often resort to non-lifted inference when computing conditional probabilities. The reason is that conditioning on evidence breaks many of the model's symmetries, which can preempt standard lifting techniques. Recent theoretical results show, for example, that conditioning on evidence which corresponds to binary relations is #P-hard, suggesting that no lifting is to be expected in the worst case. In this paper, we balance this negative result by identifying the Boolean rank of the evidence as a key parameter for characterizing the complexity of conditioning in lifted inference. In particular, we show that conditioning on binary evidence with bounded Boolean rank is efficient. This opens up the possibility of approximating evidence by a low-rank Boolean matrix factorization, which we investigate both theoretically and empirically.\n    ",
        "submission_date": "2013-11-26T00:00:00",
        "last_modified_date": "2013-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.6594",
        "title": "Auto-adaptative Laplacian Pyramids for High-dimensional Data Analysis",
        "authors": [
            "\u00c1ngela Fern\u00e1ndez",
            "Neta Rabin",
            "Dalia Fishelov",
            "Jos\u00e9 R. Dorronsoro"
        ],
        "abstract": "Non-linear dimensionality reduction techniques such as manifold learning algorithms have become a common way for processing and analyzing high-dimensional patterns that often have attached a target that corresponds to the value of an unknown function. Their application to new points consists in two steps: first, embedding the new data point into the low dimensional space and then, estimating the function value on the test point from its neighbors in the embedded space.\n",
        "submission_date": "2013-11-26T00:00:00",
        "last_modified_date": "2014-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.6709",
        "title": "A Framework for Semi-automated Web Service Composition in Semantic Web",
        "authors": [
            "Debajyoti Mukhopadhyay",
            "Archana Chougule"
        ],
        "abstract": "Number of web services available on Internet and its usage are increasing very fast. In many cases, one service is not enough to complete the business requirement; composition of web services is carried out. Autonomous composition of web services to achieve new functionality is generating considerable attention in semantic web domain. Development time and effort for new applications can be reduced with service composition. Various approaches to carry out automated composition of web services are discussed in literature. Web service composition using ontologies is one of the effective approaches. In this paper we demonstrate how the ontology based composition can be made faster for each customer. We propose a framework to provide precomposed web services to fulfil user requirements. We detail how ontology merging can be used for composition which expedites the whole process. We discuss how framework provides customer specific ontology merging and repository. We also elaborate on how merging of ontologies is carried out.\n    ",
        "submission_date": "2013-11-26T00:00:00",
        "last_modified_date": "2013-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.6907",
        "title": "A Constraint Programming Approach for Mining Sequential Patterns in a Sequence Database",
        "authors": [
            "Jean-Philippe M\u00e9tivier",
            "Samir Loudni",
            "Thierry Charnois"
        ],
        "abstract": "Constraint-based pattern discovery is at the core of numerous data mining tasks. Patterns are extracted with respect to a given set of constraints (frequency, closedness, size, etc). In the context of sequential pattern mining, a large number of devoted techniques have been developed for solving particular classes of constraints. The aim of this paper is to investigate the use of Constraint Programming (CP) to model and mine sequential patterns in a sequence database. Our CP approach offers a natural way to simultaneously combine in a same framework a large set of constraints coming from various origins. Experiments show the feasibility and the interest of our approach.\n    ",
        "submission_date": "2013-11-27T00:00:00",
        "last_modified_date": "2013-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.7071",
        "title": "Sparse Linear Dynamical System with Its Application in Multivariate Clinical Time Series",
        "authors": [
            "Zitao Liu",
            "Milos Hauskrecht"
        ],
        "abstract": "Linear Dynamical System (LDS) is an elegant mathematical framework for modeling and learning multivariate time series. However, in general, it is difficult to set the dimension of its hidden state space. A small number of hidden states may not be able to model the complexities of a time series, while a large number of hidden states can lead to overfitting. In this paper, we study methods that impose an $\\ell_1$ regularization on the transition matrix of an LDS model to alleviate the problem of choosing the optimal number of hidden states. We incorporate a generalized gradient descent method into the Maximum a Posteriori (MAP) framework and use Expectation Maximization (EM) to iteratively achieve sparsity on the transition matrix of an LDS model. We show that our Sparse Linear Dynamical System (SLDS) improves the predictive performance when compared to ordinary LDS on a multivariate clinical time series dataset.\n    ",
        "submission_date": "2013-11-27T00:00:00",
        "last_modified_date": "2013-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.7139",
        "title": "Introduction to Neutrosophic Measure, Neutrosophic Integral, and Neutrosophic Probability",
        "authors": [
            "Florentin Smarandache"
        ],
        "abstract": "In this paper, we introduce for the first time the notions of neutrosophic measure and neutrosophic integral, and we develop the 1995 notion of neutrosophic probability. We present many practical examples. It is possible to define the neutrosophic measure and consequently the neutrosophic integral and neutrosophic probability in many ways, because there are various types of indeterminacies, depending on the problem we need to solve. Neutrosophics study the indeterminacy. Indeterminacy is different from randomness. It can be caused by physical space materials and type of construction, by items involved in the space, etc.\n    ",
        "submission_date": "2013-11-27T00:00:00",
        "last_modified_date": "2013-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.7215",
        "title": "Solving Minimum Vertex Cover Problem Using Learning Automata",
        "authors": [
            "Aylin Mousavian",
            "Alireza Rezvanian",
            "Mohammad Reza Meybodi"
        ],
        "abstract": "Minimum vertex cover problem is an NP-Hard problem with the aim of finding minimum number of vertices to cover graph. In this paper, a learning automaton based algorithm is proposed to find minimum vertex cover in graph. In the proposed algorithm, each vertex of graph is equipped with a learning automaton that has two actions in the candidate or non-candidate of the corresponding vertex cover set. Due to characteristics of learning automata, this algorithm significantly reduces the number of covering vertices of graph. The proposed algorithm based on learning automata iteratively minimize the candidate vertex cover through the update its action probability. As the proposed algorithm proceeds, a candidate solution nears to optimal solution of the minimum vertex cover problem. In order to evaluate the proposed algorithm, several experiments conducted on DIMACS dataset which compared to conventional methods. Experimental results show the major superiority of the proposed algorithm over the other methods.\n    ",
        "submission_date": "2013-11-28T00:00:00",
        "last_modified_date": "2013-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.0032",
        "title": "Top-k Query Answering in Datalog+/- Ontologies under Subjective Reports (Technical Report)",
        "authors": [
            "Thomas Lukasiewicz",
            "Maria Vanina Martinez",
            "Cristian Molinaro",
            "Livia Predoiu",
            "Gerardo I. Simari"
        ],
        "abstract": "The use of preferences in query answering, both in traditional databases and in ontology-based data access, has recently received much attention, due to its many real-world applications. In this paper, we tackle the problem of top-k query answering in Datalog+/- ontologies subject to the querying user's preferences and a collection of (subjective) reports of other users. Here, each report consists of scores for a list of features, its author's preferences among the features, as well as other information. Theses pieces of information of every report are then combined, along with the querying user's preferences and his/her trust into each report, to rank the query results. We present two alternative such rankings, along with algorithms for top-k (atomic) query answering under these rankings. We also show that, under suitable assumptions, these algorithms run in polynomial time in the data complexity. We finally present more general reports, which are associated with sets of atoms rather than single atoms.\n    ",
        "submission_date": "2013-11-29T00:00:00",
        "last_modified_date": "2013-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.0127",
        "title": "Characterizing and Extending Answer Set Semantics using Possibility Theory",
        "authors": [
            "Kim Bauters",
            "Steven Schockaert",
            "Martine De Cock",
            "Dirk Vermeir"
        ],
        "abstract": "Answer Set Programming (ASP) is a popular framework for modeling combinatorial problems. However, ASP cannot easily be used for reasoning about uncertain information. Possibilistic ASP (PASP) is an extension of ASP that combines possibilistic logic and ASP. In PASP a weight is associated with each rule, where this weight is interpreted as the certainty with which the conclusion can be established when the body is known to hold. As such, it allows us to model and reason about uncertain information in an intuitive way. In this paper we present new semantics for PASP, in which rules are interpreted as constraints on possibility distributions. Special models of these constraints are then identified as possibilistic answer sets. In addition, since ASP is a special case of PASP in which all the rules are entirely certain, we obtain a new characterization of ASP in terms of constraints on possibility distributions. This allows us to uncover a new form of disjunction, called weak disjunction, that has not been previously considered in the literature. In addition to introducing and motivating the semantics of weak disjunction, we also pinpoint its computational complexity. In particular, while the complexity of most reasoning tasks coincides with standard disjunctive ASP, we find that brave reasoning for programs with weak disjunctions is easier.\n    ",
        "submission_date": "2013-11-30T00:00:00",
        "last_modified_date": "2013-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.0144",
        "title": "Knowing Whether",
        "authors": [
            "Jie Fan",
            "Yanjing Wang",
            "Hans van Ditmarsch"
        ],
        "abstract": "Knowing whether a proposition is true means knowing that it is true or knowing that it is false. In this paper, we study logics with a modal operator Kw for knowing whether but without a modal operator K for knowing that. This logic is not a normal modal logic, because we do not have Kw (phi -> psi) -> (Kw phi -> Kw psi). Knowing whether logic cannot define many common frame properties, and its expressive power less than that of basic modal logic over classes of models without reflexivity. These features make axiomatizing knowing whether logics non-trivial. We axiomatize knowing whether logic over various frame classes. We also present an extension of knowing whether logic with public announcement operators and we give corresponding reduction axioms for that. We compare our work in detail to two recent similar proposals.\n    ",
        "submission_date": "2013-11-30T00:00:00",
        "last_modified_date": "2013-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.0735",
        "title": "Use of the C4.5 machine learning algorithm to test a clinical guideline-based decision support system",
        "authors": [
            "Jean-Baptiste Lamy",
            "Anis Ellini",
            "Vahid Ebrahiminia",
            "Jean-Daniel Zucker",
            "Hector Falcoff",
            "Alain Venot"
        ],
        "abstract": "Well-designed medical decision support system (DSS) have been shown to improve health care quality. However, before they can be used in real clinical situations, these systems must be extensively tested, to ensure that they conform to the clinical guidelines (CG) on which they are based. Existing methods cannot be used for the systematic testing of all possible test cases. We describe here a new exhaustive dynamic verification method. In this method, the DSS is considered to be a black box, and the Quinlan C4.5 algorithm is used to build a decision tree from an exhaustive set of DSS input vectors and outputs. This method was successfully used for the testing of a medical DSS relating to chronic diseases: the ASTI critiquing module for type 2 diabetes.\n    ",
        "submission_date": "2013-12-03T00:00:00",
        "last_modified_date": "2013-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.0736",
        "title": "A generic system for critiquing physicians' prescriptions: usability, satisfaction and lessons learnt",
        "authors": [
            "Jean-Baptiste Lamy",
            "Vahid Ebrahiminia",
            "Brigitte Seroussi",
            "Jacques Bouaud",
            "Christian Simon",
            "Madeleine Favre",
            "Hector Falcoff",
            "Alain Venot"
        ],
        "abstract": "Clinical decision support systems have been developed to help physicians to take clinical guidelines into account during consultations. The ASTI critiquing module is one such systems; it provides the physician with automatic criticisms when a drug prescription does not follow the guidelines. It was initially developed for hypertension and type 2 diabetes, but is designed to be generic enough for application to all chronic diseases. We present here the results of usability and satisfaction evaluations for the ASTI critiquing module, obtained with GPs for a newly implemented guideline concerning dyslipaemia, and we discuss the lessons learnt and the difficulties encountered when building a generic DSS for critiquing physicians' prescriptions.\n    ",
        "submission_date": "2013-12-03T00:00:00",
        "last_modified_date": "2013-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.0750",
        "title": "A semi-automatic semantic method for mapping SNOMED CT concepts to VCM Icons",
        "authors": [
            "Jean-Baptiste Lamy",
            "Rosy Tsopra",
            "Alain Venot",
            "Catherine Duclos"
        ],
        "abstract": "VCM (Visualization of Concept in Medicine) is an iconic language for representing key medical concepts by icons. However, the use of this language with reference terminologies, such as SNOMED CT, will require the mapping of its icons to the terms of these terminologies. Here, we present and evaluate a semi-automatic semantic method for the mapping of SNOMED CT concepts to VCM icons. Both SNOMED CT and VCM are compositional in nature; SNOMED CT is expressed in description logic and VCM semantics are formalized in an OWL ontology. The proposed method involves the manual mapping of a limited number of underlying concepts from the VCM ontology, followed by automatic generation of the rest of the mapping. We applied this method to the clinical findings of the SNOMED CT CORE subset, and 100 randomly-selected mappings were evaluated by three experts. The results obtained were promising, with 82 of the SNOMED CT concepts correctly linked to VCM icons according to the experts. Most of the errors were easy to fix.\n    ",
        "submission_date": "2013-12-03T00:00:00",
        "last_modified_date": "2013-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.0790",
        "title": "Test Set Selection using Active Information Acquisition for Predictive Models",
        "authors": [
            "Sneha Chaudhari",
            "Pankaj Dayama",
            "Vinayaka Pandit",
            "Indrajit Bhattacharya"
        ],
        "abstract": "In this paper, we consider active information acquisition when the prediction model is meant to be applied on a targeted subset of the population. The goal is to label a pre-specified fraction of customers in the target or test set by iteratively querying for information from the non-target or training set. The number of queries is limited by an overall budget. Arising in the context of two rather disparate applications- banking and medical diagnosis, we pose the active information acquisition problem as a constrained optimization problem. We propose two greedy iterative algorithms for solving the above problem. We conduct experiments with synthetic data and compare results of our proposed algorithms with few other baseline approaches. The experimental results show that our proposed approaches perform better than the baseline schemes.\n    ",
        "submission_date": "2013-12-03T00:00:00",
        "last_modified_date": "2014-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.0841",
        "title": "Combining Simulated Annealing and Monte Carlo Tree Search for Expression Simplification",
        "authors": [
            "Ben Ruijl",
            "Jos Vermaseren",
            "Aske Plaat",
            "Jaap van den Herik"
        ],
        "abstract": "In many applications of computer algebra large expressions must be simplified to make repeated numerical evaluations tractable. Previous works presented heuristically guided improvements, e.g., for Horner schemes. The remaining expression is then further reduced by common subexpression elimination. A recent approach successfully applied a relatively new algorithm, Monte Carlo Tree Search (MCTS) with UCT as the selection criterion, to find better variable orderings. Yet, this approach is fit for further improvements since it is sensitive to the so-called exploration-exploitation constant $C_p$ and the number of tree updates $N$. In this paper we propose a new selection criterion called Simulated Annealing UCT (SA-UCT) that has a dynamic exploration-exploitation parameter, which decreases with the iteration number $i$ and thus reduces the importance of exploration over time. First, we provide an intuitive explanation in terms of the exploration-exploitation behavior of the algorithm. Then, we test our algorithm on three large expressions of different origins. We observe that SA-UCT widens the interval of good initial values $C_p$ where best results are achieved. The improvement is large (more than a tenfold) and facilitates the selection of an appropriate $C_p$.\n    ",
        "submission_date": "2013-12-03T00:00:00",
        "last_modified_date": "2013-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.1003",
        "title": "High Throughput Virtual Screening with Data Level Parallelism in Multi-core Processors",
        "authors": [
            "Upul Senanayake",
            "Rahal Prabuddha",
            "Roshan Ragel"
        ],
        "abstract": "Improving the throughput of molecular docking, a computationally intensive phase of the virtual screening process, is a highly sought area of research since it has a significant weight in the drug designing process. With such improvements, the world might find cures for incurable diseases like HIV disease and Cancer sooner. Our approach presented in this paper is to utilize a multi-core environment to introduce Data Level Parallelism (DLP) to the Autodock Vina software, which is a widely used for molecular docking software. Autodock Vina already exploits Instruction Level Parallelism (ILP) in multi-core environments and therefore optimized for such environments. However, with the results we have obtained, it can be clearly seen that our approach has enhanced the throughput of the already optimized software by more than six times. This will dramatically reduce the time consumed for the lead identification phase in drug designing along with the shift in the processor technology from multi-core to many-core of the current era. Therefore, we believe that the contribution of this project will effectively make it possible to expand the number of small molecules docked against a drug target and improving the chances to design drugs for incurable diseases.\n    ",
        "submission_date": "2013-12-04T00:00:00",
        "last_modified_date": "2013-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.1146",
        "title": "Case-Based Merging Techniques in OAKPLAN",
        "authors": [
            "Anna Roub\u00ed\u010dkov\u00e1",
            "Ivan Serina"
        ],
        "abstract": "Case-based planning can take advantage of former problem-solving experiences by storing in a plan library previously generated plans that can be reused to solve similar planning problems in the future. Although comparative worst-case complexity analyses of plan generation and reuse techniques reveal that it is not possible to achieve provable efficiency gain of reuse over generation, we show that the case-based planning approach can be an effective alternative to plan generation when similar reuse candidates can be chosen.\n    ",
        "submission_date": "2013-12-04T00:00:00",
        "last_modified_date": "2013-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.1887",
        "title": "Constraints on the search space of argumentation",
        "authors": [
            "Julio Lemos"
        ],
        "abstract": "Drawing from research on computational models of argumentation (particularly the Carneades Argumentation System), we explore the graphical representation of arguments in a dispute; then, comparing two different traditions on the limits of the justification of decisions, and devising an intermediate, semi-formal, model, we also show that it can shed light on the theory of dispute resolution.\n",
        "submission_date": "2013-12-06T00:00:00",
        "last_modified_date": "2013-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.1971",
        "title": "Modeling Suspicious Email Detection using Enhanced Feature Selection",
        "authors": [
            "Sarwat Nizamani",
            "Nasrullah Memon",
            "Uffe Kock Wiil",
            "Panagiotis Karampelas"
        ],
        "abstract": "The paper presents a suspicious email detection model which incorporates enhanced feature selection. In the paper we proposed the use of feature selection strategies along with classification technique for terrorists email detection. The presented model focuses on the evaluation of machine learning algorithms such as decision tree (ID3), logistic regression, Na\u00efve Bayes (NB), and Support Vector Machine (SVM) for detecting emails containing suspicious content. In the literature, various algorithms achieved good accuracy for the desired task. However, the results achieved by those algorithms can be further improved by using appropriate feature selection mechanisms. We have identified the use of a specific feature selection scheme that improves the performance of the existing algorithms.\n    ",
        "submission_date": "2013-12-06T00:00:00",
        "last_modified_date": "2013-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.2242",
        "title": "CLIC: A Framework for Distributed, On-Demand, Human-Machine Cognitive Systems",
        "authors": [
            "N. Mavridis",
            "S. Konstantopoulos",
            "I. Vetsikas",
            "I. Heldal",
            "P. Karampiperis",
            "G. Mathiason",
            "S. Thill",
            "K. Stathis",
            "V. Karkaletsis"
        ],
        "abstract": "Traditional Artificial Cognitive Systems (for example, intelligent robots) share a number of limitations. First, they are usually made up only of machine components; humans are only playing the role of user or supervisor. And yet, there are tasks in which the current state of the art of AI has much worse performance or is more expensive than humans: thus, it would be highly beneficial to have a systematic way of creating systems with both human and machine components, possibly with remote non-expert humans providing short-duration real-time services. Second, their components are often dedicated to only one system, and underutilized for a big part of their lifetime. Third, there is no inherent support for robust operation, and if a new better component becomes available, one cannot easily replace the old component. Fourth, they are viewed as a resource to be developed and owned, not as a utility. Thus, we are presenting CLIC: a framework for constructing cognitive systems that overcome the above limitations. The architecture of CLIC provides specific mechanisms for creating and operating cognitive systems that fulfill a set of desiderata: First, that are distributed yet situated, interacting with the physical world though sensing and actuation services, and that are also combining human as well as machine services. Second, that are made up of components that are time-shared and re-usable. Third, that provide increased robustness through self-repair. Fourth, that are constructed and reconstructed on the fly, with components that dynamically enter and exit the system during operation, on the basis of availability, pricing, and need. Importantly, fifth, the cognitive systems created and operated by CLIC do not need to be owned and can be provided on demand, as a utility; thus transforming human-machine situated intelligence to a service, and opening up many interesting opportunities.\n    ",
        "submission_date": "2013-12-08T00:00:00",
        "last_modified_date": "2013-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.2506",
        "title": "An Application of Answer Set Programming to the Field of Second Language Acquisition",
        "authors": [
            "Daniela Inclezan"
        ],
        "abstract": "This paper explores the contributions of Answer Set Programming (ASP) to the study of an established theory from the field of Second Language Acquisition: Input Processing. The theory describes default strategies that learners of a second language use in extracting meaning out of a text, based on their knowledge of the second language and their background knowledge about the world. We formalized this theory in ASP, and as a result we were able to determine opportunities for refining its natural language description, as well as directions for future theory development. We applied our model to automating the prediction of how learners of English would interpret sentences containing the passive voice. We present a system, PIas, that uses these predictions to assist language instructors in designing teaching materials. To appear in Theory and Practice of Logic Programming (TPLP).\n    ",
        "submission_date": "2013-12-09T00:00:00",
        "last_modified_date": "2013-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.2551",
        "title": "A state vector algebra for algorithmic implementation of second-order logic",
        "authors": [
            "Dmitry Lesnik",
            "Tobias Schaefer"
        ],
        "abstract": "We present a mathematical framework for mapping second-order logic relations onto a simple state vector algebra. Using this algebra, basic theorems of set theory can be proven in an algorithmic way, hence by an expert system. We illustrate the use of the algebra with simple examples and show that, in principle, all theorems of basic set theory can be recovered in an elementary way. The developed technique can be used for an automated theorem proving in the 1st and 2nd order logic.\n    ",
        "submission_date": "2013-12-09T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.2709",
        "title": "Phishing Detection by determining reliability factor using rough set theory",
        "authors": [
            "Anugrah Kumar",
            "Sanjiban Shekar Roy",
            "Sarvesh SS Rawat",
            "Sanklan Saxena"
        ],
        "abstract": "Phishing is a common online weapon, used against users, by Phishers for acquiring a confidential information through deception. Since the inception of internet, nearly everything, ranging from money transaction to sharing information, is done online in most parts of the world. This has also given rise to malicious activities such as Phishing. Detecting Phishing is an intricate process due to complexity, ambiguity and copious amount of possibilities of factors responsible for phishing . Rough sets can be a powerful tool, when working on such kind of Applications containing vague or imprecise data. This paper proposes an approach towards Phishing Detection Using Rough Set Theory. The Thirteen basic factors, directly responsible towards Phishing, are grouped into four Strata. Reliability Factor is determined on the basis of the outcome of these strata, using Rough Set Theory . Reliability Factor determines the possibility of a suspected site to be Valid or Fake. Using Rough set Theory most and the least influential factors towards Phishing are also determined.\n    ",
        "submission_date": "2013-12-10T00:00:00",
        "last_modified_date": "2013-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.2798",
        "title": "OntoVerbal: a Generic Tool and Practical Application to SNOMED CT",
        "authors": [
            "Shao Fen Liang",
            "Donia Scott",
            "Robert Stevens",
            "Alan Rector"
        ],
        "abstract": "Ontology development is a non-trivial task requiring expertise in the chosen ontological language. We propose a method for making the content of ontologies more transparent by presenting, through the use of natural language generation, naturalistic descriptions of ontology classes as textual paragraphs. The method has been implemented in a proof-of- concept system, OntoVerbal, that automatically generates paragraph-sized textual descriptions of ontological classes expressed in OWL. OntoVerbal has been applied to ontologies that can be loaded into Prot\u00e9g\u00e9 and been evaluated with SNOMED CT, showing that it provides coherent, well-structured and accurate textual descriptions of ontology classes.\n    ",
        "submission_date": "2013-12-10T00:00:00",
        "last_modified_date": "2013-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.3060",
        "title": "Representing Knowledge Base into Database for WAP and Web-based Expert System",
        "authors": [
            "Istiadi",
            "Emma Budi Sulistiarini"
        ],
        "abstract": "Expert System is developed as consulting service for users spread or public requires affordable access. The Internet has become a medium for such services, but presence of mobile devices make the access becomes more widespread by utilizing mobile web and WAP (Wireless Application Protocol). Applying expert systems applications over the web and WAP requires a knowledge base representation that can be accessed simultaneously. This paper proposes single database to accommodate the knowledge representation with decision tree mapping approach. Because of the database exist, consulting application through both web and WAP can access it to provide expert system services options for more affordable for public.\n    ",
        "submission_date": "2013-12-11T00:00:00",
        "last_modified_date": "2013-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.3825",
        "title": "Parkinson's Disease Motor Symptoms in Machine Learning: A Review",
        "authors": [
            "Claas Ahlrichs",
            "Michael Lawo"
        ],
        "abstract": "This paper reviews related work and state-of-the-art publications for recognizing motor symptoms of Parkinson's Disease (PD). It presents research efforts that were undertaken to inform on how well traditional machine learning algorithms can handle this task. In particular, four PD related motor symptoms are highlighted (i.e. tremor, bradykinesia, freezing of gait and dyskinesia) and their details summarized. Thus the primary objective of this research is to provide a literary foundation for development and improvement of algorithms for detecting PD related motor symptoms.\n    ",
        "submission_date": "2013-12-13T00:00:00",
        "last_modified_date": "2013-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.3903",
        "title": "A Methodology for Player Modeling based on Machine Learning",
        "authors": [
            "Marlos C. Machado"
        ],
        "abstract": "AI is gradually receiving more attention as a fundamental feature to increase the immersion in digital games. Among the several AI approaches, player modeling is becoming an important one. The main idea is to understand and model the player characteristics and behaviors in order to develop a better AI. In this work, we discuss several aspects of this new field. We proposed a taxonomy to organize the area, discussing several facets of this topic, ranging from implementation decisions up to what a model attempts to describe. We then classify, in our taxonomy, some of the most important works in this field. We also presented a generic approach to deal with player modeling using ML, and we instantiated this approach to model players' preferences in the game Civilization IV. The instantiation of this approach has several steps. We first discuss a generic representation, regardless of what is being modeled, and evaluate it performing experiments with the strategy game Civilization IV. Continuing the instantiation of the proposed approach we evaluated the applicability of using game score information to distinguish different preferences. We presented a characterization of virtual agents in the game, comparing their behavior with their stated preferences. Once we have characterized these agents, we were able to observe that different preferences generate different behaviors, measured by several game indicators. We then tackled the preference modeling problem as a binary classification task, with a supervised learning approach. We compared four different methods, based on different paradigms (SVM, AdaBoost, NaiveBayes and JRip), evaluating them on a set of matches played by different virtual agents. We conclude our work using the learned models to infer human players' preferences. Using some of the evaluated classifiers we obtained accuracies over 60% for most of the inferred preferences.\n    ",
        "submission_date": "2013-12-13T00:00:00",
        "last_modified_date": "2013-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.3971",
        "title": "Balancing bike sharing systems (BBSS): instance generation from the CitiBike NYC data",
        "authors": [
            "Tommaso Urli"
        ],
        "abstract": "Bike sharing systems are a very popular means to provide bikes to citizens in a simple and cheap way. The idea is to install bike stations at various points in the city, from which a registered user can easily loan a bike by removing it from a specialized rack. After the ride, the user may return the bike at any station (if there is a free rack). Services of this kind are mainly public or semi-public, often aimed at increasing the attractiveness of non-motorized means of transportation, and are usually free, or almost free, of charge for the users. Depending on their location, bike stations have specific patterns regarding when they are empty or full. For instance, in cities where most jobs are located near the city centre, the commuters cause certain peaks in the morning: the central bike stations are filled, while the stations in the outskirts are emptied. Furthermore, stations located on top of a hill are more likely to be empty, since users are less keen on cycling uphill to return the bike, and often leave their bike at a more reachable station. These issues result in substantial user dissatisfaction which may eventually cause the users to abandon the service. This is why nowadays most bike sharing system providers take measures to rebalance them. Over the last few years, balancing bike sharing systems (BBSS) has become increasingly studied in optimization. As such, generating meaningful instance to serve as a benchmark for the proposed approaches is an important task. In this technical report we describe the procedure we used to generate BBSS problem instances from data of the CitiBike NYC bike sharing system.\n    ",
        "submission_date": "2013-12-13T00:00:00",
        "last_modified_date": "2013-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.4026",
        "title": "Achieving Fully Proportional Representation: Approximability Results",
        "authors": [
            "Piotr Skowron",
            "Piotr Faliszewski",
            "Arkadii Slinko"
        ],
        "abstract": "We study the complexity of (approximate) winner determination under the Monroe and Chamberlin--Courant multiwinner voting rules, which determine the set of representatives by optimizing the total (dis)satisfaction of the voters with their representatives. The total (dis)satisfaction is calculated either as the sum of individual (dis)satisfactions (the utilitarian case) or as the (dis)satisfaction of the worst off voter (the egalitarian case). We provide good approximation algorithms for the satisfaction-based utilitarian versions of the Monroe and Chamberlin--Courant rules, and inapproximability results for the dissatisfaction-based utilitarian versions of them and also for all egalitarian cases. Our algorithms are applicable and particularly appealing when voters submit truncated ballots. We provide experimental evaluation of the algorithms both on real-life preference-aggregation data and on synthetic data. These experiments show that our simple and fast algorithms can in many cases find near-perfect solutions.\n    ",
        "submission_date": "2013-12-14T00:00:00",
        "last_modified_date": "2013-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.4231",
        "title": "Dependence space of matroids and its application to attribute reduction",
        "authors": [
            "Aiping Huang",
            "William Zhu"
        ],
        "abstract": "Attribute reduction is a basic issue in knowledge representation and data mining. Rough sets provide a theoretical foundation for the issue. Matroids generalized from matrices have been widely used in many fields, particularly greedy algorithm design, which plays an important role in attribute reduction. Therefore, it is meaningful to combine matroids with rough sets to solve the optimization problems. In this paper, we introduce an existing algebraic structure called dependence space to study the reduction problem in terms of matroids. First, a dependence space of matroids is constructed. Second, the characterizations for the space such as consistent sets and reducts are studied through matroids. Finally, we investigate matroids by the means of the space and present two expressions for their bases. In a word, this paper provides new approaches to study attribute reduction.\n    ",
        "submission_date": "2013-12-16T00:00:00",
        "last_modified_date": "2015-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.4232",
        "title": "Geometric lattice structure of covering and its application to attribute reduction through matroids",
        "authors": [
            "Aiping Huang",
            "William Zhu"
        ],
        "abstract": "The reduction of covering decision systems is an important problem in data mining, and covering-based rough sets serve as an efficient technique to process the problem. Geometric lattices have been widely used in many fields, especially greedy algorithm design which plays an important role in the reduction problems. Therefore, it is meaningful to combine coverings with geometric lattices to solve the optimization problems. In this paper, we obtain geometric lattices from coverings through matroids and then apply them to the issue of attribute reduction. First, a geometric lattice structure of a covering is constructed through transversal matroids. Then its atoms are studied and used to describe the lattice. Second, considering that all the closed sets of a finite matroid form a geometric lattice, we propose a dependence space through matroids and study the attribute reduction issues of the space, which realizes the application of geometric lattices to attribute reduction. Furthermore, a special type of information system is taken as an example to illustrate the application. In a word, this work points out an interesting view, namely, geometric lattice to study the attribute reduction issues of information systems.\n    ",
        "submission_date": "2013-12-16T00:00:00",
        "last_modified_date": "2014-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.4234",
        "title": "Connectedness of graphs and its application to connected matroids through covering-based rough sets",
        "authors": [
            "Aiping Huang",
            "William Zhu"
        ],
        "abstract": "Graph theoretical ideas are highly utilized by computer science fields especially data mining. In this field, a data structure can be designed in the form of tree. Covering is a widely used form of data representation in data mining and covering-based rough sets provide a systematic approach to this type of representation. In this paper, we study the connectedness of graphs through covering-based rough sets and apply it to connected matroids. First, we present an approach to inducing a covering by a graph, and then study the connectedness of the graph from the viewpoint of the covering approximation operators. Second, we construct a graph from a matroid, and find the matroid and the graph have the same connectedness, which makes us to use covering-based rough sets to study connected matroids. In summary, this paper provides a new approach to studying graph theory and matroid theory.\n    ",
        "submission_date": "2013-12-16T00:00:00",
        "last_modified_date": "2015-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.4353",
        "title": "Abstraction in decision-makers with limited information processing capabilities",
        "authors": [
            "Tim Genewein",
            "Daniel A. Braun"
        ],
        "abstract": "A distinctive property of human and animal intelligence is the ability to form abstractions by neglecting irrelevant information which allows to separate structure from noise. From an information theoretic point of view abstractions are desirable because they allow for very efficient information processing. In artificial systems abstractions are often implemented through computationally costly formations of groups or clusters. In this work we establish the relation between the free-energy framework for decision making and rate-distortion theory and demonstrate how the application of rate-distortion for decision-making leads to the emergence of abstractions. We argue that abstractions are induced due to a limit in information processing capacity.\n    ",
        "submission_date": "2013-12-16T00:00:00",
        "last_modified_date": "2013-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.4839",
        "title": "Reasoning about the Impacts of Information Sharing",
        "authors": [
            "Chatschik Bisdikian",
            "Federico Cerutti",
            "Yuqing Tang",
            "Nir Oren"
        ],
        "abstract": "In this paper we describe a decision process framework allowing an agent to decide what information it should reveal to its neighbours within a communication graph in order to maximise its utility. We assume that these neighbours can pass information onto others within the graph. The inferences made by agents receiving the messages can have a positive or negative impact on the information providing agent, and our decision process seeks to identify how a message should be modified in order to be most beneficial to the information producer. Our decision process is based on the provider's subjective beliefs about others in the system, and therefore makes extensive use of the notion of trust. Our core contributions are therefore the construction of a model of information propagation; the description of the agent's decision procedure; and an analysis of some of its properties.\n    ",
        "submission_date": "2013-11-19T00:00:00",
        "last_modified_date": "2013-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.5097",
        "title": "A Cellular Automaton Based Controller for a Ms. Pac-Man Agent",
        "authors": [
            "Alexander Darer",
            "Peter Lewis"
        ],
        "abstract": "Video games can be used as an excellent test bed for Artificial Intelligence (AI) techniques. They are challenging and non-deterministic, this makes it very difficult to write strong AI players. An example of such a video game is Ms. Pac-Man. In this paper we will outline some of the previous techniques used to build AI controllers for Ms. Pac-Man as well as presenting a new and novel solution. My technique utilises a Cellular Automaton (CA) to build a representation of the environment at each time step of the game. The basis of the representation is a 2-D grid of cells. Each cell has a state and a set of rules which determine whether or not that cell will dominate (i.e. pass its state value onto) adjacent cells at each update. Once a certain number of update iterations have been completed, the CA represents the state of the environment in the game, the goals and the dangers. At this point, Ms. Pac-Man will decide her next move based only on her adjacent cells, that is to say, she has no knowledge of the state of the environment as a whole, she will simply follow the strongest path. This technique shows promise and allows the controller to achieve high scores in a live game, the behaviour it exhibits is interesting and complex. Moreover, this behaviour is produced by using very simple rules which are applied many times to each cell in the grid. Simple local interactions with complex global results are truly achieved.\n    ",
        "submission_date": "2013-12-18T00:00:00",
        "last_modified_date": "2013-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.5162",
        "title": "Sistem pendukung keputusan kelayakan TKI ke luar negeri menggunakan FMADM",
        "authors": [
            "Ardina Ariani",
            "Leon Andretti Abdillah",
            "Firamon Syakti"
        ],
        "abstract": "BP3TKI Palembang is the government agencies that coordinate, execute and selection of prospective migrants registration and placement. To simplify the existing procedures and improve decision-making is necessary to build a decision support system (DSS) to determine eligibility for employment abroad by applying Fuzzy Multiple Attribute Decision Making (FMADM), using the linear sequential systems development methods. The system is built using Microsoft Visual Basic. Net 2010 and SQL Server 2008 database. The design of the system using use case diagrams and class diagrams to identify the needs of users and systems as well as systems implementation guidelines. This Decision Support System able to rank and produce the prospective migrants, making it easier for parties to take decision BP3TKI the workers who will be working out of the country.\n    ",
        "submission_date": "2013-12-17T00:00:00",
        "last_modified_date": "2013-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.5378",
        "title": "Skolemization for Weighted First-Order Model Counting",
        "authors": [
            "Guy Van den Broeck",
            "Wannes Meert",
            "Adnan Darwiche"
        ],
        "abstract": "First-order model counting emerged recently as a novel reasoning task, at the core of efficient algorithms for probabilistic logics. We present a Skolemization algorithm for model counting problems that eliminates existential quantifiers from a first-order logic theory without changing its weighted model count. For certain subsets of first-order logic, lifted model counters were shown to run in time polynomial in the number of objects in the domain of discourse, where propositional model counters require exponential time. However, these guarantees apply only to Skolem normal form theories (i.e., no existential quantifiers) as the presence of existential quantifiers reduces lifted model counters to propositional ones. Since textbook Skolemization is not sound for model counting, these restrictions precluded efficient model counting for directed models, such as probabilistic logic programs, which rely on existential quantification. Our Skolemization procedure extends the applicability of first-order model counters to these representations. Moreover, it simplifies the design of lifted model counting algorithms.\n    ",
        "submission_date": "2013-12-19T00:00:00",
        "last_modified_date": "2014-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.5515",
        "title": "Conservative, Proportional and Optimistic Contextual Discounting in the Belief Functions Theory",
        "authors": [
            "Marek Kurdej",
            "V\u00e9ronique Cherfaoui"
        ],
        "abstract": "Information discounting plays an important role in the theory of belief functions and, generally, in information fusion. Nevertheless, neither classical uniform discounting nor contextual cannot model certain use cases, notably temporal discounting. In this article, new contextual discounting schemes, conservative, proportional and optimistic, are proposed. Some properties of these discounting operations are examined. Classical discounting is shown to be a special case of these schemes. Two motivating cases are discussed: modelling of source reliability and application to temporal discounting.\n    ",
        "submission_date": "2013-12-19T00:00:00",
        "last_modified_date": "2013-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.5713",
        "title": "Giving the AI definition a form suitable for the engineer",
        "authors": [
            "Dimiter Dobrev"
        ],
        "abstract": "Artificial Intelligence - what is this? That is the question! In earlier papers we already gave a formal definition for AI, but if one desires to build an actual AI implementation, the following issues require attention and are treated here: the data format to be used, the idea of Undef and Nothing symbols, various ways for defining the \"meaning of life\", and finally, a new notion of \"incorrect move\". These questions are of minor importance in the theoretical discussion, but we already know the answer of the question \"Does AI exist?\" Now we want to make the next step and to create this program.\n    ",
        "submission_date": "2013-12-19T00:00:00",
        "last_modified_date": "2015-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.5714",
        "title": "Avoiding Confusion between Predictors and Inhibitors in Value Function Approximation",
        "authors": [
            "Patrick C. Connor",
            "Thomas P. Trappenberg"
        ],
        "abstract": "In reinforcement learning, the goal is to seek rewards and avoid punishments. A simple scalar captures the value of a state or of taking an action, where expected future rewards increase and punishments decrease this quantity. Naturally an agent should learn to predict this quantity to take beneficial actions, and many value function approximators exist for this purpose. In the present work, however, we show how value function approximators can cause confusion between predictors of an outcome of one valence (e.g., a signal of reward) and the inhibitor of the opposite valence (e.g., a signal canceling expectation of punishment). We show this to be a problem for both linear and non-linear value function approximators, especially when the amount of data (or experience) is limited. We propose and evaluate a simple resolution: to instead predict reward and punishment values separately, and rectify and add them to get the value needed for decision making. We evaluate several function approximators in this slightly different value function approximation architecture and show that this approach is able to circumvent the confusion and thereby achieve lower value-prediction errors.\n    ",
        "submission_date": "2013-12-19T00:00:00",
        "last_modified_date": "2015-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6096",
        "title": "Properties of Answer Set Programming with Convex Generalized Atoms",
        "authors": [
            "Mario Alviano",
            "Wolfgang Faber"
        ],
        "abstract": "In recent years, Answer Set Programming (ASP), logic programming under the stable model or answer set semantics, has seen several extensions by generalizing the notion of an atom in these programs: be it aggregate atoms, HEX atoms, generalized quantifiers, or abstract constraints, the idea is to have more complicated satisfaction patterns in the lattice of Herbrand interpretations than traditional, simple atoms. In this paper we refer to any of these constructs as generalized atoms. Several semantics with differing characteristics have been proposed for these extensions, rendering the big picture somewhat blurry. In this paper, we analyze the class of programs that have convex generalized atoms (originally proposed by Liu and Truszczynski in [10]) in rule bodies and show that for this class many of the proposed semantics coincide. This is an interesting result, since recently it has been shown that this class is the precise complexity boundary for the FLP semantics. We investigate whether similar results also hold for other semantics, and discuss the implications of our findings.\n    ",
        "submission_date": "2013-12-20T00:00:00",
        "last_modified_date": "2013-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6105",
        "title": "Hybrid Automated Reasoning Tools: from Black-box to Clear-box Integration",
        "authors": [
            "Marcello Balduccini",
            "Yulia Lierler"
        ],
        "abstract": "Recently, researchers in answer set programming and constraint programming spent significant efforts in the development of hybrid languages and solving algorithms combining the strengths of these traditionally separate fields. These efforts resulted in a new research area: constraint answer set programming (CASP). CASP languages and systems proved to be largely successful at providing efficient solutions to problems involving hybrid reasoning tasks, such as scheduling problems with elements of planning. Yet, the development of CASP systems is difficult, requiring non-trivial expertise in multiple areas. This suggests a need for a study identifying general development principles of hybrid systems. Once these principles and their implications are well understood, the development of hybrid languages and systems may become a well-established and well-understood routine process. As a step in this direction, in this paper we conduct a case study aimed at evaluating various integration schemas of CASP methods.\n    ",
        "submission_date": "2013-12-20T00:00:00",
        "last_modified_date": "2013-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6113",
        "title": "Aspartame: Solving Constraint Satisfaction Problems with Answer Set Programming",
        "authors": [
            "Mutsunori Banbara",
            "Martin Gebser",
            "Katsumi Inoue",
            "Torsten Schaub",
            "Takehide Soh",
            "Naoyuki Tamura",
            "Matthias Weise"
        ],
        "abstract": "Encoding finite linear CSPs as Boolean formulas and solving them by using modern SAT solvers has proven to be highly effective, as exemplified by the award-winning sugar system. We here develop an alternative approach based on ASP. This allows us to use first-order encodings providing us with a high degree of flexibility for easy experimentation with different implementations. The resulting system aspartame re-uses parts of sugar for parsing and normalizing CSPs. The obtained set of facts is then combined with an ASP encoding that can be grounded and solved by off-the-shelf ASP systems. We establish the competitiveness of our approach by empirically contrasting aspartame and sugar.\n    ",
        "submission_date": "2013-12-20T00:00:00",
        "last_modified_date": "2013-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6130",
        "title": "A Functional View of Strong Negation in Answer Set Programming",
        "authors": [
            "Michael Bartholomew",
            "Joohyung Lee"
        ],
        "abstract": "The distinction between strong negation and default negation has been useful in answer set programming. We present an alternative account of strong negation, which lets us view strong negation in terms of the functional stable model semantics by Bartholomew and Lee. More specifically, we show that, under complete interpretations, minimizing both positive and negative literals in the traditional answer set semantics is essentially the same as ensuring the uniqueness of Boolean function values under the functional stable model semantics. The same account lets us view Lifschitz's two-valued logic programs as a special case of the functional stable model semantics. In addition, we show how non-Boolean intensional functions can be eliminated in favor of Boolean intensional functions, and furthermore can be represented using strong negation, which provides a way to compute the functional stable model semantics using existing ASP solvers. We also note that similar results hold with the functional stable model semantics by Cabalar.\n    ",
        "submission_date": "2013-12-20T00:00:00",
        "last_modified_date": "2013-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6134",
        "title": "An Algebra of Causal Chains",
        "authors": [
            "Pedro Cabalar",
            "Jorge Fandinno"
        ],
        "abstract": "In this work we propose a multi-valued extension of logic programs under the stable models semantics where each true atom in a model is associated with a set of justifications, in a similar spirit than a set of proof trees. The main contribution of this paper is that we capture justifications into an algebra of truth values with three internal operations: an addition '+' representing alternative justifications for a formula, a commutative product '*' representing joint interaction of causes and a non-commutative product '.' acting as a concatenation or proof constructor. Using this multi-valued semantics, we obtain a one-to-one correspondence between the syntactic proof tree of a standard (non-causal) logic program and the interpretation of each true atom in a model. Furthermore, thanks to this algebraic characterization we can detect semantic properties like redundancy and relevance of the obtained justifications. We also identify a lattice-based characterization of this algebra, defining a direct consequences operator, proving its continuity and that its least fix point can be computed after a finite number of iterations. Finally, we define the concept of causal stable model by introducing an analogous transformation to Gelfond and Lifschitz's program reduct.\n    ",
        "submission_date": "2013-12-20T00:00:00",
        "last_modified_date": "2013-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6138",
        "title": "Query Answering in Object Oriented Knowledge Bases in Logic Programming: Description and Challenge for ASP",
        "authors": [
            "Vinay K. Chaudhri",
            "Stijn Heymans",
            "Michael Wessel",
            "Tran Cao Son"
        ],
        "abstract": "Research on developing efficient and scalable ASP solvers can substantially benefit by the availability of data sets to experiment with. KB_Bio_101 contains knowledge from a biology textbook, has been developed as part of Project Halo, and has recently become available for research use. KB_Bio_101 is one of the largest KBs available in ASP and the reasoning with it is undecidable in general. We give a description of this KB and ASP programs for a suite of queries that have been of practical interest. We explain why these queries pose significant practical challenges for the current ASP solvers.\n    ",
        "submission_date": "2013-12-20T00:00:00",
        "last_modified_date": "2013-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6140",
        "title": "The DIAMOND System for Argumentation: Preliminary Report",
        "authors": [
            "Stefan Ellmauthaler",
            "Hannes Strass"
        ],
        "abstract": "Abstract dialectical frameworks (ADFs) are a powerful generalisation of Dung's abstract argumentation frameworks. In this paper we present an answer set programming based software system, called DIAMOND (DIAlectical MOdels eNcoDing). It translates ADFs into answer set programs whose stable models correspond to models of the ADF with respect to several semantics (i.e. admissible, complete, stable, grounded).\n    ",
        "submission_date": "2013-12-20T00:00:00",
        "last_modified_date": "2013-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6143",
        "title": "A System for Interactive Query Answering with Answer Set Programming",
        "authors": [
            "Martin Gebser",
            "Philipp Obermeier",
            "Torsten Schaub"
        ],
        "abstract": "Reactive answer set programming has paved the way for incorporating online information into operative solving processes. Although this technology was originally devised for dealing with data streams in dynamic environments, like assisted living and cognitive robotics, it can likewise be used to incorporate facts, rules, or queries provided by a user. As a result, we present the design and implementation of a system for interactive query answering with reactive answer set programming. Our system quontroller is based on the reactive solver oclingo and implemented as a dedicated front-end. We describe its functionality and implementation, and we illustrate its features by some selected use cases.\n    ",
        "submission_date": "2013-12-20T00:00:00",
        "last_modified_date": "2013-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6146",
        "title": "Generating Shortest Synchronizing Sequences using Answer Set Programming",
        "authors": [
            "Canan G\u00fcni\u00e7en",
            "Esra Erdem",
            "H\u00fcsn\u00fc Yenig\u00fcn"
        ],
        "abstract": "For a finite state automaton, a synchronizing sequence is an input sequence that takes all the states to the same state. Checking the existence of a synchronizing sequence and finding a synchronizing sequence, if one exists, can be performed in polynomial time. However, the problem of finding a shortest synchronizing sequence is known to be NP-hard. In this work, the usefulness of Answer Set Programming to solve this optimization problem is investigated, in comparison with brute-force algorithms and SAT-based approaches.\n",
        "submission_date": "2013-12-20T00:00:00",
        "last_modified_date": "2013-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6149",
        "title": "On the Semantics of Gringo",
        "authors": [
            "Amelia Harrison",
            "Vladimir Lifschitz",
            "Fangkai Yang"
        ],
        "abstract": "Input languages of answer set solvers are based on the mathematically simple concept of a stable model. But many useful constructs available in these languages, including local variables, conditional literals, and aggregates, cannot be easily explained in terms of stable models in the sense of the original definition of this concept and its straightforward generalizations. Manuals written by designers of answer set solvers usually explain such constructs using examples and informal comments that appeal to the user's intuition, without references to any precise semantics. We propose to approach the problem of defining the semantics of gringo programs by translating them into the language of infinitary propositional formulas. This semantics allows us to study equivalent transformations of gringo programs using natural deduction in infinitary propositional logic.\n    ",
        "submission_date": "2013-12-20T00:00:00",
        "last_modified_date": "2013-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6151",
        "title": "Abstract Modular Systems and Solvers",
        "authors": [
            "Yuliya Lierler",
            "Miroslaw Truszczynski"
        ],
        "abstract": "Integrating diverse formalisms into modular knowledge representation systems offers increased expressivity, modeling convenience and computational benefits. We introduce concepts of abstract modules and abstract modular systems to study general principles behind the design and analysis of model-finding programs, or solvers, for integrated heterogeneous multi-logic systems. We show how abstract modules and abstract modular systems give rise to transition systems, which are a natural and convenient representation of solvers pioneered by the SAT community. We illustrate our approach by showing how it applies to answer set programming and propositional logic, and to multi-logic systems based on these two formalisms.\n    ",
        "submission_date": "2013-12-20T00:00:00",
        "last_modified_date": "2013-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6156",
        "title": "Negation in the Head of CP-logic Rules",
        "authors": [
            "Joost Vennekens"
        ],
        "abstract": "CP-logic is a probabilistic extension of the logic FO(ID). Unlike ASP, both of these logics adhere to a Tarskian informal semantics, in which interpretations represent objective states-of-affairs. In other words, these logics lack the epistemic component of ASP, in which interpretations represent the beliefs or knowledge of a rational agent. Consequently, neither CP-logic nor FO(ID) have the need for two kinds of negations: there is only one negation, and its meaning is that of objective falsehood. Nevertheless, the formal semantics of this objective negation is mathematically more similar to ASP's negation-as-failure than to its classical negation. The reason is that both CP-logic and FO(ID) have a constructive semantics in which all atoms start out as false, and may only become true as the result of a rule application. This paper investigates the possibility of adding the well-known ASP feature of allowing negation in the head of rules to CP-logic. Because CP-logic only has one kind of negation, it is of necessity this ''negation-as-failure like'' negation that will be allowed in the head. We investigate the intuitive meaning of such a construct and the benefits that arise from it.\n    ",
        "submission_date": "2013-12-20T00:00:00",
        "last_modified_date": "2013-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6558",
        "title": "Predictive User Modeling with Actionable Attributes",
        "authors": [
            "Indre Zliobaite",
            "Mykola Pechenizkiy"
        ],
        "abstract": "Different machine learning techniques have been proposed and used for modeling individual and group user needs, interests and preferences. In the traditional predictive modeling instances are described by observable variables, called attributes. The goal is to learn a model for predicting the target variable for unseen instances. For example, for marketing purposes a company consider profiling a new user based on her observed web browsing behavior, referral keywords or other relevant information. In many real world applications the values of some attributes are not only observable, but can be actively decided by a decision maker. Furthermore, in some of such applications the decision maker is interested not only to generate accurate predictions, but to maximize the probability of the desired outcome. For example, a direct marketing manager can choose which type of a special offer to send to a client (actionable attribute), hoping that the right choice will result in a positive response with a higher probability. We study how to learn to choose the value of an actionable attribute in order to maximize the probability of a desired outcome in predictive modeling. We emphasize that not all instances are equally sensitive to changes in actions. Accurate choice of an action is critical for those instances, which are on the borderline (e.g. users who do not have a strong opinion one way or the other). We formulate three supervised learning approaches for learning to select the value of an actionable attribute at an instance level. We also introduce a focused training procedure which puts more emphasis on the situations where varying the action is the most likely to take the effect. The proof of concept experimental validation on two real-world case studies in web analytics and e-learning domains highlights the potential of the proposed approaches.\n    ",
        "submission_date": "2013-12-23T00:00:00",
        "last_modified_date": "2013-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6726",
        "title": "Bounded Rational Decision-Making in Changing Environments",
        "authors": [
            "Jordi Grau-Moya",
            "Daniel A. Braun"
        ],
        "abstract": "A perfectly rational decision-maker chooses the best action with the highest utility gain from a set of possible actions. The optimality principles that describe such decision processes do not take into account the computational costs of finding the optimal action. Bounded rational decision-making addresses this problem by specifically trading off information-processing costs and expected utility. Interestingly, a similar trade-off between energy and entropy arises when describing changes in thermodynamic systems. This similarity has been recently used to describe bounded rational agents. Crucially, this framework assumes that the environment does not change while the decision-maker is computing the optimal policy. When this requirement is not fulfilled, the decision-maker will suffer inefficiencies in utility, that arise because the current policy is optimal for an environment in the past. Here we borrow concepts from non-equilibrium thermodynamics to quantify these inefficiencies and illustrate with simulations its relationship with computational resources.\n    ",
        "submission_date": "2013-12-24T00:00:00",
        "last_modified_date": "2013-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6764",
        "title": "Bounded Recursive Self-Improvement",
        "authors": [
            "E. Nivel",
            "K. R. Th\u00f3risson",
            "B. R. Steunebrink",
            "H. Dindo",
            "G. Pezzulo",
            "M. Rodriguez",
            "C. Hernandez",
            "D. Ognibene",
            "J. Schmidhuber",
            "R. Sanz",
            "H. P. Helgason",
            "A. Chella",
            "G. K. Jonsson"
        ],
        "abstract": "We have designed a machine that becomes increasingly better at behaving in underspecified circumstances, in a goal-directed way, on the job, by modeling itself and its environment as experience accumulates. Based on principles of autocatalysis, endogeny, and reflectivity, the work provides an architectural blueprint for constructing systems with high levels of operational autonomy in underspecified circumstances, starting from a small seed. Through value-driven dynamic priority scheduling controlling the parallel execution of a vast number of reasoning threads, the system achieves recursive self-improvement after it leaves the lab, within the boundaries imposed by its designers. A prototype system has been implemented and demonstrated to learn a complex real-world task, real-time multimodal dialogue with humans, by on-line observation. Our work presents solutions to several challenges that must be solved for achieving artificial general intelligence.\n    ",
        "submission_date": "2013-12-24T00:00:00",
        "last_modified_date": "2013-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6832",
        "title": "The Value Iteration Algorithm is Not Strongly Polynomial for Discounted Dynamic Programming",
        "authors": [
            "Eugene A. Feinberg",
            "Jefferson Huang"
        ],
        "abstract": "This note provides a simple example demonstrating that, if exact computations are allowed, the number of iterations required for the value iteration algorithm to find an optimal policy for discounted dynamic programming problems may grow arbitrarily quickly with the size of the problem. In particular, the number of iterations can be exponential in the number of actions. Thus, unlike policy iterations, the value iteration algorithm is not strongly polynomial for discounted dynamic programming.\n    ",
        "submission_date": "2013-12-19T00:00:00",
        "last_modified_date": "2013-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6996",
        "title": "A New Approach to Constraint Weight Learning for Variable Ordering in CSPs",
        "authors": [
            "Muhammad Rezaul Karim"
        ],
        "abstract": "A Constraint Satisfaction Problem (CSP) is a framework used for modeling and solving constrained problems. Tree-search algorithms like backtracking try to construct a solution to a CSP by selecting the variables of the problem one after another. The order in which these algorithm select the variables potentially have significant impact on the search performance. Various heuristics have been proposed for choosing good variable ordering. Many powerful variable ordering heuristics weigh the constraints first and then utilize the weights for selecting good order of the variables. Constraint weighting are basically employed to identify global bottlenecks in a CSP.\n",
        "submission_date": "2013-12-25T00:00:00",
        "last_modified_date": "2013-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.7326",
        "title": "Replica Exchange using q-Gaussian Swarm Quantum Particle Intelligence Method",
        "authors": [
            "Hiqmet Kamberaj"
        ],
        "abstract": "We present a newly developed Replica Exchange algorithm using q -Gaussian Swarm Quantum Particle Optimization (REX@q-GSQPO) method for solving the problem of finding the global optimum. The basis of the algorithm is to run multiple copies of independent swarms at different values of q parameter. Based on an energy criterion, chosen to satisfy the detailed balance, we are swapping the particle coordinates of neighboring swarms at regular iteration intervals. The swarm replicas with high q values are characterized by high diversity of particles allowing escaping local minima faster, while the low q replicas, characterized by low diversity of particles, are used to sample more efficiently the local basins. We compare the new algorithm with the standard Gaussian Swarm Quantum Particle Optimization (GSQPO) and q-Gaussian Swarm Quantum Particle Optimization (q-GSQPO) algorithms, and we found that the new algorithm is more robust in terms of the number of fitness function calls, and more efficient in terms ability convergence to the global minimum. In additional, we also provide a method of optimally allocating the swarm replicas among different q values. Our algorithm is tested for three benchmark functions, which are known to be multimodal problems, at different dimensionalities. In addition, we considered a polyalanine peptide of 12 residues modeled using a G\u014d coarse-graining potential energy function.\n    ",
        "submission_date": "2013-11-17T00:00:00",
        "last_modified_date": "2013-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.7422",
        "title": "Proceedings of Answer Set Programming and Other Computing Paradigms (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey",
        "authors": [
            "Michael Fink",
            "Yuliya Lierler"
        ],
        "abstract": "This volume contains the papers presented at the sixth workshop on Answer Set Programming and Other Computing Paradigms (ASPOCP 2013) held on August 25th, 2013 in Istanbul, co-located with the 29th International Conference on Logic Programming (ICLP 2013). It thus continues a series of previous events co-located with ICLP, aiming at facilitating the discussion about crossing the boundaries of current ASP techniques in theory, solving, and applications, in combination with or inspired by other computing paradigms.\n    ",
        "submission_date": "2013-12-28T00:00:00",
        "last_modified_date": "2013-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.7485",
        "title": "A General Algorithm for Deciding Transportability of Experimental Results",
        "authors": [
            "Elias Bareinboim",
            "Judea Pearl"
        ],
        "abstract": "Generalizing empirical findings to new environments, settings, or populations is essential in most scientific explorations. This article treats a particular problem of generalizability, called \"transportability\", defined as a license to transfer information learned in experimental studies to a different population, on which only observational studies can be conducted. Given a set of assumptions concerning commonalities and differences between the two populations, Pearl and Bareinboim (2011) derived sufficient conditions that permit such transfer to take place. This article summarizes their findings and supplements them with an effective procedure for deciding when and how transportability is feasible. It establishes a necessary and sufficient condition for deciding when causal effects in the target population are estimable from both the statistical information available and the causal information transferred from the experiments. The article further provides a complete algorithm for computing the transport formula, that is, a way of combining observational and experimental information to synthesize bias-free estimate of the desired causal relation. Finally, the article examines the differences between transportability and other variants of generalizability.\n    ",
        "submission_date": "2013-12-29T00:00:00",
        "last_modified_date": "2013-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.7740",
        "title": "Assessment of Customer Credit through Combined Clustering of Artificial Neural Networks, Genetics Algorithm and Bayesian Probabilities",
        "authors": [
            "Reza Mortezapour",
            "Mehdi Afzali"
        ],
        "abstract": "Today, with respect to the increasing growth of demand to get credit from the customers of banks and finance and credit institutions, using an effective and efficient method to decrease the risk of non-repayment of credit given is very necessary. Assessment of customers' credit is one of the most important and the most essential duties of banks and institutions, and if an error occurs in this field, it would leads to the great losses for banks and institutions. Thus, using the predicting computer systems has been significantly progressed in recent decades. The data that are provided to the credit institutions' managers help them to make a straight decision for giving the credit or not-giving it. In this paper, we will assess the customer credit through a combined classification using artificial neural networks, genetics algorithm and Bayesian probabilities simultaneously, and the results obtained from three methods mentioned above would be used to achieve an appropriate and final result. We use the K_folds cross validation test in order to assess the method and finally, we compare the proposed method with the methods such as Clustering-Launched Classification (CLC), Support Vector Machine (SVM) as well as GA+SVM where the genetics algorithm has been used to improve them.\n    ",
        "submission_date": "2013-12-30T00:00:00",
        "last_modified_date": "2013-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0563",
        "title": "Interpolating Conditional Density Trees",
        "authors": [
            "Scott Davies",
            "Andrew Moore"
        ],
        "abstract": "Joint distributions over many variables are frequently modeled by decomposing them into products of simpler, lower-dimensional conditional distributions, such as in sparsely connected Bayesian networks.  However, automatically learning such models can be very computationally expensive when there are many datapoints and many continuous variables with complex nonlinear relationships, particularly when no good ways of decomposing the joint distribution are known a priori.  In such situations, previous research has generally focused on the use of discretization techniques in which each continuous variable has a single discretization that is used throughout the entire network. \\ In this paper, we present and compare a wide variety of tree-based algorithms for learning and evaluating conditional density estimates over continuous variables.  These trees can be thought of as discretizations that vary according to the particular interactions being modeled; however, the density within a given leaf of the tree need not be assumed constant, and we show that such nonuniform leaf densities lead to more accurate density estimation.  We have developed Bayesian network structure-learning algorithms that employ these tree-based conditional density representations, and we show that they can be used to practically learn complex joint probability models over dozens of continuous variables from thousands of datapoints.  We focus on finding models that are simultaneously accurate, fast to learn, and fast to evaluate once they are learned. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0567",
        "title": "The Thing That We Tried Didn't Work Very Well : Deictic Representation in Reinforcement Learning",
        "authors": [
            "Sarah Finney",
            "Natalia Gardiol",
            "Leslie Pack Kaelbling",
            "Tim Oates"
        ],
        "abstract": "Most reinforcement learning methods operate on propositional representations of the world state.  Such representations are often intractably large and generalize poorly.  Using a deictic representation is believed to be a viable alternative: they promise generalization while allowing the use of existing reinforcement-learning methods.  Yet, there are few experiments on learning with deictic representations reported in the literature.  In this paper we explore the effectiveness of two forms of deictic representation and a na\u00efve propositional representation in a simple blocks-world domain.  We find, empirically, that the deictic representations actually worsen learning performance. We conclude with a discussion of possible causes of these results and strategies for more effective learning in domains with objects. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0573",
        "title": "Coordinates: Probabilistic Forecasting of Presence and Availability",
        "authors": [
            "Eric J. Horvitz",
            "Paul Koch",
            "Carl Kadie",
            "Andy Jacobs"
        ],
        "abstract": "We present methods employed in Coordinate, a prototype service that supports collaboration and communication by learning predictive models that provide forecasts of users      s AND     ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0575",
        "title": "CFW: A Collaborative Filtering System Using Posteriors Over Weights Of Evidence",
        "authors": [
            "Carl Kadie",
            "Christopher Meek",
            "David Heckerman"
        ],
        "abstract": "We describe CFW, a computationally efficient algorithm for collaborative filtering that uses posteriors over weights of evidence. In experiments on real data, we show that this method predicts as well or better than other methods in situations where the size of the user query is small. The new approach works particularly well when the user s query CONTAINS low frequency(unpopular) ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0577",
        "title": "Efficient Nash Computation in Large Population Games with Bounded Influence",
        "authors": [
            "Michael Kearns",
            "Yishay Mansour"
        ],
        "abstract": "We introduce a general representation of large-population games in which each player      s influence ON the others IS centralized AND limited,     but may otherwise be ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0600",
        "title": "An MDP-based Recommender System",
        "authors": [
            "Guy Shani",
            "Ronen I. Brafman",
            "David Heckerman"
        ],
        "abstract": "Typical Recommender systems adopt a static view of the recommendation process and treat it as a prediction problem. We argue that it is more appropriate to view the problem of generating recommendations as a sequential decision problem and, consequently, that Markov decision processes (MDP) provide a more appropriate model for Recommender systems. MDPs introduce two benefits: they take into account the long-term effects of each recommendation, and they take into account the expected value of each recommendation. To succeed in practice, an MDP-based Recommender system must employ a strong initial model; and the bulk of this paper is concerned with the generation of such a model. In particular, we suggest the use of an n-gram predictive model for generating the initial MDP. Our n-gram model induces a Markov-chain model of user behavior whose predictive accuracy is greater than that of existing predictive models. We describe our predictive model in detail and evaluate its performance on real data. In addition, we show how the model can be used in an MDP-based Recommender system.\n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0604",
        "title": "Discriminative Probabilistic Models for Relational Data",
        "authors": [
            "Ben Taskar",
            "Pieter Abbeel",
            "Daphne Koller"
        ],
        "abstract": "In many supervised learning tasks, the entities to be labeled are related to each other in complex ways and their labels are not independent.  For example, in hypertext classification, the labels of linked pages are highly correlated.  A standard approach is to classify each entity independently, ignoring the correlations between them.  Recently, Probabilistic Relational Models, a relational version of Bayesian networks, were used to define a joint probabilistic model for a collection of related entities.  In this paper, we present an alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach.  First, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models.  Second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy. We show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities. We provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies.  \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0607",
        "title": "Particle Filters in Robotics (Invited Talk)",
        "authors": [
            "Sebastian Thrun"
        ],
        "abstract": "This presentation will introduce the audience to a new, emerging body of research on sequential Monte Carlo techniques in robotics.  In recent years, particle filters have solved several hard perceptual robotic problems. Early successes were limited to low-dimensional problems, such as the problem of robot localization in environments with known maps. More recently, researchers have begun exploiting structural properties of robotic domains that have led to successful particle filter applications in spaces with as many as 100,000 dimensions. The presentation will discuss specific tricks      necessary to make these techniques work in real - world domains,and also discuss open challenges for researchers IN the UAI community.\n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0613",
        "title": "IPF for Discrete Chain Factor Graphs",
        "authors": [
            "Wim Wiegerinck",
            "Tom Heskes"
        ],
        "abstract": "Iterative Proportional Fitting (IPF), combined with EM, is commonly used as an algorithm for likelihood maximization in undirected graphical models.  In this paper, we present two iterative algorithms that generalize upon IPF.  The first one is for likelihood maximization in discrete chain factor graphs, which we define as a wide class of discrete variable models including undirected graphical models and Bayesian networks, but also chain graphs and sigmoid belief networks. The second one is for conditional likelihood maximization in standard undirected models and Bayesian networks.  In both algorithms, the iteration steps are expressed in closed form. Numerical simulations show that the algorithms are competitive with state of the art methods. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0701",
        "title": "Similarity Assessment through blocking and affordance assignment in Textual CBR",
        "authors": [
            "R.Rajendra Prasath",
            "Pinar \u00d6zt\u00fcrk"
        ],
        "abstract": "It has been conceived that children learn new objects through their affordances, that is, the actions that can be taken on them. We suggest that web pages also have affordances defined in terms of the users' information need they meet. An assumption of the proposed approach is that different parts of a text may not be equally important / relevant to a given query. Judgment on the relevance of a web document requires, therefore, a thorough look into its parts, rather than treating it as a monolithic content. We propose a method to extract and assign affordances to texts and then use these affordances to retrieve the corresponding web pages. The overall approach presented in the paper relies on case-based representations that bridge the queries to the affordances of web documents. We tested our method on the tourism domain and the results are promising.\n    ",
        "submission_date": "2013-01-04T00:00:00",
        "last_modified_date": "2013-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0932",
        "title": "Knowledge Sharing: A Model",
        "authors": [
            "Sufianto Mahfudz",
            "Mahyuddin K. M. Nasution",
            "Sawaluddin Nasution"
        ],
        "abstract": "We know anything because we learn about it, there is anything we ever share about it, but now a lot of media that can represent how it happened as infrastructure of the knowledge sharing. This paper aims to introduce a model for understanding a problem in knowledge sharing based on interaction.\n    ",
        "submission_date": "2013-01-05T00:00:00",
        "last_modified_date": "2013-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0958",
        "title": "Probabilistic entailment in the setting of coherence: The role of quasi conjunction and inclusion relation",
        "authors": [
            "Angelo Gilio",
            "Giuseppe Sanfilippo"
        ],
        "abstract": "In this paper, by adopting a coherence-based probabilistic approach to default reasoning, we focus the study on the logical operation of quasi conjunction and the Goodman-Nguyen inclusion relation for conditional events. We recall that quasi conjunction is a basic notion for defining consistency of conditional knowledge bases. By deepening some results given in a previous paper we show that, given any finite family of conditional events F and any nonempty subset S of F, the family F p-entails the quasi conjunction C(S); then, given any conditional event E|H, we analyze the equivalence between p-entailment of E|H from F and p-entailment of E|H from C(S), where S is some nonempty subset of F. We also illustrate some alternative theorems related with p-consistency and p-entailment. Finally, we deepen the study of the connections between the notions of p-entailment and inclusion relation by introducing for a pair (F,E|H) the (possibly empty) class K of the subsets S of F such that C(S) implies E|H. We show that the class K satisfies many properties; in particular K is additive and has a greatest element which can be determined by applying a suitable algorithm.\n    ",
        "submission_date": "2013-01-06T00:00:00",
        "last_modified_date": "2013-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.1299",
        "title": "Automated Variational Inference in Probabilistic Programming",
        "authors": [
            "David Wingate",
            "Theophane Weber"
        ],
        "abstract": "We present a new algorithm for approximate inference in probabilistic programs, based on a stochastic gradient for variational programs. This method is efficient without restrictions on the probabilistic program; it is particularly practical for distributions which are not analytically tractable, including highly structured distributions that arise in probabilistic programs. We show how to automatically derive mean-field probabilistic programs and optimize them, and demonstrate that our perspective improves inference efficiency over other algorithms.\n    ",
        "submission_date": "2013-01-07T00:00:00",
        "last_modified_date": "2013-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.1332",
        "title": "A Logic Programming Approach to Integration Network Inference",
        "authors": [
            "Daniel Ritter"
        ],
        "abstract": "The discovery, representation and reconstruction of (technical) integration networks from Network Mining (NM) raw data is a difficult problem for enterprises. This is due to large and complex IT landscapes within and across enterprise boundaries, heterogeneous technology stacks, and fragmented data. To remain competitive, visibility into the enterprise and partner IT networks on different, interrelated abstraction levels is desirable.\n",
        "submission_date": "2013-01-07T00:00:00",
        "last_modified_date": "2013-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.1386",
        "title": "SPARC - Sorted ASP with Consistency Restoring Rules",
        "authors": [
            "Evgenii Balai",
            "Michael Gelfond",
            "Yuanlin Zhang"
        ],
        "abstract": "This is a preliminary report on the work aimed at making CR-Prolog -- a version of ASP with consistency restoring rules -- more suitable for use in teaching and large applications. First we describe a sorted version of CR-Prolog called SPARC. Second, we translate a basic version of the CR-Prolog into the language of DLV and compare the performance with the state of the art CR-Prolog solver. The results form the foundation for future more efficient and user friendly implementation of SPARC and shed some light on the relationship between two useful knowledge representation constructs: consistency restoring rules and weak constraints of DLV.\n    ",
        "submission_date": "2013-01-08T00:00:00",
        "last_modified_date": "2013-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.1390",
        "title": "Eliminating Unfounded Set Checking for HEX-Programs",
        "authors": [
            "Thomas Eiter",
            "Michael Fink",
            "Thomas Krennwallner",
            "Christoph Redl",
            "Peter Sch\u00fcller"
        ],
        "abstract": "HEX-programs are an extension of the Answer Set Programming (ASP) paradigm incorporating external means of computation into the declarative programming language through so-called external atoms. Their semantics is defined in terms of minimal models of the Faber-Leone-Pfeifer (FLP) reduct. Developing native solvers for HEX-programs based on an appropriate notion of unfounded sets has been subject to recent research for reasons of efficiency. Although this has lead to an improvement over naive minimality checking using the FLP reduct, testing for foundedness remains a computationally expensive task. In this work we improve on HEX-program evaluation in this respect by identifying a syntactic class of programs, that can be efficiently recognized and allows to entirely skip the foundedness check. Moreover, we develop criteria for decomposing a program into components, such that the search for unfounded sets can be restricted. Observing that our results apply to many HEX-program applications provides analytic evidence for the significance and effectiveness of our approach, which is complemented by a brief discussion of preliminary experimental validation.\n    ",
        "submission_date": "2013-01-08T00:00:00",
        "last_modified_date": "2013-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.1391",
        "title": "Backdoors to Normality for Disjunctive Logic Programs",
        "authors": [
            "Johannes Klaus Fichte",
            "Stefan Szeider"
        ],
        "abstract": "Over the last two decades, propositional satisfiability (SAT) has become one of the most successful and widely applied techniques for the solution of NP-complete problems. The aim of this paper is to investigate theoretically how Sat can be utilized for the efficient solution of problems that are harder than NP or co-NP. In particular, we consider the fundamental reasoning problems in propositional disjunctive answer set programming (ASP), Brave Reasoning and Skeptical Reasoning, which ask whether a given atom is contained in at least one or in all answer sets, respectively. Both problems are located at the second level of the Polynomial Hierarchy and thus assumed to be harder than NP or co-NP. One cannot transform these two reasoning problems into SAT in polynomial time, unless the Polynomial Hierarchy collapses. We show that certain structural aspects of disjunctive logic programs can be utilized to break through this complexity barrier, using new techniques from Parameterized Complexity. In particular, we exhibit transformations from Brave and Skeptical Reasoning to SAT that run in time O(2^k n^2) where k is a structural parameter of the instance and n the input size. In other words, the reduction is fixed-parameter tractable for parameter k. As the parameter k we take the size of a smallest backdoor with respect to the class of normal (i.e., disjunction-free) programs. Such a backdoor is a set of atoms that when deleted makes the program normal. In consequence, the combinatorial explosion, which is expected when transforming a problem from the second level of the Polynomial Hierarchy to the first level, can now be confined to the parameter k, while the running time of the reduction is polynomial in the input size n, where the order of the polynomial is independent of k.\n    ",
        "submission_date": "2013-01-08T00:00:00",
        "last_modified_date": "2013-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.1393",
        "title": "Two New Definitions of Stable Models of Logic Programs with Generalized Quantifiers",
        "authors": [
            "Joohyung Lee",
            "Yunsong Meng"
        ],
        "abstract": "We present alternative definitions of the first-order stable model semantics and its extension to incorporate generalized quantifiers by referring to the familiar notion of a reduct instead of referring to the SM operator in the original definitions. Also, we extend the FLP stable model semantics to allow generalized quantifiers by referring to an operator that is similar to the $\\sm$ operator. For a reasonable syntactic class of logic programs, we show that the two stable model semantics of generalized quantifiers are interchangeable.\n    ",
        "submission_date": "2013-01-08T00:00:00",
        "last_modified_date": "2013-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.1394",
        "title": "Lloyd-Topor Completion and General Stable Models",
        "authors": [
            "Vladimir Lifschitz",
            "Fangkai Yang"
        ],
        "abstract": "We investigate the relationship between the generalization of program completion defined in 1984 by Lloyd and Topor and the generalization of the stable model semantics introduced recently by Ferraris et al. The main theorem can be used to characterize, in some cases, the general stable models of a logic program by a first-order formula. The proof uses Truszczynski's stable model semantics of infinitary propositional formulas.\n    ",
        "submission_date": "2013-01-08T00:00:00",
        "last_modified_date": "2013-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.1395",
        "title": "Extending FO(ID) with Knowledge Producing Definitions: Preliminary Results",
        "authors": [
            "Joost Vennekens",
            "Marc Denecker"
        ],
        "abstract": "Previous research into the relation between ASP and classical logic has identified at least two different ways in which the former extends the latter. First, ASP program typically contain sets of rules that can be naturally interpreted as inductive definitions, and the language FO(ID) has shown that such inductive definitions can elegantly be added to classical logic in a modular way. Second, there is of course also the well-known epistemic component of ASP, which was mainly emphasized in the early papers on stable model semantics. To investigate whether this kind of knowledge can also, and in a similarly modular way, be added to classical logic, the language of Ordered Epistemic Logic was presented in recent work. However, this logic views the epistemic component as entirely separate from the inductive definition component, thus ignoring any possible interplay between the two. In this paper, we present a language that extends the inductive definition construct found in FO(ID) with an epistemic component, making such interplay possible. The eventual goal of this work is to discover whether it is really appropriate to view the epistemic component and the inductive definition component of ASP as two separate extensions of classical logic, or whether there is also something of importance in the combination of the two.\n    ",
        "submission_date": "2013-01-08T00:00:00",
        "last_modified_date": "2013-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.1753",
        "title": "FCA - An Approach On LEACH Protocol Of Wireless Sensor Networks Using Fuzzy Logic",
        "authors": [
            "Vaibhav Godbole"
        ],
        "abstract": "In order to gather information more efficiently, wireless sensor networks are partitioned into clusters. The most of the proposed clustering algorithms do not consider the location of the base station. This situation causes hot spots problem in multi-hop wireless sensor networks. In this paper, we propose a fuzzy clustering algorithm (FCA) which aims to prolong the lifetime of wireless sensor networks. FCA adjusts the cluster-head radius considering the residual energy and the distance to the base station parameters of the sensor nodes. This helps decreasing the intra-cluster work of the sensor nodes which are closer to the base station or have lower battery level. We utilize fuzzy logic for handling the uncertainties in cluster-head radius estimation. We compare our algorithm with LEACH according to first node dies, half of the nodes alive and energy-efficiency metrics. Our simulation results show that FCA performs better than other algorithms in most of the cases. Therefore, our proposed algorithm is a stable and energy-efficient clustering algorithm.\n    ",
        "submission_date": "2013-01-09T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.1932",
        "title": "An Approach for Classification of Dysfluent and Fluent Speech Using K-NN And SVM",
        "authors": [
            "P. Mahesha",
            "D. S. Vinod"
        ],
        "abstract": "This paper presents a new approach for classification of dysfluent and fluent speech using Mel-Frequency Cepstral Coefficient (MFCC). The speech is fluent when person's speech flows easily and smoothly. Sounds combine into syllable, syllables mix together into words and words link into sentences with little effort. When someone's speech is dysfluent, it is irregular and does not flow effortlessly. Therefore, a dysfluency is a break in the smooth, meaningful flow of speech. Stuttering is one such disorder in which the fluent flow of speech is disrupted by occurrences of dysfluencies such as repetitions, prolongations, interjections and so on. In this work we have considered three types of dysfluencies such as repetition, prolongation and interjection to characterize dysfluent speech. After obtaining dysfluent and fluent speech, the speech signals are analyzed in order to extract MFCC features. The k-Nearest Neighbor (k-NN) and Support Vector Machine (SVM) classifiers are used to classify the speech as dysfluent and fluent speech. The 80% of the data is used for training and 20% for testing. The average accuracy of 86.67% and 93.34% is obtained for dysfluent and fluent speech respectively.\n    ",
        "submission_date": "2013-01-09T00:00:00",
        "last_modified_date": "2013-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.1950",
        "title": "Syntactic Analysis Based on Morphological Characteristic Features of the Romanian Language",
        "authors": [
            "Bogdan Patrut"
        ],
        "abstract": "This paper refers to the syntactic analysis of phrases in Romanian, as an important process of natural language processing. We will suggest a real-time solution, based on the idea of using some words or groups of words that indicate grammatical category; and some specific endings of some parts of sentence. Our idea is based on some characteristics of the Romanian language, where some prepositions, adverbs or some specific endings can provide a lot of information about the structure of a complex sentence. Such characteristics can be found in other languages, too, such as French. Using a special grammar, we developed a system (DIASEXP) that can perform a dialogue in natural language with assertive and interogative sentences about a \"story\" (a set of sentences describing some events from the real life).\n    ",
        "submission_date": "2013-01-09T00:00:00",
        "last_modified_date": "2013-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2253",
        "title": "Efficient Approximation for Triangulation of Minimum Treewidth",
        "authors": [
            "Eyal Amir"
        ],
        "abstract": "We present four novel approximation algorithms for finding  triangulation of minimum treewidth. Two of the algorithms improve on the  running times of algorithms by Robertson and Seymour, and Becker and   Geiger that approximate the optimum by factors  of 4 and 3 2/3, respectively.  A third algorithm is faster than  those but gives an approximation factor of 4 1/2. The last  algorithm is yet faster, producing factor-O(lg/k) approximations  in polynomial time.  Finding triangulations of minimum treewidth for graphs is central to  many problems in computer science.  Real-world problems in  artificial intelligence, VLSI design and databases are efficiently  solvable if we have an efficient approximation algorithm for them.  We report on experimental results confirming the effectiveness of  our algorithms for large graphs associated with real-world problems.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2261",
        "title": "Semi-Instrumental Variables: A Test for Instrument Admissibility",
        "authors": [
            "Tianjiao Chu",
            "Richard Scheines",
            "Peter L. Spirtes"
        ],
        "abstract": "In a causal graphical model, an instrument for a variable X and its effect Y is a random variable that is a cause of X and independent of all the causes of Y except X. (Pearl (1995), Spirtes et al (2000)). Instrumental variables can be used to estimate how the distribution of an effect will respond to a manipulation of its causes, even in the presence of unmeasured common causes (confounders). In typical instrumental variable estimation, instruments are chosen based on domain knowledge. There is currently no statistical test for validating a variable as an instrument. In this paper, we introduce the concept of semi-instrument, which generalizes the concept of instrument. We show that in the framework of additive models, under certain conditions, we can test whether a variable is semi-instrumental. Moreover, adding some distribution assumptions, we can test whether two semi-instruments are instrumental. We give algorithms to estimate the p-value that a random variable is semi-instrumental, and the p-value that two semi-instruments are both instrumental. These algorithms can be used to test the experts' choice of instruments, or to identify instruments automatically. \n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2269",
        "title": "Learning the Dimensionality of Hidden Variables",
        "authors": [
            "Gal Elidan",
            "Nir Friedman"
        ],
        "abstract": "A serious problem in learning probabilistic models is the presence of hidden variables. These variables are not observed, yet interact with several of the observed variables. Detecting hidden variables poses two problems: determining the relations to other variables in the model and determining the number of states of the hidden variable. In this paper, we address the latter problem in the context of Bayesian networks. We describe an approach that utilizes a score-based agglomerative state-clustering. As we show, this approach allows us to efficiently evaluate models with a range of cardinalities for the hidden variable. We show how to extend this procedure to deal with multiple interacting hidden variables. We demonstrate the effectiveness of this approach by evaluating it on synthetic and real-life data. We show that our approach learns models with hidden variables that generalize better and have better structure than previous approaches. \n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2270",
        "title": "Multivariate Information Bottleneck",
        "authors": [
            "Nir Friedman",
            "Ori Mosenzon",
            "Noam Slonim",
            "Naftali Tishby"
        ],
        "abstract": "The Information bottleneck method is an unsupervised non-parametric data organization technique. Given a joint distribution P(A,B), this method constructs a new variable T that extracts partitions, or clusters, over the values of A that are informative about B. The information bottleneck has already been applied to document classification, gene expression, neural code, and spectral analysis. In this paper, we introduce a general principled framework for multivariate extensions of the information bottleneck method. This allows us to consider multiple systems of data partitions that are inter-related. Our approach utilizes Bayesian networks for specifying the systems of clusters and what information each captures. We show that this construction provides insight about bottleneck variations and enables us to characterize solutions of these variations. We also present a general framework for iterative algorithms for constructing solutions, and apply it to several examples. \n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2280",
        "title": "Estimating Well-Performing Bayesian Networks using Bernoulli Mixtures",
        "authors": [
            "Geoff A. Jarrad"
        ],
        "abstract": "A novel method for estimating Bayesian network (BN) parameters from data is presented which provides improved performance on test data. Previous research has shown the value of representing conditional probability distributions (CPDs) via neural networks(Neal 1992), noisy-OR gates (Neal 1992, Diez 1993)and decision trees (Friedman and Goldszmidt 1996).The Bernoulli mixture network (BMN) explicitly represents the CPDs of discrete BN nodes as mixtures of local distributions,each having a different set of ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2281",
        "title": "Graphical Models for Game Theory",
        "authors": [
            "Michael Kearns",
            "Michael L. Littman",
            "Satinder Singh"
        ],
        "abstract": "In this work, we introduce graphical modelsfor multi-player game theory, and give powerful algorithms for computing their Nash equilibria in certain cases. An n-player game is given by an undirected graph on n nodes and a set of n local matrices. The interpretation is that the payoff to player i is determined entirely by the actions of player i and his neighbors in the graph, and thus the payoff matrix to player i is indexed only by these players. We thus view the global n-player game as being composed of interacting local games, each involving many fewer players. Each player's action may have global impact, but it occurs through the propagation of local ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2015-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2283",
        "title": "Improved learning of Bayesian networks",
        "authors": [
            "Tomas Kocka",
            "Robert Castelo"
        ],
        "abstract": "The search space of Bayesian Network structures is usually defined as Acyclic Directed Graphs (DAGs) and the search is done by local transformations of DAGs. But the space of Bayesian Networks is ordered by DAG Markov model inclusion and it is natural to consider that a good search policy should take this into account. First attempt to do this (Chickering 1996) was using equivalence classes of DAGs instead of DAGs itself. This approach produces better results but it is significantly slower. We present a compromise between these two approaches. It uses DAGs to search the space in such a way that the ordering by inclusion is taken into account. This is achieved by repetitive usage of local moves within the equivalence class of DAGs. We show that this new approach produces better results than the original DAGs approach without substantial change in time complexity. We present empirical results, within the framework of heuristic search and Markov Chain Monte Carlo, provided through the Alarm dataset.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2311",
        "title": "Maximum Likelihood Bounded Tree-Width Markov Networks",
        "authors": [
            "Nathan Srebro"
        ],
        "abstract": "Chow and Liu (1968) studied the problem of learning a maximumlikelihood Markov tree. We generalize their work to more complexMarkov networks by considering the problem of learning a maximumlikelihood Markov network of bounded complexity. We discuss howtree-width is in many ways the appropriate measure of complexity andthus analyze the problem of learning a maximum likelihood Markovnetwork of bounded ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2315",
        "title": "The Optimal Reward Baseline for Gradient-Based Reinforcement Learning",
        "authors": [
            "Lex Weaver",
            "Nigel Tao"
        ],
        "abstract": "There exist a number of reinforcement learning algorithms which learnby climbing the gradient of expected reward.  Their long-runconvergence has been proved, even in partially observableenvironments with non-deterministic actions, and without the need fora system model.  However, the variance of the gradient estimator hasbeen found to be a significant practical problem.  Recent approacheshave discounted future rewards, introducing a bias-variance trade-offinto the gradient estimate.  We incorporate a reward baseline into thelearning system, and show that it affects variance without introducingfurther bias.  In particular, as we approach the zero-bias,high-variance parameterization, the optimal (or variance minimizing)constant reward baseline is equal to the long-term average expectedreward.  Modified policy-gradient algorithms are presented, and anumber of experiments demonstrate their improvement over previous work.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2318",
        "title": "Statistical Modeling in Continuous Speech Recognition (CSR)(Invited Talk)",
        "authors": [
            "Steve Young"
        ],
        "abstract": "Automatic continuous speech recognition (CSR) is sufficiently mature that a variety of real world applications are now possible including large vocabulary transcription and interactive spoken dialogues. This paper reviews the evolution of the statistical modelling techniques which underlie current-day systems, specifically hidden Markov models (HMMs) and N-grams. Starting from a description of the speech signal and its parameterisation, the various modelling assumptions and their consequences are discussed. It then describes various techniques by which the effects of these assumptions can be mitigated. Despite the progress that has been made, the limitations of current modelling techniques are still evident. The paper therefore concludes with a brief review of some of the more fundamental modelling work now in progress.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2320",
        "title": "Using Temporal Data for Making Recommendations",
        "authors": [
            "Andrew Zimdars",
            "David Maxwell Chickering",
            "Christopher Meek"
        ],
        "abstract": "We treat collaborative filtering as a univariate time series estimation problem:  given a user's previous votes, predict the next vote.  We describe two families of methods for transforming data to encode time order in ways amenable to off-the-shelf classification and density estimation tools, and examine the results of using these approaches on several real-world data sets.  The improvements in predictive accuracy we realize recommend the use of other predictive algorithms that exploit the temporal order of data.\n    ",
        "submission_date": "2013-01-10T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2678",
        "title": "Verification of Agent-Based Artifact Systems",
        "authors": [
            "Francesco Belardinelli",
            "Alessio Lomuscio",
            "Fabio Patrizi"
        ],
        "abstract": "Artifact systems are a novel paradigm for specifying and implementing business processes described in terms of interacting modules called artifacts. Artifacts consist of data and lifecycles, accounting respectively for the relational structure of the artifacts' states and their possible evolutions over time. In this paper we put forward artifact-centric multi-agent systems, a novel formalisation of artifact systems in the context of multi-agent systems operating on them. Differently from the usual process-based models of services, the semantics we give explicitly accounts for the data structures on which artifact systems are defined. We study the model checking problem for artifact-centric multi-agent systems against specifications written in a quantified version of temporal-epistemic logic expressing the knowledge of the agents in the exchange. We begin by noting that the problem is undecidable in general. We then identify two noteworthy restrictions, one syntactical and one semantical, that enable us to find bisimilar finite abstractions and therefore reduce the model checking problem to the instance on finite models. Under these assumptions we show that the model checking problem for these systems is EXPSPACE-complete. We then introduce artifact-centric programs, compact and declarative representations of the programs governing both the artifact system and the agents. We show that, while these in principle generate infinite-state systems, under natural conditions their verification problem can be solved on finite abstractions that can be effectively computed from the programs. Finally we exemplify the theoretical results of the paper through a mainstream procurement scenario from the artifact systems literature.\n    ",
        "submission_date": "2013-01-12T00:00:00",
        "last_modified_date": "2013-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2811",
        "title": "Cutting Recursive Autoencoder Trees",
        "authors": [
            "Christian Scheible",
            "Hinrich Schuetze"
        ],
        "abstract": "Deep Learning models enjoy considerable success in Natural Language Processing. While deep architectures produce useful representations that lead to improvements in various tasks, they are often difficult to interpret. This makes the analysis of learned structures particularly difficult. In this paper, we rely on empirical tests to see whether a particular structure makes sense. We present an analysis of the Semi-Supervised Recursive Autoencoder, a well-known model that produces structural representations of text. We show that for certain tasks, the structure of the autoencoder can be significantly reduced without loss of classification accuracy and we evaluate the produced structures using human judgment.\n    ",
        "submission_date": "2013-01-13T00:00:00",
        "last_modified_date": "2013-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3535",
        "title": "Airport Gate Scheduling for Passengers, Aircraft, and Operation",
        "authors": [
            "Sang Hyun Kim",
            "Eric Feron",
            "John-Paul Clarke",
            "Aude Marzuoli",
            "Daniel Delahaye"
        ],
        "abstract": "Passengers' experience is becoming a key metric to evaluate the air transportation system's performance. Efficient and robust tools to handle airport operations are needed along with a better understanding of passengers' interests and concerns. Among various airport operations, this paper studies airport gate scheduling for improved passengers' experience. Three objectives accounting for passengers, aircraft, and operation are presented. Trade-offs between these objectives are analyzed, and a balancing objective function is proposed. The results show that the balanced objective can improve the efficiency of traffic flow in passenger terminals and on ramps, as well as the robustness of gate operations.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3764",
        "title": "Adaptive learning rates and parallelization for stochastic, sparse, non-smooth gradients",
        "authors": [
            "Tom Schaul",
            "Yann LeCun"
        ],
        "abstract": "Recent work has established an empirically successful framework for adapting learning rates for stochastic gradient descent (SGD). This effectively removes all needs for tuning, while automatically reducing learning rates over time on stationary problems, and permitting learning rates to grow appropriately in non-stationary tasks. Here, we extend the idea in three directions, addressing proper minibatch parallelization, including reweighted updates for sparse or orthogonal gradients, improving robustness on non-smooth loss functions, in the process replacing the diagonal Hessian estimation procedure that may not always be available by a robust finite-difference approximation. The final algorithm integrates all these components, has linear complexity and is hyper-parameter free.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3837",
        "title": "Dynamic Bayesian Multinets",
        "authors": [
            "Jeff A. Bilmes"
        ],
        "abstract": "In this work, dynamic Bayesian multinets are introduced where a Markov chain state at time t determines conditional independence patterns between random variables lying within a local time window surrounding t.  It is shown how information-theoretic criterion functions can be used to induce sparse, discriminative, and class-conditional network structures that yield an optimal approximation to the class posterior probability, and therefore are useful for the classification task.  Using a new structure learning heuristic, the resulting models are tested on a medium-vocabulary isolated-word speech recognition task.  It is demonstrated that these discriminatively structured dynamic Bayesian multinets, when trained in a maximum likelihood setting using EM, can outperform both HMMs and other dynamic Bayesian networks with a similar number of parameters.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3852",
        "title": "Mix-nets: Factored Mixtures of Gaussians in Bayesian Networks With Mixed Continuous And Discrete Variables",
        "authors": [
            "Scott Davies",
            "Andrew Moore"
        ],
        "abstract": "Recently developed techniques have made it possible to quickly learn accurate probability density functions from data in low-dimensional continuous space. In particular, mixtures of Gaussians can be fitted to data very quickly using an accelerated EM algorithm that employs multiresolution kd-trees (Moore, 1999). In this paper, we propose a kind of Bayesian networks in which low-dimensional mixtures of Gaussians over different subsets of the domain's variables are combined into a coherent joint probability model over the entire domain.  The network is also capable of modeling complex dependencies between discrete variables and continuous variables without requiring discretization of the continuous variables.  We present efficient heuristic algorithms for automatically learning these networks from data, and perform comparative experiments illustrated how well these networks model real scientific data and synthetic data. We also briefly discuss some possible improvements to the networks, as well as possible applications.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3853",
        "title": "Rao-Blackwellised Particle Filtering for Dynamic Bayesian Networks",
        "authors": [
            "Arnaud Doucet",
            "Nando de Freitas",
            "Kevin Murphy",
            "Stuart Russell"
        ],
        "abstract": "Particle filters (PFs) are powerful sampling-based inference/learning algorithms for dynamic Bayesian networks (DBNs). They allow us to treat, in a principled way, any type of probability distribution, nonlinearity and non-stationarity. They have appeared in several fields under such names as \"condensation\", \"sequential Monte Carlo\" and \"survival of the fittest\". In this paper, we show how we can exploit the structure of the DBN to increase the efficiency of particle filtering, using a technique known as Rao-Blackwellisation. Essentially, this samples some of the variables, and marginalizes out the rest exactly, using the Kalman filter, HMM filter, junction tree algorithm, or any other finite dimensional optimal filter. We show that Rao-Blackwellised particle filters (RBPFs) lead to more accurate estimates than standard PFs. We demonstrate RBPFs on two problems, namely non-stationary online regression with radial basis function networks and robot localization and map building. We also discuss other potential application areas and provide references to some finite dimensional optimal filters.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3856",
        "title": "Being Bayesian about Network Structure",
        "authors": [
            "Nir Friedman",
            "Daphne Koller"
        ],
        "abstract": "In many domains, we are interested in analyzing the structure of the underlying distribution, e.g., whether one variable is a direct parent of the other.  Bayesian model-selection attempts to find the MAP model and use its structure to answer these questions.  However, when the amount of available data is modest, there might be many models that have non-negligible posterior.  Thus, we want compute the Bayesian posterior of a feature, i.e., the total posterior probability of all models that contain it.  In this paper, we propose a new approach for this task.  We first show how to efficiently compute a sum over the exponential number of networks that are consistent with a fixed ordering over network variables. This allows us to compute, for a given ordering, both the marginal probability of the data and the posterior of a feature.  We then use this result as the basis for an algorithm that approximates the Bayesian posterior of a feature.  Our approach uses a Markov Chain Monte Carlo (MCMC) method, but over orderings rather than over network structures.  The space of orderings is much smaller and more regular than the space of structures, and has a smoother posterior `landscape'.  We present empirical results on synthetic and real-life datasets that compare our approach to full model averaging (when possible), to MCMC over network structures, and to a non-Bayesian bootstrap approach. \n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3867",
        "title": "Fast Planning in Stochastic Games",
        "authors": [
            "Michael Kearns",
            "Yishay Mansour",
            "Satinder Singh"
        ],
        "abstract": "Stochastic games generalize Markov decision processes (MDPs) to a multiagent setting by allowing the state transitions to depend jointly on all player actions, and having rewards determined by multiplayer matrix games at each state. We consider the problem of computing Nash equilibria in stochastic games, the analogue of planning in MDPs. We begin by providing a generalization of finite-horizon value iteration that computes a Nash strategy for each player in generalsum stochastic games. The algorithm takes an arbitrary Nash selection function as input, which allows the translation of local choices between multiple Nash equilibria into the selection of a single global Nash equilibrium.\n",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3870",
        "title": "Game Networks",
        "authors": [
            "Pierfrancesco La Mura"
        ],
        "abstract": "We introduce Game networks (G nets), a novel representation for multi-agent decision problems. Compared to other game-theoretic representations, such as strategic or extensive forms, G nets are more structured and more compact; more fundamentally, G nets constitute a computationally advantageous framework for strategic inference, as both probability and utility independencies are captured in the structure of the network and can be exploited in order to simplify the inference process. An important aspect of multi-agent reasoning is the identification of some or all of the strategic equilibria in a game; we present original convergence methods for strategic equilibrium which can take advantage of strategic separabilities in the G net structure in order to simplify the computations. Specifically, we describe a method which identifies a unique equilibrium as a function of the game payoffs, and one which identifies all equilibria.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3875",
        "title": "Tractable Bayesian Learning of Tree Belief Networks",
        "authors": [
            "Marina Meila",
            "Tommi S. Jaakkola"
        ],
        "abstract": "In this paper we present decomposable priors, a family of priors over structure and parameters of tree belief nets for which Bayesian learning with complete observations is tractable, in the sense that the posterior is also decomposable and can be completely determined analytically in polynomial time.  This follows from two main results: First, we show that factored distributions over spanning trees in a graph can be integrated in closed form. Second, we examine priors over tree parameters and show that a set of assumptions similar to (Heckerman and al. 1995) constrain the tree parameter priors to be a compactly parameterized product of Dirichlet distributions. Beside allowing for exact Bayesian learning, these results permit us to formulate a new class of tractable latent variable models in which the likelihood of a data point is computed through an ensemble average over tree structures.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3895",
        "title": "Dynamic Trees: A Structured Variational Method Giving Efficient Propagation Rules",
        "authors": [
            "Amos J. Storkey"
        ],
        "abstract": "Dynamic trees are mixtures of tree structured belief networks. They solve some of the problems of fixed tree networks at the cost of making exact inference intractable. For this reason approximate methods such as sampling or mean field approaches have been used. However, mean field approximations assume a factorized distribution over node states. Such a distribution seems unlickely in the posterior, as nodes are highly correlated in the prior. Here a structured variational approach is used, where the posterior distribution over the non-evidential nodes is itself approximated by a dynamic tree. It turns out that this form can be used tractably and efficiently. The result is a set of update rules which can propagate information through the network to obtain both a full variational approximation, and the relevant marginals. The progagtion rules are more efficient than the mean field approach and give noticeable quantitative and qualitative improvement in the inference. The marginals calculated give better approximations to the posterior than loopy propagation on a small toy problem.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3899",
        "title": "Model-Based Hierarchical Clustering",
        "authors": [
            "Shivakumar Vaithyanathan",
            "Byron E Dom"
        ],
        "abstract": "We present an approach to model-based hierarchical clustering by formulating an objective function based on a Bayesian analysis. This model organizes the data into a cluster hierarchy while specifying a complex feature-set partitioning that is a key component of our model. Features can have either a unique distribution in every cluster or a common distribution over some (or even all) of the clusters. The cluster subsets over which these features have such a common distribution correspond to the nodes (clusters) of the tree representing the hierarchy. We apply this general model to the problem of document clustering for which we use a multinomial likelihood function and Dirichlet priors.  Our algorithm consists of a two-stage process wherein we first perform a flat clustering followed by a modified hierarchical agglomerative merging process that includes determining the features that will have common distributions over the merged clusters. The regularization induced by using the marginal likelihood automatically determines the optimal model structure including number of clusters, the depth of the tree and the subset of features to be modeled as having a common distribution at each node. We present experimental results on both synthetic data and a real document collection. \n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3901",
        "title": "Variational Approximations between Mean Field Theory and the Junction Tree Algorithm",
        "authors": [
            "Wim Wiegerinck"
        ],
        "abstract": "Recently, variational approximations such as the mean field approximation have received much interest.  We extend the standard mean field method by using an approximating distribution that factorises into cluster potentials.  This includes undirected graphs, directed acyclic graphs and junction trees. We derive generalized mean field equations to optimize the cluster potentials. We show that the method bridges the gap between the standard mean field approximation and the exact junction tree algorithm. In addition, we address the problem of how to choose the graphical structure of the approximating distribution.  From the generalised mean field equations we derive rules to simplify the structure of the approximating distribution in advance without affecting the quality of the approximation. We also show how the method fits into some other variational approximations that are currently popular.  \n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.4351",
        "title": "Applying machine learning techniques to improve user acceptance on ubiquitous environement",
        "authors": [
            "Djallel Bouneffouf"
        ],
        "abstract": "Ubiquitous information access becomes more and more important nowadays and research is aimed at making it adapted to users. Our work consists in applying machine learning techniques in order to adapt the information access provided by ubiquitous systems to users when the system only knows the user social group, without knowing anything about the user interest. The adaptation procedures associate actions to perceived situations of the user. Associations are based on feedback given by the user as a reaction to the behavior of the system. Our method brings a solution to some of the problems concerning the acceptance of the system by users when applying machine learning techniques to systems at the beginning of the interaction between the system and the user.\n    ",
        "submission_date": "2013-01-18T00:00:00",
        "last_modified_date": "2013-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.4753",
        "title": "Pattern Matching for Self- Tuning of MapReduce Jobs",
        "authors": [
            "Nikzad Babaii Rizvandi",
            "Javid Taheri",
            "Albert Y.Zomaya"
        ],
        "abstract": "In this paper, we study CPU utilization time patterns of several MapReduce applications. After extracting running patterns of several applications, they are saved in a reference database to be later used to tweak system parameters to efficiently execute unknown applications in future. To achieve this goal, CPU utilization patterns of new applications are compared with the already known ones in the reference database to find/predict their most probable execution patterns. Because of different patterns lengths, the Dynamic Time Warping (DTW) is utilized for such comparison; a correlation analysis is then applied to DTWs outcomes to produce feasible similarity patterns. Three real applications (WordCount, Exim Mainlog parsing and Terasort) are used to evaluate our hypothesis in tweaking system parameters in executing similar applications. Results were very promising and showed effectiveness of our approach on pseudo-distributed MapReduce platforms.\n    ",
        "submission_date": "2013-01-21T00:00:00",
        "last_modified_date": "2013-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.4780",
        "title": "From Quantitative Spatial Operator to Qualitative Spatial Relation Using Constructive Solid Geometry, Logic Rules and Optimized 9-IM Model, A Semantic Based Approach",
        "authors": [
            "Helmi Ben Hmida",
            "Christophe Cruz",
            "Frank Boochs",
            "Christophe Nicolle"
        ],
        "abstract": "The Constructive Solid Geometry (CSG) is a data model providing a set of binary Boolean operators such as Union, Difference and Intersection. In this work, these operators are used to compute topological relations between objects defined by the constraints of the nine Intersection Model (9-IM) from Egenhofer. With the help of these constraints, we define a procedure to compute the topological relations on CSG objects. These topological relations are Disjoint, Contains, Inside, Covers, CoveredBy, Equals and Overlaps, and are defined in a top-level ontology with a specific semantic definition on relation such as Transitive, Symmetric, Asymmetric, Functional, Reflexive, and Irreflexive. The results of topological relations computation are stored in the ontology allowing after what to infer on these topological relationships. In addition, logic rules based on the Semantic Web Language allows the definition of logic programs that define which topological relationships have to be computed on which kind of objects. For instance, a \"Building\" that overlaps a \"Railway\" is a \"RailStation\".\n    ",
        "submission_date": "2013-01-21T00:00:00",
        "last_modified_date": "2013-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.4783",
        "title": "From 3D Point Clouds To Semantic Objects An Ontology-Based Detection Approach",
        "authors": [
            "Helmi Ben Hmida",
            "Christophe Cruz",
            "Frank Boochs",
            "Christophe Nicolle"
        ],
        "abstract": "This paper presents a knowledge-based detection of objects approach using the OWL ontology language, the Semantic Web Rule Language, and 3D processing built-ins aiming at combining geometrical analysis of 3D point clouds and specialist's knowledge. This combination allows the detection and the annotation of objects contained in point clouds. The context of the study is the detection of railway objects such as signals, technical cupboards, electric poles, etc. Thus, the resulting enriched and populated ontology, that contains the annotations of objects in the point clouds, is used to feed a GIS systems or an IFC file for architecture purposes.\n    ",
        "submission_date": "2013-01-21T00:00:00",
        "last_modified_date": "2013-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.4848",
        "title": "Integration of knowledge to support automatic object reconstruction from images and 3D data",
        "authors": [
            "Frank Boochs",
            "Andreas Marbs",
            "Hung Truong",
            "Helmi Ben Hmida",
            "Ashish Karmacharya",
            "Christophe Cruz",
            "Adlane Habed",
            "Yvon Voisin",
            "Christophe Nicolle"
        ],
        "abstract": "Object reconstruction is an important task in many fields of application as it allows to generate digital representations of our physical world used as base for analysis, planning, construction, visualization or other aims. A reconstruction itself normally is based on reliable data (images, 3D point clouds for example) expressing the object in his complete extent. This data then has to be compiled and analyzed in order to extract all necessary geometrical elements, which represent the object and form a digital copy of it. Traditional strategies are largely based on manual interaction and interpretation, because with increasing complexity of objects human understanding is inevitable to achieve acceptable and reliable results. But human interaction is time consuming and expensive, why many researches has already been invested to use algorithmic support, what allows to speed up the process and to reduce manual work load. Presently most of such supporting algorithms are data-driven and concentate on specific features of the objects, being accessible to numerical models. By means of these models, which normally will represent geometrical (flatness, roughness, for example) or physical features (color, texture), the data is classified and analyzed. This is successful for objects with low complexity, but gets to its limits with increasing complexness of objects. Then purely numerical strategies are not able to sufficiently model the reality. Therefore, the intention of our approach is to take human cognitive strategy as an example, and to simulate extraction processes based on available human defined knowledge for the objects of interest. Such processes will introduce a semantic structure for the objects and guide the algorithms used to detect and recognize objects, which will yield a higher effectiveness. Hence, our research proposes an approach using knowledge to guide the algorithms in 3D point cloud and image processing.\n    ",
        "submission_date": "2013-01-21T00:00:00",
        "last_modified_date": "2013-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.4862",
        "title": "Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots",
        "authors": [
            "Adrien Baranes",
            "Pierre-Yves Oudeyer"
        ],
        "abstract": "We introduce the Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC) architecture as an intrinsi- cally motivated goal exploration mechanism which allows active learning of inverse models in high-dimensional redundant robots. This allows a robot to efficiently and actively learn distributions of parameterized motor skills/policies that solve a corresponding distribution of parameterized tasks/goals. The architecture makes the robot sample actively novel parameterized tasks in the task space, based on a measure of competence progress, each of which triggers low-level goal-directed learning of the motor policy pa- rameters that allow to solve it. For both learning and generalization, the system leverages regression techniques which allow to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters. We present experiments with high-dimensional continuous sensorimotor spaces in three different robotic setups: 1) learning the inverse kinematics in a highly-redundant robotic arm, 2) learning omnidirectional locomotion with motor primitives in a quadruped robot, 3) an arm learning to control a fishing rod with a flexible wire. We show that 1) exploration in the task space can be a lot faster than exploration in the actuator space for learning inverse models in redundant robots; 2) selecting goals maximizing competence progress creates developmental trajectories driving the robot to progressively focus on tasks of increasing complexity and is statistically significantly more efficient than selecting tasks randomly, as well as more efficient than different standard active motor babbling methods; 3) this architecture allows the robot to actively discover which parts of its task space it can learn to reach and which part it cannot.\n    ",
        "submission_date": "2013-01-21T00:00:00",
        "last_modified_date": "2013-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.4910",
        "title": "Computational Aspects of the Calculus of Structure",
        "authors": [
            "M\u00e1rio S. Alvim"
        ],
        "abstract": "Logic is the science of correct inferences and a logical system is a tool to prove assertions in a certain logic in a correct way. There are many logical systems, and many ways of formalizing them, e.g., using natural deduction or sequent calculus. Calculus of structures (CoS) is a new formalism proposed by Alessio Guglielmi in 2004 that generalizes sequent calculus in the sense that inference rules can be applied at any depth inside a formula, rather than only to the main connective. With this feature, proofs in CoS are shorter than in any other formalism supporting analytical proofs. Although it is great to have the freedom and expressiveness of CoS, under the point of view of proof search more freedom means a larger search space. And that should be restricted when looking for complete automation of deductive systems. Some efforts were made to reduce this non-determinism, but they are all basically operational approaches, and no solid theoretical result regarding the computational behaviour of CoS has been achieved so far. The main focus of this thesis is to discuss ways to propose a proof search strategy for CoS suitable to implementation. This strategy should be theoretical instead of purely operational. We introduce the concept of incoherence number of substructures inside structures and we use this concept to achieve our main result: there is an algorithm that, according to our conjecture, corresponds to a proof search strategy to every provable structure in the subsystem of FBV (the multiplicative linear logic MLL plus the rule mix) containing only pairwise distinct atoms. Our algorithm is implemented and we believe our strategy is a good starting point to exploit the computational aspects of CoS in more general systems, like BV itself.\n    ",
        "submission_date": "2013-01-21T00:00:00",
        "last_modified_date": "2013-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.5022",
        "title": "A formalization of re-identification in terms of compatible probabilities",
        "authors": [
            "Vicen\u00e7 Torra",
            "Klara Stokes"
        ],
        "abstract": "Re-identification algorithms are used in data privacy to measure disclosure risk. They model the situation in which an adversary attacks a published database by means of linking the information of this adversary with the database.\n",
        "submission_date": "2013-01-21T00:00:00",
        "last_modified_date": "2013-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.5154",
        "title": "A Rational and Efficient Algorithm for View Revision in Databases",
        "authors": [
            "Radhakrishnan Delhibabu",
            "Gerhard Lakemeyer"
        ],
        "abstract": "The dynamics of belief and knowledge is one of the major components of any autonomous system that should be able to incorporate new pieces of information. In this paper, we argue that to apply rationality result of belief dynamics theory to various practical problems, it should be generalized in two respects: first of all, it should allow a certain part of belief to be declared as immutable; and second, the belief state need not be deductively closed. Such a generalization of belief dynamics, referred to as base dynamics, is presented, along with the concept of a generalized revision algorithm for Horn knowledge bases. We show that Horn knowledge base dynamics has interesting connection with kernel change and abduction. Finally, we also show that both variants are rational in the sense that they satisfy certain rationality postulates stemming from philosophical works on belief dynamics.\n    ",
        "submission_date": "2013-01-22T00:00:00",
        "last_modified_date": "2013-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.5349",
        "title": "Toward the Automatic Generation of a Semantic VRML Model from Unorganized 3D Point Clouds",
        "authors": [
            "Helmi Ben Hmida",
            "Christophe Cruz",
            "Christophe Nicolle",
            "Frank Boochs"
        ],
        "abstract": "This paper presents our experience regarding the creation of 3D semantic facility model out of unorganized 3D point clouds. Thus, a knowledge-based detection approach of objects using the OWL ontology language is presented. This knowledge is used to define SWRL detection rules. In addition, the combination of 3D processing built-ins and topological Built-Ins in SWRL rules aims at combining geometrical analysis of 3D point clouds and specialist's knowledge. This combination allows more flexible and intelligent detection and the annotation of objects contained in 3D point clouds. The created WiDOP prototype takes a set of 3D point clouds as input, and produces an indexed scene of colored objects visualized within VRML language as output. The context of the study is the detection of railway objects materialized within the Deutsche Bahn scene such as signals, technical cupboards, electric poles, etc. Therefore, the resulting enriched and populated domain ontology, that contains the annotations of objects in the point clouds, is used to feed a GIS system.\n    ",
        "submission_date": "2013-01-21T00:00:00",
        "last_modified_date": "2013-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.5488",
        "title": "Multi-class Generalized Binary Search for Active Inverse Reinforcement Learning",
        "authors": [
            "Francisco Melo",
            "Manuel Lopes"
        ],
        "abstract": "This paper addresses the problem of learning a task from demonstration. We adopt the framework of inverse reinforcement learning, where tasks are represented in the form of a reward function. Our contribution is a novel active learning algorithm that enables the learning agent to query the expert for more informative demonstrations, thus leading to more sample-efficient learning. For this novel algorithm (Generalized Binary Search for Inverse Reinforcement Learning, or GBS-IRL), we provide a theoretical bound on sample complexity and illustrate its applicability on several different tasks. To our knowledge, GBS-IRL is the first active IRL algorithm with provable sample complexity bounds. We also discuss our method in light of other existing methods in the literature and its general applicability in multi-class classification problems. Finally, motivated by recent work on learning from demonstration in robots, we also discuss how different forms of human feedback can be integrated in a transparent manner in our learning framework.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6479",
        "title": "Ontology-based Data Access: A Study through Disjunctive Datalog, CSP, and MMSNP",
        "authors": [
            "Meghyn Bienvenu",
            "Balder ten Cate",
            "Carsten Lutz",
            "Frank Wolter"
        ],
        "abstract": "Ontology-based data access is concerned with querying incomplete data sources in the presence of domain-specific knowledge provided by an ontology. A central notion in this setting is that of an ontology-mediated query, which is a database query coupled with an ontology. In this paper, we study several classes of ontology-mediated queries, where the database queries are given as some form of conjunctive query and the ontologies are formulated in description logics or other relevant fragments of first-order logic, such as the guarded fragment and the unary-negation fragment. The contributions of the paper are three-fold. First, we characterize the expressive power of ontology-mediated queries in terms of fragments of disjunctive datalog. Second, we establish intimate connections between ontology-mediated queries and constraint satisfaction problems (CSPs) and their logical generalization, MMSNP formulas. Third, we exploit these connections to obtain new results regarding (i) first-order rewritability and datalog-rewritability of ontology-mediated queries, (ii) P/NP dichotomies for ontology-mediated queries, and (iii) the query containment problem for ontology-mediated queries.\n    ",
        "submission_date": "2013-01-28T00:00:00",
        "last_modified_date": "2013-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6678",
        "title": "An Application of Uncertain Reasoning to Requirements Engineering",
        "authors": [
            "Philip S. Barry",
            "Kathryn Blackmond Laskey"
        ],
        "abstract": "This paper examines the use of Bayesian Networks to tackle one of the tougher problems in requirements engineering, translating user requirements into system requirements.  The approach taken is to model domain knowledge as Bayesian Network fragments that are glued together to form a complete view of the domain specific system requirements.  User requirements are introduced as evidence and the propagation of belief is used to determine what are the appropriate system requirements as indicated by user requirements.  This concept has been demonstrated in the development of a system specification and the results are presented here.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6684",
        "title": "Comparing Bayesian Network Classifiers",
        "authors": [
            "Jie Cheng",
            "Russell Greiner"
        ],
        "abstract": "In this paper, we empirically evaluate algorithms for learning four types of Bayesian network (BN) classifiers - Naive-Bayes, tree augmented Naive-Bayes, BN augmented Naive-Bayes and general BNs, where the latter two are learned using two variants of a conditional-independence (CI) based BN-learning algorithm. Experimental results show the obtained classifiers, learned using the CI based algorithms, are competitive with (or superior to) the best known classifiers, based on both Bayesian networks and other formalisms; and that the computational time for learning and using these classifiers is relatively small. Moreover, these results also suggest a way to learn yet more effective classifiers; we demonstrate empirically that this new algorithm does work as expected. Collectively, these results argue that BN classifiers deserve more attention in machine learning and data mining communities.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6695",
        "title": "Data Analysis with Bayesian Networks: A Bootstrap Approach",
        "authors": [
            "Nir Friedman",
            "Moises Goldszmidt",
            "Abraham Wyner"
        ],
        "abstract": "In recent years there has been significant progress in algorithms and methods for inducing Bayesian networks from data. However, in complex data analysis problems, we need to go beyond being satisfied with inducing networks with high scores. We need to provide confidence measures on features of these networks: Is the existence of an edge between two nodes warranted? Is the Markov blanket of a given node robust? Can we say something about the ordering of the variables? We should be able to address these questions, even when the amount of data is not enough to induce a high scoring network. In this paper we propose Efron's Bootstrap as a computationally efficient approach for answering these questions. In addition, we propose to use these confidence measures to induce better structures from the data, and to detect the presence of latent variables.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6696",
        "title": "Learning Bayesian Network Structure from Massive Datasets: The \"Sparse Candidate\" Algorithm",
        "authors": [
            "Nir Friedman",
            "Iftach Nachman",
            "Dana Pe'er"
        ],
        "abstract": "Learning Bayesian networks is often cast as an optimization problem, where the computational task is to find a structure that maximizes a statistically motivated score.  By and large, existing learning tools address this optimization problem using standard heuristic search techniques.  Since the search space is extremely large, such search procedures can spend most of the time examining candidates that are extremely unreasonable. This problem becomes critical when we deal with data sets that are large either in the number of instances, or the number of attributes.   In this paper, we introduce an algorithm that achieves faster learning by restricting the search space.  This iterative algorithm restricts the parents of each variable to belong to a small subset of candidates. We then search for a network that satisfies these constraints. The learned network is then used for selecting better candidates for the next iteration.  We evaluate this algorithm both on synthetic and real-life data. Our results show that it is significantly faster than alternative search procedures without loss of quality in the learned structures.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6714",
        "title": "Expected Utility Networks",
        "authors": [
            "Pierfrancesco La Mura",
            "Yoav Shoham"
        ],
        "abstract": "We introduce a new class of graphical representations, expected utility networks (EUNs), and discuss some of its properties and potential applications to artificial intelligence and economic theory. In EUNs not only probabilities, but also utilities enjoy a modular representation. EUNs are undirected graphs with two types of arc, representing probability and utility dependencies respectively. The representation of utilities is based on a novel notion of conditional utility independence, which we introduce and discuss in the context of other existing proposals. Just as probabilistic inference involves the computation of conditional probabilities, strategic inference involves the computation of conditional expected utilities for alternative plans of action. We define a new notion of conditional expected utility (EU) independence, and show that in EUNs node separation with respect to the probability and utility subgraphs implies conditional EU independence.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6723",
        "title": "A Bayesian Network Classifier that Combines a Finite Mixture Model and a Naive Bayes Model",
        "authors": [
            "Stefano Monti",
            "Gregory F. Cooper"
        ],
        "abstract": "In this paper we present a new Bayesian network model for classification that combines the naive-Bayes (NB) classifier and the finite-mixture (FM) classifier. The resulting classifier aims at relaxing the strong assumptions on which the two component models are based, in an attempt to improve on their classification performance, both in terms of accuracy and in terms of calibration of the estimated probabilities. The proposed classifier is obtained by superimposing a finite mixture model on the set of feature variables of a naive Bayes model. We present experimental results that compare the predictive performance on real datasets of the new classifier with the predictive performance of the NB classifier and the FM classifier.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6728",
        "title": "The Decision-Theoretic Interactive Video Advisor",
        "authors": [
            "Hien Nguyen",
            "Peter Haddawy"
        ],
        "abstract": "The need to help people choose among large numbers of items and to filter through large amounts of information has led to a flood of research in construction of personal recommendation agents.  One of the central issues in constructing such agents is the representation and elicitation of user preferences or interests.  This topic has long been studied in Decision Theory, but surprisingly  little work in the area of recommender systems has made use of formal decision-theoretic techniques. This paper describes DIVA, a decision-theoretic agent for recommending movies that contains a number of novel features.  DIVA represents user preferences using pairwise comparisons among items, rather than numeric ratings.  It uses a novel similarity measure based on the concept of the probability of conflict between two orderings of items.  The system has a rich representation of preference, distinguishing between a user's general taste in movies and his immediate interests. It takes an incremental approach to preference elicitation in which the user can provide feedback if not satisfied with the recommendation list.  We empirically evaluate the performance of the system using the EachMovie collaborative filtering database.\n    ",
        "submission_date": "2013-01-23T00:00:00",
        "last_modified_date": "2013-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6905",
        "title": "Towards a Logic-Based Unifying Framework for Computing",
        "authors": [
            "Robert Kowalski",
            "Fariba Sadri"
        ],
        "abstract": "In this paper we propose a logic-based, framework inspired by artificial intelligence, but scaled down for practical database and programming applications. Computation in the framework is viewed as the task of generating a sequence of state transitions, with the purpose of making an agent's goals all true. States are represented by sets of atomic sentences (or facts), representing the values of program variables, tuples in a coordination language, facts in relational databases, or Herbrand models. \nIn the model-theoretic semantics, the entire sequence of states and events are combined into a single model-theoretic structure, by associating timestamps with facts and events. But in the operational semantics, facts are updated destructively, without timestamps. We show that the model generated by destructive updates is identical to the model generated by reasoning with facts containing timestamps. We also extend the model with intentional predicates and composite event predicates defined by logic programs containing conditions in first-order logic, which query the current state.\n    ",
        "submission_date": "2013-01-29T00:00:00",
        "last_modified_date": "2014-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7189",
        "title": "Approximate Counting of Graphical Models Via MCMC Revisited",
        "authors": [
            "Jose M. Pe\u00f1a"
        ],
        "abstract": "In Pe\u00f1a (2007), MCMC sampling is applied to approximately calculate the ratio of essential graphs (EGs) to directed acyclic graphs (DAGs) for up to 20 nodes. In the present paper, we extend that work from 20 to 31 nodes. We also extend that work by computing the approximate ratio of connected EGs to connected DAGs, of connected EGs to EGs, and of connected DAGs to DAGs. Furthermore, we prove that the latter ratio is asymptotically 1. We also discuss the implications of these results for learning DAGs from data.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7364",
        "title": "Query Expansion in Information Retrieval Systems using a Bayesian Network-Based Thesaurus",
        "authors": [
            "Luis M. de Campos",
            "Juan M. Fernandez-Luna",
            "Juan F. Huete"
        ],
        "abstract": "Information Retrieval (IR) is concerned with the identification of documents in a collection that are relevant to a given information need, usually represented as a query containing terms or keywords, which are supposed to be a good description of what the user is looking for. IR systems may improve their effectiveness (i.e., increasing the number of relevant documents retrieved) by using a process of query expansion, which automatically adds new terms to the original query posed by an user. In this paper we develop a method of query expansion based on Bayesian networks. Using a learning algorithm, we construct a Bayesian network that represents some of the relationships among the terms appearing in a given document collection; this network is then used as a thesaurus (specific for that collection). We also report the results obtained by our method on three standard test collections.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7373",
        "title": "The Bayesian Structural EM Algorithm",
        "authors": [
            "Nir Friedman"
        ],
        "abstract": "In recent years there has been a flurry of works on learning Bayesian networks from data. One of the hard problems in this area is how to effectively learn the structure of a belief network from incomplete data- that is, in the presence of missing values or hidden variables. In a recent paper, I introduced an algorithm called Structural EM that combines the standard Expectation Maximization (EM) algorithm, which optimizes parameters, with structure search for model selection. That algorithm learns networks based on penalized likelihood scores, which include the BIC/MDL score and various approximations to the Bayesian score. In this paper, I extend Structural EM to deal directly with Bayesian model selection. I prove the convergence of the resulting algorithm and show how to  apply it for learning a large class of probabilistic models, including Bayesian networks and some variants thereof.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7382",
        "title": "Inferring Informational Goals from Free-Text Queries: A Bayesian Approach",
        "authors": [
            "David Heckerman",
            "Eric J. Horvitz"
        ],
        "abstract": "People using consumer software applications typically do not use technical jargon when querying an online database of help topics. Rather, they attempt to communicate their goals with common words and phrases that describe software functionality in terms of structure and objects they understand. We describe a Bayesian approach to modeling the relationship between words in a user's query for assistance and the informational goals of the user. After reviewing the general method, we describe several extensions that center on integrating additional distinctions and structure about language usage and user goals into the Bayesian models.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7388",
        "title": "Implementing Resolute Choice Under Uncertainty",
        "authors": [
            "Jean-Yves Jaffray"
        ],
        "abstract": "The adaptation to situations of sequential choice under uncertainty of decision criteria which deviate from (subjective) expected utility raises the problem of ensuring the selection of a nondominated strategy. In particular, when following the suggestion of Machina and McClennen of giving up separability (also known as consequentialism), which requires the choice of a substrategy in a subtree to depend only on data relevant to that subtree, one must renounce to the use of dynamic programming, since Bellman's principle is no longer valid. An interpretation of McClennen's resolute choice, based on cooperation between the successive Selves of the decision maker, is proposed. Implementations of resolute choice which prevent Money Pumps negative prices of information or, more generally, choices of dominated strategies, while remaining computationally tractable, are proposed.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7413",
        "title": "Switching Portfolios",
        "authors": [
            "Yoram Singer"
        ],
        "abstract": "A constant rebalanced portfolio is an asset allocation algorithm which keeps the same distribution of wealth among a set of assets along a period of time.  Recently, there has been work on on-line portfolio selection algorithms which are competitive with the best constant rebalanced portfolio determined in hindsight. By their nature, these algorithms employ the assumption that high returns can be achieved using a fixed asset allocation strategy. However, stock markets are far from being stationary and in many cases the wealth achieved by a constant rebalanced portfolio is much smaller than the wealth achieved by an ad-hoc investment strategy that adapts to changes in the market.  In this paper we present an efficient Bayesian portfolio selection algorithm that is able to track a changing market. We also describe a simple extension of the algorithm for the case of a general transaction cost, including the transactions cost models recently investigated by Blum and kalai. We provide a simple analysis of the competitiveness of the algorithm and check its performance on real stock data from the New York Stock Exchange accumulated during a 22-year period.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7415",
        "title": "Learning Mixtures of DAG Models",
        "authors": [
            "Bo Thiesson",
            "Christopher Meek",
            "David Maxwell Chickering",
            "David Heckerman"
        ],
        "abstract": "We describe computationally efficient methods for learning mixtures in which each component is a directed acyclic graphical model (mixtures of DAGs or MDAGs). We argue that simple search-and-score algorithms are infeasible for a variety of problems, and introduce a feasible approach in which parameter and structure search is interleaved and expected data is treated as real data. Our approach can be viewed as a combination of (1) the Cheeseman--Stutz asymptotic approximation for model posterior probability and (2) the Expectation--Maximization algorithm. We evaluate our procedure for selecting among MDAGs on synthetic and real examples.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7673",
        "title": "Toward a Dynamic Programming Solution for the 4-peg Tower of Hanoi Problem with Configurations",
        "authors": [
            "Neng-Fa Zhou",
            "Jonathan Fruhman"
        ],
        "abstract": "The Frame-Stewart algorithm for the 4-peg variant of the Tower of Hanoi, introduced in 1941, partitions disks into intermediate towers before moving the remaining disks to their destination. Algorithms that partition the disks have not been proven to be optimal, although they have been verified for up to 30 disks. This paper presents a dynamic programming approach to this algorithm, using tabling in B-Prolog. This study uses a variation of the problem, involving configurations of disks, in order to contrast the tabling approach with the approaches utilized by other solvers. A comparison of different partitioning locations for the Frame-Stewart algorithm indicates that, although certain partitions are optimal for the classic problem, they need to be modified for certain configurations, and that random configurations might require an entirely new algorithm.\n    ",
        "submission_date": "2013-01-31T00:00:00",
        "last_modified_date": "2013-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.0126",
        "title": "Proceedings of the 12th International Colloquium on Implementation of Constraint and LOgic Programming Systems",
        "authors": [
            "Nicos Angelopoulos",
            "Roberto Bagnara"
        ],
        "abstract": "This volume contains the papers presented at CICLOPS'12: 12th International Colloquium on Implementation of Constraint and LOgic Programming Systems held on Tueseday September 4th, 2012 in Budapest.\n",
        "submission_date": "2013-02-01T00:00:00",
        "last_modified_date": "2013-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.0386",
        "title": "Fast Damage Recovery in Robotics with the T-Resilience Algorithm",
        "authors": [
            "Sylvain Koos",
            "Antoine Cully",
            "Jean-Baptiste Mouret"
        ],
        "abstract": "Damage recovery is critical for autonomous robots that need to operate for a long time without assistance. Most current methods are complex and costly because they require anticipating each potential damage in order to have a contingency plan ready. As an alternative, we introduce the T-resilience algorithm, a new algorithm that allows robots to quickly and autonomously discover compensatory behaviors in unanticipated situations. This algorithm equips the robot with a self-model and discovers new behaviors by learning to avoid those that perform differently in the self-model and in reality. Our algorithm thus does not identify the damaged parts but it implicitly searches for efficient behaviors that do not use them. We evaluate the T-Resilience algorithm on a hexapod robot that needs to adapt to leg removal, broken legs and motor failures; we compare it to stochastic local search, policy gradient and the self-modeling algorithm proposed by Bongard et al. The behavior of the robot is assessed on-board thanks to a RGB-D sensor and a SLAM algorithm. Using only 25 tests on the robot and an overall running time of 20 minutes, T-Resilience consistently leads to substantially better results than the other approaches.\n    ",
        "submission_date": "2013-02-02T00:00:00",
        "last_modified_date": "2013-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.0723",
        "title": "Multi-Robot Informative Path Planning for Active Sensing of Environmental Phenomena: A Tale of Two Algorithms",
        "authors": [
            "Nannan Cao",
            "Kian Hsiang Low",
            "John M. Dolan"
        ],
        "abstract": "A key problem of robotic environmental sensing and monitoring is that of active sensing: How can a team of robots plan the most informative observation paths to minimize the uncertainty in modeling and predicting an environmental phenomenon? This paper presents two principled approaches to efficient information-theoretic path planning based on entropy and mutual information criteria for in situ active sensing of an important broad class of widely-occurring environmental phenomena called anisotropic fields. Our proposed algorithms are novel in addressing a trade-off between active sensing performance and time efficiency. An important practical consequence is that our algorithms can exploit the spatial correlation structure of Gaussian process-based anisotropic fields to improve time efficiency while preserving near-optimal active sensing performance. We analyze the time complexity of our algorithms and prove analytically that they scale better than state-of-the-art algorithms with increasing planning horizon length. We provide theoretical guarantees on the active sensing performance of our algorithms for a class of exploration tasks called transect sampling, which, in particular, can be improved with longer planning time and/or lower spatial correlation along the transect. Empirical evaluation on real-world anisotropic field data shows that our algorithms can perform better or at least as well as the state-of-the-art algorithms while often incurring a few orders of magnitude less computational time, even when the field conditions are less favorable.\n    ",
        "submission_date": "2013-02-04T00:00:00",
        "last_modified_date": "2013-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.0785",
        "title": "Beyond Markov Chains, Towards Adaptive Memristor Network-based Music Generation",
        "authors": [
            "Ella Gale",
            "Oliver Matthews",
            "Ben de Lacy Costello",
            "Andrew Adamatzky"
        ],
        "abstract": "We undertook a study of the use of a memristor network for music generation, making use of the memristor's memory to go beyond the Markov hypothesis. Seed transition matrices are created and populated using memristor equations, and which are shown to generate musical melodies and change in style over time as a result of feedback into the transition matrix. The spiking properties of simple memristor networks are demonstrated and discussed with reference to applications of music making. The limitations of simulating composing memristor networks in von Neumann hardware is discussed and a hardware solution based on physical memristor properties is presented.\n    ",
        "submission_date": "2013-02-04T00:00:00",
        "last_modified_date": "2013-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1528",
        "title": "A Bayesian Approach to Learning Bayesian Networks with Local Structure",
        "authors": [
            "David Maxwell Chickering",
            "David Heckerman",
            "Christopher Meek"
        ],
        "abstract": "Recently several researchers have investigated techniques for using data to learn Bayesian networks containing compact representations for the conditional probability distributions (CPDs) stored at each node. The majority of this work has concentrated on using decision-tree representations for the CPDs. In addition, researchers typically apply non-Bayesian (or asymptotically Bayesian) scoring functions such as MDL to evaluate the goodness-of-fit of networks to the data. In this paper we investigate a Bayesian approach to learning Bayesian networks that contain the more general decision-graph representations of the CPDs. First, we describe how to evaluate the posterior probability that is, the Bayesian score of such a network, given a database of observed cases. Second, we describe various search spaces that can be used, in conjunction with a scoring function and a search procedure, to identify one or more high-scoring networks. Finally, we present an experimental evaluation of the search spaces, using a greedy algorithm and a Bayesian scoring function.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1539",
        "title": "Image Segmentation in Video Sequences: A Probabilistic Approach",
        "authors": [
            "Nir Friedman",
            "Stuart Russell"
        ],
        "abstract": "\"Background subtraction\" is an old technique for finding moving objects in a video sequence for example, cars driving on a freeway. The idea is that subtracting the current image from a timeaveraged background image will leave only nonstationary objects. It is, however, a crude approximation to the task of classifying each pixel of the current image; it fails with slow-moving objects and does not distinguish shadows from moving objects. The basic idea of this paper is that we can classify each pixel using a model of how that pixel looks when it is part of different classes. We learn a mixture-of-Gaussians classification model for each pixel using an unsupervised technique- an efficient, incremental version of EM. Unlike the standard image-averaging approach, this automatically updates the mixture component for each class according to likelihood of membership; hence slow-moving objects are handled perfectly. Our approach also identifies and eliminates shadows much more effectively than other techniques such as thresholding. Application of this method as part of the Roadwatch traffic surveillance project is expected to result in significant improvements in vehicle identification and tracking.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1559",
        "title": "Incremental Map Generation by Low Cost Robots Based on Possibility/Necessity Grids",
        "authors": [
            "Maite Lopez-Sanchez",
            "Ramon Lopez de Mantaras",
            "Carles Sierra"
        ],
        "abstract": "In this paper we present some results obtained with a troupe of low-cost robots designed to cooperatively explore and adquire the map of unknown structured orthogonal environments.  In order to improve the covering of the explored zone, the robots show different behaviours and cooperate by transferring each other the perceived environment when they meet.  The returning robots deliver to a host computer their partial maps and the host incrementally generates the map of the environment by means of apossibility/ necessity grid.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1568",
        "title": "Conditional Utility, Utility Independence, and Utility Networks",
        "authors": [
            "Yoav Shoham"
        ],
        "abstract": "We introduce a new interpretation of two related notions - conditional utility and utility independence.  Unlike the traditional interpretation, the new interpretation renders the notions the direct analogues of their probabilistic counterparts.  To capture these notions formally, we appeal to the notion of utility distribution, introduced in previous paper.  We show that utility distributions, which have a structure that is identical to that of probability distributions, can be viewed as a special case of an additive multiattribute utility functions, and show how this special case permits us to capture the novel senses of conditional utility and utility independence.  Finally, we present the notion of utility networks, which do for utilities what Bayesian networks do for probabilities.  Specifically, utility networks exploit the new interpretation of conditional utility and utility independence to compactly represent a utility distribution.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1571",
        "title": "Score and Information for Recursive Exponential Models with Incomplete Data",
        "authors": [
            "Bo Thiesson"
        ],
        "abstract": "Recursive graphical models usually underlie the statistical modelling concerning probabilistic expert systems based on Bayesian networks.  This paper defines a version of these models, denoted as recursive exponential models, which have evolved by the desire to impose sophisticated domain knowledge onto local fragments of a model.  Besides the structural knowledge, as specified by a given model, the statistical modelling may also include expert opinion about the values of parameters in the model.  It is shown how to translate imprecise expert knowledge into approximately conjugate prior distributions. Based on possibly incomplete data, the score and the observed information are derived for these models.  This accounts for both the traditional score and observed information, derived as derivatives of the log-likelihood, and the posterior score and observed information, derived as derivatives of the log-posterior distribution.  Throughout the paper the specialization into recursive graphical models is accounted for by a simple example.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1669",
        "title": "Possible and Necessary Winner Problem in Social Polls",
        "authors": [
            "Serge Gaspers",
            "Victor Naroditskiy",
            "Nina Narodytska",
            "Toby Walsh"
        ],
        "abstract": "Social networks are increasingly being used to conduct polls. We introduce a simple model of such social polling. We suppose agents vote sequentially, but the order in which agents choose to vote is not necessarily fixed. We also suppose that an agent's vote is influenced by the votes of their friends who have already voted. Despite its simplicity, this model provides useful insights into a number of areas including social polling, sequential voting, and manipulation. We prove that the number of candidates and the network structure affect the computational complexity of computing which candidate necessarily or possibly can win in such a social poll. For social networks with bounded treewidth and a bounded number of candidates, we provide polynomial algorithms for both problems. In other cases, we prove that computing which candidates necessarily or possibly win are computationally intractable.\n    ",
        "submission_date": "2013-02-07T00:00:00",
        "last_modified_date": "2013-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1700",
        "title": "Fast Image Scanning with Deep Max-Pooling Convolutional Neural Networks",
        "authors": [
            "Alessandro Giusti",
            "Dan C. Cire\u015fan",
            "Jonathan Masci",
            "Luca M. Gambardella",
            "J\u00fcrgen Schmidhuber"
        ],
        "abstract": "Deep Neural Networks now excel at image classification, detection and segmentation. When used to scan images by means of a sliding window, however, their high computational complexity can bring even the most powerful hardware to its knees. We show how dynamic programming can speedup the process by orders of magnitude, even when max-pooling layers are present.\n    ",
        "submission_date": "2013-02-07T00:00:00",
        "last_modified_date": "2013-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.2223",
        "title": "WNtags: A Web-Based Tool For Image Labeling And Retrieval With Lexical Ontologies",
        "authors": [
            "Marko Horvat",
            "Anton Grbin",
            "Gordan Gledec"
        ],
        "abstract": "Ever growing number of image documents available on the Internet continuously motivates research in better annotation models and more efficient retrieval methods. Formal knowledge representation of objects and events in pictures, their interaction as well as context complexity becomes no longer an option for a quality image repository, but a necessity. We present an ontology-based online image annotation tool WNtags and demonstrate its usefulness in several typical multimedia retrieval tasks using International Affective Picture System emotionally annotated image database. WNtags is built around WordNet lexical ontology but considers Suggested Upper Merged Ontology as the preferred labeling formalism. WNtags uses sets of weighted WordNet synsets as high-level image semantic descriptors and query matching is performed with word stemming and node distance metrics. We also elaborate our near future plans to expand image content description with induced affect as in stimuli for research of human emotion and attention.\n    ",
        "submission_date": "2013-02-09T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.2828",
        "title": "Multi-agent RRT*: Sampling-based Cooperative Pathfinding (Extended Abstract)",
        "authors": [
            "Michal \u010c\u00e1p",
            "Peter Nov\u00e1k",
            "Ji\u0159\u00ed Vok\u0159\u00ednek",
            "Michal P\u011bchou\u010dek"
        ],
        "abstract": "Cooperative pathfinding is a problem of finding a set of non-conflicting trajectories for a number of mobile agents. Its applications include planning for teams of mobile robots, such as autonomous aircrafts, cars, or underwater vehicles. The state-of-the-art algorithms for cooperative pathfinding typically rely on some heuristic forward-search pathfinding technique, where A* is often the algorithm of choice. Here, we propose MA-RRT*, a novel algorithm for multi-agent path planning that builds upon a recently proposed asymptotically-optimal sampling-based algorithm for finding single-agent shortest path called RRT*. We experimentally evaluate the performance of the algorithm and show that the sampling-based approach offers better scalability than the classical forward-search approach in relatively large, but sparse environments, which are typical in real-world applications such as multi-aircraft collision avoidance.\n    ",
        "submission_date": "2013-02-12T00:00:00",
        "last_modified_date": "2013-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3556",
        "title": "Object Recognition with Imperfect Perception and Redundant Description",
        "authors": [
            "Claude Barrouil",
            "Jerome Lemaire"
        ],
        "abstract": "This paper deals with a scene recognition system in a robotics contex.  The general problem is to match images with <I>a priori</I> descriptions.  A typical mission would consist in identifying an object in an installation with a vision system situated at the end of a manipulator and with a human operator provided description, formulated in a pseudo-natural language, and possibly redundant.  The originality of this work comes from the nature of the description, from the special attention given to the management of imprecision and uncertainty in the interpretation process and from the way to assess the description redundancy so as to reinforce the overall matching likelihood.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3558",
        "title": "A Sufficiently Fast Algorithm for Finding Close to Optimal Junction Trees",
        "authors": [
            "Ann Becker",
            "Dan Geiger"
        ],
        "abstract": "An algorithm is developed for finding a close to optimal junction tree of a given graph G. The algorithm has a worst case complexity O(c^k n^a) where a and c are constants, n is the number of vertices, and k is the size of the largest clique in a junction tree of G in which this size is minimized.  The algorithm guarantees that the logarithm of the size of the state space of the heaviest clique in the junction tree produced is less than a constant factor off the optimal value.  When k = O(log n), our algorithm yields a polynomial inference algorithm for Bayesian networks. \n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3567",
        "title": "Efficient Approximations for the Marginal Likelihood of Incomplete Data Given a Bayesian Network",
        "authors": [
            "David Maxwell Chickering",
            "David Heckerman"
        ],
        "abstract": "We discuss Bayesian methods for learning Bayesian networks when data sets are incomplete. In particular, we examine asymptotic approximations for the marginal likelihood of incomplete data given a Bayesian network. We consider the Laplace approximation and the less accurate but more efficient BIC/MDL approximation. We also consider approximations proposed by Draper (1993) and Cheeseman and Stutz (1995). These approximations are as efficient as BIC/MDL, but their accuracy has not been studied in any depth. We compare the accuracy of these approximations under the assumption that the Laplace approximation is the most accurate. In experiments using synthetic data generated from discrete naive-Bayes models having a hidden root node, we find that the CS measure is the most accurate.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2015-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3580",
        "title": "Asymptotic Model Selection for Directed Networks with Hidden Variables",
        "authors": [
            "Dan Geiger",
            "David Heckerman",
            "Christopher Meek"
        ],
        "abstract": "We extend the Bayesian Information Criterion (BIC), an asymptotic approximation for the marginal likelihood, to Bayesian networks with hidden variables. This approximation can be used to select models given large samples of data. The standard BIC as well as our extension punishes the complexity of a model according to the dimension of its parameters. We argue that the dimension of a Bayesian network with hidden variables is the rank of the Jacobian matrix of the transformation between the parameters of the network and the parameters of the observable variables. We compute the dimensions of several networks including the naive Bayes model with a hidden root node.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3593",
        "title": "Toward a Market Model for Bayesian Inference",
        "authors": [
            "David M. Pennock",
            "Michael P. Wellman"
        ],
        "abstract": "We present a methodology for representing probabilistic relationships in a general-equilibrium economic model.  Specifically, we define a precise mapping from a Bayesian network with binary nodes to a market price system where consumers and producers trade in uncertain propositions.  We demonstrate the correspondence between the equilibrium prices of goods in this economy and the probabilities represented by the Bayesian network.  A computational market model such as this may provide a useful framework for investigations of belief aggregation, distributed probabilistic inference, resource allocation under uncertainty, and other problems of decentralized uncertainty.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3988",
        "title": "A solution concept for games with altruism and cooperation",
        "authors": [
            "Valerio Capraro"
        ],
        "abstract": "Over the years, numerous experiments have been accumulated to show that cooperation is not casual and depends on the payoffs of the game. These findings suggest that humans have attitude to cooperation by nature and the same person may act more or less cooperatively depending on the particular payoffs. In other words, people do not act a priori as single agents, but they forecast how the game would be played if they formed coalitions and then they play according to their best forecast. In this paper we formalize this idea and we define a new solution concept for one-shot normal form games. We prove that this \\emph{cooperative equilibrium} exists for all finite games and it explains a number of different experimental findings, such as (1) the rate of cooperation in the Prisoner's dilemma depends on the cost-benefit ratio; (2) the rate of cooperation in the Traveler's dilemma depends on the bonus/penalty; (3) the rate of cooperation in the Publig Goods game depends on the pro-capite marginal return and on the numbers of players; (4) the rate of cooperation in the Bertrand competition depends on the number of players; (5) players tend to be fair in the bargaining problem; (6) players tend to be fair in the Ultimatum game; (7) players tend to be altruist in the Dictator game; (8) offers in the Ultimatum game are larger than offers in the Dictator game.\n    ",
        "submission_date": "2013-02-16T00:00:00",
        "last_modified_date": "2013-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4245",
        "title": "Gaussian Process Kernels for Pattern Discovery and Extrapolation",
        "authors": [
            "Andrew Gordon Wilson",
            "Ryan Prescott Adams"
        ],
        "abstract": "Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation. We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation. These kernels are derived by modelling a spectral density -- the Fourier transform of a kernel -- with a Gaussian mixture. The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic. We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data. We also show that we can reconstruct standard covariances within our framework.\n    ",
        "submission_date": "2013-02-18T00:00:00",
        "last_modified_date": "2013-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4545",
        "title": "Preference-Based Unawareness",
        "authors": [
            "Burkhard C. Schipper"
        ],
        "abstract": "Morris (1996, 1997) introduced preference-based definitions of knowledge and belief in standard state-space structures. This paper extends this preference-based approach to unawareness structures (Heifetz, Meier, and Schipper, 2006, 2008). By defining unawareness and knowledge in terms of preferences over acts in unawareness structures and showing their equivalence to the epistemic notions of unawareness and knowledge, we try to build a bridge between decision theory and epistemic logic. Unawareness of an event is characterized behaviorally as the event being null and its negation being null.\n    ",
        "submission_date": "2013-02-19T00:00:00",
        "last_modified_date": "2013-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4888",
        "title": "Exploiting Social Tags for Cross-Domain Collaborative Filtering",
        "authors": [
            "Yue Shi",
            "Martha Larson",
            "Alan Hanjalic"
        ],
        "abstract": "One of the most challenging problems in recommender systems based on the collaborative filtering (CF) concept is data sparseness, i.e., limited user preference data is available for making recommendations. Cross-domain collaborative filtering (CDCF) has been studied as an effective mechanism to alleviate data sparseness of one domain using the knowledge about user preferences from other domains. A key question to be answered in the context of CDCF is what common characteristics can be deployed to link different domains for effective knowledge transfer. In this paper, we assess the usefulness of user-contributed (social) tags in this respect. We do so by means of the Generalized Tag-induced Cross-domain Collaborative Filtering (GTagCDCF) approach that we propose in this paper and that we developed based on the general collective matrix factorization framework. Assessment is done by a series of experiments, using publicly available CF datasets that represent three cross-domain cases, i.e., two two-domain cases and one three-domain case. A comparative analysis on two-domain cases involving GTagCDCF and several state-of-the-art CDCF approaches indicates the increased benefit of using social tags as representatives of explicit links between domains for CDCF as compared to the implicit links deployed by the existing CDCF methods. In addition, we show that users from different domains can already benefit from GTagCDCF if they only share a few common tags. Finally, we use the three-domain case to validate the robustness of GTagCDCF with respect to the scale of datasets and the varying number of domains.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4964",
        "title": "Estimating Continuous Distributions in Bayesian Classifiers",
        "authors": [
            "George H. John",
            "Pat Langley"
        ],
        "abstract": "When modeling a probability distribution with a Bayesian network, we are faced with the problem of how to handle continuous variables.  Most previous work has either solved the problem by discretizing, or assumed that the data are generated by a single Gaussian. In this paper we abandon the normality assumption and instead use statistical methods for nonparametric density estimation.  For a naive Bayesian classifier, we present experimental results on a variety of natural and artificial domains, comparing two methods of density estimation: assuming normality and modeling each conditional distribution with a single Gaussian; and using nonparametric kernel density estimation. We observe large reductions in error on several natural and artificial data sets, which suggests that kernel estimation is a useful tool for learning Bayesian models. \n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.5681",
        "title": "Weighted Sets of Probabilities and Minimax Weighted Expected Regret: New Approaches for Representing Uncertainty and Making Decisions",
        "authors": [
            "Joseph Y. Halpern",
            "Samantha Leung"
        ],
        "abstract": "We consider a setting where an agent's uncertainty is represented by a set of probability measures, rather than a single measure. Measure-by-measure updating of such a set of measures upon acquiring new information is well-known to suffer from problems; agents are not always able to learn appropriately. To deal with these problems, we propose using weighted sets of probabilities: a representation where each measure is associated with a weight, which denotes its significance. We describe a natural approach to updating in such a situation and a natural approach to determining the weights. We then show how this representation can be used in decision-making, by modifying a standard approach to decision making -- minimizing expected regret -- to obtain minimax weighted expected regret (MWER). We provide an axiomatization that characterizes preferences induced by MWER both in the static and dynamic case.\n    ",
        "submission_date": "2013-02-21T00:00:00",
        "last_modified_date": "2013-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6031",
        "title": "Phoneme discrimination using KS algebra I",
        "authors": [
            "Ondrej Such"
        ],
        "abstract": "In our work we define a new algebra of operators as a substitute for fuzzy logic. Its primary purpose is for construction of binary discriminators for phonemes based on spectral content. It is optimized for design of non-parametric computational circuits, and makes uses of 4 operations: $\\min$, $\\max$, the difference and generalized additively homogenuous means.\n    ",
        "submission_date": "2013-02-25T00:00:00",
        "last_modified_date": "2013-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6557",
        "title": "Geodesic-based Salient Object Detection",
        "authors": [
            "Richard M Jiang"
        ],
        "abstract": "Saliency detection has been an intuitive way to provide useful cues for object detection and segmentation, as desired for many vision and graphics applications. In this paper, we provided a robust method for salient object detection and segmentation. Other than using various pixel-level contrast definitions, we exploited global image structures and proposed a new geodesic method dedicated for salient object detection. In the proposed approach, a new geodesic scheme, namely geodesic tunneling is proposed to tackle with textures and local chaotic structures. With our new geodesic approach, a geodesic saliency map is estimated in correspondence to spatial structures in an image. Experimental evaluation on a salient object benchmark dataset validated that our algorithm consistently outperformed a number of the state-of-art saliency methods, yielding higher precision and better recall rates. With the robust saliency estimation, we also present an unsupervised hierarchical salient object cut scheme simply using adaptive saliency thresholding, which attained the highest score in our F-measure test. We also applied our geodesic cut scheme to a number of image editing tasks as demonstrated in additional experiments.\n    ",
        "submission_date": "2013-02-26T00:00:00",
        "last_modified_date": "2013-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6584",
        "title": "Variational Algorithms for Marginal MAP",
        "authors": [
            "Qiang Liu",
            "Alexander Ihler"
        ],
        "abstract": "The marginal maximum a posteriori probability (MAP) estimation problem, which calculates the mode of the marginal posterior distribution of a subset of variables with the remaining variables marginalized, is an important inference problem in many models, such as those with hidden variables or uncertain parameters. Unfortunately, marginal MAP can be NP-hard even on trees, and has attracted less attention in the literature compared to the joint MAP (maximization) and marginalization problems. We derive a general dual representation for marginal MAP that naturally integrates the marginalization and maximization operations into a joint variational optimization problem, making it possible to easily extend most or all variational-based algorithms to marginal MAP. In particular, we derive a set of \"mixed-product\" message passing algorithms for marginal MAP, whose form is a hybrid of max-product, sum-product and a novel \"argmax-product\" message updates. We also derive a class of convergent algorithms based on proximal point methods, including one that transforms the marginal MAP problem into a sequence of standard marginalization problems. Theoretically, we provide guarantees under which our algorithms give globally or locally optimal solutions, and provide novel upper bounds on the optimal objectives. Empirically, we demonstrate that our algorithms significantly outperform the existing approaches, including a state-of-the-art algorithm based on local search methods.\n    ",
        "submission_date": "2013-02-26T00:00:00",
        "last_modified_date": "2013-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6617",
        "title": "Arriving on time: estimating travel time distributions on large-scale road networks",
        "authors": [
            "Timothy Hunter",
            "Aude Hofleitner",
            "Jack Reilly",
            "Walid Krichene",
            "Jerome Thai",
            "Anastasios Kouvelas",
            "Pieter Abbeel",
            "Alexandre Bayen"
        ],
        "abstract": "Most optimal routing problems focus on minimizing travel time or distance traveled. Oftentimes, a more useful objective is to maximize the probability of on-time arrival, which requires statistical distributions of travel times, rather than just mean values. We propose a method to estimate travel time distributions on large-scale road networks, using probe vehicle data collected from GPS. We present a framework that works with large input of data, and scales linearly with the size of the network. Leveraging the planar topology of the graph, the method computes efficiently the time correlations between neighboring streets. First, raw probe vehicle traces are compressed into pairs of travel times and number of stops for each traversed road segment using a `stop-and-go' algorithm developed for this work. The compressed data is then used as input for training a path travel time model, which couples a Markov model along with a Gaussian Markov random field. Finally, scalable inference algorithms are developed for obtaining path travel time distributions from the composite MM-GMRF model. We illustrate the accuracy and scalability of our model on a 505,000 road link network spanning the San Francisco Bay Area.\n    ",
        "submission_date": "2013-02-26T00:00:00",
        "last_modified_date": "2013-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6677",
        "title": "Taming the Curse of Dimensionality: Discrete Integration by Hashing and Optimization",
        "authors": [
            "Stefano Ermon",
            "Carla P. Gomes",
            "Ashish Sabharwal",
            "Bart Selman"
        ],
        "abstract": "Integration is affected by the curse of dimensionality and quickly becomes intractable as the dimensionality of the problem grows. We propose a randomized algorithm that, with high probability, gives a constant-factor approximation of a general discrete integral defined over an exponentially large set. This algorithm relies on solving only a small number of instances of a discrete combinatorial optimization problem subject to randomly generated parity constraints used as a hash function. As an application, we demonstrate that with a small number of MAP queries we can efficiently approximate the partition function of discrete graphical models, which can in turn be used, for instance, for marginal computation or model selection.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6813",
        "title": "On Modal Logics for Qualitative Possibility in a Fuzzy Setting",
        "authors": [
            "Petr Hajek",
            "Dagmar Harmancov\u00e1",
            "Francesc Esteva",
            "Pere Garcia",
            "Lluis Godo"
        ],
        "abstract": "Within the possibilistic approach to uncertainty modeling, the paper presents a modal logical system to reason about qualitative (comparative) statements of the possibility (and necessity) of fuzzy propositions.  We relate this qualitative modal logic to the many--valued analogues MVS5 and MVKD45 of the well known modal logics of knowledge and belief S5 and KD45 respectively.  Completeness results are obtained for such logics and therefore, they extend previous existing results for qualitative possibilistic logics in the classical non-fuzzy setting.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6838",
        "title": "Three Approaches to Probability Model Selection",
        "authors": [
            "William B. Poland",
            "Ross D. Shachter"
        ],
        "abstract": "This paper compares three approaches to the problem of selecting among probability models to fit data (1) use of statistical criteria such as Akaike's information criterion and Schwarz's \"Bayesian information criterion,\" (2) maximization of the posterior probability of the model, and (3) maximization of an effectiveness ratio? trading off accuracy and computational cost.  The unifying characteristic of the approaches is that all can be viewed as maximizing a penalized likelihood function.  The second approach with suitable prior distributions has been shown to reduce to the first. This paper shows that the third approach reduces to the second for a particular form of the effectiveness ratio, and illustrates all three approaches with the problem of selecting the number of components in a mixture of Gaussian distributions.  Unlike the first two approaches, the third can be used even when the candidate models are chosen for computational efficiency, without regard to physical interpretation, so that the likelihood and the prior distribution over models cannot be interpreted literally.  As the most general and computationally oriented of the approaches, it is especially useful for artificial intelligence applications.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.7056",
        "title": "KSU KDD: Word Sense Induction by Clustering in Topic Space",
        "authors": [
            "Wesam Elshamy",
            "Doina Caragea",
            "William Hsu"
        ],
        "abstract": "We describe our language-independent unsupervised word sense induction system. This system only uses topic features to cluster different word senses in their global context topic space. Using unlabeled data, this system trains a latent Dirichlet allocation (LDA) topic model then uses it to infer the topics distribution of the test instances. By clustering these topics distributions in their topic space we cluster them into different senses. Our hypothesis is that closeness in topic space reflects similarity between different word senses. This system participated in SemEval-2 word sense induction and disambiguation task and achieved the second highest V-measure score among all other systems.\n    ",
        "submission_date": "2013-02-28T00:00:00",
        "last_modified_date": "2013-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.7175",
        "title": "Estimating the Maximum Expected Value: An Analysis of (Nested) Cross Validation and the Maximum Sample Average",
        "authors": [
            "Hado van Hasselt"
        ],
        "abstract": "We investigate the accuracy of the two most common estimators for the maximum expected value of a general set of random variables: a generalization of the maximum sample average, and cross validation. No unbiased estimator exists and we show that it is non-trivial to select a good estimator without knowledge about the distributions of the random variables. We investigate and bound the bias and variance of the aforementioned estimators and prove consistency. The variance of cross validation can be significantly reduced, but not without risking a large bias. The bias and variance of different variants of cross validation are shown to be very problem-dependent, and a wrong choice can lead to very inaccurate estimates.\n    ",
        "submission_date": "2013-02-28T00:00:00",
        "last_modified_date": "2013-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.0691",
        "title": "Learning AMP Chain Graphs and some Marginal Models Thereof under Faithfulness: Extended Version",
        "authors": [
            "Jose M. Pe\u00f1a"
        ],
        "abstract": "This paper deals with chain graphs under the Andersson-Madigan-Perlman (AMP) interpretation. In particular, we present a constraint based algorithm for learning an AMP chain graph a given probability distribution is faithful to. Moreover, we show that the extension of Meek's conjecture to AMP chain graphs does not hold, which compromises the development of efficient and correct score+search learning algorithms under assumptions weaker than faithfulness.\n",
        "submission_date": "2013-03-04T00:00:00",
        "last_modified_date": "2014-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.0794",
        "title": "Reducing Validity in Epistemic ATL to Validity in Epistemic CTL",
        "authors": [
            "Dimitar P. Guelev"
        ],
        "abstract": "We propose a validity preserving translation from a subset of epistemic Alternating-time Temporal Logic (ATL) to epistemic Computation Tree Logic (CTL). The considered subset of epistemic ATL is known to have the finite model property and decidable model-checking. This entails the decidability of validity but the implied algorithm is unfeasible. Reducing the validity problem to that in a corresponding system of CTL makes the techniques for automated deduction for that logic available for the handling of the apparently more complex system of ATL.\n    ",
        "submission_date": "2013-03-04T00:00:00",
        "last_modified_date": "2013-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.0875",
        "title": "LT^2C^2: A language of thought with Turing-computable Kolmogorov complexity",
        "authors": [
            "Sergio Romano",
            "Mariano Sigman",
            "Santiago Figueira"
        ],
        "abstract": "In this paper, we present a theoretical effort to connect the theory of program size to psychology by implementing a concrete language of thought with Turing-computable Kolmogorov complexity (LT^2C^2) satisfying the following requirements: 1) to be simple enough so that the complexity of any given finite binary sequence can be computed, 2) to be based on tangible operations of human reasoning (printing, repeating,...), 3) to be sufficiently powerful to generate all possible sequences but not too powerful as to identify regularities which would be invisible to humans. We first formalize LT^2C^2, giving its syntax and semantics and defining an adequate notion of program size. Our setting leads to a Kolmogorov complexity function relative to LT^2C^2 which is computable in polynomial time, and it also induces a prediction algorithm in the spirit of Solomonoff's inductive inference theory. We then prove the efficacy of this language by investigating regularities in strings produced by participants attempting to generate random strings. Participants had a profound understanding of randomness and hence avoided typical misconceptions such as exaggerating the number of alternations. We reasoned that remaining regularities would express the algorithmic nature of human thoughts, revealed in the form of specific patterns. Kolmogorov complexity relative to LT^2C^2 passed three expected tests examined here: 1) human sequences were less complex than control PRNG sequences, 2) human sequences were not stationary, showing decreasing values of complexity resulting from fatigue, 3) each individual showed traces of algorithmic stability since fitting of partial sequences was more effective to predict subsequent sequences than average fits. This work extends on previous efforts to combine notions of Kolmogorov complexity theory and algorithmic information theory to psychology, by explicitly ...\n    ",
        "submission_date": "2013-03-04T00:00:00",
        "last_modified_date": "2013-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.0934",
        "title": "GURLS: a Least Squares Library for Supervised Learning",
        "authors": [
            "Andrea Tacchetti",
            "Pavan K Mallapragada",
            "Matteo Santoro",
            "Lorenzo Rosasco"
        ],
        "abstract": "We present GURLS, a least squares, modular, easy-to-extend software library for efficient supervised learning. GURLS is targeted to machine learning practitioners, as well as non-specialists. It offers a number state-of-the-art training strategies for medium and large-scale learning, and routines for efficient model selection. The library is particularly well suited for multi-output problems (multi-category/multi-label). GURLS is currently available in two independent implementations: Matlab and C++. It takes advantage of the favorable properties of regularized least squares algorithm to exploit advanced tools in linear algebra. Routines to handle computations with very large matrices by means of memory-mapped storage and distributed task execution are available. The package is distributed under the BSD licence and is available for download at ",
        "submission_date": "2013-03-05T00:00:00",
        "last_modified_date": "2013-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1232",
        "title": "Japanese-Spanish Thesaurus Construction Using English as a Pivot",
        "authors": [
            "Jessica Ram\u00edrez",
            "Masayuki Asahara",
            "Yuji Matsumoto"
        ],
        "abstract": "We present the results of research with the goal of automatically creating a multilingual thesaurus based on the freely available resources of Wikipedia and WordNet. Our goal is to increase resources for natural language processing tasks such as machine translation targeting the Japanese-Spanish language pair. Given the scarcity of resources, we use existing English resources as a pivot for creating a trilingual Japanese-Spanish-English thesaurus. Our approach consists of extracting the translation tuples from Wikipedia, disambiguating them by mapping them to WordNet word senses. We present results comparing two methods of disambiguation, the first using VSM on Wikipedia article texts and WordNet definitions, and the second using categorical information extracted from Wikipedia, We find that mixing the two methods produces favorable results. Using the proposed method, we have constructed a multilingual Spanish-Japanese-English thesaurus consisting of 25,375 entries. The same method can be applied to any pair of languages that are linked to English in Wikipedia.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1384",
        "title": "Causality in concurrent systems",
        "authors": [
            "Silvia Crafa",
            "Federica Russo"
        ],
        "abstract": "Concurrent systems identify systems, either software, hardware or even biological systems, that are characterized by sets of independent actions that can be executed in any order or simultaneously. Computer scientists resort to a causal terminology to describe and analyse the relations between the actions in these systems. However, a thorough discussion about the meaning of causality in such a context has not been developed yet. This paper aims to fill the gap. First, the paper analyses the notion of causation in concurrent systems and attempts to build bridges with the existing philosophical literature, highlighting similarities and divergences between them. Second, the paper analyses the use of counterfactual reasoning in ex-post analysis in concurrent systems (i.e. execution trace analysis).\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.2071",
        "title": "Application of the SP theory of intelligence to the understanding of natural vision and the development of computer vision",
        "authors": [
            "J. Gerard Wolff"
        ],
        "abstract": "The SP theory of intelligence aims to simplify and integrate concepts in computing and cognition, with information compression as a unifying theme. This article discusses how it may be applied to the understanding of natural vision and the development of computer vision. The theory, which is described quite fully elsewhere, is described here in outline but with enough detail to ensure that the rest of the article makes sense.\n",
        "submission_date": "2013-03-08T00:00:00",
        "last_modified_date": "2015-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.2096",
        "title": "Gene-Machine, a new search heuristic algorithm",
        "authors": [
            "Alfredo Garcia Woods"
        ],
        "abstract": "This paper introduces Gene-Machine, an efficient and new search heuristic algorithm, based in the building-block hypothesis. It is inspired by natural evolution, but does not use some of the concepts present in genetic algorithms like population, mutation and generation. This heuristic exhibits good performance in comparison with genetic algorithms, and can be used to generate useful solutions to optimization and search problems.\n    ",
        "submission_date": "2013-03-08T00:00:00",
        "last_modified_date": "2013-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.2975",
        "title": "Towards Automated Proof Strategy Generalisation",
        "authors": [
            "Gudmund Grov",
            "Ewen Maclean"
        ],
        "abstract": "The ability to automatically generalise (interactive) proofs and use such generalisations to discharge related conjectures is a very hard problem which remains unsolved. Here, we develop a notion of goal types to capture key properties of goals, which enables abstractions over the specific order and number of sub-goals arising when composing tactics. We show that the goal types form a lattice, and utilise this property in the techniques we develop to automatically generalise proof strategies in order to reuse it for proofs of related conjectures. We illustrate our approach with an example.\n    ",
        "submission_date": "2013-03-12T00:00:00",
        "last_modified_date": "2013-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.3319",
        "title": "A new type of judgement theorems for attribute characters in information system",
        "authors": [
            "Anhui Tan"
        ],
        "abstract": "The research of attribute characters in information system which contains core, necessary, unnecessary is a basic and important issue in attribute reduct. Many methods for the judgement of attribute characters are based on the relationship between the objects and attributes. In this paper, a new type of judgement theorems which are absolutely based on the relationship among attributes is proposed for the judgement of attribute characters. The method is through comparing the two new attribute sets $E(a)$ and $N(a)$ with respect to the designated attribute $a$ which is proposed in this paper. We conclude that which type of the attribute $a$ belongs to is determined by the relationship between $E(a)$ and $N(a)$ in essence. Secondly, more concise and clear results are given about the judgment of the attribute characters through analyzing the properties of refinement and precise-refinement between $E(a)$ and $N(a)$ in topology. In addition, the relationship among attributes are discussed which is useful for constructing a reduct in the last section of this paper. In the last, we propose a reduct algorithm based on $E(a)$, and this algorithm is an extended application of the analysis of attribute characters above.\n    ",
        "submission_date": "2013-03-14T00:00:00",
        "last_modified_date": "2013-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.3761",
        "title": "Update report: LEO-II version 1.5",
        "authors": [
            "Christoph Benzm\u00fcller",
            "Nik Sultana"
        ],
        "abstract": "Recent improvements of the LEO-II theorem prover are presented. These improvements include a revised ATP interface, new translations into first-order logic, rule support for the axiom of choice, detection of defined equality, and more flexible strategy scheduling.\n    ",
        "submission_date": "2013-03-15T00:00:00",
        "last_modified_date": "2013-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5016",
        "title": "Quasi Conjunction, Quasi Disjunction, T-norms and T-conorms: Probabilistic Aspects",
        "authors": [
            "Angelo Gilio",
            "Giuseppe Sanfilippo"
        ],
        "abstract": "We make a probabilistic analysis related to some inference rules which play an important role in nonmonotonic reasoning. In a coherence-based setting, we study the extensions of a probability assessment defined on $n$ conditional events to their quasi conjunction, and by exploiting duality, to their quasi disjunction. The lower and upper bounds coincide with some well known t-norms and t-conorms: minimum, product, Lukasiewicz, and Hamacher t-norms and their dual t-conorms. On this basis we obtain Quasi And and Quasi Or rules. These are rules for which any finite family of conditional events p-entails the associated quasi conjunction and quasi disjunction. We examine some cases of logical dependencies, and we study the relations among coherence, inclusion for conditional events, and p-entailment. We also consider the Or rule, where quasi conjunction and quasi disjunction of premises coincide with the conclusion. We analyze further aspects of quasi conjunction and quasi disjunction, by computing probabilistic bounds on premises from bounds on conclusions. Finally, we consider biconditional events, and we introduce the notion of an $n$-conditional event. Then we give a probabilistic interpretation for a generalized Loop rule. In an appendix we provide explicit expressions for the Hamacher t-norm and t-conorm in the unitary hypercube.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5403",
        "title": "An Entropy-based Learning Algorithm of Bayesian Conditional Trees",
        "authors": [
            "Dan Geiger"
        ],
        "abstract": "This article offers a modification of Chow and Liu's learning algorithm in the context of handwritten digit recognition.  The modified algorithm directs the user to group digits into several classes consisting of digits that are hard to distinguish and then constructing an optimal conditional tree representation for each class of digits instead of for each single digit as done by Chow and Liu (1968).  Advantages and extensions of the new method are discussed.  Related works of Wong and Wang (1977) and Wong and Poon (1989) which offer a different entropy-based learning algorithm are shown to rest on inappropriate assumptions.\n    ",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5887",
        "title": "A Behavioural Foundation for Natural Computing and a Programmability Test",
        "authors": [
            "Hector Zenil"
        ],
        "abstract": "What does it mean to claim that a physical or natural system computes? One answer, endorsed here, is that computing is about programming a system to behave in different ways. This paper offers an account of what it means for a physical system to compute based on this notion. It proposes a behavioural characterisation of computing in terms of a measure of programmability, which reflects a system's ability to react to external stimuli. The proposed measure of programmability is useful for classifying computers in terms of the apparent algorithmic complexity of their evolution in time. I make some specific proposals in this connection and discuss this approach in the context of other behavioural approaches, notably Turing's test of machine intelligence. I also anticipate possible objections and consider the applicability of these proposals to the task of relating abstract computation to nature-like computation.\n    ",
        "submission_date": "2013-03-23T00:00:00",
        "last_modified_date": "2013-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.6145",
        "title": "Particles Prefer Walking Along the Axes: Experimental Insights into the Behavior of a Particle Swarm",
        "authors": [
            "Manuel Schmitt",
            "Rolf Wanka"
        ],
        "abstract": "Particle swarm optimization (PSO) is a widely used nature-inspired meta-heuristic for solving continuous optimization problems. However, when running the PSO algorithm, one encounters the phenomenon of so-called stagnation, that means in our context, the whole swarm starts to converge to a solution that is not (even a local) optimum. The goal of this work is to point out possible reasons why the swarm stagnates at these non-optimal points. To achieve our results, we use the newly defined potential of a swarm. The total potential has a portion for every dimension of the search space, and it drops when the swarm approaches the point of convergence. As it turns out experimentally, the swarm is very likely to come sometimes into \"unbalanced\" states, i. e., almost all potential belongs to one axis. Therefore, the swarm becomes blind for improvements still possible in any other direction. Finally, we show how in the light of the potential and these observations, a slightly adapted PSO rebalances the potential and therefore increases the quality of the solution.\n    ",
        "submission_date": "2013-03-25T00:00:00",
        "last_modified_date": "2013-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.7077",
        "title": "On the speed of constraint propagation and the time complexity of arc consistency testing",
        "authors": [
            "Christoph Berkholz",
            "Oleg Verbitsky"
        ],
        "abstract": "Establishing arc consistency on two relational structures is one of the most popular heuristics for the constraint satisfaction problem. We aim at determining the time complexity of arc consistency testing. The input structures $G$ and $H$ can be supposed to be connected colored graphs, as the general problem reduces to this particular case. We first observe the upper bound $O(e(G)v(H)+v(G)e(H))$, which implies the bound $O(e(G)e(H))$ in terms of the number of edges and the bound $O((v(G)+v(H))^3)$ in terms of the number of vertices. We then show that both bounds are tight up to a constant factor as long as an arc consistency algorithm is based on constraint propagation (like any algorithm currently known).\n",
        "submission_date": "2013-03-28T00:00:00",
        "last_modified_date": "2013-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.7085",
        "title": "Semantic Matching of Security Policies to Support Security Experts",
        "authors": [
            "Othman Benammar",
            "Hicham Elasri",
            "Abderrahim Sekkaki"
        ],
        "abstract": "Management of security policies has become increasingly difficult given the number of domains to manage, taken into consideration their extent and their complexity. Security experts has to deal with a variety of frameworks and specification languages used in different domains that may belong to any Cloud Computing or Distributed Systems. This wealth of frameworks and languages make the management task and the interpretation of the security policies so difficult. Each approach provides its own conflict management method or tool, the security expert will be forced to manage all these tools, which makes the field maintenance and time consuming expensive. In order to hide this complexity and to facilitate some security experts tasks and automate the others, we propose a security policies aligning based on ontologies process; this process enables to detect and resolve security policies conflicts and to support security experts in managing tasks.\n    ",
        "submission_date": "2013-03-28T00:00:00",
        "last_modified_date": "2013-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.7327",
        "title": "Symmetries in Modal Logics",
        "authors": [
            "Carlos Areces",
            "Guillaume Hoffmann",
            "Ezequiel Orbe"
        ],
        "abstract": " We generalize the notion of symmetries of propositional formulas in conjunctive normal form to modal formulas.  Our framework uses the coinductive models and, hence, the results apply to a wide class of modal logics including, for example, hybrid logics.  Our main result shows that the symmetries of a modal formula preserve entailment.\n    ",
        "submission_date": "2013-03-29T00:00:00",
        "last_modified_date": "2013-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.7335",
        "title": "Formalizing the Confluence of Orthogonal Rewriting Systems",
        "authors": [
            "Ana Cristina Rocha Oliveira",
            "Mauricio Ayala-Rinc\u00f3n"
        ],
        "abstract": "Orthogonality is a discipline of programming that in a syntactic manner guarantees determinism of functional   specifications. Essentially, orthogonality avoids, on the one side,  the inherent ambiguity of non determinism, prohibiting the existence  of different rules that specify the same function and that may apply  simultaneously (non-ambiguity), and, on the other side, it  eliminates the possibility of occurrence of repetitions of variables  in the left-hand side of these rules (left linearity). In the  theory of term rewriting systems (TRSs) determinism is captured by  the well-known property of confluence, that basically states that  whenever different computations or simplifications from a term are  possible, the computed answers should coincide. Although the proofs  are technically elaborated, confluence is well-known to be a  consequence of orthogonality. Thus, orthogonality is an important  mathematical discipline intrinsic to the specification of recursive  functions that is naturally applied in functional programming and  specification.  Starting from a formalization of the theory of TRSs  in the proof assistant PVS, this work describes how confluence of  orthogonal TRSs has been formalized, based on axiomatizations of  properties of rules, positions and substitutions involved in  parallel steps of reduction, in this proof assistant. Proofs for  some similar but restricted properties such as the property of  confluence of non-ambiguous and (left and right) linear TRSs have  been fully formalized.\n\n    ",
        "submission_date": "2013-03-29T00:00:00",
        "last_modified_date": "2013-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.0030",
        "title": "Note on Combinatorial Engineering Frameworks for Hierarchical Modular Systems",
        "authors": [
            "Mark Sh. Levin"
        ],
        "abstract": "The paper briefly describes a basic set of special combinatorial engineering frameworks for solving complex problems in the field of hierarchical modular systems. The frameworks consist of combinatorial problems (and corresponding models), which are interconnected/linked (e.g., by preference relation). Mainly, hierarchical morphological system model is used. The list of basic standard combinatorial engineering (technological) frameworks is the following: (1) design of system hierarchical model, (2) combinatorial synthesis ('bottom-up' process for system design), (3) system evaluation, (4) detection of system bottlenecks, (5) system improvement (re-design, upgrade), (6) multi-stage design (design of system trajectory), (7) combinatorial modeling of system evolution/development and system forecasting. The combinatorial engineering frameworks are targeted to maintenance of some system life cycle stages. The list of main underlaying combinatorial optimization problems involves the following: knapsack problem, multiple-choice problem, assignment problem, spanning trees, morphological clique problem.\n    ",
        "submission_date": "2013-03-29T00:00:00",
        "last_modified_date": "2013-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.0160",
        "title": "Parallel Computation Is ESS",
        "authors": [
            "Nabarun Mondal",
            "Partha P. Ghosh"
        ],
        "abstract": "There are enormous amount of examples of Computation in nature, exemplified across multiple species in biology. One crucial aim for these computations across all life forms their ability to learn and thereby increase the chance of their survival. In the current paper a formal definition of autonomous learning is proposed. From that definition we establish a Turing Machine model for learning, where rule tables can be added or deleted, but can not be modified. Sequential and parallel implementations of this model are discussed. It is found that for general purpose learning based on this model, the implementations capable of parallel execution would be evolutionarily stable. This is proposed to be of the reasons why in Nature parallelism in computation is found in abundance.\n    ",
        "submission_date": "2013-03-31T00:00:00",
        "last_modified_date": "2013-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.0564",
        "title": "On the definition of a confounder",
        "authors": [
            "Tyler J. VanderWeele",
            "Ilya Shpitser"
        ],
        "abstract": "The causal inference literature has provided a clear formal definition of confounding expressed in terms of counterfactual independence. The literature has not, however, come to any consensus on a formal definition of a confounder, as it has given priority to the concept of confounding over that of a confounder. We consider a number of candidate definitions arising from various more informal statements made in the literature. We consider the properties satisfied by each candidate definition, principally focusing on (i) whether under the candidate definition control for all \"confounders\" suffices to control for \"confounding\" and (ii) whether each confounder in some context helps eliminate or reduce confounding bias. Several of the candidate definitions do not have these two properties. Only one candidate definition of those considered satisfies both properties. We propose that a \"confounder\" be defined as a pre-exposure covariate C for which there exists a set of other covariates X such that effect of the exposure on the outcome is unconfounded conditional on (X,C) but such that for no proper subset of (X,C) is the effect of the exposure on the outcome unconfounded given the subset. We also provide a conditional analogue of the above definition; and we propose a variable that helps reduce bias but not eliminate bias be referred to as a \"surrogate confounder.\" These definitions are closely related to those given by Robins and Morgenstern [Comput. Math. Appl. 14 (1987) 869-916]. The implications that hold among the various candidate definitions are discussed.\n    ",
        "submission_date": "2013-04-02T00:00:00",
        "last_modified_date": "2013-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.0640",
        "title": "Event management for large scale event-driven digital hardware spiking neural networks",
        "authors": [
            "Louis-Charles Caron",
            "\\and Michiel D'Haene",
            "\\and Fr\u00e9d\u00e9ric Mailhot",
            "\\and Benjamin Schrauwen",
            "\\and Jean Rouat"
        ],
        "abstract": "The interest in brain-like computation has led to the design of a plethora of innovative neuromorphic systems. Individually, spiking neural networks (SNNs), event-driven simulation and digital hardware neuromorphic systems get a lot of attention. Despite the popularity of event-driven SNNs in software, very few digital hardware architectures are found. This is because existing hardware solutions for event management scale badly with the number of events. This paper introduces the structured heap queue, a pipelined digital hardware data structure, and demonstrates its suitability for event management. The structured heap queue scales gracefully with the number of events, allowing the efficient implementation of large scale digital hardware event-driven SNNs. The scaling is linear for memory, logarithmic for logic resources and constant for processing time. The use of the structured heap queue is demonstrated on field-programmable gate array (FPGA) with an image segmentation experiment and a SNN of 65~536 neurons and 513~184 synapses. Events can be processed at the rate of 1 every 7 clock cycles and a 406$\\times$158 pixel image is segmented in 200 ms.\n    ",
        "submission_date": "2013-04-02T00:00:00",
        "last_modified_date": "2013-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1014",
        "title": "A Novel Frank-Wolfe Algorithm. Analysis and Applications to Large-Scale SVM Training",
        "authors": [
            "Hector Allende",
            "Emanuele Frandi",
            "Ricardo Nanculef",
            "Claudio Sartori"
        ],
        "abstract": "Recently, there has been a renewed interest in the machine learning community for variants of a sparse greedy approximation procedure for concave optimization known as {the Frank-Wolfe (FW) method}. In particular, this procedure has been successfully applied to train large-scale instances of non-linear Support Vector Machines (SVMs). Specializing FW to SVM training has allowed to obtain efficient algorithms but also important theoretical results, including convergence analysis of training algorithms and new characterizations of model sparsity.\n",
        "submission_date": "2013-04-03T00:00:00",
        "last_modified_date": "2013-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1098",
        "title": "Occupancy Grids: A Stochastic Spatial Representation for Active Robot Perception",
        "authors": [
            "A. Elfes"
        ],
        "abstract": "In this paper we provide an overview of a new framework for robot perception, real-world modelling, and navigation that uses a stochastic tesselated representation of spatial information called the Occupancy Grid. The Occupancy Grid is a multi-dimensional random field model that maintains probabilistic estimates of the occupancy state of each cell in a spatial lattice. Bayesian estimation mechanisms employing stochastic sensor models allow incremental updating of the Occupancy Grid using multi-view, multi-sensor data, composition of multiple maps, decision-making, and incorporation of robot and sensor position uncertainty. We present the underlying stochastic formulation of the Occupancy Grid framework, and discuss its application to a variety of robotic tusks. These include range-based mapping, multi-sensor integration, path-planning and obstacle avoidance, handling of robot position uncertainty, incorporation of pre-compiled maps, recovery of geometric representations, and other related problems. The experimental results show that the Occupancy Grid approach generates dense world models, is robust under sensor uncertainty and errors, and allows explicit handling of uncertainty. It supports the development of robust and agile sensor interpretation methods, incremental discovery procedures, and composition of information from multiple sources. Furthermore, the results illustrate that robotic tasks can be addressed through operations performed di- rectly on the Occupancy Grid, and that these operations have strong parallels to operations performed in the image processing domain.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1517",
        "title": "Model-based Influence Diagrams for Machine Vision",
        "authors": [
            "Tod S. Levitt",
            "John Mark Agosta",
            "Thomas O. Binford"
        ],
        "abstract": "We show an approach to automated control of machine vision systems based on incremental creation and evaluation of a particular family of influence diagrams that represent hypotheses of imagery interpretation and possible subsequent processing decisions.  In our approach, model-based machine vision techniques are integrated with hierarchical Bayesian inference to provide a framework for representing and matching instances of objects and relationships in imagery and for accruing probabilities to rank order conflicting scene interpretations.  We extend a result of Tatman and Shachter to show that the sequence of processing decisions derived from evaluating the diagrams at each stage is the same as the sequence that would have been derived by evaluating the final influence diagram that contains all random variables created during the run of the vision system.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2024",
        "title": "A General Framework for Interacting Bayes-Optimally with Self-Interested Agents using Arbitrary Parametric Model and Model Prior",
        "authors": [
            "Trong Nghia Hoang",
            "Kian Hsiang Low"
        ],
        "abstract": "Recent advances in Bayesian reinforcement learning (BRL) have shown that Bayes-optimality is theoretically achievable by modeling the environment's latent dynamics using Flat-Dirichlet-Multinomial (FDM) prior. In self-interested multi-agent environments, the transition dynamics are mainly controlled by the other agent's stochastic behavior for which FDM's independence and modeling assumptions do not hold. As a result, FDM does not allow the other agent's behavior to be generalized across different states nor specified using prior domain knowledge. To overcome these practical limitations of FDM, we propose a generalization of BRL to integrate the general class of parametric models and model priors, thus allowing practitioners' domain knowledge to be exploited to produce a fine-grained and compact representation of the other agent's behavior. Empirical evaluation shows that our approach outperforms existing multi-agent reinforcement learning algorithms.\n    ",
        "submission_date": "2013-04-07T00:00:00",
        "last_modified_date": "2014-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2363",
        "title": "Multiple decision trees",
        "authors": [
            "Suk Wah Kwok",
            "Chris Carter"
        ],
        "abstract": "This paper describes experiments, on two domains, to investigate the effect of averaging over predictions of multiple decision trees, instead of using a single tree.  Other authors have pointed out theoretical and commonsense reasons for preferring the multiple tree approach.  Ideally, we would like to consider predictions from all trees, weighted by their probability.  However, there is a vast number of different trees, and it is difficult to estimate the probability of each tree.  We sidestep the estimation problem by using a modified version of the ID3 algorithm to build good trees, and average over only these trees.  Our results are encouraging.  For each domain, we managed to produce a small number of good trees.  We find that it is best to average across sets of trees with different structure; this usually gives better performance than any of the constituent trees, including the ID3 tree.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2367",
        "title": "Utility-Based Control for Computer Vision",
        "authors": [
            "Tod S. Levitt",
            "Thomas O. Binford",
            "Gil J. Ettinger",
            "Patrice Gelband"
        ],
        "abstract": "Several key issues arise in implementing computer vision recognition of world objects in terms of Bayesian networks.  Computational efficiency is a driving force.  Perceptual networks are very deep, typically fifteen levels of structure.  Images are wide, e.g., an unspecified-number of edges may appear anywhere in an image 512 x 512 pixels or larger.  For efficiency, we dynamically instantiate hypotheses of observed objects.  The network is not fixed, but is created incrementally at runtime.  Generation of hypotheses of world objects and indexing of models for recognition are important, but they are not considered here [4,11].  This work is aimed at near-term implementation with parallel computation in a radar surveillance system, ADRIES [5, 15], and a system for industrial part recognition, SUCCESSOR [2].  For many applications, vision must be faster to be practical and so efficiently controlling the machine vision process is critical.  Perceptual operators may scan megapixels and may require minutes of computation time.  It is necessary to avoid unnecessary sensor actions and computation.  Parallel computation is available at several levels of processor capability.  The potential for parallel, distributed computation for high-level vision means distributing non-homogeneous computations. This paper addresses the problem of task control in machine vision systems based on Bayesian probability models.  We separate control and inference to extend the previous work [3] to maximize utility instead of probability.  Maximizing utility allows adopting perceptual strategies for efficient information gathering with sensors and analysis of sensor data.  Results of controlling machine vision via utility to recognize military situations are presented in this paper.  Future work extends this to industrial part recognition for SUCCESSOR.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2382",
        "title": "Predicting the Likely Behaviors of Continuous Nonlinear Systems in Equilibrium",
        "authors": [
            "Alexander Yeh"
        ],
        "abstract": "This paper introduces a method for predicting the likely behaviors of continuous nonlinear systems in equilibrium in which the input values can vary.  The method uses a parameterized equation model and a lower bound on the input joint density to bound the likelihood that some behavior will occur, such as a state variable being inside a given numeric range.  Using a bound on the density instead of the density itself is desirable because often the input density's parameters and shape are not exactly known.  The new method is called SAB after its basic operations: split the input value space into smaller regions, and then bound those regions' possible behaviors and the probability of being in them.  SAB finds rough bounds at first, and then refines them as more time is given.  In contrast to other researchers' methods, SAB can (1) find all the possible system behaviors, and indicate how likely they are, (2) does not approximate the distribution of possible outcomes without some measure of the error magnitude, (3) does not use discretized variable values, which limit the events one can find probability bounds for, (4) can handle density bounds, and (5) can handle such criteria as two state variables both being inside a numeric range.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2743",
        "title": "Comparisons of Reasoning Mechanisms for Computer Vision",
        "authors": [
            "Ze-Nian Li"
        ],
        "abstract": "An evidential reasoning mechanism based on the Dempster-Shafer theory of evidence is introduced. Its performance in real-world image analysis is compared with other mechanisms based on the Bayesian formalism and a simple weight combination method.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2749",
        "title": "Evidential Reasoning in Image Understanding",
        "authors": [
            "Minchuan Zhang",
            "Su-shing Chen"
        ],
        "abstract": "In this paper, we present some results of evidential reasoning in understanding multispectral images of remote sensing systems. The Dempster-Shafer approach of combination of evidences is pursued to yield contextual classification results, which are compared with previous results of the Bayesian context free classification, contextual classifications of dynamic programming and stochastic relaxation approaches.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2757",
        "title": "Estimation Procedures for Robust Sensor Control",
        "authors": [
            "Greg Hager",
            "Max Mintz"
        ],
        "abstract": "Many robotic sensor estimation problems can characterized in terms of nonlinear measurement systems. These systems are contaminated with noise and may be underdetermined from a single observation. In order to get reliable estimation results, the system must choose views which result in an overdetermined system. This is the sensor control problem.  Accurate and reliable sensor control requires an estimation procedure which yields both estimates and measures of its own performance. In the case of nonlinear measurement systems, computationally simple closed-form estimation solutions may not exist. However, approximation techniques provide viable alternatives. In this paper, we evaluate three estimation techniques: the extended Kalman filter, a discrete Bayes approximation, and an iterative Bayes approximation. We present mathematical results and simulation statistics illustrating operating conditions where the extended Kalman filter is inappropriate for sensor control, and discuss issues in the use of the discrete Bayes approximation.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2888",
        "title": "Roborobo! a Fast Robot Simulator for Swarm and Collective Robotics",
        "authors": [
            "Nicolas Bredeche",
            "Jean-Marc Montanier",
            "Berend Weel",
            "Evert Haasdijk"
        ],
        "abstract": "Roborobo! is a multi-platform, highly portable, robot simulator for large-scale collective robotics experiments. Roborobo! is coded in C++, and follows the KISS guideline (\"Keep it simple\"). Therefore, its external dependency is solely limited to the widely available SDL library for fast 2D Graphics. Roborobo! is based on a Khepera/ePuck model. It is targeted for fast single and multi-robots simulation, and has already been used in more than a dozen published research mainly concerned with evolutionary swarm robotics, including environment-driven self-adaptation and distributed evolutionary optimization, as well as online onboard embodied evolution and embodied morphogenesis.\n    ",
        "submission_date": "2013-04-10T00:00:00",
        "last_modified_date": "2013-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3088",
        "title": "Information and Multi-Sensor Coordination",
        "authors": [
            "Greg Hager",
            "Hugh F. Durrant-Whyte"
        ],
        "abstract": "The control and integration of distributed, multi-sensor perceptual systems is a complex and challenging problem. The observations or opinions of different sensors are often disparate incomparable and are usually only partial views. Sensor information is inherently uncertain and in addition the individual sensors may themselves be in error with respect to the system as a whole. The successful operation of a multi-sensor system must account for this uncertainty and provide for the aggregation of disparate information in an intelligent and robust manner. We consider the sensors of a multi-sensor system to be members or agents of a team, able to offer opinions and bargain in group decisions. We will analyze the coordination and control of this structure using a theory of team decision-making. We present some new analytic results on multi-sensor aggregation and detail a simulation which we use to investigate our ideas. This simulation provides a basis for the analysis of complex agent structures cooperating in the presence of uncertainty. The results of this study are discussed with reference to multi-sensor robot systems, distributed Al and decision making under uncertainty.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3449",
        "title": "Statistical Mechanics Algorithm for Response to Targets (SMART)",
        "authors": [
            "Lester Ingber"
        ],
        "abstract": "It is proposed to apply modern methods of nonlinear nonequilibrium statistical mechanics to develop software algorithms that will optimally respond to targets within short response times with minimal computer resources. This Statistical Mechanics Algorithm for Response to Targets (SMART) can be developed with a view towards its future implementation into a hardwired Statistical Algorithm Multiprocessor (SAM) to enhance the efficiency and speed of response to targets (SMART_SAM).\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3733",
        "title": "General Quantum Hilbert Space Modeling Scheme for Entanglement",
        "authors": [
            "Diederik Aerts",
            "Sandro Sozzo"
        ],
        "abstract": "We work out a classification scheme for quantum modeling in Hilbert space of any kind of composite entity violating Bell's inequalities and exhibiting entanglement. Our theoretical framework includes situations with entangled states and product measurements ('customary quantum situation'), and also situations with both entangled states and entangled measurements ('nonlocal box situation', 'nonlocal non-marginal box situation'). We show that entanglement is structurally a joint property of states and measurements. Furthermore, entangled measurements enable quantum modeling of situations that are usually believed to be 'beyond quantum'. Our results are also extended from pure states to quantum mixtures.\n    ",
        "submission_date": "2013-04-12T00:00:00",
        "last_modified_date": "2013-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3940",
        "title": "Unveiling the link between logical fallacies and web persuasion",
        "authors": [
            "Antonio Lieto",
            "Fabiana Vernero"
        ],
        "abstract": "In the last decade Human-Computer Interaction (HCI) has started to focus attention on forms of persuasive interaction where computer technologies have the goal of changing users behavior and attitudes according to a predefined direction. In this work, we hypothesize a strong connection between logical fallacies (forms of reasoning which are logically invalid but cognitively effective) and some common persuasion strategies adopted within web technologies. With the aim of empirically evaluating our hypothesis, we carried out a pilot study on a sample of 150 e-commerce websites.\n    ",
        "submission_date": "2013-04-14T00:00:00",
        "last_modified_date": "2013-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.4371",
        "title": "Efficient Computation of Mean Truncated Hitting Times on Very Large Graphs",
        "authors": [
            "Joel Lang",
            "James Henderson"
        ],
        "abstract": "Previous work has shown the effectiveness of random walk hitting times as a measure of dissimilarity in a variety of graph-based learning problems such as collaborative filtering, query suggestion or finding paraphrases. However, application of hitting times has been limited to small datasets because of computational restrictions. This paper develops a new approximation algorithm with which hitting times can be computed on very large, disk-resident graphs, making their application possible to problems which were previously out of reach. This will potentially benefit a range of large-scale problems.\n    ",
        "submission_date": "2013-04-16T00:00:00",
        "last_modified_date": "2013-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.4910",
        "title": "A Junction Tree Framework for Undirected Graphical Model Selection",
        "authors": [
            "Divyanshu Vats",
            "Robert Nowak"
        ],
        "abstract": "An undirected graphical model is a joint probability distribution defined on an undirected graph G*, where the vertices in the graph index a collection of random variables and the edges encode conditional independence relationships among random variables. The undirected graphical model selection (UGMS) problem is to estimate the graph G* given observations drawn from the undirected graphical model. This paper proposes a framework for decomposing the UGMS problem into multiple subproblems over clusters and subsets of the separators in a junction tree. The junction tree is constructed using a graph that contains a superset of the edges in G*. We highlight three main properties of using junction trees for UGMS. First, different regularization parameters or different UGMS algorithms can be used to learn different parts of the graph. This is possible since the subproblems we identify can be solved independently of each other. Second, under certain conditions, a junction tree based UGMS algorithm can produce consistent results with fewer observations than the usual requirements of existing algorithms. Third, both our theoretical and experimental results show that the junction tree framework does a significantly better job at finding the weakest edges in a graph than existing methods. This property is a consequence of both the first and second properties. Finally, we note that our framework is independent of the choice of the UGMS algorithm and can be used as a wrapper around standard UGMS algorithms for more accurate graph estimation.\n    ",
        "submission_date": "2013-04-17T00:00:00",
        "last_modified_date": "2013-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5185",
        "title": "Temporal Description Logic for Ontology-Based Data Access (Extended Version)",
        "authors": [
            "Alessandro Artale",
            "Roman Kontchakov",
            "Frank Wolter",
            "Michael Zakharyaschev"
        ],
        "abstract": "Our aim is to investigate ontology-based data access over temporal data with validity time and ontologies capable of temporal conceptual modelling. To this end, we design a temporal description logic, TQL, that extends the standard ontology language OWL 2 QL, provides basic means for temporal conceptual modelling and ensures first-order rewritability of conjunctive queries for suitably defined data instances with validity time.\n    ",
        "submission_date": "2013-04-18T00:00:00",
        "last_modified_date": "2013-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5409",
        "title": "Separating the Real from the Synthetic: Minutiae Histograms as Fingerprints of Fingerprints",
        "authors": [
            "Carsten Gottschlich",
            "Stephan Huckemann"
        ],
        "abstract": "In this study we show that by the current state-of-the-art synthetically generated fingerprints can easily be discriminated from real fingerprints. We propose a method based on second order extended minutiae histograms (MHs) which can distinguish between real and synthetic prints with very high accuracy. MHs provide a fixed-length feature vector for a fingerprint which are invariant under rotation and translation. This 'test of realness' can be applied to synthetic fingerprints produced by any method. In this work, tests are conducted on the 12 publicly available databases of FVC2000, FVC2002 and FVC2004 which are well established benchmarks for evaluating the performance of fingerprint recognition algorithms; 3 of these 12 databases consist of artificial fingerprints generated by the SFinGe software. Additionally, we evaluate the discriminative performance on a database of synthetic fingerprints generated by the software of Bicz versus real fingerprint images. We conclude with suggestions for the improvement of synthetic fingerprint generation.\n    ",
        "submission_date": "2013-04-19T00:00:00",
        "last_modified_date": "2014-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5479",
        "title": "Local Backbones",
        "authors": [
            "Ronald de Haan",
            "Iyad Kanj",
            "Stefan Szeider"
        ],
        "abstract": "A backbone of a propositional CNF formula is a variable whose truth value is the same in every truth assignment that satisfies the formula. The notion of backbones for CNF formulas has been studied in various contexts. In this paper, we introduce local variants of backbones, and study the computational complexity of detecting them. In particular, we consider k-backbones, which are backbones for sub-formulas consisting of at most k clauses, and iterative k-backbones, which are backbones that result after repeated instantiations of k-backbones. We determine the parameterized complexity of deciding whether a variable is a k-backbone or an iterative k-backbone for various restricted formula classes, including Horn, definite Horn, and Krom. We also present some first empirical results regarding backbones for CNF-Satisfiability (SAT). The empirical results we obtain show that a large fraction of the backbones of structured SAT instances are local, in contrast to random instances, which appear to have few local backbones.\n    ",
        "submission_date": "2013-04-19T00:00:00",
        "last_modified_date": "2014-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5530",
        "title": "Inexact Coordinate Descent: Complexity and Preconditioning",
        "authors": [
            "Rachael Tappenden",
            "Peter Richt\u00e1rik",
            "Jacek Gondzio"
        ],
        "abstract": "In this paper we consider the problem of minimizing a convex function using a randomized block coordinate descent method. One of the key steps at each iteration of the algorithm is determining the update to a block of variables. Existing algorithms assume that in order to compute the update, a particular subproblem is solved exactly. In his work we relax this requirement, and allow for the subproblem to be solved inexactly, leading to an inexact block coordinate descent method. Our approach incorporates the best known results for exact updates as a special case. Moreover, these theoretical guarantees are complemented by practical considerations: the use of iterative techniques to determine the update as well as the use of preconditioning for further acceleration.\n    ",
        "submission_date": "2013-04-19T00:00:00",
        "last_modified_date": "2014-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5566",
        "title": "A Markov Model for Ontology Alignment",
        "authors": [
            "Michael E. Cotterell",
            "Terrance Medina"
        ],
        "abstract": "The explosion of available data along with the need to integrate and utilize that data has led to a pressing interest in data integration techniques. In terms of Semantic Web technologies, Ontology Alignment is a key step in the process of integrating heterogeneous knowledge bases. In this paper, we present the Edge Confidence technique, a modification and improvement over the popular Similarity Flooding technique for Ontology Alignment.\n    ",
        "submission_date": "2013-04-20T00:00:00",
        "last_modified_date": "2013-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5610",
        "title": "Tight Performance Bounds for Approximate Modified Policy Iteration with Non-Stationary Policies",
        "authors": [
            "Boris Lesner",
            "Bruno Scherrer"
        ],
        "abstract": "We consider approximate dynamic programming for the infinite-horizon stationary $\\gamma$-discounted optimal control problem formalized by Markov Decision Processes. While in the exact case it is known that there always exists an optimal policy that is stationary, we show that when using value function approximation, looking for a non-stationary policy may lead to a better performance guarantee. We define a non-stationary variant of MPI that unifies a broad family of approximate DP algorithms of the literature. For this algorithm we provide an error propagation analysis in the form of a performance bound of the resulting policies that can improve the usual performance bound by a factor $O(1-\\gamma)$, which is significant when the discount factor $\\gamma$ is close to 1. Doing so, our approach unifies recent results for Value and Policy Iteration. Furthermore, we show, by constructing a specific deterministic MDP, that our performance guarantee is tight.\n    ",
        "submission_date": "2013-04-20T00:00:00",
        "last_modified_date": "2013-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5822",
        "title": "Bargaining for Revenue Shares on Tree Trading Networks",
        "authors": [
            "Arpita Ghosh",
            "Satyen Kale",
            "Kevin Lang",
            "Benjamin Moseley"
        ],
        "abstract": "We study trade networks with a tree structure, where a seller with a single indivisible good is connected to buyers, each with some value for the good, via a unique path of intermediaries. Agents in the tree make multiplicative revenue share offers to their parent nodes, who choose the best offer and offer part of it to their parent, and so on; the winning path is determined by who finally makes the highest offer to the seller. In this paper, we investigate how these revenue shares might be set via a natural bargaining process between agents on the tree, specifically, egalitarian bargaining between endpoints of each edge in the tree. We investigate the fixed point of this system of bargaining equations and prove various desirable for this solution concept, including (i) existence, (ii) uniqueness, (iii) efficiency, (iv) membership in the core, (v) strict monotonicity, (vi) polynomial-time computability to any given accuracy. Finally, we present numerical evidence that asynchronous dynamics with randomly ordered updates always converges to the fixed point, indicating that the fixed point shares might arise from decentralized bargaining amongst agents on the trade network.\n    ",
        "submission_date": "2013-04-22T00:00:00",
        "last_modified_date": "2013-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.6383",
        "title": "The Stochastic Gradient Descent for the Primal L1-SVM Optimization Revisited",
        "authors": [
            "Constantinos Panagiotakopoulos",
            "Petroula Tsampouka"
        ],
        "abstract": "We reconsider the stochastic (sub)gradient approach to the unconstrained primal L1-SVM optimization. We observe that if the learning rate is inversely proportional to the number of steps, i.e., the number of times any training pattern is presented to the algorithm, the update rule may be transformed into the one of the classical perceptron with margin in which the margin threshold increases linearly with the number of steps. Moreover, if we cycle repeatedly through the possibly randomly permuted training set the dual variables defined naturally via the expansion of the weight vector as a linear combination of the patterns on which margin errors were made are shown to obey at the end of each complete cycle automatically the box constraints arising in dual optimization. This renders the dual Lagrangian a running lower bound on the primal objective tending to it at the optimum and makes available an upper bound on the relative accuracy achieved which provides a meaningful stopping criterion. In addition, we propose a mechanism of presenting the same pattern repeatedly to the algorithm which maintains the above properties. Finally, we give experimental evidence that algorithms constructed along these lines exhibit a considerably improved performance.\n    ",
        "submission_date": "2013-04-23T00:00:00",
        "last_modified_date": "2014-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.7045",
        "title": "An Algorithm for Training Polynomial Networks",
        "authors": [
            "Roi Livni",
            "Shai Shalev-Shwartz",
            "Ohad Shamir"
        ],
        "abstract": "We consider deep neural networks, in which the output of each node is a quadratic function of its inputs. Similar to other deep architectures, these networks can compactly represent any function on a finite training set. The main goal of this paper is the derivation of an efficient layer-by-layer algorithm for training such networks, which we denote as the \\emph{Basis Learner}. The algorithm is a universal learner in the sense that the training error is guaranteed to decrease at every iteration, and can eventually reach zero under mild conditions. We present practical implementations of this algorithm, as well as preliminary experimental results. We also compare our deep architecture to other shallow architectures for learning polynomials, in particular kernel learning.\n    ",
        "submission_date": "2013-04-26T00:00:00",
        "last_modified_date": "2014-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.7244",
        "title": "Relation-algebraic and Tool-supported Control of Condorcet Voting",
        "authors": [
            "Rudolf Berghammer",
            "Henning Schnoor"
        ],
        "abstract": "We present a relation-algebraic model of Condorcet voting and, based on it, relation-algebraic solutions of the constructive control problem via the removal of voters.\n",
        "submission_date": "2013-03-28T00:00:00",
        "last_modified_date": "2013-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.7423",
        "title": "On Integrating Fuzzy Knowledge Using a Novel Evolutionary Algorithm",
        "authors": [
            "Nafisa Afrin Chowdhury",
            "Murshida Khatun",
            "M.M.A. Hashem"
        ],
        "abstract": "Fuzzy systems may be considered as knowledge-based systems that incorporates human knowledge into their knowledge base through fuzzy rules and fuzzy membership functions. The intent of this study is to present a fuzzy knowledge integration framework using a Novel Evolutionary Strategy (NES), which can simultaneously integrate multiple fuzzy rule sets and their membership function sets. The proposed approach consists of two phases: fuzzy knowledge encoding and fuzzy knowledge integration. Four application domains, the hepatitis diagnosis, the sugarcane breeding prediction, Iris plants classification, and Tic-tac-toe endgame were used to show the performance ofthe proposed knowledge approach. Results show that the fuzzy knowledge base derived using our approach performs better than Genetic Algorithm based approach.\n    ",
        "submission_date": "2013-04-28T00:00:00",
        "last_modified_date": "2013-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.7507",
        "title": "Measuring Cultural Relativity of Emotional Valence and Arousal using Semantic Clustering and Twitter",
        "authors": [
            "Eugene Yuta Bann",
            "Joanna J. Bryson"
        ],
        "abstract": "Researchers since at least Darwin have debated whether and to what extent emotions are universal or culture-dependent. However, previous studies have primarily focused on facial expressions and on a limited set of emotions. Given that emotions have a substantial impact on human lives, evidence for cultural emotional relativity might be derived by applying distributional semantics techniques to a text corpus of self-reported behaviour. Here, we explore this idea by measuring the valence and arousal of the twelve most popular emotion keywords expressed on the micro-blogging site Twitter. We do this in three geographical regions: Europe, Asia and North America. We demonstrate that in our sample, the valence and arousal levels of the same emotion keywords differ significantly with respect to these geographical regions --- Europeans are, or at least present themselves as more positive and aroused, North Americans are more negative and Asians appear to be more positive but less aroused when compared to global valence and arousal levels of the same emotion keywords. Our work is the first in kind to programatically map large text corpora to a dimensional model of affect.\n    ",
        "submission_date": "2013-04-28T00:00:00",
        "last_modified_date": "2013-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.7607",
        "title": "A Discrete State Transition Algorithm for Generalized Traveling Salesman Problem",
        "authors": [
            "Xiaolin Tang",
            "Chunhua Yang",
            "Xiaojun Zhou",
            "Weihua Gui"
        ],
        "abstract": "Generalized traveling salesman problem (GTSP) is an extension of classical traveling salesman problem (TSP), which is a combinatorial optimization problem and an NP-hard problem. In this paper, an efficient discrete state transition algorithm (DSTA) for GTSP is proposed, where a new local search operator named \\textit{K-circle}, directed by neighborhood information in space, has been introduced to DSTA to shrink search space and strengthen search ability. A novel robust update mechanism, restore in probability and risk in probability (Double R-Probability), is used in our work to escape from local minima. The proposed algorithm is tested on a set of GTSP instances. Compared with other heuristics, experimental results have demonstrated the effectiveness and strong adaptability of DSTA and also show that DSTA has better search ability than its competitors.\n    ",
        "submission_date": "2013-04-29T00:00:00",
        "last_modified_date": "2013-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.7820",
        "title": "Challenges on Probabilistic Modeling for Evolving Networks",
        "authors": [
            "Jianguo Ding",
            "Pascal Bouvry"
        ],
        "abstract": "With the emerging of new networks, such as wireless sensor networks, vehicle networks, P2P networks, cloud computing, mobile Internet, or social networks, the network dynamics and complexity expands from system design, hardware, software, protocols, structures, integration, evolution, application, even to business goals. Thus the dynamics and uncertainty are unavoidable characteristics, which come from the regular network evolution and unexpected hardware defects, unavoidable software errors, incomplete management information and dependency relationship between the entities among the emerging complex networks. Due to the complexity of emerging networks, it is not always possible to build precise models in modeling and optimization (local and global) for networks. This paper presents a survey on probabilistic modeling for evolving networks and identifies the new challenges which emerge on the probabilistic models and optimization strategies in the potential application areas of network performance, network management and network security for evolving networks.\n    ",
        "submission_date": "2013-04-30T00:00:00",
        "last_modified_date": "2013-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.7855",
        "title": "Enhancements to ACL2 in Versions 5.0, 6.0, and 6.1",
        "authors": [
            "Matt Kaufmann",
            "J Strother Moore"
        ],
        "abstract": "We report on highlights of the ACL2 enhancements introduced in ACL2 releases since the 2011 ACL2 Workshop.  Although many enhancements are critical for soundness or robustness, we focus in this paper on those improvements that could benefit users who are aware of them, but that might not be discovered in everyday practice. \n    ",
        "submission_date": "2013-04-30T00:00:00",
        "last_modified_date": "2013-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.7920",
        "title": "From Ordinary Differential Equations to Structural Causal Models: the deterministic case",
        "authors": [
            "Joris M. Mooij",
            "Dominik Janzing",
            "Bernhard Sch\u00f6lkopf"
        ],
        "abstract": "We show how, and under which conditions, the equilibrium states of a first-order Ordinary Differential Equation (ODE) system can be described with a deterministic Structural Causal Model (SCM). Our exposition sheds more light on the concept of causality as expressed within the framework of Structural Causal Models, especially for cyclic models.\n    ",
        "submission_date": "2013-04-30T00:00:00",
        "last_modified_date": "2013-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.0191",
        "title": "Benefits of Semantics on Web Service Composition from a Complex Network Perspective",
        "authors": [
            "Chantal Cherifi",
            "Vincent Labatut",
            "Jean-Fran\u00e7ois Santucci"
        ],
        "abstract": "The number of publicly available Web services (WS) is continuously growing, and in parallel, we are witnessing a rapid development in semantic-related web technologies. The intersection of the semantic web and WS allows the development of semantic WS. In this work, we adopt a complex network perspective to perform a comparative analysis of the syntactic and semantic approaches used to describe WS. From a collection of publicly available WS descriptions, we extract syntactic and semantic WS interaction networks. We take advantage of tools from the complex network field to analyze them and determine their properties. We show that WS interaction networks exhibit some of the typical characteristics observed in real-world networks, such as short average distance between nodes and community structure. By comparing syntactic and semantic networks through their properties, we show the introduction of semantics in WS descriptions should improve the composition process.\n    ",
        "submission_date": "2013-05-01T00:00:00",
        "last_modified_date": "2013-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.0423",
        "title": "Testing Hypotheses by Regularized Maximum Mean Discrepancy",
        "authors": [
            "Somayeh Danafar",
            "Paola M.V. Rancoita",
            "Tobias Glasmachers",
            "Kevin Whittingstall",
            "Juergen Schmidhuber"
        ],
        "abstract": "Do two data samples come from different distributions? Recent studies of this fundamental problem focused on embedding probability distributions into sufficiently rich characteristic Reproducing Kernel Hilbert Spaces (RKHSs), to compare distributions by the distance between their embeddings. We show that Regularized Maximum Mean Discrepancy (RMMD), our novel measure for kernel-based hypothesis testing, yields substantial improvements even when sample sizes are small, and excels at hypothesis tests involving multiple comparisons with power control. We derive asymptotic distributions under the null and alternative hypotheses, and assess power control. Outstanding results are obtained on: challenging EEG data, MNIST, the Berkley Covertype, and the Flare-Solar dataset.\n    ",
        "submission_date": "2013-05-02T00:00:00",
        "last_modified_date": "2013-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.0626",
        "title": "An Improved EM algorithm",
        "authors": [
            "Fuqiang Chen"
        ],
        "abstract": "In this paper, we firstly give a brief introduction of expectation maximization (EM) algorithm, and then discuss the initial value sensitivity of expectation maximization algorithm. Subsequently, we give a short proof of EM's convergence. Then, we implement experiments with the expectation maximization algorithm (We implement all the experiments on Gaussion mixture model (GMM)). Our experiment with expectation maximization is performed in the following three cases: initialize randomly; initialize with result of K-means; initialize with result of K-medoids. The experiment result shows that expectation maximization algorithm depend on its initial state or parameters. And we found that EM initialized with K-medoids performed better than both the one initialized with K-means and the one initialized randomly.\n    ",
        "submission_date": "2013-05-03T00:00:00",
        "last_modified_date": "2013-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.0751",
        "title": "Marginal AMP Chain Graphs",
        "authors": [
            "Jose M. Pe\u00f1a"
        ],
        "abstract": "We present a new family of models that is based on graphs that may have undirected, directed and bidirected edges. We name these new models marginal AMP (MAMP) chain graphs because each of them is Markov equivalent to some AMP chain graph under marginalization of some of its nodes. However, MAMP chain graphs do not only subsume AMP chain graphs but also multivariate regression chain graphs. We describe global and pairwise Markov properties for MAMP chain graphs and prove their equivalence for compositional graphoids. We also characterize when two MAMP chain graphs are Markov equivalent.\n",
        "submission_date": "2013-05-03T00:00:00",
        "last_modified_date": "2014-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.1578",
        "title": "Projective simulation for classical learning agents: a comprehensive investigation",
        "authors": [
            "Julian Mautner",
            "Adi Makmal",
            "Daniel Manzano",
            "Markus Tiersch",
            "Hans J. Briegel"
        ],
        "abstract": "We study the model of projective simulation (PS), a novel approach to artificial intelligence based on stochastic processing of episodic memory which was recently introduced [H.J. Briegel and G. De las Cuevas. Sci. Rep. 2, 400, (2012)]. Here we provide a detailed analysis of the model and examine its performance, including its achievable efficiency, its learning times and the way both properties scale with the problems' dimension. In addition, we situate the PS agent in different learning scenarios, and study its learning abilities. A variety of new scenarios are being considered, thereby demonstrating the model's flexibility. Furthermore, to put the PS scheme in context, we compare its performance with those of Q-learning and learning classifier systems, two popular models in the field of reinforcement learning. It is shown that PS is a competitive artificial intelligence model of unique properties and strengths.\n    ",
        "submission_date": "2013-05-07T00:00:00",
        "last_modified_date": "2014-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.1690",
        "title": "Unsatisfiable Cores for Constraint Programming",
        "authors": [
            "Nicholas Downing",
            "Thibaut Feydy",
            "Peter J. Stuckey"
        ],
        "abstract": "Constraint Programming (CP) solvers typically tackle optimization problems by repeatedly finding solutions to a problem while placing tighter and tighter bounds on the solution cost. This approach is somewhat naive, especially for soft-constraint optimization problems in which the soft constraints are mostly satisfied. Unsatisfiable-core approaches to solving soft constraint problems in Boolean Satisfiability (e.g. MAXSAT) force all soft constraints to hold initially. When solving fails they return an unsatisfiable core, as a set of soft constraints that cannot hold simultaneously. Using this information the problem is relaxed to allow certain soft constraint(s) to be violated and solving continues. Since Lazy Clause Generation (LCG) solvers can also return unsatisfiable cores we can adapt the MAXSAT unsatisfiable core approach to CP. We implement the original MAXSAT unsatisfiable core solving algorithms WPM1, MSU3 in a state-of-the-art LCG solver and show that there exist problems which benefit from this hybrid approach.\n    ",
        "submission_date": "2013-05-08T00:00:00",
        "last_modified_date": "2013-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.1704",
        "title": "The Extended Parameter Filter",
        "authors": [
            "Yusuf Erol",
            "Lei Li",
            "Bharath Ramsundar",
            "Stuart J. Russell"
        ],
        "abstract": "The parameters of temporal models, such as dynamic Bayesian networks, may be modelled in a Bayesian context as static or atemporal variables that influence transition probabilities at every time step. Particle filters fail for models that include such variables, while methods that use Gibbs sampling of parameter variables may incur a per-sample cost that grows linearly with the length of the observation sequence. Storvik devised a method for incremental computation of exact sufficient statistics that, for some cases, reduces the per-sample cost to a constant. In this paper, we demonstrate a connection between Storvik's filter and a Kalman filter in parameter space and establish more general conditions under which Storvik's filter works. Drawing on an analogy to the extended Kalman filter, we develop and analyze, both theoretically and experimentally, a Taylor approximation to the parameter posterior that allows Storvik's method to be applied to a broader class of models. Our experiments on both synthetic examples and real applications show improvement over existing methods.\n    ",
        "submission_date": "2013-05-08T00:00:00",
        "last_modified_date": "2013-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.2038",
        "title": "A Rank Minrelation - Majrelation Coefficient",
        "authors": [
            "Patrick E. Meyer"
        ],
        "abstract": "Improving the detection of relevant variables using a new bivariate measure could importantly impact variable selection and large network inference methods. In this paper, we propose a new statistical coefficient that we call the rank minrelation coefficient. We define a minrelation of X to Y (or equivalently a majrelation of Y to X) as a measure that estimate p(Y > X) when X and Y are continuous random variables. The approach is similar to Lin's concordance coefficient that rather focuses on estimating p(X = Y). In other words, if a variable X exhibits a minrelation to Y then, as X increases, Y is likely to increases too. However, on the contrary to concordance or correlation, the minrelation is not symmetric. More explicitly, if X decreases, little can be said on Y values (except that the uncertainty on Y actually increases). In this paper, we formally define this new kind of bivariate dependencies and propose a new statistical coefficient in order to detect those dependencies. We show through several key examples that this new coefficient has many interesting properties in order to select relevant variables, in particular when compared to correlation.\n    ",
        "submission_date": "2013-05-09T00:00:00",
        "last_modified_date": "2013-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.2218",
        "title": "Stochastic gradient descent algorithms for strongly convex functions at O(1/T) convergence rates",
        "authors": [
            "Shenghuo Zhu"
        ],
        "abstract": "With a weighting scheme proportional to t, a traditional stochastic gradient descent (SGD) algorithm achieves a high probability convergence rate of O({\\kappa}/T) for strongly convex functions, instead of O({\\kappa} ln(T)/T). We also prove that an accelerated SGD algorithm also achieves a rate of O({\\kappa}/T).\n    ",
        "submission_date": "2013-05-09T00:00:00",
        "last_modified_date": "2013-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.2299",
        "title": "Fast Collision Checking: From Single Robots to Multi-Robot Teams",
        "authors": [
            "Joshua Bialkowski",
            "Michael Otte",
            "Emilio Frazzoli"
        ],
        "abstract": "We examine three different algorithms that enable the collision certificate method from [Bialkowski, et al.] to handle the case of a centralized multi-robot team. By taking advantage of symmetries in the configuration space of multi-robot teams, our methods can significantly reduce the number of collision checks vs. both [Bialkowski, et al.] and standard collision checking implementations.\n    ",
        "submission_date": "2013-05-10T00:00:00",
        "last_modified_date": "2013-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.2752",
        "title": "Hybrid fuzzy logic and pid controller based ph neutralization pilot plant",
        "authors": [
            "Oumair Naseer",
            "Atif Ali Khan"
        ],
        "abstract": "Use of Control theory within process control industries has changed rapidly due to the increase complexity of instrumentation, real time requirements, minimization of operating costs and highly nonlinear characteristics of chemical process. Previously developed process control technologies which are mostly based on a single controller are not efficient in terms of signal transmission delays, processing power for computational needs and signal to noise ratio. Hybrid controller with efficient system modelling is essential to cope with the current challenges of process control in terms of control performance. This paper presents an optimized mathematical modelling and advance hybrid controller (Fuzzy Logic and PID) design along with practical implementation and validation of pH neutralization pilot plant. This procedure is particularly important for control design and automation of Physico-chemical systems for process control industry.\n    ",
        "submission_date": "2013-05-13T00:00:00",
        "last_modified_date": "2013-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.4228",
        "title": "The state-of-the-art in web-scale semantic information processing for cloud computing",
        "authors": [
            "Wei Yu",
            "Junpeng Chen"
        ],
        "abstract": "Based on integrated infrastructure of resource sharing and computing in distributed environment, cloud computing involves the provision of dynamically scalable and provides virtualized resources as services over the Internet. These applications also bring a large scale heterogeneous and distributed information which pose a great challenge in terms of the semantic ambiguity. It is critical for application services in cloud computing environment to provide users intelligent service and precise information. Semantic information processing can help users deal with semantic ambiguity and information overload efficiently through appropriate semantic models and semantic information processing technology. The semantic information processing have been successfully employed in many fields such as the knowledge representation, natural language understanding, intelligent web search, etc. The purpose of this report is to give an overview of existing technologies for semantic information processing in cloud computing environment, to propose a research direction for addressing distributed semantic reasoning and parallel semantic computing by exploiting semantic information newly available in cloud computing environment.\n    ",
        "submission_date": "2013-05-18T00:00:00",
        "last_modified_date": "2013-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.4455",
        "title": "SHARE: A Web Service Based Framework for Distributed Querying and Reasoning on the Semantic Web",
        "authors": [
            "Ben P Vandervalk",
            "E Luke McCarthy",
            "Mark D Wilkinson"
        ],
        "abstract": "Here we describe the SHARE system, a web service based framework for distributed querying and reasoning on the semantic web. The main innovations of SHARE are: (1) the extension of a SPARQL query engine to perform on-demand data retrieval from web services, and (2) the extension of an OWL reasoner to test property restrictions by means of web service invocations. In addition to enabling queries across distributed datasets, the system allows for a target dataset that is significantly larger than is possible under current, centralized approaches. Although the architecture is equally applicable to all types of data, the SHARE system targets bioinformatics, due to the large number of interoperable web services that are already available in this area. SHARE is built entirely on semantic web standards, and is the successor of the BioMOBY project.\n    ",
        "submission_date": "2013-05-20T00:00:00",
        "last_modified_date": "2013-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.5827",
        "title": "Semantic Web Search based on Ontology Modeling using Protege Reasoner",
        "authors": [
            "Monica Shekhar",
            "Saravanaguru RA. K"
        ],
        "abstract": "The Semantic Web works on the existing Web which presents the meaning of information as well-defined vocabularies understood by the people. Semantic Search, at the same time, works on improving the accuracy if a search by understanding the intent of the search and providing contextually relevant results. This paper describes a semantic approach toward web search through a PHP application. The goal was to parse through a user's browsing history and return semantically relevant web pages for the search query provided.\n    ",
        "submission_date": "2013-05-24T00:00:00",
        "last_modified_date": "2013-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.6129",
        "title": "Information-Theoretic Approach to Efficient Adaptive Path Planning for Mobile Robotic Environmental Sensing",
        "authors": [
            "Kian Hsiang Low",
            "John M. Dolan",
            "Pradeep Khosla"
        ],
        "abstract": "Recent research in robot exploration and mapping has focused on sampling environmental hotspot fields. This exploration task is formalized by Low, Dolan, and Khosla (2008) in a sequential decision-theoretic planning under uncertainty framework called MASP. The time complexity of solving MASP approximately depends on the map resolution, which limits its use in large-scale, high-resolution exploration and mapping. To alleviate this computational difficulty, this paper presents an information-theoretic approach to MASP (iMASP) for efficient adaptive path planning; by reformulating the cost-minimizing iMASP as a reward-maximizing problem, its time complexity becomes independent of map resolution and is less sensitive to increasing robot team size as demonstrated both theoretically and empirically. Using the reward-maximizing dual, we derive a novel adaptive variant of maximum entropy sampling, thus improving the induced exploration policy performance. It also allows us to establish theoretical bounds quantifying the performance advantage of optimal adaptive over non-adaptive policies and the performance quality of approximately optimal vs. optimal adaptive policies. We show analytically and empirically the superior performance of iMASP-based policies for sampling the log-Gaussian process to that of policies for the widely-used Gaussian process in mapping the hotspot field. Lastly, we provide sufficient conditions that, when met, guarantee adaptivity has no benefit under an assumed environment model.\n    ",
        "submission_date": "2013-05-27T00:00:00",
        "last_modified_date": "2013-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.6537",
        "title": "A Cooperative Coevolutionary Genetic Algorithm for Learning Bayesian Network Structures",
        "authors": [
            "Arthur Carvalho"
        ],
        "abstract": "We propose a cooperative coevolutionary genetic algorithm for learning Bayesian network structures from fully observable data sets. Since this problem can be decomposed into two dependent subproblems, that is to find an ordering of the nodes and an optimal connectivity matrix, our algorithm uses two subpopulations, each one representing a subtask. We describe the empirical results obtained with simulations of the Alarm and Insurance networks. We show that our algorithm outperforms the deterministic algorithm K2.\n    ",
        "submission_date": "2013-05-28T00:00:00",
        "last_modified_date": "2013-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.7196",
        "title": "For a Semantic Web based Peer-reviewing and Publication of Research Results",
        "authors": [
            "Philippe A. Martin"
        ],
        "abstract": "This article shows why the diffusion and peer-reviewing of research results would be more efficient, precise and relevant if all or at least some parts of the descriptions and peer-reviews of research results took the form of a fine-grained semantic network, within articles or knowledge bases, as part of the Semantic Web. This article also shows some ways this can be done and hence how research journal/proceeding publishers could allow this. So far, the World Wide Web Consortium (W3C) has not proposed simple notations and cooperation protocols - similar to those illustrated or referred to in this article - but it now seems likely that Wikipedia/Wikidata, Google or the W3C will propose them sooner or later. Then, research journal/proceeding publishers and researchers may or may not quickly use this approach.\n    ",
        "submission_date": "2013-05-30T00:00:00",
        "last_modified_date": "2013-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.7200",
        "title": "Organizing Linked Data Quality Related Methods",
        "authors": [
            "Philippe A. Martin"
        ],
        "abstract": "This article presents the top-level of an ontology categorizing and generalizing best practices and quality criteria or measures for Linked Data. It permits to compare these techniques and have a synthetic organized view of what can or should be done for knowledge sharing purposes. This ontology is part of a general knowledge base that can be accessed and complemented by any Web user. Thus, it can be seen as a cooperatively built library for the above cited elements. Since they permit to evaluate information objects and create better ones, these elements also permit knowledge-based tools and techniques - as well as knowledge providers - to be evaluated and categorized based on their input/output information objects. One top-level distinction permitting to organize this ontology is the one between content, medium and containers of descriptions. Various structural, ontological, syntactical and lexical distinctions are then used.\n    ",
        "submission_date": "2013-05-30T00:00:00",
        "last_modified_date": "2013-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.7437",
        "title": "Modelling Electricity Consumption in Office Buildings: An Agent Based Approach",
        "authors": [
            "Tao Zhang",
            "Peer-Olaf Siebers",
            "Uwe Aickelin"
        ],
        "abstract": "In this paper, we develop an agent-based model which integrates four important elements, i.e. organisational energy management policies/regulations, energy management technologies, electric appliances and equipment, and human behaviour, to simulate the electricity consumption in office buildings. Based on a case study, we use this model to test the effectiveness of different electricity management strategies, and solve practical office electricity consumption problems. This paper theoretically contributes to an integration of the four elements involved in the complex organisational issue of office electricity consumption, and practically contributes to an application of an agent-based approach for office building electricity consumption study.\n    ",
        "submission_date": "2013-05-31T00:00:00",
        "last_modified_date": "2013-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.7458",
        "title": "Validation of a Microsimulation of the Port of Dover",
        "authors": [
            "Chris Roadknight",
            "Uwe Aickelin",
            "Galina Sherman"
        ],
        "abstract": "Modelling and simulating the traffic of heavily used but secure environments such as seaports and airports is of increasing importance. Errors made when simulating these environments can have long standing economic, social and environmental implications. This paper discusses issues and problems that may arise when designing a simulation strategy. Data for the Port is presented, methods for lightweight vehicle assessment that can be used to calibrate and validate simulations are also discussed along with a diagnosis of overcalibration issues. We show that decisions about where the intelligence lies in a system has important repercussions for the reliability of system statistics. Finally, conclusions are drawn about how microsimulations can be moved forward as a robust planning tool for the 21st century.\n    ",
        "submission_date": "2013-05-31T00:00:00",
        "last_modified_date": "2013-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.7471",
        "title": "Investigating Mathematical Models of Immuno-Interactions with Early-Stage Cancer under an Agent-Based Modelling Perspective",
        "authors": [
            "Grazziela P. Figueredo",
            "Peer-Olaf Siebers",
            "Uwe Aickelin"
        ],
        "abstract": "Many advances in research regarding immuno-interactions with cancer were developed with the help of ordinary differential equation (ODE) models. These models, however, are not effectively capable of representing problems involving individual localisation, memory and emerging properties, which are common characteristics of cells and molecules of the immune system. Agent-based modelling and simulation is an alternative paradigm to ODE models that overcomes these limitations. In this paper we investigate the potential contribution of agent-based modelling and simulation when compared to ODE modelling and simulation. We seek answers to the following questions: Is it possible to obtain an equivalent agent-based model from the ODE formulation? Do the outcomes differ? Are there any benefits of using one method compared to the other? To answer these questions, we have considered three case studies using established mathematical models of immune interactions with early-stage cancer. These case studies were re-conceptualised under an agent-based perspective and the simulation results were then compared with those from the ODE models. Our results show that it is possible to obtain equivalent agent-based models (i.e. implementing the same mechanisms); the simulation output of both types of models however might differ depending on the attributes of the system to be modelled. In some cases, additional insight from using agent-based modelling was obtained. Overall, we can confirm that agent-based modelling is a useful addition to the tool set of immunologists, as it has extra features that allow for simulations with characteristics that are closer to the biological phenomena.\n    ",
        "submission_date": "2013-05-31T00:00:00",
        "last_modified_date": "2013-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.0386",
        "title": "Improved and Generalized Upper Bounds on the Complexity of Policy Iteration",
        "authors": [
            "Bruno Scherrer"
        ],
        "abstract": "Given a Markov Decision Process (MDP) with $n$ states and a totalnumber $m$ of actions, we study the number of iterations needed byPolicy Iteration (PI) algorithms to converge to the optimal$\\gamma$-discounted policy. We consider two variations of PI: Howard'sPI that changes the actions in all states with a positive advantage,and Simplex-PI that only changes the action in the state with maximaladvantage.  We show that Howard's PI terminates after at most $O\\left(\\frac{m}{1-\\gamma}\\log\\left(\\frac{1}{1-\\gamma}\\right)\\right)$iterations, improving by a factor $O(\\log n)$ a result by Hansen etal., while Simplex-PI terminates after at most $O\\left(\\frac{nm}{1-\\gamma}\\log\\left(\\frac{1}{1-\\gamma}\\right)\\right)$iterations, improving by a factor $O(\\log n)$ a result by Ye. Undersome structural properties of the MDP, we then consider bounds thatare independent of the discount factor~$\\gamma$: quantities ofinterest are bounds $\\tau\\_t$ and $\\tau\\_r$---uniform on all states andpolicies---respectively on the \\emph{expected time spent in transientstates} and \\emph{the inverse of the frequency of visits in recurrentstates} given that the process starts from the uniform ",
        "submission_date": "2013-06-03T00:00:00",
        "last_modified_date": "2016-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.0686",
        "title": "Online Learning under Delayed Feedback",
        "authors": [
            "Pooria Joulani",
            "Andr\u00e1s Gy\u00f6rgy",
            "Csaba Szepesv\u00e1ri"
        ],
        "abstract": "Online learning with delayed feedback has received increasing attention recently due to its several applications in distributed, web-based learning problems. In this paper we provide a systematic study of the topic, and analyze the effect of delay on the regret of online learning algorithms. Somewhat surprisingly, it turns out that delay increases the regret in a multiplicative way in adversarial problems, and in an additive way in stochastic problems. We give meta-algorithms that transform, in a black-box fashion, algorithms developed for the non-delayed case into ones that can handle the presence of delays in the feedback loop. Modifications of the well-known UCB algorithm are also developed for the bandit problem with delayed feedback, with the advantage over the meta-algorithms that they can be implemented with lower complexity.\n    ",
        "submission_date": "2013-06-04T00:00:00",
        "last_modified_date": "2013-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.0694",
        "title": "Iterated Tabu Search Algorithm for Packing Unequal Circles in a Circle",
        "authors": [
            "Tao Ye",
            "Wenqi Huang",
            "Zhipeng Lu"
        ],
        "abstract": "This paper presents an Iterated Tabu Search algorithm (denoted by ITS-PUCC) for solving the problem of Packing Unequal Circles in a Circle. The algorithm exploits the continuous and combinatorial nature of the unequal circles packing problem. It uses a continuous local optimization method to generate locally optimal packings. Meanwhile, it builds a neighborhood structure on the set of local minimum via two appropriate perturbation moves and integrates two combinatorial optimization methods, Tabu Search and Iterated Local Search, to systematically search for good local minima. Computational experiments on two sets of widely-used test instances prove its effectiveness and efficiency. For the first set of 46 instances coming from the famous circle packing contest and the second set of 24 instances widely used in the literature, the algorithm is able to discover respectively 14 and 16 better solutions than the previous best-known records.\n    ",
        "submission_date": "2013-06-04T00:00:00",
        "last_modified_date": "2013-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.1267",
        "title": "Loop Calculus and Bootstrap-Belief Propagation for Perfect Matchings on Arbitrary Graphs",
        "authors": [
            "Michael Chertkov",
            "Andrew Gelfand",
            "Jinwoo Shin"
        ],
        "abstract": "This manuscript discusses computation of the Partition Function (PF) and the Minimum Weight Perfect Matching (MWPM) on arbitrary, non-bipartite graphs. We present two novel problem formulations - one for computing the PF of a Perfect Matching (PM) and one for finding MWPMs - that build upon the inter-related Bethe Free Energy, Belief Propagation (BP), Loop Calculus (LC), Integer Linear Programming (ILP) and Linear Programming (LP) frameworks. First, we describe an extension of the LC framework to the PM problem. The resulting formulas, coined (fractional) Bootstrap-BP, express the PF of the original model via the BFE of an alternative PM problem. We then study the zero-temperature version of this Bootstrap-BP formula for approximately solving the MWPM problem. We do so by leveraging the Bootstrap-BP formula to construct a sequence of MWPM problems, where each new problem in the sequence is formed by contracting odd-sized cycles (or blossoms) from the previous problem. This Bootstrap-and-Contract procedure converges reliably and generates an empirically tight upper bound for the MWPM. We conclude by discussing the relationship between our iterative procedure and the famous Blossom Algorithm of Edmonds '65 and demonstrate the performance of the Bootstrap-and-Contract approach on a variety of weighted PM problems.\n    ",
        "submission_date": "2013-06-05T00:00:00",
        "last_modified_date": "2013-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.1421",
        "title": "Bayesian Inference of Natural Rankings in Incomplete Competition Networks",
        "authors": [
            "Juyong Park",
            "Soon-Hyung Yook"
        ],
        "abstract": "Competition between a complex system's constituents and a corresponding reward mechanism based on it have profound influence on the functioning, stability, and evolution of the system. But determining the dominance hierarchy or ranking among the constituent parts from the strongest to the weakest -- essential in determining reward or penalty -- is almost always an ambiguous task due to the incomplete nature of competition networks. Here we introduce ``Natural Ranking,\" a desirably unambiguous ranking method applicable to a complete (full) competition network, and formulate an analytical model based on the Bayesian formula inferring the expected mean and error of the natural ranking of nodes from an incomplete network. We investigate its potential and uses in solving issues in ranking by applying to a real-world competition network of economic and social importance.\n    ",
        "submission_date": "2013-06-06T00:00:00",
        "last_modified_date": "2013-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.1520",
        "title": "Policy Search: Any Local Optimum Enjoys a Global Performance Guarantee",
        "authors": [
            "Bruno Scherrer",
            "Matthieu Geist"
        ],
        "abstract": "Local Policy Search is a popular reinforcement learning approach for handling large state spaces. Formally, it searches locally in a paramet erized policy space in order to maximize the associated value function averaged over some predefined distribution. It is probably commonly b elieved that the best one can hope in general from such an approach is to get a local optimum of this criterion. In this article, we show th e following surprising result: \\emph{any} (approximate) \\emph{local optimum} enjoys a \\emph{global performance guarantee}. We compare this g uarantee with the one that is satisfied by Direct Policy Iteration, an approximate dynamic programming algorithm that does some form of Poli cy Search: if the approximation error of Local Policy Search may generally be bigger (because local search requires to consider a space of s tochastic policies), we argue that the concentrability coefficient that appears in the performance bound is much nicer. Finally, we discuss several practical and theoretical consequences of our analysis.\n    ",
        "submission_date": "2013-06-06T00:00:00",
        "last_modified_date": "2013-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.1849",
        "title": "New Results on Equilibria in Strategic Candidacy",
        "authors": [
            "J\u00e9r\u00f4me Lang",
            "Nicolas Maudet",
            "Maria Polukarov",
            "Alice Cohen-Hadria"
        ],
        "abstract": "We consider a voting setting where candidates have preferences about the outcome of the election and are free to join or leave the election. The corresponding candidacy game, where candidates choose strategically to participate or not, has been studied %initially by Dutta et al., who showed that no non-dictatorial voting procedure satisfying unanimity is candidacy-strategyproof, that is, is such that the joint action where all candidates enter the election is always a pure strategy Nash equilibrium. Dutta et al. also showed that for some voting tree procedures, there are candidacy games with no pure Nash equilibria, and that for the rule that outputs the sophisticated winner of voting by successive elimination, all games have a pure Nash equilibrium. No results were known about other voting rules. Here we prove several such results. For four candidates, the message is, roughly, that most scoring rules (with the exception of Borda) do not guarantee the existence of a pure Nash equilibrium but that Condorcet-consistent rules, for an odd number of voters, do. For five candidates, most rules we study no longer have this guarantee. Finally, we identify one prominent rule that guarantees the existence of a pure Nash equilibrium for any number of candidates (and for an odd number of voters): the Copeland rule. We also show that under mild assumptions on the voting rule, the existence of strong equilibria cannot be guaranteed.\n    ",
        "submission_date": "2013-06-07T00:00:00",
        "last_modified_date": "2016-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.4040",
        "title": "An Algorithm to Find Optimal Attack Paths in Nondeterministic Scenarios",
        "authors": [
            "Carlos Sarraute",
            "Gerardo Richarte",
            "Jorge Lucangeli Obes"
        ],
        "abstract": "As penetration testing frameworks have evolved and have become more complex, the problem of controlling automatically the pentesting tool has become an important question. This can be naturally addressed as an attack planning problem. Previous approaches to this problem were based on modeling the actions and assets in the PDDL language, and using off-the-shelf AI tools to generate attack plans. These approaches however are limited. In particular, the planning is classical (the actions are deterministic) and thus not able to handle the uncertainty involved in this form of attack planning.\n",
        "submission_date": "2013-06-17T00:00:00",
        "last_modified_date": "2013-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.4044",
        "title": "Attack Planning in the Real World",
        "authors": [
            "Jorge Lucangeli Obes",
            "Carlos Sarraute",
            "Gerardo Richarte"
        ],
        "abstract": "Assessing network security is a complex and difficult task. Attack graphs have been proposed as a tool to help network administrators understand the potential weaknesses of their network. However, a problem has not yet been addressed by previous work on this subject; namely, how to actually execute and validate the attack paths resulting from the analysis of the attack graph. In this paper we present a complete PDDL representation of an attack model, and an implementation that integrates a planner into a penetration testing tool. This allows to automatically generate attack paths for penetration testing scenarios, and to validate these attacks by executing the corresponding actions -including exploits- against the real target network. We present an algorithm for transforming the information present in the penetration testing tool to the planning domain, and show how the scalability issues of attack graphs can be solved using current planners. We include an analysis of the performance of our solution, showing how our model scales to medium-sized networks and the number of actions available in current penetration testing tools.\n    ",
        "submission_date": "2013-06-18T00:00:00",
        "last_modified_date": "2013-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.4532",
        "title": "Verifying the Steane code with Quantomatic",
        "authors": [
            "Ross Duncan",
            "Maxime Lucas"
        ],
        "abstract": "In this paper we give a partially mechanized proof of the correctness of Steane's 7-qubit error correcting code, using the tool Quantomatic. To the best of our knowledge, this represents the largest and most complicated verification task yet carried out using Quantomatic.\n    ",
        "submission_date": "2013-06-19T00:00:00",
        "last_modified_date": "2014-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.4753",
        "title": "Galerkin Methods for Complementarity Problems and Variational Inequalities",
        "authors": [
            "Geoffrey J. Gordon"
        ],
        "abstract": "Complementarity problems and variational inequalities arise in a wide variety of areas, including machine learning, planning, game theory, and physical simulation. In all of these areas, to handle large-scale problem instances, we need fast approximate solution methods. One promising idea is Galerkin approximation, in which we search for the best answer within the span of a given set of basis functions. Bertsekas proposed one possible Galerkin method for variational inequalities. However, this method can exhibit two problems in practice: its approximation error is worse than might be expected based on the ability of the basis to represent the desired solution, and each iteration requires a projection step that is not always easy to implement efficiently. So, in this paper, we present a new Galerkin method with improved behavior: our new error bounds depend directly on the distance from the true solution to the subspace spanned by our basis, and the only projections we require are onto the feasible region or onto the span of our basis.\n    ",
        "submission_date": "2013-06-20T00:00:00",
        "last_modified_date": "2013-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.4999",
        "title": "Safeguarding E-Commerce against Advisor Cheating Behaviors: Towards More Robust Trust Models for Handling Unfair Ratings",
        "authors": [
            "Lizi Zhang"
        ],
        "abstract": "In electronic marketplaces, after each transaction buyers will rate the products provided by the sellers. To decide the most trustworthy sellers to transact with, buyers rely on trust models to leverage these ratings to evaluate the reputation of sellers. Although the high effectiveness of different trust models for handling unfair ratings have been claimed by their designers, recently it is argued that these models are vulnerable to more intelligent attacks, and there is an urgent demand that the robustness of the existing trust models has to be evaluated in a more comprehensive way. In this work, we classify the existing trust models into two broad categories and propose an extendable e-marketplace testbed to evaluate their robustness against different unfair rating attacks comprehensively. On top of highlighting the robustness of the existing trust models for handling unfair ratings is far from what they were claimed to be, we further propose and validate a novel combination mechanism for the existing trust models, Discount-then-Filter, to notably enhance their robustness against the investigated attacks.\n    ",
        "submission_date": "2013-06-20T00:00:00",
        "last_modified_date": "2013-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.5215",
        "title": "Epistemology of Modeling and Simulation: How can we gain Knowledge from Simulations?",
        "authors": [
            "Andreas Tolk",
            "Saikou Y. Diallo",
            "Jose J. Padilla",
            "Ross Gore"
        ],
        "abstract": "Epistemology is the branch of philosophy that deals with gaining knowledge. It is closely related to ontology. The branch that deals with questions like \"What is real?\" and \"What do we know?\" as it provides these components. When using modeling and simulation, we usually imply that we are doing so to either apply knowledge, in particular when we are using them for training and teaching, or that we want to gain new knowledge, for example when doing analysis or conducting virtual experiments. This paper looks at the history of science to give a context to better cope with the question, how we can gain knowledge from simulation. It addresses aspects of computability and the general underlying mathematics, and applies the findings to validation and verification and development of federations. As simulations are understood as computable executable hypotheses, validation can be understood as hypothesis testing and theory building. The mathematical framework allows furthermore addressing some challenges when developing federations and the potential introduction of contradictions when composing different theories, as they are represented by the federated simulation systems.\n    ",
        "submission_date": "2013-06-21T00:00:00",
        "last_modified_date": "2013-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.5279",
        "title": "Affect Control Processes: Intelligent Affective Interaction using a Partially Observable Markov Decision Process",
        "authors": [
            "Jesse Hoey",
            "Tobias Schroeder",
            "Areej Alhothali"
        ],
        "abstract": "This paper describes a novel method for building affectively intelligent human-interactive agents. The method is based on a key sociological insight that has been developed and extensively verified over the last twenty years, but has yet to make an impact in artificial intelligence. The insight is that resource bounded humans will, by default, act to maintain affective consistency. Humans have culturally shared fundamental affective sentiments about identities, behaviours, and objects, and they act so that the transient affective sentiments created during interactions confirm the fundamental sentiments. Humans seek and create situations that confirm or are consistent with, and avoid and supress situations that disconfirm or are inconsistent with, their culturally shared affective sentiments. This \"affect control principle\" has been shown to be a powerful predictor of human behaviour. In this paper, we present a probabilistic and decision-theoretic generalisation of this principle, and we demonstrate how it can be leveraged to build affectively intelligent artificial agents. The new model, called BayesAct, can maintain multiple hypotheses about sentiments simultaneously as a probability distribution, and can make use of an explicit utility function to make value-directed action choices. This allows the model to generate affectively intelligent interactions with people by learning about their identity, predicting their behaviours using the affect control principle, and taking actions that are simultaneously goal-directed and affect-sensitive. We demonstrate this generalisation with a set of simulations. We then show how our model can be used as an emotional \"plug-in\" for artificially intelligent systems that interact with humans in two different settings: an exam practice assistant (tutor) and an assistive device for persons with a cognitive disability.\n    ",
        "submission_date": "2013-06-22T00:00:00",
        "last_modified_date": "2014-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.5667",
        "title": "Using Genetic Programming to Model Software",
        "authors": [
            "W. B. Langdon",
            "M. Harman"
        ],
        "abstract": "We study a generic program to investigate the scope for automatically customising it for a vital current task, which was not considered when it was first written. In detail, we show genetic programming (GP) can evolve models of aspects of BLAST's output when it is used to map Solexa Next-Gen DNA sequences to the human genome.\n    ",
        "submission_date": "2013-06-24T00:00:00",
        "last_modified_date": "2013-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.5707",
        "title": "Synthesizing Manipulation Sequences for Under-Specified Tasks using Unrolled Markov Random Fields",
        "authors": [
            "Jaeyong Sung",
            "Bart Selman",
            "Ashutosh Saxena"
        ],
        "abstract": "Many tasks in human environments require performing a sequence of navigation and manipulation steps involving objects. In unstructured human environments, the location and configuration of the objects involved often change in unpredictable ways. This requires a high-level planning strategy that is robust and flexible in an uncertain environment. We propose a novel dynamic planning strategy, which can be trained from a set of example sequences. High level tasks are expressed as a sequence of primitive actions or controllers (with appropriate parameters). Our score function, based on Markov Random Field (MRF), captures the relations between environment, controllers, and their arguments. By expressing the environment using sets of attributes, the approach generalizes well to unseen scenarios. We train the parameters of our MRF using a maximum margin learning method. We provide a detailed empirical validation of our overall framework demonstrating successful plan strategies for a variety of tasks.\n    ",
        "submission_date": "2013-06-24T00:00:00",
        "last_modified_date": "2014-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.6294",
        "title": "Learning Trajectory Preferences for Manipulators via Iterative Improvement",
        "authors": [
            "Ashesh Jain",
            "Brian Wojcik",
            "Thorsten Joachims",
            "Ashutosh Saxena"
        ],
        "abstract": "We consider the problem of learning good trajectories for manipulation tasks. This is challenging because the criterion defining a good trajectory varies with users, tasks and environments. In this paper, we propose a co-active online learning framework for teaching robots the preferences of its users for object manipulation tasks. The key novelty of our approach lies in the type of feedback expected from the user: the human user does not need to demonstrate optimal trajectories as training data, but merely needs to iteratively provide trajectories that slightly improve over the trajectory currently proposed by the system. We argue that this co-active preference feedback can be more easily elicited from the user than demonstrations of optimal trajectories, which are often challenging and non-intuitive to provide on high degrees of freedom manipulators. Nevertheless, theoretical regret bounds of our algorithm match the asymptotic rates of optimal trajectory algorithms. We demonstrate the generalizability of our algorithm on a variety of grocery checkout tasks, for whom, the preferences were not only influenced by the object being manipulated but also by the surrounding environment.\\footnote{For more details and a demonstration video, visit: \\url{",
        "submission_date": "2013-06-26T00:00:00",
        "last_modified_date": "2013-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.6709",
        "title": "A Survey on Metric Learning for Feature Vectors and Structured Data",
        "authors": [
            "Aur\u00e9lien Bellet",
            "Amaury Habrard",
            "Marc Sebban"
        ],
        "abstract": "The need for appropriate ways to measure the distance or similarity between data is ubiquitous in machine learning, pattern recognition and data mining, but handcrafting such good metrics for specific problems is generally difficult. This has led to the emergence of metric learning, which aims at automatically learning a metric from data and has attracted a lot of interest in machine learning and related fields for the past ten years. This survey paper proposes a systematic review of the metric learning literature, highlighting the pros and cons of each approach. We pay particular attention to Mahalanobis distance metric learning, a well-studied and successful framework, but additionally present a wide range of methods that have recently emerged as powerful alternatives, including nonlinear metric learning, similarity learning and local metric learning. Recent trends and extensions, such as semi-supervised metric learning, metric learning for histogram data and the derivation of generalization guarantees, are also covered. Finally, this survey addresses metric learning for structured data, in particular edit distance learning, and attempts to give an overview of the remaining challenges in metric learning for the years to come.\n    ",
        "submission_date": "2013-06-28T00:00:00",
        "last_modified_date": "2014-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.6843",
        "title": "Error AMP Chain Graphs",
        "authors": [
            "Jose M. Pe\u00f1a"
        ],
        "abstract": "Any regular Gaussian probability distribution that can be represented by an AMP chain graph (CG) can be expressed as a system of linear equations with correlated errors whose structure depends on the CG. However, the CG represents the errors implicitly, as no nodes in the CG correspond to the errors. We propose in this paper to add some deterministic nodes to the CG in order to represent the errors explicitly. We call the result an EAMP CG. We will show that, as desired, every AMP CG is Markov equivalent to its corresponding EAMP CG under marginalization of the error nodes. We will also show that every EAMP CG under marginalization of the error nodes is Markov equivalent to some LWF CG under marginalization of the error nodes, and that the latter is Markov equivalent to some directed and acyclic graph (DAG) under marginalization of the error nodes and conditioning on some selection nodes. This is important because it implies that the independence model represented by an AMP CG can be accounted for by some data generating process that is partially observed and has selection bias. Finally, we will show that EAMP CGs are closed under marginalization. This is a desirable feature because it guarantees parsimonious models under marginalization.\n    ",
        "submission_date": "2013-06-28T00:00:00",
        "last_modified_date": "2013-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.0024",
        "title": "Investigation of \"Enhancing flexibility and robustness in multi-agent task scheduling\"",
        "authors": [
            "Daan Wilmer"
        ],
        "abstract": "Wilson et al. propose a measure of flexibility in project scheduling problems and propose several ways of distributing flexibility over tasks without overrunning the deadline. These schedules prove quite robust: delays of some tasks do not necessarily lead to delays of subsequent tasks. The number of tasks that finish late depends, among others, on the way of distributing flexibility.\n",
        "submission_date": "2013-06-28T00:00:00",
        "last_modified_date": "2013-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.0201",
        "title": "Simulating Ability: Representing Skills in Games",
        "authors": [
            "Magnus Lie Hetland"
        ],
        "abstract": "Throughout the history of games, representing the abilities of the various agents acting on behalf of the players has been a central concern. With increasingly sophisticated games emerging, these simulations have become more realistic, but the underlying mechanisms are still, to a large extent, of an ad hoc nature. This paper proposes using a logistic model from psychometrics as a unified mechanism for task resolution in simulation-oriented games.\n    ",
        "submission_date": "2013-06-30T00:00:00",
        "last_modified_date": "2013-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.0426",
        "title": "An Empirical Study into Annotator Agreement, Ground Truth Estimation, and Algorithm Evaluation",
        "authors": [
            "Thomas A. Lampert",
            "Andr\u00e9 Stumpf",
            "Pierre Gan\u00e7arski"
        ],
        "abstract": "Although agreement between annotators has been studied in the past from a statistical viewpoint, little work has attempted to quantify the extent to which this phenomenon affects the evaluation of computer vision (CV) object detection algorithms. Many researchers utilise ground truth (GT) in experiments and more often than not this GT is derived from one annotator's opinion. How does the difference in opinion affect an algorithm's evaluation? Four examples of typical CV problems are chosen, and a methodology is applied to each to quantify the inter-annotator variance and to offer insight into the mechanisms behind agreement and the use of GT. It is found that when detecting linear objects annotator agreement is very low. The agreement in object position, linear or otherwise, can be partially explained through basic image properties. Automatic object detectors are compared to annotator agreement and it is found that a clear relationship exists. Several methods for calculating GTs from a number of annotations are applied and the resulting differences in the performance of the object detectors are quantified. It is found that the rank of a detector is highly dependent upon the method used to form the GT. It is also found that although the STAPLE and LSML GT estimation methods appear to represent the mean of the performance measured using the individual annotations, when there are few annotations, or there is a large variance in them, these estimates tend to degrade. Furthermore, one of the most commonly adopted annotation combination methods--consensus voting--accentuates more obvious features, which results in an overestimation of the algorithm's performance. Finally, it is concluded that in some datasets it may not be possible to state with any confidence that one algorithm outperforms another when evaluating upon one GT and a method for calculating confidence bounds is discussed.\n    ",
        "submission_date": "2013-07-01T00:00:00",
        "last_modified_date": "2016-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.0802",
        "title": "A Statistical Learning Theory Framework for Supervised Pattern Discovery",
        "authors": [
            "Jonathan H. Huggins",
            "Cynthia Rudin"
        ],
        "abstract": "This paper formalizes a latent variable inference problem we call {\\em supervised pattern discovery}, the goal of which is to find sets of observations that belong to a single ``pattern.'' We discuss two versions of the problem and prove uniform risk bounds for both. In the first version, collections of patterns can be generated in an arbitrary manner and the data consist of multiple labeled collections. In the second version, the patterns are assumed to be generated independently by identically distributed processes. These processes are allowed to take an arbitrary form, so observations within a pattern are not in general independent of each other. The bounds for the second version of the problem are stated in terms of a new complexity measure, the quasi-Rademacher complexity.\n    ",
        "submission_date": "2013-07-02T00:00:00",
        "last_modified_date": "2014-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.0803",
        "title": "Data Fusion by Matrix Factorization",
        "authors": [
            "Marinka \u017ditnik",
            "Bla\u017e Zupan"
        ],
        "abstract": "For most problems in science and engineering we can obtain data sets that describe the observed system from various perspectives and record the behavior of its individual components. Heterogeneous data sets can be collectively mined by data fusion. Fusion can focus on a specific target relation and exploit directly associated data together with contextual data and data about system's constraints. In the paper we describe a data fusion approach with penalized matrix tri-factorization (DFMF) that simultaneously factorizes data matrices to reveal hidden associations. The approach can directly consider any data that can be expressed in a matrix, including those from feature-based representations, ontologies, associations and networks. We demonstrate the utility of DFMF for gene function prediction task with eleven different data sources and for prediction of pharmacologic actions by fusing six data sources. Our data fusion algorithm compares favorably to alternative data integration approaches and achieves higher accuracy than can be obtained from any single data source alone.\n    ",
        "submission_date": "2013-07-02T00:00:00",
        "last_modified_date": "2015-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.0813",
        "title": "Multi-Task Policy Search",
        "authors": [
            "Marc Peter Deisenroth",
            "Peter Englert",
            "Jan Peters",
            "Dieter Fox"
        ],
        "abstract": "Learning policies that generalize across multiple tasks is an important and challenging research topic in reinforcement learning and robotics. Training individual policies for every single potential task is often impractical, especially for continuous task variations, requiring more principled approaches to share and transfer knowledge among similar tasks. We present a novel approach for learning a nonlinear feedback policy that generalizes across multiple tasks. The key idea is to define a parametrized policy as a function of both the state and the task, which allows learning a single policy that generalizes across multiple known and unknown tasks. Applications of our novel approach to reinforcement and imitation learning in real-robot experiments are shown.\n    ",
        "submission_date": "2013-07-02T00:00:00",
        "last_modified_date": "2014-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.1277",
        "title": "Evidence and plausibility in neighborhood structures",
        "authors": [
            "Johan van Benthem",
            "David Fern\u00e1ndez-Duque",
            "Eric Pacuit"
        ],
        "abstract": "The intuitive notion of evidence has both semantic and syntactic features. In this paper, we develop an {\\em evidence logic} for epistemic agents faced with possibly contradictory evidence from different sources. The logic is based on a neighborhood semantics, where a neighborhood $N$ indicates that the agent has reason to believe that the true state of the world lies in $N$. Further notions of relative plausibility between worlds and beliefs based on the latter ordering are then defined in terms of this evidence structure, yielding our intended models for evidence-based beliefs. In addition, we also consider a second more general flavor, where belief and plausibility are modeled using additional primitive relations, and we prove a representation theorem showing that each such general model is a $p$-morphic image of an intended one. This semantics invites a number of natural special cases, depending on how uniform we make the evidence sets, and how coherent their total structure. We give a structural study of the resulting `uniform' and `flat' models. Our main result are sound and complete axiomatizations for the logics of all four major model classes with respect to the modal language of evidence, belief and safe belief. We conclude with an outlook toward logics for the dynamics of changing evidence, and the resulting language extensions and connections with logics of plausibility change.\n    ",
        "submission_date": "2013-07-04T00:00:00",
        "last_modified_date": "2013-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.1408",
        "title": "An investigation into the relationship between type-2 FOU size and environmental uncertainty in robotic control",
        "authors": [
            "Naisan Benatar",
            "Uwe Aickelin",
            "Jonathan M. Garibaldi"
        ],
        "abstract": "It has been suggested that, when faced with large amounts of uncertainty in situations of automated control, type-2 fuzzy logic based controllers will out-perform the simpler type-1 varieties due to the latter lacking the flexibility to adapt accordingly. This paper aims to investigate this problem in detail in order to analyse when a type-2 controller will improve upon type-1 performance. A robotic sailing boat is subjected to several experiments in which the uncertainty and difficulty of the sailing problem is increased in order to observe the effects on measured performance. Improved performance is observed but not in every case. The size of the FOU is shown to be have a large effect on performance with potentially severe performance penalties for incorrectly sized footprints.\n    ",
        "submission_date": "2013-07-04T00:00:00",
        "last_modified_date": "2013-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.1944",
        "title": "READ-EVAL-PRINT in Parallel and Asynchronous Proof-checking",
        "authors": [
            "Makarius Wenzel"
        ],
        "abstract": "The LCF tradition of interactive theorem proving, which was started by Milner in the 1970-ies, appears to be tied to the classic READ-EVAL-PRINT-LOOP of sequential and synchronous evaluation of prover commands.  We break up this loop and retrofit the read-eval-print phases into a model of parallel and asynchronous proof processing.  Thus we explain some key concepts of the Isabelle/Scala approach to prover interaction and integration, and the Isabelle/jEdit Prover IDE as front-end technology.  We hope to open up the scientific discussion about non-trivial interaction models for ITP systems again, and help getting other old-school proof assistants on a similar track.\n\n    ",
        "submission_date": "2013-07-08T00:00:00",
        "last_modified_date": "2013-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.2191",
        "title": "A Knowledge-based Treatment of Human-Automation Systems",
        "authors": [
            "Yoram Moses",
            "Marcia K. Shamo"
        ],
        "abstract": "In a supervisory control system the human agent knowledge of past, current, and future system behavior is critical for system performance. Being able to reason about that knowledge in a precise and structured manner is central to effective system design. In this paper we introduce the application of a well-established formal approach to reasoning about knowledge to the modeling and analysis of complex human-automation systems. An intuitive notion of knowledge in human-automation systems is sketched and then cast as a formal model. We present a case study in which the approach is used to model and reason about a classic problem from the human-automation systems literature; the results of our analysis provide evidence for the validity and value of reasoning about complex systems in terms of the knowledge of the system agents. To conclude, we discuss research directions that will extend this approach, and note several systems in the aviation and human-robot team domains that are of particular interest.\n    ",
        "submission_date": "2013-07-08T00:00:00",
        "last_modified_date": "2013-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.2579",
        "title": "Tuned Models of Peer Assessment in MOOCs",
        "authors": [
            "Chris Piech",
            "Jonathan Huang",
            "Zhenghao Chen",
            "Chuong Do",
            "Andrew Ng",
            "Daphne Koller"
        ],
        "abstract": "In massive open online courses (MOOCs), peer grading serves as a critical tool for scaling the grading of complex, open-ended assignments to courses with tens or hundreds of thousands of students. But despite promising initial trials, it does not always deliver accurate results compared to human experts. In this paper, we develop algorithms for estimating and correcting for grader biases and reliabilities, showing significant improvement in peer grading accuracy on real data with 63,199 peer grades from Coursera's HCI course offerings --- the largest peer grading networks analysed to date. We relate grader biases and reliabilities to other student factors such as student engagement, performance as well as commenting style. We also show that our model can lead to more intelligent assignment of graders to gradees.\n    ",
        "submission_date": "2013-07-09T00:00:00",
        "last_modified_date": "2013-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.2982",
        "title": "Fast Exact Search in Hamming Space with Multi-Index Hashing",
        "authors": [
            "Mohammad Norouzi",
            "Ali Punjani",
            "David J. Fleet"
        ],
        "abstract": "There is growing interest in representing image data and feature descriptors using compact binary codes for fast near neighbor search. Although binary codes are motivated by their use as direct indices (addresses) into a hash table, codes longer than 32 bits are not being used as such, as it was thought to be ineffective. We introduce a rigorous way to build multiple hash tables on binary code substrings that enables exact k-nearest neighbor search in Hamming space. The approach is storage efficient and straightforward to implement. Theoretical analysis shows that the algorithm exhibits sub-linear run-time behavior for uniformly distributed codes. Empirical results show dramatic speedups over a linear scan baseline for datasets of up to one billion codes of 64, 128, or 256 bits.\n    ",
        "submission_date": "2013-07-11T00:00:00",
        "last_modified_date": "2014-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.3004",
        "title": "Routing in Wireless Mesh Networks: Two Soft Computing Based Approaches",
        "authors": [
            "Sharad Sharma",
            "Shakti Kumar",
            "Brahmjit Singh"
        ],
        "abstract": "Due to dynamic network conditions, routing is the most critical part in WMNs and needs to be optimised. The routing strategies developed for WMNs must be efficient to make it an operationally self configurable network. Thus we need to resort to near shortest path evaluation. This lays down the requirement of some soft computing approaches such that a near shortest path is available in an affordable computing time. This paper proposes a Fuzzy Logic based integrated cost measure in terms of delay, throughput and jitter. Based upon this distance (cost) between two adjacent nodes we evaluate minimal shortest path that updates routing tables. We apply two recent soft computing approaches namely Big Bang Big Crunch (BB-BC) and Biogeography Based Optimization (BBO) approaches to enumerate shortest or near short paths. BB-BC theory is related with the evolution of the universe whereas BBO is inspired by dynamical equilibrium in the number of species on an island. Both the algorithms have low computational time and high convergence speed. Simulation results show that the proposed routing algorithms find the optimal shortest path taking into account three most important parameters of network dynamics. It has been further observed that for the shortest path problem BB-BC outperforms BBO in terms of speed and percent error between the evaluated minimal path and the actual shortest path.\n    ",
        "submission_date": "2013-07-11T00:00:00",
        "last_modified_date": "2013-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.3011",
        "title": "Soft Computing Framework for Routing in Wireless Mesh Networks: An Integrated Cost Function Approach",
        "authors": [
            "Shakti Kumar",
            "Brahmjit Singh",
            "Sharad Sharma"
        ],
        "abstract": "Dynamic behaviour of a WMN imposes stringent constraints on the routing policy of the network. In the shortest path based routing the shortest paths needs to be evaluated within a given time frame allowed by the WMN dynamics. The exact reasoning based shortest path evaluation methods usually fail to meet this rigid requirement. Thus, requiring some soft computing based approaches which can replace \"best for sure\" solutions with \"good enough\" solutions. This paper proposes a framework for optimal routing in the WMNs; where we investigate the suitability of Big Bang-Big Crunch (BB-BC), a soft computing based approach to evaluate shortest/near-shortest path. In order to make routing optimal we first propose to replace distance between the adjacent nodes with an integrated cost measure that takes into account throughput, delay, jitter and residual energy of a node. A fuzzy logic based inference mechanism evaluates this cost measure at each node. Using this distance measure we apply BB-BC optimization algorithm to evaluate shortest/near shortest path to update the routing tables periodically as dictated by network requirements. A large number of simulations were conducted and it has been observed that BB-BC algorithm appears to be a high potential candidate suitable for routing in WMNs.\n    ",
        "submission_date": "2013-07-11T00:00:00",
        "last_modified_date": "2013-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.3626",
        "title": "Learning an Integrated Distance Metric for Comparing Structure of Complex Networks",
        "authors": [
            "Sadegh Aliakbary",
            "Sadegh Motallebi",
            "Jafar Habibi",
            "Ali Movaghar"
        ],
        "abstract": "Graph comparison plays a major role in many network applications. We often need a similarity metric for comparing networks according to their structural properties. Various network features - such as degree distribution and clustering coefficient - provide measurements for comparing networks from different points of view, but a global and integrated distance metric is still missing. In this paper, we employ distance metric learning algorithms in order to construct an integrated distance metric for comparing structural properties of complex networks. According to natural witnesses of network similarities (such as network categories) the distance metric is learned by the means of a dataset of some labeled real networks. For evaluating our proposed method which is called NetDistance, we applied it as the distance metric in K-nearest-neighbors classification. Empirical results show that NetDistance outperforms previous methods, at least 20 percent, with respect to precision.\n    ",
        "submission_date": "2013-07-13T00:00:00",
        "last_modified_date": "2013-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.3667",
        "title": "Logics of formal inconsistency arising from systems of fuzzy logic",
        "authors": [
            "Marcelo Coniglio",
            "Francesc Esteva",
            "Llu\u00eds Godo"
        ],
        "abstract": "This paper proposes the meeting of fuzzy logic with paraconsistency in a very precise and foundational way. Specifically, in this paper we introduce expansions of the fuzzy logic MTL by means of primitive operators for consistency and inconsistency in the style of the so-called Logics of Formal Inconsistency (LFIs). The main novelty of the present approach is the definition of postulates for this type of operators over MTL-algebras, leading to the definition and axiomatization of a family of logics, expansions of MTL, whose degree-preserving counterpart are paraconsistent and moreover LFIs.\n    ",
        "submission_date": "2013-07-13T00:00:00",
        "last_modified_date": "2014-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.3802",
        "title": "Probability Distinguishes Different Types of Conditional Statements",
        "authors": [
            "Joseph W. Norman"
        ],
        "abstract": "The language of probability is used to define several different types of conditional statements. There are four principal types: subjunctive, material, existential, and feasibility. Two further types of conditionals are defined using the propositional calculus and Boole's mathematical logic: truth-functional and Boolean feasibility (which turn out to be special cases of probabilistic conditionals). Each probabilistic conditional is quantified by a fractional parameter between zero and one that says whether it is purely affirmative, purely negative, or intermediate in its sense. Conditionals can be specialized further by their content to express factuality and counterfactuality, and revised or reformulated to account for exceptions and confounding factors. The various conditionals have distinct mathematical representations: through intermediate probability expressions and logical formulas, each conditional is eventually translated into a set of polynomial equations and inequalities (with real coefficients). The polynomial systems from different types of conditionals exhibit different patterns of behavior, concerning for example opposing conditionals or false antecedents. Interesting results can be computed from the relevant polynomial systems using well-known methods from algebra and computer science. Among other benefits, the proposed framework of analysis offers paraconsistent procedures for logical deduction that produce such familiar results as modus ponens, transitivity, disjunction introduction, and disjunctive syllogism; all while avoiding any explosion of consequences from inconsistent premises. Several example problems from Goodman and Adams are analyzed. A new perspective called polylogicism is presented: mathematical logic that respects the diversity among conditionals in particular and logic problems in general.\n    ",
        "submission_date": "2013-07-15T00:00:00",
        "last_modified_date": "2014-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.3824",
        "title": "The Fundamental Learning Problem that Genetic Algorithms with Uniform Crossover Solve Efficiently and Repeatedly As Evolution Proceeds",
        "authors": [
            "Keki M. Burjorjee"
        ],
        "abstract": "This paper establishes theoretical bonafides for implicit concurrent multivariate effect evaluation--implicit concurrency for short---a broad and versatile computational learning efficiency thought to underlie general-purpose, non-local, noise-tolerant optimization in genetic algorithms with uniform crossover (UGAs). We demonstrate that implicit concurrency is indeed a form of efficient learning by showing that it can be used to obtain close-to-optimal bounds on the time and queries required to approximately correctly solve a constrained version (k=7, \\eta=1/5) of a recognizable computational learning problem: learning parities with noisy membership queries. We argue that a UGA that treats the noisy membership query oracle as a fitness function can be straightforwardly used to approximately correctly learn the essential attributes in O(log^1.585 n) queries and O(n log^1.585 n) time, where n is the total number of attributes. Our proof relies on an accessible symmetry argument and the use of statistical hypothesis testing to reject a global null hypothesis at the 10^-100 level of significance. It is, to the best of our knowledge, the first relatively rigorous identification of efficient computational learning in an evolutionary algorithm on a non-trivial learning problem.\n    ",
        "submission_date": "2013-07-15T00:00:00",
        "last_modified_date": "2013-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.4101",
        "title": "Decision Making for Inconsistent Expert Judgments Using Negative Probabilities",
        "authors": [
            "J. Acacio de Barros"
        ],
        "abstract": "In this paper we provide a simple random-variable example of inconsistent information, and analyze it using three different approaches: Bayesian, quantum-like, and negative probabilities. We then show that, at least for this particular example, both the Bayesian and the quantum-like approaches have less normative power than the negative probabilities one.\n    ",
        "submission_date": "2013-07-15T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.4479",
        "title": "Model checking coalitional games in shortage resource scenarios",
        "authors": [
            "Dario Della Monica",
            "Margherita Napoli",
            "Mimmo Parente"
        ],
        "abstract": "Verification of multi-agents systems (MAS) has been recently studied taking into account the need of expressing resource bounds.  Several  logics  for specifying properties of MAS    have been presented in quite a variety of scenarios with bounded resources.  In this paper, we study a different formalism,  called Priced Resource-Bounded Alternating-time Temporal Logic (PRBATL),  whose  main novelty consists in moving  the notion of resources from a syntactic level (part of the formula) to a semantic one (part of the model). This allows us to track the evolution of the resource availability along the computations and provides us with a formalisms capable to model a number of real-world scenarios.  Two relevant aspects are the notion of global availability of the resources on the market, that are shared  by the agents,  and the notion of price of resources, depending on their availability.   In a previous work of ours, an initial step towards this new formalism was introduced, along with an EXPTIME algorithm for the model checking problem. In this paper we better analyze the features of the proposed formalism, also in comparison with previous approaches. The main technical contribution  is the proof of the EXPTIME-hardness of the the model checking problem for PRBATL, based on a reduction from the acceptance problem for Linearly-Bounded Alternating Turing Machines. In particular, since the problem has multiple parameters, we show two fixed-parameter reductions.\n\n    ",
        "submission_date": "2013-07-17T00:00:00",
        "last_modified_date": "2013-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.4514",
        "title": "Supervised Metric Learning with Generalization Guarantees",
        "authors": [
            "Aur\u00e9lien Bellet"
        ],
        "abstract": "The crucial importance of metrics in machine learning algorithms has led to an increasing interest in optimizing distance and similarity functions, an area of research known as metric learning. When data consist of feature vectors, a large body of work has focused on learning a Mahalanobis distance. Less work has been devoted to metric learning from structured objects (such as strings or trees), most of it focusing on optimizing a notion of edit distance. We identify two important limitations of current metric learning approaches. First, they allow to improve the performance of local algorithms such as k-nearest neighbors, but metric learning for global algorithms (such as linear classifiers) has not been studied so far. Second, the question of the generalization ability of metric learning methods has been largely ignored. In this thesis, we propose theoretical and algorithmic contributions that address these limitations. Our first contribution is the derivation of a new kernel function built from learned edit probabilities. Our second contribution is a novel framework for learning string and tree edit similarities inspired by the recent theory of (e,g,t)-good similarity functions. Using uniform stability arguments, we establish theoretical guarantees for the learned similarity that give a bound on the generalization error of a linear classifier built from that similarity. In our third contribution, we extend these ideas to metric learning from feature vectors by proposing a bilinear similarity learning method that efficiently optimizes the (e,g,t)-goodness. Generalization guarantees are derived for our approach, highlighting that our method minimizes a tighter bound on the generalization error of the classifier. Our last contribution is a framework for establishing generalization bounds for a large class of existing metric learning algorithms based on a notion of algorithmic robustness.\n    ",
        "submission_date": "2013-07-17T00:00:00",
        "last_modified_date": "2013-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.4847",
        "title": "Efficient Reinforcement Learning in Deterministic Systems with Value Function Generalization",
        "authors": [
            "Zheng Wen",
            "Benjamin Van Roy"
        ],
        "abstract": "We consider the problem of reinforcement learning over episodes of a finite-horizon deterministic system and as a solution propose optimistic constraint propagation (OCP), an algorithm designed to synthesize efficient exploration and value function generalization. We establish that when the true value function lies within a given hypothesis class, OCP selects optimal actions over all but at most K episodes, where K is the eluder dimension of the given hypothesis class. We establish further efficiency and asymptotic performance guarantees that apply even if the true value function does not lie in the given hypothesis class, for the special case where the hypothesis class is the span of pre-specified indicator functions over disjoint sets. We also discuss the computational complexity of OCP and present computational results involving two illustrative examples.\n    ",
        "submission_date": "2013-07-18T00:00:00",
        "last_modified_date": "2016-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.5636",
        "title": "A generalized back-door criterion",
        "authors": [
            "Marloes H. Maathuis",
            "Diego Colombo"
        ],
        "abstract": "We generalize Pearl's back-door criterion for directed acyclic graphs (DAGs) to more general types of graphs that describe Markov equivalence classes of DAGs and/or allow for arbitrarily many hidden variables. We also give easily checkable necessary and sufficient graphical criteria for the existence of a set of variables that satisfies our generalized back-door criterion, when considering a single intervention and a single outcome variable. Moreover, if such a set exists, we provide an explicit set that fulfills the criterion. We illustrate the results in several examples. R-code is available in the R-package pcalg.\n    ",
        "submission_date": "2013-07-22T00:00:00",
        "last_modified_date": "2015-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.5713",
        "title": "Understanding Humans' Strategies in Maze Solving",
        "authors": [
            "Min Zhao",
            "Andre G. Marquez"
        ],
        "abstract": "Navigating through a visual maze relies on the strategic use of eye movements to select and identify the route. When navigating the maze, there are trade-offs between exploring to the environment and relying on memory. This study examined strategies used to navigating through novel and familiar mazes that were viewed from above and traversed by a mouse cursor. Eye and mouse movements revealed two modes that almost never occurred concurrently: exploration and guidance. Analyses showed that people learned mazes and were able to devise and carry out complex, multi-faceted strategies that traded-off visual exploration against active motor performance. These strategies took into account available visual information, memory, confidence, the estimated cost in time for exploration, and idiosyncratic tolerance for error. Understanding the strategies humans used for maze solving is valuable for applications in cognitive neuroscience as well as in AI, robotics and human-robot interactions.\n    ",
        "submission_date": "2013-07-22T00:00:00",
        "last_modified_date": "2013-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.5837",
        "title": "An Information Theoretic Measure of Judea Pearl's Identifiability and Causal Influence",
        "authors": [
            "Robert R. Tucci"
        ],
        "abstract": "In this paper, we define a new information theoretic measure that we call the \"uprooted information\". We show that a necessary and sufficient condition for a probability $P(s|do(t))$ to be \"identifiable\" (in the sense of Pearl) in a graph $G$ is that its uprooted information be non-negative for all models of the graph $G$. In this paper, we also give a new algorithm for deciding, for a Bayesian net that is semi-Markovian, whether a probability $P(s|do(t))$ is identifiable, and, if it is identifiable, for expressing it without allusions to confounding variables. Our algorithm is closely based on a previous algorithm by Tian and Pearl, but seems to correct a small flaw in theirs. In this paper, we also find a {\\it necessary and sufficient graphical condition} for a probability $P(s|do(t))$ to be identifiable when $t$ is a singleton set. So far, in the prior literature, it appears that only a {\\it sufficient graphical condition} has been given for this. By \"graphical\" we mean that it is directly based on Judea Pearl's 3 rules of do-calculus.\n    ",
        "submission_date": "2013-07-21T00:00:00",
        "last_modified_date": "2013-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.6883",
        "title": "A gradient descent technique coupled with a dynamic simulation to determine the near optimum orientation of floor plan designs",
        "authors": [
            "Eug\u00e9nio Rodrigues",
            "Ad\u00e9lio Rodrigues Gaspar",
            "\u00c1lvaro Gomes"
        ],
        "abstract": "A prototype tool to assist architects during the early design stage of floor plans has been developed, consisting of an Evolutionary Program for the Space Allocation Problem (EPSAP), which generates sets of floor plan alternatives according to the architect's preferences; and a Floor Plan Performance Optimization Program (FPOP), which optimizes the selected solutions according to thermal performance criteria. The design variables subject to optimization are window position and size, overhangs, fins, wall positioning, and building orientation. A procedure using a transformation operator with gradient descent, such as behavior, coupled with a dynamic simulation engine was developed for the thermal evaluation and optimization process. However, the need to evaluate all possible alternatives regarding designing variables being used during the optimization process leads to an intensive use of thermal simulation, which dramatically increases the simulation time, rendering it unpractical. An alternative approach is a smart optimization approach, which utilizes an oriented and adaptive search technique to efficiently find the near optimum solution. This paper presents the search methodology for the building orientation of floor plan designs, and the corresponding efficiency and effectiveness indicators. The calculations are based on 100 floor plan designs generated by EPSAP. All floor plans have the same design program, location, and weather data, changing only their geometry. Dynamic simulation of buildings was effectively used together with the optimization procedure in this approach to significantly improve the designs. The use of the orientation variable has been included in the algorithm.\n    ",
        "submission_date": "2013-07-25T00:00:00",
        "last_modified_date": "2013-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.7127",
        "title": "Man and Machine: Questions of Risk, Trust and Accountability in Today's AI Technology",
        "authors": [
            "Piyush Ahuja"
        ],
        "abstract": "Artificial Intelligence began as a field probing some of the most fundamental questions of science - the nature of intelligence and the design of intelligent artifacts. But it has grown into a discipline that is deeply entwined with commerce and society. Today's AI technology, such as expert systems and intelligent assistants, pose some difficult questions of risk, trust and accountability. In this paper, we present these concerns, examining them in the context of historical developments that have shaped the nature and direction of AI research. We also suggest the exploration and further development of two paradigms, human intelligence-machine cooperation, and a sociological view of intelligence, which might help address some of these concerns.\n    ",
        "submission_date": "2013-07-26T00:00:00",
        "last_modified_date": "2013-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.7129",
        "title": "An Architecture for Autonomously Controlling Robot with Embodiment in Real World",
        "authors": [
            "Megumi Fujita",
            "Yuki Goto",
            "Naoyuki Nide",
            "Ken Satoh",
            "Hiroshi Hosobe"
        ],
        "abstract": "In the real world, robots with embodiment face various issues such as dynamic continuous changes of the environment and input/output disturbances. The key to solving these issues can be found in daily life; people `do actions associated with sensing' and `dynamically change their plans when necessary'. We propose the use of a new concept, enabling robots to do these two things, for autonomously controlling mobile robots. We implemented our concept to make two experiments under static/dynamic environments. The results of these experiments show that our idea provides a way to adapt to dynamic changes of the environment in the real world.\n    ",
        "submission_date": "2013-07-26T00:00:00",
        "last_modified_date": "2013-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.7198",
        "title": "Self-Learning for Player Localization in Sports Video",
        "authors": [
            "Kenji Okuma",
            "David G. Lowe",
            "James J. Little"
        ],
        "abstract": "This paper introduces a novel self-learning framework that automates the label acquisition process for improving models for detecting players in broadcast footage of sports games. Unlike most previous self-learning approaches for improving appearance-based object detectors from videos, we allow an unknown, unconstrained number of target objects in a more generalized video sequence with non-static camera views. Our self-learning approach uses a latent SVM learning algorithm and deformable part models to represent the shape and colour information of players, constraining their motions, and learns the colour of the playing field by a gentle Adaboost algorithm. We combine those image cues and discover additional labels automatically from unlabelled data. In our experiments, our approach exploits both labelled and unlabelled data in sparsely labelled videos of sports games, providing a mean performance improvement of over 20% in the average precision for detecting sports players and improved tracking, when videos contain very few labelled images.\n    ",
        "submission_date": "2013-07-27T00:00:00",
        "last_modified_date": "2013-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.7303",
        "title": "Learning to Understand by Evolving Theories",
        "authors": [
            "Martin E. Mueller",
            "Madhura D. Thosar"
        ],
        "abstract": "In this paper, we describe an approach that enables an autonomous system to infer the semantics of a command (i.e. a symbol sequence representing an action) in terms of the relations between changes in the observations and the action instances. We present a method of how to induce a theory (i.e. a semantic description) of the meaning of a command in terms of a minimal set of background knowledge. The only thing we have is a sequence of observations from which we extract what kinds of effects were caused by performing the command. This way, we yield a description of the semantics of the action and, hence, a definition.\n    ",
        "submission_date": "2013-07-27T00:00:00",
        "last_modified_date": "2013-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.7398",
        "title": "ROSoClingo: A ROS package for ASP-based robot control",
        "authors": [
            "Benjamin Andres",
            "Philipp Obermeier",
            "Orkunt Sabuncu",
            "Torsten Schaub",
            "David Rajaratnam"
        ],
        "abstract": "Knowledge representation and reasoning capacities are vital to cognitive robotics because they provide higher level cognitive functions for reasoning about actions, environments, goals, perception, etc. Although Answer Set Programming (ASP) is well suited for modelling such functions, there was so far no seamless way to use ASP in a robotic environment. We address this shortcoming and show how a recently developed reactive ASP system can be harnessed to provide appropriate reasoning capacities within a robotic system. To be more precise, we furnish a package integrating the reactive ASP solver oClingo with the popular open-source robotic middleware ROS. The resulting system, ROSoClingo, provides a generic way by which an ASP program can be used to control the behaviour of a robot and to respond to the results of the robot's actions.\n    ",
        "submission_date": "2013-07-28T00:00:00",
        "last_modified_date": "2013-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.7405",
        "title": "Reasoning for Moving Blocks Problem: Formal Representation and Implementation",
        "authors": [
            "P. A. Wa\u0142\u0229ga"
        ],
        "abstract": "The combined approach of the Qualitative Reasoning and Probabilistic Functions for the knowledge representation is proposed. The method aims at represent uncertain, qualitative knowledge that is essential for the moving blocks task's execution. The attempt to formalize the commonsense knowledge is performed with the Situation Calculus language for reasoning and robot's beliefs representation. The method is implemented in the Prolog programming language and tested for a specific simulated scenario. In most cases the implementation enables us to solve a given task, i.e., move blocks to desired positions. The example of robot's reasoning and main parts of the implemented program's code are presented.\n    ",
        "submission_date": "2013-07-28T00:00:00",
        "last_modified_date": "2013-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.7461",
        "title": "Levels of Integration between Low-Level Reasoning and Task Planning",
        "authors": [
            "Esra Erdem",
            "Volkan Patoglu",
            "Peter Sch\u00fcller"
        ],
        "abstract": "We provide a systematic analysis of levels of integration between discrete high-level reasoning and continuous low-level reasoning to address hybrid planning problems in robotics. We identify four distinct strategies for such an integration: (i) low-level checks are done for all possible cases in advance and then this information is used during plan generation, (ii) low-level checks are done exactly when they are needed during the search for a plan, (iii) first all plans are computed and then infeasible ones are filtered, and (iv) by means of replanning, after finding a plan, low-level checks identify whether it is infeasible or not; if it is infeasible, a new plan is computed considering the results of previous low- level checks. We perform experiments on hybrid planning problems in robotic manipulation and legged locomotion domains considering these four methods of integration, as well as some of their combinations. We analyze the usefulness of levels of integration in these domains, both from the point of view of computational efficiency (in time and space) and from the point of view of plan quality relative to its feasibility. We discuss advantages and disadvantages of each strategy in the light of experimental results and provide some guidelines on choosing proper strategies for a given domain.\n    ",
        "submission_date": "2013-07-29T00:00:00",
        "last_modified_date": "2013-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.7793",
        "title": "Multi-dimensional Parametric Mincuts for Constrained MAP Inference",
        "authors": [
            "Yongsub Lim",
            "Kyomin Jung",
            "Pushmeet Kohli"
        ],
        "abstract": "In this paper, we propose novel algorithms for inferring the Maximum a Posteriori (MAP) solution of discrete pairwise random field models under multiple constraints. We show how this constrained discrete optimization problem can be formulated as a multi-dimensional parametric mincut problem via its Lagrangian dual, and prove that our algorithm isolates all constraint instances for which the problem can be solved exactly. These multiple solutions enable us to even deal with `soft constraints' (higher order penalty functions). Moreover, we propose two practical variants of our algorithm to solve problems with hard constraints. We also show how our method can be applied to solve various constrained discrete optimization problems such as submodular minimization and shortest path computation. Experimental evaluation using the foreground-background image segmentation problem with statistic constraints reveals that our method is faster and its results are closer to the ground truth labellings compared with the popular continuous relaxation based methods.\n    ",
        "submission_date": "2013-07-30T00:00:00",
        "last_modified_date": "2013-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.8049",
        "title": "Optimistic Concurrency Control for Distributed Unsupervised Learning",
        "authors": [
            "Xinghao Pan",
            "Joseph E. Gonzalez",
            "Stefanie Jegelka",
            "Tamara Broderick",
            "Michael I. Jordan"
        ],
        "abstract": "Research on distributed machine learning algorithms has focused primarily on one of two extremes - algorithms that obey strict concurrency constraints or algorithms that obey few or no such constraints. We consider an intermediate alternative in which algorithms optimistically assume that conflicts are unlikely and if conflicts do arise a conflict-resolution protocol is invoked. We view this \"optimistic concurrency control\" paradigm as particularly appropriate for large-scale machine learning algorithms, particularly in the unsupervised setting. We demonstrate our approach in three problem areas: clustering, feature learning and online facility location. We evaluate our methods via large-scale experiments in a cluster computing environment.\n    ",
        "submission_date": "2013-07-30T00:00:00",
        "last_modified_date": "2013-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.0183",
        "title": "An n-ary Constraint for the Stable Marriage Problem",
        "authors": [
            "Chris Unsworth",
            "Patrick Prosser"
        ],
        "abstract": "We present an n-ary constraint for the stable marriage problem. This constraint acts between two sets of integer variables where the domains of those variables represent preferences. Our constraint enforces stability and disallows bigamy. For a stable marriage instance with $n$ men and $n$ women we require only one of these constraints, and the complexity of enforcing arc-consistency is $O(n^2)$ which is optimal in the size of input. Our computational studies show that our n-ary constraint is significantly faster and more space efficient than the encodings presented in \\cite{cp01}. We also introduce a new problem to the constraint community, the sex-equal stable marriage problem.\n    ",
        "submission_date": "2013-08-01T00:00:00",
        "last_modified_date": "2013-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.0689",
        "title": "Measure Transformer Semantics for Bayesian Machine Learning",
        "authors": [
            "Johannes Borgstr\u00f6m",
            "Andrew D Gordon",
            "Michael Greenberg",
            "James Margetson",
            "Jurgen Van Gael"
        ],
        "abstract": "The Bayesian approach to machine learning amounts to computing posterior distributions of random variables from a probabilistic model of how the variables are related (that is, a prior distribution) and a set of observations of variables. There is a trend in machine learning towards expressing Bayesian models as probabilistic programs. As a foundation for this kind of programming, we propose a core functional calculus with primitives for sampling prior distributions and observing variables. We define measure-transformer combinators inspired by theorems in measure theory, and use these to give a rigorous semantics to our core calculus. The original features of our semantics include its support for discrete, continuous, and hybrid measures, and, in particular, for observations of zero-probability events. We compile our core language to a small imperative language that is processed by an existing inference engine for factor graphs, which are data structures that enable many efficient inference algorithms. This allows efficient approximate inference of posterior marginal distributions, treating thousands of observations per second for large instances of realistic models.\n    ",
        "submission_date": "2013-08-03T00:00:00",
        "last_modified_date": "2013-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.0725",
        "title": "A Rough Computing based Performance Evaluation Approach for Educational Institutions",
        "authors": [
            "Debi Prasanna Acharjya",
            "Debarati Bhattacharjee"
        ],
        "abstract": "Performance evaluation of various organizations especially educational institutions is a very important area of research and needs to be cultivated more. In this paper, we propose a performance evaluation for educational institutions using rough set on fuzzy approximation spaces with ordering rules and information entropy. In order to measure the performance of educational institutions, we construct an evaluation index system. Rough set on fuzzy approximation spaces with ordering is applied to explore the evaluation index data of each level. Furthermore, the concept of information entropy is used to determine the weighting coefficients of evaluation indexes. Also, we find the most important indexes that influence the weighting coefficients. The proposed approach is validated and shows the practical viability. Moreover, the proposed approach can be applicable to any organizations.\n    ",
        "submission_date": "2013-08-03T00:00:00",
        "last_modified_date": "2013-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.1484",
        "title": "A Multi-Swarm Cellular PSO based on Clonal Selection Algorithm in Dynamic Environments",
        "authors": [
            "Somayeh Nabizadeh",
            "Alireza Rezvanian",
            "Mohammd Reza Meybodi"
        ],
        "abstract": "Many real-world problems are dynamic optimization problems. In this case, the optima in the environment change dynamically. Therefore, traditional optimization algorithms disable to track and find optima. In this paper, a new multi-swarm cellular particle swarm optimization based on clonal selection algorithm (CPSOC) is proposed for dynamic environments. In the proposed algorithm, the search space is partitioned into cells by a cellular automaton. Clustered particles in each cell, which make a sub-swarm, are evolved by the particle swarm optimization and clonal selection algorithm. Experimental results on Moving Peaks Benchmark demonstrate the superiority of the CPSOC its popular methods.\n    ",
        "submission_date": "2013-08-07T00:00:00",
        "last_modified_date": "2013-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.1603",
        "title": "A Note on Topology Preservation in Classification, and the Construction of a Universal Neuron Grid",
        "authors": [
            "Dietmar Volz"
        ],
        "abstract": "It will be shown that according to theorems of K. Menger, every neuron grid if identified with a curve is able to preserve the adopted qualitative structure of a data space. Furthermore, if this identification is made, the neuron grid structure can always be mapped to a subset of a universal neuron grid which is constructable in three space dimensions. Conclusions will be drawn for established neuron grid types as well as neural fields.\n    ",
        "submission_date": "2013-08-07T00:00:00",
        "last_modified_date": "2018-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.2350",
        "title": "Learning Features and their Transformations by Spatial and Temporal Spherical Clustering",
        "authors": [
            "Jayanta K. Dutta",
            "Bonny Banerjee"
        ],
        "abstract": "Learning features invariant to arbitrary transformations in the data is a requirement for any recognition system, biological or artificial. It is now widely accepted that simple cells in the primary visual cortex respond to features while the complex cells respond to features invariant to different transformations. We present a novel two-layered feedforward neural model that learns features in the first layer by spatial spherical clustering and invariance to transformations in the second layer by temporal spherical clustering. Learning occurs in an online and unsupervised manner following the Hebbian rule. When exposed to natural videos acquired by a camera mounted on a cat's head, the first and second layer neurons in our model develop simple and complex cell-like receptive field properties. The model can predict by learning lateral connections among the first layer neurons. A topographic map to their spatial features emerges by exponentially decaying the flow of activation with distance from one neuron to another in the first layer that fire in close temporal proximity, thereby minimizing the pooling length in an online manner simultaneously with feature learning.\n    ",
        "submission_date": "2013-08-10T00:00:00",
        "last_modified_date": "2013-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.2655",
        "title": "KL-based Control of the Learning Schedule for Surrogate Black-Box Optimization",
        "authors": [
            "Ilya Loshchilov",
            "Marc Schoenauer",
            "Mich\u00e8le Sebag"
        ],
        "abstract": "This paper investigates the control of an ML component within the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) devoted to black-box optimization. The known CMA-ES weakness is its sample complexity, the number of evaluations of the objective function needed to approximate the global optimum. This weakness is commonly addressed through surrogate optimization, learning an estimate of the objective function a.k.a. surrogate model, and replacing most evaluations of the true objective function with the (inexpensive) evaluation of the surrogate model. This paper presents a principled control of the learning schedule (when to relearn the surrogate model), based on the Kullback-Leibler divergence of the current search distribution and the training distribution of the former surrogate model. The experimental validation of the proposed approach shows significant performance gains on a comprehensive set of ill-conditioned benchmark problems, compared to the best state of the art including the quasi-Newton high-precision BFGS method.\n    ",
        "submission_date": "2013-08-12T00:00:00",
        "last_modified_date": "2013-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.3136",
        "title": "Toward the Coevolution of Novel Vertical-Axis Wind Turbines",
        "authors": [
            "Richard J. Preen",
            "Larry Bull"
        ],
        "abstract": "The production of renewable and sustainable energy is one of the most important challenges currently facing mankind. Wind has made an increasing contribution to the world's energy supply mix, but still remains a long way from reaching its full potential. In this paper, we investigate the use of artificial evolution to design vertical-axis wind turbine prototypes that are physically instantiated and evaluated under fan generated wind conditions. Initially a conventional evolutionary algorithm is used to explore the design space of a single wind turbine and later a cooperative coevolutionary algorithm is used to explore the design space of an array of wind turbines. Artificial neural networks are used throughout as surrogate models to assist learning and found to reduce the number of fabrications required to reach a higher aerodynamic efficiency. Unlike in other approaches, such as computational fluid dynamics simulations, no mathematical formulations are used and no model assumptions are made.\n    ",
        "submission_date": "2013-08-13T00:00:00",
        "last_modified_date": "2015-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.3324",
        "title": "History Based Coalition Formation in Hedonic Context Using Trust",
        "authors": [
            "Ahmadreza Ghaffarizadeh",
            "Vicki H. Allan"
        ],
        "abstract": "In this paper we address the problem of coalition formation in hedonic context. Our modelling tries to be as realistic as possible. In previous models, once an agent joins a coalition it would not be able to leave the coalition and join the new one; in this research we made it possible to leave a coalition but put some restrictions to control the behavior of agents. Leaving or staying of an agent in a coalition will affect on the trust of the other agents included in this coalition. Agents will use the trust values in computing the expected utility of coalitions. Three different risk behaviors are introduced for agents that want to initiate a coalition. Using these risk behaviors, some simulations are made and results are analyzed.\n    ",
        "submission_date": "2013-08-15T00:00:00",
        "last_modified_date": "2013-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.3513",
        "title": "Hidden Parameter Markov Decision Processes: A Semiparametric Regression Approach for Discovering Latent Task Parametrizations",
        "authors": [
            "Finale Doshi-Velez",
            "George Konidaris"
        ],
        "abstract": "Control applications often feature tasks with similar, but not identical, dynamics. We introduce the Hidden Parameter Markov Decision Process (HiP-MDP), a framework that parametrizes a family of related dynamical systems with a low-dimensional set of latent factors, and introduce a semiparametric regression approach for learning its structure from data. In the control setting, we show that a learned HiP-MDP rapidly identifies the dynamics of a new task instance, allowing an agent to flexibly adapt to task variations.\n    ",
        "submission_date": "2013-08-15T00:00:00",
        "last_modified_date": "2013-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.3780",
        "title": "Decision Theory with Resource-Bounded Agents",
        "authors": [
            "Joseph Y. Halpern",
            "Rafael Pass",
            "Lior Seeman"
        ],
        "abstract": "There have been two major lines of research aimed at capturing resource-bounded players in game theory. The first, initiated by Rubinstein, charges an agent for doing costly computation; the second, initiated by Neyman, does not charge for computation, but limits the computation that agents can do, typically by modeling agents as finite automata. We review recent work on applying both approaches in the context of decision theory. For the first approach, we take the objects of choice in a decision problem to be Turing machines, and charge players for the ``complexity'' of the Turing machine chosen (e.g., its running time). This approach can be used to explain well-known phenomena like first-impression-matters biases (i.e., people tend to put more weight on evidence they hear early on) and belief polarization (two people with different prior beliefs, hearing the same evidence, can end up with diametrically opposed conclusions) as the outcomes of quite rational decisions. For the second approach, we model people as finite automata, and provide a simple algorithm that, on a problem that captures a number of settings of interest, provably performs optimally as the number of states in the automaton increases.\n    ",
        "submission_date": "2013-08-17T00:00:00",
        "last_modified_date": "2013-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.3898",
        "title": "Firefly Algorithm: Recent Advances and Applications",
        "authors": [
            "Xin-She Yang",
            "Xingshi He"
        ],
        "abstract": "Nature-inspired metaheuristic algorithms, especially those based on swarm intelligence, have attracted much attention in the last ten years. Firefly algorithm appeared in about five years ago, its literature has expanded dramatically with diverse applications. In this paper, we will briefly review the fundamentals of firefly algorithm together with a selection of recent publications. Then, we discuss the optimality associated with balancing exploration and exploitation, which is essential for all metaheuristic algorithms. By comparing with intermittent search strategy, we conclude that metaheuristics such as firefly algorithm are better than the optimal intermittent search strategy. We also analyse algorithms and their implications for higher-dimensional optimization problems.\n    ",
        "submission_date": "2013-08-18T00:00:00",
        "last_modified_date": "2013-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.4013",
        "title": "Incentives for Privacy Tradeoff in Community Sensing",
        "authors": [
            "Adish Singla",
            "Andreas Krause"
        ],
        "abstract": "Community sensing, fusing information from populations of privately-held sensors, presents a great opportunity to create efficient and cost-effective sensing applications. Yet, reasonable privacy concerns often limit the access to such data streams. How should systems valuate and negotiate access to private information, for example in return for monetary incentives? How should they optimally choose the participants from a large population of strategic users with privacy concerns, and compensate them for information shared? In this paper, we address these questions and present a novel mechanism, SeqTGreedy, for budgeted recruitment of participants in community sensing. We first show that privacy tradeoffs in community sensing can be cast as an adaptive submodular optimization problem. We then design a budget feasible, incentive compatible (truthful) mechanism for adaptive submodular maximization, which achieves near-optimal utility for a large class of sensing applications. This mechanism is general, and of independent interest. We demonstrate the effectiveness of our approach in a case study of air quality monitoring, using data collected from the Mechanical Turk platform. Compared to the state of the art, our approach achieves up to 30% reduction in cost in order to achieve a desired level of utility.\n    ",
        "submission_date": "2013-08-19T00:00:00",
        "last_modified_date": "2013-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.4189",
        "title": "Seeing What You're Told: Sentence-Guided Activity Recognition In Video",
        "authors": [
            "N. Siddharth",
            "Andrei Barbu",
            "Jeffrey Mark Siskind"
        ],
        "abstract": "We present a system that demonstrates how the compositional structure of events, in concert with the compositional structure of language, can interplay with the underlying focusing mechanisms in video action recognition, thereby providing a medium, not only for top-down and bottom-up integration, but also for multi-modal integration between vision and language. We show how the roles played by participants (nouns), their characteristics (adjectives), the actions performed (verbs), the manner of such actions (adverbs), and changing spatial relations between participants (prepositions) in the form of whole sentential descriptions mediated by a grammar, guides the activity-recognition process. Further, the utility and expressiveness of our framework is demonstrated by performing three separate tasks in the domain of multi-activity videos: sentence-guided focus of attention, generation of sentential descriptions of video, and query-based video search, simply by leveraging the framework in different manners.\n    ",
        "submission_date": "2013-08-19T00:00:00",
        "last_modified_date": "2014-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.4526",
        "title": "Formalization, Mechanization and Automation of G\u00f6del's Proof of God's Existence",
        "authors": [
            "Christoph Benzm\u00fcller",
            "Bruno Woltzenlogel Paleo"
        ],
        "abstract": "G\u00f6del's ontological proof has been analysed for the first-time with an unprecedent degree of detail and formality with the help of higher-order theorem provers. The following has been done (and in this order): A detailed natural deduction proof. A formalization of the axioms, definitions and theorems in the TPTP THF syntax. Automatic verification of the consistency of the axioms and definitions with Nitpick. Automatic demonstration of the theorems with the provers LEO-II and Satallax. A step-by-step formalization using the Coq proof assistant. A formalization using the Isabelle proof assistant, where the theorems (and some additional lemmata) have been automated with Sledgehammer and Metis.\n    ",
        "submission_date": "2013-08-21T00:00:00",
        "last_modified_date": "2017-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.5032",
        "title": "How Did Humans Become So Creative? A Computational Approach",
        "authors": [
            "Liane Gabora",
            "Steve DiPaola"
        ],
        "abstract": "This paper summarizes efforts to computationally model two transitions in the evolution of human creativity: its origins about two million years ago, and the 'big bang' of creativity about 50,000 years ago. Using a computational model of cultural evolution in which neural network based agents evolve ideas for actions through invention and imitation, we tested the hypothesis that human creativity began with onset of the capacity for recursive recall. We compared runs in which agents were limited to single-step actions to runs in which they used recursive recall to chain simple actions into complex ones. Chaining resulted in higher diversity, open-ended novelty, no ceiling on the mean fitness of actions, and greater ability to make use of learning. Using a computational model of portrait painting, we tested the hypothesis that the explosion of creativity in the Middle/Upper Paleolithic was due to onset of con-textual focus: the capacity to shift between associative and analytic thought. This resulted in faster convergence on portraits that resembled the sitter, employed painterly techniques, and were rated as preferable. We conclude that recursive recall and contextual focus provide a computationally plausible explanation of how humans evolved the means to transform this planet.\n    ",
        "submission_date": "2013-08-23T00:00:00",
        "last_modified_date": "2019-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.5332",
        "title": "An Integrated Framework for Diagnosis and Prognosis of Hybrid Systems",
        "authors": [
            "Elodie Chanthery",
            "Pauline Ribot"
        ],
        "abstract": "Complex systems are naturally hybrid: their dynamic behavior is both continuous and discrete. For these systems, maintenance and repair are an increasing part of the total cost of final product. Efficient diagnosis and prognosis techniques have to be adopted to detect, isolate and anticipate faults. This paper presents an original integrated theoretical framework for diagnosis and prognosis of hybrid systems. The formalism used for hybrid diagnosis is enriched in order to be able to follow the evolution of an aging law for each fault of the system. The paper presents a methodology for interleaving diagnosis and prognosis in a hybrid framework. \n    ",
        "submission_date": "2013-08-24T00:00:00",
        "last_modified_date": "2013-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.0085",
        "title": "Artificial Intelligence Based Cognitive Routing for Cognitive Radio Networks",
        "authors": [
            "Junaid Qadir"
        ],
        "abstract": "Cognitive radio networks (CRNs) are networks of nodes equipped with cognitive radios that can optimize performance by adapting to network conditions. While cognitive radio networks (CRN) are envisioned as intelligent networks, relatively little research has focused on the network level functionality of CRNs. Although various routing protocols, incorporating varying degrees of adaptiveness, have been proposed for CRNs, it is imperative for the long term success of CRNs that the design of cognitive routing protocols be pursued by the research community. Cognitive routing protocols are envisioned as routing protocols that fully and seamless incorporate AI-based techniques into their design. In this paper, we provide a self-contained tutorial on various AI and machine-learning techniques that have been, or can be, used for developing cognitive routing protocols. We also survey the application of various classes of AI techniques to CRNs in general, and to the problem of routing in particular. We discuss various decision making techniques and learning techniques from AI and document their current and potential applications to the problem of routing in CRNs. We also highlight the various inference, reasoning, modeling, and learning sub tasks that a cognitive routing protocol must solve. Finally, open research issues and future directions of work are identified.\n    ",
        "submission_date": "2013-08-31T00:00:00",
        "last_modified_date": "2013-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.0671",
        "title": "BayesOpt: A Library for Bayesian optimization with Robotics Applications",
        "authors": [
            "Ruben Martinez-Cantin"
        ],
        "abstract": "The purpose of this paper is twofold. On one side, we present a general framework for Bayesian optimization and we compare it with some related fields in active learning and Bayesian numerical analysis. On the other hand, Bayesian optimization and related problems (bandits, sequential experimental design) are highly dependent on the surrogate model that is selected. However, there is no clear standard in the literature. Thus, we present a fast and flexible toolbox that allows to test and combine different models and criteria with little effort. It includes most of the state-of-the-art contributions, algorithms and models. Its speed also removes part of the stigma that Bayesian optimization methods are only good for \"expensive functions\". The software is free and it can be used in many operating systems and computer languages.\n    ",
        "submission_date": "2013-09-03T00:00:00",
        "last_modified_date": "2013-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.0866",
        "title": "On the Robustness of Temporal Properties for Stochastic Models",
        "authors": [
            "Ezio Bartocci",
            "Luca Bortolussi",
            "Laura Nenzi",
            "Guido Sanguinetti"
        ],
        "abstract": "Stochastic models such as Continuous-Time Markov Chains (CTMC) and Stochastic Hybrid Automata (SHA) are powerful formalisms to model and to reason about the dynamics of biological systems, due to their ability to capture the stochasticity inherent in biological processes. A classical question in formal modelling with clear relevance to biological modelling is the model checking problem. i.e. calculate the probability that a behaviour, expressed for instance in terms of a certain temporal logic formula, may occur in a given stochastic process. However, one may not only be interested in the notion of satisfiability, but also in the capacity of a system to mantain a particular emergent behaviour unaffected by the perturbations, caused e.g. from extrinsic noise, or by possible small changes in the model parameters. To address this issue, researchers from the verification community have recently proposed several notions of robustness for temporal logic providing suitable definitions of distance between a trajectory of a (deterministic) dynamical system and the boundaries of the set of trajectories satisfying the property of interest. The contributions of this paper are twofold. First, we extend the notion of robustness to stochastic systems, showing that this naturally leads to a distribution of robustness scores. By discussing two examples, we show how to approximate the distribution of the robustness score and its key indicators: the average robustness and the conditional average robustness. Secondly, we show how to combine these indicators with the satisfaction probability to address the system design problem, where the goal is to optimize some control parameters of a stochastic model in order to best maximize robustness of the desired specifications.\n    ",
        "submission_date": "2013-09-03T00:00:00",
        "last_modified_date": "2013-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.0962",
        "title": "Random Variables Recorded under Mutually Exclusive Conditions: Contextuality-by-Default",
        "authors": [
            "Ehtibar N. Dzhafarov",
            "Janne V. Kujala"
        ],
        "abstract": "We present general principles underlying analysis of the dependence of random variables (outputs) on deterministic conditions (inputs). Random outputs recorded under mutually exclusive input values are labeled by these values and considered stochastically unrelated, possessing no joint distribution. An input that does not directly influence an output creates a context for the latter. Any constraint imposed on the dependence of random outputs on inputs can be characterized by considering all possible couplings (joint distributions) imposed on stochastically unrelated outputs. The target application of these principles is a quantum mechanical system of entangled particles, with directions of spin measurements chosen for each particle being inputs and the spins recorded outputs. The sphere of applicability, however, spans systems across physical, biological, and behavioral sciences.\n    ",
        "submission_date": "2013-09-04T00:00:00",
        "last_modified_date": "2015-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.1524",
        "title": "Guided Self-Organization of Input-Driven Recurrent Neural Networks",
        "authors": [
            "Oliver Obst",
            "Joschka Boedecker"
        ],
        "abstract": "We review attempts that have been made towards understanding the computational properties and mechanisms of input-driven dynamical systems like RNNs, and reservoir computing networks in particular. We provide details on methods that have been developed to give quantitative answers to the questions above. Following this, we show how self-organization may be used to improve reservoirs for better performance, in some cases guided by the measures presented before. We also present a possible way to quantify task performance using an information-theoretic approach, and finally discuss promising future directions aimed at a better understanding of how these systems perform their computations and how to best guide self-organized processes for their optimization.\n    ",
        "submission_date": "2013-09-06T00:00:00",
        "last_modified_date": "2013-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.2080",
        "title": "Structure Learning of Probabilistic Logic Programs by Searching the Clause Space",
        "authors": [
            "Elena Bellodi",
            "Fabrizio Riguzzi"
        ],
        "abstract": "Learning probabilistic logic programming languages is receiving an increasing attention and systems are available for learning the parameters (PRISM, LeProbLog, LFI-ProbLog and EMBLEM) or both the structure and the parameters (SEM-CP-logic and SLIPCASE) of these languages. In this paper we present the algorithm SLIPCOVER for \"Structure LearnIng of Probabilistic logic programs by searChing OVER the clause space\". It performs a beam search in the space of probabilistic clauses and a greedy search in the space of theories, using the log likelihood of the data as the guiding heuristics. To estimate the log likelihood SLIPCOVER performs Expectation Maximization with EMBLEM. The algorithm has been tested on five real world datasets and compared with SLIPCASE, SEM-CP-logic, Aleph and two algorithms for learning Markov Logic Networks (Learning using Structural Motifs (LSM) and ALEPH++ExactL1). SLIPCOVER achieves higher areas under the precision-recall and ROC curves in most cases.\n    ",
        "submission_date": "2013-09-09T00:00:00",
        "last_modified_date": "2013-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.2796",
        "title": "Decision Trees for Function Evaluation - Simultaneous Optimization of Worst and Expected Cost",
        "authors": [
            "Ferdinando Cicalese",
            "Eduardo Laber",
            "Aline Medeiros Saettler"
        ],
        "abstract": "In several applications of automatic diagnosis and active learning a central problem is the evaluation of a discrete function by adaptively querying the values of its variables until the values read uniquely determine the value of the function. In general, the process of reading the value of a variable might involve some cost, computational or even a fee to be paid for the experiment required for obtaining the value. This cost should be taken into account when deciding the next variable to read. The goal is to design a strategy for evaluating the function incurring little cost (in the worst case or in expectation according to a prior distribution on the possible variables' assignments). Our algorithm builds a strategy (decision tree) which attains a logarithmic approxima- tion simultaneously for the expected and worst cost spent. This is best possible under the assumption that $P \\neq NP.$\n    ",
        "submission_date": "2013-09-11T00:00:00",
        "last_modified_date": "2014-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.3060",
        "title": "On SAT representations of XOR constraints",
        "authors": [
            "Matthew Gwynne",
            "Oliver Kullmann"
        ],
        "abstract": "We study the representation of systems S of linear equations over the two-element field (aka xor- or parity-constraints) via conjunctive normal forms F (boolean clause-sets). First we consider the problem of finding an \"arc-consistent\" representation (\"AC\"), meaning that unit-clause propagation will fix all forced assignments for all possible instantiations of the xor-variables. Our main negative result is that there is no polysize AC-representation in general. On the positive side we show that finding such an AC-representation is fixed-parameter tractable (fpt) in the number of equations. Then we turn to a stronger criterion of representation, namely propagation completeness (\"PC\") --- while AC only covers the variables of S, now all the variables in F (the variables in S plus auxiliary variables) are considered for PC. We show that the standard translation actually yields a PC representation for one equation, but fails so for two equations (in fact arbitrarily badly). We show that with a more intelligent translation we can also easily compute a translation to PC for two equations. We conjecture that computing a representation in PC is fpt in the number of equations.\n    ",
        "submission_date": "2013-09-12T00:00:00",
        "last_modified_date": "2013-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.3187",
        "title": "Cache Performance Study of Portfolio-Based Parallel CDCL SAT Solvers",
        "authors": [
            "Roberto As\u00edn",
            "Juan Olate",
            "Leo Ferres"
        ],
        "abstract": "Parallel SAT solvers are becoming mainstream. Their performance has made them win the past two SAT competitions consecutively and are in the limelight of research and industry. The problem is that it is not known exactly what is needed to make them perform even better; that is, how to make them solve more problems in less time. Also, it is also not know how well they scale in massive multi-core environments which, predictably, is the scenario of comming new hardware. In this paper we show that cache contention is a main culprit of a slowing down in scalability, and provide empirical results that for some type of searches, physically sharing the clause Database between threads is beneficial.\n    ",
        "submission_date": "2013-09-12T00:00:00",
        "last_modified_date": "2013-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.3197",
        "title": "Inducing Honest Reporting Without Observing Outcomes: An Application to the Peer-Review Process",
        "authors": [
            "Arthur Carvalho",
            "Stanko Dimitrov",
            "Kate Larson"
        ],
        "abstract": "When eliciting opinions from a group of experts, traditional devices used to promote honest reporting assume that there is an observable future outcome. In practice, however, this assumption is not always reasonable. In this paper, we propose a scoring method built on strictly proper scoring rules to induce honest reporting without assuming observable outcomes. Our method provides scores based on pairwise comparisons between the reports made by each pair of experts in the group. For ease of exposition, we introduce our scoring method by illustrating its application to the peer-review process. In order to do so, we start by modeling the peer-review process using a Bayesian model where the uncertainty regarding the quality of the manuscript is taken into account. Thereafter, we introduce our scoring method to evaluate the reported reviews. Under the assumptions that reviewers are Bayesian decision-makers and that they cannot influence the reviews of other reviewers, we show that risk-neutral reviewers strictly maximize their expected scores by honestly disclosing their reviews. We also show how the group's scores can be used to find a consensual review. Experimental results show that encouraging honest reporting through the proposed scoring method creates more accurate reviews than the traditional peer-review process.\n    ",
        "submission_date": "2013-09-12T00:00:00",
        "last_modified_date": "2013-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.3699",
        "title": "Local Support Vector Machines:Formulation and Analysis",
        "authors": [
            "Ravi Ganti",
            "Alexander Gray"
        ],
        "abstract": "We provide a formulation for Local Support Vector Machines (LSVMs) that generalizes previous formulations, and brings out the explicit connections to local polynomial learning used in nonparametric estimation literature. We investigate the simplest type of LSVMs called Local Linear Support Vector Machines (LLSVMs). For the first time we establish conditions under which LLSVMs make Bayes consistent predictions at each test point $x_0$. We also establish rates at which the local risk of LLSVMs converges to the minimum value of expected local risk at each point $x_0$. Using stability arguments we establish generalization error bounds for LLSVMs.\n    ",
        "submission_date": "2013-09-14T00:00:00",
        "last_modified_date": "2013-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.3775",
        "title": "Beyond the quantum formalism: consequences of a neural-oscillator model to quantum cognition",
        "authors": [
            "J. Acacio de Barros"
        ],
        "abstract": "In this paper we present a neural oscillator model of stimulus response theory that exhibits quantum-like behavior. We then show that without adding any additional assumptions, a quantum model constructed to fit observable pairwise correlations has no predictive power over the unknown triple moment, obtainable through the activation of multiple oscillators. We compare this with the results obtained in de Barros (2013), where a criteria of rationality gives optimal ranges for the triple moment.\n    ",
        "submission_date": "2013-09-15T00:00:00",
        "last_modified_date": "2014-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.4035",
        "title": "Domain and Function: A Dual-Space Model of Semantic Relations and Compositions",
        "authors": [
            "Peter D. Turney"
        ],
        "abstract": "Given appropriate representations of the semantic relations between carpenter and wood and between mason and stone (for example, vectors in a vector space model), a suitable algorithm should be able to recognize that these relations are highly similar (carpenter is to wood as mason is to stone; the relations are analogous). Likewise, with representations of dog, house, and kennel, an algorithm should be able to recognize that the semantic composition of dog and house, dog house, is highly similar to kennel (dog house and kennel are synonymous). It seems that these two tasks, recognizing relations and compositions, are closely connected. However, up to now, the best models for relations are significantly different from the best models for compositions. In this paper, we introduce a dual-space model that unifies these two tasks. This model matches the performance of the best previous models for relations and compositions. The dual-space model consists of a space for measuring domain similarity and a space for measuring function similarity. Carpenter and wood share the same domain, the domain of carpentry. Mason and stone share the same domain, the domain of masonry. Carpenter and mason share the same function, the function of artisans. Wood and stone share the same function, the function of materials. In the composition dog house, kennel has some domain overlap with both dog and house (the domains of pets and buildings). The function of kennel is similar to the function of house (the function of shelters). By combining domain and function similarities in various ways, we can model relations, compositions, and other aspects of semantics.\n    ",
        "submission_date": "2013-09-16T00:00:00",
        "last_modified_date": "2013-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.4291",
        "title": "Models and algorithms for skip-free Markov decision processes on trees",
        "authors": [
            "E. J. Collins"
        ],
        "abstract": "We introduce a class of models for multidimensional control problems which we call skip-free Markov decision processes on trees. We describe and analyse an algorithm applicable to Markov decision processes of this type that are skip-free in the negative direction. Starting with the finite average cost case, we show that the algorithm combines the advantages of both value iteration and policy iteration -- it is guaranteed to converge to an optimal policy and optimal value function after a finite number of iterations but the computational effort required for each iteration step is comparable with that for value iteration. We show that the algorithm can also be used to solve discounted cost models and continuous time models, and that a suitably modified algorithm can be used to solve communicating models.\n    ",
        "submission_date": "2013-09-17T00:00:00",
        "last_modified_date": "2013-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.4744",
        "title": "Modeling the Role of Context Dependency in the Recognition and Manifestation of Entrepreneurial Opportunity",
        "authors": [
            "Murad A. Mithani",
            "Tomas Veloz",
            "Liane Gabora"
        ],
        "abstract": "The paper uses the SCOP theory of concepts to model the role of environmental context on three levels of entrepreneurial opportunity: idea generation, idea development, and entrepreneurial decision. The role of contextual-fit in the generation and development of ideas is modeled as the collapse of their superposition state into one of the potential states that composes this superposition. The projection of this collapsed state on the socio-economic basis results in interference of the developed idea with the perceptions of the supporting community, undergoing an eventual collapse for an entrepreneurial decision that reflects the shared vision of its stakeholders. The developed idea may continue to evolve due to continuous or discontinuous changes in the environment. The model offers unique insights into the effects of external influences on entrepreneurial decisions.\n    ",
        "submission_date": "2013-09-18T00:00:00",
        "last_modified_date": "2019-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.4927",
        "title": "A finite axiomatization of conditional independence and inclusion dependencies",
        "authors": [
            "Miika Hannula",
            "Juha Kontinen"
        ],
        "abstract": "We present a complete finite axiomatization of the unrestricted implication problem for inclusion and conditional independence atoms in the context of dependence logic. For databases, our result implies a finite axiomatization of the unrestricted implication problem for inclusion, functional, and embedded multivalued dependencies in the unirelational case.\n    ",
        "submission_date": "2013-09-19T00:00:00",
        "last_modified_date": "2013-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6129",
        "title": "Partition-Merge: Distributed Inference and Modularity Optimization",
        "authors": [
            "Vincent Blondel",
            "Kyomin Jung",
            "Pushmeet Kohli",
            "Devavrat Shah"
        ],
        "abstract": "This paper presents a novel meta algorithm, Partition-Merge (PM), which takes existing centralized algorithms for graph computation and makes them distributed and faster. In a nutshell, PM divides the graph into small subgraphs using our novel randomized partitioning scheme, runs the centralized algorithm on each partition separately, and then stitches the resulting solutions to produce a global solution. We demonstrate the efficiency of the PM algorithm on two popular problems: computation of Maximum A Posteriori (MAP) assignment in an arbitrary pairwise Markov Random Field (MRF), and modularity optimization for community detection. We show that the resulting distributed algorithms for these problems essentially run in time linear in the number of nodes in the graph, and perform as well -- or even better -- than the original centralized algorithm as long as the graph has geometric structures. Here we say a graph has geometric structures, or polynomial growth property, when the number of nodes within distance r of any given node grows no faster than a polynomial function of r. More precisely, if the centralized algorithm is a C-factor approximation with constant C \\ge 1, the resulting distributed algorithm is a (C+\\delta)-factor approximation for any small \\delta>0; but if the centralized algorithm is a non-constant (e.g. logarithmic) factor approximation, then the resulting distributed algorithm becomes a constant factor approximation. For general graphs, we compute explicit bounds on the loss of performance of the resulting distributed algorithm with respect to the centralized algorithm.\n    ",
        "submission_date": "2013-09-24T00:00:00",
        "last_modified_date": "2013-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6449",
        "title": "Exploring Programmable Self-Assembly in Non-DNA based Molecular Computing",
        "authors": [
            "German Terrazas",
            "Hector Zenil",
            "Natalio Krasnogor"
        ],
        "abstract": "Self-assembly is a phenomenon observed in nature at all scales where autonomous entities build complex structures, without external influences nor centralised master plan. Modelling such entities and programming correct interactions among them is crucial for controlling the manufacture of desired complex structures at the molecular and supramolecular scale. This work focuses on a programmability model for non DNA-based molecules and complex behaviour analysis of their self-assembled conformations. In particular, we look into modelling, programming and simulation of porphyrin molecules self-assembly and apply Kolgomorov complexity-based techniques to classify and assess simulation results in terms of information content. The analysis focuses on phase transition, clustering, variability and parameter discovery which as a whole pave the way to the notion of complex systems programmability.\n    ",
        "submission_date": "2013-09-25T00:00:00",
        "last_modified_date": "2013-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6815",
        "title": "Lower Bounds for Exact Model Counting and Applications in Probabilistic Databases",
        "authors": [
            "Paul Beame",
            "Jerry Li",
            "Sudeepa Roy",
            "Dan Suciu"
        ],
        "abstract": "The best current methods for exactly computing the number of satisfying assignments, or the satisfying probability, of Boolean formulas can be seen, either directly or indirectly, as building 'decision-DNNF' (decision decomposable negation normal form) representations of the input Boolean formulas. Decision-DNNFs are a special case of 'd-DNNF's where 'd' stands for 'deterministic'. We show that any decision-DNNF can be converted into an equivalent 'FBDD' (free binary decision diagram) -- also known as a 'read-once branching program' (ROBP or 1-BP) -- with only a quasipolynomial increase in representation size in general, and with only a polynomial increase in size in the special case of monotone k-DNF formulas. Leveraging known exponential lower bounds for FBDDs, we then obtain similar exponential lower bounds for decision-DNNFs which provide lower bounds for the recent algorithms. We also separate the power of decision-DNNFs from d-DNNFs and a generalization of decision-DNNFs known as AND-FBDDs. Finally we show how these imply exponential lower bounds for natural problems associated with probabilistic databases.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6820",
        "title": "SparsityBoost: A New Scoring Function for Learning Bayesian Network Structure",
        "authors": [
            "Eliot Brenner",
            "David Sontag"
        ],
        "abstract": "We give a new consistent scoring function for structure learning of Bayesian networks. In contrast to traditional approaches to scorebased structure learning, such as BDeu or MDL, the complexity penalty that we propose is data-dependent and is given by the probability that a conditional independence test correctly shows that an edge cannot exist. What really distinguishes this new scoring function from earlier work is that it has the property of becoming computationally easier to maximize as the amount of data increases. We prove a polynomial sample complexity result, showing that maximizing this score is guaranteed to correctly learn a structure with no false edges and a distribution close to the generating distribution, whenever there exists a Bayesian network which is a perfect map for the data generating distribution. Although the new score can be used with any search algorithm, we give empirical results showing that it is particularly effective when used together with a linear programming relaxation approach to Bayesian network structure learning.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6849",
        "title": "Cyclic Causal Discovery from Continuous Equilibrium Data",
        "authors": [
            "Joris Mooij",
            "Tom Heskes"
        ],
        "abstract": "We propose a method for learning cyclic causal models from a combination of observational and interventional equilibrium data. Novel aspects of the proposed method are its ability to work with continuous data (without assuming linearity) and to deal with feedback loops. Within the context of biochemical reactions, we also propose a novel way of modeling interventions that modify the activity of compounds instead of their abundance. For computational reasons, we approximate the nonlinear causal mechanisms by (coupled) local linearizations, one for each experimental condition. We apply the method to reconstruct a cellular signaling network from the flow cytometry data measured by Sachs et al. (2005). We show that our method finds evidence in the data for feedback loops and that it gives a more accurate quantitative description of the data at comparable model complexity.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6851",
        "title": "Treedy: A Heuristic for Counting and Sampling Subsets",
        "authors": [
            "Teppo Niinimaki",
            "Mikko Koivisto"
        ],
        "abstract": "Consider a collection of weighted subsets of a ground set N. Given a query subset Q of N, how fast can one (1) find the weighted sum over all subsets of Q, and (2) sample a subset of Q proportionally to the weights? We present a tree-based greedy heuristic, Treedy, that for a given positive tolerance d answers such counting and sampling queries to within a guaranteed relative error d and total variation distance d, respectively. Experimental results on artificial instances and in application to Bayesian structure discovery in Bayesian networks show that approximations yield dramatic savings in running time compared to exact computation, and that Treedy typically outperforms a previously proposed sorting-based heuristic.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6860",
        "title": "Identifying Finite Mixtures of Nonparametric Product Distributions and Causal Inference of Confounders",
        "authors": [
            "Eleni Sgouritsa",
            "Dominik Janzing",
            "Jonas Peters",
            "Bernhard Schoelkopf"
        ],
        "abstract": "We propose a kernel method to identify finite mixtures of nonparametric product distributions. It is based on a Hilbert space embedding of the joint distribution. The rank of the constructed tensor is equal to the number of mixture components. We present an algorithm to recover the components by partitioning the data points into clusters such that the variables are jointly conditionally independent given the cluster. This method can be used to identify finite confounders.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6863",
        "title": "Sparse Nested Markov models with Log-linear Parameters",
        "authors": [
            "Ilya Shpitser",
            "Robin J. Evans",
            "Thomas S. Richardson",
            "James M. Robins"
        ],
        "abstract": "Hidden variables are ubiquitous in practical data analysis, and therefore modeling marginal densities and doing inference with the resulting models is an important problem in statistics, machine learning, and causal inference. Recently, a new type of graphical model, called the nested Markov model, was developed which captures equality constraints found in marginals of directed acyclic graph (DAG) models. Some of these constraints, such as the so called `Verma constraint', strictly generalize conditional independence. To make modeling and inference with nested Markov models practical, it is necessary to limit the number of parameters in the model, while still correctly capturing the constraints in the marginal of a DAG model. Placing such limits is similar in spirit to sparsity methods for undirected graphical models, and regression models. In this paper, we give a log-linear parameterization which allows sparse modeling with nested Markov models. We illustrate the advantages of this parameterization with a simulation study.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6883",
        "title": "Predicate Logic as a Modeling Language: Modeling and Solving some Machine Learning and Data Mining Problems with IDP3",
        "authors": [
            "Maurice Bruynooghe",
            "Hendrik Blockeel",
            "Bart Bogaerts",
            "Broes De Cat",
            "Stef De Pooter",
            "Joachim Jansen",
            "Anthony Labarre",
            "Jan Ramon",
            "Marc Denecker",
            "Sicco Verwer"
        ],
        "abstract": "This paper provides a gentle introduction to problem solving with the IDP3 system. The core of IDP3 is a finite model generator that supports first order logic enriched with types, inductive definitions, aggregates and partial functions. It offers its users a modeling language that is a slight extension of predicate logic and allows them to solve a wide range of search problems. Apart from a small introductory example, applications are selected from problems that arose within machine learning and data mining research. These research areas have recently shown a strong interest in declarative modeling and constraint solving as opposed to algorithmic approaches. The paper illustrates that the IDP3 system can be a valuable tool for researchers with such an interest.\n",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2014-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.7393",
        "title": "HeteSim: A General Framework for Relevance Measure in Heterogeneous Networks",
        "authors": [
            "Chuan Shi",
            "Xiangnan Kong",
            "Yue Huang",
            "Philip S. Yu",
            "Bin Wu"
        ],
        "abstract": "Similarity search is an important function in many applications, which usually focuses on measuring the similarity between objects with the same type. However, in many scenarios, we need to measure the relatedness between objects with different types. With the surge of study on heterogeneous networks, the relevance measure on objects with different types becomes increasingly important. In this paper, we study the relevance search problem in heterogeneous networks, where the task is to measure the relatedness of heterogeneous objects (including objects with the same type or different types). A novel measure HeteSim is proposed, which has the following attributes: (1) a uniform measure: it can measure the relatedness of objects with the same or different types in a uniform framework; (2) a path-constrained measure: the relatedness of object pairs are defined based on the search path that connect two objects through following a sequence of node types; (3) a semi-metric measure: HeteSim has some good properties (e.g., self-maximum and symmetric), that are crucial to many data mining tasks. Moreover, we analyze the computation characteristics of HeteSim and propose the corresponding quick computation strategies. Empirical studies show that HeteSim can effectively and efficiently evaluate the relatedness of heterogeneous objects.\n    ",
        "submission_date": "2013-09-28T00:00:00",
        "last_modified_date": "2013-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.7405",
        "title": "A Model of the Mechanisms Underlying Exploratory Behaviour",
        "authors": [
            "Liane Gabora",
            "Patrick Colgan"
        ],
        "abstract": "A model of the mechanisms underlying exploratory behaviour, based on empirical research and refined using a computer simulation, is presented. The behaviour of killifish from two lakes, one with killifish predators and one without, was compared in the laboratory. Plotting average activity in a novel environment versus time resulted in an inverted-U-shaped curve for both groups; however, the curve for killifish from the lake without predators was (1) steeper, (2) reached a peak value earlier, (S) reached a higher peak value, and (4) subsumed less area than the curve for killifish from the lake with predators. We hypothesize that the shape of the exploration curve reflects a competition between motivational subsystems that excite and inhibit exploratory behaviour in a way that is tuned to match the affordance probabilities of the animal's environment. A computer implementation of this model produced curves which differed along the same four dimensions as differentiate the two killifish curves. All four differences were reproduced in the model by tuning a single parameter: the time-dependent component of the decay-rate of the exploration-inhibiting subsystem.\n    ",
        "submission_date": "2013-09-28T00:00:00",
        "last_modified_date": "2019-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.7407",
        "title": "Concept Combination and the Origins of Complex Cognition",
        "authors": [
            "Liane Gabora",
            "Kirsty Kitto"
        ],
        "abstract": "At the core of our uniquely human cognitive abilities is the capacity to see things from different perspectives, or to place them in a new context. We propose that this was made possible by two cognitive transitions. First, the large brain of Homo erectus facilitated the onset of recursive recall: the ability to string thoughts together into a stream of potentially abstract or imaginative thought. This hypothesis is supported by a set of computational models where an artificial society of agents evolved to generate more diverse and valuable cultural outputs under conditions of recursive recall. We propose that the capacity to see things in context arose much later, following the appearance of anatomically modern humans. This second transition was brought on by the onset of contextual focus: the capacity to shift between a minimally contextual analytic mode of thought, and a highly contextual associative mode of thought, conducive to combining concepts in new ways and 'breaking out of a rut'. When contextual focus is implemented in an art-generating computer program, the resulting artworks are seen as more creative and appealing. We summarize how both transitions can be modeled using a theory of concepts which highlights the manner in which different contexts can lead to modern humans attributing very different meanings to the interpretation of one concept.\n    ",
        "submission_date": "2013-09-28T00:00:00",
        "last_modified_date": "2019-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.0576",
        "title": "Learning Lambek grammars from proof frames",
        "authors": [
            "Roberto Bonato",
            "Christian Retor\u00e9"
        ],
        "abstract": "In addition to their limpid interface with semantics, categorial grammars enjoy another important property: learnability. This was first noticed by Buskowsky and Penn and further studied by Kanazawa, for Bar-Hillel categorial grammars.\n",
        "submission_date": "2013-10-02T00:00:00",
        "last_modified_date": "2013-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.0967",
        "title": "The SAT-UNSAT transition in the adversarial SAT problem",
        "authors": [
            "Marco Bardoscia",
            "Daniel Nagaj",
            "Antonello Scardicchio"
        ],
        "abstract": "Adversarial SAT (AdSAT) is a generalization of the satisfiability (SAT) problem in which two players try to make a boolean formula true (resp. false) by controlling their respective sets of variables. AdSAT belongs to a higher complexity class in the polynomial hierarchy than SAT and therefore the nature of the critical region and the transition are not easily paralleled to those of SAT and worth of independent study. AdSAT also provides an upper bound for the transition threshold of the quantum satisfiability problem (QSAT). We present a complete algorithm for AdSAT, show that 2-AdSAT is in $\\mathbf{P}$, and then study two stochastic algorithms (simulated annealing and its improved variant) and compare their performances in detail for 3-AdSAT. Varying the density of clauses $\\alpha$ we find a sharp SAT-UNSAT transition at a critical value whose upper bound is $\\alpha_c \\lesssim 1.5$, thus providing a much stricter upper bound for the QSAT transition than those previously found.\n    ",
        "submission_date": "2013-10-03T00:00:00",
        "last_modified_date": "2014-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.1137",
        "title": "GOTCHA Password Hackers!",
        "authors": [
            "Jeremiah Blocki",
            "Manuel Blum",
            "Anupam Datta"
        ],
        "abstract": "We introduce GOTCHAs (Generating panOptic Turing Tests to Tell Computers and Humans Apart) as a way of preventing automated offline dictionary attacks against user selected passwords. A GOTCHA is a randomized puzzle generation protocol, which involves interaction between a computer and a human. Informally, a GOTCHA should satisfy two key properties: (1) The puzzles are easy for the human to solve. (2) The puzzles are hard for a computer to solve even if it has the random bits used by the computer to generate the final puzzle --- unlike a CAPTCHA. Our main theorem demonstrates that GOTCHAs can be used to mitigate the threat of offline dictionary attacks against passwords by ensuring that a password cracker must receive constant feedback from a human being while mounting an attack. Finally, we provide a candidate construction of GOTCHAs based on Inkblot images. Our construction relies on the usability assumption that users can recognize the phrases that they originally used to describe each Inkblot image --- a much weaker usability assumption than previous password systems based on Inkblots which required users to recall their phrase exactly. We conduct a user study to evaluate the usability of our GOTCHA construction. We also generate a GOTCHA challenge where we encourage artificial intelligence and security researchers to try to crack several passwords protected with our scheme.\n    ",
        "submission_date": "2013-10-04T00:00:00",
        "last_modified_date": "2013-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.1187",
        "title": "Labeled Directed Acyclic Graphs: a generalization of context-specific independence in directed graphical models",
        "authors": [
            "Johan Pensar",
            "Henrik Nyman",
            "Timo Koski",
            "Jukka Corander"
        ],
        "abstract": "We introduce a novel class of labeled directed acyclic graph (LDAG) models for finite sets of discrete variables. LDAGs generalize earlier proposals for allowing local structures in the conditional probability distribution of a node, such that unrestricted label sets determine which edges can be deleted from the underlying directed acyclic graph (DAG) for a given context. Several properties of these models are derived, including a generalization of the concept of Markov equivalence classes. Efficient Bayesian learning of LDAGs is enabled by introducing an LDAG-based factorization of the Dirichlet prior for the model parameters, such that the marginal likelihood can be calculated analytically. In addition, we develop a novel prior distribution for the model structures that can appropriately penalize a model for its labeling complexity. A non-reversible Markov chain Monte Carlo algorithm combined with a greedy hill climbing approach is used for illustrating the useful properties of LDAG models for both real and synthetic data sets.\n    ",
        "submission_date": "2013-10-04T00:00:00",
        "last_modified_date": "2013-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.1537",
        "title": "SIMD Parallel MCMC Sampling with Applications for Big-Data Bayesian Analytics",
        "authors": [
            "Alireza S. Mahani",
            "Mansour T.A. Sharabiani"
        ],
        "abstract": "Computational intensity and sequential nature of estimation techniques for Bayesian methods in statistics and machine learning, combined with their increasing applications for big data analytics, necessitate both the identification of potential opportunities to parallelize techniques such as MCMC sampling, and the development of general strategies for mapping such parallel algorithms to modern CPUs in order to elicit the performance up the compute-based and/or memory-based hardware limits. Two opportunities for Single-Instruction Multiple-Data (SIMD) parallelization of MCMC sampling for probabilistic graphical models are presented. In exchangeable models with many observations such as Bayesian Generalized Linear Models, child-node contributions to the conditional posterior of each node can be calculated concurrently. In undirected graphs with discrete nodes, concurrent sampling of conditionally-independent nodes can be transformed into a SIMD form. High-performance libraries with multi-threading and vectorization capabilities can be readily applied to such SIMD opportunities to gain decent speedup, while a series of high-level source-code and runtime modifications provide further performance boost by reducing parallelization overhead and increasing data locality for NUMA architectures. For big-data Bayesian GLM graphs, the end-result is a routine for evaluating the conditional posterior and its gradient vector that is 5 times faster than a naive implementation using (built-in) multi-threaded Intel MKL BLAS, and reaches within the striking distance of the memory-bandwidth-induced hardware limit. The proposed optimization strategies improve the scaling of performance with number of cores and width of vector units (applicable to many-core SIMD processors such as Intel Xeon Phi and GPUs), resulting in cost-effectiveness, energy efficiency, and higher speed on multi-core x86 processors.\n    ",
        "submission_date": "2013-10-06T00:00:00",
        "last_modified_date": "2014-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.1597",
        "title": "Cross-lingual Pseudo-Projected Expectation Regularization for Weakly Supervised Learning",
        "authors": [
            "Mengqiu Wang",
            "Christopher D. Manning"
        ],
        "abstract": "We consider a multilingual weakly supervised learning scenario where knowledge from annotated corpora in a resource-rich language is transferred via bitext to guide the learning in other languages. Past approaches project labels across bitext and use them as features or gold labels for training. We propose a new method that projects model expectations rather than labels, which facilities transfer of model uncertainty across language boundaries. We encode expectations as constraints and train a discriminative CRF model using Generalized Expectation Criteria (Mann and McCallum, 2010). Evaluated on standard Chinese-English and German-English NER datasets, our method demonstrates F1 scores of 64% and 60% when no labeled data is used. Attaining the same accuracy with supervised CRFs requires 12k and 1.5k labeled sentences. Furthermore, when combined with labeled examples, our method yields significant improvements over state-of-the-art supervised methods, achieving best reported numbers to date on Chinese OntoNotes and German CoNLL-03 datasets.\n    ",
        "submission_date": "2013-10-06T00:00:00",
        "last_modified_date": "2013-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.2098",
        "title": "A short note on the axiomatic requirements of uncertainty measure",
        "authors": [
            "Xinyang Deng",
            "Yong Deng"
        ],
        "abstract": "In this note, we argue that the axiomatic requirement of range to the measure of aggregated total uncertainty (ATU) in Dempster-Shafer theory is not reasonable.\n    ",
        "submission_date": "2013-10-08T00:00:00",
        "last_modified_date": "2013-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.2627",
        "title": "A Sparse and Adaptive Prior for Time-Dependent Model Parameters",
        "authors": [
            "Dani Yogatama",
            "Bryan R. Routledge",
            "Noah A. Smith"
        ],
        "abstract": "We consider the scenario where the parameters of a probabilistic model are expected to vary over time. We construct a novel prior distribution that promotes sparsity and adapts the strength of correlation between parameters at successive timesteps, based on the data. We derive approximate variational inference procedures for learning and prediction with this prior. We test the approach on two tasks: forecasting financial quantities from relevant text, and modeling language contingent on time-varying financial measurements.\n    ",
        "submission_date": "2013-10-09T00:00:00",
        "last_modified_date": "2015-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.3225",
        "title": "A Turing test for free will",
        "authors": [
            "Seth Lloyd"
        ],
        "abstract": "Before Alan Turing made his crucial contributions to the theory of computation, he studied the question of whether quantum mechanics could throw light on the nature of free will. This article investigates the roles of quantum mechanics and computation in free will. Although quantum mechanics implies that events are intrinsically unpredictable, the `pure stochasticity' of quantum mechanics adds only randomness to decision making processes, not freedom. By contrast, the theory of computation implies that even when our decisions arise from a completely deterministic decision-making process, the outcomes of that process can be intrinsically unpredictable, even to -- especially to -- ourselves. I argue that this intrinsic computational unpredictability of the decision making process is what give rise to our impression that we possess free will. Finally, I propose a `Turing test' for free will: a decision maker who passes this test will tend to believe that he, she, or it possesses free will, whether the world is deterministic or not.\n    ",
        "submission_date": "2013-10-11T00:00:00",
        "last_modified_date": "2013-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.3692",
        "title": "Changing the Environment based on Intrinsic Motivation",
        "authors": [
            "Christoph Salge",
            "Daniel Polani"
        ],
        "abstract": "One of the remarkable feats of intelligent life is that it restructures the world it lives in for its own benefit. This extended abstract outlines how the information-theoretic principle of empowerment, as an intrinsic motivation, can be used to restructure the environment an agent lives in. We present a first qualitative evaluation of how an agent in a 3d-gridworld builds a staircase-like structure, which reflects the agent's embodiment.\n    ",
        "submission_date": "2013-10-14T00:00:00",
        "last_modified_date": "2013-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.3781",
        "title": "An Agent-based Model of the Cognitive Mechanisms Underlying the Origins of Creative Cultural Evolution",
        "authors": [
            "Liane Gabora",
            "Maryam Saberi"
        ],
        "abstract": "Human culture is uniquely cumulative and open-ended. Using a computational model of cultural evolution in which neural network based agents evolve ideas for actions through invention and imitation, we tested the hypothesis that this is due to the capacity for recursive recall. We compared runs in which agents were limited to single-step actions to runs in which they used recursive recall to chain simple actions into complex ones. Chaining resulted in higher cultural diversity, open-ended generation of novelty, and no ceiling on the mean fitness of actions. Both chaining and no-chaining runs exhibited convergence on optimal actions, but without chaining this set was static while with chaining it was ever-changing. Chaining increased the ability to capitalize on the capacity for learning. These findings show that the recursive recall hypothesis provides a computationally plausible explanation of why humans alone have evolved the cultural means to transform this planet.\n    ",
        "submission_date": "2013-10-14T00:00:00",
        "last_modified_date": "2019-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.4756",
        "title": "Effectiveness of pre- and inprocessing for CDCL-based SAT solving",
        "authors": [
            "Andreas Wotzlaw",
            "Alexander van der Grinten",
            "Ewald Speckenmeyer"
        ],
        "abstract": "Applying pre- and inprocessing techniques to simplify CNF formulas both before and during search can considerably improve the performance of modern SAT solvers. These algorithms mostly aim at reducing the number of clauses, literals, and variables in the formula. However, to be worthwhile, it is necessary that their additional runtime does not exceed the runtime saved during the subsequent SAT solver execution. In this paper we investigate the efficiency and the practicability of selected simplification algorithms for CDCL-based SAT solving. We first analyze them by means of their expected impact on the CNF formula and SAT solving at all. While testing them on real-world and combinatorial SAT instances, we show which techniques and combinations of them yield a desirable speedup and which ones should be avoided.\n    ",
        "submission_date": "2013-10-17T00:00:00",
        "last_modified_date": "2013-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.4938",
        "title": "A Logic-based Approach for Recognizing Textual Entailment Supported by Ontological Background Knowledge",
        "authors": [
            "Andreas Wotzlaw",
            "Ravi Coote"
        ],
        "abstract": "We present the architecture and the evaluation of a new system for recognizing textual entailment (RTE). In RTE we want to identify automatically the type of a logical relation between two input texts. In particular, we are interested in proving the existence of an entailment between them. We conceive our system as a modular environment allowing for a high-coverage syntactic and semantic text analysis combined with logical inference. For the syntactic and semantic analysis we combine a deep semantic analysis with a shallow one supported by statistical models in order to increase the quality and the accuracy of results. For RTE we use logical inference of first-order employing model-theoretic techniques and automated reasoning tools. The inference is supported with problem-relevant background knowledge extracted automatically and on demand from external sources like, e.g., WordNet, YAGO, and OpenCyc, or other, more experimental sources with, e.g., manually defined presupposition resolutions, or with axiomatized general and common sense knowledge. The results show that fine-grained and consistent knowledge coming from diverse sources is a necessary condition determining the correctness and traceability of results.\n    ",
        "submission_date": "2013-10-18T00:00:00",
        "last_modified_date": "2013-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.5042",
        "title": "Distributional semantics beyond words: Supervised learning of analogy and paraphrase",
        "authors": [
            "Peter D. Turney"
        ],
        "abstract": "There have been several efforts to extend distributional semantics beyond individual words, to measure the similarity of word pairs, phrases, and sentences (briefly, tuples; ordered sets of words, contiguous or noncontiguous). One way to extend beyond words is to compare two tuples using a function that combines pairwise similarities between the component words in the tuples. A strength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase). However, past work required hand-coding the combination function for different tasks. The main contribution of this paper is that combination functions are generated by supervised learning. We achieve state-of-the-art results in measuring relational similarity between word pairs (SAT analogies and SemEval~2012 Task 2) and measuring compositional similarity between noun-modifier phrases and unigrams (multiple-choice paraphrase questions).\n    ",
        "submission_date": "2013-10-18T00:00:00",
        "last_modified_date": "2013-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.5288",
        "title": "GPatt: Fast Multidimensional Pattern Extrapolation with Gaussian Processes",
        "authors": [
            "Andrew Gordon Wilson",
            "Elad Gilboa",
            "Arye Nehorai",
            "John P. Cunningham"
        ],
        "abstract": "Gaussian processes are typically used for smoothing and interpolation on small datasets. We introduce a new Bayesian nonparametric framework -- GPatt -- enabling automatic pattern extrapolation with Gaussian processes on large multidimensional datasets. GPatt unifies and extends highly expressive kernels and fast exact inference techniques. Without human intervention -- no hand crafting of kernel features, and no sophisticated initialisation procedures -- we show that GPatt can solve large scale pattern extrapolation, inpainting, and kernel discovery problems, including a problem with 383400 training points. We find that GPatt significantly outperforms popular alternative scalable Gaussian process methods in speed and accuracy. Moreover, we discover profound differences between each of these methods, suggesting expressive kernels, nonparametric representations, and exact inference are useful for modelling large scale multidimensional patterns.\n    ",
        "submission_date": "2013-10-20T00:00:00",
        "last_modified_date": "2013-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.5463",
        "title": "Engineering Crowdsourced Stream Processing Systems",
        "authors": [
            "Muhammad Imran",
            "Ioanna Lykourentzou",
            "Yannick Naudet",
            "Carlos Castillo"
        ],
        "abstract": "A crowdsourced stream processing system (CSP) is a system that incorporates crowdsourced tasks in the processing of a data stream. This can be seen as enabling crowdsourcing work to be applied on a sample of large-scale data at high speed, or equivalently, enabling stream processing to employ human intelligence. It also leads to a substantial expansion of the capabilities of data processing systems. Engineering a CSP system requires the combination of human and machine computation elements. From a general systems theory perspective, this means taking into account inherited as well as emerging properties from both these elements. In this paper, we position CSP systems within a broader taxonomy, outline a series of design principles and evaluation metrics, present an extensible framework for their design, and describe several design patterns. We showcase the capabilities of CSP systems by performing a case study that applies our proposed framework to the design and analysis of a real system (AIDR) that classifies social media messages during time-critical crisis events. Results show that compared to a pure stream processing system, AIDR can achieve a higher data classification accuracy, while compared to a pure crowdsourcing solution, the system makes better use of human workers by requiring much less manual work effort.\n    ",
        "submission_date": "2013-10-21T00:00:00",
        "last_modified_date": "2014-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.5488",
        "title": "A practical approach to ontology-enabled control systems for astronomical instrumentation",
        "authors": [
            "Wim Pessemier",
            "Gert Raskin",
            "Hans Van Winckel",
            "Geert Deconinck",
            "Philippe Saey"
        ],
        "abstract": "Even though modern service-oriented and data-oriented architectures promise to deliver loosely coupled control systems, they are inherently brittle as they commonly depend on a priori agreed interfaces and data models. At the same time, the Semantic Web and a whole set of accompanying standards and tools are emerging, advocating ontologies as the basis for knowledge exchange. In this paper we aim to identify a number of key ideas from the myriad of knowledge-based practices that can readily be implemented by control systems today. We demonstrate with a practical example (a three-channel imager for the Mercator Telescope) how ontologies developed in the Web Ontology Language (OWL) can serve as a meta-model for our instrument, covering as many engineering aspects of the project as needed. We show how a concrete system model can be built on top of this meta-model via a set of Domain Specific Languages (DSLs), supporting both formal verification and the generation of software and documentation artifacts. Finally we reason how the available semantics can be exposed at run-time by adding a \"semantic layer\" that can be browsed, queried, monitored etc. by any OPC UA-enabled client.\n    ",
        "submission_date": "2013-10-21T00:00:00",
        "last_modified_date": "2013-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.5781",
        "title": "RANSAC: Identification of Higher-Order Geometric Features and Applications in Humanoid Robot Soccer",
        "authors": [
            "Madison Flannery",
            "Shannon Fenn",
            "David Budden"
        ],
        "abstract": "The ability for an autonomous agent to self-localise is directly proportional to the accuracy and precision with which it can perceive salient features within its local environment. The identification of such features by recognising geometric profile allows robustness against lighting variations, which is necessary in most industrial robotics applications. This paper details a framework by which the random sample consensus (RANSAC) algorithm, often applied to parameter fitting in linear models, can be extended to identify higher-order geometric features. Goalpost identification within humanoid robot soccer is investigated as an application, with the developed system yielding an order-of-magnitude improvement in classification performance relative to a traditional histogramming methodology.\n    ",
        "submission_date": "2013-10-22T00:00:00",
        "last_modified_date": "2013-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.6257",
        "title": "Dissociation and Propagation for Approximate Lifted Inference with Standard Relational Database Management Systems",
        "authors": [
            "Wolfgang Gatterbauer",
            "Dan Suciu"
        ],
        "abstract": "Probabilistic inference over large data sets is a challenging data management problem since exact inference is generally #P-hard and is most often solved approximately with sampling-based methods today. This paper proposes an alternative approach for approximate evaluation of conjunctive queries with standard relational databases: In our approach, every query is evaluated entirely in the database engine by evaluating a fixed number of query plans, each providing an upper bound on the true probability, then taking their minimum. We provide an algorithm that takes into account important schema information to enumerate only the minimal necessary plans among all possible plans. Importantly, this algorithm is a strict generalization of all known PTIME self-join-free conjunctive queries: A query is in PTIME if and only if our algorithm returns one single plan. Furthermore, our approach is a generalization of a family of efficient ranking methods from graphs to hypergraphs. We also adapt three relational query optimization techniques to evaluate all necessary plans very fast. We give a detailed experimental evaluation of our approach and, in the process, provide a new way of thinking about the value of probabilistic methods over non-probabilistic methods for ranking query answers. We also note that the techniques developed in this paper apply immediately to lifted inference from statistical relational models since lifted inference corresponds to PTIME plans in probabilistic databases.\n    ",
        "submission_date": "2013-10-23T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.6288",
        "title": "Spatial-Spectral Boosting Analysis for Stroke Patients' Motor Imagery EEG in Rehabilitation Training",
        "authors": [
            "Hao Zhang",
            "Liqing Zhang"
        ],
        "abstract": "Current studies about motor imagery based rehabilitation training systems for stroke subjects lack an appropriate analytic method, which can achieve a considerable classification accuracy, at the same time detects gradual changes of imagery patterns during rehabilitation process and disinters potential mechanisms about motor function recovery. In this study, we propose an adaptive boosting algorithm based on the cortex plasticity and spectral band shifts. This approach models the usually predetermined spatial-spectral configurations in EEG study into variable preconditions, and introduces a new heuristic of stochastic gradient boost for training base learners under these preconditions. We compare our proposed algorithm with commonly used methods on datasets collected from 2 months' clinical experiments. The simulation results demonstrate the effectiveness of the method in detecting the variations of stroke patients' EEG patterns. By chronologically reorganizing the weight parameters of the learned additive model, we verify the spatial compensatory mechanism on impaired cortex and detect the changes of accentuation bands in spectral domain, which may contribute important prior knowledge for rehabilitation practice.\n    ",
        "submission_date": "2013-10-23T00:00:00",
        "last_modified_date": "2013-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.6343",
        "title": "Provable Bounds for Learning Some Deep Representations",
        "authors": [
            "Sanjeev Arora",
            "Aditya Bhaskara",
            "Rong Ge",
            "Tengyu Ma"
        ],
        "abstract": "We give algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others. Our generative model is an $n$ node multilayer neural net that has degree at most $n^{\\gamma}$ for some $\\gamma <1$ and each edge has a random edge weight in $[-1,1]$. Our algorithm learns {\\em almost all} networks in this class with polynomial running time. The sample complexity is quadratic or cubic depending upon the details of the model.\n",
        "submission_date": "2013-10-23T00:00:00",
        "last_modified_date": "2013-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.7163",
        "title": "Generalized Thompson Sampling for Contextual Bandits",
        "authors": [
            "Lihong Li"
        ],
        "abstract": "Thompson Sampling, one of the oldest heuristics for solving multi-armed bandits, has recently been shown to demonstrate state-of-the-art performance. The empirical success has led to great interests in theoretical understanding of this heuristic. In this paper, we approach this problem in a way very different from existing efforts. In particular, motivated by the connection between Thompson Sampling and exponentiated updates, we propose a new family of algorithms called Generalized Thompson Sampling in the expert-learning framework, which includes Thompson Sampling as a special case. Similar to most expert-learning algorithms, Generalized Thompson Sampling uses a loss function to adjust the experts' weights. General regret bounds are derived, which are also instantiated to two important loss functions: square loss and logarithmic loss. In contrast to existing bounds, our results apply to quite general contextual bandits. More importantly, they quantify the effect of the \"prior\" distribution on the regret bounds.\n    ",
        "submission_date": "2013-10-27T00:00:00",
        "last_modified_date": "2013-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.7610",
        "title": "Distributed Reinforcement Learning via Gossip",
        "authors": [
            "Adwaitvedant S. Mathkar",
            "Vivek S. Borkar"
        ],
        "abstract": "We consider the classical TD(0) algorithm implemented on a network of agents wherein the agents also incorporate the updates received from neighboring agents using a gossip-like mechanism. The combined scheme is shown to converge for both discounted and average cost problems.\n    ",
        "submission_date": "2013-10-28T00:00:00",
        "last_modified_date": "2013-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.7682",
        "title": "Contextualizing concepts using a mathematical generalization of the quantum formalism",
        "authors": [
            "Liane Gabora",
            "Diederik Aerts"
        ],
        "abstract": "We outline the rationale and preliminary results of using the state context property (SCOP) formalism, originally developed as a generalization of quantum mechanics, to describe the contextual manner in which concepts are evoked, used and combined to generate meaning. The quantum formalism was developed to cope with problems arising in the description of (i) the measurement process, and (ii) the generation of new states with new properties when particles become entangled. Similar problems arising with concepts motivated the formal treatment introduced here. Concepts are viewed not as fixed representations, but entities existing in states of potentiality that require interaction with a context-a stimulus or another concept-to 'collapse' to an instantiated form (e.g. exemplar, prototype, or other possibly imaginary instance). The stimulus situation plays the role of the measurement in physics, acting as context that induces a change of the cognitive state from superposition state to collapsed state. The collapsed state is more likely to consist of a conjunction of concepts for associative than analytic thought because more stimulus or concept properties take part in the collapse. We provide two contextual measures of conceptual distance-one using collapse probabilities and the other weighted properties-and show how they can be applied to conjunctions using the pet fish problem.\n    ",
        "submission_date": "2013-10-29T00:00:00",
        "last_modified_date": "2013-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.7950",
        "title": "Technical Report: Distribution Temporal Logic: Combining Correctness with Quality of Estimation",
        "authors": [
            "Austin Jones",
            "Mac Schwager",
            "Calin Belta"
        ],
        "abstract": "We present a new temporal logic called Distribution Temporal Logic (DTL) defined over predicates of belief states and hidden states of partially observable systems. DTL can express properties involving uncertainty and likelihood that cannot be described by existing logics. A co-safe formulation of DTL is defined and algorithmic procedures are given for monitoring executions of a partially observable Markov decision process with respect to such formulae. A simulation case study of a rescue robotics application outlines our approach.\n    ",
        "submission_date": "2013-09-09T00:00:00",
        "last_modified_date": "2013-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.7961",
        "title": "Evaluation the efficiency of artificial bee colony and the firefly algorithm in solving the continuous optimization problem",
        "authors": [
            "Seyyed Reza Khaze",
            "Isa maleki",
            "Sohrab Hojjatkhah",
            "Ali Bagherinia"
        ],
        "abstract": "Now the Meta-Heuristic algorithms have been used vastly in solving the problem of continuous optimization. In this paper the Artificial Bee Colony (ABC) algorithm and the Firefly Algorithm (FA) are valuated. And for presenting the efficiency of the algorithms and also for more analysis of them, the continuous optimization problems which are of the type of the problems of vast limit of answer and the close optimized points are tested. So, in this paper the efficiency of the ABC algorithm and FA are presented for solving the continuous optimization problems and also the said algorithms are studied from the accuracy in reaching the optimized solution and the resulting time and the reliability of the optimized answer points of view.\n    ",
        "submission_date": "2013-09-16T00:00:00",
        "last_modified_date": "2013-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.7981",
        "title": "Toward a Formal Model of the Shifting Relationship between Concepts and Contexts during Associative Thought",
        "authors": [
            "Tomas Veloz",
            "Liane Gabora",
            "Mark Eyjolfson",
            "Diederik Aerts"
        ],
        "abstract": "The quantum inspired State Context Property (SCOP) theory of concepts is unique amongst theories of concepts in offering a means of incorporating that for each concept in each different context there are an unlimited number of exemplars, or states, of varying degrees of typicality. Working with data from a study in which participants were asked to rate the typicality of exemplars of a concept for different contexts, and introducing an exemplar typicality threshold, we built a SCOP model of how states of a concept arise differently in associative versus analytic (or divergent and convergent) modes of thought. Introducing measures of state robustness and context relevance, we show that by varying the threshold, the relevance of different contexts changes, and seemingly atypical states can become typical. The formalism provides a pivotal step toward a formal explanation of creative thought proesses.\n    ",
        "submission_date": "2013-10-29T00:00:00",
        "last_modified_date": "2013-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.8583",
        "title": "A Hybrid Local Search for Simplified Protein Structure Prediction",
        "authors": [
            "Swakkhar Shatabda",
            "M.A. Hakim Newton",
            "Duc Nghia Pham",
            "Abdul Sattar"
        ],
        "abstract": "Protein structure prediction based on Hydrophobic-Polar energy model essentially becomes searching for a conformation having a compact hydrophobic core at the center. The hydrophobic core minimizes the interaction energy between the amino acids of the given protein. Local search algorithms can quickly find very good conformations by moving repeatedly from the current solution to its \"best\" neighbor. However, once such a compact hydrophobic core is found, the search stagnates and spends enormous effort in quest of an alternative core. In this paper, we attempt to restructure segments of a conformation with such compact core. We select one large segment or a number of small segments and apply exhaustive local search. We also apply a mix of heuristics so that one heuristic can help escape local minima of another. We evaluated our algorithm by using Face Centered Cubic (FCC) Lattice on a set of standard benchmark proteins and obtain significantly better results than that of the state-of-the-art methods.\n    ",
        "submission_date": "2013-10-31T00:00:00",
        "last_modified_date": "2013-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.0541",
        "title": "Free-configuration Biased Sampling for Motion Planning: Errata",
        "authors": [
            "Joshua Bialkowski",
            "Michael Otte",
            "Emilio Frazzoli"
        ],
        "abstract": "This document contains improved and updated proofs of convergence for the sampling method presented in our paper \"Free-configuration Biased Sampling for Motion Planning\".\n    ",
        "submission_date": "2013-11-03T00:00:00",
        "last_modified_date": "2013-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.1213",
        "title": "A Big Data Approach to Computational Creativity",
        "authors": [
            "Lav R. Varshney",
            "Florian Pinel",
            "Kush R. Varshney",
            "Debarun Bhattacharjya",
            "Angela Schoergendorfer",
            "Yi-Min Chee"
        ],
        "abstract": "Computational creativity is an emerging branch of artificial intelligence that places computers in the center of the creative process. Broadly, creativity involves a generative step to produce many ideas and a selective step to determine the ones that are the best. Many previous attempts at computational creativity, however, have not been able to achieve a valid selective step. This work shows how bringing data sources from the creative domain and from hedonic psychophysics together with big data analytics techniques can overcome this shortcoming to yield a system that can produce novel and high-quality creative artifacts. Our data-driven approach is demonstrated through a computational creativity system for culinary recipes and menus we developed and deployed, which can operate either autonomously or semi-autonomously with human interaction. We also comment on the volume, velocity, variety, and veracity of data in computational creativity.\n    ",
        "submission_date": "2013-11-05T00:00:00",
        "last_modified_date": "2013-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.1436",
        "title": "Adaptive Measurement-Based Policy-Driven QoS Management with Fuzzy-Rule-based Resource Allocation",
        "authors": [
            "Suleiman Y. Yerima",
            "Gerard P. Parr",
            "Sally I. McClean",
            "Philip J. Morrow"
        ],
        "abstract": "Fixed and wireless networks are increasingly converging towards common connectivity with IP-based core networks. Providing effective end-to-end resource and QoS management in such complex heterogeneous converged network scenarios requires unified, adaptive and scalable solutions to integrate and co-ordinate diverse QoS mechanisms of different access technologies with IP-based QoS. Policy-Based Network Management (PBNM) is one approach that could be employed to address this challenge. Hence, a policy-based framework for end-to-end QoS management in converged networks, CNQF (Converged Networks QoS Management Framework) has been proposed within our project. In this paper, the CNQF architecture, a Java implementation of its prototype and experimental validation of key elements are discussed. We then present a fuzzy-based CNQF resource management approach and study the performance of our implementation with real traffic flows on an experimental testbed. The results demonstrate the efficacy of our resource-adaptive approach for practical PBNM systems.\n    ",
        "submission_date": "2013-11-06T00:00:00",
        "last_modified_date": "2013-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.1704",
        "title": "Scalable Recommendation with Poisson Factorization",
        "authors": [
            "Prem Gopalan",
            "Jake M. Hofman",
            "David M. Blei"
        ],
        "abstract": "We develop a Bayesian Poisson matrix factorization model for forming recommendations from sparse user behavior data. These data are large user/item matrices where each user has provided feedback on only a small subset of items, either explicitly (e.g., through star ratings) or implicitly (e.g., through views or purchases). In contrast to traditional matrix factorization approaches, Poisson factorization implicitly models each user's limited attention to consume items. Moreover, because of the mathematical form of the Poisson likelihood, the model needs only to explicitly consider the observed entries in the matrix, leading to both scalable computation and good predictive performance. We develop a variational inference algorithm for approximate posterior inference that scales up to massive data sets. This is an efficient algorithm that iterates over the observed entries and adjusts an approximate posterior over the user/item representations. We apply our method to large real-world user data containing users rating movies, users listening to songs, and users reading scientific papers. In all these settings, Bayesian Poisson factorization outperforms state-of-the-art matrix factorization methods.\n    ",
        "submission_date": "2013-11-07T00:00:00",
        "last_modified_date": "2014-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.1761",
        "title": "Exploring Deep and Recurrent Architectures for Optimal Control",
        "authors": [
            "Sergey Levine"
        ],
        "abstract": "Sophisticated multilayer neural networks have achieved state of the art results on multiple supervised tasks. However, successful applications of such multilayer networks to control have so far been limited largely to the perception portion of the control pipeline. In this paper, we explore the application of deep and recurrent neural networks to a continuous, high-dimensional locomotion task, where the network is used to represent a control policy that maps the state of the system (represented by joint angles) directly to the torques at each joint. By using a recent reinforcement learning algorithm called guided policy search, we can successfully train neural network controllers with thousands of parameters, allowing us to compare a variety of architectures. We discuss the differences between the locomotion control task and previous supervised perception tasks, present experimental results comparing various architectures, and discuss future directions in the application of techniques from deep learning to the problem of optimal control.\n    ",
        "submission_date": "2013-11-07T00:00:00",
        "last_modified_date": "2013-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.1764",
        "title": "Automatic ontology generation for data mining using fca and clustering",
        "authors": [
            "Amel Grissa Touzi",
            "Hela Ben Massoud",
            "Alaya Ayadi"
        ],
        "abstract": "Motivated by the increased need for formalized representations of the domain of Data Mining, the success of using Formal Concept Analysis (FCA) and Ontology in several Computer Science fields, we present in this paper a new approach for automatic generation of Fuzzy Ontology of Data Mining (FODM), through the fusion of conceptual clustering, fuzzy logic, and FCA. In our approach, we propose to generate ontology taking in consideration another degree of granularity into the process of generation. Indeed, we suggest to define an ontology between classes resulting from a preliminary classification on the data. We prove that this approach optimize the definition of the ontology, offered a better interpretation of the data and optimized both the space memory and the execution time for exploiting this data.\n    ",
        "submission_date": "2013-11-07T00:00:00",
        "last_modified_date": "2013-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.2106",
        "title": "Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints",
        "authors": [
            "Rishabh Iyer",
            "Jeff Bilmes"
        ],
        "abstract": "We investigate two new optimization problems -- minimizing a submodular function subject to a submodular lower bound constraint (submodular cover) and maximizing a submodular function subject to a submodular upper bound constraint (submodular knapsack). We are motivated by a number of real-world applications in machine learning including sensor placement and data subset selection, which require maximizing a certain submodular function (like coverage or diversity) while simultaneously minimizing another (like cooperative cost). These problems are often posed as minimizing the difference between submodular functions [14, 35] which is in the worst case inapproximable. We show, however, that by phrasing these problems as constrained optimization, which is more natural for many applications, we achieve a number of bounded approximation guarantees. We also show that both these problems are closely related and an approximation algorithm solving one can be used to obtain an approximation guarantee for the other. We provide hardness results for both problems thus showing that our approximation factors are tight up to log-factors. Finally, we empirically demonstrate the performance and good scalability properties of our algorithms.\n    ",
        "submission_date": "2013-11-08T00:00:00",
        "last_modified_date": "2013-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.2702",
        "title": "Verifiable Source Code Documentation in Controlled Natural Language",
        "authors": [
            "Tobias Kuhn",
            "Alexandre Bergel"
        ],
        "abstract": "Writing documentation about software internals is rarely considered a rewarding activity. It is highly time-consuming and the resulting documentation is fragile when the software is continuously evolving in a multi-developer setting. Unfortunately, traditional programming environments poorly support the writing and maintenance of documentation. Consequences are severe as the lack of documentation on software structure negatively impacts the overall quality of the software product. We show that using a controlled natural language with a reasoner and a query engine is a viable technique for verifying the consistency and accuracy of documentation and source code. Using ACE, a state-of-the-art controlled natural language, we present positive results on the comprehensibility and the general feasibility of creating and verifying documentation. As a case study, we used automatic documentation verification to identify and fix severe flaws in the architecture of a non-trivial piece of software. Moreover, a user experiment shows that our language is faster and easier to learn and understand than other formal languages for software documentation.\n    ",
        "submission_date": "2013-11-12T00:00:00",
        "last_modified_date": "2013-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.3368",
        "title": "Anytime Belief Propagation Using Sparse Domains",
        "authors": [
            "Sameer Singh",
            "Sebastian Riedel",
            "Andrew McCallum"
        ],
        "abstract": "Belief Propagation has been widely used for marginal inference, however it is slow on problems with large-domain variables and high-order factors. Previous work provides useful approximations to facilitate inference on such models, but lacks important anytime properties such as: 1) providing accurate and consistent marginals when stopped early, 2) improving the approximation when run longer, and 3) converging to the fixed point of BP. To this end, we propose a message passing algorithm that works on sparse (partially instantiated) domains, and converges to consistent marginals using dynamic message scheduling. The algorithm grows the sparse domains incrementally, selecting the next value to add using prioritization schemes based on the gradients of the marginal inference objective. Our experiments demonstrate local anytime consistency and fast convergence, providing significant speedups over BP to obtain low-error marginals: up to 25 times on grid models, and up to 6 times on a real-world natural language processing task.\n    ",
        "submission_date": "2013-11-14T00:00:00",
        "last_modified_date": "2013-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.3735",
        "title": "Ensemble Relational Learning based on Selective Propositionalization",
        "authors": [
            "Nicola Di Mauro",
            "Floriana Esposito"
        ],
        "abstract": "Dealing with structured data needs the use of expressive representation formalisms that, however, puts the problem to deal with the computational complexity of the machine learning process. Furthermore, real world domains require tools able to manage their typical uncertainty. Many statistical relational learning approaches try to deal with these problems by combining the construction of relevant relational features with a probabilistic tool. When the combination is static (static propositionalization), the constructed features are considered as boolean features and used offline as input to a statistical learner; while, when the combination is dynamic (dynamic propositionalization), the feature construction and probabilistic tool are combined into a single process. In this paper we propose a selective propositionalization method that search the optimal set of relational features to be used by a probabilistic learner in order to minimize a loss function. The new propositionalization approach has been combined with the random subspace ensemble method. Experiments on real-world datasets shows the validity of the proposed method.\n    ",
        "submission_date": "2013-11-15T00:00:00",
        "last_modified_date": "2013-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.4180",
        "title": "Towards a New Science of a Clinical Data Intelligence",
        "authors": [
            "Volker Tresp",
            "Sonja Zillner",
            "Maria J. Costa",
            "Yi Huang",
            "Alexander Cavallaro",
            "Peter A. Fasching",
            "Andre Reis",
            "Martin Sedlmayr",
            "Thomas Ganslandt",
            "Klemens Budde",
            "Carl Hinrichs",
            "Danilo Schmidt",
            "Philipp Daumke",
            "Daniel Sonntag",
            "Thomas Wittenberg",
            "Patricia G. Oppelt",
            "Denis Krompass"
        ],
        "abstract": "In this paper we define Clinical Data Intelligence as the analysis of data generated in the clinical routine with the goal of improving patient care. We define a science of a Clinical Data Intelligence as a data analysis that permits the derivation of scientific, i.e., generalizable and reliable results. We argue that a science of a Clinical Data Intelligence is sensible in the context of a Big Data analysis, i.e., with data from many patients and with complete patient information. We discuss that Clinical Data Intelligence requires the joint efforts of knowledge engineering, information extraction (from textual and other unstructured data), and statistics and statistical machine learning. We describe some of our main results as conjectures and relate them to a recently funded research project involving two major German university hospitals.\n    ",
        "submission_date": "2013-11-17T00:00:00",
        "last_modified_date": "2013-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.6079",
        "title": "Local Similarities, Global Coding: An Algorithm for Feature Coding and its Applications",
        "authors": [
            "Amirreza Shaban",
            "Hamid R. Rabiee",
            "Mahyar Najibi"
        ],
        "abstract": "Data coding as a building block of several image processing algorithms has been received great attention recently. Indeed, the importance of the locality assumption in coding approaches is studied in numerous works and several methods are proposed based on this concept. We probe this assumption and claim that taking the similarity between a data point and a more global set of anchor points does not necessarily weaken the coding method as long as the underlying structure of the anchor points are taken into account. Based on this fact, we propose to capture this underlying structure by assuming a random walker over the anchor points. We show that our method is a fast approximate learning algorithm based on the diffusion map kernel. The experiments on various datasets show that making different state-of-the-art coding algorithms aware of this structure boosts them in different learning tasks.\n    ",
        "submission_date": "2013-11-24T00:00:00",
        "last_modified_date": "2014-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.6876",
        "title": "Want a Good Answer? Ask a Good Question First!",
        "authors": [
            "Yuan Yao",
            "Hanghang Tong",
            "Tao Xie",
            "Leman Akoglu",
            "Feng Xu",
            "Jian Lu"
        ],
        "abstract": "Community Question Answering (CQA) websites have become valuable repositories which host a massive volume of human knowledge. To maximize the utility of such knowledge, it is essential to evaluate the quality of an existing question or answer, especially soon after it is posted on the CQA website.\n",
        "submission_date": "2013-11-27T00:00:00",
        "last_modified_date": "2013-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.0049",
        "title": "One-Class Classification: Taxonomy of Study and Review of Techniques",
        "authors": [
            "Shehroz S.Khan",
            "Michael G.Madden"
        ],
        "abstract": "One-class classification (OCC) algorithms aim to build classification models when the negative class is either absent, poorly sampled or not well defined. This unique situation constrains the learning of efficient classifiers by defining class boundary just with the knowledge of positive class. The OCC problem has been considered and applied under many research themes, such as outlier/novelty detection and concept learning. In this paper we present a unified view of the general problem of OCC by presenting a taxonomy of study for OCC problems, which is based on the availability of training data, algorithms used and the application domains applied. We further delve into each of the categories of the proposed taxonomy and present a comprehensive literature review of the OCC algorithms, techniques and methodologies with a focus on their significance, limitations and applications. We conclude our paper by discussing some open research problems in the field of OCC and present our vision for future research.\n    ",
        "submission_date": "2013-11-30T00:00:00",
        "last_modified_date": "2013-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.0200",
        "title": "A Combined Approach for Constraints over Finite Domains and Arrays",
        "authors": [
            "S\u00e9bastien Bardin",
            "Arnaud Gotlieb"
        ],
        "abstract": "Arrays are ubiquitous in the context of software verification. However, effective reasoning over arrays is still rare in CP, as local reasoning is dramatically ill-conditioned for constraints over arrays. In this paper, we propose an approach combining both global symbolic reasoning and local consistency filtering in order to solve constraint systems involving arrays (with accesses, updates and size constraints) and finite-domain constraints over their elements and indexes. Our approach, named FDCC, is based on a combination of a congruence closure algorithm for the standard theory of arrays and a CP solver over finite domains. The tricky part of the work lies in the bi-directional communication mechanism between both solvers. We identify the significant information to share, and design ways to master the communication overhead. Experiments on random instances show that FDCC solves more formulas than any portfolio combination of the two solvers taken in isolation, while overhead is kept reasonable.\n    ",
        "submission_date": "2013-12-01T00:00:00",
        "last_modified_date": "2013-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.1423",
        "title": "ABC-SG: A New Artificial Bee Colony Algorithm-Based Distance of Sequential Data Using Sigma Grams",
        "authors": [
            "Muhammad Marwan Muhammad Fuad"
        ],
        "abstract": "The problem of similarity search is one of the main problems in computer science. This problem has many applications in text-retrieval, web search, computational biology, bioinformatics and others. Similarity between two data objects can be depicted using a similarity measure or a distance metric. There are numerous distance metrics in the literature, some are used for a particular data type, and others are more general. In this paper we present a new distance metric for sequential data which is based on the sum of n-grams. The novelty of our distance is that these n-grams are weighted using artificial bee colony; a recent optimization algorithm based on the collective intelligence of a swarm of bees on their search for nectar. This algorithm has been used in optimizing a large number of numerical problems. We validate the new distance experimentally.\n    ",
        "submission_date": "2013-12-05T00:00:00",
        "last_modified_date": "2013-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.1752",
        "title": "Particle Swarm Optimization of Information-Content Weighting of Symbolic Aggregate Approximation",
        "authors": [
            "Muhammad Marwan Muhammad Fuad"
        ],
        "abstract": "Bio-inspired optimization algorithms have been gaining more popularity recently. One of the most important of these algorithms is particle swarm optimization (PSO). PSO is based on the collective intelligence of a swam of particles. Each particle explores a part of the search space looking for the optimal position and adjusts its position according to two factors; the first is its own experience and the second is the collective experience of the whole swarm. PSO has been successfully used to solve many optimization problems. In this work we use PSO to improve the performance of a well-known representation method of time series data which is the symbolic aggregate approximation (SAX). As with other time series representation methods, SAX results in loss of information when applied to represent time series. In this paper we use PSO to propose a new minimum distance WMD for SAX to remedy this problem. Unlike the original minimum distance, the new distance sets different weights to different segments of the time series according to their information content. This weighted minimum distance enhances the performance of SAX as we show through experiments using different time series datasets.\n    ",
        "submission_date": "2013-12-06T00:00:00",
        "last_modified_date": "2013-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.1760",
        "title": "Towards Normalizing the Edit Distance Using a Genetic Algorithms Based Scheme",
        "authors": [
            "Muhammad Marwan Muhammad Fuad"
        ],
        "abstract": "The normalized edit distance is one of the distances derived from the edit distance. It is useful in some applications because it takes into account the lengths of the two strings compared. The normalized edit distance is not defined in terms of edit operations but rather in terms of the edit path. In this paper we propose a new derivative of the edit distance that also takes into consideration the lengths of the two strings, but the new distance is related directly to the edit distance. The particularity of the new distance is that it uses the genetic algorithms to set the values of the parameters it uses. We conduct experiments to test the new distance and we obtain promising results.\n    ",
        "submission_date": "2013-12-06T00:00:00",
        "last_modified_date": "2013-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.2062",
        "title": "A Novel Hierarchical Ant based QoS aware Intelligent Routing Scheme for MANETS",
        "authors": [
            "Debajit Sensarma",
            "Koushik Majumder"
        ],
        "abstract": "MANET is a collection of mobile devices with no centralized control and no pre-existing infrastructures. Due to the nodal mobility, supporting QoS during routing in this type of networks is a very challenging task. To tackle this type of overhead many routing algorithms with clustering approach have been proposed. Clustering is an effective method for resource management regarding network performance, routing protocol design, QoS etc. Most of the flat network architecture contains homogeneous capacity of nodes but in real time nodes are with heterogeneous capacity and transmission power. Hierarchical routing provides routing through this kind of heterogeneous nodes. Here, routes can be recorded hierarchically, across clusters to increase routing flexibility. Besides this, it increases scalability and robustness of routes. In this paper, a novel ant based QoS aware routing is proposed on a three level hierarchical cluster based topology in MANET which will be more scalable and efficient compared to flat architecture and will give better throughput.\n    ",
        "submission_date": "2013-12-07T00:00:00",
        "last_modified_date": "2013-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.2710",
        "title": "Improving circuit miniaturization and its efficiency using Rough Set Theory",
        "authors": [
            "Sarvesh SS Rawat",
            "Dheeraj Dilip Mor",
            "Anugrah Kumar",
            "Sanjiban Shekar Roy",
            "Rohit kumar"
        ],
        "abstract": "High-speed, accuracy, meticulousness and quick response are notion of the vital necessities for modern digital world. An efficient electronic circuit unswervingly affects the maneuver of the whole system. Different tools are required to unravel different types of engineering tribulations. Improving the efficiency, accuracy and low power consumption in an electronic circuit is always been a bottle neck problem. So the need of circuit miniaturization is always there. It saves a lot of time and power that is wasted in switching of gates, the wiring-crises is reduced, cross-sectional area of chip is reduced, the number of transistors that can implemented in chip is multiplied many folds. Therefore to trounce with this problem we have proposed an Artificial intelligence (AI) based approach that make use of Rough Set Theory for its implementation. Theory of rough set has been proposed by Z Pawlak in the year 1982. Rough set theory is a new mathematical tool which deals with uncertainty and vagueness. Decisions can be generated using rough set theory by reducing the unwanted and superfluous data. We have condensed the number of gates without upsetting the productivity of the given circuit. This paper proposes an approach with the help of rough set theory which basically lessens the number of gates in the circuit, based on decision rules.\n    ",
        "submission_date": "2013-12-10T00:00:00",
        "last_modified_date": "2013-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.3020",
        "title": "Sparse Allreduce: Efficient Scalable Communication for Power-Law Data",
        "authors": [
            "Huasha Zhao",
            "John Canny"
        ],
        "abstract": "Many large datasets exhibit power-law statistics: The web graph, social networks, text data, click through data etc. Their adjacency graphs are termed natural graphs, and are known to be difficult to partition. As a consequence most distributed algorithms on these graphs are communication intensive. Many algorithms on natural graphs involve an Allreduce: a sum or average of partitioned data which is then shared back to the cluster nodes. Examples include PageRank, spectral partitioning, and many machine learning algorithms including regression, factor (topic) models, and clustering. In this paper we describe an efficient and scalable Allreduce primitive for power-law data. We point out scaling problems with existing butterfly and round-robin networks for Sparse Allreduce, and show that a hybrid approach improves on both. Furthermore, we show that Sparse Allreduce stages should be nested instead of cascaded (as in the dense case). And that the optimum throughput Allreduce network should be a butterfly of heterogeneous degree where degree decreases with depth into the network. Finally, a simple replication scheme is introduced to deal with node failures. We present experiments showing significant improvements over existing systems such as PowerGraph and Hadoop.\n    ",
        "submission_date": "2013-12-11T00:00:00",
        "last_modified_date": "2013-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.3613",
        "title": "Augur: a Modeling Language for Data-Parallel Probabilistic Inference",
        "authors": [
            "Jean-Baptiste Tristan",
            "Daniel Huang",
            "Joseph Tassarotti",
            "Adam Pocock",
            "Stephen J. Green",
            "Guy L. Steele Jr"
        ],
        "abstract": "It is time-consuming and error-prone to implement inference procedures for each new probabilistic model. Probabilistic programming addresses this problem by allowing a user to specify the model and having a compiler automatically generate an inference procedure for it. For this approach to be practical, it is important to generate inference code that has reasonable performance. In this paper, we present a probabilistic programming language and compiler for Bayesian networks designed to make effective use of data-parallel architectures such as GPUs. Our language is fully integrated within the Scala programming language and benefits from tools such as IDE support, type-checking, and code completion. We show that the compiler can generate data-parallel inference code scalable to thousands of GPU cores by making use of the conditional independence relationships in the Bayesian network.\n    ",
        "submission_date": "2013-12-12T00:00:00",
        "last_modified_date": "2014-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.4287",
        "title": "Strategic Argumentation is NP-Complete",
        "authors": [
            "Guido Governatori",
            "Francesco Olivieri",
            "Simone Scannapieco",
            "Antonino Rotolo",
            "Matteo Cristani"
        ],
        "abstract": "In this paper we study the complexity of strategic argumentation for dialogue games. A dialogue game is a 2-player game where the parties play arguments. We show how to model dialogue games in a skeptical, non-monotonic formalism, and we show that the problem of deciding what move (set of rules) to play at each turn is an NP-complete problem.\n    ",
        "submission_date": "2013-12-16T00:00:00",
        "last_modified_date": "2013-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.4637",
        "title": "Constraint Reduction using Marginal Polytope Diagrams for MAP LP Relaxations",
        "authors": [
            "Zhen Zhang",
            "Qinfeng Shi",
            "Yanning Zhang",
            "Chunhua Shen",
            "Anton van den Hengel"
        ],
        "abstract": "LP relaxation-based message passing algorithms provide an effective tool for MAP inference over Probabilistic Graphical Models. However, different LP relaxations often have different objective functions and variables of differing dimensions, which presents a barrier to effective comparison and analysis. In addition, the computational complexity of LP relaxation-based methods grows quickly with the number of constraints. Reducing the number of constraints without sacrificing the quality of the solutions is thus desirable.\n",
        "submission_date": "2013-12-17T00:00:00",
        "last_modified_date": "2014-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.4640",
        "title": "A Review of Temporal Aspects of Hand Gesture Analysis Applied to Discourse Analysis and Natural Conversation",
        "authors": [
            "Renata Cristina Barros Madeo",
            "Priscilla Koch Wagner",
            "Sarajane Marques Peres"
        ],
        "abstract": "Lately, there has been an increasing interest in hand gesture analysis systems. Recent works have employed pattern recognition techniques and have focused on the development of systems with more natural user interfaces. These systems may use gestures to control interfaces or recognize sign language gestures, which can provide systems with multimodal interaction; or consist in multimodal tools to help psycholinguists to understand new aspects of discourse analysis and to automate laborious tasks. Gestures are characterized by several aspects, mainly by movements and sequence of postures. Since data referring to movements or sequences carry temporal information, this paper presents a literature review about temporal aspects of hand gesture analysis, focusing on applications related to natural conversation and psycholinguistic analysis, using Systematic Literature Review methodology. In our results, we organized works according to type of analysis, methods, highlighting the use of Machine Learning techniques, and applications.\n    ",
        "submission_date": "2013-12-17T00:00:00",
        "last_modified_date": "2013-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.4704",
        "title": "RDF Translator: A RESTful Multi-Format Data Converter for the Semantic Web",
        "authors": [
            "Alex Stolz",
            "Bene Rodriguez-Castro",
            "Martin Hepp"
        ],
        "abstract": "The interdisciplinary nature of the Semantic Web and the many projects put forward by the community led to a large number of widely accepted serialization formats for RDF. Most of these RDF syntaxes have been developed out of a necessity to serve specific purposes better than existing ones, e.g. RDFa was proposed as an extension to HTML for embedding non-intrusive RDF statements in human-readable documents. Nonetheless, the RDF serialization formats are generally transducible among themselves given that they are commonly based on the RDF model. In this paper, we present (1) a RESTful Web service based on the HTTP protocol that translates between different serializations. In addition to its core functionality, our proposed solution provides (2) features to accommodate frequent needs of Semantic Web developers, namely a straightforward user interface with copy-to-clipboard functionality, syntax highlighting, persistent URI links for easy sharing, cool URI patterns, and content negotiation using respective HTTP headers. We demonstrate the benefit of our converter by presenting two use cases.\n    ",
        "submission_date": "2013-12-17T00:00:00",
        "last_modified_date": "2013-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.4794",
        "title": "Semantic Annotation: The Mainstay of Semantic Web",
        "authors": [
            "Thabet Slimani"
        ],
        "abstract": "Given that semantic Web realization is based on the critical mass of metadata accessibility and the representation of data with formal knowledge, it needs to generate metadata that is specific, easy to understand and well-defined. However, semantic annotation of the web documents is the successful way to make the Semantic Web vision a reality. This paper introduces the Semantic Web and its vision (stack layers) with regard to some concept definitions that helps the understanding of semantic annotation. Additionally, this paper introduces the semantic annotation categories, tools, domains and models.\n    ",
        "submission_date": "2013-12-17T00:00:00",
        "last_modified_date": "2013-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.4814",
        "title": "Mining Malware Specifications through Static Reachability Analysis",
        "authors": [
            "Hugo Daniel Macedo",
            "Tayssir Touili"
        ],
        "abstract": "The number of malicious software (malware) is growing out of control. Syntactic signature based detection cannot cope with such growth and manual construction of malware signature databases needs to be replaced by computer learning based approaches. Currently, a single modern signature capturing the semantics of a malicious behavior can be used to replace an arbitrarily large number of old-fashioned syntactical signatures. However teaching computers to learn such behaviors is a challenge. Existing work relies on dynamic analysis to extract malicious behaviors, but such technique does not guarantee the coverage of all behaviors. To sidestep this limitation we show how to learn malware signatures using static reachability analysis. The idea is to model binary programs using pushdown systems (that can be used to model the stack operations occurring during the binary code execution), use reachability analysis to extract behaviors in the form of trees, and use subtrees that are common among the trees extracted from a training set of malware files as signatures. To detect malware we propose to use a tree automaton to compactly store malicious behavior trees and check if any of the subtrees extracted from the file under analysis is malicious. Experimental data shows that our approach can be used to learn signatures from a training set of malware files and use them to detect a test set of malware that is 5 times the size of the training set.\n    ",
        "submission_date": "2013-12-17T00:00:00",
        "last_modified_date": "2013-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.4828",
        "title": "Subjective Logic Operators in Trust Assessment: an Empirical Study",
        "authors": [
            "Federico Cerutti",
            "Alice Toniolo",
            "Nir Oren",
            "Timothy J. Norman"
        ],
        "abstract": "Computational trust mechanisms aim to produce trust ratings from both direct and indirect information about agents' behaviour. Subjective Logic (SL) has been widely adopted as the core of such systems via its fusion and discount operators. In recent research we revisited the semantics of these operators to explore an alternative, geometric interpretation. In this paper we present a principled desiderata for discounting and fusion operators in SL. Building upon this we present operators that satisfy these desirable properties, including a family of discount operators. We then show, through a rigorous empirical study, that specific, geometrically interpreted operators significantly outperform standard SL operators in estimating ground truth. These novel operators offer real advantages for computational models of trust and reputation, in which they may be employed without modifying other aspects of an existing system.\n    ",
        "submission_date": "2013-11-19T00:00:00",
        "last_modified_date": "2013-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.4851",
        "title": "Representing, Simulating and Analysing Ho Chi Minh City Tsunami Plan by Means of Process Models",
        "authors": [
            "Le Nguyen Tuan Thanh",
            "Chihab Hanachi",
            "Serge Stinckwich",
            "Ho Tuong Vinh"
        ],
        "abstract": "This paper considers the textual plan (guidelines) proposed by People's Committee of Ho Chi Minh City (Vietnam) to manage earthquake and tsunami, and try to represent it in a more formal way, in order to provide means to simulate, analyse and adapt it. We first present a state of the art about coordination models for disaster management with a focus on process oriented approaches. We give an overview of the different dimensions of the textual tsunami plan of Ho Chi Minh City and then the graphical representation of its process with BPMN (Business Process Model and Notation). We finally show how to exploit this process with workflow tools to simulate (YAWL tool) and analyse it (ProM tool).\n    ",
        "submission_date": "2013-12-17T00:00:00",
        "last_modified_date": "2013-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.5198",
        "title": "Learning Semantic Script Knowledge with Event Embeddings",
        "authors": [
            "Ashutosh Modi",
            "Ivan Titov"
        ],
        "abstract": "Induction of common sense knowledge about prototypical sequences of events has recently received much attention. Instead of inducing this knowledge in the form of graphs, as in much of the previous work, in our method, distributed representations of event realizations are computed based on distributed representations of predicates and their arguments, and then these representations are used to predict prototypical event orderings. The parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated from texts. We show that this approach results in a substantial boost in ordering performance with respect to previous methods.\n    ",
        "submission_date": "2013-12-18T00:00:00",
        "last_modified_date": "2014-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6214",
        "title": "Volumetric Spanners: an Efficient Exploration Basis for Learning",
        "authors": [
            "Elad Hazan",
            "Zohar Karnin",
            "Raghu Mehka"
        ],
        "abstract": "Numerous machine learning problems require an exploration basis - a mechanism to explore the action space. We define a novel geometric notion of exploration basis with low variance, called volumetric spanners, and give efficient algorithms to construct such a basis.\n",
        "submission_date": "2013-12-21T00:00:00",
        "last_modified_date": "2014-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6546",
        "title": "Fair assignment of indivisible objects under ordinal preferences",
        "authors": [
            "Haris Aziz",
            "Serge Gaspers",
            "Simon Mackenzie",
            "Toby Walsh"
        ],
        "abstract": "We consider the discrete assignment problem in which agents express ordinal preferences over objects and these objects are allocated to the agents in a fair manner. We use the stochastic dominance relation between fractional or randomized allocations to systematically define varying notions of proportionality and envy-freeness for discrete assignments. The computational complexity of checking whether a fair assignment exists is studied for these fairness notions. We also characterize the conditions under which a fair assignment is guaranteed to exist. For a number of fairness concepts, polynomial-time algorithms are presented to check whether a fair assignment exists. Our algorithmic results also extend to the case of unequal entitlements of agents. Our NP-hardness result, which holds for several variants of envy-freeness, answers an open question posed by Bouveret, Endriss, and Lang (ECAI 2010). We also propose fairness concepts that always suggest a non-empty set of assignments with meaningful fairness properties. Among these concepts, optimal proportionality and optimal weak proportionality appear to be desirable fairness concepts.\n    ",
        "submission_date": "2013-12-23T00:00:00",
        "last_modified_date": "2015-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6599",
        "title": "Image Processing based Systems and Techniques for the Recognition of Ancient and Modern Coins",
        "authors": [
            "Shatrughan Modi",
            "Dr. Seema Bawa"
        ],
        "abstract": "Coins are frequently used in everyday life at various places like in banks, grocery stores, supermarkets, automated weighing machines, vending machines etc. So, there is a basic need to automate the counting and sorting of coins. For this machines need to recognize the coins very fast and accurately, as further transaction processing depends on this recognition. Three types of systems are available in the market: Mechanical method based systems, Electromagnetic method based systems and Image processing based systems. This paper presents an overview of available systems and techniques based on image processing to recognize ancient and modern coins.\n    ",
        "submission_date": "2013-12-23T00:00:00",
        "last_modified_date": "2013-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6615",
        "title": "Automated Coin Recognition System using ANN",
        "authors": [
            "Shatrughan Modi",
            "Dr. Seema Bawa"
        ],
        "abstract": "Coins are integral part of our day to day life. We use coins everywhere like grocery store, banks, buses, trains etc. So it becomes a basic need that coins can be sorted and counted automatically. For this it is necessary that coins can be recognized automatically. In this paper we have developed an ANN (Artificial Neural Network) based Automated Coin Recognition System for the recognition of Indian Coins of denomination Rs. 1, 2, 5 and 10 with rotation invariance. We have taken images from both sides of coin. So this system is capable of recognizing coins from both sides. Features are extracted from images using techniques of Hough Transformation, Pattern Averaging etc. Then, the extracted features are passed as input to a trained Neural Network. 97.74% recognition rate has been achieved during the experiments i.e. only 2.26% miss recognition, which is quite encouraging.\n    ",
        "submission_date": "2013-12-23T00:00:00",
        "last_modified_date": "2013-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6947",
        "title": "Formal Ontology Learning on Factual IS-A Corpus in English using Description Logics",
        "authors": [
            "Sourish Dasgupta",
            "Ankur Padia",
            "Kushal Shah",
            "Prasenjit Majumder"
        ],
        "abstract": "Ontology Learning (OL) is the computational task of generating a knowledge base in the form of an ontology given an unstructured corpus whose content is in natural language (NL). Several works can be found in this area most of which are limited to statistical and lexico-syntactic pattern matching based techniques Light-Weight OL. These techniques do not lead to very accurate learning mostly because of several linguistic nuances in NL. Formal OL is an alternative (less explored) methodology were deep linguistics analysis is made using theory and tools found in computational linguistics to generate formal axioms and definitions instead simply inducing a taxonomy. In this paper we propose \"Description Logic (DL)\" based formal OL framework for learning factual IS-A type sentences in English. We claim that semantic construction of IS-A sentences is non trivial. Hence, we also claim that such sentences requires special studies in the context of OL before any truly formal OL can be proposed. We introduce a learner tool, called DLOL_IS-A, that generated such ontologies in the owl format. We have adopted \"Gold Standard\" based OL evaluation on IS-A rich WCL v.1.1 dataset and our own Community representative IS-A dataset. We observed significant improvement of DLOL_IS-A when compared to the light-weight OL tool Text2Onto and formal OL tool FRED.\n    ",
        "submission_date": "2013-12-25T00:00:00",
        "last_modified_date": "2016-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6948",
        "title": "Description Logics based Formalization of Wh-Queries",
        "authors": [
            "Sourish Dasgupta",
            "Rupali KaPatel",
            "Ankur Padia",
            "Kushal Shah"
        ],
        "abstract": "The problem of Natural Language Query Formalization (NLQF) is to translate a given user query in natural language (NL) into a formal language so that the semantic interpretation has equivalence with the NL interpretation. Formalization of NL queries enables logic based reasoning during information retrieval, database query, question-answering, etc. Formalization also helps in Web query normalization and indexing, query intent analysis, etc. In this paper we are proposing a Description Logics based formal methodology for wh-query intent (also called desire) identification and corresponding formal translation. We evaluated the scalability of our proposed formalism using Microsoft Encarta 98 query dataset and OWL-S TC v.4.0 dataset.\n    ",
        "submission_date": "2013-12-25T00:00:00",
        "last_modified_date": "2013-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6995",
        "title": "Towards Using Unlabeled Data in a Sparse-coding Framework for Human Activity Recognition",
        "authors": [
            "Sourav Bhattacharya",
            "Petteri Nurmi",
            "Nils Hammerla",
            "Thomas Pl\u00f6tz"
        ],
        "abstract": "We propose a sparse-coding framework for activity recognition in ubiquitous and mobile computing that alleviates two fundamental problems of current supervised learning approaches. (i) It automatically derives a compact, sparse and meaningful feature representation of sensor data that does not rely on prior expert knowledge and generalizes extremely well across domain boundaries. (ii) It exploits unlabeled sample data for bootstrapping effective activity recognizers, i.e., substantially reduces the amount of ground truth annotation required for model estimation. Such unlabeled data is trivial to obtain, e.g., through contemporary smartphones carried by users as they go about their everyday activities.\n",
        "submission_date": "2013-12-25T00:00:00",
        "last_modified_date": "2014-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.7606",
        "title": "Distributed Policy Evaluation Under Multiple Behavior Strategies",
        "authors": [
            "Sergio Valcarcel Macua",
            "Jianshu Chen",
            "Santiago Zazo",
            "Ali H. Sayed"
        ],
        "abstract": "We apply diffusion strategies to develop a fully-distributed cooperative reinforcement learning algorithm in which agents in a network communicate only with their immediate neighbors to improve predictions about their environment. The algorithm can also be applied to off-policy learning, meaning that the agents can predict the response to a behavior different from the actual policies they are following. The proposed distributed strategy is efficient, with linear complexity in both computation time and memory footprint. We provide a mean-square-error performance analysis and establish convergence under constant step-size updates, which endow the network with continuous learning capabilities. The results show a clear gain from cooperation: when the individual agents can estimate the solution, cooperation increases stability and reduces bias and variance of the prediction error; but, more importantly, the network is able to approach the optimal solution even when none of the individual agents can (e.g., when the individual behavior policies restrict each agent to sample a small portion of the state space).\n    ",
        "submission_date": "2013-12-30T00:00:00",
        "last_modified_date": "2014-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.7832",
        "title": "Defining implication relation for classical logic",
        "authors": [
            "Li Fu"
        ],
        "abstract": "In classical logic, \"P implies Q\" is equivalent to \"not-P or Q\". It is well known that the equivalence is problematic. Actually, from \"P implies Q\", \"not-P or Q\" can be inferred (\"Implication-to-Disjunction\" is valid), whereas from \"not-P or Q\", \"P implies Q\" cannot be inferred in general (\"Disjunction-to-Implication\" is not generally valid), so the equivalence between them is invalid in general. This work aims to remove the incorrect Disjunction-to-Implication from classical logic (CL). The logical system (the logic IRL) this paper proposes has the expected properties: (a) CL is obtained by adding Disjunction-to-Implication to IRL, and (b) Disjunction-to-Implication is not derivable in IRL; while (c) fundamental laws in classical logic, including law of excluded middle (LEM) and principle of double negation, law of non-contradiction (LNC) and ex contradictione quodlibet (ECQ), conjunction elimination and disjunction introduction, and hypothetical syllogism and disjunctive syllogism, are all retained in IRL.\n    ",
        "submission_date": "2013-12-30T00:00:00",
        "last_modified_date": "2025-05-02T00:00:00"
    }
]