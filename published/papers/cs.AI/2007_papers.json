[
    {
        "url": "https://arxiv.org/abs/0704.1394",
        "title": "Calculating Valid Domains for BDD-Based Interactive Configuration",
        "authors": [
            "Tarik Hadzic",
            "Rune Moller Jensen",
            "Henrik Reif Andersen"
        ],
        "abstract": "  In these notes we formally describe the functionality of Calculating Valid Domains from the BDD representing the solution space of valid configurations. The formalization is largely based on the CLab configuration framework.\n    ",
        "submission_date": "2007-04-11T00:00:00",
        "last_modified_date": "2007-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0704.1675",
        "title": "Exploiting Social Annotation for Automatic Resource Discovery",
        "authors": [
            "Anon Plangprasopchok",
            "Kristina Lerman"
        ],
        "abstract": "  Information integration applications, such as mediators or mashups, that require access to information resources currently rely on users manually discovering and integrating them in the application. Manual resource discovery is a slow process, requiring the user to sift through results obtained via keyword-based search. Although search methods have advanced to include evidence from document contents, its metadata and the contents and link structure of the referring pages, they still do not adequately cover information sources -- often called ``the hidden Web''-- that dynamically generate documents in response to a query. The recently popular social bookmarking sites, which allow users to annotate and share metadata about various information sources, provide rich evidence for resource discovery. In this paper, we describe a probabilistic model of the user annotation process in a social bookmarking system ",
        "submission_date": "2007-04-12T00:00:00",
        "last_modified_date": "2007-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0704.2010",
        "title": "A study of structural properties on profiles HMMs",
        "authors": [
            "Juliana S Bernardes",
            "Alberto Davila",
            "Vitor Santos Costa",
            "Gerson Zaverucha"
        ],
        "abstract": "  Motivation: Profile hidden Markov Models (pHMMs) are a popular and very useful tool in the detection of the remote homologue protein families. Unfortunately, their performance is not always satisfactory when proteins are in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm and tool that tries to improve pHMM performance by using structural information while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs. Each pHMM is constructed by weighting each residue in an aligned protein according to a specific structural property of the residue. Properties used were primary, secondary and tertiary structures, accessibility and packing. HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP database to perform our experiments. Throughout, we apply leave-one-family-out cross-validation over protein superfamilies. First, we used the MAMMOTH-mult structural aligner to align the training set proteins. Then, we performed two sets of experiments. In a first experiment, we compared structure weighted models against standard pHMMs and against each other. In a second experiment, we compared the voting model against individual pHMMs. We compare method performance through ROC curves and through Precision/Recall curves, and assess significance through the paired two tailed t-test. Our results show significant performance improvements of all structurally weighted models over default HMMER, and a significant improvement in sensitivity of the combined models over both the original model and the structurally weighted models.\n    ",
        "submission_date": "2007-04-16T00:00:00",
        "last_modified_date": "2008-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0704.3157",
        "title": "Experimenting with recursive queries in database and logic programming systems",
        "authors": [
            "Giorgio Terracina",
            "Nicola Leone",
            "Vincenzino Lio",
            "Claudio Panetta"
        ],
        "abstract": "  This paper considers the problem of reasoning on massive amounts of (possibly distributed) data. Presently, existing proposals show some limitations: {\\em (i)} the quantity of data that can be handled contemporarily is limited, due to the fact that reasoning is generally carried out in main-memory; {\\em (ii)} the interaction with external (and independent) DBMSs is not trivial and, in several cases, not allowed at all; {\\em (iii)} the efficiency of present implementations is still not sufficient for their utilization in complex reasoning tasks involving massive amounts of data. This paper provides a contribution in this setting; it presents a new system, called DLV$^{DB}$, which aims to solve these problems. Moreover, the paper reports the results of a thorough experimental analysis we have carried out for comparing our system with several state-of-the-art systems (both logic and databases) on some classical deductive problems; the other tested systems are: LDL++, XSB, Smodels and three top-level commercial DBMSs. DLV$^{DB}$ significantly outperforms even the commercial Database Systems on recursive queries. To appear in Theory and Practice of Logic Programming (TPLP)\n    ",
        "submission_date": "2007-04-24T00:00:00",
        "last_modified_date": "2007-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0704.3395",
        "title": "General-Purpose Computing on a Semantic Network Substrate",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "abstract": "This article presents a model of general-purpose computing on a semantic network substrate. The concepts presented are applicable to any semantic network representation. However, due to the standards and technological infrastructure devoted to the Semantic Web effort, this article is presented from this point of view. In the proposed model of computing, the application programming interface, the run-time program, and the state of the computing virtual machine are all represented in the Resource Description Framework (RDF). The implementation of the concepts presented provides a practical computing paradigm that leverages the highly-distributed and standardized representational-layer of the Semantic Web.\n    ",
        "submission_date": "2007-04-25T00:00:00",
        "last_modified_date": "2010-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0704.3433",
        "title": "Bayesian approach to rough set",
        "authors": [
            "Tshilidzi Marwala",
            "Bodie Crossingham"
        ],
        "abstract": "  This paper proposes an approach to training rough set models using Bayesian framework trained using Markov Chain Monte Carlo (MCMC) method. The prior probabilities are constructed from the prior knowledge that good rough set models have fewer rules. Markov Chain Monte Carlo sampling is conducted through sampling in the rough set granule space and Metropolis algorithm is used as an acceptance criteria. The proposed method is tested to estimate the risk of HIV given demographic data. The results obtained shows that the proposed approach is able to achieve an average accuracy of 58% with the accuracy varying up to 66%. In addition the Bayesian rough set give the probabilities of the estimated HIV status as well as the linguistic rules describing how the demographic parameters drive the risk of HIV.\n    ",
        "submission_date": "2007-04-25T00:00:00",
        "last_modified_date": "2007-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0704.3453",
        "title": "An Adaptive Strategy for the Classification of G-Protein Coupled Receptors",
        "authors": [
            "S. Mohamed",
            "D. Rubin",
            "T. Marwala"
        ],
        "abstract": "  One of the major problems in computational biology is the inability of existing classification models to incorporate expanding and new domain knowledge. This problem of static classification models is addressed in this paper by the introduction of incremental learning for problems in bioinformatics. Many machine learning tools have been applied to this problem using static machine learning structures such as neural networks or support vector machines that are unable to accommodate new information into their existing models. We utilize the fuzzy ARTMAP as an alternate machine learning system that has the ability of incrementally learning new data as it becomes available. The fuzzy ARTMAP is found to be comparable to many of the widespread machine learning systems. The use of an evolutionary strategy in the selection and combination of individual classifiers into an ensemble system, coupled with the incremental learning ability of the fuzzy ARTMAP is proven to be suitable as a pattern classifier. The algorithm presented is tested using data from the G-Coupled Protein Receptors Database and shows good accuracy of 83%. The system presented is also generally applicable, and can be used in problems in genomics and proteomics.\n    ",
        "submission_date": "2007-04-25T00:00:00",
        "last_modified_date": "2007-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0704.3515",
        "title": "Comparing Robustness of Pairwise and Multiclass Neural-Network Systems for Face Recognition",
        "authors": [
            "J. Uglov",
            "V. Schetinin",
            "C. Maple"
        ],
        "abstract": "  Noise, corruptions and variations in face images can seriously hurt the performance of face recognition systems. To make such systems robust, multiclass neuralnetwork classifiers capable of learning from noisy data have been suggested. However on large face data sets such systems cannot provide the robustness at a high level. In this paper we explore a pairwise neural-network system as an alternative approach to improving the robustness of face recognition. In our experiments this approach is shown to outperform the multiclass neural-network system in terms of the predictive accuracy on the face images corrupted by noise.\n    ",
        "submission_date": "2007-04-26T00:00:00",
        "last_modified_date": "2007-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0704.3886",
        "title": "A Note on Ontology and Ordinary Language",
        "authors": [
            "Walid S. Saba"
        ],
        "abstract": "  We argue for a compositional semantics grounded in a strongly typed ontology that reflects our commonsense view of the world and the way we talk about it. Assuming such a structure we show that the semantics of various natural language phenomena may become nearly trivial.\n    ",
        "submission_date": "2007-04-30T00:00:00",
        "last_modified_date": "2007-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0704.3905",
        "title": "Ensemble Learning for Free with Evolutionary Algorithms ?",
        "authors": [
            "Christian Gagn\u00e9",
            "Mich\u00e8le Sebag",
            "Marc Schoenauer",
            "Marco Tomassini"
        ],
        "abstract": "  Evolutionary Learning proceeds by evolving a population of classifiers, from which it generally returns (with some notable exceptions) the single best-of-run classifier as final result. In the meanwhile, Ensemble Learning, one of the most efficient approaches in supervised Machine Learning for the last decade, proceeds by building a population of diverse classifiers. Ensemble Learning with Evolutionary Computation thus receives increasing attention. The Evolutionary Ensemble Learning (EEL) approach presented in this paper features two contributions. First, a new fitness function, inspired by co-evolution and enforcing the classifier diversity, is presented. Further, a new selection criterion based on the classification margin is proposed. This criterion is used to extract the classifier ensemble from the final population only (Off-line) or incrementally along evolution (On-line). Experiments on a set of benchmark problems show that Off-line outperforms single-hypothesis evolutionary learning and state-of-art Boosting and generates smaller classifier ensembles.\n    ",
        "submission_date": "2007-04-30T00:00:00",
        "last_modified_date": "2007-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.0197",
        "title": "Fault Classification in Cylinders Using Multilayer Perceptrons, Support Vector Machines and Guassian Mixture Models",
        "authors": [
            "Tshilidzi Marwala",
            "Unathi Mahola",
            "Snehashish Chakraverty"
        ],
        "abstract": "  Gaussian mixture models (GMM) and support vector machines (SVM) are introduced to classify faults in a population of cylindrical shells. The proposed procedures are tested on a population of 20 cylindrical shells and their performance is compared to the procedure, which uses multi-layer perceptrons (MLP). The modal properties extracted from vibration data are used to train the GMM, SVM and MLP. It is observed that the GMM produces 98%, SVM produces 94% classification accuracy while the MLP produces 88% classification rates.\n    ",
        "submission_date": "2007-05-02T00:00:00",
        "last_modified_date": "2007-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.0588",
        "title": "Clustering Co-occurrence of Maximal Frequent Patterns in Streams",
        "authors": [
            "Edgar H. de Graaf",
            "Joost N. Kok",
            "Walter A. Kosters"
        ],
        "abstract": "  One way of getting a better view of data is using frequent patterns. In this paper frequent patterns are subsets that occur a minimal number of times in a stream of itemsets. However, the discovery of frequent patterns in streams has always been problematic. Because streams are potentially endless it is in principle impossible to say if a pattern is often occurring or not. Furthermore the number of patterns can be huge and a good overview of the structure of the stream is lost quickly. The proposed approach will use clustering to facilitate the analysis of the structure of the stream.\n",
        "submission_date": "2007-05-04T00:00:00",
        "last_modified_date": "2007-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.0593",
        "title": "Clustering with Lattices in the Analysis of Graph Patterns",
        "authors": [
            "Edgar H. de Graaf",
            "Joost N. Kok",
            "Walter A. Kosters"
        ],
        "abstract": "  Mining frequent subgraphs is an area of research where we have a given set of graphs (each graph can be seen as a transaction), and we search for (connected) subgraphs contained in many of these graphs. In this work we will discuss techniques used in our framework Lattice2SAR for mining and analysing frequent subgraph data and their corresponding lattice information. Lattice information is provided by the graph mining algorithm gSpan; it contains all supergraph-subgraph relations of the frequent subgraph patterns -- and their supports.\n",
        "submission_date": "2007-05-04T00:00:00",
        "last_modified_date": "2007-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.0693",
        "title": "Learning to Bluff",
        "authors": [
            "Evan Hurwitz",
            "Tshilidzi Marwala"
        ],
        "abstract": "  The act of bluffing confounds game designers to this day. The very nature of bluffing is even open for debate, adding further complication to the process of creating intelligent virtual players that can bluff, and hence play, realistically. Through the use of intelligent, learning agents, and carefully designed agent outlooks, an agent can in fact learn to predict its opponents reactions based not only on its own cards, but on the actions of those around it. With this wider scope of understanding, an agent can in learn to bluff its opponents, with the action representing not an illogical action, as bluffing is often viewed, but rather as an act of maximising returns through an effective statistical optimisation. By using a tee dee lambda learning algorithm to continuously adapt neural network agent intelligence, agents have been shown to be able to learn to bluff without outside prompting, and even to learn to call each others bluffs in free, competitive play.\n    ",
        "submission_date": "2007-05-07T00:00:00",
        "last_modified_date": "2007-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.0734",
        "title": "Soft constraint abstraction based on semiring homomorphism",
        "authors": [
            "Sanjiang Li",
            "Mingsheng Ying"
        ],
        "abstract": "  The semiring-based constraint satisfaction problems (semiring CSPs), proposed by Bistarelli, Montanari and Rossi \\cite{BMR97}, is a very general framework of soft constraints. In this paper we propose an abstraction scheme for soft constraints that uses semiring homomorphism. To find optimal solutions of the concrete problem, the idea is, first working in the abstract problem and finding its optimal solutions, then using them to solve the concrete problem.\n",
        "submission_date": "2007-05-05T00:00:00",
        "last_modified_date": "2007-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.0761",
        "title": "Bayesian Approach to Neuro-Rough Models",
        "authors": [
            "Tshilidzi Marwala",
            "Bodie Crossingham"
        ],
        "abstract": "  This paper proposes a neuro-rough model based on multi-layered perceptron and rough set. The neuro-rough model is then tested on modelling the risk of HIV from demographic data. The model is formulated using Bayesian framework and trained using Monte Carlo method and Metropolis criterion. When the model was tested to estimate the risk of HIV infection given the demographic data it was found to give the accuracy of 62%. The proposed model is able to combine the accuracy of the Bayesian MLP model and the transparency of Bayesian rough set model.\n    ",
        "submission_date": "2007-05-06T00:00:00",
        "last_modified_date": "2007-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.0969",
        "title": "Artificial Neural Networks and Support Vector Machines for Water Demand Time Series Forecasting",
        "authors": [
            "Ishmael S. Msiza",
            "Fulufhelo V. Nelwamondo",
            "Tshilidzi Marwala"
        ],
        "abstract": "  Water plays a pivotal role in many physical processes, and most importantly in sustaining human life, animal life and plant life. Water supply entities therefore have the responsibility to supply clean and safe water at the rate required by the consumer. It is therefore necessary to implement mechanisms and systems that can be employed to predict both short-term and long-term water demands. The increasingly growing field of computational intelligence techniques has been proposed as an efficient tool in the modelling of dynamic phenomena. The primary objective of this paper is to compare the efficiency of two computational intelligence techniques in water demand forecasting. The techniques under comparison are the Artificial Neural Networks (ANNs) and the Support Vector Machines (SVMs). In this study it was observed that the ANNs perform better than the SVMs. This performance is measured against the generalisation ability of the two.\n    ",
        "submission_date": "2007-05-07T00:00:00",
        "last_modified_date": "2007-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.1031",
        "title": "Fuzzy Artmap and Neural Network Approach to Online Processing of Inputs with Missing Values",
        "authors": [
            "F.V. Nelwamondo",
            "T. Marwala"
        ],
        "abstract": "  An ensemble based approach for dealing with missing data, without predicting or imputing the missing values is proposed. This technique is suitable for online operations of neural networks and as a result, is used for online condition monitoring. The proposed technique is tested in both classification and regression problems. An ensemble of Fuzzy-ARTMAPs is used for classification whereas an ensemble of multi-layer perceptrons is used for the regression problem. Results obtained using this ensemble-based technique are compared to those obtained using a combination of auto-associative neural networks and genetic algorithms and findings show that this method can perform up to 9% better in regression problems. Another advantage of the proposed technique is that it eliminates the need for finding the best estimate of the data, and hence, saves time.\n    ",
        "submission_date": "2007-05-08T00:00:00",
        "last_modified_date": "2007-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.1110",
        "title": "Mining Patterns with a Balanced Interval",
        "authors": [
            "Edgar de Graaf Joost Kok Walter Kosters"
        ],
        "abstract": "  In many applications it will be useful to know those patterns that occur with a balanced interval, e.g., a certain combination of phone numbers are called almost every Friday or a group of products are sold a lot on Tuesday and Thursday.\n",
        "submission_date": "2007-05-08T00:00:00",
        "last_modified_date": "2007-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.1209",
        "title": "Artificial Intelligence for Conflict Management",
        "authors": [
            "E. Habtemariam",
            "T. Marwala",
            "M. Lagazio"
        ],
        "abstract": "  Militarised conflict is one of the risks that have a significant impact on society. Militarised Interstate Dispute (MID) is defined as an outcome of interstate interactions, which result on either peace or conflict. Effective prediction of the possibility of conflict between states is an important decision support tool for policy makers. In a previous research, neural networks (NNs) have been implemented to predict the MID. Support Vector Machines (SVMs) have proven to be very good prediction techniques and are introduced for the prediction of MIDs in this study and compared to neural networks. The results show that SVMs predict MID better than NNs while NNs give more consistent and easy to interpret sensitivity analysis than SVMs.\n    ",
        "submission_date": "2007-05-09T00:00:00",
        "last_modified_date": "2007-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.1244",
        "title": "Evolving Symbolic Controllers",
        "authors": [
            "Nicolas Godzik",
            "Marc Schoenauer",
            "Mich\u00e8le Sebag"
        ],
        "abstract": "  The idea of symbolic controllers tries to bridge the gap between the top-down manual design of the controller architecture, as advocated in Brooks' subsumption architecture, and the bottom-up designer-free approach that is now standard within the Evolutionary Robotics community. The designer provides a set of elementary behavior, and evolution is given the goal of assembling them to solve complex tasks. Two experiments are presented, demonstrating the efficiency and showing the recursiveness of this approach. In particular, the sensitivity with respect to the proposed elementary behaviors, and the robustness w.r.t. generalization of the resulting controllers are studied in detail.\n    ",
        "submission_date": "2007-05-09T00:00:00",
        "last_modified_date": "2007-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.1309",
        "title": "Robust Multi-Cellular Developmental Design",
        "authors": [
            "Alexandre Devert",
            "Nicolas Bred\u00e8che",
            "Marc Schoenauer"
        ],
        "abstract": "  This paper introduces a continuous model for Multi-cellular Developmental Design. The cells are fixed on a 2D grid and exchange \"chemicals\" with their neighbors during the growth process. The quantity of chemicals that a cell produces, as well as the differentiation value of the cell in the phenotype, are controlled by a Neural Network (the genotype) that takes as inputs the chemicals produced by the neighboring cells at the previous time step. In the proposed model, the number of iterations of the growth process is not pre-determined, but emerges during evolution: only organisms for which the growth process stabilizes give a phenotype (the stable state), others are declared nonviable. The optimization of the controller is done using the NEAT algorithm, that optimizes both the topology and the weights of the Neural Networks. Though each cell only receives local information from its neighbors, the experimental results of the proposed approach on the 'flags' problems (the phenotype must match a given 2D pattern) are almost as good as those of a direct regression approach using the same model with global information. Moreover, the resulting multi-cellular organisms exhibit almost perfect self-healing characteristics.\n    ",
        "submission_date": "2007-05-09T00:00:00",
        "last_modified_date": "2007-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.1999",
        "title": "A first-order Temporal Logic for Actions",
        "authors": [
            "Camilla Schwind"
        ],
        "abstract": "  We present a multi-modal action logic with first-order modalities, which contain terms which can be unified with the terms inside the subsequent formulas and which can be quantified. This makes it possible to handle simultaneously time and states. We discuss applications of this language to action theory where it is possible to express many temporal aspects of actions, as for example, beginning, end, time points, delayed preconditions and results, duration and many others. We present tableaux rules for a decidable fragment of this logic.\n    ",
        "submission_date": "2007-05-14T00:00:00",
        "last_modified_date": "2007-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.2011",
        "title": "Multi-Dimensional Recurrent Neural Networks",
        "authors": [
            "Alex Graves",
            "Santiago Fernandez",
            "Juergen Schmidhuber"
        ],
        "abstract": "  Recurrent neural networks (RNNs) have proved effective at one dimensional sequence learning tasks, such as speech and online handwriting recognition. Some of the properties that make RNNs suitable for such tasks, for example robustness to input warping, and the ability to access contextual information, are also desirable in multidimensional domains. However, there has so far been no direct way of applying RNNs to data with more than one spatio-temporal dimension. This paper introduces multi-dimensional recurrent neural networks (MDRNNs), thereby extending the potential applicability of RNNs to vision, video processing, medical imaging and many other areas, while avoiding the scaling problems that have plagued other multi-dimensional models. Experimental results are provided for two image segmentation tasks.\n    ",
        "submission_date": "2007-05-14T00:00:00",
        "last_modified_date": "2007-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.2235",
        "title": "Response Prediction of Structural System Subject to Earthquake Motions using Artificial Neural Network",
        "authors": [
            "S. Chakraverty",
            "T. Marwala",
            "Pallavi Gupta",
            "Thando Tettey"
        ],
        "abstract": "  This paper uses Artificial Neural Network (ANN) models to compute response of structural system subject to Indian earthquakes at Chamoli and Uttarkashi ground motion data. The system is first trained for a single real earthquake data. The trained ANN architecture is then used to simulate earthquakes with various intensities and it was found that the predicted responses given by ANN model are accurate for practical purposes. When the ANN is trained by a part of the ground motion data, it can also identify the responses of the structural system well. In this way the safeness of the structural systems may be predicted in case of future earthquakes without waiting for the earthquake to occur for the lessons. Time period and the corresponding maximum response of the building for an earthquake has been evaluated, which is again trained to predict the maximum response of the building at different time periods. The trained time period versus maximum response ANN model is also tested for real earthquake data of other place, which was not used in the training and was found to be in good agreement.\n    ",
        "submission_date": "2007-05-15T00:00:00",
        "last_modified_date": "2007-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.2236",
        "title": "Fault Classification using Pseudomodal Energies and Neuro-fuzzy modelling",
        "authors": [
            "Tshilidzi Marwala",
            "Thando Tettey",
            "Snehashish Chakraverty"
        ],
        "abstract": "  This paper presents a fault classification method which makes use of a Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the vibration signals of cylindrical shells. The calculation of Pseudomodal Energies, for the purposes of condition monitoring, has previously been found to be an accurate method of extracting features from vibration signals. This calculation is therefore used to extract features from vibration signals obtained from a diverse population of cylindrical shells. Some of the cylinders in the population have faults in different substructures. The pseudomodal energies calculated from the vibration signals are then used as inputs to a neuro-fuzzy model. A leave-one-out cross-validation process is used to test the performance of the model. It is found that the neuro-fuzzy model is able to classify faults with an accuracy of 91.62%, which is higher than the previously used multilayer perceptron.\n    ",
        "submission_date": "2007-05-15T00:00:00",
        "last_modified_date": "2007-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.2305",
        "title": "Fuzzy and Multilayer Perceptron for Evaluation of HV Bushings",
        "authors": [
            "Sizwe M. Dhlamini",
            "Tshilidzi Marwala",
            "Thokozani Majozi"
        ],
        "abstract": "  The work proposes the application of fuzzy set theory (FST) to diagnose the condition of high voltage bushings. The diagnosis uses dissolved gas analysis (DGA) data from bushings based on IEC60599 and IEEE C57-104 criteria for oil impregnated paper (OIP) bushings. FST and neural networks are compared in terms of accuracy and computational efficiency. Both FST and NN simulations were able to diagnose the bushings condition with 10% error. By using fuzzy theory, the maintenance department can classify bushings and know the extent of degradation in the component.\n    ",
        "submission_date": "2007-05-16T00:00:00",
        "last_modified_date": "2007-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.2310",
        "title": "On-Line Condition Monitoring using Computational Intelligence",
        "authors": [
            "C.B. Vilakazi",
            "T. Marwala",
            "P. Mautla",
            "E. Moloto"
        ],
        "abstract": "  This paper presents bushing condition monitoring frameworks that use multi-layer perceptrons (MLP), radial basis functions (RBF) and support vector machines (SVM) classifiers. The first level of the framework determines if the bushing is faulty or not while the second level determines the type of fault. The diagnostic gases in the bushings are analyzed using the dissolve gas analysis. MLP gives superior performance in terms of accuracy and training time than SVM and RBF. In addition, an on-line bushing condition monitoring approach, which is able to adapt to newly acquired data are introduced. This approach is able to accommodate new classes that are introduced by incoming data and is implemented using an incremental learning algorithm that uses MLP. The testing results improved from 67.5% to 95.8% as new data were introduced and the testing results improved from 60% to 95.3% as new conditions were introduced. On average the confidence value of the framework on its decision was 0.92.\n    ",
        "submission_date": "2007-05-16T00:00:00",
        "last_modified_date": "2007-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.3360",
        "title": "The Road to Quantum Artificial Intelligence",
        "authors": [
            "Kyriakos N. Sgarbas"
        ],
        "abstract": "  This paper overviews the basic principles and recent advances in the emerging field of Quantum Computation (QC), highlighting its potential application to Artificial Intelligence (AI). The paper provides a very brief introduction to basic QC issues like quantum registers, quantum gates and quantum algorithms and then it presents references, ideas and research guidelines on how QC can be used to deal with some basic AI problems, such as search and pattern matching, as soon as quantum computers become widely available.\n    ",
        "submission_date": "2007-05-23T00:00:00",
        "last_modified_date": "2007-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.4302",
        "title": "Truecluster matching",
        "authors": [
            "Jens Oehlschl\u00e4gel"
        ],
        "abstract": "  Cluster matching by permuting cluster labels is important in many clustering contexts such as cluster validation and cluster ensemble techniques. The classic approach is to minimize the euclidean distance between two cluster solutions which induces inappropriate stability in certain settings. Therefore, we present the truematch algorithm that introduces two improvements best explained in the crisp case. First, instead of maximizing the trace of the cluster crosstable, we propose to maximize a chi-square transformation of this crosstable. Thus, the trace will not be dominated by the cells with the largest counts but by the cells with the most non-random observations, taking into account the marginals. Second, we suggest a probabilistic component in order to break ties and to make the matching algorithm truly random on random data. The truematch algorithm is designed as a building block of the truecluster framework and scales in polynomial time. First simulation results confirm that the truematch algorithm gives more consistent truecluster results for unequal cluster sizes. Free R software is available.\n    ",
        "submission_date": "2007-05-29T00:00:00",
        "last_modified_date": "2007-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.4566",
        "title": "Loop corrections for message passing algorithms in continuous variable models",
        "authors": [
            "Bastian Wemmenhove",
            "Bert Kappen"
        ],
        "abstract": "  In this paper we derive the equations for Loop Corrected Belief Propagation on a continuous variable Gaussian model. Using the exactness of the averages for belief propagation for Gaussian models, a different way of obtaining the covariances is found, based on Belief Propagation on cavity graphs. We discuss the relation of this loop correction algorithm to Expectation Propagation algorithms for the case in which the model is no longer Gaussian, but slightly perturbed by nonlinear terms.\n    ",
        "submission_date": "2007-05-31T00:00:00",
        "last_modified_date": "2007-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0706.0022",
        "title": "Modeling Computations in a Semantic Network",
        "authors": [
            "Marko A. Rodriguez",
            "Johan Bollen"
        ],
        "abstract": "  Semantic network research has seen a resurgence from its early history in the cognitive sciences with the inception of the Semantic Web initiative. The Semantic Web effort has brought forth an array of technologies that support the encoding, storage, and querying of the semantic network data structure at the world stage. Currently, the popular conception of the Semantic Web is that of a data modeling medium where real and conceptual entities are related in semantically meaningful ways. However, new models have emerged that explicitly encode procedural information within the semantic network substrate. With these new technologies, the Semantic Web has evolved from a data modeling medium to a computational medium. This article provides a classification of existing computational modeling efforts and the requirements of supporting technologies that will aid in the further growth of this burgeoning domain.\n    ",
        "submission_date": "2007-05-31T00:00:00",
        "last_modified_date": "2007-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0706.0465",
        "title": "Virtual Sensor Based Fault Detection and Classification on a Plasma Etch Reactor",
        "authors": [
            "D. A. Sofge"
        ],
        "abstract": "  The SEMATECH sponsored J-88-E project teaming Texas Instruments with NeuroDyne (et al.) focused on Fault Detection and Classification (FDC) on a Lam 9600 aluminum plasma etch reactor, used in the process of semiconductor fabrication. Fault classification was accomplished by implementing a series of virtual sensor models which used data from real sensors (Lam Station sensors, Optical Emission Spectroscopy, and RF Monitoring) to predict recipe setpoints and wafer state characteristics. Fault detection and classification were performed by comparing predicted recipe and wafer state values with expected values. Models utilized include linear PLS, Polynomial PLS, and Neural Network PLS. Prediction of recipe setpoints based upon sensor data provides a capability for cross-checking that the machine is maintaining the desired setpoints. Wafer state characteristics such as Line Width Reduction and Remaining Oxide were estimated on-line using these same process sensors (Lam, OES, RFM). Wafer-to-wafer measurement of these characteristics in a production setting (where typically this information may be only sparsely available, if at all, after batch processing runs with numerous wafers have been completed) would provide important information to the operator that the process is or is not producing wafers within acceptable bounds of product quality. Production yield is increased, and correspondingly per unit cost is reduced, by providing the operator with the opportunity to adjust the process or machine before etching more wafers.\n    ",
        "submission_date": "2007-06-04T00:00:00",
        "last_modified_date": "2007-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0706.1137",
        "title": "Automatically Restructuring Practice Guidelines using the GEM DTD",
        "authors": [
            "Amanda Bouffier",
            "Thierry Poibeau"
        ],
        "abstract": "  This paper describes a system capable of semi-automatically filling an XML template from free texts in the clinical domain (practice guidelines). The XML template includes semantic information not explicitly encoded in the text (pairs of conditions and actions/recommendations). Therefore, there is a need to compute the exact scope of conditions over text sequences expressing the required actions. We present a system developed for this task. We show that it yields good performance when applied to the analysis of French practice guidelines.\n    ",
        "submission_date": "2007-06-08T00:00:00",
        "last_modified_date": "2007-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0706.1290",
        "title": "Temporal Reasoning without Transitive Tables",
        "authors": [
            "Sylviane R. Schwer"
        ],
        "abstract": "  Representing and reasoning about qualitative temporal information is an essential part of many artificial intelligence tasks. Lots of models have been proposed in the litterature for representing such temporal information. All derive from a point-based or an interval-based framework. One fundamental reasoning task that arises in applications of these frameworks is given by the following scheme: given possibly indefinite and incomplete knowledge of the binary relationships between some temporal objects, find the consistent scenarii between all these objects. All these models require transitive tables -- or similarly inference rules-- for solving such tasks. We have defined an alternative model, S-languages - to represent qualitative temporal information, based on the only two relations of \\emph{precedence} and \\emph{simultaneity}. In this paper, we show how this model enables to avoid transitive tables or inference rules to handle this kind of problem.\n    ",
        "submission_date": "2007-06-09T00:00:00",
        "last_modified_date": "2007-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0706.3639",
        "title": "A Collection of Definitions of Intelligence",
        "authors": [
            "Shane Legg",
            "Marcus Hutter"
        ],
        "abstract": "  This paper is a survey of a large number of informal definitions of ``intelligence'' that the authors have collected over the years. Naturally, compiling a complete list would be impossible as many definitions of intelligence are buried deep inside articles and books. Nevertheless, the 70-odd definitions presented here are, to the authors' knowledge, the largest and most well referenced collection there is.\n    ",
        "submission_date": "2007-06-25T00:00:00",
        "last_modified_date": "2007-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0706.4375",
        "title": "A Robust Linguistic Platform for Efficient and Domain specific Web Content Analysis",
        "authors": [
            "Thierry Hamon",
            "Adeline Nazarenko",
            "Thierry Poibeau",
            "Sophie Aubin",
            "Julien Derivi\u00e8re"
        ],
        "abstract": "  Web semantic access in specific domains calls for specialized search engines with enhanced semantic querying and indexing capacities, which pertain both to information retrieval (IR) and to information extraction (IE). A rich linguistic analysis is required either to identify the relevant semantic units to index and weight them according to linguistic specific statistical distribution, or as the basis of an information extraction process. Recent developments make Natural Language Processing (NLP) techniques reliable enough to process large collections of documents and to enrich them with semantic annotations. This paper focuses on the design and the development of a text processing platform, Ogmios, which has been developed in the ALVIS project. The Ogmios platform exploits existing NLP modules and resources, which may be tuned to specific domains and produces linguistically annotated documents. We show how the three constraints of genericity, domain semantic awareness and performance can be handled all together.\n    ",
        "submission_date": "2007-06-29T00:00:00",
        "last_modified_date": "2007-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0707.0701",
        "title": "Clustering and Feature Selection using Sparse Principal Component Analysis",
        "authors": [
            "Ronny Luss",
            "Alexandre d'Aspremont"
        ],
        "abstract": "  In this paper, we study the application of sparse principal component analysis (PCA) to clustering and feature selection problems. Sparse PCA seeks sparse factors, or linear combinations of the data variables, explaining a maximum amount of variance in the data while having only a limited number of nonzero coefficients. PCA is often used as a simple clustering technique and sparse factors allow us here to interpret the clusters in terms of a reduced set of variables. We begin with a brief introduction and motivation on sparse PCA and detail our implementation of the algorithm in d'Aspremont et al. (2005). We then apply these results to some classic clustering and feature selection problems arising in biology.\n    ",
        "submission_date": "2007-07-04T00:00:00",
        "last_modified_date": "2008-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0707.0704",
        "title": "Model Selection Through Sparse Maximum Likelihood Estimation",
        "authors": [
            "Onureena Banerjee",
            "Laurent El Ghaoui",
            "Alexandre d'Aspremont"
        ],
        "abstract": "  We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added l_1-norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our first algorithm uses block coordinate descent, and can be interpreted as recursive l_1-norm penalized regression. Our second algorithm, based on Nesterov's first order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright & Jordan (2006)), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data.\n    ",
        "submission_date": "2007-07-04T00:00:00",
        "last_modified_date": "2007-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0707.0705",
        "title": "Optimal Solutions for Sparse Principal Component Analysis",
        "authors": [
            "Alexandre d'Aspremont",
            "Francis Bach",
            "Laurent El Ghaoui"
        ],
        "abstract": "  Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefficients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semidefinite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all target numbers of non zero coefficients, with total complexity O(n^3), where n is the number of variables. We then use the same relaxation to derive sufficient conditions for global optimality of a solution, which can be tested in O(n^3) per pattern. We discuss applications in subset selection and sparse recovery and show on artificial examples and biological data that our algorithm does provide globally optimal solutions in many cases.\n    ",
        "submission_date": "2007-07-04T00:00:00",
        "last_modified_date": "2007-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0707.1452",
        "title": "Clusters, Graphs, and Networks for Analysing Internet-Web-Supported Communication within a Virtual Community",
        "authors": [
            "Xavier Polanco"
        ],
        "abstract": "  The proposal is to use clusters, graphs and networks as models in order to analyse the Web structure. Clusters, graphs and networks provide knowledge representation and organization. Clusters were generated by co-site analysis. The sample is a set of academic Web sites from the countries belonging to the European Union. These clusters are here revisited from the point of view of graph theory and social network analysis. This is a quantitative and structural analysis. In fact, the Internet is a computer network that connects people and organizations. Thus we may consider it to be a social network. The set of Web academic sites represents an empirical social network, and is viewed as a virtual community. The network structural properties are here analysed applying together cluster analysis, graph theory and social network analysis.\n    ",
        "submission_date": "2007-07-10T00:00:00",
        "last_modified_date": "2007-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0707.2506",
        "title": "Mixed Integer Linear Programming For Exact Finite-Horizon Planning In Decentralized Pomdps",
        "authors": [
            "Raghav Aras",
            "Alain Dutech",
            "Fran\u00e7ois Charpillet"
        ],
        "abstract": "  We consider the problem of finding an n-agent joint-policy for the optimal finite-horizon control of a decentralized Pomdp (Dec-Pomdp). This is a problem of very high complexity (NEXP-hard in n >= 2). In this paper, we propose a new mathematical programming approach for the problem. Our approach is based on two ideas: First, we represent each agent's policy in the sequence-form and not in the tree-form, thereby obtaining a very compact representation of the set of joint-policies. Second, using this compact representation, we solve this problem as an instance of combinatorial optimization for which we formulate a mixed integer linear program (MILP). The optimal solution of the MILP directly yields an optimal joint-policy for the Dec-Pomdp. Computational experience shows that formulating and solving the MILP requires significantly less time to solve benchmark Dec-Pomdp problems than existing algorithms. For example, the multi-agent tiger problem for horizon 4 is solved in 72 secs with the MILP whereas existing algorithms require several hours to solve it.\n    ",
        "submission_date": "2007-07-17T00:00:00",
        "last_modified_date": "2007-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0707.3781",
        "title": "Bijective Faithful Translations among Default Logics",
        "authors": [
            "Paolo Liberatore"
        ],
        "abstract": "  In this article, we study translations between variants of defaults logics such that the extensions of the theories that are the input and the output of the translation are in a bijective correspondence. We assume that a translation can introduce new variables and that the result of translating a theory can either be produced in time polynomial in the size of the theory or its output is polynomial in that size; we however restrict to the case in which the original theory has extensions. This study fills a gap between two previous pieces of work, one studying bijective translations among restrictions of default logics, and the other one studying non-bijective translations between default logics variants.\n    ",
        "submission_date": "2007-07-25T00:00:00",
        "last_modified_date": "2007-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0707.4289",
        "title": "A Leaf Recognition Algorithm for Plant Classification Using Probabilistic Neural Network",
        "authors": [
            "Stephen Gang Wu",
            "Forrest Sheng Bao",
            "Eric You Xu",
            "Yu-Xuan Wang",
            "Yi-Fan Chang",
            "Qiao-Liang Xiang"
        ],
        "abstract": "  In this paper, we employ Probabilistic Neural Network (PNN) with image and data processing techniques to implement a general purpose automated leaf recognition algorithm. 12 leaf features are extracted and orthogonalized into 5 principal variables which consist the input vector of the PNN. The PNN is trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater than 90%. Compared with other approaches, our algorithm is an accurate artificial intelligence approach which is fast in execution and easy in implementation.\n    ",
        "submission_date": "2007-07-29T00:00:00",
        "last_modified_date": "2007-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0708.0505",
        "title": "A preliminary analysis on metaheuristics methods applied to the Haplotype Inference Problem",
        "authors": [
            "Luca Di Gaspero",
            "Andrea Roli"
        ],
        "abstract": "  Haplotype Inference is a challenging problem in bioinformatics that consists in inferring the basic genetic constitution of diploid organisms on the basis of their genotype. This information allows researchers to perform association studies for the genetic variants involved in diseases and the individual responses to therapeutic agents.\n",
        "submission_date": "2007-08-03T00:00:00",
        "last_modified_date": "2007-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0708.0927",
        "title": "Modeling Visual Information Processing in Brain: A Computer Vision Point of View and Approach",
        "authors": [
            "Emanuel Diamant"
        ],
        "abstract": "  We live in the Information Age, and information has become a critically important component of our life. The success of the Internet made huge amounts of it easily available and accessible to everyone. To keep the flow of this information manageable, means for its faultless circulation and effective handling have become urgently required. Considerable research efforts are dedicated today to address this necessity, but they are seriously hampered by the lack of a common agreement about \"What is information?\" In particular, what is \"visual information\" - human's primary input from the surrounding world. The problem is further aggravated by a long-lasting stance borrowed from the biological vision research that assumes human-like information processing as an enigmatic mix of perceptual and cognitive vision faculties. I am trying to find a remedy for this bizarre situation. Relying on a new definition of \"information\", which can be derived from Kolmogorov's compexity theory and Chaitin's notion of algorithmic information, I propose a unifying framework for visual information processing, which explicitly accounts for the perceptual and cognitive image processing peculiarities. I believe that this framework will be useful to overcome the difficulties that are impeding our attempts to develop the right model of human-like intelligent image processing.\n    ",
        "submission_date": "2007-08-07T00:00:00",
        "last_modified_date": "2007-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0708.1527",
        "title": "A Data-Parallel Version of Aleph",
        "authors": [
            "Stasinos Konstantopoulos"
        ],
        "abstract": "  This is to present work on modifying the Aleph ILP system so that it evaluates the hypothesised clauses in parallel by distributing the data-set among the nodes of a parallel or distributed machine. The paper briefly discusses MPI, the interface used to access message- passing libraries for parallel computers and clusters. It then proceeds to describe an extension of YAP Prolog with an MPI interface and an implementation of data-parallel clause evaluation for Aleph through this interface. The paper concludes by testing the data-parallel Aleph on artificially constructed data-sets.\n    ",
        "submission_date": "2007-08-10T00:00:00",
        "last_modified_date": "2007-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0708.2303",
        "title": "Compositional Semantics Grounded in Commonsense Metaphysics",
        "authors": [
            "Walid S. Saba"
        ],
        "abstract": "  We argue for a compositional semantics grounded in a strongly typed ontology that reflects our commonsense view of the world and the way we talk about it in ordinary language. Assuming the existence of such a structure, we show that the semantics of various natural language phenomena may become nearly trivial.\n    ",
        "submission_date": "2007-08-17T00:00:00",
        "last_modified_date": "2007-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0708.4170",
        "title": "Raising a Hardness Result",
        "authors": [
            "Paolo Liberatore"
        ],
        "abstract": "  This article presents a technique for proving problems hard for classes of the polynomial hierarchy or for PSPACE. The rationale of this technique is that some problem restrictions are able to simulate existential or universal quantifiers. If this is the case, reductions from Quantified Boolean Formulae (QBF) to these restrictions can be transformed into reductions from QBFs having one more quantifier in the front. This means that a proof of hardness of a problem at level n in the polynomial hierarchy can be split into n separate proofs, which may be simpler than a proof directly showing a reduction from a class of QBFs to the considered problem.\n    ",
        "submission_date": "2007-08-30T00:00:00",
        "last_modified_date": "2007-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0708.4311",
        "title": "2006: Celebrating 75 years of AI - History and Outlook: the Next 25 Years",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "abstract": "  When Kurt Goedel layed the foundations of theoretical computer science in 1931, he also introduced essential concepts of the theory of Artificial Intelligence (AI). Although much of subsequent AI research has focused on heuristics, which still play a major role in many practical AI applications, in the new millennium AI theory has finally become a full-fledged formal science, with important optimality results for embodied agents living in unknown environments, obtained through a combination of theory a la Goedel and probability theory. Here we look back at important milestones of AI history, mention essential recent results, and speculate about what we may expect from the next 25 years, emphasizing the significance of the ongoing dramatic hardware speedups, and discussing Goedel-inspired, self-referential, self-improving universal problem solvers.\n    ",
        "submission_date": "2007-08-31T00:00:00",
        "last_modified_date": "2007-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0709.0116",
        "title": "On Ultrametric Algorithmic Information",
        "authors": [
            "Fionn Murtagh"
        ],
        "abstract": "  How best to quantify the information of an object, whether natural or artifact, is a problem of wide interest. A related problem is the computability of an object. We present practical examples of a new way to address this problem. By giving an appropriate representation to our objects, based on a hierarchical coding of information, we exemplify how it is remarkably easy to compute complex objects. Our algorithmic complexity is related to the length of the class of objects, rather than to the length of the object.\n    ",
        "submission_date": "2007-09-02T00:00:00",
        "last_modified_date": "2007-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0709.0522",
        "title": "Qualitative Belief Conditioning Rules (QBCR)",
        "authors": [
            "Florentin Smarandache",
            "Jean Dezert"
        ],
        "abstract": "  In this paper we extend the new family of (quantitative) Belief Conditioning Rules (BCR) recently developed in the Dezert-Smarandache Theory (DSmT) to their qualitative counterpart for belief revision. Since the revision of quantitative as well as qualitative belief assignment given the occurrence of a new event (the conditioning constraint) can be done in many possible ways, we present here only what we consider as the most appealing Qualitative Belief Conditioning Rules (QBCR) which allow to revise the belief directly with words and linguistic labels and thus avoids the introduction of ad-hoc translations of quantitative beliefs into quantitative ones for solving the problem.\n    ",
        "submission_date": "2007-09-04T00:00:00",
        "last_modified_date": "2007-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0709.0674",
        "title": "Simple Algorithmic Principles of Discovery, Subjective Beauty, Selective Attention, Curiosity & Creativity",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "abstract": "  I postulate that human or other intelligent agents function or should function as follows. They store all sensory observations as they come - the data is holy. At any time, given some agent's current coding capabilities, part of the data is compressible by a short and hopefully fast program / description / explanation / world model. In the agent's subjective eyes, such data is more regular and more \"beautiful\" than other data. It is well-known that knowledge of regularity and repeatability may improve the agent's ability to plan actions leading to external rewards. In absence of such rewards, however, known beauty is boring. Then \"interestingness\" becomes the first derivative of subjective beauty: as the learning agent improves its compression algorithm, formerly apparently random data parts become subjectively more regular and beautiful. Such progress in compressibility is measured and maximized by the curiosity drive: create action sequences that extend the observation history and yield previously unknown / unpredictable but quickly learnable algorithmic regularity. We discuss how all of the above can be naturally implemented on computers, through an extension of passive unsupervised learning to the case of active data selection: we reward a general reinforcement learner (with access to the adaptive compressor) for actions that improve the subjective compressibility of the growing data. An unusually large breakthrough in compressibility deserves the name \"discovery\". The \"creativity\" of artists, dancers, musicians, pure mathematicians can be viewed as a by-product of this principle. Several qualitative examples support this hypothesis.\n    ",
        "submission_date": "2007-09-05T00:00:00",
        "last_modified_date": "2007-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0709.1099",
        "title": "Multi-Sensor Fusion Method using Dynamic Bayesian Network for Precise Vehicle Localization and Road Matching",
        "authors": [
            "Cherif Smaili",
            "Maan El Badaoui El Najjar",
            "Fran\u00e7ois Charpillet"
        ],
        "abstract": "  This paper presents a multi-sensor fusion strategy for a novel road-matching method designed to support real-time navigational features within advanced driving-assistance systems. Managing multihypotheses is a useful strategy for the road-matching problem. The multi-sensor fusion and multi-modal estimation are realized using Dynamical Bayesian Network. Experimental results, using data from Antilock Braking System (ABS) sensors, a differential Global Positioning System (GPS) receiver and an accurate digital roadmap, illustrate the performances of this approach, especially in ambiguous situations.\n    ",
        "submission_date": "2007-09-07T00:00:00",
        "last_modified_date": "2007-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0709.1167",
        "title": "Using RDF to Model the Structure and Process of Systems",
        "authors": [
            "Marko A. Rodriguez",
            "Jennifer H. Watkins",
            "Johan Bollen",
            "Carlos Gershenson"
        ],
        "abstract": "  Many systems can be described in terms of networks of discrete elements and their various relationships to one another. A semantic network, or multi-relational network, is a directed labeled graph consisting of a heterogeneous set of entities connected by a heterogeneous set of relationships. Semantic networks serve as a promising general-purpose modeling substrate for complex systems. Various standardized formats and tools are now available to support practical, large-scale semantic network models. First, the Resource Description Framework (RDF) offers a standardized semantic network data model that can be further formalized by ontology modeling languages such as RDF Schema (RDFS) and the Web Ontology Language (OWL). Second, the recent introduction of highly performant triple-stores (i.e. semantic network databases) allows semantic network models on the order of $10^9$ edges to be efficiently stored and manipulated. RDF and its related technologies are currently used extensively in the domains of computer science, digital library science, and the biological sciences. This article will provide an introduction to RDF/RDFS/OWL and an examination of its suitability to model discrete element complex systems.\n    ",
        "submission_date": "2007-09-08T00:00:00",
        "last_modified_date": "2007-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0709.1667",
        "title": "Solving Constraint Satisfaction Problems through Belief Propagation-guided decimation",
        "authors": [
            "Andrea Montanari",
            "Federico Ricci-Tersenghi",
            "Guilhem Semerjian"
        ],
        "abstract": "Message passing algorithms have proved surprisingly successful in solving hard constraint satisfaction problems on sparse random graphs. In such applications, variables are fixed sequentially to satisfy the constraints. Message passing is run after each step. Its outcome provides an heuristic to make choices at next step. This approach has been referred to as `decimation,' with reference to analogous procedures in statistical physics.\n",
        "submission_date": "2007-09-11T00:00:00",
        "last_modified_date": "2019-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0709.1701",
        "title": "Enrichment of Qualitative Beliefs for Reasoning under Uncertainty",
        "authors": [
            "Xinde Li",
            "Xinhan Huang",
            "Florentin Smarandache",
            "Jean Dezert"
        ],
        "abstract": "  This paper deals with enriched qualitative belief functions for reasoning under uncertainty and for combining information expressed in natural language through linguistic labels. In this work, two possible enrichments (quantitative and/or qualitative) of linguistic labels are considered and operators (addition, multiplication, division, etc) for dealing with them are proposed and explained. We denote them $qe$-operators, $qe$ standing for \"qualitative-enriched\" operators. These operators can be seen as a direct extension of the classical qualitative operators ($q$-operators) proposed recently in the Dezert-Smarandache Theory of plausible and paradoxist reasoning (DSmT). $q$-operators are also justified in details in this paper. The quantitative enrichment of linguistic label is a numerical supporting degree in $[0,\\infty)$, while the qualitative enrichment takes its values in a finite ordered set of linguistic values. Quantitative enrichment is less precise than qualitative enrichment, but it is expected more close with what human experts can easily provide when expressing linguistic labels with supporting degrees. Two simple examples are given to show how the fusion of qualitative-enriched belief assignments can be done.\n    ",
        "submission_date": "2007-09-11T00:00:00",
        "last_modified_date": "2007-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0709.2065",
        "title": "Toward Psycho-robots",
        "authors": [
            "Andrei Khrennikov"
        ],
        "abstract": "  We try to perform geometrization of psychology by representing mental states, <<ideas>>, by points of a metric space, <<mental space>>. Evolution of ideas is described by dynamical systems in metric mental space. We apply the mental space approach for modeling of flows of unconscious and conscious information in the human brain. In a series of models, Models 1-4, we consider cognitive systems with increasing complexity of psychological behavior determined by structure of flows of ideas. Since our models are in fact models of the AI-type, one immediately recognizes that they can be used for creation of AI-systems, which we call psycho-robots, exhibiting important elements of human psyche. Creation of such psycho-robots may be useful improvement of domestic robots. At the moment domestic robots are merely simple working devices (e.g. vacuum cleaners or lawn mowers) . However, in future one can expect demand in systems which be able not only perform simple work tasks, but would have elements of human self-developing psyche. Such AI-psyche could play an important role both in relations between psycho-robots and their owners as well as between psycho-robots. Since the presence of a huge numbers of psycho-complexes is an essential characteristic of human psychology, it would be interesting to model them in the AI-framework.\n    ",
        "submission_date": "2007-09-13T00:00:00",
        "last_modified_date": "2007-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0709.2506",
        "title": "Autoencoder, Principal Component Analysis and Support Vector Regression for Data Imputation",
        "authors": [
            "Vukosi N. Marivate",
            "Fulufhelo V. Nelwamodo",
            "Tshilidzi Marwala"
        ],
        "abstract": "  Data collection often results in records that have missing values or variables. This investigation compares 3 different data imputation models and identifies their merits by using accuracy measures. Autoencoder Neural Networks, Principal components and Support Vector regression are used for prediction and combined with a genetic algorithm to then impute missing variables. The use of PCA improves the overall performance of the autoencoder network while the use of support vector regression shows promising potential for future investigation. Accuracies of up to 97.4 % on imputation of some of the variables were achieved.\n    ",
        "submission_date": "2007-09-16T00:00:00",
        "last_modified_date": "2007-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0709.3974",
        "title": "Fitness landscape of the cellular automata majority problem: View from the Olympus",
        "authors": [
            "S\u00e9bastien Verel",
            "Philippe Collard",
            "Marco Tomassini",
            "Leonardo Vanneschi"
        ],
        "abstract": "  In this paper we study cellular automata (CAs) that perform the computational Majority task. This task is a good example of what the phenomenon of emergence in complex systems is. We take an interest in the reasons that make this particular fitness landscape a difficult one. The first goal is to study the landscape as such, and thus it is ideally independent from the actual heuristics used to search the space. However, a second goal is to understand the features a good search technique for this particular problem space should possess. We statistically quantify in various ways the degree of difficulty of searching this landscape. Due to neutrality, investigations based on sampling techniques on the whole landscape are difficult to conduct. So, we go exploring the landscape from the top. Although it has been proved that no CA can perform the task perfectly, several efficient CAs for this task have been found. Exploiting similarities between these CAs and symmetries in the landscape, we define the Olympus landscape which is regarded as the ''heavenly home'' of the best local optima known (blok). Then we measure several properties of this subspace. Although it is easier to find relevant CAs in this subspace than in the overall landscape, there are structural reasons that prevent a searcher from finding overfitted CAs in the Olympus. Finally, we study dynamics and performance of genetic algorithms on the Olympus in order to confirm our analysis and to find efficient CAs for the Majority problem with low computational cost.\n    ",
        "submission_date": "2007-09-25T00:00:00",
        "last_modified_date": "2007-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0709.4010",
        "title": "Local search heuristics: Fitness Cloud versus Fitness Landscape",
        "authors": [
            "Philippe Collard",
            "S\u00e9bastien Verel",
            "Manuel Clergue"
        ],
        "abstract": "  This paper introduces the concept of fitness cloud as an alternative way to visualize and analyze search spaces than given by the geographic notion of fitness landscape. It is argued that the fitness cloud concept overcomes several deficiencies of the landscape representation. Our analysis is based on the correlation between fitness of solutions and fitnesses of nearest solutions according to some neighboring. We focus on the behavior of local search heuristics, such as hill climber, on the well-known NK fitness landscape. In both cases the fitness vs. fitness correlation is shown to be related to the epistatic parameter K.\n    ",
        "submission_date": "2007-09-25T00:00:00",
        "last_modified_date": "2007-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0709.4011",
        "title": "Measuring the Evolvability Landscape to study Neutrality",
        "authors": [
            "S\u00e9bastien Verel",
            "Philippe Collard",
            "Manuel Clergue"
        ],
        "abstract": "  This theoretical work defines the measure of autocorrelation of evolvability in the context of neutral fitness landscape. This measure has been studied on the classical MAX-SAT problem. This work highlight a new characteristic of neutral fitness landscapes which allows to design new adapted metaheuristic.\n    ",
        "submission_date": "2007-09-25T00:00:00",
        "last_modified_date": "2007-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0709.4015",
        "title": "From Texts to Structured Documents: The Case of Health Practice Guidelines",
        "authors": [
            "Amanda Bouffier"
        ],
        "abstract": "  This paper describes a system capable of semi-automatically filling an XML template from free texts in the clinical domain (practice guidelines). The XML template includes semantic information not explicitly encoded in the text (pairs of conditions and actions/recommendations). Therefore, there is a need to compute the exact scope of conditions over text sequences expressing the required actions. We present in this paper the rules developed for this task. We show that the system yields good performance when applied to the analysis of French practice guidelines.\n    ",
        "submission_date": "2007-09-25T00:00:00",
        "last_modified_date": "2007-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0710.0013",
        "title": "Lagrangian Relaxation for MAP Estimation in Graphical Models",
        "authors": [
            "Jason K. Johnson",
            "Dmitry M. Malioutov",
            "Alan S. Willsky"
        ],
        "abstract": "  We develop a general framework for MAP estimation in discrete and Gaussian graphical models using Lagrangian relaxation techniques. The key idea is to reformulate an intractable estimation problem as one defined on a more tractable graph, but subject to additional constraints. Relaxing these constraints gives a tractable dual problem, one defined by a thin graph, which is then optimized by an iterative procedure. When this iterative optimization leads to a consistent estimate, one which also satisfies the constraints, then it corresponds to an optimal MAP estimate of the original model. Otherwise there is a ``duality gap'', and we obtain a bound on the optimal solution. Thus, our approach combines convex optimization with dynamic programming techniques applicable for thin graphs. The popular tree-reweighted max-product (TRMP) method may be seen as solving a particular class of such relaxations, where the intractable graph is relaxed to a set of spanning trees. We also consider relaxations to a set of small induced subgraphs, thin subgraphs (e.g. loops), and a connected tree obtained by ``unwinding'' cycles. In addition, we propose a new class of multiscale relaxations that introduce ``summary'' variables. The potential benefits of such generalizations include: reducing or eliminating the ``duality gap'' in hard problems, reducing the number or Lagrange multipliers in the dual problem, and accelerating convergence of the iterative optimization procedure.\n    ",
        "submission_date": "2007-09-28T00:00:00",
        "last_modified_date": "2007-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0710.2611",
        "title": "Geometric Analogue of Holographic Reduced Representation",
        "authors": [
            "Diederik Aerts",
            "Marek Czachor",
            "Bart De Moor"
        ],
        "abstract": "  Holographic reduced representations (HRR) are based on superpositions of convolution-bound $n$-tuples, but the $n$-tuples cannot be regarded as vectors since the formalism is basis dependent. This is why HRR cannot be associated with geometric structures. Replacing convolutions by geometric products one arrives at reduced representations analogous to HRR but interpretable in terms of geometry. Variable bindings occurring in both HRR and its geometric analogue mathematically correspond to two different representations of $Z_2\\times...\\times Z_2$ (the additive group of binary $n$-tuples with addition modulo 2). As opposed to standard HRR, variable binding performed by means of geometric product allows for computing exact inverses of all nonzero vectors, a procedure even simpler than approximate inverses employed in HRR. The formal structure of the new reduced representation is analogous to cartoon computation, a geometric analogue of quantum computation.\n    ",
        "submission_date": "2007-10-15T00:00:00",
        "last_modified_date": "2007-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0710.3185",
        "title": "Fuzzy Modeling of Electrical Impedance Tomography Image of the Lungs",
        "authors": [
            "Harki Tanaka",
            "Neli Regina Siqueira Ortega",
            "Mauricio Stanzione Galizia",
            "Joao Batista Borges Sobrinho",
            "Marcelo Britto Passos Amato"
        ],
        "abstract": "  Electrical Impedance Tomography (EIT) is a functional imaging method that is being developed for bedside use in critical care medicine. Aiming at improving the chest anatomical resolution of EIT images we developed a fuzzy model based on EIT high temporal resolution and the functional information contained in the pulmonary perfusion and ventilation signals. EIT data from an experimental animal model were collected during normal ventilation and apnea while an injection of hypertonic saline was used as a reference . The fuzzy model was elaborated in three parts: a modeling of the heart, a pulmonary map from ventilation images and, a pulmonary map from perfusion images. Image segmentation was performed using a threshold method and a ventilation/perfusion map was generated. EIT images treated by the fuzzy model were compared with the hypertonic saline injection method and CT-scan images, presenting good results in both qualitative (the image obtained by the model was very similar to that of the CT-scan) and quantitative (the ROC curve provided an area equal to 0.93) point of view. Undoubtedly, these results represent an important step in the EIT images area, since they open the possibility of developing EIT-based bedside clinical methods, which are not available nowadays. These achievements could serve as the base to develop EIT diagnosis system for some life-threatening diseases commonly found in critical care medicine.\n    ",
        "submission_date": "2007-10-16T00:00:00",
        "last_modified_date": "2007-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0710.3561",
        "title": "Stationary probability density of stochastic search processes in global optimization",
        "authors": [
            "Arturo Berrones"
        ],
        "abstract": "  A method for the construction of approximate analytical expressions for the stationary marginal densities of general stochastic search processes is proposed. By the marginal densities, regions of the search space that with high probability contain the global optima can be readily defined. The density estimation procedure involves a controlled number of linear operations, with a computational cost per iteration that grows linearly with problem size.\n    ",
        "submission_date": "2007-10-18T00:00:00",
        "last_modified_date": "2007-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0710.4231",
        "title": "Analyzing covert social network foundation behind terrorism disaster",
        "authors": [
            "Yoshiharu Maeno",
            "Yukio Ohsawa"
        ],
        "abstract": "  This paper addresses a method to analyze the covert social network foundation hidden behind the terrorism disaster. It is to solve a node discovery problem, which means to discover a node, which functions relevantly in a social network, but escaped from monitoring on the presence and mutual relationship of nodes. The method aims at integrating the expert investigator's prior understanding, insight on the terrorists' social network nature derived from the complex graph theory, and computational data processing. The social network responsible for the 9/11 attack in 2001 is used to execute simulation experiment to evaluate the performance of the method.\n    ",
        "submission_date": "2007-10-23T00:00:00",
        "last_modified_date": "2007-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0710.4734",
        "title": "Computational Intelligence Characterization Method of Semiconductor Device",
        "authors": [
            "Eric Liau",
            "Doris Schmitt-Landsiedel"
        ],
        "abstract": "  Characterization of semiconductor devices is used to gather as much data about the device as possible to determine weaknesses in design or trends in the manufacturing process. In this paper, we propose a novel multiple trip point characterization concept to overcome the constraint of single trip point concept in device characterization phase. In addition, we use computational intelligence techniques (e.g. neural network, fuzzy and genetic algorithm) to further manipulate these sets of multiple trip point values and tests based on semiconductor test equipments, Our experimental results demonstrate an excellent design parameter variation analysis in device characterization phase, as well as detection of a set of worst case tests that can provoke the worst case variation, while traditional approach was not capable of detecting them.\n    ",
        "submission_date": "2007-10-25T00:00:00",
        "last_modified_date": "2007-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0710.4975",
        "title": "Node discovery problem for a social network",
        "authors": [
            "Yoshiharu Maeno"
        ],
        "abstract": "  Methods to solve a node discovery problem for a social network are presented. Covert nodes refer to the nodes which are not observable directly. They transmit the influence and affect the resulting collaborative activities among the persons in a social network, but do not appear in the surveillance logs which record the participants of the collaborative activities. Discovering the covert nodes is identifying the suspicious logs where the covert nodes would appear if the covert nodes became overt. The performance of the methods is demonstrated with a test dataset generated from computationally synthesized networks and a real organization.\n    ",
        "submission_date": "2007-10-26T00:00:00",
        "last_modified_date": "2009-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0711.0694",
        "title": "Performance Bounds for Lambda Policy Iteration and Application to the Game of Tetris",
        "authors": [
            "Bruno Scherrer"
        ],
        "abstract": "We consider the discrete-time infinite-horizon optimal control problem formalized by Markov Decision Processes. We revisit the work of Bertsekas and Ioffe, that introduced $\\lambda$ Policy Iteration, a family of algorithms parameterized by $\\lambda$ that generalizes the standard algorithms Value Iteration and Policy Iteration, and has some deep connections with the Temporal Differences algorithm TD($\\lambda$) described by Sutton and Barto. We deepen the original theory developped by the authors by providing convergence rate bounds which generalize standard bounds for Value Iteration described for instance by Puterman. Then, the main contribution of this paper is to develop the theory of this algorithm when it is used in an approximate form and show that this is sound. Doing so, we extend and unify the separate analyses developped by Munos for Approximate Value Iteration and Approximate Policy Iteration. Eventually, we revisit the use of this algorithm in the training of a Tetris playing controller as originally done by Bertsekas and Ioffe. We provide an original performance bound that can be applied to such an undiscounted control problem. Our empirical results are different from those of Bertsekas and Ioffe (which were originally qualified as \"paradoxical\" and \"intriguing\"), and much more conform to what one would expect from a learning experiment. We discuss the possible reason for such a difference.\n    ",
        "submission_date": "2007-11-05T00:00:00",
        "last_modified_date": "2011-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0711.1466",
        "title": "Predicting relevant empty spots in social interaction",
        "authors": [
            "Yoshiharu Maeno",
            "Yukio Ohsawa"
        ],
        "abstract": "  An empty spot refers to an empty hard-to-fill space which can be found in the records of the social interaction, and is the clue to the persons in the underlying social network who do not appear in the records. This contribution addresses a problem to predict relevant empty spots in social interaction. Homogeneous and inhomogeneous networks are studied as a model underlying the social interaction. A heuristic predictor function approach is presented as a new method to address the problem. Simulation experiment is demonstrated over a homogeneous network. A test data in the form of baskets is generated from the simulated communication. Precision to predict the empty spots is calculated to demonstrate the performance of the presented approach.\n    ",
        "submission_date": "2007-11-09T00:00:00",
        "last_modified_date": "2008-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0711.1814",
        "title": "Building Rules on Top of Ontologies for the Semantic Web with Inductive Logic Programming",
        "authors": [
            "Francesca A. Lisi"
        ],
        "abstract": "  Building rules on top of ontologies is the ultimate goal of the logical layer of the Semantic Web. To this aim an ad-hoc mark-up language for this layer is currently under discussion. It is intended to follow the tradition of hybrid knowledge representation and reasoning systems such as $\\mathcal{AL}$-log that integrates the description logic $\\mathcal{ALC}$ and the function-free Horn clausal language \\textsc{Datalog}. In this paper we consider the problem of automating the acquisition of these rules for the Semantic Web. We propose a general framework for rule induction that adopts the methodological apparatus of Inductive Logic Programming and relies on the expressive and deductive power of $\\mathcal{AL}$-log. The framework is valid whatever the scope of induction (description vs. prediction) is. Yet, for illustrative purposes, we also discuss an instantiation of the framework which aims at description and turns out to be useful in Ontology Refinement.\n",
        "submission_date": "2007-11-12T00:00:00",
        "last_modified_date": "2007-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0711.2909",
        "title": "Comparing the notions of optimality in CP-nets, strategic games and soft constraints",
        "authors": [
            "Krzysztof R. Apt",
            "Francesca Rossi",
            "Kristen Brent Venable"
        ],
        "abstract": "  The notion of optimality naturally arises in many areas of applied mathematics and computer science concerned with decision making. Here we consider this notion in the context of three formalisms used for different purposes in reasoning about multi-agent systems: strategic games, CP-nets, and soft constraints. To relate the notions of optimality in these formalisms we introduce a natural qualitative modification of the notion of a strategic game. We show then that the optimal outcomes of a CP-net are exactly the Nash equilibria of such games. This allows us to use the techniques of game theory to search for optimal outcomes of CP-nets and vice-versa, to use techniques developed for CP-nets to search for Nash equilibria of the considered games. Then, we relate the notion of optimality used in the area of soft constraints to that used in a generalization of strategic games, called graphical games. In particular we prove that for a natural class of soft constraints that includes weighted constraints every optimal solution is both a Nash equilibrium and Pareto efficient joint strategy. For a natural mapping in the other direction we show that Pareto efficient joint strategies coincide with the optimal solutions of soft constraints.\n    ",
        "submission_date": "2007-11-19T00:00:00",
        "last_modified_date": "2008-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0711.3235",
        "title": "A Game-Theoretic Analysis of Updating Sets of Probabilities",
        "authors": [
            "Peter D. Grunwald",
            "Joseph Y. Halpern"
        ],
        "abstract": "  We consider how an agent should update her uncertainty when it is represented by a set $\u00b6$ of probability distributions and the agent observes that a random variable $X$ takes on value $x$, given that the agent makes decisions using the minimax criterion, perhaps the best-studied and most commonly-used criterion in the literature. We adopt a game-theoretic framework, where the agent plays against a bookie, who chooses some distribution from $\u00b6$. We consider two reasonable games that differ in what the bookie knows when he makes his choice. Anomalies that have been observed before, like time inconsistency, can be understood as arising important because different games are being played, against bookies with different information. We characterize the important special cases in which the optimal decision rules according to the minimax criterion amount to either conditioning or simply ignoring the information. Finally, we consider the relationship between conditioning and calibration when uncertainty is described by sets of probabilities.\n    ",
        "submission_date": "2007-11-20T00:00:00",
        "last_modified_date": "2007-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0711.3419",
        "title": "Translating OWL and Semantic Web Rules into Prolog: Moving Toward Description Logic Programs",
        "authors": [
            "Ken Samuel",
            "Leo Obrst",
            "Suzette Stoutenberg",
            "Karen Fox",
            "Paul Franklin",
            "Adrian Johnson",
            "Ken Laskey",
            "Deborah Nichols",
            "Steve Lopez",
            "Jason Peterson"
        ],
        "abstract": "  To appear in Theory and Practice of Logic Programming (TPLP), 2008.\n",
        "submission_date": "2007-11-21T00:00:00",
        "last_modified_date": "2007-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.0451",
        "title": "A Reactive Tabu Search Algorithm for Stimuli Generation in Psycholinguistics",
        "authors": [
            "Alejandro Chinea Manrique De Lara"
        ],
        "abstract": "  The generation of meaningless \"words\" matching certain statistical and/or linguistic criteria is frequently needed for experimental purposes in Psycholinguistics. Such stimuli receive the name of pseudowords or nonwords in the Cognitive Neuroscience literatue. The process for building nonwords sometimes has to be based on linguistic units such as syllables or morphemes, resulting in a numerical explosion of combinations when the size of the nonwords is increased. In this paper, a reactive tabu search scheme is proposed to generate nonwords of variables size. The approach builds pseudowords by using a modified Metaheuristic algorithm based on a local search procedure enhanced by a feedback-based scheme. Experimental results show that the new algorithm is a practical and effective tool for nonword generation.\n    ",
        "submission_date": "2007-12-04T00:00:00",
        "last_modified_date": "2007-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.0836",
        "title": "Evolving localizations in reaction-diffusion cellular automata",
        "authors": [
            "Andrew Adamatzky",
            "Larry Bull",
            "Pierre Collet",
            "Emmanuel Sapin"
        ],
        "abstract": "  We consider hexagonal cellular automata with immediate cell neighbourhood and three cell-states. Every cell calculates its next state depending on the integral representation of states in its neighbourhood, i.e. how many neighbours are in each one state. We employ evolutionary algorithms to breed local transition functions that support mobile localizations (gliders), and characterize sets of the functions selected in terms of quasi-chemical systems. Analysis of the set of functions evolved allows to speculate that mobile localizations are likely to emerge in the quasi-chemical systems with limited diffusion of one reagent, a small number of molecules is required for amplification of travelling localizations, and reactions leading to stationary localizations involve relatively equal amount of quasi-chemical species. Techniques developed can be applied in cascading signals in nature-inspired spatially extended computing devices, and phenomenological studies and classification of non-linear discrete systems.\n    ",
        "submission_date": "2007-12-05T00:00:00",
        "last_modified_date": "2007-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.0948",
        "title": "A Common View on Strong, Uniform, and Other Notions of Equivalence in Answer-Set Programming",
        "authors": [
            "Stefan Woltran"
        ],
        "abstract": "  Logic programming under the answer-set semantics nowadays deals with numerous different notions of program equivalence. This is due to the fact that equivalence for substitution (known as strong equivalence) and ordinary equivalence are different concepts. The former holds, given programs P and Q, iff P can be faithfully replaced by Q within any context R, while the latter holds iff P and Q provide the same output, that is, they have the same answer sets. Notions in between strong and ordinary equivalence have been introduced as theoretical tools to compare incomplete programs and are defined by either restricting the syntactic structure of the considered context programs R or by bounding the set A of atoms allowed to occur in R (relativized equivalence).For the latter approach, different A yield properly different equivalence notions, in general. For the former approach, however, it turned out that any ``reasonable'' syntactic restriction to R coincides with either ordinary, strong, or uniform equivalence. In this paper, we propose a parameterization for equivalence notions which takes care of both such kinds of restrictions simultaneously by bounding, on the one hand, the atoms which are allowed to occur in the rule heads of the context and, on the other hand, the atoms which are allowed to occur in the rule bodies of the context. We introduce a general semantical characterization which includes known ones as SE-models (for strong equivalence) or UE-models (for uniform equivalence) as special cases. Moreover,we provide complexity bounds for the problem in question and sketch a possible implementation method.\n",
        "submission_date": "2007-12-06T00:00:00",
        "last_modified_date": "2007-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.1097",
        "title": "On Using Unsatisfiability for Solving Maximum Satisfiability",
        "authors": [
            "Joao Marques-Silva",
            "Jordi Planes"
        ],
        "abstract": "  Maximum Satisfiability (MaxSAT) is a well-known optimization pro- blem, with several practical applications. The most widely known MAXS AT algorithms are ineffective at solving hard problems instances from practical application domains. Recent work proposed using efficient Boolean Satisfiability (SAT) solvers for solving the MaxSAT problem, based on identifying and eliminating unsatisfiable subformulas. However, these algorithms do not scale in practice. This paper analyzes existing MaxSAT algorithms based on unsatisfiable subformula identification. Moreover, the paper proposes a number of key optimizations to these MaxSAT algorithms and a new alternative algorithm. The proposed optimizations and the new algorithm provide significant performance improvements on MaxSAT instances from practical applications. Moreover, the efficiency of the new generation of unsatisfiability-based MaxSAT solvers becomes effectively indexed to the ability of modern SAT solvers to proving unsatisfiability and identifying unsatisfiable subformulas.\n    ",
        "submission_date": "2007-12-07T00:00:00",
        "last_modified_date": "2007-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.1182",
        "title": "Cumulative and Averaging Fission of Beliefs",
        "authors": [
            "Audun Josang"
        ],
        "abstract": "  Belief fusion is the principle of combining separate beliefs or bodies of evidence originating from different sources. Depending on the situation to be modelled, different belief fusion methods can be applied. Cumulative and averaging belief fusion is defined for fusing opinions in subjective logic, and for fusing belief functions in general. The principle of fission is the opposite of fusion, namely to eliminate the contribution of a specific belief from an already fused belief, with the purpose of deriving the remaining belief. This paper describes fission of cumulative belief as well as fission of averaging belief in subjective logic. These operators can for example be applied to belief revision in Bayesian belief networks, where the belief contribution of a given evidence source can be determined as a function of a given fused belief and its other contributing beliefs.\n    ",
        "submission_date": "2007-12-07T00:00:00",
        "last_modified_date": "2007-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.1529",
        "title": "Ontology and Formal Semantics - Integration Overdue",
        "authors": [
            "Walid S. Saba"
        ],
        "abstract": "  In this note we suggest that difficulties encountered in natural language semantics are, for the most part, due to the use of mere symbol manipulation systems that are devoid of any content. In such systems, where there is hardly any link with our common-sense view of the world, and it is quite difficult to envision how one can formally account for the considerable amount of content that is often implicit, but almost never explicitly stated in our everyday discourse. The solution, in our opinion, is a compositional semantics grounded in an ontology that reflects our commonsense view of the world and the way we talk about it in ordinary language. In the compositional logic we envision there are ontological (or first-intension) concepts, and logical (or second-intension) concepts, and where the ontological concepts include not only Davidsonian events, but other abstract objects as well (e.g., states, processes, properties, activities, attributes, etc.) It will be demonstrated here that in such a framework, a number of challenges in the semantics of natural language (e.g., metonymy, intensionality, metaphor, etc.) can be properly and uniformly addressed.\n    ",
        "submission_date": "2007-12-01T00:00:00",
        "last_modified_date": "2007-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.2141",
        "title": "Numerical Sensitivity and Efficiency in the Treatment of Epistemic and Aleatory Uncertainty",
        "authors": [
            "Eric Chojnacki",
            "Jean Baccou",
            "S\u00e9bastien Destercke"
        ],
        "abstract": "  The treatment of both aleatory and epistemic uncertainty by recent methods often requires an high computational effort. In this abstract, we propose a numerical sampling method allowing to lighten the computational burden of treating the information by means of so-called fuzzy random variables.\n    ",
        "submission_date": "2007-12-13T00:00:00",
        "last_modified_date": "2007-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.2389",
        "title": "Decomposition During Search for Propagation-Based Constraint Solvers",
        "authors": [
            "Martin Mann",
            "Guido Tack",
            "Sebastian Will"
        ],
        "abstract": "  We describe decomposition during search (DDS), an integration of And/Or tree search into propagation-based constraint solvers. The presented search algorithm dynamically decomposes sub-problems of a constraint satisfaction problem into independent partial problems, avoiding redundant work.\n",
        "submission_date": "2007-12-14T00:00:00",
        "last_modified_date": "2008-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.3147",
        "title": "Common knowledge logic in a higher order proof assistant?",
        "authors": [
            "Pierre Lescanne"
        ],
        "abstract": "  This paper presents experiments on common knowledge logic, conducted with the help of the proof assistant Coq. The main feature of common knowledge logic is the eponymous modality that says that a group of agents shares a knowledge about a certain proposition in a inductive way. This modality is specified by using a fixpoint approach. Furthermore, from these experiments, we discuss and compare the structure of theorems that can be proved in specific theories that use common knowledge logic. Those structures manifests the interplay between the theory (as implemented in the proof assistant Coq) and the metatheory.\n    ",
        "submission_date": "2007-12-19T00:00:00",
        "last_modified_date": "2008-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.3329",
        "title": "Universal Intelligence: A Definition of Machine Intelligence",
        "authors": [
            "Shane Legg",
            "Marcus Hutter"
        ],
        "abstract": "  A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: We take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this equation formally captures the concept of machine intelligence in the broadest reasonable sense. We then show how this formal definition is related to the theory of universal optimal learning agents. Finally, we survey the many other tests and definitions of intelligence that have been proposed for machines.\n    ",
        "submission_date": "2007-12-20T00:00:00",
        "last_modified_date": "2007-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.3825",
        "title": "Tests of Machine Intelligence",
        "authors": [
            "Shane Legg",
            "Marcus Hutter"
        ],
        "abstract": "  Although the definition and measurement of intelligence is clearly of fundamental importance to the field of artificial intelligence, no general survey of definitions and tests of machine intelligence exists. Indeed few researchers are even aware of alternatives to the Turing test and its many derivatives. In this paper we fill this gap by providing a short survey of the many tests of machine intelligence that have been proposed.\n    ",
        "submission_date": "2007-12-22T00:00:00",
        "last_modified_date": "2007-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.4126",
        "title": "TRUST-TECH based Methods for Optimization and Learning",
        "authors": [
            "Chandan K. Reddy"
        ],
        "abstract": "  Many problems that arise in machine learning domain deal with nonlinearity and quite often demand users to obtain global optimal solutions rather than local optimal ones. Optimization problems are inherent in machine learning algorithms and hence many methods in machine learning were inherited from the optimization literature. Popularly known as the initialization problem, the ideal set of parameters required will significantly depend on the given initialization values. The recently developed TRUST-TECH (TRansformation Under STability-reTaining Equilibria CHaracterization) methodology systematically explores the subspace of the parameters to obtain a complete set of local optimal solutions. In this thesis work, we propose TRUST-TECH based methods for solving several optimization and machine learning problems. Two stages namely, the local stage and the neighborhood-search stage, are repeated alternatively in the solution space to achieve improvements in the quality of the solutions. Our methods were tested on both synthetic and real datasets and the advantages of using this novel framework are clearly manifested. This framework not only reduces the sensitivity to initialization, but also allows the flexibility for the practitioners to use various global and local methods that work well for a particular problem of interest. Other hierarchical stochastic algorithms like evolutionary algorithms and smoothing algorithms are also studied and frameworks for combining these methods with TRUST-TECH have been proposed and evaluated on several test systems.\n    ",
        "submission_date": "2007-12-25T00:00:00",
        "last_modified_date": "2007-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.4318",
        "title": "Convergence of Expected Utilities with Algorithmic Probability Distributions",
        "authors": [
            "Peter de Blanc"
        ],
        "abstract": "  We consider an agent interacting with an unknown environment. The environment is a function which maps natural numbers to natural numbers; the agent's set of hypotheses about the environment contains all such functions which are computable and compatible with a finite set of known input-output pairs, and the agent assigns a positive probability to each such hypothesis. We do not require that this probability distribution be computable, but it must be bounded below by a positive computable function. The agent has a utility function on outputs from the environment. We show that if this utility function is bounded below in absolute value by an unbounded computable function, then the expected utility of any input is undefined. This implies that a computable utility function will have convergent expected utilities iff that function is bounded.\n    ",
        "submission_date": "2007-12-28T00:00:00",
        "last_modified_date": "2007-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0701013",
        "title": "Attribute Value Weighting in K-Modes Clustering",
        "authors": [
            "Zengyou He",
            "Xaiofei Xu",
            "Shengchun Deng"
        ],
        "abstract": "  In this paper, the traditional k-modes clustering algorithm is extended by weighting attribute value matches in dissimilarity computation. The use of attribute value weighting technique makes it possible to generate clusters with stronger intra-similarities, and therefore achieve better clustering performance. Experimental results on real life datasets show that these value weighting based k-modes algorithms are superior to the standard k-modes algorithm with respect to clustering accuracy.\n    ",
        "submission_date": "2007-01-03T00:00:00",
        "last_modified_date": "2007-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0701095",
        "title": "Propositional theories are strongly equivalent to logic programs",
        "authors": [
            "Pedro Cabalar",
            "Paolo Ferraris"
        ],
        "abstract": "  This paper presents a property of propositional theories under the answer sets semantics (called Equilibrium Logic for this general syntax): any theory can always be reexpressed as a strongly equivalent disjunctive logic program, possibly with negation in the head. We provide two different proofs for this result: one involving a syntactic transformation, and one that constructs a program starting from the countermodels of the theory in the intermediate logic of here-and-there.\n    ",
        "submission_date": "2007-01-16T00:00:00",
        "last_modified_date": "2007-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0701125",
        "title": "Universal Algorithmic Intelligence: A mathematical top->down approach",
        "authors": [
            "Marcus Hutter"
        ],
        "abstract": "  Sequential decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known. Solomonoff's theory of universal induction formally solves the problem of sequence prediction for unknown prior distribution. We combine both ideas and get a parameter-free theory of universal Artificial Intelligence. We give strong arguments that the resulting AIXI model is the most intelligent unbiased agent possible. We outline how the AIXI model can formally solve a number of problem classes, including sequence prediction, strategic games, function minimization, reinforcement and supervised learning. The major drawback of the AIXI model is that it is uncomputable. To overcome this problem, we construct a modified algorithm AIXItl that is still effectively more intelligent than any other time t and length l bounded agent. The computation time of AIXItl is of the order t x 2^l. The discussion includes formal definitions of intelligence order relations, the horizon problem and relations of the AIXI theory to other AI approaches.\n    ",
        "submission_date": "2007-01-20T00:00:00",
        "last_modified_date": "2007-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0701174",
        "title": "A Prototype for Educational Planning Using Course Constraints to Simulate Student Populations",
        "authors": [
            "T. Hadzilacos",
            "D. Kalles",
            "D. Koumanakos",
            "V. Mitsionis"
        ],
        "abstract": "  Distance learning universities usually afford their students the flexibility to advance their studies at their own pace. This can lead to a considerable fluctuation of student populations within a program's courses, possibly affecting the academic viability of a program as well as the related required resources. Providing a method that estimates this population could be of substantial help to university management and academic personnel. We describe how to use course precedence constraints to calculate alternative tuition paths and then use Markov models to estimate future populations. In doing so, we identify key issues of a large scale potential deployment.\n    ",
        "submission_date": "2007-01-26T00:00:00",
        "last_modified_date": "2009-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0701184",
        "title": "Structure and Problem Hardness: Goal Asymmetry and DPLL Proofs in<br> SAT-Based Planning",
        "authors": [
            "Joerg Hoffmann",
            "Carla Gomes",
            "Bart Selman"
        ],
        "abstract": "  In Verification and in (optimal) AI Planning, a successful method is to formulate the application as boolean satisfiability (SAT), and solve it with state-of-the-art DPLL-based procedures. There is a lack of understanding of why this works so well. Focussing on the Planning context, we identify a form of problem structure concerned with the symmetrical or asymmetrical nature of the cost of achieving the individual planning goals. We quantify this sort of structure with a simple numeric parameter called AsymRatio, ranging between 0 and 1. We run experiments in 10 benchmark domains from the International Planning Competitions since 2000; we show that AsymRatio is a good indicator of SAT solver performance in 8 of these domains. We then examine carefully crafted synthetic planning domains that allow control of the amount of structure, and that are clean enough for a rigorous analysis of the combinatorial search space. The domains are parameterized by size, and by the amount of structure. The CNFs we examine are unsatisfiable, encoding one planning step less than the length of the optimal plan. We prove upper and lower bounds on the size of the best possible DPLL refutations, under different settings of the amount of structure, as a function of size. We also identify the best possible sets of branching variables (backdoors). With minimum AsymRatio, we prove exponential lower bounds, and identify minimal backdoors of size linear in the number of variables. With maximum AsymRatio, we identify logarithmic DPLL refutations (and backdoors), showing a doubly exponential gap between the two structural extreme cases. The reasons for this behavior -- the proof arguments -- illuminate the prototypical patterns of structure causing the empirical behavior observed in the competition benchmarks.\n    ",
        "submission_date": "2007-01-29T00:00:00",
        "last_modified_date": "2007-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0702028",
        "title": "Uniform and Partially Uniform Redistribution Rules",
        "authors": [
            "Florentin Smarandache",
            "Jean Dezert"
        ],
        "abstract": "This short paper introduces two new fusion rules for combining quantitative basic belief assignments. These rules although very simple have not been proposed in literature so far and could serve as useful alternatives because of their low computation cost with respect to the recent advanced Proportional Conflict Redistribution rules developed in the DSmT framework.\n    ",
        "submission_date": "2007-02-05T00:00:00",
        "last_modified_date": "2011-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0702170",
        "title": "Generic Global Constraints based on MDDs",
        "authors": [
            "Peter Tiedemann",
            "Henrik Reif Andersen",
            "Rasmus Pagh"
        ],
        "abstract": "  Constraint Programming (CP) has been successfully applied to both constraint satisfaction and constraint optimization problems. A wide variety of specialized global constraints provide critical assistance in achieving a good model that can take advantage of the structure of the problem in the search for a solution. However, a key outstanding issue is the representation of 'ad-hoc' constraints that do not have an inherent combinatorial nature, and hence are not modeled well using narrowly specialized global constraints. We attempt to address this issue by considering a hybrid of search and compilation. Specifically we suggest the use of Reduced Ordered Multi-Valued Decision Diagrams (ROMDDs) as the supporting data structure for a generic global constraint. We give an algorithm for maintaining generalized arc consistency (GAC) on this constraint that amortizes the cost of the GAC computation over a root-to-leaf path in the search tree without requiring asymptotically more space than used for the MDD. Furthermore we present an approach for incrementally maintaining the reduced property of the MDD during the search, and show how this can be used for providing domain entailment detection. Finally we discuss how to apply our approach to other similar data structures such as AOMDDs and Case DAGs. The technique used can be seen as an extension of the GAC algorithm for the regular language constraint on finite length input.\n    ",
        "submission_date": "2007-02-28T00:00:00",
        "last_modified_date": "2007-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0703060",
        "title": "Redesigning Decision Matrix Method with an indeterminacy-based inference process",
        "authors": [
            "Jose L. Salmeron",
            "Florentin Smarandache"
        ],
        "abstract": "  For academics and practitioners concerned with computers, business and mathematics, one central issue is supporting decision makers. In this paper, we propose a generalization of Decision Matrix Method (DMM), using Neutrosophic logic. It emerges as an alternative to the existing logics and it represents a mathematical model of uncertainty and indeterminacy. This paper proposes the Neutrosophic Decision Matrix Method as a more realistic tool for decision making. In addition, a de-neutrosophication process is included.\n    ",
        "submission_date": "2007-03-13T00:00:00",
        "last_modified_date": "2007-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0703091",
        "title": "Multimodal Meaning Representation for Generic Dialogue Systems Architectures",
        "authors": [
            "Fr\u00e9d\u00e9ric Landragin",
            "Alexandre Denis",
            "Annalisa Ricci",
            "Laurent Romary"
        ],
        "abstract": "  An unified language for the communicative acts between agents is essential for the design of multi-agents architectures. Whatever the type of interaction (linguistic, multimodal, including particular aspects such as force feedback), whatever the type of application (command dialogue, request dialogue, database querying), the concepts are common and we need a generic meta-model. In order to tend towards task-independent systems, we need to clarify the modules parameterization procedures. In this paper, we focus on the characteristics of a meta-model designed to represent meaning in linguistic and multimodal applications. This meta-model is called MMIL for MultiModal Interface Language, and has first been specified in the framework of the IST MIAMM European project. What we want to test here is how relevant is MMIL for a completely different context (a different task, a different interaction type, a different linguistic domain). We detail the exploitation of MMIL in the framework of the IST OZONE European project, and we draw the conclusions on the role of MMIL in the parameterization of task-independent dialogue managers.\n    ",
        "submission_date": "2007-03-16T00:00:00",
        "last_modified_date": "2007-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0703124",
        "title": "Modelling Complexity in Musical Rhythm",
        "authors": [
            "Cheng-Yuan Liou",
            "Tai-Hei Wu",
            "Chia-Ying Lee"
        ],
        "abstract": "  This paper constructs a tree structure for the music rhythm using the L-system. It models the structure as an automata and derives its complexity. It also solves the complexity for the L-system. This complexity can resolve the similarity between trees. This complexity serves as a measure of psychological complexity for rhythms. It resolves the music complexity of various compositions including the Mozart effect K488.\n",
        "submission_date": "2007-03-26T00:00:00",
        "last_modified_date": "2007-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0703130",
        "title": "Space-contained conflict revision, for geographic information",
        "authors": [
            "Omar Doukari",
            "Robert Jeansoulin"
        ],
        "abstract": "  Using qualitative reasoning with geographic information, contrarily, for instance, with robotics, looks not only fastidious (i.e.: encoding knowledge Propositional Logics PL), but appears to be computational complex, and not tractable at all, most of the time. However, knowledge fusion or revision, is a common operation performed when users merge several different data sets in a unique decision making process, without much support. Introducing logics would be a great improvement, and we propose in this paper, means for deciding -a priori- if one application can benefit from a complete revision, under only the assumption of a conjecture that we name the \"containment conjecture\", which limits the size of the minimal conflicts to revise. We demonstrate that this conjecture brings us the interesting computational property of performing a not-provable but global, revision, made of many local revisions, at a tractable size. We illustrate this approach on an application.\n    ",
        "submission_date": "2007-03-26T00:00:00",
        "last_modified_date": "2007-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0703156",
        "title": "Case Base Mining for Adaptation Knowledge Acquisition",
        "authors": [
            "Mathieu D'Aquin",
            "Fadi Badra",
            "Sandrine Lafrogne",
            "Jean Lieber",
            "Amedeo Napoli",
            "Laszlo Szathmary"
        ],
        "abstract": "  In case-based reasoning, the adaptation of a source case in order to solve the target problem is at the same time crucial and difficult to implement. The reason for this difficulty is that, in general, adaptation strongly depends on domain-dependent knowledge. This fact motivates research on adaptation knowledge acquisition (AKA). This paper presents an approach to AKA based on the principles and techniques of knowledge discovery from databases and data-mining. It is implemented in CABAMAKA, a system that explores the variations within the case base to elicit adaptation knowledge. This system has been successfully tested in an application of case-based reasoning to decision support in the domain of breast cancer treatment.\n    ",
        "submission_date": "2007-03-30T00:00:00",
        "last_modified_date": "2007-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0704.0047",
        "title": "Intelligent location of simultaneously active acoustic emission sources: Part I",
        "authors": [
            "T. Kosel",
            "I. Grabec"
        ],
        "abstract": "  The intelligent acoustic emission locator is described in Part I, while Part II discusses blind source separation, time delay estimation and location of two simultaneously active continuous acoustic emission sources.\n",
        "submission_date": "2007-04-01T00:00:00",
        "last_modified_date": "2007-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0704.0050",
        "title": "Intelligent location of simultaneously active acoustic emission sources: Part II",
        "authors": [
            "T. Kosel",
            "I. Grabec"
        ],
        "abstract": "  Part I describes an intelligent acoustic emission locator, while Part II discusses blind source separation, time delay estimation and location of two continuous acoustic emission sources.\n",
        "submission_date": "2007-04-01T00:00:00",
        "last_modified_date": "2007-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0704.0304",
        "title": "The World as Evolving Information",
        "authors": [
            "Carlos Gershenson"
        ],
        "abstract": "This paper discusses the benefits of describing the world as information, especially in the study of the evolution of life and cognition. Traditional studies encounter problems because it is difficult to describe life and cognition in terms of matter and energy, since their laws are valid only at the physical scale. However, if matter and energy, as well as life and cognition, are described in terms of information, evolution can be described consistently as information becoming more complex.\n",
        "submission_date": "2007-04-03T00:00:00",
        "last_modified_date": "2010-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0704.0985",
        "title": "Architecture for Pseudo Acausal Evolvable Embedded Systems",
        "authors": [
            "Mohd Abubakr",
            "R.M.Vinay"
        ],
        "abstract": "  Advances in semiconductor technology are contributing to the increasing complexity in the design of embedded systems. Architectures with novel techniques such as evolvable nature and autonomous behavior have engrossed lot of attention. This paper demonstrates conceptually evolvable embedded systems can be characterized basing on acausal nature. It is noted that in acausal systems, future input needs to be known, here we make a mechanism such that the system predicts the future inputs and exhibits pseudo acausal nature. An embedded system that uses theoretical framework of acausality is proposed. Our method aims at a novel architecture that features the hardware evolability and autonomous behavior alongside pseudo acausality. Various aspects of this architecture are discussed in detail along with the limitations.\n    ",
        "submission_date": "2007-04-07T00:00:00",
        "last_modified_date": "2007-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0704.1028",
        "title": "A neural network approach to ordinal regression",
        "authors": [
            "Jianlin Cheng"
        ],
        "abstract": "  Ordinal regression is an important type of learning, which has properties of both classification and regression. Here we describe a simple and effective approach to adapt a traditional neural network to learn ordinal categories. Our approach is a generalization of the perceptron method for ordinal regression. On several benchmark datasets, our method (NNRank) outperforms a neural network classification method. Compared with the ordinal regression methods using Gaussian processes and support vector machines, NNRank achieves comparable performance. Moreover, NNRank has the advantages of traditional neural networks: learning in both online and batch modes, handling very large training datasets, and making rapid predictions. These features make NNRank a useful and complementary tool for large-scale data processing tasks such as information retrieval, web page ranking, collaborative filtering, and protein ranking in Bioinformatics.\n    ",
        "submission_date": "2007-04-08T00:00:00",
        "last_modified_date": "2007-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0704.1409",
        "title": "Preconditioned Temporal Difference Learning",
        "authors": [
            "Yao HengShuai"
        ],
        "abstract": "  This paper has been withdrawn by the author. This draft is withdrawn for its poor quality in english, unfortunately produced by the author when he was just starting his science route. Look at the ICML version instead: ",
        "submission_date": "2007-04-11T00:00:00",
        "last_modified_date": "2012-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0704.1676",
        "title": "Personalizing Image Search Results on Flickr",
        "authors": [
            "Kristina Lerman",
            "Anon Plangprasopchok",
            "Chio Wong"
        ],
        "abstract": "  The social media site Flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. Flickr offers multiple ways of browsing or searching it. One option is tag search, which returns all images tagged with a specific keyword. If the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. We claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. We show how to exploit this metadata to personalize search results for the user, thereby improving search performance. First, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. Secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. The users' interests can similarly be described by the tags they used for annotating their images. The latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.\n    ",
        "submission_date": "2007-04-12T00:00:00",
        "last_modified_date": "2007-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0704.1783",
        "title": "Unicast and Multicast Qos Routing with Soft Constraint Logic Programming",
        "authors": [
            "Stefano Bistarelli",
            "Ugo Montanari",
            "Francesca Rossi",
            "Francesco Santini"
        ],
        "abstract": "  We present a formal model to represent and solve the unicast/multicast routing problem in networks with Quality of Service (QoS) requirements. To attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different QoS metric value (e.g. bandwidth, cost, delay, packet loss). The second step consists in writing this graph as a program in Soft Constraint Logic Programming (SCLP): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to QoS routing problems. Moreover, c-semiring structures are a convenient tool to model QoS metrics. At last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.\n    ",
        "submission_date": "2007-04-13T00:00:00",
        "last_modified_date": "2008-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0704.2083",
        "title": "Introduction to Arabic Speech Recognition Using CMUSphinx System",
        "authors": [
            "H. Satori",
            "M. Harti",
            "N. Chenfour"
        ],
        "abstract": "  In this paper Arabic was investigated from the speech recognition problem point of view. We propose a novel approach to build an Arabic Automated Speech Recognition System (ASR). This system is based on the open source CMU Sphinx-4, from the Carnegie Mellon University. CMU Sphinx is a large-vocabulary; speaker-independent, continuous speech recognition system based on discrete Hidden Markov Models (HMMs). We build a model using utilities from the OpenSource CMU Sphinx. We will demonstrate the possible adaptability of this system to Arabic voice recognition.\n    ",
        "submission_date": "2007-04-17T00:00:00",
        "last_modified_date": "2007-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0704.2201",
        "title": "Arabic Speech Recognition System using CMU-Sphinx4",
        "authors": [
            "H. Satori",
            "M. Harti",
            "N. Chenfour"
        ],
        "abstract": "  In this paper we present the creation of an Arabic version of Automated Speech Recognition System (ASR). This system is based on the open source Sphinx-4, from the Carnegie Mellon University. Which is a speech recognition system based on discrete hidden Markov models (HMMs). We investigate the changes that must be made to the model to adapt Arabic voice recognition.\n",
        "submission_date": "2007-04-17T00:00:00",
        "last_modified_date": "2007-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0704.3359",
        "title": "Direct Optimization of Ranking Measures",
        "authors": [
            "Quoc Le",
            "Alexander Smola"
        ],
        "abstract": "  Web page ranking and collaborative filtering require the optimization of sophisticated performance measures. Current Support Vector approaches are unable to optimize them directly and focus on pairwise comparisons instead. We present a new approach which allows direct optimization of the relevant loss functions. This is achieved via structured estimation in Hilbert spaces. It is most related to Max-Margin-Markov networks optimization of multivariate performance measures. Key to our approach is that during training the ranking problem can be viewed as a linear assignment problem, which can be solved by the Hungarian Marriage algorithm. At test time, a sort operation is sufficient, as our algorithm assigns a relevance score to every (document, query) pair. Experiments show that the our algorithm is fast and that it works very well.\n    ",
        "submission_date": "2007-04-25T00:00:00",
        "last_modified_date": "2007-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.0025",
        "title": "Can the Internet cope with stress?",
        "authors": [
            "Andreas Martin Lisewski"
        ],
        "abstract": "  When will the Internet become aware of itself? In this note the problem is approached by asking an alternative question: Can the Internet cope with stress? By extrapolating the psychological difference between coping and defense mechanisms a distributed software experiment is outlined which could reject the hypothesis that the Internet is not a conscious entity.\n    ",
        "submission_date": "2007-05-01T00:00:00",
        "last_modified_date": "2007-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.0199",
        "title": "The Parameter-Less Self-Organizing Map algorithm",
        "authors": [
            "Erik Berglund",
            "Joaquin Sitte"
        ],
        "abstract": "  The Parameter-Less Self-Organizing Map (PLSOM) is a new neural network algorithm based on the Self-Organizing Map (SOM). It eliminates the need for a learning rate and annealing schemes for learning rate and neighbourhood size. We discuss the relative performance of the PLSOM and the SOM and demonstrate some tasks in which the SOM fails but the PLSOM performs satisfactory. Finally we discuss some example applications of the PLSOM and present a proof of ordering under certain limited conditions.\n    ",
        "submission_date": "2007-05-02T00:00:00",
        "last_modified_date": "2007-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.0760",
        "title": "Equivalence of LP Relaxation and Max-Product for Weighted Matching in General Graphs",
        "authors": [
            "Sujay Sanghavi"
        ],
        "abstract": "  Max-product belief propagation is a local, iterative algorithm to find the mode/MAP estimate of a probability distribution. While it has been successfully employed in a wide variety of applications, there are relatively few theoretical guarantees of convergence and correctness for general loopy graphs that may have many short cycles. Of these, even fewer provide exact ``necessary and sufficient'' characterizations.\n",
        "submission_date": "2007-05-05T00:00:00",
        "last_modified_date": "2007-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.1617",
        "title": "Non-Computability of Consciousness",
        "authors": [
            "Daegene Song"
        ],
        "abstract": "  With the great success in simulating many intelligent behaviors using computing devices, there has been an ongoing debate whether all conscious activities are computational processes. In this paper, the answer to this question is shown to be no. A certain phenomenon of consciousness is demonstrated to be fully represented as a computational process using a quantum computer. Based on the computability criterion discussed with Turing machines, the model constructed is shown to necessarily involve a non-computable element. The concept that this is solely a quantum effect and does not work for a classical case is also discussed.\n    ",
        "submission_date": "2007-05-11T00:00:00",
        "last_modified_date": "2007-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.1673",
        "title": "Using artificial intelligence for data reduction in mechanical engineering",
        "authors": [
            "L. Mdlazi",
            "C.J. Stander",
            "P.S. Heyns",
            "T. Marwala"
        ],
        "abstract": "  In this paper artificial neural networks and support vector machines are used to reduce the amount of vibration data that is required to estimate the Time Domain Average of a gear vibration signal. Two models for estimating the time domain average of a gear vibration signal are proposed. The models are tested on data from an accelerated gear life test rig. Experimental results indicate that the required data for calculating the Time Domain Average of a gear vibration signal can be reduced by up to 75% when the proposed models are implemented.\n    ",
        "submission_date": "2007-05-11T00:00:00",
        "last_modified_date": "2007-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.2307",
        "title": "A Study in a Hybrid Centralised-Swarm Agent Community",
        "authors": [
            "Bradley van Aardt",
            "Tshilidzi Marwala"
        ],
        "abstract": "  This paper describes a systems architecture for a hybrid Centralised/Swarm based multi-agent system. The issue of local goal assignment for agents is investigated through the use of a global agent which teaches the agents responses to given situations. We implement a test problem in the form of a Pursuit game, where the Multi-Agent system is a set of captor agents. The agents learn solutions to certain board positions from the global agent if they are unable to find a solution. The captor agents learn through the use of multi-layer perceptron neural networks. The global agent is able to solve board positions through the use of a Genetic Algorithm. The cooperation between agents and the results of the simulation are discussed here. .\n    ",
        "submission_date": "2007-05-16T00:00:00",
        "last_modified_date": "2007-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.2485",
        "title": "Using Genetic Algorithms to Optimise Rough Set Partition Sizes for HIV Data Analysis",
        "authors": [
            "Bodie Crossingham",
            "Tshilidzi Marwala"
        ],
        "abstract": "  In this paper, we present a method to optimise rough set partition sizes, to which rule extraction is performed on HIV data. The genetic algorithm optimisation technique is used to determine the partition sizes of a rough set in order to maximise the rough sets prediction accuracy. The proposed method is tested on a set of demographic properties of individuals obtained from the South African antenatal survey. Six demographic variables were used in the analysis, these variables are; race, age of mother, education, gravidity, parity, and age of father, with the outcome or decision being either HIV positive or negative. Rough set theory is chosen based on the fact that it is easy to interpret the extracted rules. The prediction accuracy of equal width bin partitioning is 57.7% while the accuracy achieved after optimising the partitions is 72.8%. Several other methods have been used to analyse the HIV data and their results are stated and compared to that of rough set theory (RST).\n    ",
        "submission_date": "2007-05-17T00:00:00",
        "last_modified_date": "2007-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.2516",
        "title": "Condition Monitoring of HV Bushings in the Presence of Missing Data Using Evolutionary Computing",
        "authors": [
            "Sizwe M. Dhlamini*",
            "Fulufhelo V. Nelwamondo**",
            "Tshilidzi Marwala**"
        ],
        "abstract": "  The work proposes the application of neural networks with particle swarm optimisation (PSO) and genetic algorithms (GA) to compensate for missing data in classifying high voltage bushings. The classification is done using DGA data from 60966 bushings based on IEEEc57.104, IEC599 and IEEE production rates methods for oil impregnated paper (OIP) bushings. PSO and GA were compared in terms of accuracy and computational efficiency. Both GA and PSO simulations were able to estimate missing data values to an average 95% accuracy when only one variable was missing. However PSO rapidly deteriorated to 66% accuracy with two variables missing simultaneously, compared to 84% for GA. The data estimated using GA was found to classify the conditions of bushings than the PSO.\n    ",
        "submission_date": "2007-05-17T00:00:00",
        "last_modified_date": "2007-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.2765",
        "title": "On the monotonization of the training set",
        "authors": [
            "Rustem Takhanov"
        ],
        "abstract": "  We consider the problem of minimal correction of the training set to make it consistent with monotonic constraints. This problem arises during analysis of data sets via techniques that require monotone data. We show that this problem is NP-hard in general and is equivalent to finding a maximal independent set in special orgraphs. Practically important cases of that problem considered in detail. These are the cases when a partial order given on the replies set is a total order or has a dimension 2. We show that the second case can be reduced to maximization of a quadratic convex function on a convex set. For this case we construct an approximate polynomial algorithm based on convex optimization.\n    ",
        "submission_date": "2007-05-18T00:00:00",
        "last_modified_date": "2007-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.3561",
        "title": "Generalizing Consistency and other Constraint Properties to Quantified Constraints",
        "authors": [
            "Lucas Bordeaux",
            "Marco Cadoli",
            "Toni Mancini"
        ],
        "abstract": "  Quantified constraints and Quantified Boolean Formulae are typically much more difficult to reason with than classical constraints, because quantifier alternation makes the usual notion of solution inappropriate. As a consequence, basic properties of Constraint Satisfaction Problems (CSP), such as consistency or substitutability, are not completely understood in the quantified case. These properties are important because they are the basis of most of the reasoning methods used to solve classical (existentially quantified) constraints, and one would like to benefit from similar reasoning methods in the resolution of quantified constraints. In this paper, we show that most of the properties that are used by solvers for CSP can be generalized to quantified CSP. This requires a re-thinking of a number of basic concepts; in particular, we propose a notion of outcome that generalizes the classical notion of solution and on which all definitions are based. We propose a systematic study of the relations which hold between these properties, as well as complexity results regarding the decision of these properties. Finally, and since these problems are typically intractable, we generalize the approach used in CSP and propose weaker, easier to check notions based on locality, which allow to detect these properties incompletely but in polynomial time.\n    ",
        "submission_date": "2007-05-24T00:00:00",
        "last_modified_date": "2007-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.3766",
        "title": "On complexity of optimized crossover for binary representations",
        "authors": [
            "Anton Eremeev"
        ],
        "abstract": "  We consider the computational complexity of producing the best possible offspring in a crossover, given two solutions of the parents. The crossover operators are studied on the class of Boolean linear programming problems, where the Boolean vector of variables is used as the solution representation. By means of efficient reductions of the optimized gene transmitting crossover problems (OGTC) we show the polynomial solvability of the OGTC for the maximum weight set packing problem, the minimum weight set partition problem and for one of the versions of the simple plant location problem. We study a connection between the OGTC for linear Boolean programming problem and the maximum weight independent set problem on 2-colorable hypergraph and prove the NP-hardness of several special cases of the OGTC problem in Boolean linear programming.\n    ",
        "submission_date": "2007-05-25T00:00:00",
        "last_modified_date": "2007-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0705.4584",
        "title": "Modeling Epidemic Spread in Synthetic Populations - Virtual Plagues in Massively Multiplayer Online Games",
        "authors": [
            "Magnus Boman",
            "Stefan J. Johansson"
        ],
        "abstract": "  A virtual plague is a process in which a behavior-affecting property spreads among characters in a Massively Multiplayer Online Game (MMOG). The MMOG individuals constitute a synthetic population, and the game can be seen as a form of interactive executable model for studying disease spread, albeit of a very special kind. To a game developer maintaining an MMOG, recognizing, monitoring, and ultimately controlling a virtual plague is important, regardless of how it was initiated. The prospect of using tools, methods and theory from the field of epidemiology to do this seems natural and appealing. We will address the feasibility of such a prospect, first by considering some basic measures used in epidemiology, then by pointing out the differences between real world epidemics and virtual plagues. We also suggest directions for MMOG developer control through epidemiological modeling. Our aim is understanding the properties of virtual plagues, rather than trying to eliminate them or mitigate their effects, as would be in the case of real infectious disease.\n    ",
        "submission_date": "2007-05-31T00:00:00",
        "last_modified_date": "2007-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0706.0585",
        "title": "A Novel Model of Working Set Selection for SMO Decomposition Methods",
        "authors": [
            "Zhendong Zhao",
            "Lei Yuan",
            "Yuxuan Wang",
            "Forrest Sheng Bao",
            "Shunyi Zhang Yanfei Sun"
        ],
        "abstract": "  In the process of training Support Vector Machines (SVMs) by decomposition methods, working set selection is an important technique, and some exciting schemes were employed into this field. To improve working set selection, we propose a new model for working set selection in sequential minimal optimization (SMO) decomposition methods. In this model, it selects B as working set without reselection. Some properties are given by simple proof, and experiments demonstrate that the proposed method is in general faster than existing methods.\n    ",
        "submission_date": "2007-06-05T00:00:00",
        "last_modified_date": "2007-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0706.1001",
        "title": "Epistemic Analysis of Strategic Games with Arbitrary Strategy Sets",
        "authors": [
            "Krzysztof R. Apt"
        ],
        "abstract": "  We provide here an epistemic analysis of arbitrary strategic games based on the possibility correspondences. Such an analysis calls for the use of transfinite iterations of the corresponding operators. Our approach is based on Tarski's Fixpoint Theorem and applies both to the notions of rationalizability and the iterated elimination of strictly dominated strategies.\n    ",
        "submission_date": "2007-06-07T00:00:00",
        "last_modified_date": "2007-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0706.4323",
        "title": "Theory of Finite or Infinite Trees Revisited",
        "authors": [
            "Khalil Djelloul",
            "Thi-bich-hanh Dao",
            "Thom Fruehwirth"
        ],
        "abstract": "  We present in this paper a first-order axiomatization of an extended theory $T$ of finite or infinite trees, built on a signature containing an infinite set of function symbols and a relation $\\fini(t)$ which enables to distinguish between finite or infinite trees. We show that $T$ has at least one model and prove its completeness by giving not only a decision procedure, but a full first-order constraint solver which gives clear and explicit solutions for any first-order constraint satisfaction problem in $T$. The solver is given in the form of 16 rewriting rules which transform any first-order constraint $\\phi$ into an equivalent disjunction $\\phi$ of simple formulas such that $\\phi$ is either the formula $\\true$ or the formula $\\false$ or a formula having at least one free variable, being equivalent neither to $\\true$ nor to $\\false$ and where the solutions of the free variables are expressed in a clear and explicit way. The correctness of our rules implies the completeness of $T$. We also describe an implementation of our algorithm in CHR (Constraint Handling Rules) and compare the performance with an implementation in C++ and that of a recent decision procedure for decomposable theories.\n    ",
        "submission_date": "2007-06-28T00:00:00",
        "last_modified_date": "2007-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0707.0498",
        "title": "The Role of Time in the Creation of Knowledge",
        "authors": [
            "Roy E. Murphy"
        ],
        "abstract": "  This paper I assume that in humans the creation of knowledge depends on a discrete time, or stage, sequential decision-making process subjected to a stochastic, information transmitting environment. For each time-stage, this environment randomly transmits Shannon type information-packets to the decision-maker, who examines each of them for relevancy and then determines his optimal choices. Using this set of relevant information-packets, the decision-maker adapts, over time, to the stochastic nature of his environment, and optimizes the subjective expected rate-of-growth of knowledge. The decision-maker's optimal actions, lead to a decision function that involves, over time, his view of the subjective entropy of the environmental process and other important parameters at each time-stage of the process. Using this model of human behavior, one could create psychometric experiments using computer simulation and real decision-makers, to play programmed games to measure the resulting human performance.\n    ",
        "submission_date": "2007-07-03T00:00:00",
        "last_modified_date": "2007-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0707.0808",
        "title": "The Cyborg Astrobiologist: Porting from a wearable computer to the Astrobiology Phone-cam",
        "authors": [
            "Alexandra Bartolo",
            "Patrick C. McGuire",
            "Kenneth P. Camilleri",
            "Christopher Spiteri",
            "Jonathan C. Borg",
            "Philip J. Farrugia",
            "Jens Ormo",
            "Javier Gomez-Elvira",
            "Jose Antonio Rodriguez-Manfredi",
            "Enrique Diaz-Martinez",
            "Helge Ritter",
            "Robert Haschke",
            "Markus Oesker",
            "Joerg Ontrup"
        ],
        "abstract": "  We have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. This camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. We envision that the `Astrobiology Phone-cam' exploration system can be fruitfully used in other problem domains as well.\n    ",
        "submission_date": "2007-07-05T00:00:00",
        "last_modified_date": "2007-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0707.3030",
        "title": "Optimal Design of Ad Hoc Injection Networks by Using Genetic Algorithms",
        "authors": [
            "Gregoire Danoy",
            "Pascal Bouvry",
            "Matthias R. Brust",
            "Enrique Alba"
        ],
        "abstract": "  This work aims at optimizing injection networks, which consist in adding a set of long-range links (called bypass links) in mobile multi-hop ad hoc networks so as to improve connectivity and overcome network partitioning. To this end, we rely on small-world network properties, that comprise a high clustering coefficient and a low characteristic path length. We investigate the use of two genetic algorithms (generational and steady-state) to optimize three instances of this topology control problem and present results that show initial evidence of their capacity to solve it.\n    ",
        "submission_date": "2007-07-20T00:00:00",
        "last_modified_date": "2007-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0707.3205",
        "title": "Neutrality and Many-Valued Logics",
        "authors": [
            "Andrew Schumann",
            "Florentin Smarandache"
        ],
        "abstract": "  In this book, we consider various many-valued logics: standard, linear, hyperbolic, parabolic, non-Archimedean, p-adic, interval, neutrosophic, etc. We survey also results which show the tree different proof-theoretic frameworks for many-valued logics, e.g. frameworks of the following deductive calculi: Hilbert's style, sequent, and hypersequent. We present a general way that allows to construct systematically analytic calculi for a large family of non-Archimedean many-valued logics: hyperrational-valued, hyperreal-valued, and p-adic valued logics characterized by a special format of semantics with an appropriate rejection of Archimedes' axiom. These logics are built as different extensions of standard many-valued logics (namely, Lukasiewicz's, Goedel's, Product, and Post's logics). The informal sense of Archimedes' axiom is that anything can be measured by a ruler. Also logical multiple-validity without Archimedes' axiom consists in that the set of truth values is infinite and it is not well-founded and well-ordered. On the base of non-Archimedean valued logics, we construct non-Archimedean valued interval neutrosophic logic INL by which we can describe neutrality phenomena.\n    ",
        "submission_date": "2007-07-21T00:00:00",
        "last_modified_date": "2007-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0707.3457",
        "title": "A Generalized Information Formula as the Bridge between Shannon and Popper",
        "authors": [
            "Chenguang Lu"
        ],
        "abstract": "  A generalized information formula related to logical probability and fuzzy set is deduced from the classical information formula. The new information measure accords with to Popper's criterion for knowledge evolution very much. In comparison with square error criterion, the information criterion does not only reflect error of a proposition, but also reflects the particularity of the event described by the proposition. It gives a proposition with less logical probability higher evaluation. The paper introduces how to select a prediction or sentence from many for forecasts and language translations according to the generalized information criterion. It also introduces the rate fidelity theory, which comes from the improvement of the rate distortion theory in the classical information theory by replacing distortion (i.e. average error) criterion with the generalized mutual information criterion, for data compression and communication efficiency. Some interesting conclusions are obtained from the rate-fidelity function in relation to image communication. It also discusses how to improve Popper's theory.\n    ",
        "submission_date": "2007-07-24T00:00:00",
        "last_modified_date": "2007-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0707.3559",
        "title": "Practical Approach to Knowledge-based Question Answering with Natural Language Understanding and Advanced Reasoning",
        "authors": [
            "Wilson Wong"
        ],
        "abstract": "  This research hypothesized that a practical approach in the form of a solution framework known as Natural Language Understanding and Reasoning for Intelligence (NaLURI), which combines full-discourse natural language understanding, powerful representation formalism capable of exploiting ontological information and reasoning approach with advanced features, will solve the following problems without compromising practicality factors: 1) restriction on the nature of question and response, and 2) limitation to scale across domains and to real-life natural language text.\n    ",
        "submission_date": "2007-07-24T00:00:00",
        "last_modified_date": "2007-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0707.3972",
        "title": "Learning Probabilistic Models of Word Sense Disambiguation",
        "authors": [
            "Ted Pedersen"
        ],
        "abstract": "  This dissertation presents several new methods of supervised and unsupervised learning of word sense disambiguation models. The supervised methods focus on performing model searches through a space of probabilistic models, and the unsupervised methods rely on the use of Gibbs Sampling and the Expectation Maximization (EM) algorithm. In both the supervised and unsupervised case, the Naive Bayesian model is found to perform well. An explanation for this success is presented in terms of learning rates and bias-variance decompositions.\n    ",
        "submission_date": "2007-07-26T00:00:00",
        "last_modified_date": "2007-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0708.1150",
        "title": "A Practical Ontology for the Large-Scale Modeling of Scholarly Artifacts and their Usage",
        "authors": [
            "Marko A. Rodriguez",
            "Johah Bollen",
            "Herbert Van de Sompel"
        ],
        "abstract": "  The large-scale analysis of scholarly artifact usage is constrained primarily by current practices in usage data archiving, privacy issues concerned with the dissemination of usage data, and the lack of a practical ontology for modeling the usage domain. As a remedy to the third constraint, this article presents a scholarly ontology that was engineered to represent those classes for which large-scale bibliographic and usage data exists, supports usage research, and whose instantiation is scalable to the order of 50 million articles along with their associated artifacts (e.g. authors and journals) and an accompanying 1 billion usage events. The real world instantiation of the presented abstract ontology is a semantic network model of the scholarly community which lends the scholarly process to statistical analysis and computational support. We present the ontology, discuss its instantiation, and provide some example inference rules for calculating various scholarly artifact metrics.\n    ",
        "submission_date": "2007-08-08T00:00:00",
        "last_modified_date": "2007-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0708.1964",
        "title": "Solving the subset-sum problem with a light-based device",
        "authors": [
            "Mihai Oltean",
            "Oana Muntean"
        ],
        "abstract": "  We propose a special computational device which uses light rays for solving the subset-sum problem. The device has a graph-like representation and the light is traversing it by following the routes given by the connections between nodes. The nodes are connected by arcs in a special way which lets us to generate all possible subsets of the given set. To each arc we assign either a number from the given set or a predefined constant. When the light is passing through an arc it is delayed by the amount of time indicated by the number placed in that arc. At the destination node we will check if there is a ray whose total delay is equal to the target value of the subset sum problem (plus some constants).\n    ",
        "submission_date": "2007-08-14T00:00:00",
        "last_modified_date": "2007-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0708.2432",
        "title": "A structure from motion inequality",
        "authors": [
            "Oliver Knill",
            "Jose Ramirez-Herran"
        ],
        "abstract": "  We state an elementary inequality for the structure from motion problem for m cameras and n points. This structure from motion inequality relates space dimension, camera parameter dimension, the number of cameras and number points and global symmetry properties and provides a rigorous criterion for which reconstruction is not possible with probability 1. Mathematically the inequality is based on Frobenius theorem which is a geometric incarnation of the fundamental theorem of linear algebra. The paper also provides a general mathematical formalism for the structure from motion problem. It includes the situation the points can move while the camera takes the pictures.\n    ",
        "submission_date": "2007-08-18T00:00:00",
        "last_modified_date": "2007-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0708.2438",
        "title": "On Ullman's theorem in computer vision",
        "authors": [
            "Oliver Knill",
            "Jose Ramirez-Herran"
        ],
        "abstract": "  Both in the plane and in space, we invert the nonlinear Ullman transformation for 3 points and 3 orthographic cameras. While Ullman's theorem assures a unique reconstruction modulo a reflection for 3 cameras and 4 points, we find a locally unique reconstruction for 3 cameras and 3 points. Explicit reconstruction formulas allow to decide whether picture data of three cameras seeing three points can be realized as a point-camera configuration.\n    ",
        "submission_date": "2007-08-17T00:00:00",
        "last_modified_date": "2007-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0708.2442",
        "title": "Space and camera path reconstruction for omni-directional vision",
        "authors": [
            "Oliver Knill",
            "Jose Ramirez-Herran"
        ],
        "abstract": "  In this paper, we address the inverse problem of reconstructing a scene as well as the camera motion from the image sequence taken by an omni-directional camera. Our structure from motion results give sharp conditions under which the reconstruction is unique. For example, if there are three points in general position and three omni-directional cameras in general position, a unique reconstruction is possible up to a similarity. We then look at the reconstruction problem with m cameras and n points, where n and m can be large and the over-determined system is solved by least square methods. The reconstruction is robust and generalizes to the case of a dynamic environment where landmarks can move during the movie capture. Possible applications of the result are computer assisted scene reconstruction, 3D scanning, autonomous robot navigation, medical tomography and city reconstructions.\n    ",
        "submission_date": "2007-08-17T00:00:00",
        "last_modified_date": "2007-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0709.0178",
        "title": "Effective Generation of Subjectively Random Binary Sequences",
        "authors": [
            "Yasmine B. Sanderson"
        ],
        "abstract": "  We present an algorithm for effectively generating binary sequences which would be rated by people as highly likely to have been generated by a random process, such as flipping a fair coin.\n    ",
        "submission_date": "2007-09-03T00:00:00",
        "last_modified_date": "2008-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0709.1190",
        "title": "Belief-Propagation for Weighted b-Matchings on Arbitrary Graphs and its Relation to Linear Programs with Integer Solutions",
        "authors": [
            "Mohsen Bayati",
            "Christian Borgs",
            "Jennifer Chayes",
            "Riccardo Zecchina"
        ],
        "abstract": "We consider the general problem of finding the minimum weight $\\bm$-matching on arbitrary graphs. We prove that, whenever the linear programming (LP) relaxation of the problem has no fractional solutions, then the belief propagation (BP) algorithm converges to the correct solution. We also show that when the LP relaxation has a fractional solution then the BP algorithm can be used to solve the LP relaxation. Our proof is based on the notion of graph covers and extends the analysis of (Bayati-Shah-Sharma 2005 and Huang-Jebara 2007}.\n",
        "submission_date": "2007-09-08T00:00:00",
        "last_modified_date": "2011-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0709.1699",
        "title": "Efficient Tabling Mechanisms for Transaction Logic Programs",
        "authors": [
            "Paul Fodor"
        ],
        "abstract": "  In this paper we present efficient evaluation algorithms for the Horn Transaction Logic (a generalization of the regular Horn logic programs with state updates). We present two complementary methods for optimizing the implementation of Transaction Logic. The first method is based on tabling and we modified the proof theory to table calls and answers on states (practically, equivalent to dynamic programming). The call-answer table is indexed on the call and a signature of the state in which the call was made. The answer columns contain the answer unification and a signature of the state after the call was executed. The states are signed efficiently using a technique based on tries and counting. The second method is based on incremental evaluation and it applies when the data oracle contains derived relations. The deletions and insertions (executed in the transaction oracle) change the state of the database. Using the heuristic of inertia (only a part of the state changes in response to elementary updates), most of the time it is cheaper to compute only the changes in the state than to recompute the entire state from scratch. The two methods are complementary by the fact that the first method optimizes the evaluation when a call is repeated in the same state, and the second method optimizes the evaluation of a new state when a call-state pair is not found by the tabling mechanism (i.e. the first method). The proof theory of Transaction Logic with the application of tabling and incremental evaluation is sound and complete with respect to its model theory.\n    ",
        "submission_date": "2007-09-11T00:00:00",
        "last_modified_date": "2007-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0709.3965",
        "title": "Evolving Classifiers: Methods for Incremental Learning",
        "authors": [
            "Greg Hulley",
            "Tshilidzi Marwala"
        ],
        "abstract": "  The ability of a classifier to take on new information and classes by evolving the classifier without it having to be fully retrained is known as incremental learning. Incremental learning has been successfully applied to many classification problems, where the data is changing and is not all available at once. In this paper there is a comparison between Learn++, which is one of the most recent incremental learning algorithms, and the new proposed method of Incremental Learning Using Genetic Algorithm (ILUGA). Learn++ has shown good incremental learning capabilities on benchmark datasets on which the new ILUGA method has been tested. ILUGA has also shown good incremental learning ability using only a few classifiers and does not suffer from catastrophic forgetting. The results obtained for ILUGA on the Optical Character Recognition (OCR) and Wine datasets are good, with an overall accuracy of 93% and 94% respectively showing a 4% improvement over Learn++.MT for the difficult multi-class OCR dataset.\n    ",
        "submission_date": "2007-09-25T00:00:00",
        "last_modified_date": "2007-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0709.3967",
        "title": "Classification of Images Using Support Vector Machines",
        "authors": [
            "Gidudu Anthony",
            "Hulley Greg",
            "Marwala Tshilidzi"
        ],
        "abstract": "  Support Vector Machines (SVMs) are a relatively new supervised classification technique to the land cover mapping community. They have their roots in Statistical Learning Theory and have gained prominence because they are robust, accurate and are effective even when using a small training sample. By their nature SVMs are essentially binary classifiers, however, they can be adopted to handle the multiple classification tasks common in remote sensing studies. The two approaches commonly used are the One-Against-One (1A1) and One-Against-All (1AA) techniques. In this paper, these approaches are evaluated in as far as their impact and implication for land cover mapping. The main finding from this research is that whereas the 1AA technique is more predisposed to yielding unclassified and mixed pixels, the resulting classification accuracy is not significantly different from 1A1 approach. It is the authors conclusions that ultimately the choice of technique adopted boils down to personal preference and the uniqueness of the dataset at hand.\n    ",
        "submission_date": "2007-09-25T00:00:00",
        "last_modified_date": "2007-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0709.4655",
        "title": "Mining for trees in a graph is NP-complete",
        "authors": [
            "Jan Van den Bussche"
        ],
        "abstract": "  Mining for trees in a graph is shown to be NP-complete.\n    ",
        "submission_date": "2007-09-28T00:00:00",
        "last_modified_date": "2007-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0710.0009",
        "title": "Bio-linguistic transition and Baldwin effect in an evolutionary naming-game model",
        "authors": [
            "Adam Lipowski",
            "Dorota Lipowska"
        ],
        "abstract": "  We examine an evolutionary naming-game model where communicating agents are equipped with an evolutionarily selected learning ability. Such a coupling of biological and linguistic ingredients results in an abrupt transition: upon a small change of a model control parameter a poorly communicating group of linguistically unskilled agents transforms into almost perfectly communicating group with large learning abilities. When learning ability is kept fixed, the transition appears to be continuous. Genetic imprinting of the learning abilities proceeds via Baldwin effect: initially unskilled communicating agents learn a language and that creates a niche in which there is an evolutionary pressure for the increase of learning ",
        "submission_date": "2007-10-01T00:00:00",
        "last_modified_date": "2007-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0710.0213",
        "title": "Optimising the topology of complex neural networks",
        "authors": [
            "Fei Jiang",
            "Hugues Berry",
            "Marc Schoenauer"
        ],
        "abstract": "  In this paper, we study instances of complex neural networks, i.e. neural netwo rks with complex topologies. We use Self-Organizing Map neural networks whose n eighbourhood relationships are defined by a complex network, to classify handwr itten digits. We show that topology has a small impact on performance and robus tness to neuron failures, at least at long learning times. Performance may howe ver be increased (by almost 10%) by artificial evolution of the network topo logy. In our experimental conditions, the evolved networks are more random than their parents, but display a more heterogeneous degree distribution.\n    ",
        "submission_date": "2007-10-01T00:00:00",
        "last_modified_date": "2007-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0710.1481",
        "title": "What's in a Name?",
        "authors": [
            "Stasinos Konstantopoulos"
        ],
        "abstract": "  This paper describes experiments on identifying the language of a single name in isolation or in a document written in a different language. A new corpus has been compiled and made available, matching names against languages. This corpus is used in a series of experiments measuring the performance of general language models and names-only language models on the language identification task. Conclusions are drawn from the comparison between using general language models and names-only language models and between identifying the language of isolated names and the language of very short document fragments. Future research directions are outlined.\n    ",
        "submission_date": "2007-10-08T00:00:00",
        "last_modified_date": "2007-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0710.1924",
        "title": "A Heuristic Routing Mechanism Using a New Addressing Scheme",
        "authors": [
            "Mohsen Ravanbakhsh",
            "Yasin Abbasi-Yadkori",
            "Maghsoud Abbaspour",
            "Hamid Sarbazi-Azad"
        ],
        "abstract": "  Current methods of routing are based on network information in the form of routing tables, in which routing protocols determine how to update the tables according to the network changes. Despite the variability of data in routing tables, node addresses are constant. In this paper, we first introduce the new concept of variable addresses, which results in a novel framework to cope with routing problems using heuristic solutions. Then we propose a heuristic routing mechanism based on the application of genes for determination of network addresses in a variable address network and describe how this method flexibly solves different problems and induces new ideas in providing integral solutions for variety of problems. The case of ad-hoc networks is where simulation results are more supportive and original solutions have been proposed for issues like mobility.\n    ",
        "submission_date": "2007-10-10T00:00:00",
        "last_modified_date": "2007-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0710.2227",
        "title": "A System for Predicting Subcellular Localization of Yeast Genome Using Neural Network",
        "authors": [
            "Sabu M. Thampi",
            "K. Chandra Sekaran"
        ],
        "abstract": "  The subcellular location of a protein can provide valuable information about its function. With the rapid increase of sequenced genomic data, the need for an automated and accurate tool to predict subcellular localization becomes increasingly important. Many efforts have been made to predict protein subcellular localization. This paper aims to merge the artificial neural networks and bioinformatics to predict the location of protein in yeast genome. We introduce a new subcellular prediction method based on a backpropagation neural network. The results show that the prediction within an error limit of 5 to 10 percentage can be achieved with the system.\n    ",
        "submission_date": "2007-10-11T00:00:00",
        "last_modified_date": "2007-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0710.2446",
        "title": "The structure of verbal sequences analyzed with unsupervised learning techniques",
        "authors": [
            "Catherine Recanati",
            "Nicoleta Rogovschi",
            "Youn\u00e8s Bennani"
        ],
        "abstract": "  Data mining allows the exploration of sequences of phenomena, whereas one usually tends to focus on isolated phenomena or on the relation between two phenomena. It offers invaluable tools for theoretical analyses and exploration of the structure of sentences, texts, dialogues, and speech. We report here the results of an attempt at using it for inspecting sequences of verbs from French accounts of road accidents. This analysis comes from an original approach of unsupervised training allowing the discovery of the structure of sequential data. The entries of the analyzer were only made of the verbs appearing in the sentences. It provided a classification of the links between two successive verbs into four distinct clusters, allowing thus text segmentation. We give here an interpretation of these clusters by applying a statistical analysis to independent semantic annotations.\n    ",
        "submission_date": "2007-10-12T00:00:00",
        "last_modified_date": "2007-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0710.2782",
        "title": "Effective linkage learning using low-order statistics and clustering",
        "authors": [
            "Leonardo Emmendorfer",
            "Aurora Pozo"
        ],
        "abstract": "  The adoption of probabilistic models for the best individuals found so far is a powerful approach for evolutionary computation. Increasingly more complex models have been used by estimation of distribution algorithms (EDAs), which often result better effectiveness on finding the global optima for hard optimization problems. Supervised and unsupervised learning of Bayesian networks are very effective options, since those models are able to capture interactions of high order among the variables of a problem. Diversity preservation, through niching techniques, has also shown to be very important to allow the identification of the problem structure as much as for keeping several global optima. Recently, clustering was evaluated as an effective niching technique for EDAs, but the performance of simpler low-order EDAs was not shown to be much improved by clustering, except for some simple multimodal problems. This work proposes and evaluates a combination operator guided by a measure from information theory which allows a clustered low-order EDA to effectively solve a comprehensive range of benchmark optimization problems.\n    ",
        "submission_date": "2007-10-15T00:00:00",
        "last_modified_date": "2007-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0710.5501",
        "title": "Discriminated Belief Propagation",
        "authors": [
            "Uli Sorger"
        ],
        "abstract": "  Near optimal decoding of good error control codes is generally a difficult task. However, for a certain type of (sufficiently) good codes an efficient decoding algorithm with near optimal performance exists. These codes are defined via a combination of constituent codes with low complexity trellis representations. Their decoding algorithm is an instance of (loopy) belief propagation and is based on an iterative transfer of constituent beliefs. The beliefs are thereby given by the symbol probabilities computed in the constituent trellises. Even though weak constituent codes are employed close to optimal performance is obtained, i.e., the encoder/decoder pair (almost) achieves the information theoretic capacity. However, (loopy) belief propagation only performs well for a rather specific set of codes, which limits its applicability.\n",
        "submission_date": "2007-10-29T00:00:00",
        "last_modified_date": "2007-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0711.1401",
        "title": "Towards a Sound Theory of Adaptation for the Simple Genetic Algorithm",
        "authors": [
            "Keki Burjorjee"
        ],
        "abstract": "  The pace of progress in the fields of Evolutionary Computation and Machine Learning is currently limited -- in the former field, by the improbability of making advantageous extensions to evolutionary algorithms when their capacity for adaptation is poorly understood, and in the latter by the difficulty of finding effective semi-principled reductions of hard real-world problems to relatively simple optimization problems. In this paper we explain why a theory which can accurately explain the simple genetic algorithm's remarkable capacity for adaptation has the potential to address both these limitations. We describe what we believe to be the impediments -- historic and analytic -- to the discovery of such a theory and highlight the negative role that the building block hypothesis (BBH) has played. We argue based on experimental results that a fundamental limitation which is widely believed to constrain the SGA's adaptive ability (and is strongly implied by the BBH) is in fact illusionary and does not exist. The SGA therefore turns out to be more powerful than it is currently thought to be. We give conditions under which it becomes feasible to numerically approximate and study the multivariate marginals of the search distribution of an infinite population SGA over multiple generations even when its genomes are long, and explain why this analysis is relevant to the riddle of the SGA's remarkable adaptive abilities.\n    ",
        "submission_date": "2007-11-09T00:00:00",
        "last_modified_date": "2009-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0711.2058",
        "title": "Computer Model of a \"Sense of Humour\". I. General Algorithm",
        "authors": [
            "I. M. Suslov"
        ],
        "abstract": "  A computer model of a \"sense of humour\" is proposed. The humorous effect is interpreted as a specific malfunction in the course of information processing due to the need for the rapid deletion of the false version transmitted into consciousness. The biological function of a sense of humour consists in speeding up the bringing of information into consciousness and in fuller use of the resources of the brain.\n    ",
        "submission_date": "2007-11-13T00:00:00",
        "last_modified_date": "2007-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0711.2061",
        "title": "Computer Model of a \"Sense of Humour\". II. Realization in Neural Networks",
        "authors": [
            "I. M. Suslov"
        ],
        "abstract": "  The computer realization of a \"sense of humour\" requires the creation of an algorithm for solving the \"linguistic problem\", i.e. the problem of recognizing a continuous sequence of polysemantic images. Such algorithm may be realized in the Hopfield model of a neural network after its proper modification.\n    ",
        "submission_date": "2007-11-13T00:00:00",
        "last_modified_date": "2007-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0711.2270",
        "title": "Can a Computer Laugh ?",
        "authors": [
            "I. M. Suslov"
        ],
        "abstract": "  A computer model of \"a sense of humour\" suggested previously [",
        "submission_date": "2007-11-14T00:00:00",
        "last_modified_date": "2007-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0711.2478",
        "title": "A Compact Self-organizing Cellular Automata-based Genetic Algorithm",
        "authors": [
            "Vasileios Barmpoutis",
            "Gary F. Dargush"
        ],
        "abstract": "  A Genetic Algorithm (GA) is proposed in which each member of the population can change schemata only with its neighbors according to a rule. The rule methodology and the neighborhood structure employ elements from the Cellular Automata (CA) strategies. Each member of the GA population is assigned to a cell and crossover takes place only between adjacent cells, according to the predefined rule. Although combinations of CA and GA approaches have appeared previously, here we rely on the inherent self-organizing features of CA, rather than on parallelism. This conceptual shift directs us toward the evolution of compact populations containing only a handful of members. We find that the resulting algorithm can search the design space more efficiently than traditional GA strategies due to its ability to exploit mutations within this compact self-organizing population. Consequently, premature convergence is avoided and the final results often are more accurate. In order to reinforce the superior mutation capability, a re-initialization strategy also is implemented. Ten test functions and two benchmark structural engineering truss design problems are examined in order to demonstrate the performance of the method.\n    ",
        "submission_date": "2007-11-15T00:00:00",
        "last_modified_date": "2007-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0711.2914",
        "title": "Image Classification Using SVMs: One-against-One Vs One-against-All",
        "authors": [
            "Gidudu Anthony",
            "Hulley Gregg",
            "Marwala Tshilidzi"
        ],
        "abstract": "  Support Vector Machines (SVMs) are a relatively new supervised classification technique to the land cover mapping community. They have their roots in Statistical Learning Theory and have gained prominence because they are robust, accurate and are effective even when using a small training sample. By their nature SVMs are essentially binary classifiers, however, they can be adopted to handle the multiple classification tasks common in remote sensing studies. The two approaches commonly used are the One-Against-One (1A1) and One-Against-All (1AA) techniques. In this paper, these approaches are evaluated in as far as their impact and implication for land cover mapping. The main finding from this research is that whereas the 1AA technique is more predisposed to yielding unclassified and mixed pixels, the resulting classification accuracy is not significantly different from 1A1 approach. It is the authors conclusion therefore that ultimately the choice of technique adopted boils down to personal preference and the uniqueness of the dataset at hand.\n    ",
        "submission_date": "2007-11-19T00:00:00",
        "last_modified_date": "2007-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0711.3197",
        "title": "How to realize \"a sense of humour\" in computers ?",
        "authors": [
            "I. M. Suslov"
        ],
        "abstract": "  Computer model of a \"sense of humour\" suggested previously [",
        "submission_date": "2007-11-20T00:00:00",
        "last_modified_date": "2007-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0711.4309",
        "title": "Knowware: the third star after Hardware and Software",
        "authors": [
            "Ruqian Lu"
        ],
        "abstract": "  This book proposes to separate knowledge from software and to make it a commodity that is called knowware. The architecture, representation and function of Knowware are discussed. The principles of knowware engineering and its three life cycle models: furnace model, crystallization model and spiral model are proposed and analyzed. Techniques of software/knowware co-engineering are introduced. A software component whose knowledge is replaced by knowware is called mixware. An object and component oriented development schema of mixware is introduced. In particular, the tower model and ladder model for mixware development are proposed and discussed. Finally, knowledge service and knowware based Web service are introduced and compared with Web service. In summary, knowware, software and hardware should be considered as three equally important underpinnings of IT industry.\n",
        "submission_date": "2007-11-27T00:00:00",
        "last_modified_date": "2007-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0711.4507",
        "title": "The Second Law as a Cause of the Evolution",
        "authors": [
            "Oded Kafri"
        ],
        "abstract": "  It is a common belief that in any environment where life is possible, life will be generated. Here it is suggested that the cause for a spontaneous generation of complex systems is probability driven processes. Based on equilibrium thermodynamics, it is argued that in low occupation number statistical systems, the second law of thermodynamics yields an increase of thermal entropy and a canonic energy distribution. However, in high occupation number statistical systems, the same law for the same reasons yields an increase of information and a Benford's law/power-law energy distribution. It is therefore, plausible, that eventually the heat death is not necessarily the end of the universe.\n    ",
        "submission_date": "2007-11-28T00:00:00",
        "last_modified_date": "2007-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0711.4902",
        "title": "Circumspect descent prevails in solving random constraint satisfaction problems",
        "authors": [
            "Mikko Alava",
            "John Ardelius",
            "Erik Aurell",
            "Petteri Kaski",
            "Supriya Krishnamurthy",
            "Pekka Orponen",
            "Sakari Seitz"
        ],
        "abstract": "  We study the performance of stochastic local search algorithms for random instances of the $K$-satisfiability ($K$-SAT) problem. We introduce a new stochastic local search algorithm, ChainSAT, which moves in the energy landscape of a problem instance by {\\em never going upwards} in energy. ChainSAT is a \\emph{focused} algorithm in the sense that it considers only variables occurring in unsatisfied clauses. We show by extensive numerical investigations that ChainSAT and other focused algorithms solve large $K$-SAT instances almost surely in linear time, up to high clause-to-variable ratios $\\alpha$; for example, for K=4 we observe linear-time performance well beyond the recently postulated clustering and condensation transitions in the solution space. The performance of ChainSAT is a surprise given that by design the algorithm gets trapped into the first local energy minimum it encounters, yet no such minima are encountered. We also study the geometry of the solution space as accessed by stochastic local search algorithms.\n    ",
        "submission_date": "2007-11-30T00:00:00",
        "last_modified_date": "2007-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.0171",
        "title": "A Spectral Approach to Analyzing Belief Propagation for 3-Coloring",
        "authors": [
            "Amin Coja-Oghlan",
            "Elchanan Mossel",
            "Dan Vilenchik"
        ],
        "abstract": "  Contributing to the rigorous understanding of BP, in this paper we relate the convergence of BP to spectral properties of the graph. This encompasses a result for random graphs with a ``planted'' solution; thus, we obtain the first rigorous result on BP for graph coloring in the case of a complex graphical structure (as opposed to trees). In particular, the analysis shows how Belief Propagation breaks the symmetry between the $3!$ possible permutations of the color classes.\n    ",
        "submission_date": "2007-12-02T00:00:00",
        "last_modified_date": "2007-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.0744",
        "title": "Computational Chemotaxis in Ants and Bacteria over Dynamic Environments",
        "authors": [
            "Vitorino Ramos",
            "C. M. Fernandes",
            "A. C. Rosa",
            "A. Abraham"
        ],
        "abstract": "  Chemotaxis can be defined as an innate behavioural response by an organism to a directional stimulus, in which bacteria, and other single-cell or multicellular organisms direct their movements according to certain chemicals in their environment. This is important for bacteria to find food (e.g., glucose) by swimming towards the highest concentration of food molecules, or to flee from poisons. Based on self-organized computational approaches and similar stigmergic concepts we derive a novel swarm intelligent algorithm. What strikes from these observations is that both eusocial insects as ant colonies and bacteria have similar natural mechanisms based on stigmergy in order to emerge coherent and sophisticated patterns of global collective behaviour. Keeping in mind the above characteristics we will present a simple model to tackle the collective adaptation of a social swarm based on real ant colony behaviors (SSA algorithm) for tracking extrema in dynamic environments and highly multimodal complex functions described in the well-know De Jong test suite. Later, for the purpose of comparison, a recent model of artificial bacterial foraging (BFOA algorithm) based on similar stigmergic features is described and analyzed. Final results indicate that the SSA collective intelligence is able to cope and quickly adapt to unforeseen situations even when over the same cooperative foraging period, the community is requested to deal with two different and contradictory purposes, while outperforming BFOA in adaptive speed. Results indicate that the present approach deals well in severe Dynamic Optimization problems.\n    ",
        "submission_date": "2007-12-05T00:00:00",
        "last_modified_date": "2007-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.0932",
        "title": "Dimensionality Reduction and Reconstruction using Mirroring Neural Networks and Object Recognition based on Reduced Dimension Characteristic Vector",
        "authors": [
            "Dasika Ratna Deepthi",
            "Sujeet Kuchibhotla",
            "K.Eswaran"
        ],
        "abstract": "  In this paper, we present a Mirroring Neural Network architecture to perform non-linear dimensionality reduction and Object Recognition using a reduced lowdimensional characteristic vector. In addition to dimensionality reduction, the network also reconstructs (mirrors) the original high-dimensional input vector from the reduced low-dimensional data. The Mirroring Neural Network architecture has more number of processing elements (adalines) in the outer layers and the least number of elements in the central layer to form a converging-diverging shape in its configuration. Since this network is able to reconstruct the original image from the output of the innermost layer (which contains all the information about the input pattern), these outputs can be used as object signature to classify patterns. The network is trained to minimize the discrepancy between actual output and the input by back propagating the mean squared error from the output layer to the input layer. After successfully training the network, it can reduce the dimension of input vectors and mirror the patterns fed to it. The Mirroring Neural Network architecture gave very good results on various test patterns.\n    ",
        "submission_date": "2007-12-06T00:00:00",
        "last_modified_date": "2007-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.0938",
        "title": "Automatic Pattern Classification by Unsupervised Learning Using Dimensionality Reduction of Data with Mirroring Neural Networks",
        "authors": [
            "Dasika Ratna Deepthi",
            "G.R.Aditya Krishna",
            "K. Eswaran"
        ],
        "abstract": "  This paper proposes an unsupervised learning technique by using Multi-layer Mirroring Neural Network and Forgy's clustering algorithm. Multi-layer Mirroring Neural Network is a neural network that can be trained with generalized data inputs (different categories of image patterns) to perform non-linear dimensionality reduction and the resultant low-dimensional code is used for unsupervised pattern classification using Forgy's algorithm. By adapting the non-linear activation function (modified sigmoidal function) and initializing the weights and bias terms to small random values, mirroring of the input pattern is initiated. In training, the weights and bias terms are changed in such a way that the input presented is reproduced at the output by back propagating the error. The mirroring neural network is capable of reducing the input vector to a great degree (approximately 1/30th the original size) and also able to reconstruct the input pattern at the output layer from this reduced code units. The feature set (output of central hidden layer) extracted from this network is fed to Forgy's algorithm, which classify input data patterns into distinguishable classes. In the implementation of Forgy's algorithm, initial seed points are selected in such a way that they are distant enough to be perfectly grouped into different categories. Thus a new method of unsupervised learning is formulated and demonstrated in this paper. This method gave impressive results when applied to classification of different image patterns.\n    ",
        "submission_date": "2007-12-06T00:00:00",
        "last_modified_date": "2007-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.1310",
        "title": "About Algorithm for Transformation of Logic Functions (ATLF)",
        "authors": [
            "Lev Cherbanski"
        ],
        "abstract": "  In this article the algorithm for transformation of logic functions which are given by truth tables is considered. The suggested algorithm allows the transformation of many-valued logic functions with the required number of variables and can be looked in this sense as universal.\n    ",
        "submission_date": "2007-12-08T00:00:00",
        "last_modified_date": "2007-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.1345",
        "title": "Sequential operators in computability logic",
        "authors": [
            "Giorgi Japaridze"
        ],
        "abstract": "  Computability logic (CL) (see ",
        "submission_date": "2007-12-09T00:00:00",
        "last_modified_date": "2008-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.1365",
        "title": "Population stratification using a statistical model on hypergraphs",
        "authors": [
            "Alexei Vazquez"
        ],
        "abstract": "  Population stratification is a problem encountered in several areas of biology and public health. We tackle this problem by mapping a population and its elements attributes into a hypergraph, a natural extension of the concept of graph or network to encode associations among any number of elements. On this hypergraph, we construct a statistical model reflecting our intuition about how the elements attributes can emerge from a postulated population structure. Finally, we introduce the concept of stratification representativeness as a mean to identify the simplest stratification already containing most of the information about the population structure. We demonstrate the power of this framework stratifying an animal and a human population based on phenotypic and genotypic properties, respectively.\n    ",
        "submission_date": "2007-12-09T00:00:00",
        "last_modified_date": "2007-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.3654",
        "title": "Improving the Performance of PieceWise Linear Separation Incremental Algorithms for Practical Hardware Implementations",
        "authors": [
            "Alejandro Chinea Manrique De Lara",
            "Juan Manuel Moreno",
            "Arostegui Jordi Madrenas",
            "Joan Cabestany"
        ],
        "abstract": "  In this paper we shall review the common problems associated with Piecewise Linear Separation incremental algorithms. This kind of neural models yield poor performances when dealing with some classification problems, due to the evolving schemes used to construct the resulting networks. So as to avoid this undesirable behavior we shall propose a modification criterion. It is based upon the definition of a function which will provide information about the quality of the network growth process during the learning phase. This function is evaluated periodically as the network structure evolves, and will permit, as we shall show through exhaustive benchmarks, to considerably improve the performance(measured in terms of network complexity and generalization capabilities) offered by the networks generated by these incremental models.\n    ",
        "submission_date": "2007-12-21T00:00:00",
        "last_modified_date": "2007-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0712.4402",
        "title": "Judgment",
        "authors": [
            "Ruadhan O'Flanagan"
        ],
        "abstract": "  The concept of a judgment as a logical action which introduces new information into a deductive system is examined. This leads to a way of mathematically representing implication which is distinct from the familiar material implication, according to which \"If A then B\" is considered to be equivalent to \"B or not-A\". This leads, in turn, to a resolution of the paradox of the raven.\n    ",
        "submission_date": "2007-12-28T00:00:00",
        "last_modified_date": "2008-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0701039",
        "title": "On the Complexity of the Numerically Definite Syllogistic and Related Fragments",
        "authors": [
            "Ian Pratt-Hartmann"
        ],
        "abstract": "  In this paper, we determine the complexity of the satisfiability problem for various logics obtained by adding numerical quantifiers, and other constructions, to the traditional syllogistic. In addition, we demonstrate the incompleteness of some recently proposed proof-systems for these logics.\n    ",
        "submission_date": "2007-01-06T00:00:00",
        "last_modified_date": "2007-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0701057",
        "title": "Cooperative Optimization for Energy Minimization: A Case Study of Stereo Matching",
        "authors": [
            "Xiaofei Huang"
        ],
        "abstract": "  Often times, individuals working together as a team can solve hard problems beyond the capability of any individual in the team. Cooperative optimization is a newly proposed general method for attacking hard optimization problems inspired by cooperation principles in team playing. It has an established theoretical foundation and has demonstrated outstanding performances in solving real-world optimization problems. With some general settings, a cooperative optimization algorithm has a unique equilibrium and converges to it with an exponential rate regardless initial conditions and insensitive to perturbations. It also possesses a number of global optimality conditions for identifying global optima so that it can terminate its search process efficiently. This paper offers a general description of cooperative optimization, addresses a number of design issues, and presents a case study to demonstrate its power.\n    ",
        "submission_date": "2007-01-09T00:00:00",
        "last_modified_date": "2007-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0701083",
        "title": "A Backtracking-Based Algorithm for Computing Hypertree-Decompositions",
        "authors": [
            "Georg Gottlob",
            "Marko Samer"
        ],
        "abstract": "  Hypertree decompositions of hypergraphs are a generalization of tree decompositions of graphs. The corresponding hypertree-width is a measure for the cyclicity and therefore tractability of the encoded computation problem. Many NP-hard decision and computation problems are known to be tractable on instances whose structure corresponds to hypergraphs of bounded hypertree-width. Intuitively, the smaller the hypertree-width, the faster the computation problem can be solved. In this paper, we present the new backtracking-based algorithm det-k-decomp for computing hypertree decompositions of small width. Our benchmark evaluations have shown that det-k-decomp significantly outperforms opt-k-decomp, the only exact hypertree decomposition algorithm so far. Even compared to the best heuristic algorithm, we obtained competitive results as long as the hypergraphs are not too large.\n    ",
        "submission_date": "2007-01-14T00:00:00",
        "last_modified_date": "2007-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0701120",
        "title": "Algorithmic Complexity Bounds on Future Prediction Errors",
        "authors": [
            "A. Chernov",
            "M. Hutter",
            "J. Schmidhuber"
        ],
        "abstract": "  We bound the future loss when predicting any (computably) stochastic sequence online. Solomonoff finitely bounded the total deviation of his universal predictor $M$ from the true distribution $mu$ by the algorithmic complexity of $mu$. Here we assume we are at a time $t>1$ and already observed $x=x_1...x_t$. We bound the future prediction performance on $x_{t+1}x_{t+2}...$ by a new variant of algorithmic complexity of $mu$ given $x$, plus the complexity of the randomness deficiency of $x$. The new complexity is monotone in its condition in the sense that this complexity can only decrease if the condition is prolonged. We also briefly discuss potential generalizations to Bayesian model classes and to classification problems.\n    ",
        "submission_date": "2007-01-19T00:00:00",
        "last_modified_date": "2007-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0701127",
        "title": "A novel set of rotationally and translationally invariant features for images based on the non-commutative bispectrum",
        "authors": [
            "Risi Kondor"
        ],
        "abstract": "  We propose a new set of rotationally and translationally invariant features for image or pattern recognition and classification. The new features are cubic polynomials in the pixel intensities and provide a richer representation of the original image than most existing systems of invariants. Our construction is based on the generalization of the concept of bispectrum to the three-dimensional rotation group SO(3), and a projection of the image onto the sphere.\n    ",
        "submission_date": "2007-01-20T00:00:00",
        "last_modified_date": "2007-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0701139",
        "title": "Time and the Prisoner's Dilemma",
        "authors": [
            "Yishay Mor",
            "Jeffrey S. Rosenschein"
        ],
        "abstract": "  This paper examines the integration of computational complexity into game theoretic models. The example focused on is the Prisoner's Dilemma, repeated for a finite length of time. We show that a minimal bound on the players' computational ability is sufficient to enable cooperative behavior.\n",
        "submission_date": "2007-01-22T00:00:00",
        "last_modified_date": "2007-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0702011",
        "title": "Dealing With Logical Omniscience: Expressiveness and Pragmatics",
        "authors": [
            "Joseph Y. Halpern",
            "Riccardo Pucella"
        ],
        "abstract": "  We examine four approaches for dealing with the logical omniscience problem and their potential applicability: the syntactic approach, awareness, algorithmic knowledge, and impossible possible worlds. Although in some settings these approaches are equi-expressive and can capture all epistemic states, in other settings of interest (especially with probability in the picture), we show that they are not equi-expressive. We then consider the pragmatics of dealing with logical omniscience-- how to choose an approach and construct an appropriate model.\n    ",
        "submission_date": "2007-02-01T00:00:00",
        "last_modified_date": "2007-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0702072",
        "title": "Logic Programming with Satisfiability",
        "authors": [
            "Michael Codish",
            "Vitaly Lagoon",
            "Peter J. Stuckey"
        ],
        "abstract": "  This paper presents a Prolog interface to the MiniSat satisfiability solver. Logic program- ming with satisfiability combines the strengths of the two paradigms: logic programming for encoding search problems into satisfiability on the one hand and efficient SAT solving on the other. This synergy between these two exposes a programming paradigm which we propose here as a logic programming pearl. To illustrate logic programming with SAT solving we give an example Prolog program which solves instances of Partial MAXSAT.\n    ",
        "submission_date": "2007-02-13T00:00:00",
        "last_modified_date": "2007-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0702082",
        "title": "Invariant template matching in systems with spatiotemporal coding: a vote for instability",
        "authors": [
            "Ivan Tyukin",
            "Tatiana Tyukina",
            "Cees van Leeuwen"
        ],
        "abstract": "  We consider the design of a pattern recognition that matches templates to images, both of which are spatially sampled and encoded as temporal sequences. The image is subject to a combination of various perturbations. These include ones that can be modeled as parameterized uncertainties such as image blur, luminance, translation, and rotation as well as unmodeled ones. Biological and neural systems require that these perturbations be processed through a minimal number of channels by simple adaptation mechanisms. We found that the most suitable mathematical framework to meet this requirement is that of weakly attracting sets. This framework provides us with a normative and unifying solution to the pattern recognition problem. We analyze the consequences of its explicit implementation in neural systems. Several properties inherent to the systems designed in accordance with our normative mathematical argument coincide with known empirical facts. This is illustrated in mental rotation, visual search and blur/intensity adaptation. We demonstrate how our results can be applied to a range of practical problems in template matching and pattern recognition.\n    ",
        "submission_date": "2007-02-14T00:00:00",
        "last_modified_date": "2007-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0702096",
        "title": "Overcoming Hierarchical Difficulty by Hill-Climbing the Building Block Structure",
        "authors": [
            "David Iclanzan",
            "Dan Dumitrescu"
        ],
        "abstract": "  The Building Block Hypothesis suggests that Genetic Algorithms (GAs) are well-suited for hierarchical problems, where efficient solving requires proper problem decomposition and assembly of solution from sub-solution with strong non-linear interdependencies. The paper proposes a hill-climber operating over the building block (BB) space that can efficiently address hierarchical problems. The new Building Block Hill-Climber (BBHC) uses past hill-climb experience to extract BB information and adapts its neighborhood structure accordingly. The perpetual adaptation of the neighborhood structure allows the method to climb the hierarchical structure solving successively the hierarchical levels. It is expected that for fully non deceptive hierarchical BB structures the BBHC can solve hierarchical problems in linearithmic time. Empirical results confirm that the proposed method scales almost linearly with the problem size thus clearly outperforms population based recombinative methods.\n    ",
        "submission_date": "2007-02-16T00:00:00",
        "last_modified_date": "2007-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0702144",
        "title": "Slope One Predictors for Online Rating-Based Collaborative Filtering",
        "authors": [
            "Daniel Lemire",
            "Anna Maclachlan"
        ],
        "abstract": "  Rating-based collaborative filtering is the process of predicting how a user would rate a given item from other user ratings. We propose three related slope one schemes with predictors of the form f(x) = x + b, which precompute the average difference between the ratings of one item and another for users who rated both. Slope one algorithms are easy to implement, efficient to query, reasonably accurate, and they support both online queries and dynamic updates, which makes them good candidates for real-world systems. The basic slope one scheme is suggested as a new reference scheme for collaborative filtering. By factoring in items that a user liked separately from items that a user disliked, we achieve results competitive with slower memory-based schemes over the standard benchmark EachMovie and Movielens data sets while better fulfilling the desiderata of CF applications.\n    ",
        "submission_date": "2007-02-24T00:00:00",
        "last_modified_date": "2008-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0702149",
        "title": "Coupling Control and Human-Centered Automation in Mathematical Models of Complex Systems",
        "authors": [
            "Roderick V.N. Melnik"
        ],
        "abstract": "  In this paper we analyze mathematically how human factors can be effectively incorporated into the analysis and control of complex systems. As an example, we focus our discussion around one of the key problems in the Intelligent Transportation Systems (ITS) theory and practice, the problem of speed control, considered here as a decision making process with limited information available. The problem is cast mathematically in the general framework of control problems and is treated in the context of dynamically changing environments where control is coupled to human-centered automation. Since in this case control might not be limited to a small number of control settings, as it is often assumed in the control literature, serious difficulties arise in the solution of this problem. We demonstrate that the problem can be reduced to a set of Hamilton-Jacobi-Bellman equations where human factors are incorporated via estimations of the system Hamiltonian. In the ITS context, these estimations can be obtained with the use of on-board equipment like sensors/receivers/actuators, in-vehicle communication devices, etc. The proposed methodology provides a way to integrate human factor into the solving process of the models for other complex dynamic systems.\n    ",
        "submission_date": "2007-02-25T00:00:00",
        "last_modified_date": "2007-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0703087",
        "title": "Social Information Processing in Social News Aggregation",
        "authors": [
            "Kristina Lerman"
        ],
        "abstract": "  The rise of the social media sites, such as blogs, wikis, Digg and Flickr among others, underscores the transformation of the Web to a participatory medium in which users are collaboratively creating, evaluating and distributing information. The innovations introduced by social media has lead to a new paradigm for interacting with information, what we call 'social information processing'. In this paper, we study how social news aggregator Digg exploits social information processing to solve the problems of document recommendation and rating. First, we show, by tracking stories over time, that social networks play an important role in document recommendation. The second contribution of this paper consists of two mathematical models. The first model describes how collaborative rating and promotion of stories emerges from the independent decisions made by many users. The second model describes how a user's influence, the number of promoted stories and the user's social network, changes in time. We find qualitative agreement between predictions of the model and user data gathered from Digg.\n    ",
        "submission_date": "2007-03-15T00:00:00",
        "last_modified_date": "2007-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0703095",
        "title": "Copula Component Analysis",
        "authors": [
            "Jian Ma",
            "Zengqi Sun"
        ],
        "abstract": "  A framework named Copula Component Analysis (CCA) for blind source separation is proposed as a generalization of Independent Component Analysis (ICA). It differs from ICA which assumes independence of sources that the underlying components may be dependent with certain structure which is represented by Copula. By incorporating dependency structure, much accurate estimation can be made in principle in the case that the assumption of independence is invalidated. A two phrase inference method is introduced for CCA which is based on the notion of multidimensional ICA.\n    ",
        "submission_date": "2007-03-20T00:00:00",
        "last_modified_date": "2007-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0703118",
        "title": "Mathematical model of interest matchmaking in electronic social networks",
        "authors": [
            "Andreas de Vries"
        ],
        "abstract": "  The problem of matchmaking in electronic social networks is formulated as an optimization problem. In particular, a function measuring the matching degree of fields of interest of a search profile with those of an advertising profile is proposed.\n    ",
        "submission_date": "2007-03-23T00:00:00",
        "last_modified_date": "2007-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0703135",
        "title": "Dependency Parsing with Dynamic Bayesian Network",
        "authors": [
            "Virginia Savova",
            "Leonid Peshkin"
        ],
        "abstract": "  Exact parsing with finite state automata is deemed inappropriate because of the unbounded non-locality languages overwhelmingly exhibit. We propose a way to structure the parsing task in order to make it amenable to local classification methods. This allows us to build a Dynamic Bayesian Network which uncovers the syntactic dependency structure of English sentences. Experiments with the Wall Street Journal demonstrate that the model successfully learns from labeled data.\n    ",
        "submission_date": "2007-03-27T00:00:00",
        "last_modified_date": "2007-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0703138",
        "title": "Reinforcement Learning for Adaptive Routing",
        "authors": [
            "Leonid Peshkin",
            "Virginia Savova"
        ],
        "abstract": "  Reinforcement learning means learning a policy--a mapping of observations into actions--based on feedback from the environment. The learning can be viewed as browsing a set of policies while evaluating them by trial through interaction with the environment. We present an application of gradient ascent algorithm for reinforcement learning to a complex domain of packet routing in network communication and compare the performance of this algorithm to other routing methods on a benchmark problem.\n    ",
        "submission_date": "2007-03-28T00:00:00",
        "last_modified_date": "2007-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/physics/0703126",
        "title": "The Laplace-Jaynes approach to induction",
        "authors": [
            "P. G. L. Porta Mana",
            "A. M\u00e5nsson",
            "G. Bj\u00f6rk"
        ],
        "abstract": "  An approach to induction is presented, based on the idea of analysing the context of a given problem into `circumstances'. This approach, fully Bayesian in form and meaning, provides a complement or in some cases an alternative to that based on de Finetti's representation theorem and on the notion of infinite exchangeability. In particular, it gives an alternative interpretation of those formulae that apparently involve `unknown probabilities' or `propensities'. Various advantages and applications of the presented approach are discussed, especially in comparison to that based on exchangeability. Generalisations are also discussed.\n    ",
        "submission_date": "2007-03-12T00:00:00",
        "last_modified_date": "2007-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/q-bio/0701009",
        "title": "Attribute Exploration of Discrete Temporal Transitions",
        "authors": [
            "Johannes Wollbold"
        ],
        "abstract": "  Discrete temporal transitions occur in a variety of domains, but this work is mainly motivated by applications in molecular biology: explaining and analyzing observed transcriptome and proteome time series by literature and database knowledge. The starting point of a formal concept analysis model is presented. The objects of a formal context are states of the interesting entities, and the attributes are the variable properties defining the current state (e.g. observed presence or absence of proteins). Temporal transitions assign a relation to the objects, defined by deterministic or non-deterministic transition rules between sets of pre- and postconditions. This relation can be generalized to its transitive closure, i.e. states are related if one results from the other by a transition sequence of arbitrary length. The focus of the work is the adaptation of the attribute exploration algorithm to such a relational context, so that questions concerning temporal dependencies can be asked during the exploration process and be answered from the computed stem base. Results are given for the abstract example of a game and a small gene regulatory network relevant to a biomedical question.\n    ",
        "submission_date": "2007-01-04T00:00:00",
        "last_modified_date": "2007-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/quant-ph/0702072",
        "title": "Markovian Entanglement Networks",
        "authors": [
            "Pierfrancesco La Mura",
            "Lukasz Swiatczak"
        ],
        "abstract": "  Graphical models of probabilistic dependencies have been extensively investigated in the context of classical uncertainty. However, in some domains (most notably, in computational physics and quantum computing) the nature of the relevant uncertainty is non-classical, and the laws of classical probability theory are superseded by those of quantum mechanics. In this paper we introduce Markovian Entanglement Networks (MEN), a novel class of graphical representations of quantum-mechanical dependencies in the context of such non-classical systems. MEN are the quantum-mechanical analogue of Markovian Networks, a family of undirected graphical representations which, in the classical domain, exploit a notion of conditional independence among subsystems.\n",
        "submission_date": "2007-02-07T00:00:00",
        "last_modified_date": "2007-02-07T00:00:00"
    }
]