[
    {
        "url": "https://arxiv.org/abs/1201.0328",
        "title": "Let us first agree on what the term \"semantics\" means: An unorthodox approach to an age-old debate",
        "authors": [
            "Emanuel Diamant"
        ],
        "abstract": "Traditionally, semantics has been seen as a feature of human language. The advent of the information era has led to its widespread redefinition as an information feature. Contrary to this praxis, I define semantics as a special kind of information. Revitalizing the ideas of Bar-Hillel and Carnap I have recreated and re-established the notion of semantics as the notion of Semantic Information. I have proposed a new definition of information (as a description, a linguistic text, a piece of a story or a tale) and a clear segregation between two different types of information - physical and semantic information. I hope, I have clearly explained the (usually obscured and mysterious) interrelations between data and physical information as well as the relation between physical information and semantic information. Consequently, usually indefinable notions of \"information\", \"knowledge\", \"memory\", \"learning\" and \"semantics\" have also received their suitable illumination and explanation.\n    ",
        "submission_date": "2012-01-01T00:00:00",
        "last_modified_date": "2012-01-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.0414",
        "title": "Continuity in Information Algebras",
        "authors": [
            "Xuechong Guan",
            "Yongming Li"
        ],
        "abstract": "In this paper, the continuity and strong continuity in domain-free information algebras and labeled information algebras are introduced respectively. A more general concept of continuous function which is defined between two domain-free continuous information algebras is presented. It is shown that, with the operations combination and focusing, the set of all continuous functions between two domain-free s-continuous information algebras forms a new s-continuous information algebra. By studying the relationship between domain-free information algebras and labeled information algebras, it is demonstrated that they do correspond to each other on s-compactness.\n    ",
        "submission_date": "2012-01-02T00:00:00",
        "last_modified_date": "2012-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.0478",
        "title": "Technical Note: Exploring \u03a3^P_2 / \u03a0^P_2-hardness for Argumentation Problems with fixed distance to tractable classes",
        "authors": [
            "Wolfgang Dvo\u0159\u00e1k"
        ],
        "abstract": "We study the complexity of reasoning in abstracts argumentation frameworks close to graph classes that allow for efficient reasoning methods, i.e.\\ to one of the classes of acyclic, noeven, biparite and symmetric AFs. In this work we show that certain reasoning problems on the second level of the polynomial hierarchy still maintain their full complexity when restricted to instances of fixed distance to one of the above graph classes.\n    ",
        "submission_date": "2012-01-02T00:00:00",
        "last_modified_date": "2012-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.0564",
        "title": "The RegularGcc Matrix Constraint",
        "authors": [
            "Ronald de Haan",
            "Nina Narodytska",
            "Toby Walsh"
        ],
        "abstract": "We study propagation of the RegularGcc global constraint. This ensures that each row of a matrix of decision variables satisfies a Regular constraint, and each column satisfies a Gcc constraint. On the negative side, we prove that propagation is NP-hard even under some strong restrictions (e.g. just 3 values, just 4 states in the automaton, or just 5 columns to the matrix). On the positive side, we identify two cases where propagation is fixed parameter tractable. In addition, we show how to improve propagation over a simple decomposition into separate Regular and Gcc constraints by identifying some necessary but insufficient conditions for a solution. We enforce these conditions with some additional weighted row automata. Experimental results demonstrate the potential of these methods on some standard benchmark problems.\n    ",
        "submission_date": "2012-01-03T00:00:00",
        "last_modified_date": "2012-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.2004",
        "title": "Optimal Fuzzy Model Construction with Statistical Information using Genetic Algorithm",
        "authors": [
            "Md. Amjad Hossain",
            "Pintu Chandra Shill",
            "Bishnu Sarker",
            "Kazuyuki Murase"
        ],
        "abstract": "Fuzzy rule based models have a capability to approximate any continuous function to any degree of accuracy on a compact domain. The majority of FLC design process relies on heuristic knowledge of experience operators. In order to make the design process automatic we present a genetic approach to learn fuzzy rules as well as membership function parameters. Moreover, several statistical information criteria such as the Akaike information criterion (AIC), the Bhansali-Downham information criterion (BDIC), and the Schwarz-Rissanen information criterion (SRIC) are used to construct optimal fuzzy models by reducing fuzzy rules. A genetic scheme is used to design Takagi-Sugeno-Kang (TSK) model for identification of the antecedent rule parameters and the identification of the consequent parameters. Computer simulations are presented confirming the performance of the constructed fuzzy logic controller.\n    ",
        "submission_date": "2012-01-10T00:00:00",
        "last_modified_date": "2012-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.2073",
        "title": "Pbm: A new dataset for blog mining",
        "authors": [
            "Mehwish Aziz",
            "Muhammad Rafi"
        ],
        "abstract": "Text mining is becoming vital as Web 2.0 offers collaborative content creation and sharing. Now Researchers have growing interest in text mining methods for discovering knowledge. Text mining researchers come from variety of areas like: Natural Language Processing, Computational Linguistic, Machine Learning, and Statistics. A typical text mining application involves preprocessing of text, stemming and lemmatization, tagging and annotation, deriving knowledge patterns, evaluating and interpreting the results. There are numerous approaches for performing text mining tasks, like: clustering, categorization, sentimental analysis, and summarization. There is a growing need to standardize the evaluation of these tasks. One major component of establishing standardization is to provide standard datasets for these tasks. Although there are various standard datasets available for traditional text mining tasks, but there are very few and expensive datasets for blog-mining task. Blogs, a new genre in web 2.0 is a digital diary of web user, which has chronological entries and contains a lot of useful knowledge, thus offers a lot of challenges and opportunities for text mining. In this paper, we report a new indigenous dataset for Pakistani Political Blogosphere. The paper describes the process of data collection, organization, and standardization. We have used this dataset for carrying out various text mining tasks for blogosphere, like: blog-search, political sentiments analysis and tracking, identification of influential blogger, and clustering of the blog-posts. We wish to offer this dataset free for others who aspire to pursue further in this domain.\n    ",
        "submission_date": "2012-01-10T00:00:00",
        "last_modified_date": "2012-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.2084",
        "title": "Sentence based semantic similarity measure for blog-posts",
        "authors": [
            "Mehwish Aziz",
            "Muhammad Rafi"
        ],
        "abstract": "Blogs-Online digital diary like application on web 2.0 has opened new and easy way to voice opinion, thoughts, and like-dislike of every Internet user to the World. Blogosphere has no doubt the largest user-generated content repository full of knowledge. The potential of this knowledge is still to be explored. Knowledge discovery from this new genre is quite difficult and challenging as it is totally different from other popular genre of web-applications like World Wide Web (WWW). Blog-posts unlike web documents are small in size, thus lack in context and contain relaxed grammatical structures. Hence, standard text similarity measure fails to provide good results. In this paper, specialized requirements for comparing a pair of blog-posts is thoroughly investigated. Based on this we proposed a novel algorithm for sentence oriented semantic similarity measure of a pair of blog-posts. We applied this algorithm on a subset of political blogosphere of Pakistan, to cluster the blogs on different issues of political realm and to identify the influential bloggers.\n    ",
        "submission_date": "2012-01-10T00:00:00",
        "last_modified_date": "2012-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.2711",
        "title": "Ultrametric Model of Mind, I: Review",
        "authors": [
            "Fionn Murtagh"
        ],
        "abstract": "We mathematically model Ignacio Matte Blanco's principles of symmetric and asymmetric being through use of an ultrametric topology. We use for this the highly regarded 1975 book of this Chilean psychiatrist and pyschoanalyst (born 1908, died 1995). Such an ultrametric model corresponds to hierarchical clustering in the empirical data, e.g. text. We show how an ultrametric topology can be used as a mathematical model for the structure of the logic that reflects or expresses Matte Blanco's symmetric being, and hence of the reasoning and thought processes involved in conscious reasoning or in reasoning that is lacking, perhaps entirely, in consciousness or awareness of itself. In a companion paper we study how symmetric (in the sense of Matte Blanco's) reasoning can be demarcated in a context of symmetric and asymmetric reasoning provided by narrative text.\n    ",
        "submission_date": "2012-01-13T00:00:00",
        "last_modified_date": "2012-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.2719",
        "title": "Ultrametric Model of Mind, II: Application to Text Content Analysis",
        "authors": [
            "Fionn Murtagh"
        ],
        "abstract": "In a companion paper, Murtagh (2012), we discussed how Matte Blanco's work linked the unrepressed unconscious (in the human) to symmetric logic and thought processes. We showed how ultrametric topology provides a most useful representational and computational framework for this. Now we look at the extent to which we can find ultrametricity in text. We use coherent and meaningful collections of nearly 1000 texts to show how we can measure inherent ultrametricity. On the basis of our findings we hypothesize that inherent ultrametricty is a basis for further exploring unconscious thought processes.\n    ",
        "submission_date": "2012-01-13T00:00:00",
        "last_modified_date": "2012-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.3107",
        "title": "Tacit knowledge mining algorithm based on linguistic truth-valued concept lattice",
        "authors": [
            "Li Yang",
            "Yuhui Wang"
        ],
        "abstract": "This paper is the continuation of our research work about linguistic truth-valued concept lattice. In order to provide a mathematical tool for mining tacit knowledge, we establish a concrete model of 6-ary linguistic truth-valued concept lattice and introduce a mining algorithm through the structure consistency. Specifically, we utilize the attributes to depict knowledge, propose the 6-ary linguistic truth-valued attribute extended context and congener context to characterize tacit knowledge, and research the necessary and sufficient conditions of forming tacit knowledge. We respectively give the algorithms of generating the linguistic truth-valued congener context and constructing the linguistic truth-valued concept lattice.\n    ",
        "submission_date": "2012-01-15T00:00:00",
        "last_modified_date": "2012-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.3204",
        "title": "Evaluation of a Simple, Scalable, Parallel Best-First Search Strategy",
        "authors": [
            "Akihiro Kishimoto",
            "Alex Fukunaga",
            "Adi Botea"
        ],
        "abstract": "Large-scale, parallel clusters composed of commodity processors are increasingly available, enabling the use of vast processing capabilities and distributed RAM to solve hard search problems. We investigate Hash-Distributed A* (HDA*), a simple approach to parallel best-first search that asynchronously distributes and schedules work among processors based on a hash function of the search state. We use this approach to parallelize the A* algorithm in an optimal sequential version of the Fast Downward planner, as well as a 24-puzzle solver. The scaling behavior of HDA* is evaluated experimentally on a shared memory, multicore machine with 8 cores, a cluster of commodity machines using up to 64 cores, and large-scale high-performance clusters, using up to 2400 processors. We show that this approach scales well, allowing the effective utilization of large amounts of distributed memory to optimally solve problems which require terabytes of RAM. We also compare HDA* to Transposition-table Driven Scheduling (TDS), a hash-based parallelization of IDA*, and show that, in planning, HDA* significantly outperforms TDS. A simple hybrid which combines HDA* and TDS to exploit strengths of both algorithms is proposed and evaluated.\n    ",
        "submission_date": "2012-01-16T00:00:00",
        "last_modified_date": "2012-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.3408",
        "title": "The computation of first order moments on junction trees",
        "authors": [
            "Milos B. Djuric",
            "Velimir M. Ilic",
            "Miomir S. Stankovic"
        ],
        "abstract": "We review some existing methods for the computation of first order moments on junction trees using Shafer-Shenoy algorithm. First, we consider the problem of first order moments computation as vertices problem in junction trees. In this way, the problem is solved using the memory space of an order of the junction tree edge-set cardinality. After that, we consider two algorithms, Lauritzen-Nilsson algorithm, and Mau\u00e1 et al. algorithm, which computes the first order moments as the normalization problem in junction tree, using the memory space of an order of the junction tree leaf-set cardinality.\n    ",
        "submission_date": "2012-01-17T00:00:00",
        "last_modified_date": "2012-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.3851",
        "title": "Combinatorial Modelling and Learning with Prediction Markets",
        "authors": [
            "Jinli Hu"
        ],
        "abstract": "Combining models in appropriate ways to achieve high performance is commonly seen in machine learning fields today. Although a large amount of combinatorial models have been created, little attention is drawn to the commons in different models and their connections. A general modelling technique is thus worth studying to understand model combination deeply and shed light on creating new models. Prediction markets show a promise of becoming such a generic, flexible combinatorial model. By reviewing on several popular combinatorial models and prediction market models, this paper aims to show how the market models can generalise different combinatorial stuctures and how they implement these popular combinatorial models in specific conditions. Besides, we will see among different market models, Storkey's \\emph{Machine Learning Markets} provide more fundamental, generic modelling mechanisms than the others, and it has a significant appeal for both theoretical study and application.\n    ",
        "submission_date": "2012-01-18T00:00:00",
        "last_modified_date": "2012-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.3868",
        "title": "A Dichotomy for 2-Constraint Forbidden CSP Patterns",
        "authors": [
            "Martin C. Cooper",
            "Guillaume Escamocher"
        ],
        "abstract": "Although the CSP (constraint satisfaction problem) is NP-complete, even in the case when all constraints are binary, certain classes of instances are tractable. We study classes of instances defined by excluding subproblems. This approach has recently led to the discovery of novel tractable classes. The complete characterisation of all tractable classes defined by forbidding patterns (where a pattern is simply a compact representation of a set of subproblems) is a challenging problem. We demonstrate a dichotomy in the case of forbidden patterns consisting of either one or two constraints. This has allowed us to discover new tractable classes including, for example, a novel generalisation of 2SAT.\n    ",
        "submission_date": "2012-01-18T00:00:00",
        "last_modified_date": "2012-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.4080",
        "title": "Progress in animation of an EMA-controlled tongue model for acoustic-visual speech synthesis",
        "authors": [
            "Ingmar Steiner",
            "Slim Ouni"
        ],
        "abstract": "We present a technique for the animation of a 3D kinematic tongue model, one component of the talking head of an acoustic-visual (AV) speech synthesizer. The skeletal animation approach is adapted to make use of a deformable rig controlled by tongue motion capture data obtained with electromagnetic articulography (EMA), while the tongue surface is extracted from volumetric magnetic resonance imaging (MRI) data. Initial results are shown and future work outlined.\n    ",
        "submission_date": "2012-01-19T00:00:00",
        "last_modified_date": "2012-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.4089",
        "title": "A Description Logic Primer",
        "authors": [
            "Markus Kr\u00f6tzsch",
            "Frantisek Simancik",
            "Ian Horrocks"
        ],
        "abstract": "This paper provides a self-contained first introduction to description logics (DLs). The main concepts and features are explained with examples before syntax and semantics of the DL SROIQ are defined in detail. Additional sections review light-weight DL languages, discuss the relationship to the Web Ontology Language OWL and give pointers to further reading.\n    ",
        "submission_date": "2012-01-19T00:00:00",
        "last_modified_date": "2013-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.4777",
        "title": "A probabilistic methodology for multilabel classification",
        "authors": [
            "Alfonso E. Romero",
            "Luis M. de Campos"
        ],
        "abstract": "Multilabel classification is a relatively recent subfield of machine learning. Unlike to the classical approach, where instances are labeled with only one category, in multilabel classification, an arbitrary number of categories is chosen to label an instance. Due to the problem complexity (the solution is one among an exponential number of alternatives), a very common solution (the binary method) is frequently used, learning a binary classifier for every category, and combining them all afterwards. The assumption taken in this solution is not realistic, and in this work we give examples where the decisions for all the labels are not taken independently, and thus, a supervised approach should learn those existing relationships among categories to make a better classification. Therefore, we show here a generic methodology that can improve the results obtained by a set of independent probabilistic binary classifiers, by using a combination procedure with a classifier trained on the co-occurrences of the labels. We show an exhaustive experimentation in three different standard corpora of labeled documents (Reuters-21578, Ohsumed-23 and RCV1), which present noticeable improvements in all of them, when using our methodology, in three probabilistic base classifiers.\n    ",
        "submission_date": "2012-01-23T00:00:00",
        "last_modified_date": "2013-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.5426",
        "title": "Constraint Propagation as Information Maximization",
        "authors": [
            "A. Nait Abdallah",
            "M.H. van Emden"
        ],
        "abstract": "This paper draws on diverse areas of computer science to develop a unified view of computation:\n",
        "submission_date": "2012-01-26T00:00:00",
        "last_modified_date": "2013-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.5472",
        "title": "A multiagent urban traffic simulation",
        "authors": [
            "Pierrick Tranouez",
            "Eric Daud\u00e9",
            "Patrice Langlois"
        ],
        "abstract": "We built a multiagent simulation of urban traffic to model both ordinary traffic and emergency or crisis mode traffic. This simulation first builds a modeled road network based on detailed geographical information. On this network, the simulation creates two populations of agents: the Transporters and the Mobiles. Transporters embody the roads themselves; they are utilitarian and meant to handle the low level realism of the simulation. Mobile agents embody the vehicles that circulate on the network. They have one or several destinations they try to reach using initially their beliefs of the structure of the network (length of the edges, speed limits, number of lanes etc.). Nonetheless, when confronted to a dynamic, emergent prone environment (other vehicles, unexpectedly closed ways or lanes, traffic jams etc.), the rather reactive agent will activate more cognitive modules to adapt its beliefs, desires and intentions. It may change its destination(s), change the tactics used to reach the destination (favoring less used roads, following other agents, using general headings), etc. We describe our current validation of our model and the next planned improvements, both in validation and in functionalities.\n    ",
        "submission_date": "2012-01-26T00:00:00",
        "last_modified_date": "2012-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.5604",
        "title": "Discrete and fuzzy dynamical genetic programming in the XCSF learning classifier system",
        "authors": [
            "Richard J. Preen",
            "Larry Bull"
        ],
        "abstract": "A number of representation schemes have been presented for use within learning classifier systems, ranging from binary encodings to neural networks. This paper presents results from an investigation into using discrete and fuzzy dynamical system representations within the XCSF learning classifier system. In particular, asynchronous random Boolean networks are used to represent the traditional condition-action production system rules in the discrete case and asynchronous fuzzy logic networks in the continuous-valued case. It is shown possible to use self-adaptive, open-ended evolution to design an ensemble of such dynamical systems within XCSF to solve a number of well-known test problems.\n    ",
        "submission_date": "2012-01-26T00:00:00",
        "last_modified_date": "2015-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.5841",
        "title": "The thermodynamic cost of fast thought",
        "authors": [
            "Alexandre de Castro"
        ],
        "abstract": "After more than sixty years, Shannon's research [1-3] continues to raise fundamental questions, such as the one formulated by Luce [4,5], which is still unanswered: \"Why is information theory not very applicable to psychological problems, despite apparent similarities of concepts?\" On this topic, Pinker [6], one of the foremost defenders of the computational theory of mind [6], has argued that thought is simply a type of computation, and that the gap between human cognition and computational models may be illusory. In this context, in his latest book, titled Thinking Fast and Slow [8], Kahneman [7,8] provides further theoretical interpretation by differentiating the two assumed systems of the cognitive functioning of the human mind. He calls them intuition (system 1) determined to be an associative (automatic, fast and perceptual) machine, and reasoning (system 2) required to be voluntary and to operate logical- deductively. In this paper, we propose an ansatz inspired by Ausubel's learning theory for investigating, from the constructivist perspective [9-12], information processing in the working memory of cognizers. Specifically, a thought experiment is performed utilizing the mind of a dual-natured creature known as Maxwell's demon: a tiny \"man-machine\" solely equipped with the characteristics of system 1, which prevents it from reasoning. The calculation presented here shows that [...]. This result indicates that when the system 2 is shut down, both an intelligent being, as well as a binary machine, incur the same energy cost per unit of information processed, which mathematically proves the computational attribute of the system 1, as Kahneman [7,8] theorized. This finding links information theory to human psychological features and opens a new path toward the conception of a multi-bit reasoning machine.\n    ",
        "submission_date": "2012-01-27T00:00:00",
        "last_modified_date": "2013-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.5943",
        "title": "Cognitive Memory Network",
        "authors": [
            "Alex Pappachen James",
            "Sima Dimitrijev"
        ],
        "abstract": "A resistive memory network that has no crossover wiring is proposed to overcome the hardware limitations to size and functional complexity that is associated with conventional analogue neural networks. The proposed memory network is based on simple network cells that are arranged in a hierarchical modular architecture. Cognitive functionality of this network is demonstrated by an example of character recognition. The network is trained by an evolutionary process to completely recognise characters deformed by random noise, rotation, scaling and shifting\n    ",
        "submission_date": "2012-01-28T00:00:00",
        "last_modified_date": "2012-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.5959",
        "title": "Memory Based Machine Intelligence Techniques in VLSI hardware",
        "authors": [
            "Alex Pappachen James"
        ],
        "abstract": "We briefly introduce the memory based approaches to emulate machine intelligence in VLSI hardware, describing the challenges and advantages. Implementation of artificial intelligence techniques in VLSI hardware is a practical and difficult problem. Deep architectures, hierarchical temporal memories and memory networks are some of the contemporary approaches in this area of research. The techniques attempt to emulate low level intelligence tasks and aim at providing scalable solutions to high level intelligence problems such as sparse coding and contextual processing.\n    ",
        "submission_date": "2012-01-28T00:00:00",
        "last_modified_date": "2012-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.6511",
        "title": "Ontologies for the Integration of Air Quality Models and 3D City Models",
        "authors": [
            "Claudine M\u00e9tral",
            "Gilles Falquet",
            "Kostas Karatzas"
        ],
        "abstract": "The holistic approach to sustainable urban planning implies using different models in an integrated way that is capable of simulating the urban system. As the interconnection of such models is not a trivial task, one of the key elements that may be applied is the description of the urban geometric properties in an \"interoperable\" way. Focusing on air quality as one of the most pronounced urban problems, the geometric aspects of a city may be described by objects such as those defined in CityGML, so that an appropriate air quality model can be applied for estimating the quality of the urban air on the basis of atmospheric flow and chemistry equations.\n",
        "submission_date": "2012-01-31T00:00:00",
        "last_modified_date": "2012-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.6583",
        "title": "Empowerment for Continuous Agent-Environment Systems",
        "authors": [
            "Tobias Jung",
            "Daniel Polani",
            "Peter Stone"
        ],
        "abstract": "This paper develops generalizations of empowerment to continuous states. Empowerment is a recently introduced information-theoretic quantity motivated by hypotheses about the efficiency of the sensorimotor loop in biological organisms, but also from considerations stemming from curiosity-driven learning. Empowemerment measures, for agent-environment systems with stochastic transitions, how much influence an agent has on its environment, but only that influence that can be sensed by the agent sensors. It is an information-theoretic generalization of joint controllability (influence on environment) and observability (measurement by sensors) of the environment by the agent, both controllability and observability being usually defined in control theory as the dimensionality of the control/observation spaces. Earlier work has shown that empowerment has various interesting and relevant properties, e.g., it allows us to identify salient states using only the dynamics, and it can act as intrinsic reward without requiring an external reward. However, in this previous work empowerment was limited to the case of small-scale and discrete domains and furthermore state transition probabilities were assumed to be known. The goal of this paper is to extend empowerment to the significantly more important and relevant case of continuous vector-valued state spaces and initially unknown state transition probabilities. The continuous state space is addressed by Monte-Carlo approximation; the unknown transitions are addressed by model learning and prediction for which we apply Gaussian processes regression with iterated forecasting. In a number of well-known continuous control tasks we examine the dynamics induced by empowerment and include an application to exploration and online model learning.\n    ",
        "submission_date": "2012-01-31T00:00:00",
        "last_modified_date": "2012-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.6604",
        "title": "Gaussian Processes for Sample Efficient Reinforcement Learning with RMAX-like Exploration",
        "authors": [
            "Tobias Jung",
            "Peter Stone"
        ],
        "abstract": "We present an implementation of model-based online reinforcement learning (RL) for continuous domains with deterministic transitions that is specifically designed to achieve low sample complexity. To achieve low sample complexity, since the environment is unknown, an agent must intelligently balance exploration and exploitation, and must be able to rapidly generalize from observations. While in the past a number of related sample efficient RL algorithms have been proposed, to allow theoretical analysis, mainly model-learners with weak generalization capabilities were considered. Here, we separate function approximation in the model learner (which does require samples) from the interpolation in the planner (which does not require samples). For model-learning we apply Gaussian processes regression (GP) which is able to automatically adjust itself to the complexity of the problem (via Bayesian hyperparameter selection) and, in practice, often able to learn a highly accurate model from very little data. In addition, a GP provides a natural way to determine the uncertainty of its predictions, which allows us to implement the \"optimism in the face of uncertainty\" principle used to efficiently control exploration. Our method is evaluated on four common benchmark domains.\n    ",
        "submission_date": "2012-01-31T00:00:00",
        "last_modified_date": "2012-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.6615",
        "title": "Feature Selection for Value Function Approximation Using Bayesian Model Selection",
        "authors": [
            "Tobias Jung",
            "Peter Stone"
        ],
        "abstract": "Feature selection in reinforcement learning (RL), i.e. choosing basis functions such that useful approximations of the unkown value function can be obtained, is one of the main challenges in scaling RL to real-world applications. Here we consider the Gaussian process based framework GPTD for approximate policy evaluation, and propose feature selection through marginal likelihood optimization of the associated hyperparameters. Our approach has two appealing benefits: (1) given just sample transitions, we can solve the policy evaluation problem fully automatically (without looking at the learning task, and, in theory, independent of the dimensionality of the state space), and (2) model selection allows us to consider more sophisticated kernels, which in turn enable us to identify relevant subspaces and eliminate irrelevant state variables such that we can achieve substantial computational savings and improved prediction performance.\n    ",
        "submission_date": "2012-01-31T00:00:00",
        "last_modified_date": "2012-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.6626",
        "title": "Learning RoboCup-Keepaway with Kernels",
        "authors": [
            "Tobias Jung",
            "Daniel Polani"
        ],
        "abstract": "We apply kernel-based methods to solve the difficult reinforcement learning problem of 3vs2 keepaway in RoboCup simulated soccer. Key challenges in keepaway are the high-dimensionality of the state space (rendering conventional discretization-based function approximation like tilecoding infeasible), the stochasticity due to noise and multiple learning agents needing to cooperate (meaning that the exact dynamics of the environment are unknown) and real-time learning (meaning that an efficient online implementation is required). We employ the general framework of approximate policy iteration with least-squares-based policy evaluation. As underlying function approximator we consider the family of regularization networks with subset of regressors approximation. The core of our proposed solution is an efficient recursive implementation with automatic supervised selection of relevant basis functions. Simulation results indicate that the behavior learned through our approach clearly outperforms the best results obtained earlier with tilecoding by Stone et al. (2005).\n    ",
        "submission_date": "2012-01-31T00:00:00",
        "last_modified_date": "2012-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.6655",
        "title": "Learning Performance of Prediction Markets with Kelly Bettors",
        "authors": [
            "Alina Beygelzimer",
            "John Langford",
            "David Pennock"
        ],
        "abstract": "In evaluating prediction markets (and other crowd-prediction mechanisms), investigators have repeatedly observed a so-called \"wisdom of crowds\" effect, which roughly says that the average of participants performs much better than the average participant. The market price---an average or at least aggregate of traders' beliefs---offers a better estimate than most any individual trader's opinion. In this paper, we ask a stronger question: how does the market price compare to the best trader's belief, not just the average trader. We measure the market's worst-case log regret, a notion common in machine learning theory. To arrive at a meaningful answer, we need to assume something about how traders behave. We suppose that every trader optimizes according to the Kelly criteria, a strategy that provably maximizes the compound growth of wealth over an (infinite) sequence of market interactions. We show several consequences. First, the market prediction is a wealth-weighted average of the individual participants' beliefs. Second, the market learns at the optimal rate, the market price reacts exactly as if updating according to Bayes' Law, and the market prediction has low worst-case log regret to the best individual participant. We simulate a sequence of markets where an underlying true probability exists, showing that the market converges to the true objective frequency as if updating a Beta distribution, as the theory predicts. If agents adopt a fractional Kelly criteria, a common practical variant, we show that agents behave like full-Kelly agents with beliefs weighted between their own and the market's, and that the market price converges to a time-discounted frequency. Our analysis provides a new justification for fractional Kelly betting, a strategy widely used in practice for ad-hoc reasons. Finally, we propose a method for an agent to learn her own optimal Kelly fraction.\n    ",
        "submission_date": "2012-01-31T00:00:00",
        "last_modified_date": "2012-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.0440",
        "title": "The implications of embodiment for behavior and cognition: animal and robotic case studies",
        "authors": [
            "Matej Hoffmann",
            "Rolf Pfeifer"
        ],
        "abstract": "In this paper, we will argue that if we want to understand the function of the brain (or the control in the case of robots), we must understand how the brain is embedded into the physical system, and how the organism interacts with the real world. While embodiment has often been used in its trivial meaning, i.e. 'intelligence requires a body', the concept has deeper and more important implications, concerned with the relation between physical and information (neural, control) processes. A number of case studies are presented to illustrate the concept. These involve animals and robots and are concentrated around locomotion, grasping, and visual perception. A theoretical scheme that can be used to embed the diverse case studies will be presented. Finally, we will establish a link between the low-level sensory-motor processes and cognition. We will present an embodied view on categorization, and propose the concepts of 'body schema' and 'forward models' as a natural extension of the embodied approach toward first representations.\n    ",
        "submission_date": "2012-02-02T00:00:00",
        "last_modified_date": "2012-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.0837",
        "title": "On the influence of intelligence in (social) intelligence testing environments",
        "authors": [
            "Javier Insa-Cabrera",
            "Jose-Luis Benacloch-Ayuso",
            "Jose Hernandez-Orallo"
        ],
        "abstract": "This paper analyses the influence of including agents of different degrees of intelligence in a multiagent system. The goal is to better understand how we can develop intelligence tests that can evaluate social intelligence. We analyse several reinforcement algorithms in several contexts of cooperation and competition. Our experimental setting is inspired by the recently developed Darwin-Wallace distribution.\n    ",
        "submission_date": "2012-02-03T00:00:00",
        "last_modified_date": "2012-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.0940",
        "title": "Improving feature selection algorithms using normalised feature histograms",
        "authors": [
            "Alex Pappachen James",
            "Akshay Maan"
        ],
        "abstract": "The proposed feature selection method builds a histogram of the most stable features from random subsets of a training set and ranks the features based on a classifier based cross-validation. This approach reduces the instability of features obtained by conventional feature selection methods that occur with variation in training data and selection criteria. Classification results on four microarray and three image datasets using three major feature selection criteria and a naive Bayes classifier show considerable improvement over benchmark results.\n    ",
        "submission_date": "2012-02-05T00:00:00",
        "last_modified_date": "2012-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.1395",
        "title": "Modification of the Elite Ant System in Order to Avoid Local Optimum Points in the Traveling Salesman Problem",
        "authors": [
            "Majid Yousefikhoshbakht",
            "Farzad Didehvar",
            "Farhad Rahmati"
        ],
        "abstract": "This article presents a new algorithm which is a modified version of the elite ant system (EAS) algorithm. The new version utilizes an effective criterion for escaping from the local optimum points. In contrast to the classical EAC algorithms, the proposed algorithm uses only a global updating, which will increase pheromone on the edges of the best (i.e. the shortest) route and will at the same time decrease the amount of pheromone on the edges of the worst (i.e. the longest) route. In order to assess the efficiency of the new algorithm, some standard traveling salesman problems (TSPs) were studied and their results were compared with classical EAC and other well-known meta-heuristic algorithms. The results indicate that the proposed algorithm has been able to improve the efficiency of the algorithms in all instances and it is competitive with other algorithms.\n    ",
        "submission_date": "2012-02-07T00:00:00",
        "last_modified_date": "2012-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.1409",
        "title": "Optimization in SMT with LA(Q) Cost Functions",
        "authors": [
            "Roberto Sebastiani",
            "Silvia Tomasi"
        ],
        "abstract": "In the contexts of automated reasoning and formal verification, important decision problems are effectively encoded into Satisfiability Modulo Theories (SMT). In the last decade efficient SMT solvers have been developed for several theories of practical interest (e.g., linear arithmetic, arrays, bit-vectors). Surprisingly, very few work has been done to extend SMT to deal with optimization problems; in particular, we are not aware of any work on SMT solvers able to produce solutions which minimize cost functions over arithmetical variables. This is unfortunate, since some problems of interest require this functionality.\n",
        "submission_date": "2012-02-07T00:00:00",
        "last_modified_date": "2012-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.1886",
        "title": "Classification of artificial intelligence ids for smurf attack",
        "authors": [
            "N.Ugtakhbayar",
            "D.Battulga",
            "Sh.Sodbileg"
        ],
        "abstract": "Many methods have been developed to secure the network infrastructure and communication over the Internet. Intrusion detection is a relatively new addition to such techniques. Intrusion detection systems (IDS) are used to find out if someone has intrusion into or is trying to get it the network. One big problem is amount of Intrusion which is increasing day by day. We need to know about network attack information using IDS, then analysing the effect. Due to the nature of IDSs which are solely signature based, every new intrusion cannot be detected; so it is important to introduce artificial intelligence (AI) methods / techniques in IDS. Introduction of AI necessitates the importance of normalization in intrusions. This work is focused on classification of AI based IDS techniques which will help better design intrusion detection systems in the future. We have also proposed a support vector machine for IDS to detect Smurf attack with much reliable accuracy.\n    ",
        "submission_date": "2012-02-09T00:00:00",
        "last_modified_date": "2012-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.1891",
        "title": "Hyper heuristic based on great deluge and its variants for exam timetabling problem",
        "authors": [
            "Ei Shwe Sin",
            "Nang Saing Moon Kham"
        ],
        "abstract": "Today, University Timetabling problems are occurred annually and they are often hard and time consuming to solve. This paper describes Hyper Heuristics (HH) method based on Great Deluge (GD) and its variants for solving large, highly constrained timetabling problems from different domains. Generally, in hyper heuristic framework, there are two main stages: heuristic selection and move acceptance. This paper emphasizes on the latter stage to develop Hyper Heuristic (HH) framework. The main contribution of this paper is that Great Deluge (GD) and its variants: Flex Deluge(FD), Non-linear(NLGD), Extended Great Deluge(EGD) are used as move acceptance method in HH by combining Reinforcement learning (RL).These HH methods are tested on exam benchmark timetabling problem and best results and comparison analysis are reported.\n    ",
        "submission_date": "2012-02-09T00:00:00",
        "last_modified_date": "2012-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.1945",
        "title": "A framework: Cluster detection and multidimensional visualization of automated data mining using intelligent agents",
        "authors": [
            "R. Jayabrabu",
            "V. Saravanan",
            "K. Vivekanandan"
        ],
        "abstract": "Data Mining techniques plays a vital role like extraction of required knowledge, finding unsuspected information to make strategic decision in a novel way which in term understandable by domain experts. A generalized frame work is proposed by considering non - domain experts during mining process for better understanding, making better decision and better finding new patters in case of selecting suitable data mining techniques based on the user profile by means of intelligent agents. KEYWORDS: Data Mining Techniques, Intelligent Agents, User Profile, Multidimensional Visualization, Knowledge Discovery.\n    ",
        "submission_date": "2012-02-09T00:00:00",
        "last_modified_date": "2012-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.2112",
        "title": "Predicting Contextual Sequences via Submodular Function Maximization",
        "authors": [
            "Debadeepta Dey",
            "Tian Yu Liu",
            "Martial Hebert",
            "J. Andrew Bagnell"
        ],
        "abstract": "Sequence optimization, where the items in a list are ordered to maximize some reward has many applications such as web advertisement placement, search, and control libraries in robotics. Previous work in sequence optimization produces a static ordering that does not take any features of the item or context of the problem into account. In this work, we propose a general approach to order the items within the sequence based on the context (e.g., perceptual information, environment description, and goals). We take a simple, efficient, reduction-based approach where the choice and order of the items is established by repeatedly learning simple classifiers or regressors for each \"slot\" in the sequence. Our approach leverages recent work on submodular function maximization to provide a formal regret reduction from submodular sequence optimization to simple cost-sensitive prediction. We apply our contextual sequence prediction algorithm to optimize control libraries and demonstrate results on two robotics problems: manipulator trajectory prediction and mobile robot path planning.\n    ",
        "submission_date": "2012-02-09T00:00:00",
        "last_modified_date": "2012-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.2167",
        "title": "Abstract Representations and Frequent Pattern Discovery",
        "authors": [
            "Eray Ozkural"
        ],
        "abstract": "We discuss the frequent pattern mining problem in a general setting. From an analysis of abstract representations, summarization and frequent pattern mining, we arrive at a generalization of the problem. Then, we show how the problem can be cast into the powerful language of algorithmic information theory. This allows us to formulate a simple algorithm to mine for all frequent patterns.\n    ",
        "submission_date": "2012-02-10T00:00:00",
        "last_modified_date": "2012-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.2523",
        "title": "Evolutionary Computation in Astronomy and Astrophysics: A Review",
        "authors": [
            "Jos\u00e9 A. Garc\u00eda Guti\u00e9rrez",
            "Carlos Cotta",
            "Antonio J. Fern\u00e1ndez-Leiva"
        ],
        "abstract": "In general Evolutionary Computation (EC) includes a number of optimization methods inspired by biological mechanisms of evolution. The methods catalogued in this area use the Darwinian principles of life evolution to produce algorithms that returns high quality solutions to hard-to-solve optimization problems. The main strength of EC is precisely that they provide good solutions even if the computational resources (e.g., running time) are limited. Astronomy and Astrophysics are two fields that often require optimizing problems of high complexity or analyzing a huge amount of data and the so-called complete optimization methods are inherently limited by the size of the problem/data. For instance, reliable analysis of large amounts of data is central to modern astrophysics and astronomical sciences in general. EC techniques perform well where other optimization methods are inherently limited (as complete methods applied to NP-hard problems), and in the last ten years, numerous proposals have come up that apply with greater or lesser success methodologies of evolutional computation to common engineering problems. Some of these problems, such as the estimation of non-lineal parameters, the development of automatic learning techniques, the implementation of control systems, or the resolution of multi-objective optimization problems, have had (and have) a special repercussion in the fields. For these reasons EC emerges as a feasible alternative for traditional methods. In this paper, we discuss some promising applications in this direction and a number of recent works in this area; the paper also includes a general description of EC to provide a global perspective to the reader and gives some guidelines of application of EC techniques for future research\n    ",
        "submission_date": "2012-02-12T00:00:00",
        "last_modified_date": "2012-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.2536",
        "title": "Message passing for quantified Boolean formulas",
        "authors": [
            "Pan Zhang",
            "Abolfazl Ramezanpour",
            "Lenka Zdeborov\u00e1",
            "Riccardo Zecchina"
        ],
        "abstract": "We introduce two types of message passing algorithms for quantified Boolean formulas (QBF). The first type is a message passing based heuristics that can prove unsatisfiability of the QBF by assigning the universal variables in such a way that the remaining formula is unsatisfiable. In the second type, we use message passing to guide branching heuristics of a Davis-Putnam Logemann-Loveland (DPLL) complete solver. Numerical experiments show that on random QBFs our branching heuristics gives robust exponential efficiency gain with respect to the state-of-art solvers. We also manage to solve some previously unsolved benchmarks from the QBFLIB library. Apart from this our study sheds light on using message passing in small systems and as subroutines in complete solvers.\n    ",
        "submission_date": "2012-02-12T00:00:00",
        "last_modified_date": "2012-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.2773",
        "title": "Decentralized Multi-agent Plan Repair in Dynamic Environments",
        "authors": [
            "Anton\u00edn Komenda",
            "Peter Nov\u00e1k",
            "Michal P\u011bchou\u010dek"
        ],
        "abstract": "Achieving joint objectives by teams of cooperative planning agents requires significant coordination and communication efforts. For a single-agent system facing a plan failure in a dynamic environment, arguably, attempts to repair the failed plan in general do not straightforwardly bring any benefit in terms of time complexity. However, in multi-agent settings the communication complexity might be of a much higher importance, possibly a high communication overhead might be even prohibitive in certain domains. We hypothesize that in decentralized systems, where coordination is enforced to achieve joint objectives, attempts to repair failed multi-agent plans should lead to lower communication overhead than replanning from scratch.\n",
        "submission_date": "2012-02-13T00:00:00",
        "last_modified_date": "2012-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.2892",
        "title": "Recommender System Based on Algorithm of Bicluster Analysis RecBi",
        "authors": [
            "Dmitry I. Ignatov",
            "Jonas Poelmans",
            "Vasily Zaharchuk"
        ],
        "abstract": "In this paper we propose two new algorithms based on biclustering analysis, which can be used at the basis of a recommender system for educational orientation of Russian School graduates. The first algorithm was designed to help students make a choice between different university faculties when some of their preferences are known. The second algorithm was developed for the special situation when nothing is known about their preferences. The final version of this recommender system will be used by Higher School of Economics.\n    ",
        "submission_date": "2012-02-13T00:00:00",
        "last_modified_date": "2012-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.2895",
        "title": "Concept Relation Discovery and Innovation Enabling Technology (CORDIET)",
        "authors": [
            "Jonas Poelmans",
            "Paul Elzinga",
            "Alexey Neznanov",
            "Stijn Viaene",
            "Sergei O. Kuznetsov",
            "Dmitry Ignatov",
            "Guido Dedene"
        ],
        "abstract": "Concept Relation Discovery and Innovation Enabling Technology (CORDIET), is a toolbox for gaining new knowledge from unstructured text data. At the core of CORDIET is the C-K theory which captures the essential elements of innovation. The tool uses Formal Concept Analysis (FCA), Emergent Self Organizing Maps (ESOM) and Hidden Markov Models (HMM) as main artifacts in the analysis process. The user can define temporal, text mining and compound attributes. The text mining attributes are used to analyze the unstructured text in documents, the temporal attributes use these document's timestamps for analysis. The compound attributes are XML rules based on text mining and temporal attributes. The user can cluster objects with object-cluster rules and can chop the data in pieces with segmentation rules. The artifacts are optimized for efficient data analysis; object labels in the FCA lattice and ESOM map contain an URL on which the user can click to open the selected document.\n    ",
        "submission_date": "2012-02-13T00:00:00",
        "last_modified_date": "2012-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3335",
        "title": "An efficient high-quality hierarchical clustering algorithm for automatic inference of software architecture from the source code of a software system",
        "authors": [
            "Sarge Rogatch"
        ],
        "abstract": "It is a high-quality algorithm for hierarchical clustering of large software source code. This effectively allows to break the complexity of tens of millions lines of source code, so that a human software engineer can comprehend a software system at high level by means of looking at its architectural diagram that is reconstructed automatically from the source code of the software system. The architectural diagram shows a tree of subsystems having OOP classes in its leaves (in the other words, a nested software decomposition). The tool reconstructs the missing (inconsistent/incomplete/inexistent) architectural documentation for a software system from its source code. This facilitates software maintenance: change requests can be performed substantially faster. Simply speaking, this unique tool allows to lift the comprehensible grain of object-oriented software systems from OOP class-level to subsystem-level. It is estimated that a commercial tool, developed on the basis of this work, will reduce software maintenance expenses 10 times on the current needs, and will allow to implement next-generation software systems which are currently too complex to be within the range of human comprehension, therefore can't yet be designed or implemented. Implemented prototype in Open Source: ",
        "submission_date": "2012-02-15T00:00:00",
        "last_modified_date": "2012-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3602",
        "title": "Towards quantitative measures in applied ontology",
        "authors": [
            "Robert Hoehndorf",
            "Michel Dumontier",
            "Georgios V. Gkoutos"
        ],
        "abstract": "Applied ontology is a relatively new field which aims to apply theories and methods from diverse disciplines such as philosophy, cognitive science, linguistics and formal logics to perform or improve domain-specific tasks. To support the development of effective research methodologies for applied ontology, we critically discuss the question how its research results should be evaluated. We propose that results in applied ontology must be evaluated within their domain of application, based on some ontology-based task within the domain, and discuss quantitative measures which would facilitate the objective evaluation and comparison of research results in applied ontology.\n    ",
        "submission_date": "2012-02-16T00:00:00",
        "last_modified_date": "2012-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3698",
        "title": "Extended Lifted Inference with Joint Formulas",
        "authors": [
            "Udi Apsel",
            "Ronen I. Brafman"
        ],
        "abstract": "The First-Order Variable Elimination (FOVE) algorithm allows exact inference to be applied directly to probabilistic relational models, and has proven to be vastly superior to the application of standard inference methods on a grounded propositional model. Still, FOVE operators can be applied under restricted conditions, often forcing one to resort to propositional inference. This paper aims to extend the applicability of FOVE by providing two new model conversion operators: the first and the primary is joint formula conversion and the second is just-different counting conversion. These new operations allow efficient inference methods to be applied directly on relational models, where no existing efficient method could be applied hitherto. In addition, aided by these capabilities, we show how to adapt FOVE to provide exact solutions to Maximum Expected Utility (MEU) queries over relational models for decision under uncertainty. Experimental evaluations show our algorithms to provide significant speedup over the alternatives.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3699",
        "title": "Learning is planning: near Bayes-optimal reinforcement learning via Monte-Carlo tree search",
        "authors": [
            "John Asmuth",
            "Michael L. Littman"
        ],
        "abstract": "Bayes-optimal behavior, while well-defined, is often difficult to achieve. Recent advances in the use of Monte-Carlo tree search (MCTS) have shown that it is possible to act near-optimally in Markov Decision Processes (MDPs) with very large or infinite state spaces. Bayes-optimal behavior in an unknown MDP is equivalent to optimal behavior in the known belief-space MDP, although the size of this belief-space MDP grows exponentially with the amount of history retained, and is potentially infinite. We show how an agent can use one particular MCTS algorithm, Forward Search Sparse Sampling (FSSS), in an efficient way to act nearly Bayes-optimally for all but a polynomial number of steps, assuming that FSSS can be used to act efficiently in any possible underlying MDP.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3707",
        "title": "A temporally abstracted Viterbi algorithm",
        "authors": [
            "Shaunak Chatterjee",
            "Stuart Russell"
        ],
        "abstract": "Hierarchical problem abstraction, when applicable, may offer exponential reductions in computational complexity. Previous work on coarse-to-fine dynamic programming (CFDP) has demonstrated this possibility using state abstraction to speed up the Viterbi algorithm. In this paper, we show how to apply temporal abstraction to the Viterbi problem. Our algorithm uses bounds derived from analysis of coarse timescales to prune large parts of the state trellis at finer timescales. We demonstrate improvements of several orders of magnitude over the standard Viterbi algorithm, as well as significant speedups over CFDP, for problems whose state variables evolve at widely differing rates.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3709",
        "title": "EDML: A Method for Learning Parameters in Bayesian Networks",
        "authors": [
            "Arthur Choi",
            "Khaled S. Refaat",
            "Adnan Darwiche"
        ],
        "abstract": "We propose a method called EDML for learning MAP parameters in binary Bayesian networks under incomplete data. The method assumes Beta priors and can be used to learn maximum likelihood parameters when the priors are uninformative. EDML exhibits interesting behaviors, especially when compared to EM. We introduce EDML, explain its origin, and study some of its properties both analytically and empirically.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3711",
        "title": "A Logical Characterization of Constraint-Based Causal Discovery",
        "authors": [
            "Tom Claassen",
            "Tom Heskes"
        ],
        "abstract": "We present a novel approach to constraint-based causal discovery, that takes the form of straightforward logical inference, applied to a list of simple, logical statements about causal relations that are derived directly from observed (in)dependencies. It is both sound and complete, in the sense that all invariant features of the corresponding partial ancestral graph (PAG) are identified, even in the presence of latent variables and selection bias. The approach shows that every identifiable causal relation corresponds to one of just two fundamental forms. More importantly, as the basic building blocks of the method do not rely on the detailed (graphical) structure of the corresponding PAG, it opens up a range of new opportunities, including more robust inference, detailed accountability, and application to large models.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3713",
        "title": "Bayesian network learning with cutting planes",
        "authors": [
            "James Cussens"
        ],
        "abstract": "The problem of learning the structure of Bayesian networks from complete discrete data with a limit on parent set size is considered. Learning is cast explicitly as an optimisation problem where the goal is to find a BN structure which maximises log marginal likelihood (BDe score). Integer programming, specifically the SCIP framework, is used to solve this optimisation problem. Acyclicity constraints are added to the integer program (IP) during solving in the form of cutting planes. Finding good cutting planes is the key to the success of the approach -the search for such cutting planes is effected using a sub-IP. Results show that this is a particularly fast method for exact BN learning.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3718",
        "title": "On the Complexity of Decision Making in Possibilistic Decision Trees",
        "authors": [
            "Helene Fargier",
            "Nahla Ben Amor",
            "Wided Guezguez"
        ],
        "abstract": "When the information about uncertainty cannot be quantified in a simple, probabilistic way, the topic of possibilistic decision theory is often a natural one to consider. The development of possibilistic decision theory has lead to a series of possibilistic criteria, e.g pessimistic possibilistic qualitative utility, possibilistic likely dominance, binary possibilistic utility and possibilistic Choquet integrals. This paper focuses on sequential decision making in possibilistic decision trees. It proposes a complexity study of the problem of finding an optimal strategy depending on the monotonicity property of the optimization criteria which allows the application of dynamic programming that offers a polytime reduction of the decision problem. It also shows that possibilistic Choquet integrals do not satisfy this property, and that in this case the optimization problem is NP - hard.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3719",
        "title": "Inference in Probabilistic Logic Programs using Weighted CNF's",
        "authors": [
            "Daan Fierens",
            "Guy Van den Broeck",
            "Ingo Thon",
            "Bernd Gutmann",
            "Luc De Raedt"
        ],
        "abstract": "Probabilistic logic programs are logic programs in which some of the facts are annotated with probabilities. Several classical probabilistic inference tasks (such as MAP and computing marginals) have not yet received a lot of attention for this formalism. The contribution of this paper is that we develop efficient inference algorithms for these tasks. This is based on a conversion of the probabilistic logic program and the query and evidence to a weighted CNF formula. This allows us to reduce the inference tasks to well-studied tasks such as weighted model counting. To solve such tasks, we employ state-of-the-art methods. We consider multiple methods for the conversion of the programs as well as for inference on the weighted CNF. The resulting approach is evaluated experimentally and shown to improve upon the state-of-the-art in probabilistic logic programming.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3721",
        "title": "Dynamic consistency and decision making under vacuous belief",
        "authors": [
            "Phan H. Giang"
        ],
        "abstract": "The ideas about decision making under ignorance in economics are combined with the ideas about uncertainty representation in computer science. The combination sheds new light on the question of how artificial agents can act in a dynamically consistent manner. The notion of sequential consistency is formalized by adapting the law of iterated expectation for plausibility measures. The necessary and sufficient condition for a certainty equivalence operator for Nehring-Puppe's preference to be sequentially consistent is given. This result sheds light on the models of decision making under uncertainty.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3723",
        "title": "Approximation by Quantization",
        "authors": [
            "Vibhav Gogate",
            "Pedro Domingos"
        ],
        "abstract": "Inference in graphical models consists of repeatedly multiplying and summing out potentials. It is generally intractable because the derived potentials obtained in this way can be exponentially large. Approximate inference techniques such as belief propagation and variational methods combat this by simplifying the derived potentials, typically by dropping variables from them. We propose an alternate method for simplifying potentials: quantizing their values. Quantization causes different states of a potential to have the same value, and therefore introduces context-specific independencies that can be exploited to represent the potential more compactly. We use algebraic decision diagrams (ADDs) to do this efficiently. We apply quantization and ADD reduction to variable elimination and junction tree propagation, yielding a family of bounded approximate inference schemes. Our experimental tests show that our new schemes significantly outperform state-of-the-art approaches on many benchmark instances.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3724",
        "title": "Probabilistic Theorem Proving",
        "authors": [
            "Vibhav Gogate",
            "Pedro Domingos"
        ],
        "abstract": "Many representation schemes combining first-order logic and probability have been proposed in recent years. Progress in unifying logical and probabilistic inference has been slower. Existing methods are mainly variants of lifted variable elimination and belief propagation, neither of which take logical structure into account. We propose the first method that has the full power of both graphical model inference and first-order theorem proving (in finite domains with Herbrand interpretations). We first define probabilistic theorem proving, their generalization, as the problem of computing the probability of a logical formula given the probabilities or weights of a set of formulas. We then show how this can be reduced to the problem of lifted weighted model counting, and develop an efficient algorithm for the latter. We prove the correctness of this algorithm, investigate its properties, and show how it generalizes previous approaches. Experiments show that it greatly outperforms lifted variable elimination when logical structure is present. Finally, we propose an algorithm for approximate probabilistic theorem proving, and show that it can greatly outperform lifted belief propagation.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3728",
        "title": "Reasoning about RoboCup Soccer Narratives",
        "authors": [
            "Hannaneh Hajishirzi",
            "Julia Hockenmaier",
            "Erik T. Mueller",
            "Eyal Amir"
        ],
        "abstract": "This paper presents an approach for learning to translate simple narratives, i.e., texts (sequences of sentences) describing dynamic systems, into coherent sequences of events without the need for labeled training data. Our approach incorporates domain knowledge in the form of preconditions and effects of events, and we show that it outperforms state-of-the-art supervised learning systems on the task of reconstructing RoboCup soccer games from their commentaries.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3729",
        "title": "Suboptimality Bounds for Stochastic Shortest Path Problems",
        "authors": [
            "Eric A. Hansen"
        ],
        "abstract": "We consider how to use the Bellman residual of the dynamic programming operator to compute suboptimality bounds for solutions to stochastic shortest path problems. Such bounds have been previously established only in the special case that \"all policies are proper,\" in which case the dynamic programming operator is known to be a contraction, and have been shown to be easily computable only in the more limited special case of discounting. Under the condition that transition costs are positive, we show that suboptimality bounds can be easily computed even when not all policies are proper. In the general case when there are no restrictions on transition costs, the analysis is more complex. But we present preliminary results that show such bounds are possible.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3739",
        "title": "Message-Passing Algorithms for Quadratic Programming Formulations of MAP Estimation",
        "authors": [
            "Akshat Kumar",
            "Shlomo Zilberstein"
        ],
        "abstract": "Computing maximum a posteriori (MAP) estimation in graphical models is an important inference problem with many applications. We present message-passing algorithms for quadratic programming (QP) formulations of MAP estimation for pairwise Markov random fields. In particular, we use the concave-convex procedure (CCCP) to obtain a locally optimal algorithm for the non-convex QP formulation. A similar technique is used to derive a globally convergent algorithm for the convex QP relaxation of MAP. We also show that a recently developed expectation-maximization (EM) algorithm for the QP formulation of MAP can be derived from the CCCP perspective. Experiments on synthetic and real-world problems confirm that our new approach is competitive with max-product and its variations. Compared with CPLEX, we achieve more than an order-of-magnitude speedup in solving optimally the convex QP relaxation.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3740",
        "title": "An Efficient Protocol for Negotiation over Combinatorial Domains with Incomplete Information",
        "authors": [
            "Minyi Li",
            "Quoc Bao Vo",
            "Ryszard Kowalczyk"
        ],
        "abstract": "We study the problem of agent-based negotiation in combinatorial domains. It is difficult to reach optimal agreements in bilateral or multi-lateral negotiations when the agents' preferences for the possible alternatives are not common knowledge. Self-interested agents often end up negotiating inefficient agreements in such situations. In this paper, we present a protocol for negotiation in combinatorial domains which can lead rational agents to reach optimal agreements under incomplete information setting. Our proposed protocol enables the negotiating agents to identify efficient solutions using distributed search that visits only a small subspace of the whole outcome space. Moreover, the proposed protocol is sufficiently general that it is applicable to most preference representation models in combinatorial domains. We also present results of experiments that demonstrate the feasibility and computational efficiency of our approach.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3741",
        "title": "Noisy Search with Comparative Feedback",
        "authors": [
            "Shiau Hong Lim",
            "Peter Auer"
        ],
        "abstract": "We present theoretical results in terms of lower and upper bounds on the query complexity of noisy search with comparative feedback. In this search model, the noise in the feedback depends on the distance between query points and the search target. Consequently, the error probability in the feedback is not fixed but varies for the queries posed by the search algorithm. Our results show that a target out of n items can be found in O(log n) queries. We also show the surprising result that for k possible answers per query, the speedup is not log k (as for k-ary search) but only log log k in some cases.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3743",
        "title": "Belief change with noisy sensing in the situation calculus",
        "authors": [
            "Jianbing Ma",
            "Weiru Liu",
            "Paul Miller"
        ],
        "abstract": "Situation calculus has been applied widely in artificial intelligence to model and reason about actions and changes in dynamic systems. Since actions carried out by agents will cause constant changes of the agents' beliefs, how to manage these changes is a very important issue. Shapiro et al. [22] is one of the studies that considered this issue. However, in this framework, the problem of noisy sensing, which often presents in real-world applications, is not considered. As a consequence, noisy sensing actions in this framework will lead to an agent facing inconsistent situation and subsequently the agent cannot proceed further. In this paper, we investigate how noisy sensing actions can be handled in iterated belief change within the situation calculus formalism. We extend the framework proposed in [22] with the capability of managing noisy sensings. We demonstrate that an agent can still detect the actual situation when the ratio of noisy sensing actions vs. accurate sensing actions is limited. We prove that our framework subsumes the iterated belief change strategy in [22] when all sensing actions are accurate. Furthermore, we prove that our framework can adequately handle belief introspection, mistaken beliefs, belief revision and belief update even with noisy sensing, as done in [22] with accurate sensing actions only. \n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3744",
        "title": "Improving the Scalability of Optimal Bayesian Network Learning with External-Memory Frontier Breadth-First Branch and Bound Search",
        "authors": [
            "Brandon Malone",
            "Changhe Yuan",
            "Eric A. Hansen",
            "Susan Bridges"
        ],
        "abstract": "Previous work has shown that the problem of learning the optimal structure of a Bayesian network can be formulated as a shortest path finding problem in a graph and solved using A* search. In this paper, we improve the scalability of this approach by developing a memory-efficient heuristic search algorithm for learning the structure of a Bayesian network. Instead of using A*, we propose a frontier breadth-first branch and bound search that leverages the layered structure of the search graph of this problem so that no more than two layers of the graph, plus solution reconstruction information, need to be stored in memory at a time. To further improve scalability, the algorithm stores most of the graph in external memory, such as hard disk, when it does not fit in RAM. Experimental results show that the resulting algorithm solves significantly larger problems than the current state of the art.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3745",
        "title": "Order-of-Magnitude Influence Diagrams",
        "authors": [
            "Radu Marinescu",
            "Nic Wilson"
        ],
        "abstract": "In this paper, we develop a qualitative theory of influence diagrams that can be used to model and solve sequential decision making tasks when only qualitative (or imprecise) information is available. Our approach is based on an order-of-magnitude approximation of both probabilities and utilities and allows for specifying partially ordered preferences via sets of utility values. We also propose a dedicated variable elimination algorithm that can be applied for solving order-of-magnitude influence diagrams.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3749",
        "title": "Compact Mathematical Programs For DEC-MDPs With Structured Agent Interactions",
        "authors": [
            "Hala Mostafa",
            "Victor Lesser"
        ],
        "abstract": "To deal with the prohibitive complexity of calculating policies in Decentralized MDPs, researchers have proposed models that exploit structured agent interactions. Settings where most agent actions are independent except for few actions that affect the transitions and/or rewards of other agents can be modeled using Event-Driven Interactions with Complex Rewards (EDI-CR). Finding the optimal joint policy can be formulated as an optimization problem. However, existing formulations are too verbose and/or lack optimality guarantees. We propose a compact Mixed Integer Linear Program formulation of EDI-CR instances. The key insight is that most action sequences of a group of agents have the same effect on a given agent. This allows us to treat these sequences similarly and use fewer variables. Experiments show that our formulation is more compact and leads to faster solution times and better solutions than existing formulations.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3754",
        "title": "A Geometric Traversal Algorithm for Reward-Uncertain MDPs",
        "authors": [
            "Eunsoo Oh",
            "Kee-Eung Kim"
        ],
        "abstract": "Markov decision processes (MDPs) are widely used in modeling decision making problems in stochastic environments. However, precise specification of the reward functions in MDPs is often very difficult. Recent approaches have focused on computing an optimal policy based on the minimax regret criterion for obtaining a robust policy under uncertainty in the reward function. One of the core tasks in computing the minimax regret policy is to obtain the set of all policies that can be optimal for some candidate reward function. In this paper, we propose an efficient algorithm that exploits the geometric properties of the reward function associated with the policies. We also present an approximate version of the method for further speed up. We experimentally demonstrate that our algorithm improves the performance by orders of magnitude.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3759",
        "title": "Compressed Inference for Probabilistic Sequential Models",
        "authors": [
            "Gungor Polatkan",
            "Oncel Tuzel"
        ],
        "abstract": "Hidden Markov models (HMMs) and conditional random fields (CRFs) are two popular techniques for modeling sequential data. Inference algorithms designed over CRFs and HMMs allow estimation of the state sequence given the observations. In several applications, estimation of the state sequence is not the end goal; instead the goal is to compute some function of it. In such scenarios, estimating the state sequence by conventional inference techniques, followed by computing the functional mapping from the estimate is not necessarily optimal. A more formal approach is to directly infer the final outcome from the observations. In particular, we consider the specific instantiation of the problem where the goal is to find the state trajectories without exact transition points and derive a novel polynomial time inference algorithm that outperforms vanilla inference techniques. We show that this particular problem arises commonly in many disparate applications and present experiments on three of them: (1) Toy robot tracking; (2) Single stroke character recognition; (3) Handwritten word recognition.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3762",
        "title": "Symbolic Dynamic Programming for Discrete and Continuous State MDPs",
        "authors": [
            "Scott Sanner",
            "Karina Valdivia Delgado",
            "Leliane Nunes de Barros"
        ],
        "abstract": "Many real-world decision-theoretic planning problems can be naturally modeled with discrete and continuous state Markov decision processes (DC-MDPs). While previous work has addressed automated decision-theoretic planning for DCMDPs, optimal solutions have only been defined so far for limited settings, e.g., DC-MDPs having hyper-rectangular piecewise linear value functions. In this work, we extend symbolic dynamic programming (SDP) techniques to provide optimal solutions for a vastly expanded class of DCMDPs. To address the inherent combinatorial aspects of SDP, we introduce the XADD - a continuous variable extension of the algebraic decision diagram (ADD) - that maintains compact representations of the exact value function. Empirically, we demonstrate an implementation of SDP with XADDs on various DC-MDPs, showing the first optimal automated solutions to DCMDPs with linear and nonlinear piecewise partitioned value functions and showing the advantages of constraint-based pruning for XADDs.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3764",
        "title": "Adjustment Criteria in Causal Diagrams: An Algorithmic Perspective",
        "authors": [
            "Johannes Textor",
            "Maciej Liskiewicz"
        ],
        "abstract": "Identifying and controlling bias is a key problem in empirical sciences. Causal diagram theory provides graphical criteria for deciding whether and how causal effects can be identified from observed (nonexperimental) data by covariate adjustment. Here we prove equivalences between existing as well as new criteria for adjustment and we provide a new simplified but still equivalent notion of d-separation. These lead to efficient algorithms for two important tasks in causal diagram analysis: (1) listing minimal covariate adjustments (with polynomial delay); and (2) identifying the subdiagram involved in biasing paths (in linear time). Our results improve upon existing exponential-time solutions for these problems, enabling users to assess the effects of covariate adjustment on diagrams with tens to hundreds of variables interactively in real time.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3767",
        "title": "Distributed Anytime MAP Inference",
        "authors": [
            "Joop van de Ven",
            "Fabio Ramos"
        ],
        "abstract": "We present a distributed anytime algorithm for performing MAP inference in graphical models. The problem is formulated as a linear programming relaxation over the edges of a graph. The resulting program has a constraint structure that allows application of the Dantzig-Wolfe decomposition principle. Subprograms are defined over individual edges and can be computed in a distributed manner. This accommodates solutions to graphs whose state space does not fit in memory. The decomposition master program is guaranteed to compute the optimal solution in a finite number of iterations, while the solution converges monotonically with each iteration. Formulating the MAP inference problem as a linear program allows additional (global) constraints to be defined; something not possible with message passing algorithms. Experimental results show that our algorithm's solution quality outperforms most current algorithms and it scales well to large problems.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3773",
        "title": "Measuring the Hardness of Stochastic Sampling on Bayesian Networks with Deterministic Causalities: the k-Test",
        "authors": [
            "Haohai Yu",
            "Robert A. van Engelen"
        ],
        "abstract": "Approximate Bayesian inference is NP-hard. Dagum and Luby defined the Local Variance Bound (LVB) to measure the approximation hardness of Bayesian inference on Bayesian networks, assuming the networks model strictly positive joint probability distributions, i.e. zero probabilities are not permitted. This paper introduces the k-test to measure the approximation hardness of inference on Bayesian networks with deterministic causalities in the probability distribution, i.e. when zero conditional probabilities are permitted. Approximation by stochastic sampling is a widely-used inference method that is known to suffer from inefficiencies due to sample rejection. The k-test predicts when rejection rates of stochastic sampling a Bayesian network will be low, modest, high, or when sampling is intractable.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3777",
        "title": "Belief Propagation by Message Passing in Junction Trees: Computing Each Message Faster Using GPU Parallelization",
        "authors": [
            "Lu Zheng",
            "Ole Mengshoel",
            "Jike Chong"
        ],
        "abstract": "Compiling Bayesian networks (BNs) to junction trees and performing belief propagation over them is among the most prominent approaches to computing posteriors in BNs. However, belief propagation over junction tree is known to be computationally intensive in the general case. Its complexity may increase dramatically with the connectivity and state space cardinality of Bayesian network nodes. In this paper, we address this computational challenge using GPU parallelization. We develop data structures and algorithms that extend existing junction tree techniques, and specifically develop a novel approach to computing each belief propagation message in parallel. We implement our approach on an NVIDIA GPU and test it using BNs from several applications. Experimentally, we study how junction tree parameters affect parallelization opportunities and hence the performance of our algorithm. We achieve speedups ranging from 0.68 to 9.18 for the BNs studied.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3887",
        "title": "Extended Mixture of MLP Experts by Hybrid of Conjugate Gradient Method and Modified Cuckoo Search",
        "authors": [
            "Hamid Salimi",
            "Davar Giveki",
            "Mohammad Ali Soltanshahi",
            "Javad Hatami"
        ],
        "abstract": "This paper investigates a new method for improving the learning algorithm of Mixture of Experts (ME) model using a hybrid of Modified Cuckoo Search (MCS) and Conjugate Gradient (CG) as a second order optimization technique. The CG technique is combined with Back-Propagation (BP) algorithm to yield a much more efficient learning algorithm for ME structure. In addition, the experts and gating networks in enhanced model are replaced by CG based Multi-Layer Perceptrons (MLPs) to provide faster and more accurate learning. The CG is considerably depends on initial weights of connections of Artificial Neural Network (ANN), so, a metaheuristic algorithm, the so-called Modified Cuckoo Search is applied in order to select the optimal weights. The performance of proposed method is compared with Gradient Decent Based ME (GDME) and Conjugate Gradient Based ME (CGME) in classification and regression problems. The experimental results show that hybrid MSC and CG based ME (MCS-CGME) has faster convergence and better performance in utilized benchmark data sets.\n    ",
        "submission_date": "2012-02-17T00:00:00",
        "last_modified_date": "2012-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.4063",
        "title": "Comparing SVM and Naive Bayes classifiers for text categorization with Wikitology as knowledge enrichment",
        "authors": [
            "Sundus Hassan",
            "Muhammad Rafi",
            "Muhammad Shahid Shaikh"
        ],
        "abstract": "The activity of labeling of documents according to their content is known as text categorization. Many experiments have been carried out to enhance text categorization by adding background knowledge to the document using knowledge repositories like Word Net, Open Project Directory (OPD), Wikipedia and Wikitology. In our previous work, we have carried out intensive experiments by extracting knowledge from Wikitology and evaluating the experiment on Support Vector Machine with 10- fold cross-validations. The results clearly indicate Wikitology is far better than other knowledge bases. In this paper we are comparing Support Vector Machine (SVM) and Na\u00efve Bayes (NB) classifiers under text enrichment through Wikitology. We validated results with 10-fold cross validation and shown that NB gives an improvement of +28.78%, on the other hand SVM gives an improvement of +6.36% when compared with baseline results. Na\u00efve Bayes classifier is better choice when external enriching is used through any external knowledge base.\n    ",
        "submission_date": "2012-02-18T00:00:00",
        "last_modified_date": "2012-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.4190",
        "title": "Generalized FMD Detection for Spectrum Sensing Under Low Signal-to-Noise Ratio",
        "authors": [
            "Feng Lin",
            "Robert C. Qiu",
            "Zhen Hu",
            "Shujie Hou",
            "James P. Browning",
            "Michael C. Wicks"
        ],
        "abstract": "Spectrum sensing is a fundamental problem in cognitive radio. We propose a function of covariance matrix based detection algorithm for spectrum sensing in cognitive radio network. Monotonically increasing property of function of matrix involving trace operation is utilized as the cornerstone for this algorithm. The advantage of proposed algorithm is it works under extremely low signal-to-noise ratio, like lower than -30 dB with limited sample data. Theoretical analysis of threshold setting for the algorithm is discussed. A performance comparison between the proposed algorithm and other state-of-the-art methods is provided, by the simulation on captured digital television (DTV) signal.\n    ",
        "submission_date": "2012-02-19T00:00:00",
        "last_modified_date": "2012-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.4828",
        "title": "Towards an Intelligent Tutor for Mathematical Proofs",
        "authors": [
            "Serge Autexier",
            "Dominik Dietrich",
            "Marvin Schiller"
        ],
        "abstract": "Computer-supported learning is an increasingly important form of study since it allows for independent learning and individualized instruction.  In this paper, we discuss a novel approach to developing an intelligent tutoring system for teaching textbook-style mathematical proofs.  We characterize the particularities of the domain and discuss common ITS design models.  Our approach is motivated by phenomena found in a corpus of tutorial dialogs that were collected in a Wizard-of-Oz experiment.  We show how an intelligent tutor for textbook-style mathematical proofs can be built on top of an adapted assertion-level proof assistant by reusing representations and proof search strategies originally developed for automated and interactive theorem proving.  The resulting prototype was successfully evaluated on a corpus of tutorial dialogs and yields good results.\n\n    ",
        "submission_date": "2012-02-22T00:00:00",
        "last_modified_date": "2012-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.5597",
        "title": "Hybrid Batch Bayesian Optimization",
        "authors": [
            "Javad Azimi",
            "Ali Jalali",
            "Xiaoli Fern"
        ],
        "abstract": "Bayesian Optimization aims at optimizing an unknown non-convex/concave function that is costly to evaluate. We are interested in application scenarios where concurrent function evaluations are possible. Under such a setting, BO could choose to either sequentially evaluate the function, one input at a time and wait for the output of the function before making the next selection, or evaluate the function at a batch of multiple inputs at once. These two different settings are commonly referred to as the sequential and batch settings of Bayesian Optimization. In general, the sequential setting leads to better optimization performance as each function evaluation is selected with more information, whereas the batch setting has an advantage in terms of the total experimental time (the number of iterations). In this work, our goal is to combine the strength of both settings. Specifically, we systematically analyze Bayesian optimization using Gaussian process as the posterior estimator and provide a hybrid algorithm that, based on the current state, dynamically switches between a sequential policy and a batch policy with variable batch sizes. We provide theoretical justification for our algorithm and present experimental results on eight benchmark BO problems. The results show that our method achieves substantial speedup (up to %78) compared to a pure sequential policy, without suffering any significant performance loss.\n    ",
        "submission_date": "2012-02-25T00:00:00",
        "last_modified_date": "2012-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.5600",
        "title": "Interaction Histories and Short Term Memory: Enactive Development of Turn-taking Behaviors in a Childlike Humanoid Robot",
        "authors": [
            "Frank Broz",
            "Chrystopher L. Nehaniv",
            "Hatice Kose-Bagci",
            "Kerstin Dautenhahn"
        ],
        "abstract": "In this article, an enactive architecture is described that allows a humanoid robot to learn to compose simple actions into turn-taking behaviors while playing interaction games with a human partner. The robot's action choices are reinforced by social feedback from the human in the form of visual attention and measures of behavioral synchronization. We demonstrate that the system can acquire and switch between behaviors learned through interaction based on social feedback from the human partner. The role of reinforcement based on a short term memory of the interaction is experimentally investigated. Results indicate that feedback based only on the immediate state is insufficient to learn certain turn-taking behaviors. Therefore some history of the interaction must be considered in the acquisition of turn-taking, which can be efficiently handled through the use of short term memory.\n    ",
        "submission_date": "2012-02-25T00:00:00",
        "last_modified_date": "2012-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.6009",
        "title": "Marginality: a numerical mapping for enhanced treatment of nominal and hierarchical attributes",
        "authors": [
            "Josep Domingo-Ferrer"
        ],
        "abstract": "The purpose of statistical disclosure control (SDC) of microdata, a.k.a. data anonymization or privacy-preserving data mining, is to publish data sets containing the answers of individual respondents in such a way that the respondents corresponding to the released records cannot be re-identified and the released data are analytically useful. SDC methods are either based on masking the original data, generating synthetic versions of them or creating hybrid versions by combining original and synthetic data. The choice of SDC methods for categorical data, especially nominal data, is much smaller than the choice of methods for numerical data. We mitigate this problem by introducing a numerical mapping for hierarchical nominal data which allows computing means, variances and covariances on them.\n    ",
        "submission_date": "2012-02-27T00:00:00",
        "last_modified_date": "2012-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.6079",
        "title": "Synthesising Graphical Theories",
        "authors": [
            "Aleks Kissinger"
        ],
        "abstract": "In recent years, diagrammatic languages have been shown to be a powerful and expressive tool for reasoning about physical, logical, and semantic processes represented as morphisms in a monoidal category. In particular, categorical quantum mechanics, or \"Quantum Picturalism\", aims to turn concrete features of quantum theory into abstract structural properties, expressed in the form of diagrammatic identities. One way we search for these properties is to start with a concrete model (e.g. a set of linear maps or finite relations) and start composing generators into diagrams and looking for graphical identities.\n",
        "submission_date": "2012-02-27T00:00:00",
        "last_modified_date": "2012-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.6153",
        "title": "One Decade of Universal Artificial Intelligence",
        "authors": [
            "Marcus Hutter"
        ],
        "abstract": "The first decade of this century has seen the nascency of the first mathematical theory of general artificial intelligence. This theory of Universal Artificial Intelligence (UAI) has made significant contributions to many theoretical, philosophical, and practical AI questions. In a series of papers culminating in book (Hutter, 2005), an exciting sound and complete mathematical model for a super intelligent agent (AIXI) has been developed and rigorously analyzed. While nowadays most AI researchers avoid discussing intelligence, the award-winning PhD thesis (Legg, 2008) provided the philosophical embedding and investigated the UAI-based universal measure of rational intelligence, which is formal, objective and non-anthropocentric. Recently, effective approximations of AIXI have been derived and experimentally investigated in JAIR paper (Veness et al. 2011). This practical breakthrough has resulted in some impressive applications, finally muting earlier critique that UAI is only a theory. For the first time, without providing any domain knowledge, the same agent is able to self-adapt to a diverse range of interactive environments. For instance, AIXI is able to learn from scratch to play TicTacToe, Pacman, Kuhn Poker, and other games by trial and error, without even providing the rules of the games.\n",
        "submission_date": "2012-02-28T00:00:00",
        "last_modified_date": "2012-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.6177",
        "title": "Can Intelligence Explode?",
        "authors": [
            "Marcus Hutter"
        ],
        "abstract": "The technological singularity refers to a hypothetical scenario in which technological advances virtually explode. The most popular scenario is the creation of super-intelligent algorithms that recursively create ever higher intelligences. It took many decades for these ideas to spread from science fiction to popular science magazines and finally to attract the attention of serious philosophers. David Chalmers' (JCS 2010) article is the first comprehensive philosophical analysis of the singularity in a respected philosophy journal. The motivation of my article is to augment Chalmers' and to discuss some issues not addressed by him, in particular what it could mean for intelligence to explode. In this course, I will (have to) provide a more careful treatment of what intelligence actually is, separate speed from intelligence explosion, compare what super-intelligent participants and classical human observers might experience and do, discuss immediate implications for the diversity and value of life, consider possible bounds on intelligence, and contemplate intelligences right at the singularity.\n    ",
        "submission_date": "2012-02-28T00:00:00",
        "last_modified_date": "2012-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.6386",
        "title": "Relational Reinforcement Learning in Infinite Mario",
        "authors": [
            "Shiwali Mohan",
            "John E. Laird"
        ],
        "abstract": "Relational representations in reinforcement learning allow for the use of structural information like the presence of objects and relationships between them in the description of value functions. Through this paper, we show that such representations allow for the inclusion of background knowledge that qualitatively describes a state and can be used to design agents that demonstrate learning behavior in domains with large state and actions spaces such as computer games.\n    ",
        "submission_date": "2012-02-28T00:00:00",
        "last_modified_date": "2012-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.6609",
        "title": "Towards an Integrated Visualization Of Semantically Enriched 3D City Models: An Ontology of 3D Visualization Techniques",
        "authors": [
            "Claudine M\u00e9tral",
            "Nizar Ghoula",
            "Gilles Falquet"
        ],
        "abstract": "3D city models - which represent in 3 dimensions the geometric elements of a city - are increasingly used for an intended wide range of applications. Such uses are made possible by using semantically enriched 3D city models and by presenting such enriched 3D city models in a way that allows decision-making processes to be carried out from the best choices among sets of objectives, and across issues and scales. In order to help in such a decision-making process we have defined a framework to find the best visualization technique(s) for a set of potentially heterogeneous data that have to be visualized within the same 3D city model, in order to perform a given task in a specific context. We have chosen an ontology-based approach. This approach and the specification and use of the resulting ontology of 3D visualization techniques are described in this paper.\n    ",
        "submission_date": "2012-02-29T00:00:00",
        "last_modified_date": "2012-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.0088",
        "title": "The Mind Grows Circuits",
        "authors": [
            "Rina Panigrahy",
            "Li Zhang"
        ],
        "abstract": "There is a vast supply of prior art that study models for mental processes. Some studies in psychology and philosophy approach it from an inner perspective in terms of experiences and percepts. Others such as neurobiology or connectionist-machines approach it externally by viewing the mind as complex circuit of neurons where each neuron is a primitive binary circuit. In this paper, we also model the mind as a place where a circuit grows, starting as a collection of primitive components at birth and then builds up incrementally in a bottom up fashion. A new node is formed by a simple composition of prior nodes when we undergo a repeated experience that can be described by that composition. Unlike neural networks, however, these circuits take \"concepts\" or \"percepts\" as inputs and outputs. Thus the growing circuits can be likened to a growing collection of lambda expressions that are built on top of one another in an attempt to compress the sensory input as a heuristic to bound its Kolmogorov Complexity.\n    ",
        "submission_date": "2012-03-01T00:00:00",
        "last_modified_date": "2012-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.0220",
        "title": "The Equational Approach to CF2 Semantics",
        "authors": [
            "Dov M. Gabbay"
        ],
        "abstract": "We introduce a family of new equational semantics for argumentation networks which can handle odd and even loops in a uniform manner. We offer one version of equational semantics which is equivalent to CF2 semantics, and a better version which gives the same results as traditional Dung semantics for even loops but can still handle odd loops.\n    ",
        "submission_date": "2012-03-01T00:00:00",
        "last_modified_date": "2012-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.0436",
        "title": "(Dual) Hoops Have Unique Halving",
        "authors": [
            "Rob Arthan",
            "Paulo Oliva"
        ],
        "abstract": "Continuous logic extends the multi-valued Lukasiewicz logic by adding a halving operator on propositions. This extension is designed to give a more satisfactory model theory for continuous structures. The semantics of these logics can be given using specialisations of algebraic structures known as hoops. As part of an investigation into the metatheory of propositional continuous logic, we were indebted to Prover9 for finding a proof of an important algebraic law.\n    ",
        "submission_date": "2012-03-02T00:00:00",
        "last_modified_date": "2013-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.0656",
        "title": "Contribution of Case Based Reasoning (CBR) in the Exploitation of Return of Experience. Application to Accident Scenarii in Railroad Transport",
        "authors": [
            "Ahmed Maalel",
            "Habib Hadj-Mabrouk"
        ],
        "abstract": "The study is from a base of accident scenarii in rail transport (feedback) in order to develop a tool to share build and sustain knowledge and safety and secondly to exploit the knowledge stored to prevent the reproduction of accidents / incidents. This tool should ultimately lead to the proposal of prevention and protection measures to minimize the risk level of a new transport system and thus to improve safety. The approach to achieving this goal largely depends on the use of artificial intelligence techniques and rarely the use of a method of automatic learning in order to develop a feasibility model of a software tool based on case based reasoning (CBR) to exploit stored knowledge in order to create know-how that can help stimulate domain experts in the task of analysis, evaluation and certification of a new system.\n    ",
        "submission_date": "2012-03-03T00:00:00",
        "last_modified_date": "2012-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.0699",
        "title": "Ambiguous Language and Differences in Beliefs",
        "authors": [
            "Joseph Y. Halpern",
            "Willemien Kets"
        ],
        "abstract": "Standard models of multi-agent modal logic do not capture the fact that information is often ambiguous, and may be interpreted in different ways by different agents. We propose a framework that can model this, and consider different semantics that capture different assumptions about the agents' beliefs regarding whether or not there is ambiguity. We consider the impact of ambiguity on a seminal result in economics: Aumann's result saying that agents with a common prior cannot agree to disagree. This result is known not to hold if agents do not have a common prior; we show that it also does not hold in the presence of ambiguity. We then consider the tradeoff between assuming a common interpretation (i.e., no ambiguity) and a common prior (i.e., shared initial beliefs).\n    ",
        "submission_date": "2012-03-04T00:00:00",
        "last_modified_date": "2012-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.1021",
        "title": "Development of an Ontology to Assist the Modeling of Accident Scenarii \"Application on Railroad Transport \"",
        "authors": [
            "Ahmed Maalel",
            "Habib Hadj mabrouk",
            "Lassad Mejri",
            "Henda Hajjami Ben Ghezela"
        ],
        "abstract": "In a world where communication and information sharing are at the heart of our business, the terminology needs are most pressing. It has become imperative to identify the terms used and defined in a consensual and coherent way while preserving linguistic diversity. To streamline and strengthen the process of acquisition, representation and exploitation of scenarii of train accidents, it is necessary to harmonize and standardize the terminology used by players in the security field. The research aims to significantly improve analytical activities and operations of the various safety studies, by tracking the error in system, hardware, software and human. This paper presents the contribution of ontology to modeling scenarii for rail accidents through a knowledge model based on a generic ontology and domain ontology. After a detailed presentation of the state of the art material, this article presents the first results of the developed model.\n    ",
        "submission_date": "2012-03-05T00:00:00",
        "last_modified_date": "2012-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.1095",
        "title": "Search Combinators",
        "authors": [
            "Tom Schrijvers",
            "Guido Tack",
            "Pieter Wuille",
            "Horst Samulowitz",
            "Peter J. Stuckey"
        ],
        "abstract": "The ability to model search in a constraint solver can be an essential asset for solving combinatorial problems. However, existing infrastructure for defining search heuristics is often inadequate. Either modeling capabilities are extremely limited or users are faced with a general-purpose programming language whose features are not tailored towards writing search heuristics. As a result, major improvements in performance may remain unexplored.\n",
        "submission_date": "2012-03-06T00:00:00",
        "last_modified_date": "2012-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.1882",
        "title": "Multi source feedback based performance appraisal system using Fuzzy logic decision support system",
        "authors": [
            "G.Meenakshi"
        ],
        "abstract": "In Multi-Source Feedback or 360 Degree Feedback, data on the performance of an individual are collected systematically from a number of stakeholders and are used for improving performance. The 360-Degree Feedback approach provides a consistent management philosophy meeting the criterion outlined previously. The 360-degree feedback appraisal process describes a human resource methodology that is frequently used for both employee appraisal and employee development. Used in employee performance appraisals, the 360-degree feedback methodology is differentiated from traditional, top-down appraisal methods in which the supervisor responsible for the appraisal provides the majority of the data. Instead it seeks to use information gained from other sources to provide a fuller picture of employees' performances. Similarly, when this technique used in employee development it augments employees' perceptions of training needs with those of the people with whom they interact. The 360-degree feedback based appraisal is a comprehensive method where in the feedback about the employee comes from all the sources that come into contact with the employee on his/her job. The respondents for an employee can be her/his peers, managers, subordinates team members, customers, suppliers and vendors. Hence anyone who comes into contact with the employee, the 360 degree appraisal has four components that include self-appraisal, superior's appraisal, subordinate's appraisal student's appraisal and peer's appraisal .The proposed system is an attempt to implement the 360 degree feedback based appraisal system in academics especially engineering colleges.\n    ",
        "submission_date": "2012-03-08T00:00:00",
        "last_modified_date": "2012-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.2556",
        "title": "A Probabilistic Transmission Expansion Planning Methodology based on Roulette Wheel Selection and Social Welfare",
        "authors": [
            "Neeraj Gupta",
            "Rajiv Shekhar",
            "Prem Kumar Kalra"
        ],
        "abstract": "A new probabilistic methodology for transmission expansion planning (TEP) that does not require a priori specification of new/additional transmission capacities and uses the concept of social welfare has been proposed. Two new concepts have been introduced in this paper: (i) roulette wheel methodology has been used to calculate the capacity of new transmission lines and (ii) load flow analysis has been used to calculate expected demand not served (EDNS). The overall methodology has been implemented on a modified IEEE 5-bus test system. Simulations show an important result: addition of only new transmission lines is not sufficient to minimize EDNS.\n    ",
        "submission_date": "2012-03-12T00:00:00",
        "last_modified_date": "2012-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3051",
        "title": "Combining Voting Rules Together",
        "authors": [
            "Nina Narodytska",
            "Toby Walsh",
            "Lirong Xia"
        ],
        "abstract": "We propose a simple method for combining together voting rules that performs a run-off between the different winners of each voting rule. We prove that this combinator has several good properties. For instance, even if just one of the base voting rules has a desirable property like Condorcet consistency, the combination inherits this property. In addition, we prove that combining voting rules together in this way can make finding a manipulation more computationally difficult. Finally, we study the impact of this combinator on approximation methods that find close to optimal manipulations.\n    ",
        "submission_date": "2012-03-14T00:00:00",
        "last_modified_date": "2012-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3227",
        "title": "Generalisation of language and knowledge models for corpus analysis",
        "authors": [
            "Anton Loss"
        ],
        "abstract": "This paper takes new look on language and knowledge modelling for corpus linguistics. Using ideas of Chaitin, a line of argument is made against language/knowledge separation in Natural Language Processing. A simplistic model, that generalises approaches to language and knowledge, is proposed. One of hypothetical consequences of this model is Strong AI.\n    ",
        "submission_date": "2012-03-14T00:00:00",
        "last_modified_date": "2012-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3376",
        "title": "Learning, Social Intelligence and the Turing Test - why an \"out-of-the-box\" Turing Machine will not pass the Turing Test",
        "authors": [
            "Bruce Edmonds",
            "Carlos Gershenson"
        ],
        "abstract": "The Turing Test (TT) checks for human intelligence, rather than any putative general intelligence. It involves repeated interaction requiring learning in the form of adaption to the human conversation partner. It is a macro-level post-hoc test in contrast to the definition of a Turing Machine (TM), which is a prior micro-level definition. This raises the question of whether learning is just another computational process, i.e. can be implemented as a TM. Here we argue that learning or adaption is fundamentally different from computation, though it does involve processes that can be seen as computations. To illustrate this difference we compare (a) designing a TM and (b) learning a TM, defining them for the purpose of the argument. We show that there is a well-defined sequence of problems which are not effectively designable but are learnable, in the form of the bounded halting problem. Some characteristics of human intelligence are reviewed including it's: interactive nature, learning abilities, imitative tendencies, linguistic ability and context-dependency. A story that explains some of these is the Social Intelligence Hypothesis. If this is broadly correct, this points to the necessity of a considerable period of acculturation (social learning in context) if an artificial intelligence is to pass the TT. Whilst it is always possible to 'compile' the results of learning into a TM, this would not be a designed TM and would not be able to continually adapt (pass future TTs). We conclude three things, namely that: a purely \"designed\" TM will never pass the TT; that there is no such thing as a general intelligence since it necessary involves learning; and that learning/adaption and computation should be clearly distinguished.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3464",
        "title": "Gibbs Sampling in Open-Universe Stochastic Languages",
        "authors": [
            "Nimar S. Arora",
            "Rodrigo de Salvo Braz",
            "Erik B. Sudderth",
            "Stuart Russell"
        ],
        "abstract": "Languages for open-universe probabilistic models (OUPMs) can represent situations with an unknown number of objects and iden- tity uncertainty. While such cases arise in a wide range of important real-world appli- cations, existing general purpose inference methods for OUPMs are far less efficient than those available for more restricted lan- guages and model classes. This paper goes some way to remedying this deficit by in- troducing, and proving correct, a generaliza- tion of Gibbs sampling to partial worlds with possibly varying model structure. Our ap- proach draws on and extends previous generic OUPM inference methods, as well as aux- iliary variable samplers for nonparametric mixture models. It has been implemented for BLOG, a well-known OUPM language. Combined with compile-time optimizations, the resulting algorithm yields very substan- tial speedups over existing methods on sev- eral test cases, and substantially improves the practicality of OUPM languages generally.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3465",
        "title": "Compiling Possibilistic Networks: Alternative Approaches to Possibilistic Inference",
        "authors": [
            "Raouia Ayachi",
            "Nahla Ben Amor",
            "Salem Benferhat",
            "Rolf Haenni"
        ],
        "abstract": "Qualitative possibilistic networks, also known as min-based possibilistic networks, are important tools for handling uncertain information in the possibility theory frame- work. Despite their importance, only the junction tree adaptation has been proposed for exact reasoning with such networks. This paper explores alternative algorithms using compilation techniques. We first propose possibilistic adaptations of standard compilation-based probabilistic methods. Then, we develop a new, purely possibilistic, method based on the transformation of the initial network into a possibilistic base. A comparative study shows that this latter performs better than the possibilistic adap- tations of probabilistic methods. This result is also confirmed by experimental results.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3466",
        "title": "Possibilistic Answer Set Programming Revisited",
        "authors": [
            "Kim Bauters",
            "Steven Schockaert",
            "Martine De Cock",
            "Dirk Vermeir"
        ],
        "abstract": "Possibilistic answer set programming (PASP) extends answer set programming (ASP) by attaching to each rule a degree of certainty. While such an extension is important from an application point of view, existing semantics are not well-motivated, and do not always yield intuitive results. To develop a more suitable semantics, we first introduce a characterization of answer sets of classical ASP programs in terms of possibilistic logic where an ASP program specifies a set of constraints on possibility distributions. This characterization is then naturally generalized to define answer sets of PASP programs. We furthermore provide a syntactic counterpart, leading to a possibilistic generalization of the well-known Gelfond-Lifschitz reduct, and we show how our framework can readily be implemented using standard ASP solvers.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3467",
        "title": "Three new sensitivity analysis methods for influence diagrams",
        "authors": [
            "Debarun Bhattacharjya",
            "Ross D. Shachter"
        ],
        "abstract": "Performing sensitivity analysis for influence diagrams using the decision circuit framework is particularly convenient, since the partial derivatives with respect to every parameter are readily available [Bhattacharjya and Shachter, 2007; 2008]. In this paper we present three non-linear sensitivity analysis methods that utilize this partial derivative information and therefore do not require re-evaluating the decision situation multiple times. Specifically, we show how to efficiently compare strategies in decision situations, perform sensitivity to risk aversion and compute the value of perfect hedging [Seyller, 2008].\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3469",
        "title": "Probabilistic Similarity Logic",
        "authors": [
            "Matthias Brocheler",
            "Lilyana Mihalkova",
            "Lise Getoor"
        ],
        "abstract": "Many machine learning applications require the ability to learn from and reason about noisy multi-relational data. To address this, several effective representations have been developed that provide both a language for expressing the structural regularities of a domain, and principled support for probabilistic inference. In addition to these two aspects, however, many applications also involve a third aspect-the need to reason about similarities-which has not been directly supported in existing frameworks. This paper introduces probabilistic similarity logic (PSL), a general-purpose framework for joint reasoning about similarity in relational domains that incorporates probabilistic reasoning about similarities and relational structure in a principled way. PSL can integrate any existing domain-specific similarity measures and also supports reasoning about similarities between sets of entities. We provide efficient inference and learning techniques for PSL and demonstrate its effectiveness both in common relational tasks and in settings that require reasoning about similarity.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3470",
        "title": "ALARMS: Alerting and Reasoning Management System for Next Generation Aircraft Hazards",
        "authors": [
            "Alan S. Carlin",
            "Nathan Schurr",
            "Janusz Marecki"
        ],
        "abstract": "The Next Generation Air Transportation System will introduce new, advanced sensor technologies into the cockpit. With the introduction of such systems, the responsibilities of the pilot are expected to dramatically increase. In the ALARMS (ALerting And Reasoning Management System) project for NASA, we focus on a key challenge of this environment, the quick and efficient handling of aircraft sensor alerts. It is infeasible to alert the pilot on the state of all subsystems at all times. Furthermore, there is uncertainty as to the true hazard state despite the evidence of the alerts, and there is uncertainty as to the effect and duration of actions taken to address these alerts. This paper reports on the first steps in the construction of an application designed to handle Next Generation alerts. In ALARMS, we have identified 60 different aircraft subsystems and 20 different underlying hazards. In this paper, we show how a Bayesian network can be used to derive the state of the underlying hazards, based on the sensor input. Then, we propose a framework whereby an automated system can plan to address these hazards in cooperation with the pilot, using a Time-Dependent Markov Process (TMDP). Different hazards and pilot states will call for different alerting automation plans. We demonstrate this emerging application of Bayesian networks and TMDPs to cockpit automation, for a use case where a small number of hazards are present, and analyze the resulting alerting automation policies.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3473",
        "title": "Lifted Inference for Relational Continuous Models",
        "authors": [
            "Jaesik Choi",
            "Eyal Amir",
            "David J. Hill"
        ],
        "abstract": "Relational Continuous Models (RCMs) represent joint probability densities over attributes of objects, when the attributes have continuous domains. With relational representations, they can model joint probability distributions over large numbers of variables compactly in a natural way. This paper presents a new exact lifted inference algorithm for RCMs, thus it scales up to large models of real world applications. The algorithm applies to Relational Pairwise Models which are (relational) products of potentials of arity 2. Our algorithm is unique in two ways. First, it substantially improves the efficiency of lifted inference with variables of continuous domains. When a relational model has Gaussian potentials, it takes only linear-time compared to cubic time of previous methods. Second, it is the first exact inference algorithm which handles RCMs in a lifted way. The algorithm is illustrated over an example from econometrics. Experimental results show that our algorithm outperforms both a groundlevel inference algorithm and an algorithm built with previously-known lifted methods.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3474",
        "title": "Distribution over Beliefs for Memory Bounded Dec-POMDP Planning",
        "authors": [
            "Gabriel Corona",
            "Francois Charpillet"
        ],
        "abstract": "We propose a new point-based method for approximate planning in Dec-POMDP which outperforms the state-of-the-art approaches in terms of solution quality. It uses a heuristic estimation of the prior probability of beliefs to choose a bounded number of policy trees: this choice is formulated as a combinatorial optimisation problem minimising the error induced by pruning.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3477",
        "title": "A Scalable Method for Solving High-Dimensional Continuous POMDPs Using Local Approximation",
        "authors": [
            "Tom Erez",
            "William D. Smart"
        ],
        "abstract": "Partially-Observable Markov Decision Processes (POMDPs) are typically solved by finding an approximate global solution to a corresponding belief-MDP. In this paper, we offer a new planning algorithm for POMDPs with continuous state, action and observation spaces. Since such domains have an inherent notion of locality, we can find an approximate solution using local optimization methods. We parameterize the belief distribution as a Gaussian mixture, and use the Extended Kalman Filter (EKF) to approximate the belief update. Since the EKF is a first-order filter, we can marginalize over the observations analytically. By using feedback control and state estimation during policy execution, we recover a behavior that is effectively conditioned on incoming observations despite the unconditioned planning. Local optimization provides no guarantees of global optimality, but it allows us to tackle domains that are at least an order of magnitude larger than the current state-of-the-art. We demonstrate the scalability of our algorithm by considering a simulated hand-eye coordination domain with 16 continuous state dimensions and 6 continuous action dimensions.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3478",
        "title": "Playing games against nature: optimal policies for renewable resource allocation",
        "authors": [
            "Stefano Ermon",
            "Jon Conrad",
            "Carla P. Gomes",
            "Bart Selman"
        ],
        "abstract": "In this paper we introduce a class of Markov decision processes that arise as a natural model for many renewable resource allocation problems. Upon extending results from the inventory control literature, we prove that they admit a closed form solution and we show how to exploit this structure to speed up its computation. We consider the application of the proposed framework to several problems arising in very different domains, and as part of the ongoing effort in the emerging field of Computational Sustainability we discuss in detail its application to the Northern Pacific Halibut marine fishery. Our approach is applied to a model based on real world data, obtaining a policy with a guaranteed lower bound on the utility function that is structurally very different from the one currently employed.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3482",
        "title": "Formula-Based Probabilistic Inference",
        "authors": [
            "Vibhav Gogate",
            "Pedro Domingos"
        ],
        "abstract": "Computing the probability of a formula given the probabilities or weights associated with other formulas is a natural extension of logical inference to the probabilistic setting. Surprisingly, this problem has received little attention in the literature to date, particularly considering that it includes many standard inference problems as special cases. In this paper, we propose two algorithms for this problem: formula decomposition and conditioning, which is an exact method, and formula importance sampling, which is an approximate method. The latter is, to our knowledge, the first application of model counting to approximate probabilistic inference. Unlike conventional variable-based algorithms, our algorithms work in the dual realm of logical formulas. Theoretically, we show that our algorithms can greatly improve efficiency by exploiting the structural information in the formulas. Empirically, we show that they are indeed quite powerful, often achieving substantial performance gains over state-of-the-art schemes.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3487",
        "title": "BEEM : Bucket Elimination with External Memory",
        "authors": [
            "Kalev Kask",
            "Rina Dechter",
            "Andrew E. Gelfand"
        ],
        "abstract": "A major limitation of exact inference algorithms for probabilistic graphical models is their extensive memory usage, which often puts real-world problems out of their reach. In this paper we show how we can extend inference algorithms, particularly Bucket Elimination, a special case of cluster (join) tree decomposition, to utilize disk memory. We provide the underlying ideas and show promising empirical results of exactly solving large problems not solvable before.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3490",
        "title": "Anytime Planning for Decentralized POMDPs using Expectation Maximization",
        "authors": [
            "Akshat Kumar",
            "Shlomo Zilberstein"
        ],
        "abstract": "Decentralized POMDPs provide an expressive framework for multi-agent sequential decision making. While fnite-horizon DECPOMDPs have enjoyed signifcant success, progress remains slow for the infnite-horizon case mainly due to the inherent complexity of optimizing stochastic controllers representing agent policies. We present a promising new class of algorithms for the infnite-horizon case, which recasts the optimization problem as inference in a mixture of DBNs. An attractive feature of this approach is the straightforward adoption of existing inference techniques in DBNs for solving DEC-POMDPs and supporting richer representations such as factored or continuous states and actions. We also derive the Expectation Maximization (EM) algorithm to optimize the joint policy represented as DBNs. Experiments on benchmark domains show that EM compares favorably against the state-of-the-art solvers.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3493",
        "title": "Solving Hybrid Influence Diagrams with Deterministic Variables",
        "authors": [
            "Yijing Li",
            "Prakash P. Shenoy"
        ],
        "abstract": "We describe a framework and an algorithm for solving hybrid influence diagrams with discrete, continuous, and deterministic chance variables, and discrete and continuous decision variables. A continuous chance variable in an influence diagram is said to be deterministic if its conditional distributions have zero variances. The solution algorithm is an extension of Shenoy's fusion algorithm for discrete influence diagrams. We describe an extended Shenoy-Shafer architecture for propagation of discrete, continuous, and utility potentials in hybrid influence diagrams that include deterministic chance variables. The algorithm and framework are illustrated by solving two small examples.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3499",
        "title": "A Delayed Column Generation Strategy for Exact k-Bounded MAP Inference in Markov Logic Networks",
        "authors": [
            "Mathias Niepert"
        ],
        "abstract": "The paper introduces k-bounded MAP inference, a parameterization of MAP inference in Markov logic networks. k-Bounded MAP states are MAP states with at most k active ground atoms of hidden (non-evidence) predicates. We present a novel delayed column generation algorithm and provide empirical evidence that the algorithm efficiently computes k-bounded MAP states for meaningful real-world graph matching problems. The underlying idea is that, instead of solving one large optimization problem, it is often more efficient to tackle several small ones.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3500",
        "title": "Comparative Analysis of Probabilistic Models for Activity Recognition with an Instrumented Walker",
        "authors": [
            "Farheen Omar",
            "Mathieu Sinn",
            "Jakub Truszkowski",
            "Pascal Poupart",
            "James Tung",
            "Allen Caine"
        ],
        "abstract": "Rollating walkers are popular mobility aids used by older adults to improve balance control. There is a need to automatically recognize the activities performed by walker users to better understand activity patterns, mobility issues and the context in which falls are more likely to happen. We design and compare several techniques to recognize walker related activities. A comprehensive evaluation with control subjects and walker users from a retirement community is presented.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3502",
        "title": "The Cost of Troubleshooting Cost Clusters with Inside Information",
        "authors": [
            "Thorsten J. Ottosen",
            "Finn Verner Jensen"
        ],
        "abstract": "Decision theoretical troubleshooting is about minimizing the expected cost of solving a certain problem like repairing a complicated man-made device. In this paper we consider situations where you have to take apart some of the device to get access to certain clusters and actions. Specifically, we investigate troubleshooting with independent actions in a tree of clusters where actions inside a cluster cannot be performed before the cluster is opened. The problem is non-trivial because there is a cost associated with opening and closing a cluster. Troubleshooting with independent actions and no clusters can be solved in O(n lg n) time (n being the number of actions) by the well-known \"P-over-C\" algorithm due to Kadane and Simon, but an efficient and optimal algorithm for a tree cluster model has not yet been found. In this paper we describe a \"bottom-up P-over-C\" O(n lg n) time algorithm and show that it is optimal when the clusters do not need to be closed to test whether the actions solved the problem.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3508",
        "title": "Merging Knowledge Bases in Possibilistic Logic by Lexicographic Aggregation",
        "authors": [
            "Guilin Qi",
            "Jianfeng Du",
            "Weiru Liu",
            "David A. Bell"
        ],
        "abstract": "Belief merging is an important but difficult problem in Artificial Intelligence, especially when sources of information are pervaded with uncertainty. Many merging operators have been proposed to deal with this problem in possibilistic logic, a weighted logic which is powerful for handling inconsistency and deal- ing with uncertainty. They often result in a possibilistic knowledge base which is a set of weighted formulas. Although possibilistic logic is inconsistency tolerant, it suers from the well-known \"drowning effect\". Therefore, we may still want to obtain a consistent possi- bilistic knowledge base as the result of merg- ing. In such a case, we argue that it is not always necessary to keep weighted informa- tion after merging. In this paper, we define a merging operator that maps a set of pos- sibilistic knowledge bases and a formula rep- resenting the integrity constraints to a clas- sical knowledge base by using lexicographic ordering. We show that it satisfies nine pos- tulates that generalize basic postulates for propositional merging given in [11]. These postulates capture the principle of minimal change in some sense. We then provide an algorithm for generating the resulting knowl- edge base of our merging operator. Finally, we discuss the compatibility of our merging operator with propositional merging and es- tablish the advantage of our merging opera- tor over existing semantic merging operators in the propositional case.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3509",
        "title": "Characterizing the Set of Coherent Lower Previsions with a Finite Number of Constraints or Vertices",
        "authors": [
            "Erik Quaeghebeur"
        ],
        "abstract": "The standard coherence criterion for lower previsions is expressed using an infinite number of linear constraints. For lower previsions that are essentially defined on some finite set of gambles on a finite possibility space, we present a reformulation of this criterion that only uses a finite number of constraints. Any such lower prevision is coherent if it lies within the convex polytope defined by these constraints. The vertices of this polytope are the extreme coherent lower previsions for the given set of gambles. Our reformulation makes it possible to compute them. We show how this is done and illustrate the procedure and its results.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3510",
        "title": "Irregular-Time Bayesian Networks",
        "authors": [
            "Michael Ramati",
            "Yuval Shahar"
        ],
        "abstract": "In many fields observations are performed irregularly along time, due to either measurement limitations or lack of a constant immanent rate. While discrete-time Markov models (as Dynamic Bayesian Networks) introduce either inefficient computation or an information loss to reasoning about such processes, continuous-time Markov models assume either a discrete state space (as Continuous-Time Bayesian Networks), or a flat continuous state space (as stochastic differential equations). To address these problems, we present a new modeling class called Irregular-Time Bayesian Networks (ITBNs), generalizing Dynamic Bayesian Networks, allowing substantially more compact representations, and increasing the expressivity of the temporal dynamics. In addition, a globally optimal solution is guaranteed when learning temporal systems, provided that they are fully observed at the same irregularly spaced time-points, and a semiparametric subclass of ITBNs is introduced to allow further adaptation to the irregular nature of the available data.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3512",
        "title": "Exact and Approximate Inference in Associative Hierarchical Networks using Graph Cuts",
        "authors": [
            "Chris Russell",
            "L'ubor Ladicky",
            "Pushmeet Kohli",
            "Philip H.S. Torr"
        ],
        "abstract": "Markov Networks are widely used through out computer vision and machine learning. An important subclass are the Associative Markov Networks which are used in a wide variety of applications. For these networks a good approximate minimum cost solution can be found efficiently using graph cut based move making algorithms such as alpha-expansion. Recently a related model has been proposed, the associative hierarchical network, which provides a natural generalisation of the Associative Markov Network for higher order cliques (i.e. clique size greater than two). This method provides a good model for object class segmentation problem in computer vision. Within this paper we briefly describe the associative hierarchical network and provide a computationally efficient method for approximate inference based on graph cuts. Our method performs well for networks containing hundreds of thousand of variables, and higher order potentials are defined over cliques containing tens of thousands of variables. Due to the size of these problems standard linear programming techniques are inapplicable. We show that our method has a bound of 4 for the solution of general associative hierarchical network with arbitrary clique size noting that few results on bounds exist for the solution of labelling of Markov Networks with higher order cliques.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3513",
        "title": "Dynamic programming in in uence diagrams with decision circuits",
        "authors": [
            "Ross D. Shachter",
            "Debarun Bhattacharjya"
        ],
        "abstract": "Decision circuits perform efficient evaluation of influence diagrams, building on the ad- vances in arithmetic circuits for belief net- work inference [Darwiche, 2003; Bhattachar- jya and Shachter, 2007]. We show how even more compact decision circuits can be con- structed for dynamic programming in influ- ence diagrams with separable value functions and conditionally independent subproblems. Once a decision circuit has been constructed based on the diagram's \"global\" graphical structure, it can be compiled to exploit \"lo- cal\" structure for efficient evaluation and sen- sitivity analysis.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3525",
        "title": "Learning Why Things Change: The Difference-Based Causality Learner",
        "authors": [
            "Mark Voortman",
            "Denver Dash",
            "Marek J. Druzdzel"
        ],
        "abstract": "In this paper, we present the Difference- Based Causality Learner (DBCL), an algorithm for learning a class of discrete-time dynamic models that represents all causation across time by means of difference equations driving change in a system. We motivate this representation with real-world mechanical systems and prove DBCL's correctness for learning structure from time series data, an endeavour that is complicated by the existence of latent derivatives that have to be detected. We also prove that, under common assumptions for causal discovery, DBCL will identify the presence or absence of feedback loops, making the model more useful for predicting the effects of manipulating variables when the system is in equilibrium. We argue analytically and show empirically the advantages of DBCL over vector autoregression (VAR) and Granger causality models as well as modified forms of Bayesian and constraintbased structure discovery algorithms. Finally, we show that our algorithm can discover causal directions of alpha rhythms in human brains from EEG data.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3528",
        "title": "Rollout Sampling Policy Iteration for Decentralized POMDPs",
        "authors": [
            "Feng Wu",
            "Shlomo Zilberstein",
            "Xiaoping Chen"
        ],
        "abstract": "We present decentralized rollout sampling policy iteration (DecRSPI) - a new algorithm for multi-agent decision problems formalized as DEC-POMDPs. DecRSPI is designed to improve scalability and tackle problems that lack an explicit model. The algorithm uses Monte- Carlo methods to generate a sample of reachable belief states. Then it computes a joint policy for each belief state based on the rollout estimations. A new policy representation allows us to represent solutions compactly. The key benefits of the algorithm are its linear time complexity over the number of agents, its bounded memory usage and good solution quality. It can solve larger problems that are intractable for existing planning algorithms. Experimental results confirm the effectiveness and scalability of the approach.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3531",
        "title": "Solving Multistage Influence Diagrams using Branch-and-Bound Search",
        "authors": [
            "Changhe Yuan",
            "Xiaojian Wu",
            "Eric A. Hansen"
        ],
        "abstract": "A branch-and-bound approach to solving influ- ence diagrams has been previously proposed in the literature, but appears to have never been implemented and evaluated - apparently due to the difficulties of computing effective bounds for the branch-and-bound search. In this paper, we describe how to efficiently compute effective bounds, and we develop a practical implementa- tion of depth-first branch-and-bound search for influence diagram evaluation that outperforms existing methods for solving influence diagrams with multiple stages.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3538",
        "title": "RAPID: A Reachable Anytime Planner for Imprecisely-sensed Domains",
        "authors": [
            "Emma Brunskill",
            "Stuart Russell"
        ],
        "abstract": "Despite the intractability of generic optimal partially observable Markov decision process planning, there exist important problems that have highly structured models. Previous researchers have used this insight to construct more efficient algorithms for factored domains, and for domains with topological structure in the flat state dynamics model. In our work, motivated by findings from the education community relevant to automated tutoring, we consider problems that exhibit a form of topological structure in the factored dynamics model. Our Reachable Anytime Planner for Imprecisely-sensed Domains (RAPID) leverages this structure to efficiently compute a good initial envelope of reachable states under the optimal MDP policy in time linear in the number of state variables. RAPID performs partially-observable planning over the limited envelope of states, and slowly expands the state space considered as time allows. RAPID performs well on a large tutoring-inspired problem simulation with 122 state variables, corresponding to a flat state space of over 10^30 states.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.4011",
        "title": "Understanding Sampling Style Adversarial Search Methods",
        "authors": [
            "Raghuram Ramanujan",
            "Ashish Sabharwal",
            "Bart Selman"
        ],
        "abstract": "UCT has recently emerged as an exciting new adversarial reasoning technique based on cleverly balancing exploration and exploitation in a Monte-Carlo sampling setting. It has been particularly successful in the game of Go but the reasons for its success are not well understood and attempts to replicate its success in other domains such as Chess have failed. We provide an in-depth analysis of the potential of UCT in domain-independent settings, in cases where heuristic values are available, and the effect of enhancing random playouts to more informed playouts between two weak minimax players. To provide further insights, we develop synthetic game tree instances and discuss interesting properties of UCT, both empirically and analytically.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.4287",
        "title": "Parameter Learning in PRISM Programs with Continuous Random Variables",
        "authors": [
            "Muhammad Asiful Islam",
            "C. R. Ramakrishnan",
            "I. V. Ramakrishnan"
        ],
        "abstract": "Probabilistic Logic Programming (PLP), exemplified by Sato and Kameya's PRISM, Poole's ICL, De Raedt et al's ProbLog and Vennekens et al's LPAD, combines statistical and logical knowledge representation and inference. Inference in these languages is based on enumerative construction of proofs over logic programs. Consequently, these languages permit very limited use of random variables with continuous distributions. In this paper, we extend PRISM with Gaussian random variables and linear equality constraints, and consider the problem of parameter learning in the extended language. Many statistical models such as finite mixture models and Kalman filter can be encoded in extended PRISM. Our EM-based learning algorithm uses a symbolic inference procedure that represents sets of derivations without enumeration. This permits us to learn the distribution parameters of extended PRISM programs with discrete as well as Gaussian variables. The learning algorithm naturally generalizes the ones used for PRISM and Hybrid Bayesian Networks.\n    ",
        "submission_date": "2012-03-19T00:00:00",
        "last_modified_date": "2012-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.5452",
        "title": "Modeling of Mixed Decision Making Process",
        "authors": [
            "Nesrine Ben Yahia",
            "Narj\u00e8s Bellamine",
            "Henda Ben Ghezala"
        ],
        "abstract": "Decision making whenever and wherever it is happened is key to organizations success. In order to make correct decision, individuals, teams and organizations need both knowledge management (to manage content) and collaboration (to manage group processes) to make that more effective and efficient. In this paper, we explain the knowledge management and collaboration convergence. Then, we propose a formal description of mixed and multimodal decision making (MDM) process where decision may be made by three possible modes: individual, collective or hybrid. Finally, we explicit the MDM process based on UML-G profile.\n    ",
        "submission_date": "2012-03-24T00:00:00",
        "last_modified_date": "2012-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.5532",
        "title": "On the Use of Non-Stationary Policies for Infinite-Horizon Discounted Markov Decision Processes",
        "authors": [
            "Bruno Scherrer"
        ],
        "abstract": "We consider infinite-horizon $\\gamma$-discounted Markov Decision Processes, for which it is known that there exists a stationary optimal policy. We consider the algorithm Value Iteration and the sequence of policies $\\pi_1,...,\\pi_k$ it implicitely generates until some iteration $k$. We provide performance bounds for non-stationary policies involving the last $m$ generated policies that reduce the state-of-the-art bound for the last stationary policy $\\pi_k$ by a factor $\\frac{1-\\gamma}{1-\\gamma^m}$. In particular, the use of non-stationary policies allows to reduce the usual asymptotic performance bounds of Value Iteration with errors bounded by $\\epsilon$ at each iteration from $\\frac{\\gamma}{(1-\\gamma)^2}\\epsilon$ to $\\frac{\\gamma}{1-\\gamma}\\epsilon$, which is significant in the usual situation when $\\gamma$ is close to 1. Given Bellman operators that can only be computed with some error $\\epsilon$, a surprising consequence of this result is that the problem of \"computing an approximately optimal non-stationary policy\" is much simpler than that of \"computing an approximately optimal stationary policy\", and even slightly simpler than that of \"approximately computing the value of some fixed policy\", since this last problem only has a guarantee of $\\frac{1}{1-\\gamma}\\epsilon$.\n    ",
        "submission_date": "2012-03-25T00:00:00",
        "last_modified_date": "2012-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.6534",
        "title": "Global preferential consistency for the topological sorting-based maximal spanning tree problem",
        "authors": [
            "R\u00e9my-Robert Joseph"
        ],
        "abstract": "We introduce a new type of fully computable problems, for DSS dedicated to maximal spanning tree problems, based on deduction and choice: preferential consistency problems. To show its interest, we describe a new compact representation of preferences specific to spanning trees, identifying an efficient maximal spanning tree sub-problem. Next, we compare this problem with the Pareto-based multiobjective one. And at last, we propose an efficient algorithm solving the associated preferential consistency problem.\n    ",
        "submission_date": "2012-03-29T00:00:00",
        "last_modified_date": "2012-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.6716",
        "title": "Creating Intelligent Linking for Information Threading in Knowledge Networks",
        "authors": [
            "Dr T.R. Gopalakrishnan Nair",
            "Meenakshi Malhotra"
        ],
        "abstract": "Informledge System (ILS) is a knowledge network with autonomous nodes and intelligent links that integrate and structure the pieces of knowledge. In this paper, we aim to put forward the link dynamics involved in intelligent processing of information in ILS. There has been advancement in knowledge management field which involve managing information in databases from a single domain. ILS works with information from multiple domains stored in distributed way in the autonomous nodes termed as Knowledge Network Node (KNN). Along with the concept under consideration, KNNs store the processed information linking concepts and processors leading to the appropriate processing of information.\n    ",
        "submission_date": "2012-03-30T00:00:00",
        "last_modified_date": "2012-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.0181",
        "title": "Expert PC Troubleshooter With Fuzzy-Logic And Self-Learning Support",
        "authors": [
            "Youssef Bassil"
        ],
        "abstract": "Expert systems use human knowledge often stored as rules within the computer to solve problems that generally would entail human intelligence. Today, with information systems turning out to be more pervasive and with the myriad advances in information technologies, automating computer fault diagnosis is becoming so fundamental that soon every enterprise has to endorse it. This paper proposes an expert system called Expert PC Troubleshooter for diagnosing computer problems. The system is composed of a user interface, a rule-base, an inference engine, and an expert interface. Additionally, the system features a fuzzy-logic module to troubleshoot POST beep errors, and an intelligent agent that assists in the knowledge acquisition process. The proposed system is meant to automate the maintenance, repair, and operations (MRO) process, and free-up human technicians from manually performing routine, laborious, and timeconsuming maintenance tasks. As future work, the proposed system is to be parallelized so as to boost its performance and speed-up its various operations.\n    ",
        "submission_date": "2012-04-01T00:00:00",
        "last_modified_date": "2012-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.0479",
        "title": "A collaborative ant colony metaheuristic for distributed multi-level lot-sizing",
        "authors": [
            "Tobias Buer",
            "J\u00f6rg Homberger",
            "Hermann Gehring"
        ],
        "abstract": "The paper presents an ant colony optimization metaheuristic for collaborative planning. Collaborative planning is used to coordinate individual plans of self-interested decision makers with private information in order to increase the overall benefit of the coalition. The method consists of a new search graph based on encoded solutions. Distributed and private information is integrated via voting mechanisms and via a simple but effective collaborative local search procedure. The approach is applied to a distributed variant of the multi-level lot-sizing problem and evaluated by means of 352 benchmark instances from the literature. The proposed approach clearly outperforms existing approaches on the sets of medium and large sized instances. While the best method in the literature so far achieves an average deviation from the best known non-distributed solutions of 46 percent for the set of the largest instances, for example, the presented approach reduces the average deviation to only 5 percent.\n    ",
        "submission_date": "2012-04-02T00:00:00",
        "last_modified_date": "2012-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.0731",
        "title": "Unit contradiction versus unit propagation",
        "authors": [
            "Olivier Bailleux"
        ],
        "abstract": "Some aspects of the result of applying unit resolution on a CNF formula can be formalized as functions with domain a set of partial truth assignments. We are interested in two ways for computing such functions, depending on whether the result is the production of the empty clause or the assignment of a variable with a given truth value. We show that these two models can compute the same functions with formulae of polynomially related sizes, and we explain how this result is related to the CNF encoding of Boolean constraints.\n    ",
        "submission_date": "2012-04-03T00:00:00",
        "last_modified_date": "2012-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.1231",
        "title": "How Many Vote Operations Are Needed to Manipulate A Voting System?",
        "authors": [
            "Lirong Xia"
        ],
        "abstract": "In this paper, we propose a framework to study a general class of strategic behavior in voting, which we call vote operations. We prove the following theorem: if we fix the number of alternatives, generate $n$ votes i.i.d. according to a distribution $\\pi$, and let $n$ go to infinity, then for any $\\epsilon >0$, with probability at least $1-\\epsilon$, the minimum number of operations that are needed for the strategic individual to achieve her goal falls into one of the following four categories: (1) 0, (2) $\\Theta(\\sqrt n)$, (3) $\\Theta(n)$, and (4) $\\infty$. This theorem holds for any set of vote operations, any individual vote distribution $\\pi$, and any integer generalized scoring rule, which includes (but is not limited to) almost all commonly studied voting rules, e.g., approval voting, all positional scoring rules (including Borda, plurality, and veto), plurality with runoff, Bucklin, Copeland, maximin, STV, and ranked pairs.\n",
        "submission_date": "2012-04-05T00:00:00",
        "last_modified_date": "2012-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.1277",
        "title": "Mouse Simulation Using Two Coloured Tapes",
        "authors": [
            "Vikram Kumar",
            "Kamran Niyazi",
            "Swapnil Mahe",
            "Swapnil Vyawahare"
        ],
        "abstract": "In this paper, we present a novel approach for Human Computer Interaction (HCI) where, we control cursor movement using a real-time camera. Current methods involve changing mouse parts such as adding more buttons or changing the position of the tracking ball. Instead, our method is to use a camera and computer vision technology, such as image segmentation and gesture recognition, to control mouse tasks (left and right clicking, double-clicking, and scrolling) and we show how it can perform everything as current mouse devices can. The software will be developed in JAVA language. Recognition and pose estimation in this system are user independent and robust as we will be using colour tapes on our finger to perform actions. The software can be used as an intuitive input interface to applications that require multi-dimensional control e.g. computer games etc.\n    ",
        "submission_date": "2012-04-05T00:00:00",
        "last_modified_date": "2012-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.1576",
        "title": "Development of knowledge Base Expert System for Natural treatment of Diabetes disease",
        "authors": [
            "Sanjeev Kumar Jha"
        ],
        "abstract": "The development of expert system for treatment of Diabetes disease by using natural methods is new information technology derived from Artificial Intelligent research using ESTA (Expert System Text Animation) System. The proposed expert system contains knowledge about various methods of natural treatment methods (Massage, Herbal/Proper Nutrition, Acupuncture, Gems) for Diabetes diseases of Human Beings. The system is developed in the ESTA (Expert System shell for Text Animation) which is Visual Prolog 7.3 Application. The knowledge for the said system will be acquired from domain experts, texts and other related sources.\n    ",
        "submission_date": "2012-04-06T00:00:00",
        "last_modified_date": "2012-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.1637",
        "title": "Characterization of Dynamic Bayesian Network",
        "authors": [
            "Nabil ghanmy",
            "Mohamed Ali Mahjoub",
            "Najoua Essoukri Ben Amara"
        ],
        "abstract": "In this report, we will be interested at Dynamic Bayesian Network (DBNs) as a model that tries to incorporate temporal dimension with uncertainty. We start with basics of DBN where we especially focus in Inference and Learning concepts and algorithms. Then we will present different levels and methods of creating DBNs as well as approaches of incorporating temporal dimension in static Bayesian network.\n    ",
        "submission_date": "2012-04-07T00:00:00",
        "last_modified_date": "2012-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.1653",
        "title": "Machine Cognition Models: EPAM and GPS",
        "authors": [
            "Ali Elouafiq"
        ],
        "abstract": "Through history, the human being tried to relay its daily tasks to other creatures, which was the main reason behind the rise of civilizations. It started with deploying animals to automate tasks in the field of agriculture(bulls), transportation (e.g. horses and donkeys), and even communication (pigeons). Millenniums after, come the Golden age with \"Al-jazari\" and other Muslim inventors, which were the pioneers of automation, this has given birth to industrial revolution in Europe, centuries after. At the end of the nineteenth century, a new era was to begin, the computational era, the most advanced technological and scientific development that is driving the mankind and the reason behind all the evolutions of science; such as medicine, communication, education, and physics. At this edge of technology engineers and scientists are trying to model a machine that behaves the same as they do, which pushed us to think about designing and implementing \"Things that-Thinks\", then artificial intelligence was. In this work we will cover each of the major discoveries and studies in the field of machine cognition, which are the \"Elementary Perceiver and Memorizer\"(EPAM) and \"The General Problem Solver\"(GPS). The First one focus mainly on implementing the human-verbal learning behavior, while the second one tries to model an architecture that is able to solve problems generally (e.g. theorem proving, chess playing, and arithmetic). We will cover the major goals and the main ideas of each model, as well as comparing their strengths and weaknesses, and finally giving their fields of applications. And Finally, we will suggest a real life implementation of a cognitive machine.\n    ",
        "submission_date": "2012-04-07T00:00:00",
        "last_modified_date": "2012-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.1681",
        "title": "The threshold EM algorithm for parameter learning in bayesian network with incomplete data",
        "authors": [
            "Fradj Ben Lamine",
            "Karim Kalti",
            "Mohamed Ali Mahjoub"
        ],
        "abstract": "Bayesian networks (BN) are used in a big range of applications but they have one issue concerning parameter learning. In real application, training data are always incomplete or some nodes are hidden. To deal with this problem many learning parameter algorithms are suggested foreground EM, Gibbs sampling and RBE algorithms. In order to limit the search space and escape from local maxima produced by executing EM algorithm, this paper presents a learning parameter algorithm that is a fusion of EM and RBE algorithms. This algorithm incorporates the range of a parameter into the EM algorithm. This range is calculated by the first step of RBE algorithm allowing a regularization of each parameter in bayesian network after the maximization step of the EM algorithm. The threshold EM algorithm is applied in brain tumor diagnosis and show some advantages and disadvantages over the EM algorithm.\n    ",
        "submission_date": "2012-04-07T00:00:00",
        "last_modified_date": "2012-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.1851",
        "title": "A Probabilistic Logic Programming Event Calculus",
        "authors": [
            "Anastasios Skarlatidis",
            "Alexander Artikis",
            "Jason Filippou",
            "Georgios Paliouras"
        ],
        "abstract": "We present a system for recognising human activity given a symbolic representation of video content. The input of our system is a set of time-stamped short-term activities (STA) detected on video frames. The output is a set of recognised long-term activities (LTA), which are pre-defined temporal combinations of STA. The constraints on the STA that, if satisfied, lead to the recognition of a LTA, have been expressed using a dialect of the Event Calculus. In order to handle the uncertainty that naturally occurs in human activity recognition, we adapted this dialect to a state-of-the-art probabilistic logic programming framework. We present a detailed evaluation and comparison of the crisp and probabilistic approaches through experimentation on a benchmark dataset of human surveillance videos.\n    ",
        "submission_date": "2012-04-09T00:00:00",
        "last_modified_date": "2013-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.1909",
        "title": "Knapsack based Optimal Policies for Budget-Limited Multi-Armed Bandits",
        "authors": [
            "Long Tran-Thanh",
            "Archie Chapman",
            "Alex Rogers",
            "Nicholas R. Jennings"
        ],
        "abstract": "In budget-limited multi-armed bandit (MAB) problems, the learner's actions are costly and constrained by a fixed budget. Consequently, an optimal exploitation policy may not be to pull the optimal arm repeatedly, as is the case in other variants of MAB, but rather to pull the sequence of different arms that maximises the agent's total reward within the budget. This difference from existing MABs means that new approaches to maximising the total reward are required. Given this, we develop two pulling policies, namely: (i) KUBE; and (ii) fractional KUBE. Whereas the former provides better performance up to 40% in our experimental settings, the latter is computationally less expensive. We also prove logarithmic upper bounds for the regret of both policies, and show that these bounds are asymptotically optimal (i.e. they only differ from the best possible regret by a constant factor).\n    ",
        "submission_date": "2012-04-09T00:00:00",
        "last_modified_date": "2012-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.2018",
        "title": "Applications of fuzzy logic to Case-Based Reasoning",
        "authors": [
            "Igor Ya. Subbotin",
            "Michael Gr. Voskoglou"
        ],
        "abstract": "The article discusses some applications of fuzzy logic ideas to formalizing of the Case-Based Reasoning (CBR) process and to measuring the effectiveness of CBR systems\n    ",
        "submission_date": "2012-04-10T00:00:00",
        "last_modified_date": "2012-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.2248",
        "title": "Robust Spatio-Temporal Signal Recovery from Noisy Counts in Social Media",
        "authors": [
            "Jun-Ming Xu",
            "Aniruddha Bhargava",
            "Robert Nowak",
            "Xiaojin Zhu"
        ],
        "abstract": "Many real-world phenomena can be represented by a spatio-temporal signal: where, when, and how much. Social media is a tantalizing data source for those who wish to monitor such signals. Unlike most prior work, we assume that the target phenomenon is known and we are given a method to count its occurrences in social media. However, counting is plagued by sample bias, incomplete data, and, paradoxically, data scarcity -- issues inadequately addressed by prior work. We formulate signal recovery as a Poisson point process estimation problem. We explicitly incorporate human population bias, time delays and spatial distortions, and spatio-temporal regularization into the model to address the noisy count issues. We present an efficient optimization algorithm and discuss its theoretical properties. We show that our model is more accurate than commonly-used baselines. Finally, we present a case study on wildlife roadkill monitoring, where our model produces qualitatively convincing results.\n    ",
        "submission_date": "2012-04-10T00:00:00",
        "last_modified_date": "2012-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.2712",
        "title": "Learning to Rank Query Recommendations by Semantic Similarities",
        "authors": [
            "Sumio Fujita",
            "Georges Dupret",
            "Ricardo Baeza-Yates"
        ],
        "abstract": "Logs of the interactions with a search engine show that users often reformulate their queries. Examining these reformulations shows that recommendations that precise the focus of a query are helpful, like those based on expansions of the original queries. But it also shows that queries that express some topical shift with respect to the original query can help user access more rapidly the information they need. We propose a method to identify from the query logs of past users queries that either focus or shift the initial query topic. This method combines various click-based, topic-based and session based ranking strategies and uses supervised learning in order to maximize the semantic similarities between the query and the recommendations, while at the same diversifying them. We evaluate our method using the query/click logs of a Japanese web search engine and we show that the combination of the three methods proposed is significantly better than any of them taken individually.\n    ",
        "submission_date": "2012-04-12T00:00:00",
        "last_modified_date": "2012-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.2713",
        "title": "Enabling Semantic Analysis of User Browsing Patterns in the Web of Data",
        "authors": [
            "Julia Hoxha",
            "Martin Junghans",
            "Sudhir Agarwal"
        ],
        "abstract": "A useful step towards better interpretation and analysis of the usage patterns is to formalize the semantics of the resources that users are accessing in the Web. We focus on this problem and present an approach for the semantic formalization of usage logs, which lays the basis for eective techniques of querying expressive usage patterns. We also present a query answering approach, which is useful to nd in the logs expressive patterns of usage behavior via formulation of semantic and temporal-based constraints. We have processed over 30 thousand user browsing sessions extracted from usage logs of DBPedia and Semantic Web Dog Food. All these events are formalized semantically using respective domain ontologies and RDF representations of the Web resources being accessed. We show the eectiveness of our approach through experimental results, providing in this way an exploratory analysis of the way users browse theWeb of Data.\n    ",
        "submission_date": "2012-04-12T00:00:00",
        "last_modified_date": "2012-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.2718",
        "title": "Leveraging Usage Data for Linked Data Movie Entity Summarization",
        "authors": [
            "Andreas Thalhammer",
            "Ioan Toma",
            "Antonio Roa-Valverde",
            "Dieter Fensel"
        ],
        "abstract": "Novel research in the field of Linked Data focuses on the problem of entity summarization. This field addresses the problem of ranking features according to their importance for the task of identifying a particular entity. Next to a more human friendly presentation, these summarizations can play a central role for semantic search engines and semantic recommender systems. In current approaches, it has been tried to apply entity summarization based on patterns that are inherent to the regarded data.\n",
        "submission_date": "2012-04-12T00:00:00",
        "last_modified_date": "2012-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.3255",
        "title": "Lower Complexity Bounds for Lifted Inference",
        "authors": [
            "Manfred Jaeger"
        ],
        "abstract": "One of the big challenges in the development of probabilistic relational (or probabilistic logical) modeling and learning frameworks is the design of inference techniques that operate on the level of the abstract model representation language, rather than on the level of ground, propositional instances of the model. Numerous approaches for such \"lifted inference\" techniques have been proposed. While it has been demonstrated that these techniques will lead to significantly more efficient inference on some specific models, there are only very recent and still quite restricted results that show the feasibility of lifted inference on certain syntactically defined classes of models. Lower complexity bounds that imply some limitations for the feasibility of lifted inference on more expressive model classes were established early on in (Jaeger 2000). However, it is not immediate that these results also apply to the type of modeling languages that currently receive the most attention, i.e., weighted, quantifier-free formulas. In this paper we extend these earlier results, and show that under the assumption that NETIME =/= ETIME, there is no polynomial lifted inference algorithm for knowledge bases of weighted, quantifier- and function-free formulas. Further strengthening earlier results, this is also shown to hold for approximate inference, and for knowledge bases not containing the equality predicate.\n    ",
        "submission_date": "2012-04-15T00:00:00",
        "last_modified_date": "2013-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.3348",
        "title": "Symmetry Breaking Constraints: Recent Results",
        "authors": [
            "Toby Walsh"
        ],
        "abstract": "Symmetry is an important problem in many combinatorial problems. One way of dealing with symmetry is to add constraints that eliminate symmetric solutions. We survey recent results in this area, focusing especially on two common and useful cases: symmetry breaking constraints for row and column symmetry, and symmetry breaking constraints for eliminating value symmetry\n    ",
        "submission_date": "2012-04-16T00:00:00",
        "last_modified_date": "2012-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.3838",
        "title": "Energy cost reduction in the synchronization of a pair of nonidentical coupled Hindmarsh-Rose neurons",
        "authors": [
            "A. Moujahid",
            "A. D'Anjou",
            "F.J. Torrealdea",
            "C. Sarasola"
        ],
        "abstract": "Many biological processes involve synchronization between nonequivalent systems, i.e, systems where the difference is limited to a rather small parameter mismatch. The maintenance of the synchronized regime in this cases is energetically costly \\cite{1}. This work studies the energy implications of synchronization phenomena in a pair of structurally flexible coupled neurons that interact through electrical coupling. We show that the forced synchronization between two nonidentical neurons creates appropriate conditions for an efficient actuation of adaptive laws able to make the neurons structurally approach their behaviours in order to decrease the flow of energy required to maintain the synchronization regime.\n    ",
        "submission_date": "2012-04-17T00:00:00",
        "last_modified_date": "2012-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.3844",
        "title": "On how percolation threshold affects PSO performance",
        "authors": [
            "Blanca Cases",
            "Alicia D'Anjou",
            "Abdelmalik Moujahid"
        ],
        "abstract": "Statistical evidence of the influence of neighborhood topology on the performance of particle swarm optimization (PSO) algorithms has been shown in many works. However, little has been done about the implications could have the percolation threshold in determining the topology of this neighborhood. This work addresses this problem for individuals that, like robots, are able to sense in a limited neighborhood around them. Based on the concept of percolation threshold, and more precisely, the disk percolation model in 2D, we show that better results are obtained for low values of radius, when individuals occasionally ask others their best visited positions, with the consequent decrease of computational complexity. On the other hand, since percolation threshold is a universal measure, it could have a great interest to compare the performance of different hybrid PSO algorithms.\n    ",
        "submission_date": "2012-04-17T00:00:00",
        "last_modified_date": "2012-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.3918",
        "title": "Eliminating the Weakest Link: Making Manipulation Intractable?",
        "authors": [
            "Jessica Davies",
            "Nina Narodytska",
            "Toby Walsh"
        ],
        "abstract": "Successive elimination of candidates is often a route to making manipulation intractable to compute. We prove that eliminating candidates does not necessarily increase the computational complexity of manipulation. However, for many voting rules used in practice, the computational complexity increases. For example, it is already known that it is NP-hard to compute how a single voter can manipulate the result of single transferable voting (the elimination version of plurality voting). We show here that it is NP-hard to compute how a single voter can manipulate the result of the elimination version of veto voting, of the closely related Coombs' rule, and of the elimination versions of a general class of scoring rules.\n    ",
        "submission_date": "2012-04-17T00:00:00",
        "last_modified_date": "2012-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.4051",
        "title": "Solution Representations and Local Search for the bi-objective Inventory Routing Problem",
        "authors": [
            "Thibaut Barth\u00e9lemy",
            "Martin Josef Geiger",
            "Marc Sevaux"
        ],
        "abstract": "The solution of the biobjective IRP is rather challenging, even for metaheuristics. We are still lacking a profound understanding of appropriate solution representations and effective neighborhood structures. Clearly, both the delivery volumes and the routing aspects of the alternatives need to be reflected in an encoding, and must be modified when searching by means of local search. Our work contributes to the better understanding of such solution representations. On the basis of an experimental investigation, the advantages and drawbacks of two encodings are studied and compared.\n    ",
        "submission_date": "2012-04-18T00:00:00",
        "last_modified_date": "2012-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.4141",
        "title": "Analysis of a Natural Gradient Algorithm on Monotonic Convex-Quadratic-Composite Functions",
        "authors": [
            "Youhei Akimoto"
        ],
        "abstract": "In this paper we investigate the convergence properties of a variant of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Our study is based on the recent theoretical foundation that the pure rank-mu update CMA-ES performs the natural gradient descent on the parameter space of Gaussian distributions. We derive a novel variant of the natural gradient method where the parameters of the Gaussian distribution are updated along the natural gradient to improve a newly defined function on the parameter space. We study this algorithm on composites of a monotone function with a convex quadratic function. We prove that our algorithm adapts the covariance matrix so that it becomes proportional to the inverse of the Hessian of the original objective function. We also show the speed of covariance matrix adaptation and the speed of convergence of the parameters. We introduce a stochastic algorithm that approximates the natural gradient with finite samples and present some simulated results to evaluate how precisely the stochastic algorithm approximates the deterministic, ideal one under finite samples and to see how similarly our algorithm and the CMA-ES perform.\n    ",
        "submission_date": "2012-04-18T00:00:00",
        "last_modified_date": "2017-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.4200",
        "title": "Discrete Dynamical Genetic Programming in XCS",
        "authors": [
            "Richard J. Preen",
            "Larry Bull"
        ],
        "abstract": "A number of representation schemes have been presented for use within Learning Classifier Systems, ranging from binary encodings to neural networks. This paper presents results from an investigation into using a discrete dynamical system representation within the XCS Learning Classifier System. In particular, asynchronous random Boolean networks are used to represent the traditional condition-action production system rules. It is shown possible to use self-adaptive, open-ended evolution to design an ensemble of such discrete dynamical systems within XCS to solve a number of well-known test problems.\n    ",
        "submission_date": "2012-04-18T00:00:00",
        "last_modified_date": "2014-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.4202",
        "title": "Fuzzy Dynamical Genetic Programming in XCSF",
        "authors": [
            "Richard J. Preen",
            "Larry Bull"
        ],
        "abstract": "A number of representation schemes have been presented for use within Learning Classifier Systems, ranging from binary encodings to Neural Networks, and more recently Dynamical Genetic Programming (DGP). This paper presents results from an investigation into using a fuzzy DGP representation within the XCSF Learning Classifier System. In particular, asynchronous Fuzzy Logic Networks are used to represent the traditional condition-action production system rules. It is shown possible to use self-adaptive, open-ended evolution to design an ensemble of such fuzzy dynamical systems within XCSF to solve several well-known continuous-valued test problems.\n    ",
        "submission_date": "2012-04-18T00:00:00",
        "last_modified_date": "2012-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.4307",
        "title": "Avian Influenza (H5N1) Warning System using Dempster-Shafer Theory and Web Mapping",
        "authors": [
            "Andino Maseleno",
            "Md. Mahmud Hasan"
        ],
        "abstract": "Based on Cumulative Number of Confirmed Human Cases of Avian Influenza (H5N1) Reported to World Health Organization (WHO) in the 2011 from 15 countries, Indonesia has the largest number death because Avian Influenza which 146 deaths. In this research, the researcher built a Web Mapping and Dempster-Shafer theory as early warning system of avian influenza. Early warning is the provision of timely and effective information, through identified institutions, that allows individuals exposed to a hazard to take action to avoid or reduce their risk and prepare for effective response. In this paper as example we use five symptoms as major symptoms which include depression, combs, wattle, bluish face region, swollen face region, narrowness of eyes, and balance disorders. Research location is in the Lampung Province, South Sumatera. The researcher reason to choose Lampung Province in South Sumatera on the basis that has a high poultry population. Geographically, Lampung province is located at 103040' to 105050' East Longitude and 6045' - 3045' South latitude, confined with: South Sumatera and Bengkulu on North Side, Sunda Strait on the Side, Java Sea on the East Side, Indonesia Ocean on the West Side. Our approach uses Dempster Shafer theory to combine beliefs in certain hypotheses under conditions of uncertainty and ignorance, and allows quantitative measurement of the belief and plausibility in our identification result. Web Mapping is also used for displaying maps on a screen to visualize the result of the identification process. The result reveal that avian influenza warning system has successfully identified the existence of avian influenza and the maps can be displayed as the visualization.\n    ",
        "submission_date": "2012-04-19T00:00:00",
        "last_modified_date": "2012-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.4311",
        "title": "Avian Influenza (H5N1) Expert System using Dempster-Shafer Theory",
        "authors": [
            "Andino Maseleno",
            "Md. Mahmud Hasan"
        ],
        "abstract": "Based on Cumulative Number of Confirmed Human Cases of Avian Influenza (H5N1) Reported to World Health Organization (WHO) in the 2011 from 15 countries, Indonesia has the largest number death because Avian Influenza which 146 deaths. In this research, the researcher built an Avian Influenza (H5N1) Expert System for identifying avian influenza disease and displaying the result of identification process. In this paper, we describe five symptoms as major symptoms which include depression, combs, wattle, bluish face region, swollen face region, narrowness of eyes, and balance disorders. We use chicken as research object. Research location is in the Lampung Province, South Sumatera. The researcher reason to choose Lampung Province in South Sumatera on the basis that has a high poultry population. Dempster-Shafer theory to quantify the degree of belief as inference engine in expert system, our approach uses Dempster-Shafer theory to combine beliefs under conditions of uncertainty and ignorance, and allows quantitative measurement of the belief and plausibility in our identification result. The result reveal that Avian Influenza (H5N1) Expert System has successfully identified the existence of avian influenza and displaying the result of identification process.\n    ",
        "submission_date": "2012-04-19T00:00:00",
        "last_modified_date": "2012-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.4541",
        "title": "Automatic Sampling of Geographic objects",
        "authors": [
            "Patrick Taillandier",
            "Julien Gaffuri"
        ],
        "abstract": "Today, one's disposes of large datasets composed of thousands of geographic objects. However, for many processes, which require the appraisal of an expert or much computational time, only a small part of these objects can be taken into account. In this context, robust sampling methods become necessary. In this paper, we propose a sampling method based on clustering techniques. Our method consists in dividing the objects in clusters, then in selecting in each cluster, the most representative objects. A case-study in the context of a process dedicated to knowledge revision for geographic data generalisation is presented. This case-study shows that our method allows to select relevant samples of objects.\n    ",
        "submission_date": "2012-04-20T00:00:00",
        "last_modified_date": "2012-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.4805",
        "title": "What's in an `is about' link? Chemical diagrams and the Information Artifact Ontology",
        "authors": [
            "Janna Hastings",
            "Colin Batchelor",
            "Fabian Neuhaus",
            "Christoph Steinbeck"
        ],
        "abstract": "The Information Artifact Ontology is an ontology in the domain of information entities. Core to the definition of what it is to be an information entity is the claim that an information entity must be `about' something, which is encoded in an axiom expressing that all information entities are about some entity. This axiom comes into conflict with ontological realism, since many information entities seem to be about non-existing entities, such as hypothetical molecules. We discuss this problem in the context of diagrams of molecules, a kind of information entity pervasively used throughout computational chemistry. We then propose a solution that recognizes that information entities such as diagrams are expressions of diagrammatic languages. In so doing, we not only address the problem of classifying diagrams that seem to be about non-existing entities but also allow a more sophisticated categorisation of information entities.\n    ",
        "submission_date": "2012-04-21T00:00:00",
        "last_modified_date": "2012-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.4914",
        "title": "Quantum Interference in Cognition: Structural Aspects of the Brain",
        "authors": [
            "Diederik Aerts",
            "Sandro Sozzo"
        ],
        "abstract": "We identify the presence of typically quantum effects, namely 'superposition' and 'interference', in what happens when human concepts are combined, and provide a quantum model in complex Hilbert space that represents faithfully experimental data measuring the situation of combining concepts. Our model shows how 'interference of concepts' explains the effects of underextension and overextension when two concepts combine to the disjunction of these two concepts. This result supports our earlier hypothesis that human thought has a superposed two-layered structure, one layer consisting of 'classical logical thought' and a superposed layer consisting of 'quantum conceptual thought'. Possible connections with recent findings of a 'grid-structure' for the brain are analyzed, and influences on the mind/brain relation, and consequences on applied disciplines, such as artificial intelligence and quantum computation, are considered.\n    ",
        "submission_date": "2012-04-22T00:00:00",
        "last_modified_date": "2012-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.4927",
        "title": "EHRs Connect Research and Practice: Where Predictive Modeling, Artificial Intelligence, and Clinical Decision Support Intersect",
        "authors": [
            "Casey Bennett",
            "Tom Doub",
            "Rebecca Selove"
        ],
        "abstract": "Objectives: Electronic health records (EHRs) are only a first step in capturing and utilizing health-related data - the challenge is turning that data into useful information. Furthermore, EHRs are increasingly likely to include data relating to patient outcomes, functionality such as clinical decision support, and genetic information as well, and, as such, can be seen as repositories of increasingly valuable information about patients' health conditions and responses to treatment over time. Methods: We describe a case study of 423 patients treated by Centerstone within Tennessee and Indiana in which we utilized electronic health record data to generate predictive algorithms of individual patient treatment response. Multiple models were constructed using predictor variables derived from clinical, financial and geographic data. Results: For the 423 patients, 101 deteriorated, 223 improved and in 99 there was no change in clinical condition. Based on modeling of various clinical indicators at baseline, the highest accuracy in predicting individual patient response ranged from 70-72% within the models tested. In terms of individual predictors, the Centerstone Assessment of Recovery Level - Adult (CARLA) baseline score was most significant in predicting outcome over time (odds ratio 4.1 + 2.27). Other variables with consistently significant impact on outcome included payer, diagnostic category, location and provision of case management services. Conclusions: This approach represents a promising avenue toward reducing the current gap between research and practice across healthcare, developing data-driven clinical decision support based on real-world populations, and serving as a component of embedded clinical artificial intelligences that \"learn\" over time.\n    ",
        "submission_date": "2012-04-22T00:00:00",
        "last_modified_date": "2012-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.4989",
        "title": "Using Belief Theory to Diagnose Control Knowledge Quality. Application to cartographic generalisation",
        "authors": [
            "Patrick Taillandier",
            "C\u00e9cile Duch\u00eane",
            "Alexis Drogoul"
        ],
        "abstract": "Both humans and artificial systems frequently use trial and error methods to problem solving. In order to be effective, this type of strategy implies having high quality control knowledge to guide the quest for the optimal solution. Unfortunately, this control knowledge is rarely perfect. Moreover, in artificial systems-as in humans-self-evaluation of one's own knowledge is often difficult. Yet, this self-evaluation can be very useful to manage knowledge and to determine when to revise it. The objective of our work is to propose an automated approach to evaluate the quality of control knowledge in artificial systems based on a specific trial and error strategy, namely the informed tree search strategy. Our revision approach consists in analysing the system's execution logs, and in using the belief theory to evaluate the global quality of the knowledge. We present a real-world industrial application in the form of an experiment using this approach in the domain of cartographic generalisation. Thus far, the results of using our approach have been encouraging.\n    ",
        "submission_date": "2012-04-23T00:00:00",
        "last_modified_date": "2012-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.4991",
        "title": "Knowledge revision in systems based on an informed tree search strategy : application to cartographic generalisation",
        "authors": [
            "Patrick Taillandier",
            "C\u00e9cile Duch\u00eane",
            "Alexis Drogoul"
        ],
        "abstract": "Many real world problems can be expressed as optimisation problems. Solving this kind of problems means to find, among all possible solutions, the one that maximises an evaluation function. One approach to solve this kind of problem is to use an informed search strategy. The principle of this kind of strategy is to use problem-specific knowledge beyond the definition of the problem itself to find solutions more efficiently than with an uninformed strategy. This kind of strategy demands to define problem-specific knowledge (heuristics). The efficiency and the effectiveness of systems based on it directly depend on the used knowledge quality. Unfortunately, acquiring and maintaining such knowledge can be fastidious. The objective of the work presented in this paper is to propose an automatic knowledge revision approach for systems based on an informed tree search strategy. Our approach consists in analysing the system execution logs and revising knowledge based on these logs by modelling the revision problem as a knowledge space exploration problem. We present an experiment we carried out in an application domain where informed search strategies are often used: cartographic generalisation.\n    ",
        "submission_date": "2012-04-23T00:00:00",
        "last_modified_date": "2012-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.5920",
        "title": "Quantified Conditional Logics are Fragments of HOL",
        "authors": [
            "Christoph Benzmueller",
            "Valerio Genovese"
        ],
        "abstract": "A semantic embedding of (constant domain) quantified conditional logic in classical higher-order logic is presented.\n    ",
        "submission_date": "2012-04-26T00:00:00",
        "last_modified_date": "2012-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.6284",
        "title": "The Network of French Legal Codes",
        "authors": [
            "Pierre Mazzega",
            "Dani\u00e8le Bourcier",
            "Romain Boulet"
        ],
        "abstract": "We propose an analysis of the codified Law of France as a structured system. Fifty two legal codes are selected on the basis of explicit legal criteria and considered as vertices with their mutual quotations forming the edges in a network which properties are analyzed relying on graph theory. We find that a group of 10 codes are simultaneously the most citing and the most cited by other codes, and are also strongly connected together so forming a \"rich club\" sub-graph. Three other code communities are also found that somewhat partition the legal field is distinct thematic sub-domains. The legal interpretation of this partition is opening new untraditional lines of research. We also conjecture that many legal systems are forming such new kind of networks that share some properties in common with small worlds but are far denser. We propose to call \"concentrated world\".\n    ",
        "submission_date": "2011-11-24T00:00:00",
        "last_modified_date": "2011-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.6346",
        "title": "Magic Sets for Disjunctive Datalog Programs",
        "authors": [
            "Mario Alviano",
            "Wolfgang Faber",
            "Gianluigi Greco",
            "Nicola Leone"
        ],
        "abstract": "In this paper, a new technique for the optimization of (partially) bound queries over disjunctive Datalog programs with stratified negation is presented. The technique exploits the propagation of query bindings and extends the Magic Set (MS) optimization technique.\n",
        "submission_date": "2012-04-27T00:00:00",
        "last_modified_date": "2012-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.6415",
        "title": "A Fuzzy Model for Analogical Problem Solving",
        "authors": [
            "Michael Gr. Voskoglou"
        ],
        "abstract": "In this paper we develop a fuzzy model for the description of the process of Analogical Reasoning by representing its main steps as fuzzy subsets of a set of linguistic labels characterizing the individuals' performance in each step and we use the Shannon- Wiener diversity index as a measure of the individuals' abilities in analogical problem solving. This model is compared with a stochastic model presented in author's earlier papers by introducing a finite Markov chain on the steps of the process of Analogical Reasoning. A classroom experiment is also presented to illustrate the use of our results in practice.\n    ",
        "submission_date": "2012-04-28T00:00:00",
        "last_modified_date": "2012-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.0243",
        "title": "Poultry Diseases Expert System using Dempster-Shafer Theory",
        "authors": [
            "Andino Maseleno",
            "Md. Mahmud Hasan"
        ],
        "abstract": "Based on World Health Organization (WHO) fact sheet in the 2011, outbreaks of poultry diseases especially Avian Influenza in poultry may raise global public health concerns due to their effect on poultry populations, their potential to cause serious disease in people, and their pandemic potential. In this research, we built a Poultry Diseases Expert System using Dempster-Shafer Theory. In this Poultry Diseases Expert System We describe five symptoms which include depression, combs, wattle, bluish face region, swollen face region, narrowness of eyes, and balance disorders. The result of the research is that Poultry Diseases Expert System has been successfully identifying poultry diseases.\n    ",
        "submission_date": "2012-05-01T00:00:00",
        "last_modified_date": "2012-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.0831",
        "title": "African Trypanosomiasis Detection using Dempster-Shafer Theory",
        "authors": [
            "Andino Maseleno",
            "Md. Mahmud Hasan"
        ],
        "abstract": "World Health Organization reports that African Trypanosomiasis affects mostly poor populations living in remote rural areas of Africa that can be fatal if properly not treated. This paper presents Dempster-Shafer Theory for the detection of African trypanosomiasis. Sustainable elimination of African trypanosomiasis as a public-health problem is feasible and requires continuous efforts and innovative approaches. In this research, we implement Dempster-Shafer theory for detecting African trypanosomiasis and displaying the result of detection process. We describe eleven symptoms as major symptoms which include fever, red urine, skin rash, paralysis, headache, bleeding around the bite, joint the paint, swollen lymph nodes, sleep disturbances, meningitis and arthritis. Dempster-Shafer theory to quantify the degree of belief, our approach uses Dempster-Shafer theory to combine beliefs under conditions of uncertainty and ignorance, and allows quantitative measurement of the belief and plausibility in our identification result.\n    ",
        "submission_date": "2012-05-03T00:00:00",
        "last_modified_date": "2012-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.0986",
        "title": "Robot Navigation using Reinforcement Learning and Slow Feature Analysis",
        "authors": [
            "Wendelin B\u00f6hmer"
        ],
        "abstract": "The application of reinforcement learning algorithms onto real life problems always bears the challenge of filtering the environmental state out of raw sensor readings. While most approaches use heuristics, biology suggests that there must exist an unsupervised method to construct such filters automatically. Besides the extraction of environmental states, the filters have to represent them in a fashion that support modern reinforcement algorithms. Many popular algorithms use a linear architecture, so one should aim at filters that have good approximation properties in combination with linear functions. This thesis wants to propose the unsupervised method slow feature analysis (SFA) for this task. Presented with a random sequence of sensor readings, SFA learns a set of filters. With growing model complexity and training examples, the filters converge against trigonometric polynomial functions. These are known to possess excellent approximation capabilities and should therfore support the reinforcement algorithms well. We evaluate this claim on a robot. The task is to learn a navigational control in a simple environment using the least square policy iteration (LSPI) algorithm. The only accessible sensor is a head mounted video camera, but without meaningful filtering, video images are not suited as LSPI input. We will show that filters learned by SFA, based on a random walk video of the robot, allow the learned control to navigate successfully in ca. 80% of the test trials.\n    ",
        "submission_date": "2012-05-04T00:00:00",
        "last_modified_date": "2012-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.1645",
        "title": "Publishing and linking transport data on the Web",
        "authors": [
            "Julien Plu",
            "Fran\u00e7ois Scharffe"
        ],
        "abstract": "Without Linked Data, transport data is limited to applications exclusively around transport. In this paper, we present a workflow for publishing and linking transport data on the Web. So we will be able to develop transport applications and to add other features which will be created from other datasets. This will be possible because transport data will be linked to these datasets. We apply this workflow to two datasets: NEPTUNE, a French standard describing a transport line, and Passim, a directory containing relevant information on transport services, in every French city.\n    ",
        "submission_date": "2012-05-08T00:00:00",
        "last_modified_date": "2012-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.1794",
        "title": "A Novel Method For Speech Segmentation Based On Speakers' Characteristics",
        "authors": [
            "Behrouz Abdolali",
            "Hossein Sameti"
        ],
        "abstract": "Speech Segmentation is the process change point detection for partitioning an input audio stream into regions each of which corresponds to only one audio source or one speaker. One application of this system is in Speaker Diarization systems. There are several methods for speaker segmentation; however, most of the Speaker Diarization Systems use BIC-based Segmentation methods. The main goal of this paper is to propose a new method for speaker segmentation with higher speed than the current methods - e.g. BIC - and acceptable accuracy. Our proposed method is based on the pitch frequency of the speech. The accuracy of this method is similar to the accuracy of common speaker segmentation methods. However, its computation cost is much less than theirs. We show that our method is about 2.4 times faster than the BIC-based method, while the average accuracy of pitch-based method is slightly higher than that of the BIC-based method.\n    ",
        "submission_date": "2012-05-08T00:00:00",
        "last_modified_date": "2012-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.1820",
        "title": "The non-algorithmic side of the mind",
        "authors": [
            "Paola Zizzi"
        ],
        "abstract": "The existence of a non-algorithmic side of the mind, conjectured by Penrose on the basis of G\u00f6del's first incompleteness theorem, is investigated here in terms of a quantum metalanguage. We suggest that, besides human ordinary thought, which can be formalized in a computable, logical language, there is another important kind of human thought, which is Turing-non-computable. This is methatought, the process of thinking about ordinary thought. Metathought can be formalized as a metalanguage, which speaks about and controls the logical language of ordinary thought. Ordinary thought has two computational modes, the quantum mode and the classical mode, the latter deriving from decoherence of the former. In order to control the logical language of the quantum mode, one needs to introduce a quantum metalanguage, which in turn requires a quantum version of Tarski Convention T.\n    ",
        "submission_date": "2012-01-15T00:00:00",
        "last_modified_date": "2012-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2541",
        "title": "An improved approach to attribute reduction with covering rough sets",
        "authors": [
            "Changzhong Wang",
            "Baiqing Sun",
            "Qinhua Hu"
        ],
        "abstract": "Attribute reduction is viewed as an important preprocessing step for pattern recognition and data mining. Most of researches are focused on attribute reduction by using rough sets. Recently, Tsang et al. discussed attribute reduction with covering rough sets in the paper [E. C.C. Tsang, D. Chen, Daniel S. Yeung, Approximations and reducts with covering generalized rough sets, Computers and Mathematics with Applications 56 (2008) 279-289], where an approach based on discernibility matrix was presented to compute all attribute reducts. In this paper, we provide an improved approach by constructing simpler discernibility matrix with covering rough sets, and then proceed to improve some characterizations of attribute reduction provided by Tsang et al. It is proved that the improved discernible matrix is equivalent to the old one, but the computational complexity of discernible matrix is greatly reduced.\n    ",
        "submission_date": "2012-05-11T00:00:00",
        "last_modified_date": "2012-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2596",
        "title": "Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence (2011)",
        "authors": [
            "Fabio Cozman",
            "Avi Pfeffer"
        ],
        "abstract": "This is the Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, which was held in Barcelona, Spain, July 14 - 17 2011.\n    ",
        "submission_date": "2012-05-11T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2597",
        "title": "Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (2010)",
        "authors": [
            "Peter Grunwald",
            "Peter Spirtes"
        ],
        "abstract": "This is the Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence, which was held on Catalina Island, CA, July 8 - 11 2010.\n    ",
        "submission_date": "2012-05-11T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2601",
        "title": "Most Relevant Explanation: Properties, Algorithms, and Evaluations",
        "authors": [
            "Changhe Yuan",
            "Xiaolu Liu",
            "Tsai-Ching Lu",
            "Heejin Lim"
        ],
        "abstract": "Most Relevant Explanation (MRE) is a method for finding multivariate explanations for given evidence in Bayesian networks [12]. This paper studies the theoretical properties of MRE and develops an algorithm for finding multiple top MRE solutions. Our study shows that MRE relies on an implicit soft relevance measure in automatically identifying the most relevant target variables and pruning less relevant variables from an explanation. The soft measure also enables MRE to capture the intuitive phenomenon of explaining away encoded in Bayesian networks. Furthermore, our study shows that the solution space of MRE has a special lattice structure which yields interesting dominance relations among the solutions. A K-MRE algorithm based on these dominance relations is developed for generating a set of top solutions that are more representative. Our empirical results show that MRE methods are promising approaches for explanation in Bayesian networks.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2613",
        "title": "Measuring Inconsistency in Probabilistic Knowledge Bases",
        "authors": [
            "Matthias Thimm"
        ],
        "abstract": "This paper develops an inconsistency measure on conditional probabilistic knowledge bases. The measure is based on fundamental principles for inconsistency measures and thus provides a solid theoretical framework for the treatment of inconsistencies in probabilistic expert systems. We illustrate its usefulness and immediate application on several examples and present some formal results. Building on this measure we use the Shapley value-a well-known solution for coalition games-to define a sophisticated indicator that is not only able to measure inconsistencies but to reveal the causes of inconsistencies in the knowledge base. Altogether these tools guide the knowledge engineer in his aim to restore consistency and therefore enable him to build a consistent and usable knowledge base that can be employed in probabilistic expert systems.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2616",
        "title": "Bisimulation-based Approximate Lifted Inference",
        "authors": [
            "Prithviraj Sen",
            "Amol Deshpande",
            "Lise Getoor"
        ],
        "abstract": "There has been a great deal of recent interest in methods for performing lifted inference; however, most of this work assumes that the first-order model is given as input to the system. Here, we describe lifted inference algorithms that determine symmetries and automatically lift the probabilistic model to speedup inference. In particular, we describe approximate lifted inference techniques that allow the user to trade off inference accuracy for computational efficiency by using a handful of tunable parameters, while keeping the error bounded. Our algorithms are closely related to the graph-theoretic concept of bisimulation. We report experiments on both synthetic and real data to show that in the presence of symmetries, run-times for inference can be improved significantly, with approximate lifted inference providing orders of magnitude speedup over ground inference.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2619",
        "title": "Regret-based Reward Elicitation for Markov Decision Processes",
        "authors": [
            "Kevin Regan",
            "Craig Boutilier"
        ],
        "abstract": "The specification of aMarkov decision process (MDP) can be difficult. Reward function specification is especially problematic; in practice, it is often cognitively complex and time-consuming for users to precisely specify rewards. This work casts the problem of specifying rewards as one of preference elicitation and aims to minimize the degree of precision with which a reward function must be specified while still allowing optimal or near-optimal policies to be produced. We first discuss how robust policies can be computed for MDPs given only partial reward information using the minimax regret criterion. We then demonstrate how regret can be reduced by efficiently eliciting reward information using bound queries, using regret-reduction as a means for choosing suitable queries. Empirical results demonstrate that regret-based reward elicitation offers an effective way to produce near-optimal policies without resorting to the precise specification of the entire reward function.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2620",
        "title": "Exact Structure Discovery in Bayesian Networks with Less Space",
        "authors": [
            "Pekka Parviainen",
            "Mikko Koivisto"
        ],
        "abstract": "The fastest known exact algorithms for scorebased structure discovery in Bayesian networks on n nodes run in time and space 2nnO(1). The usage of these algorithms is limited to networks on at most around 25 nodes mainly due to the space requirement. Here, we study space-time tradeoffs for finding an optimal network structure. When little space is available, we apply the Gurevich-Shelah recurrence-originally proposed for the Hamiltonian path problem-and obtain time 22n-snO(1) in space 2snO(1) for any s = n/2, n/4, n/8, . . .; we assume the indegree of each node is bounded by a constant. For the more practical setting with moderate amounts of space, we present a novel scheme. It yields running time 2n(3/2)pnO(1) in space 2n(3/4)pnO(1) for any p = 0, 1, . . ., n/2; these bounds hold as long as the indegrees are at most 0.238n. Furthermore, the latter scheme allows easy and efficient parallelization beyond previous algorithms. We also explore empirically the potential of the presented techniques.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2621",
        "title": "Logical Inference Algorithms and Matrix Representations for Probabilistic Conditional Independence",
        "authors": [
            "Mathias Niepert"
        ],
        "abstract": "Logical inference algorithms for conditional independence (CI) statements have important applications from testing consistency during knowledge elicitation to constraintbased structure learning of graphical models. We prove that the implication problem for CI statements is decidable, given that the size of the domains of the random variables is known and fixed. We will present an approximate logical inference algorithm which combines a falsification and a novel validation algorithm. The validation algorithm represents each set of CI statements as a sparse 0-1 matrix A and validates instances of the implication problem by solving specific linear programs with constraint matrix A. We will show experimentally that the algorithm is both effective and efficient in validating and falsifying instances of the probabilistic CI implication problem.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2624",
        "title": "Convexifying the Bethe Free Energy",
        "authors": [
            "Ofer Meshi",
            "Ariel Jaimovich",
            "Amir Globerson",
            "Nir Friedman"
        ],
        "abstract": "The introduction of loopy belief propagation (LBP) revitalized the application of graphical models in many domains. Many recent works present improvements on the basic LBP algorithm in an attempt to overcome convergence and local optima problems. Notable among these are convexified free energy approximations that lead to inference procedures with provable convergence and quality properties. However, empirically LBP still outperforms most of its convex variants in a variety of settings, as we also demonstrate here. Motivated by this fact we seek convexified free energies that directly approximate the Bethe free energy. We show that the proposed approximations compare favorably with state-of-the art convex free energy approximations.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2625",
        "title": "Convergent message passing algorithms - a unifying view",
        "authors": [
            "Talya Meltzer",
            "Amir Globerson",
            "Yair Weiss"
        ],
        "abstract": "Message-passing algorithms have emerged as powerful techniques for approximate inference in graphical models. When these algorithms converge, they can be shown to find local (or sometimes even global) optima of variational formulations to the inference problem. But many of the most popular algorithms are not guaranteed to converge. This has lead to recent interest in convergent message-passing algorithms. In this paper, we present a unified view of convergent message-passing algorithms. We present a simple derivation of an abstract algorithm, tree-consistency bound optimization (TCBO) that is provably convergent in both its sum and max product forms. We then show that many of the existing convergent algorithms are instances of our TCBO algorithm, and obtain novel convergent algorithms \"for free\" by exchanging maximizations and summations in existing algorithms. In particular, we show that Wainwright's non-convergent sum-product algorithm for tree based variational bounds, is actually convergent with the right update order for the case where trees are monotonic chains.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2633",
        "title": "MAP Estimation of Semi-Metric MRFs via Hierarchical Graph Cuts",
        "authors": [
            "M. Pawan Kumar",
            "Daphne Koller"
        ],
        "abstract": "We consider the task of obtaining the maximum a posteriori estimate of discrete pairwise random fields with arbitrary unary potentials and semimetric pairwise potentials. For this problem, we propose an accurate hierarchical move making strategy where each move is computed efficiently by solving an st-MINCUT problem. Unlike previous move making approaches, e.g. the widely used a-expansion algorithm, our method obtains the guarantees of the standard linear programming (LP) relaxation for the important special case of metric labeling. Unlike the existing LP relaxation solvers, e.g. interior-point algorithms or tree-reweighted message passing, our method is significantly faster as it uses only the efficient st-MINCUT algorithm in its design. Using both synthetic and real data experiments, we show that our technique outperforms several commonly used algorithms.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2634",
        "title": "The Temporal Logic of Causal Structures",
        "authors": [
            "Samantha Kleinberg",
            "Bud Mishra"
        ],
        "abstract": "Computational analysis of time-course data with an underlying causal structure is needed in a variety of domains, including neural spike trains, stock price movements, and gene expression levels. However, it can be challenging to determine from just the numerical time course data alone what is coordinating the visible processes, to separate the underlying prima facie causes into genuine and spurious causes and to do so with a feasible computational complexity. For this purpose, we have been developing a novel algorithm based on a framework that combines notions of causality in philosophy with algorithmic approaches built on model checking and statistical techniques for multiple hypotheses testing. The causal relationships are described in terms of temporal logic formulae, reframing the inference problem in terms of model checking. The logic used, PCTL, allows description of both the time between cause and effect and the probability of this relationship being observed. We show that equipped with these causal formulae with their associated probabilities we may compute the average impact a cause makes to its effect and then discover statistically significant causes through the concepts of multiple hypothesis testing (treating each causal relationship as a hypothesis), and false discovery control. By exploring a well-chosen family of potentially all significant hypotheses with reasonably minimal description length, it is possible to tame the algorithm's computational complexity while exploring the nearly complete search-space of all prima facie causes. We have tested these ideas in a number of domains and illustrate them here with two examples.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2635",
        "title": "Constraint Processing in Lifted Probabilistic Inference",
        "authors": [
            "Jacek Kisynski",
            "David L Poole"
        ],
        "abstract": "First-order probabilistic models combine representational power of first-order logic with graphical models. There is an ongoing effort to design lifted inference algorithms for first-order probabilistic models. We analyze lifted inference from the perspective of constraint processing and, through this viewpoint, we analyze and compare existing approaches and expose their advantages and limitations. Our theoretical results show that the wrong choice of constraint processing method can lead to exponential increase in computational complexity. Our empirical tests confirm the importance of constraint processing in lifted inference. This is the first theoretical and empirical study of constraint processing in lifted inference.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2637",
        "title": "Counting Belief Propagation",
        "authors": [
            "Kristian Kersting",
            "Babak Ahmadi",
            "Sriraam Natarajan"
        ],
        "abstract": "A major benefit of graphical models is that most knowledge is captured in the model structure. Many models, however, produce inference problems with a lot of symmetries not reflected in the graphical structure and hence not exploitable by efficient inference techniques such as belief propagation (BP). In this paper, we present a new and simple BP algorithm, called counting BP, that exploits such additional symmetries. Starting from a given factor graph, counting BP first constructs a compressed factor graph of clusternodes and clusterfactors, corresponding to sets of nodes and factors that are indistinguishable given the evidence. Then it runs a modified BP algorithm on the compressed graph that is equivalent to running BP on the original factor graph. Our experiments show that counting BP is applicable to a variety of important AI tasks such as (dynamic) relational models and boolean model counting, and that significant efficiency gains are obtainable, often by orders of magnitude.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2639",
        "title": "MAP Estimation, Message Passing, and Perfect Graphs",
        "authors": [
            "Tony S. Jebara"
        ],
        "abstract": "Efficiently finding the maximum a posteriori (MAP) configuration of a graphical model is an important problem which is often implemented using message passing algorithms. The optimality of such algorithms is only well established for singly-connected graphs and other limited settings. This article extends the set of graphs where MAP estimation is in P and where message passing recovers the exact solution to so-called perfect graphs. This result leverages recent progress in defining perfect graphs (the strong perfect graph theorem), linear programming relaxations of MAP estimation and recent convergent message passing schemes. The article converts graphical models into nand Markov random fields which are straightforward to relax into linear programs. Therein, integrality can be established in general by testing for graph perfection. This perfection test is performed efficiently using a polynomial time algorithm. Alternatively, known decomposition tools from perfect graph theory may be used to prove perfection for certain families of graphs. Thus, a general graph framework is provided for determining when MAP estimation in any graphical model is in P, has an integral linear programming relaxation and is exactly recoverable by message passing.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2642",
        "title": "Improved Mean and Variance Approximations for Belief Net Responses via Network Doubling",
        "authors": [
            "Peter Hooper",
            "Yasin Abbasi-Yadkori",
            "Russell Greiner",
            "Bret Hoehn"
        ],
        "abstract": "A Bayesian belief network models a joint distribution with an directed acyclic graph representing dependencies among variables and network parameters characterizing conditional distributions. The parameters are viewed as random variables to quantify uncertainty about their values. Belief nets are used to compute responses to queries; i.e., conditional probabilities of interest. A query is a function of the parameters, hence a random variable. Van Allen et al. (2001, 2008) showed how to quantify uncertainty about a query via a delta method approximation of its variance. We develop more accurate approximations for both query mean and variance. The key idea is to extend the query mean approximation to a \"doubled network\" involving two independent replicates. Our method assumes complete data and can be applied to discrete, continuous, and hybrid networks (provided discrete variables have only discrete parents). We analyze several improvements, and provide empirical studies to demonstrate their effectiveness.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2645",
        "title": "Distributed Parallel Inference on Large Factor Graphs",
        "authors": [
            "Joseph E. Gonzalez",
            "Yucheng Low",
            "Carlos E. Guestrin",
            "David O'Hallaron"
        ],
        "abstract": "As computer clusters become more common and the size of the problems encountered in the field of AI grows, there is an increasing demand for efficient parallel inference algorithms. We consider the problem of parallel inference on large factor graphs in the distributed memory setting of computer clusters. We develop a new efficient parallel inference algorithm, DBRSplash, which incorporates over-segmented graph partitioning, belief residual scheduling, and uniform work Splash operations. We empirically evaluate the DBRSplash algorithm on a 120 processor cluster and demonstrate linear to super-linear performance gains on large factor graph models.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2647",
        "title": "Generating Optimal Plans in Highly-Dynamic Domains",
        "authors": [
            "Christian Fritz",
            "Sheila McIlraith"
        ],
        "abstract": "Generating optimal plans in highly dynamic environments is challenging. Plans are predicated on an assumed initial state, but this state can change unexpectedly during plan generation, potentially invalidating the planning effort. In this paper we make three contributions: (1) We propose a novel algorithm for generating optimal plans in settings where frequent, unexpected events interfere with planning. It is able to quickly distinguish relevant from irrelevant state changes, and to update the existing planning search tree if necessary. (2) We argue for a new criterion for evaluating plan adaptation techniques: the relative running time compared to the \"size\" of changes. This is significant since during recovery more changes may occur that need to be recovered from subsequently, and in order for this process of repeated recovery to terminate, recovery time has to converge. (3) We show empirically that our approach can converge and find optimal plans in environments that would ordinarily defy planning due to their high dynamics.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2651",
        "title": "Seeing the Forest Despite the Trees: Large Scale Spatial-Temporal Decision Making",
        "authors": [
            "Mark Crowley",
            "John Nelson",
            "David L Poole"
        ],
        "abstract": "We introduce a challenging real-world planning problem where actions must be taken at each location in a spatial area at each point in time. We use forestry planning as the motivating application. In Large Scale Spatial-Temporal (LSST) planning problems, the state and action spaces are defined as the cross-products of many local state and action spaces spread over a large spatial area such as a city or forest. These problems possess state uncertainty, have complex utility functions involving spatial constraints and we generally must rely on simulations rather than an explicit transition model. We define LSST problems as reinforcement learning problems and present a solution using policy gradients. We compare two different policy formulations: an explicit policy that identifies each location in space and the action to take there; and an abstract policy that defines the proportion of actions to take across all locations in space. We show that the abstract policy is more robust and achieves higher rewards with far fewer parameters than the elementary policy. This abstract policy is also a better fit to the properties that practitioners in LSST problem domains require for such methods to be widely useful.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2652",
        "title": "Complexity Analysis and Variational Inference for Interpretation-based Probabilistic Description Logic",
        "authors": [
            "Fabio Gagliardi Cozman",
            "Rodrigo Bellizia Polastro"
        ],
        "abstract": "This paper presents complexity analysis and variational methods for inference in probabilistic description logics featuring Boolean operators, quantification, qualified number restrictions, nominals, inverse roles and role hierarchies. Inference is shown to be PEXP-complete, and variational methods are designed so as to exploit logical inference whenever possible.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2655",
        "title": "Mean Field Variational Approximation for Continuous-Time Bayesian Networks",
        "authors": [
            "Ido Cohn",
            "Tal El-Hay",
            "Nir Friedman",
            "Raz Kupferman"
        ],
        "abstract": "Continuous-time Bayesian networks is a natural structured representation language for multicomponent stochastic processes that evolve continuously over time. Despite the compact representation, inference in such models is intractable even in relatively simple structured networks. Here we introduce a mean field variational approximation in which we use a product of inhomogeneous Markov processes to approximate a distribution over trajectories. This variational approach leads to a globally consistent distribution, which can be efficiently queried. Additionally, it provides a lower bound on the probability of observations, thus making it attractive for learning tasks. We provide the theoretical foundations for the approximation, an efficient implementation that exploits the wide range of highly optimized ordinary differential equations (ODE) solvers, experimentally explore characterizations of processes for which this approximation is suitable, and show applications to a large-scale realworld inference problem.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2659",
        "title": "Deterministic POMDPs Revisited",
        "authors": [
            "Blai Bonet"
        ],
        "abstract": "We study a subclass of POMDPs, called Deterministic POMDPs, that is characterized by deterministic actions and observations. These models do not provide the same generality of POMDPs yet they capture a number of interesting and challenging problems, and permit more efficient algorithms. Indeed, some of the recent work in planning is built around such assumptions mainly by the quest of amenable models more expressive than the classical deterministic models. We provide results about the fundamental properties of Deterministic POMDPs, their relation with AND/OR search problems and algorithms, and their computational complexity.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2665",
        "title": "Lower Bound Bayesian Networks - An Efficient Inference of Lower Bounds on Probability Distributions in Bayesian Networks",
        "authors": [
            "Daniel Andrade",
            "Bernhard Sick"
        ],
        "abstract": "We present a new method to propagate lower bounds on conditional probability distributions in conventional Bayesian networks. Our method guarantees to provide outer approximations of the exact lower bounds. A key advantage is that we can use any available algorithms and tools for Bayesian networks in order to represent and infer lower bounds. This new method yields results that are provable exact for trees with binary variables, and results which are competitive to existing approximations in credal networks for all other network structures. Our method is not limited to a specific kind of network structure. Basically, it is also not restricted to a specific kind of inference, but we restrict our analysis to prognostic inference in this article. The computational complexity is superior to that of other existing approaches.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2857",
        "title": "Operations on soft sets revisited",
        "authors": [
            "Ping Zhu",
            "Qiaoyan Wen"
        ],
        "abstract": "Soft sets, as a mathematical tool for dealing with uncertainty, have recently gained considerable attention, including some successful applications in information processing, decision, demand analysis, and forecasting. To construct new soft sets from given soft sets, some operations on soft sets have been proposed. Unfortunately, such operations cannot keep all classical set-theoretic laws true for soft sets. In this paper, we redefine the intersection, complement, and difference of soft sets and investigate the algebraic properties of these operations along with a known union operation. We find that the new operation system on soft sets inherits all basic properties of operations on classical sets, which justifies our definitions.\n    ",
        "submission_date": "2012-05-13T00:00:00",
        "last_modified_date": "2012-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.3054",
        "title": "Approximate Modified Policy Iteration",
        "authors": [
            "Bruno Scherrer",
            "Victor Gabillon",
            "Mohammad Ghavamzadeh",
            "Matthieu Geist"
        ],
        "abstract": "Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that contains the two celebrated policy and value iteration methods. Despite its generality, MPI has not been thoroughly studied, especially its approximation form which is used when the state and/or action spaces are large or infinite. In this paper, we propose three implementations of approximate MPI (AMPI) that are extensions of well-known approximate DP algorithms: fitted-value iteration, fitted-Q iteration, and classification-based policy iteration. We provide error propagation analyses that unify those for approximate policy and value iteration. On the last classification-based implementation, we develop a finite-sample analysis that shows that MPI's main parameter allows to control the balance between the estimation error of the classifier and the overall value function approximation.\n    ",
        "submission_date": "2012-05-14T00:00:00",
        "last_modified_date": "2012-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.3380",
        "title": "Unfair items detection in educational measurement",
        "authors": [
            "Yefim Bakman"
        ],
        "abstract": "Measurement professionals cannot come to an agreement on the definition of the term 'item fairness'. In this paper a continuous measure of item unfairness is proposed. The more the unfairness measure deviates from zero, the less fair the item is. If the measure exceeds the cutoff value, the item is identified as definitely unfair. The new approach can identify unfair items that would not be identified with conventional procedures. The results are in accord with experts' judgments on the item qualities. Since no assumptions about scores distributions and/or correlations are assumed, the method is applicable to any educational test. Its performance is illustrated through application to scores of a real test.\n    ",
        "submission_date": "2012-02-11T00:00:00",
        "last_modified_date": "2012-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.3663",
        "title": "The Good, the Bad, and the Odd: Cycles in Answer-Set Programs",
        "authors": [
            "Johannes Klaus Fichte"
        ],
        "abstract": "Backdoors of answer-set programs are sets of atoms that represent clever reasoning shortcuts through the search space. Assignments to backdoor atoms reduce the given program to several programs that belong to a tractable target class. Previous research has considered target classes based on notions of acyclicity where various types of cycles (good and bad cycles) are excluded from graph representations of programs. We generalize the target classes by taking the parity of the number of negative edges on bad cycles into account and consider backdoors for such classes. We establish new hardness results and non-uniform polynomial-time tractability relative to directed or undirected cycles.\n    ",
        "submission_date": "2012-02-15T00:00:00",
        "last_modified_date": "2012-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.3964",
        "title": "Machine Recognition of Hand Written Characters using Neural Networks",
        "authors": [
            "Yusuf Perwej",
            "Ashish Chaturvedi"
        ],
        "abstract": "Even today in Twenty First Century Handwritten communication has its own stand and most of the times, in daily life it is globally using as means of communication and recording the information like to be shared with others. Challenges in handwritten characters recognition wholly lie in the variation and distortion of handwritten characters, since different people may use different style of handwriting, and direction to draw the same shape of the characters of their known script. This paper demonstrates the nature of handwritten characters, conversion of handwritten data into electronic data, and the neural network approach to make machine capable of recognizing hand written characters.\n    ",
        "submission_date": "2012-05-17T00:00:00",
        "last_modified_date": "2012-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.3966",
        "title": "Neural Networks for Handwritten English Alphabet Recognition",
        "authors": [
            "Yusuf Perwej",
            "Ashish Chaturvedi"
        ],
        "abstract": "This paper demonstrates the use of neural networks for developing a system that can recognize hand-written English alphabets. In this system, each English alphabet is represented by binary values that are used as input to a simple feature extraction system, whose output is fed to our neural network system.\n    ",
        "submission_date": "2012-05-17T00:00:00",
        "last_modified_date": "2012-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.3981",
        "title": "kLog: A Language for Logical and Relational Learning with Kernels",
        "authors": [
            "Paolo Frasconi",
            "Fabrizio Costa",
            "Luc De Raedt",
            "Kurt De Grave"
        ],
        "abstract": "We introduce kLog, a novel approach to statistical relational learning. Unlike standard approaches, kLog does not represent a probability distribution directly. It is rather a language to perform kernel-based learning on expressive logical and relational representations. kLog allows users to specify learning problems declaratively. It builds on simple but powerful concepts: learning from interpretations, entity/relationship data modeling, logic programming, and deductive databases. Access by the kernel to the rich representation is mediated by a technique we call graphicalization: the relational representation is first transformed into a graph --- in particular, a grounded entity/relationship diagram. Subsequently, a choice of graph kernel defines the feature space. kLog supports mixed numerical and symbolic data, as well as background knowledge in the form of Prolog or Datalog programs as in inductive logic programming systems. The kLog framework can be applied to tackle the same range of tasks that has made statistical relational learning so popular, including classification, regression, multitask learning, and collective classification. We also report about empirical comparisons, showing that kLog can be either more accurate, or much faster at the same level of accuracy, than Tilde and Alchemy. kLog is GPLv3 licensed and is available at ",
        "submission_date": "2012-05-17T00:00:00",
        "last_modified_date": "2014-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.4378",
        "title": "Inferring Taxi Status Using GPS Trajectories",
        "authors": [
            "Yin Zhu",
            "Yu Zheng",
            "Liuhang Zhang",
            "Darshan Santani",
            "Xing Xie",
            "Qiang Yang"
        ],
        "abstract": "In this paper, we infer the statuses of a taxi, consisting of occupied, non-occupied and parked, in terms of its GPS trajectory. The status information can enable urban computing for improving a city's transportation systems and land use planning. In our solution, we first identify and extract a set of effective features incorporating the knowledge of a single trajectory, historical trajectories and geographic data like road network. Second, a parking status detection algorithm is devised to find parking places (from a given trajectory), dividing a trajectory into segments (i.e., sub-trajectories). Third, we propose a two-phase inference model to learn the status (occupied or non-occupied) of each point from a taxi segment. This model first uses the identified features to train a local probabilistic classifier and then carries out a Hidden Semi-Markov Model (HSMM) for globally considering long term travel patterns. We evaluated our method with a large-scale real-world trajectory dataset generated by 600 taxis, showing the advantages of our method over baselines.\n    ",
        "submission_date": "2012-05-20T00:00:00",
        "last_modified_date": "2012-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.5098",
        "title": "A Simplified Description of Fuzzy TOPSIS",
        "authors": [
            "Balwinder Sodhi",
            "Prabhakar T.V"
        ],
        "abstract": "A simplified description of Fuzzy TOPSIS (Technique for Order Preference by Similarity to Ideal Situation) is presented. We have adapted the TOPSIS description from existing Fuzzy theory literature and distilled the bare minimum concepts required for understanding and applying TOPSIS. An example has been worked out to illustrate the application of TOPSIS for a multi-criteria group decision making scenario.\n    ",
        "submission_date": "2012-05-23T00:00:00",
        "last_modified_date": "2017-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.5367",
        "title": "Language-Constraint Reachability Learning in Probabilistic Graphs",
        "authors": [
            "Claudio Taranto",
            "Nicola Di Mauro",
            "Floriana Esposito"
        ],
        "abstract": "The probabilistic graphs framework models the uncertainty inherent in real-world domains by means of probabilistic edges whose value quantifies the likelihood of the edge existence or the strength of the link it represents. The goal of this paper is to provide a learning method to compute the most likely relationship between two nodes in a framework based on probabilistic graphs. In particular, given a probabilistic graph we adopted the language-constraint reachability method to compute the probability of possible interconnections that may exists between two nodes. Each of these connections may be viewed as feature, or a factor, between the two nodes and the corresponding probability as its weight. Each observed link is considered as a positive instance for its corresponding link label. Given the training set of observed links a L2-regularized Logistic Regression has been adopted to learn a model able to predict unobserved link labels. The experiments on a real world collaborative filtering problem proved that the proposed approach achieves better results than that obtained adopting classical methods.\n    ",
        "submission_date": "2012-05-24T00:00:00",
        "last_modified_date": "2012-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.5866",
        "title": "Approximate Equalities on Rough Intuitionistic Fuzzy Sets and an Analysis of Approximate Equalities",
        "authors": [
            "B. K. Tripathy",
            "G. K. Panda"
        ],
        "abstract": "In order to involve user knowledge in determining equality of sets, which may not be equal in the mathematical sense, three types of approximate (rough) equalities were introduced by Novotny and Pawlak ([8, 9, 10]). These notions were generalized by Tripathy, Mitra and Ojha ([13]), who introduced the concepts of approximate (rough) equivalences of sets. Rough equivalences capture equality of sets at a higher level than rough equalities. More properties of these concepts were established in [14]. Combining the conditions for the two types of approximate equalities, two more approximate equalities were introduced by Tripathy [12] and a comparative analysis of their relative efficiency was provided. In [15], the four types of approximate equalities were extended by considering rough fuzzy sets instead of only rough sets. In fact the concepts of leveled approximate equalities were introduced and properties were studied. In this paper we proceed further by introducing and studying the approximate equalities based on rough intuitionistic fuzzy sets instead of rough fuzzy sets. That is we introduce the concepts of approximate (rough)equalities of intuitionistic fuzzy sets and study their properties. We provide some real life examples to show the applications of rough equalities of fuzzy sets and rough equalities of intuitionistic fuzzy sets.\n    ",
        "submission_date": "2012-05-26T00:00:00",
        "last_modified_date": "2012-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.0111",
        "title": "OpenGM: A C++ Library for Discrete Graphical Models",
        "authors": [
            "Bjoern Andres",
            "Thorsten Beier",
            "Joerg H. Kappes"
        ],
        "abstract": "OpenGM is a C++ template library for defining discrete graphical models and performing inference on these models, using a wide range of state-of-the-art algorithms. No restrictions are imposed on the factor graph to allow for higher-order factors and arbitrary neighborhood structures. Large models with repetitive structure are handled efficiently because (i) functions that occur repeatedly need to be stored only once, and (ii) distinct functions can be implemented differently, using different encodings alongside each other in the same model. Several parametric functions (e.g. metrics), sparse and dense value tables are provided and so is an interface for custom C++ code. Algorithms are separated by design from the representation of graphical models and are easily exchangeable. OpenGM, its algorithms, HDF5 file format and command line tools are modular and extendible.\n    ",
        "submission_date": "2012-06-01T00:00:00",
        "last_modified_date": "2012-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.0259",
        "title": "The Causal Topography of Cognition",
        "authors": [
            "Stevan Harnad"
        ],
        "abstract": "The causal structure of cognition can be simulated but not implemented computationally, just as the causal structure of a comet can be simulated but not implemented computationally. The only thing that allows us even to imagine otherwise is that cognition, unlike a comet, is invisible (to all but the cognizer).\n    ",
        "submission_date": "2012-02-25T00:00:00",
        "last_modified_date": "2012-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.0855",
        "title": "A Mixed Observability Markov Decision Process Model for Musical Pitch",
        "authors": [
            "Pouyan Rafiei Fard",
            "Keyvan Yahya"
        ],
        "abstract": "Partially observable Markov decision processes have been widely used to provide models for real-world decision making problems. In this paper, we will provide a method in which a slightly different version of them called Mixed observability Markov decision process, MOMDP, is going to join with our problem. Basically, we aim at offering a behavioural model for interaction of intelligent agents with musical pitch environment and we will show that how MOMDP can shed some light on building up a decision making model for musical pitch conveniently.\n    ",
        "submission_date": "2012-06-05T00:00:00",
        "last_modified_date": "2012-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.0918",
        "title": "Fuzzy Knowledge Representation Based on Possibilistic and Necessary Bayesian Networks",
        "authors": [
            "Abdelkader Heni",
            "Mohamed Nazih Omri",
            "Adel Alimi"
        ],
        "abstract": "Within the framework proposed in this paper, we address the issue of extending the certain networks to a fuzzy certain networks in order to cope with a vagueness and limitations of existing models for decision under imprecise and uncertain knowledge. This paper proposes a framework that combines two disciplines to exploit their own advantages in uncertain and imprecise knowledge representation problems. The framework proposed is a possibilistic logic based one in which Bayesian nodes and their properties are represented by local necessity-valued knowledge base. Data in properties are interpreted as set of valuated formulas. In our contribution possibilistic Bayesian networks have a qualitative part and a quantitative part, represented by local knowledge bases. The general idea is to study how a fusion of these two formalisms would permit representing compact way to solve efficiently problems for knowledge representation. We show how to apply possibility and necessity measures to the problem of knowledge representation with large scale data. On the other hand fuzzification of crisp certainty degrees to fuzzy variables improves the quality of the network and tends to bring smoothness and robustness in the network performance. The general aim is to provide a new approach for decision under uncertainty that combines three methodologies: Bayesian networks certainty distribution and fuzzy logic.\n    ",
        "submission_date": "2012-06-05T00:00:00",
        "last_modified_date": "2012-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.0925",
        "title": "Possibilistic Pertinence Feedback and Semantic Networks for Goal's Extraction",
        "authors": [
            "Mohamed Nazih Omri"
        ],
        "abstract": "Pertinence Feedback is a technique that enables a user to interactively express his information requirement by modifying his original query formulation with further information. This information is provided by explicitly confirming the pertinent of some indicating objects and/or goals extracted by the system. Obviously the user cannot mark objects and/or goals as pertinent until some are extracted, so the first search has to be initiated by a query and the initial query specification has to be good enough to pick out some pertinent objects and/or goals from the Semantic Network. In this paper we present a short survey of fuzzy and Semantic approaches to Knowledge Extraction. The goal of such approaches is to define flexible Knowledge Extraction Systems able to deal with the inherent vagueness and uncertainty of the Extraction process. It has long been recognised that interactivity improves the effectiveness of Knowledge Extraction systems. Novice user's queries are the most natural and interactive medium of communication and recent progress in recognition is making it possible to build systems that interact with the user. However, given the typical novice user's queries submitted to Knowledge Extraction Systems, it is easy to imagine that the effects of goal recognition errors in novice user's queries must be severely destructive on the system's effectiveness. The experimental work reported in this paper shows that the use of possibility theory in classical Knowledge Extraction techniques for novice user's query processing is more robust than the use of the probability theory. Moreover, both possibilistic and probabilistic pertinence feedback can be effectively employed to improve the effectiveness of novice user's query processing.\n    ",
        "submission_date": "2012-06-05T00:00:00",
        "last_modified_date": "2012-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.0976",
        "title": "Loopy Belief Propagation in Bayesian Networks : origin and possibilistic perspectives",
        "authors": [
            "Amen Ajroud",
            "Mohamed Nazih Omri",
            "Habib Youssef",
            "Salem Benferhat"
        ],
        "abstract": "In this paper we present a synthesis of the work performed on two inference algorithms: the Pearl's belief propagation (BP) algorithm applied to Bayesian networks without loops (i.e. polytree) and the Loopy belief propagation (LBP) algorithm (inspired from the BP) which is applied to networks containing undirected cycles. It is known that the BP algorithm, applied to Bayesian networks with loops, gives incorrect numerical results i.e. incorrect posterior probabilities. Murphy and al. [7] find that the LBP algorithm converges on several networks and when this occurs, LBP gives a good approximation of the exact posterior probabilities. However this algorithm presents an oscillatory behaviour when it is applied to QMR (Quick Medical Reference) network [15]. This phenomenon prevents the LBP algorithm from converging towards a good approximation of posterior probabilities. We believe that the translation of the inference computation problem from the probabilistic framework to the possibilistic framework will allow performance improvement of LBP algorithm. We hope that an adaptation of this algorithm to a possibilistic causal network will show an improvement of the convergence of LBP.\n    ",
        "submission_date": "2012-06-05T00:00:00",
        "last_modified_date": "2012-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.1061",
        "title": "Use of Fuzzy Sets in Semantic Nets for Providing On-Line Assistance to User of Technological Systems",
        "authors": [
            "Mohamed Nazih Omri",
            "Mohamed Ali Mahjoub"
        ],
        "abstract": "The main objective of this paper is to develop a new semantic Network structure, based on the fuzzy sets theory, used in Artificial Intelligent system in order to provide effective on-line assistance to users of new technological systems. This Semantic Networks is used to describe the knowledge of an \"ideal\" expert while fuzzy sets are used both to describe the approximate and uncertain knowledge of novice users who intervene to match fuzzy labels of a query with categories from an \"ideal\" expert. The technical system we consider is a word processor software, with Objects such as \"Word\" and Goals such as \"Cut\" or \"Copy\". We suggest to consider the set of the system's Goals as a set of linguistic variables to which corresponds a set of possible linguistic values based on the fuzzy set. We consider, therefore, a set of interpretation's levels for these possible values to which corresponds a set of membership functions. We also propose a method to measure the similarity degree between different fuzzy linguistic variables for the partition of the semantic network in class of similar objects to make easy the diagnosis of the user's fuzzy queries.\n    ",
        "submission_date": "2012-06-05T00:00:00",
        "last_modified_date": "2012-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.1069",
        "title": "Concepts and Their Dynamics: A Quantum-Theoretic Modeling of Human Thought",
        "authors": [
            "Diederik Aerts",
            "Liane Gabora",
            "Sandro Sozzo"
        ],
        "abstract": "We analyze different aspects of our quantum modeling approach of human concepts, and more specifically focus on the quantum effects of contextuality, interference, entanglement and emergence, illustrating how each of them makes its appearance in specific situations of the dynamics of human concepts and their combinations. We point out the relation of our approach, which is based on an ontology of a concept as an entity in a state changing under influence of a context, with the main traditional concept theories, i.e. prototype theory, exemplar theory and theory theory. We ponder about the question why quantum theory performs so well in its modeling of human concepts, and shed light on this question by analyzing the role of complex amplitudes, showing how they allow to describe interference in the statistics of measurement outcomes, while in the traditional theories statistics of outcomes originates in classical probability weights, without the possibility of interference. The relevance of complex numbers, the appearance of entanglement, and the role of Fock space in explaining contextual emergence, all as unique features of the quantum modeling, are explicitly revealed in this paper by analyzing human concepts and their dynamics.\n    ",
        "submission_date": "2012-06-05T00:00:00",
        "last_modified_date": "2013-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.1291",
        "title": "Feature Weighting for Improving Document Image Retrieval System Performance",
        "authors": [
            "Mohammadreza Keyvanpour",
            "Reza Tavoli"
        ],
        "abstract": "Feature weighting is a technique used to approximate the optimal degree of influence of individual features. This paper presents a feature weighting method for Document Image Retrieval System (DIRS) based on keyword spotting. In this method, we weight the feature using coefficient of multiple correlations. Coefficient of multiple correlations can be used to describe the synthesized effects and correlation of each feature. The aim of this paper is to show that feature weighting increases the performance of DIRS. After applying the feature weighting method to DIRS the average precision is 93.23% and average recall become 98.66% respectively\n    ",
        "submission_date": "2012-06-06T00:00:00",
        "last_modified_date": "2012-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.1319",
        "title": "Certain Bayesian Network based on Fuzzy knowledge Bases",
        "authors": [
            "Abdelkader Heni",
            "Mohamed Nazih Omri",
            "Adel Alimi"
        ],
        "abstract": "In this paper, we are trying to examine trade offs between fuzzy logic and certain Bayesian networks and we propose to combine their respective advantages into fuzzy certain Bayesian networks (FCBN), a certain Bayesian networks of fuzzy random variables. This paper deals with different definitions and classifications of uncertainty, sources of uncertainty, and theories and methodologies presented to deal with uncertainty. Fuzzification of crisp certainty degrees to fuzzy variables improves the quality of the network and tends to bring smoothness and robustness in the network performance. The aim is to provide a new approach for decision under uncertainty that combines three methodologies: Bayesian networks certainty distribution and fuzzy logic. Within the framework proposed in this paper, we address the issue of extending the certain networks to a fuzzy certain networks in order to cope with a vagueness and limitations of existing models for decision under imprecise and uncertain knowledge.\n    ",
        "submission_date": "2012-06-05T00:00:00",
        "last_modified_date": "2012-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.1414",
        "title": "An Intelligent Approach for Negotiating between chains in Supply Chain Management Systems",
        "authors": [
            "Shahab Firouzi",
            "Amin Nezarat"
        ],
        "abstract": "Holding commercial negotiations and selecting the best supplier in supply chain management systems are among weaknesses of producers in production process. Therefore, applying intelligent systems may have an effective role in increased speed and improved quality in the selections .This paper introduces a system which tries to trade using multi-agents systems and holding negotiations between any agents. In this system, an intelligent agent is considered for each segment of chains which it tries to send order and receive the response with attendance in negotiation medium and communication with other agents .This paper introduces how to communicate between agents, characteristics of multi-agent and standard registration medium of each agent in the environment. JADE (Java Application Development Environment) was used for implementation and simulation of agents cooperation.\n    ",
        "submission_date": "2012-06-07T00:00:00",
        "last_modified_date": "2012-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.1418",
        "title": "A weighted combination similarity measure for mobility patterns in wireless networks",
        "authors": [
            "Thuy Van T. Duong",
            "Dinh Que Tran",
            "Cong Hung Tran"
        ],
        "abstract": "The similarity between trajectory patterns in clustering has played an important role in discovering movement behaviour of different groups of mobile objects. Several approaches have been proposed to measure the similarity between sequences in trajectory data. Most of these measures are based on Euclidean space or on spatial network and some of them have been concerned with temporal aspect or ordering types. However, they are not appropriate to characteristics of spatiotemporal mobility patterns in wireless networks. In this paper, we propose a new similarity measure for mobility patterns in cellular space of wireless network. The framework for constructing our measure is composed of two phases as follows. First, we present formal definitions to capture mathematically two spatial and temporal similarity measures for mobility patterns. And then, we define the total similarity measure by means of a weighted combination of these similarities. The truth of the partial and total similarity measures are proved in mathematics. Furthermore, instead of the time interval or ordering, our work makes use of the timestamp at which two mobility patterns share the same cell. A case study is also described to give a comparison of the combination measure with other ones.\n    ",
        "submission_date": "2012-06-07T00:00:00",
        "last_modified_date": "2012-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.1458",
        "title": "Dispelling Classes Gradually to Improve Quality of Feature Reduction Approaches",
        "authors": [
            "Shervan Fekri Ershad",
            "Sattar Hashemi"
        ],
        "abstract": "Feature reduction is an important concept which is used for reducing dimensions to decrease the computation complexity and time of classification. Since now many approaches have been proposed for solving this problem, but almost all of them just presented a fix output for each input dataset that some of them aren't satisfied cases for classification. In this we proposed an approach as processing input dataset to increase accuracy rate of each feature extraction methods. First of all, a new concept called dispelling classes gradually (DCG) is proposed to increase separability of classes based on their labels. Next, this method is used to process input dataset of the feature reduction approaches to decrease the misclassification error rate of their outputs more than when output is achieved without any processing. In addition our method has a good quality to collate with noise based on adapting dataset with feature reduction approaches. In the result part, two conditions (With process and without that) are compared to support our idea by using some of UCI datasets.\n    ",
        "submission_date": "2012-06-07T00:00:00",
        "last_modified_date": "2012-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.1534",
        "title": "Software Aging Analysis of Web Server Using Neural Networks",
        "authors": [
            "G. Sumathi",
            "R. Raju"
        ],
        "abstract": "Software aging is a phenomenon that refers to progressive performance degradation or transient failures or even crashes in long running software systems such as web servers. It mainly occurs due to the deterioration of operating system resource, fragmentation and numerical error accumulation. A primitive method to fight against software aging is software rejuvenation. Software rejuvenation is a proactive fault management technique aimed at cleaning up the system internal state to prevent the occurrence of more severe crash failures in the future. It involves occasionally stopping the running software, cleaning its internal state and restarting it. An optimized schedule for performing the software rejuvenation has to be derived in advance because a long running application could not be put down now and then as it may lead to waste of cost. This paper proposes a method to derive an accurate and optimized schedule for rejuvenation of a web server (Apache) by using Radial Basis Function (RBF) based Feed Forward Neural Network, a variant of Artificial Neural Networks (ANN). Aging indicators are obtained through experimental setup involving Apache web server and clients, which acts as input to the neural network model. This method is better than existing ones because usage of RBF leads to better accuracy and speed in convergence.\n    ",
        "submission_date": "2012-06-07T00:00:00",
        "last_modified_date": "2012-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.1557",
        "title": "Soil Data Analysis Using Classification Techniques and Soil Attribute Prediction",
        "authors": [
            "Jay Gholap",
            "Anurag Ingole",
            "Jayesh Gohil",
            "Shailesh Gargade",
            "Vahida Attar"
        ],
        "abstract": "Agricultural research has been profited by technical advances such as automation, data mining. Today, data mining is used in a vast areas and many off-the-shelf data mining system products and domain specific data mining application soft wares are available, but data mining in agricultural soil datasets is a relatively a young research field. The large amounts of data that are nowadays virtually harvested along with the crops have to be analyzed and should be used to their full extent. This research aims at analysis of soil dataset using data mining techniques. It focuses on classification of soil using various algorithms available. Another important purpose is to predict untested attributes using regression technique, and implementation of automated soil sample classification.\n    ",
        "submission_date": "2012-06-07T00:00:00",
        "last_modified_date": "2012-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.1579",
        "title": "An Efficient Hybrid Ant Colony System for the Generalized Traveling Salesman Problem",
        "authors": [
            "Mohammad Reihaneh",
            "Daniel Karapetyan"
        ],
        "abstract": "The Generalized Traveling Salesman Problem (GTSP) is an extension of the well-known Traveling Salesman Problem (TSP), where the node set is partitioned into clusters, and the objective is to find the shortest cycle visiting each cluster exactly once. In this paper, we present a new hybrid Ant Colony System (ACS) algorithm for the symmetric GTSP. The proposed algorithm is a modification of a simple ACS for the TSP improved by an efficient GTSP-specific local search procedure. Our extensive computational experiments show that the use of the local search procedure dramatically improves the performance of the ACS algorithm, making it one of the most successful GTSP metaheuristics to date.\n    ",
        "submission_date": "2012-06-07T00:00:00",
        "last_modified_date": "2012-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.1678",
        "title": "A Distributed Optimized Patient Scheduling using Partial Information",
        "authors": [
            "G. Mageshwari",
            "E. Grace Mary Kanaga"
        ],
        "abstract": "A software agent may be a member of a Multi-Agent System (MAS) which is collectively performing a range of complex and intelligent tasks. In the hospital, scheduling decisions are finding difficult to schedule because of the dynamic changes and distribution. In order to face this problem with dynamic changes in the hospital, a new method, Distributed Optimized Patient Scheduling with Grouping (DOPSG) has been proposed. The goal of this method is that there is no necessity for knowing patient agents information globally. With minimal information this method works effectively. Scheduling problem can be solved for multiple departments in the hospital. Patient agents have been scheduled to the resource agent based on the patient priority to reduce the waiting time of patient agent and to reduce idle time of resources.\n    ",
        "submission_date": "2012-06-08T00:00:00",
        "last_modified_date": "2012-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.1724",
        "title": "Softening Fuzzy Knowledge Representation Tool with the Learning of New Words in Natural Language",
        "authors": [
            "Mohamed Nazih Omri"
        ],
        "abstract": "The approach described here allows using membership function to represent imprecise and uncertain knowledge by learning in Fuzzy Semantic Networks. This representation has a great practical interest due to the possibility to realize on the one hand, the construction of this membership function from a simple value expressing the degree of interpretation of an Object or a Goal as compared to an other and on the other hand, the adjustment of the membership function during the apprenticeship. We show, how to use these membership functions to represent the interpretation of an Object (respectively of a Goal) user as compared to an system Object (respectively to a Goal). We also show the possibility to make decision for each representation of an user Object compared to a system Object. This decision is taken by determining decision coefficient calculates according to the nucleus of the membership function of the user Object.\n    ",
        "submission_date": "2012-06-08T00:00:00",
        "last_modified_date": "2012-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.1794",
        "title": "Fuzzy Knowledge Representation, Learning and Optimization with Bayesian Analysis in Fuzzy Semantic Networks",
        "authors": [
            "Mohamed Nazih Omri"
        ],
        "abstract": "This paper presents a method of optimization, based on both Bayesian Analysis technical and Gallois Lattice, of a Fuzzy Semantic Networks. The technical System we use learn by interpreting an unknown word using the links created between this new word and known words. The main link is provided by the context of the query. When novice's query is confused with an unknown verb (goal) applied to a known noun denoting either an object in the ideal user's Network or an object in the user's Network, the system infer that this new verb corresponds to one of the known goal. With the learning of new words in natural language as the interpretation, which was produced in agreement with the user, the system improves its representation scheme at each experiment with a new user and, in addition, takes advantage of previous discussions with users. The semantic Net of user objects thus obtained by these kinds of learning is not always optimal because some relationships between couple of user objects can be generalized and others suppressed according to values of forces that characterize them. Indeed, to simplify the obtained Net, we propose to proceed to an inductive Bayesian analysis, on the Net obtained from Gallois lattice. The objective of this analysis can be seen as an operation of filtering of the obtained descriptive graph.\n    ",
        "submission_date": "2012-06-08T00:00:00",
        "last_modified_date": "2012-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.2347",
        "title": "Uncertain and Approximative Knowledge Representation to Reasoning on Classification with a Fuzzy Networks Based System",
        "authors": [
            "Mohamed Nazih Omri"
        ],
        "abstract": "The approach described here allows to use the fuzzy Object Based Representation of imprecise and uncertain knowledge. This representation has a great practical interest due to the possibility to realize reasoning on classification with a fuzzy semantic network based system. For instance, the distinction between necessary, possible and user classes allows to take into account exceptions that may appear on fuzzy knowledge-base and facilitates integration of user's Objects in the base. This approach describes the theoretical aspects of the architecture of the whole experimental A.I. system we built in order to provide effective on-line assistance to users of new technological systems: the understanding of \"how it works\" and \"how to complete tasks\" from queries in quite natural languages. In our model, procedural semantic networks are used to describe the knowledge of an \"ideal\" expert while fuzzy sets are used both to describe the approximative and uncertain knowledge of novice users in fuzzy semantic networks which intervene to match fuzzy labels of a query with categories from our \"ideal\" expert.\n    ",
        "submission_date": "2012-06-11T00:00:00",
        "last_modified_date": "2012-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3111",
        "title": "The third open Answer Set Programming competition",
        "authors": [
            "Francesco Calimeri",
            "Giovambattista Ianni",
            "Francesco Ricca"
        ],
        "abstract": "Answer Set Programming (ASP) is a well-established paradigm of declarative programming in close relationship with other declarative formalisms such as SAT Modulo Theories, Constraint Handling Rules, FO(.), PDDL and many others. Since its first informal editions, ASP systems have been compared in the now well-established ASP Competition. The Third (Open) ASP Competition, as the sequel to the ASP Competitions Series held at the University of Potsdam in Germany (2006-2007) and at the University of Leuven in Belgium in 2009, took place at the University of Calabria (Italy) in the first half of 2011. Participants competed on a pre-selected collection of benchmark problems, taken from a variety of domains as well as real world applications. The Competition ran on two tracks: the Model and Solve (M&S) Track, based on an open problem encoding, and open language, and open to any kind of system based on a declarative specification paradigm; and the System Track, run on the basis of fixed, public problem encodings, written in a standard ASP language. This paper discusses the format of the Competition and the rationale behind it, then reports the results for both tracks. Comparison with the second ASP competition and state-of-the-art solutions for some of the benchmark domains is eventually discussed.\n",
        "submission_date": "2012-06-14T00:00:00",
        "last_modified_date": "2012-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3232",
        "title": "AND/OR Importance Sampling",
        "authors": [
            "Vibhav Gogate",
            "Rina Dechter"
        ],
        "abstract": "The paper introduces AND/OR importance sampling for probabilistic graphical models. In contrast to importance sampling, AND/OR importance sampling caches samples in the AND/OR space and then extracts a new sample mean from the stored samples. We prove that AND/OR importance sampling may have lower variance than importance sampling; thereby providing a theoretical justification for preferring it over importance sampling. Our empirical evaluation demonstrates that AND/OR importance sampling is far more accurate than importance sampling in many cases.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3233",
        "title": "Speeding Up Planning in Markov Decision Processes via Automatically Constructed Abstractions",
        "authors": [
            "Alejandro Isaza",
            "Csaba Szepesvari",
            "Vadim Bulitko",
            "Russell Greiner"
        ],
        "abstract": "In this paper, we consider planning in stochastic shortest path (SSP) problems, a subclass of Markov Decision Problems (MDP). We focus on medium-size problems whose state space can be fully enumerated. This problem has numerous important applications, such as navigation and planning under uncertainty. We propose a new approach for constructing a multi-level hierarchy of progressively simpler abstractions of the original problem. Once computed, the hierarchy can be used to speed up planning by first finding a policy for the most abstract level and then recursively refining it into a solution to the original problem. This approach is fully automated and delivers a speed-up of two orders of magnitude over a state-of-the-art MDP solver on sample problems while returning near-optimal solutions. We also prove theoretical bounds on the loss of solution optimality resulting from the use of abstractions.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3244",
        "title": "Bayesian network learning by compiling to weighted MAX-SAT",
        "authors": [
            "James Cussens"
        ],
        "abstract": "The problem of learning discrete Bayesian networks from data is encoded as a weighted MAX-SAT problem and the MaxWalkSat local search algorithm is used to address it. For each dataset, the per-variable summands of the (BDeu) marginal likelihood for different choices of parents ('family scores') are computed prior to applying MaxWalkSat. Each permissible choice of parents for each variable is encoded as a distinct propositional atom and the associated family score encoded as a 'soft' weighted single-literal clause. Two approaches to enforcing acyclicity are considered: either by encoding the ancestor relation or by attaching a total order to each graph and encoding that. The latter approach gives better results. Learning experiments have been conducted on 21 synthetic datasets sampled from 7 BNs. The largest dataset has 10,000 datapoints and 60 variables producing (for the 'ancestor' encoding) a weighted CNF input file with 19,932 atoms and 269,367 clauses. For most datasets, MaxWalkSat quickly finds BNs with higher BDeu score than the 'true' BN. The effect of adding prior information is assessed. It is further shown that Bayesian model averaging can be effected by collecting BNs generated during the search.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3245",
        "title": "Identifying Optimal Sequential Decisions",
        "authors": [
            "Philip Dawid",
            "Vanessa Didelez"
        ],
        "abstract": "We consider conditions that allow us to find an optimal strategy for sequential decisions from a given data situation. For the case where all interventions are unconditional (atomic), identifiability has been discussed by Pearl & Robins (1995). We argue here that an optimal strategy must be conditional, i.e. take the information available at each decision point into account. We show that the identification of an optimal sequential decision strategy is more restrictive, in the sense that conditional interventions might not always be identified when atomic interventions are. We further demonstrate that a simple graphical criterion for the identifiability of an optimal strategy can be given.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3246",
        "title": "Strategy Selection in Influence Diagrams using Imprecise Probabilities",
        "authors": [
            "Cassio Polpo de Campos",
            "Qiang Ji"
        ],
        "abstract": "This paper describes a new algorithm to solve the decision making problem in Influence Diagrams based on algorithms for credal networks. Decision nodes are associated to imprecise probability distributions and a reformulation is introduced that finds the global maximum strategy with respect to the expected utility. We work with Limited Memory Influence Diagrams, which generalize most Influence Diagram proposals and handle simultaneous decisions. Besides the global optimum method, we explore an anytime approximate solution with a guaranteed maximum error and show that imprecise probabilities are handled in a straightforward way. Complexity issues and experiments with random diagrams and an effects-based military planning problem are discussed.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3248",
        "title": "Knowledge Combination in Graphical Multiagent Model",
        "authors": [
            "Quang Duong",
            "Michael P. Wellman",
            "Satinder Singh"
        ],
        "abstract": "A graphical multiagent model (GMM) represents a joint distribution over the behavior of a set of agents. One source of knowledge about agents' behavior may come from gametheoretic analysis, as captured by several graphical game representations developed in recent years. GMMs generalize this approach to express arbitrary distributions, based on game descriptions or other sources of knowledge bearing on beliefs about agent behavior. To illustrate the flexibility of GMMs, we exhibit game-derived models that allow probabilistic deviation from equilibrium, as well as models based on heuristic action choice. We investigate three different methods of integrating these models into a single model representing the combined knowledge sources. To evaluate the predictive performance of the combined model, we treat as actual outcome the behavior produced by a reinforcement learning process. We find that combining the two knowledge sources, using any of the methods, provides better predictions than either source alone. Among the combination methods, mixing data outperforms the opinion pool and direct update methods investigated in this empirical trial.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3250",
        "title": "Almost Optimal Intervention Sets for Causal Discovery",
        "authors": [
            "Frederick Eberhardt"
        ],
        "abstract": "We conjecture that the worst case number of experiments necessary and sufficient to discover a causal graph uniquely given its observational Markov equivalence class can be specified as a function of the largest clique in the Markov equivalence class. We provide an algorithm that computes intervention sets that we believe are optimal for the above task. The algorithm builds on insights gained from the worst case analysis in Eberhardt et al. (2005) for sequences of experiments when all possible directed acyclic graphs over N variables are considered. A simulation suggests that our conjecture is correct. We also show that a generalization of our conjecture to other classes of possible graph hypotheses cannot be given easily, and in what sense the algorithm is then no longer optimal.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3251",
        "title": "Gibbs Sampling in Factorized Continuous-Time Markov Processes",
        "authors": [
            "Tal El-Hay",
            "Nir Friedman",
            "Raz Kupferman"
        ],
        "abstract": "A central task in many applications is reasoning about processes that change over continuous time. Continuous-Time Bayesian Networks is a general compact representation language for multi-component continuous-time processes. However, exact inference in such processes is exponential in the number of components, and thus infeasible for most models of interest. Here we develop a novel Gibbs sampling procedure for multi-component processes. This procedure iteratively samples a trajectory for one of the components given the remaining ones. We show how to perform exact sampling that adapts to the natural time scale of the sampled process. Moreover, we show that this sampling procedure naturally exploits the structure of the network to reduce the computational cost of each step. This procedure is the first that can provide asymptotically unbiased approximation in such processes.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3258",
        "title": "Toward Experiential Utility Elicitation for Interface Customization",
        "authors": [
            "Bowen Hui",
            "Craig Boutilier"
        ],
        "abstract": "User preferences for automated assistance often vary widely, depending on the situation, and quality or presentation of help. Developing effectivemodels to learn individual preferences online requires domain models that associate observations of user behavior with their utility functions, which in turn can be constructed using utility elicitation techniques. However, most elicitation methods ask for users' predicted utilities based on hypothetical scenarios rather than more realistic experienced utilities. This is especially true in interface customization, where users are asked to assess novel interface designs. We propose experiential utility elicitation methods for customization and compare these to predictivemethods. As experienced utilities have been argued to better reflect true preferences in behavioral decision making, the purpose here is to investigate accurate and efficient procedures that are suitable for software domains. Unlike conventional elicitation, our results indicate that an experiential approach helps people understand stochastic outcomes, as well as better appreciate the sequential utility of intelligent assistance.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3263",
        "title": "Sparse Stochastic Finite-State Controllers for POMDPs",
        "authors": [
            "Eric A. Hansen"
        ],
        "abstract": "Bounded policy iteration is an approach to solving infinite-horizon POMDPs that represents policies as stochastic finite-state controllers and iteratively improves a controller by adjusting the parameters of each node using linear programming. In the original algorithm, the size of the linear programs, and thus the complexity of policy improvement, depends on the number of parameters of each node, which grows with the size of the controller. But in practice, the number of parameters of a node with non-zero values is often very small, and does not grow with the size of the controller. Based on this observation, we develop a version of bounded policy iteration that leverages the sparse structure of a stochastic finite-state controller. In each iteration, it improves a policy by the same amount as the original algorithm, but with much better scalability.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3264",
        "title": "Sampling First Order Logical Particles",
        "authors": [
            "Hannaneh Hajishirzi",
            "Eyal Amir"
        ],
        "abstract": "Approximate inference in dynamic systems is the problem of estimating the state of the system given a sequence of actions and partial observations. High precision estimation is fundamental in many applications like diagnosis, natural language processing, tracking, planning, and robotics. In this paper we present an algorithm that samples possible deterministic executions of a probabilistic sequence. The algorithm takes advantage of a compact representation (using first order logic) for actions and world states to improve the precision of its estimation. Theoretical and empirical results show that the algorithm's expected error is smaller than propositional sampling and Sequential Monte Carlo (SMC) sampling techniques.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3265",
        "title": "The Computational Complexity of Sensitivity Analysis and Parameter Tuning",
        "authors": [
            "Johan Kwisthout",
            "Linda C. van der Gaag"
        ],
        "abstract": "While known algorithms for sensitivity analysis and parameter tuning in probabilistic networks have a running time that is exponential in the size of the network, the exact computational complexity of these problems has not been established as yet. In this paper we study several variants of the tuning problem and show that these problems are NPPP-complete in general. We further show that the problems remain NP-complete or PP-complete, for a number of restricted variants. These complexity results provide insight in whether or not recent achievements in sensitivity analysis and tuning can be extended to more general, practicable methods.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3266",
        "title": "Partitioned Linear Programming Approximations for MDPs",
        "authors": [
            "Branislav Kveton",
            "Milos Hauskrecht"
        ],
        "abstract": "Approximate linear programming (ALP) is an efficient approach to solving large factored Markov decision processes (MDPs). The main idea of the method is to approximate the optimal value function by a set of basis functions and optimize their weights by linear programming (LP). This paper proposes a new ALP approximation. Comparing to the standard ALP formulation, we decompose the constraint space into a set of low-dimensional spaces. This structure allows for solving the new LP efficiently. In particular, the constraints of the LP can be satisfied in a compact form without an exponential dependence on the treewidth of ALP constraints. We study both practical and theoretical aspects of the proposed approach. Moreover, we demonstrate its scale-up potential on an MDP with more than 2^100 states.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3271",
        "title": "Learning Arithmetic Circuits",
        "authors": [
            "Daniel Lowd",
            "Pedro Domingos"
        ],
        "abstract": "Graphical models are usually learned without regard to the cost of doing inference with them. As a result, even if a good model is learned, it may perform poorly at prediction, because it requires approximate inference. We propose an alternative: learning models with a score function that directly penalizes the cost of inference. Specifically, we learn arithmetic circuits with a penalty on the number of edges in the circuit (in which the cost of inference is linear). Our algorithm is equivalent to learning a Bayesian network with context-specific independence by greedily splitting conditional distributions, at each step scoring the candidates by compiling the resulting network into an arithmetic circuit, and using its size as the penalty. We show how this can be done efficiently, without compiling a circuit from scratch for each candidate. Experiments on several real-world domains show that our algorithm is able to learn tractable models with very large treewidth, and yields more accurate predictions than a standard context-specific Bayesian network learner, in far less time.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3272",
        "title": "Improving Gradient Estimation by Incorporating Sensor Data",
        "authors": [
            "Gregory Lawrence",
            "Stuart Russell"
        ],
        "abstract": "An efficient policy search algorithm should estimate the local gradient of the objective function, with respect to the policy parameters, from as few trials as possible. Whereas most policy search methods estimate this gradient by observing the rewards obtained during policy trials, we show, both theoretically and empirically, that taking into account the sensor data as well gives better gradient estimates and hence faster learning. The reason is that rewards obtained during policy execution vary from trial to trial due to noise in the environment; sensor data, which correlates with the noise, can be used to partially correct for this variation, resulting in an estimatorwith lower variance.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3273",
        "title": "Discovering Cyclic Causal Models by Independent Components Analysis",
        "authors": [
            "Gustavo Lacerda",
            "Peter L. Spirtes",
            "Joseph Ramsey",
            "Patrik O. Hoyer"
        ],
        "abstract": "We generalize Shimizu et al's (2006) ICA-based approach for discovering linear non-Gaussian acyclic (LiNGAM) Structural Equation Models (SEMs) from causally sufficient, continuous-valued observational data. By relaxing the assumption that the generating SEM's graph is acyclic, we solve the more general problem of linear non-Gaussian (LiNG) SEM discovery. LiNG discovery algorithms output the distribution equivalence class of SEMs which, in the large sample limit, represents the population distribution. We apply a LiNG discovery algorithm to simulated data. Finally, we give sufficient conditions under which only one of the SEMs in the output class is 'stable'.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3276",
        "title": "Explanation Trees for Causal Bayesian Networks",
        "authors": [
            "Ulf Nielsen",
            "Jean-Philippe Pellet",
            "Andr\u00e9 Elisseeff"
        ],
        "abstract": "Bayesian networks can be used to extract explanations about the observed state of a subset of variables. In this paper, we explicate the desiderata of an explanation and confront them with the concept of explanation proposed by existing methods. The necessity of taking into account causal approaches when a causal graph is available is discussed. We then introduce causal explanation trees, based on the construction of explanation trees using the measure of causal information ow (Ay and Polani, 2006). This approach is compared to several other methods on known networks.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3280",
        "title": "CT-NOR: Representing and Reasoning About Events in Continuous Time",
        "authors": [
            "Aleksandr Simma",
            "Moises Goldszmidt",
            "John MacCormick",
            "Paul Barham",
            "Richard Black",
            "Rebecca Isaacs",
            "Richard Mortier"
        ],
        "abstract": "We present a generative model for representing and reasoning about the relationships among events in continuous time. We apply the model to the domain of networked and distributed computing environments where we fit the parameters of the model from timestamp observations, and then use hypothesis testing to discover dependencies between the events and changes in behavior for monitoring and diagnosis. After introducing the model, we present an EM algorithm for fitting the parameters and then present the hypothesis testing approach for both dependence discovery and change-point detection. We validate the approach for both tasks using real data from a trace of network events at Microsoft Research Cambridge. Finally, we formalize the relationship between the proposed model and the noisy-or gate for cases when time can be discretized.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3281",
        "title": "Model-Based Bayesian Reinforcement Learning in Large Structured Domains",
        "authors": [
            "Stephane Ross",
            "Joelle Pineau"
        ],
        "abstract": "Model-based Bayesian reinforcement learning has generated significant interest in the AI community as it provides an elegant solution to the optimal exploration-exploitation tradeoff in classical reinforcement learning. Unfortunately, the applicability of this type of approach has been limited to small domains due to the high complexity of reasoning about the joint posterior over model parameters. In this paper, we consider the use of factored representations combined with online planning techniques, to improve scalability of these methods. The main contribution of this paper is a Bayesian framework for learning the structure and parameters of a dynamical system, while also simultaneously planning a (near-)optimal sequence of actions.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3282",
        "title": "Improving the Accuracy and Efficiency of MAP Inference for Markov Logic",
        "authors": [
            "Sebastian Riedel"
        ],
        "abstract": "In this work we present Cutting Plane Inference (CPI), a Maximum A Posteriori (MAP) inference method for Statistical Relational Learning. Framed in terms of Markov Logic and inspired by the Cutting Plane Method, it can be seen as a meta algorithm that instantiates small parts of a large and complex Markov Network and then solves these using a conventional MAP method. We evaluate CPI on two tasks, Semantic Role Labelling and Joint Entity Resolution, while plugging in two different MAP inference methods: the current method of choice for MAP inference in Markov Logic, MaxWalkSAT, and Integer Linear Programming. We observe that when used with CPI both methods are significantly faster than when used alone. In addition, CPI improves the accuracy of MaxWalkSAT and maintains the exactness of Integer Linear Programming.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3283",
        "title": "Observation Subset Selection as Local Compilation of Performance Profiles",
        "authors": [
            "Yan Radovilsky",
            "Solomon Eyal Shimony"
        ],
        "abstract": "Deciding what to sense is a crucial task, made harder by dependencies and by a nonadditive utility function. We develop approximation algorithms for selecting an optimal set of measurements, under a dependency structure modeled by a tree-shaped Bayesian network (BN). Our approach is a generalization of composing anytime algorithm represented by conditional performance profiles. This is done by relaxing the input monotonicity assumption, and extending the local compilation technique to more general classes of performance profiles (PPs). We apply the extended scheme to selecting a subset of measurements for choosing a maximum expectation variable in a binary valued BN, and for minimizing the worst variance in a Gaussian BN.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3284",
        "title": "Bounding Search Space Size via (Hyper)tree Decompositions",
        "authors": [
            "Lars Otten",
            "Rina Dechter"
        ],
        "abstract": "This paper develops a measure for bounding the performance of AND/OR search algorithms for solving a variety of queries over graphical models. We show how drawing a connection to the recent notion of hypertree decompositions allows to exploit determinism in the problem specification and produce tighter bounds. We demonstrate on a variety of practical problem instances that we are often able to improve upon existing bounds by several orders of magnitude.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3285",
        "title": "Dyna-Style Planning with Linear Function Approximation and Prioritized Sweeping",
        "authors": [
            "Richard S. Sutton",
            "Csaba Szepesvari",
            "Alborz Geramifard",
            "Michael P. Bowling"
        ],
        "abstract": "We consider the problem of efficiently learning optimal control policies and value functions over large state spaces in an online setting in which estimates must be available after each interaction with the world. This paper develops an explicitly model-based approach extending the Dyna architecture to linear function approximation. Dynastyle planning proceeds by generating imaginary experience from the world model and then applying model-free reinforcement learning algorithms to the imagined state transitions. Our main results are to prove that linear Dyna-style planning converges to a unique solution independent of the generating distribution, under natural conditions. In the policy evaluation setting, we prove that the limit point is the least-squares (LSTD) solution. An implication of our results is that prioritized-sweeping can be soundly extended to the linear approximation case, backing up to preceding features rather than to preceding states. We introduce two versions of prioritized sweeping with linear Dyna and briefly illustrate their performance empirically on the Mountain Car and Boyan Chain problems.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3286",
        "title": "New Techniques for Algorithm Portfolio Design",
        "authors": [
            "Matthew Streeter",
            "Stephen F. Smith"
        ],
        "abstract": "We present and evaluate new techniques for designing algorithm portfolios. In our view, the problem has both a scheduling aspect and a machine learning aspect. Prior work has largely addressed one of the two aspects in isolation. Building on recent work on the scheduling aspect of the problem, we present a technique that addresses both aspects simultaneously and has attractive theoretical guarantees. Experimentally, we show that this technique can be used to improve the performance of state-of-the-art algorithms for Boolean satisfiability, zero-one integer programming, and A.I. planning.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3289",
        "title": "Efficient inference in persistent Dynamic Bayesian Networks",
        "authors": [
            "Tomas Singliar",
            "Denver Dash"
        ],
        "abstract": "Numerous temporal inference tasks such as fault monitoring and anomaly detection exhibit a persistence property: for example, if something breaks, it stays broken until an intervention. When modeled as a Dynamic Bayesian Network, persistence adds dependencies between adjacent time slices, often making exact inference over time intractable using standard inference algorithms. However, we show that persistence implies a regular structure that can be exploited for efficient inference. We present three successively more general classes of models: persistent causal chains (PCCs), persistent causal trees (PCTs) and persistent polytrees (PPTs), and the corresponding exact inference algorithms that exploit persistence. We show that analytic asymptotic bounds for our algorithms compare favorably to junction tree inference; and we demonstrate empirically that we can perform exact smoothing on the order of 100 times faster than the approximate Boyen-Koller method on randomly generated instances of persistent tree models. We also show how to handle non-persistent variables and how persistence can be exploited effectively for approximate filtering.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3291",
        "title": "Hierarchical POMDP Controller Optimization by Likelihood Maximization",
        "authors": [
            "Marc Toussaint",
            "Laurent Charlin",
            "Pascal Poupart"
        ],
        "abstract": "Planning can often be simpli ed by decomposing the task into smaller tasks arranged hierarchically. Charlin et al. [4] recently showed that the hierarchy discovery problem can be framed as a non-convex optimization problem. However, the inherent computational di culty of solving such an optimization problem makes it hard to scale to realworld problems. In another line of research, Toussaint et al. [18] developed a method to solve planning problems by maximumlikelihood estimation. In this paper, we show how the hierarchy discovery problem in partially observable domains can be tackled using a similar maximum likelihood approach. Our technique rst transforms the problem into a dynamic Bayesian network through which a hierarchical structure can naturally be discovered while optimizing the policy. Experimental results demonstrate that this approach scales better than previous techniques based on non-convex optimization.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3292",
        "title": "Identifying Dynamic Sequential Plans",
        "authors": [
            "Jin Tian"
        ],
        "abstract": "We address the problem of identifying dynamic sequential plans in the framework of causal Bayesian networks, and show that the problem is reduced to identifying causal effects, for which there are complete identi cation algorithms available in the literature.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3293",
        "title": "Propagation using Chain Event Graphs",
        "authors": [
            "Peter Thwaites",
            "Jim Q. Smith",
            "Robert G. Cowell"
        ],
        "abstract": "A Chain Event Graph (CEG) is a graphial model which designed to embody conditional independencies in problems whose state spaces are highly asymmetric and do not admit a natural product structure. In this paer we present a probability propagation algorithm which uses the topology of the CEG to build a transporter CEG. Intriungly,the transporter CEG is directly analogous to the triangulated Bayesian Network (BN) in the more conventional junction tree propagation algorithms used with BNs. The propagation method uses factorization formulae also analogous to (but different from) the ones using potentials on cliques and separators of the BN. It appears that the methods will be typically more efficient than the BN algorithms when applied to contexts where there is significant asymmetry present.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3295",
        "title": "Refractor Importance Sampling",
        "authors": [
            "Haohai Yu",
            "Robert A. van Engelen"
        ],
        "abstract": "In this paper we introduce Refractor Importance Sampling (RIS), an improvement to reduce error variance in Bayesian network importance sampling propagation under evidential reasoning. We prove the existence of a collection of importance functions that are close to the optimal importance function under evidential reasoning. Based on this theoretic result we derive the RIS algorithm. RIS approaches the optimal importance function by applying localized arc changes to minimize the divergence between the evidence-adjusted importance function and the optimal importance function. The validity and performance of RIS is empirically tested with a large setof synthetic Bayesian networks and two real-world networks.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3296",
        "title": "Inference for Multiplicative Models",
        "authors": [
            "Ydo Wexler",
            "Christopher Meek"
        ],
        "abstract": "The paper introduces a generalization for known probabilistic models such as log-linear and graphical models, called here multiplicative models. These models, that express probabilities via product of parameters are shown to capture multiple forms of contextual independence between variables, including decision graphs and noisy-OR functions. An inference algorithm for multiplicative models is provided and its correctness is proved. The complexity analysis of the inference algorithm uses a more refined parameter than the tree-width of the underlying graph, and shows the computational cost does not exceed that of the variable elimination algorithm in graphical models. The paper ends with examples where using the new models and algorithm is computationally beneficial.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3318",
        "title": "On Local Regret",
        "authors": [
            "Michael Bowling",
            "Martin Zinkevich"
        ],
        "abstract": "Online learning aims to perform nearly as well as the best hypothesis in hindsight. For some hypothesis classes, though, even finding the best hypothesis offline is challenging. In such offline cases, local search techniques are often employed and only local optimality guaranteed. For online decision-making with such hypothesis classes, we introduce local regret, a generalization of regret that aims to perform nearly as well as only nearby hypotheses. We then present a general algorithm to minimize local regret with arbitrary locality graphs. We also show how the graph structure can be exploited to drastically speed learning. These algorithms are then demonstrated on a diverse set of online problems: online disjunct learning, online Max-SAT, and online decision tree learning.\n    ",
        "submission_date": "2012-06-14T00:00:00",
        "last_modified_date": "2012-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3382",
        "title": "Simple Regret Optimization in Online Planning for Markov Decision Processes",
        "authors": [
            "Zohar Feldman",
            "Carmel Domshlak"
        ],
        "abstract": "We consider online planning in Markov decision processes (MDPs). In online planning, the agent focuses on its current state only, deliberates about the set of possible policies from that state onwards and, when interrupted, uses the outcome of that exploratory deliberation to choose what action to perform next. The performance of algorithms for online planning is assessed in terms of simple regret, which is the agent's expected performance loss when the chosen action, rather than an optimal one, is followed.\n",
        "submission_date": "2012-06-15T00:00:00",
        "last_modified_date": "2012-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3536",
        "title": "Identifying Independence in Relational Models",
        "authors": [
            "Marc Maier",
            "David Jensen"
        ],
        "abstract": "The rules of d-separation provide a framework for deriving conditional independence facts from model structure. However, this theory only applies to simple directed graphical models. We introduce relational d-separation, a theory for deriving conditional independence in relational models. We provide a sound, complete, and computationally efficient method for relational d-separation, and we present empirical results that demonstrate effectiveness.\n    ",
        "submission_date": "2012-06-15T00:00:00",
        "last_modified_date": "2013-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3551",
        "title": "Sensitivity analysis in decision circuits",
        "authors": [
            "Debarun Bhattacharjya",
            "Ross D. Shachter"
        ],
        "abstract": "Decision circuits have been developed to perform efficient evaluation of influence diagrams [Bhattacharjya and Shachter, 2007], building on the advances in arithmetic circuits for belief network inference [Darwiche,2003]. In the process of model building and analysis, we perform sensitivity analysis to understand how the optimal solution changes in response to changes in the model. When sequential decision problems under uncertainty are represented as decision circuits, we can exploit the efficient solution process embodied in the decision circuit and the wealth of derivative information available to compute the value of information for the uncertainties in the problem and the effects of changes to model parameters on the value and the optimal strategy.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3555",
        "title": "A Dynamic Programming Algorithm for Inference in Recursive Probabilistic Programs",
        "authors": [
            "Andreas Stuhlm\u00fcller",
            "Noah D. Goodman"
        ],
        "abstract": "We describe a dynamic programming algorithm for computing the marginal distribution of discrete probabilistic programs. This algorithm takes a functional interpreter for an arbitrary probabilistic programming language and turns it into an efficient marginalizer. Because direct caching of sub-distributions is impossible in the presence of recursion, we build a graph of dependencies between sub-distributions. This factored sum-product network makes (potentially cyclic) dependencies between subproblems explicit, and corresponds to a system of equations for the marginal distribution. We solve these equations by fixed-point iteration in topological order. We illustrate this algorithm on examples used in teaching probabilistic models, computational cognitive science research, and game theory.\n    ",
        "submission_date": "2012-06-15T00:00:00",
        "last_modified_date": "2012-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3614",
        "title": "A Linear-Programming Approximation of AC Power Flows",
        "authors": [
            "Carleton Coffrin",
            "Pascal Van Hentenryck"
        ],
        "abstract": "Linear active-power-only DC power flow approximations are pervasive in the planning and control of power systems. However, these approximations fail to capture reactive power and voltage magnitudes, both of which are necessary in many applications to ensure voltage stability and AC power flow feasibility. This paper proposes linear-programming models (the LPAC models) that incorporate reactive power and voltage magnitudes in a linear power flow approximation. The LPAC models are built on a convex approximation of the cosine terms in the AC equations, as well as Taylor approximations of the remaining nonlinear terms. Experimental comparisons with AC solutions on a variety of standard IEEE and MatPower benchmarks show that the LPAC models produce accurate values for active and reactive power, phase angles, and voltage magnitudes. The potential benefits of the LPAC models are illustrated on two \"proof-of-concept\" studies in power restoration and capacitor placement.\n    ",
        "submission_date": "2012-06-16T00:00:00",
        "last_modified_date": "2013-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3658",
        "title": "Alan Turing and the \"Hard\" and \"Easy\" Problem of Cognition: Doing and Feeling",
        "authors": [
            "Stevan Harnad"
        ],
        "abstract": "The \"easy\" problem of cognitive science is explaining how and why we can do what we can do. The \"hard\" problem is explaining how and why we feel. Turing's methodology for cognitive science (the Turing Test) is based on doing: Design a model that can do anything a human can do, indistinguishably from a human, to a human, and you have explained cognition. Searle has shown that the successful model cannot be solely computational. Sensory-motor robotic capacities are necessary to ground some, at least, of the model's words, in what the robot can do with the things in the world that the words are about. But even grounding is not enough to guarantee that -- nor to explain how and why -- the model feels (if it does). That problem is much harder to solve (and perhaps insoluble).\n    ",
        "submission_date": "2012-06-16T00:00:00",
        "last_modified_date": "2012-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3959",
        "title": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (2009)",
        "authors": [
            "Jeff Bilmes",
            "Andrew Ng"
        ],
        "abstract": "This is the Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, which was held in Montreal, QC, Canada, June 18 - 21 2009.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.4329",
        "title": "An Improved Gauss-Newtons Method based Back-propagation Algorithm for Fast Convergence",
        "authors": [
            "Sudarshan Nandy",
            "Partha Pratim Sarkar",
            "Achintya Das"
        ],
        "abstract": "The present work deals with an improved back-propagation algorithm based on Gauss-Newton numerical optimization method for fast convergence. The steepest descent method is used for the back-propagation. The algorithm is tested using various datasets and compared with the steepest descent back-propagation algorithm. In the system, optimization is carried out using multilayer neural network. The efficacy of the proposed method is observed during the training period as it converges quickly for the dataset used in test. The requirement of memory for computing the steps of algorithm is also analyzed.\n    ",
        "submission_date": "2012-06-19T00:00:00",
        "last_modified_date": "2012-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.4613",
        "title": "Near-Optimal BRL using Optimistic Local Transitions",
        "authors": [
            "Mauricio Araya",
            "Olivier Buffet",
            "Vincent Thomas"
        ],
        "abstract": "Model-based Bayesian Reinforcement Learning (BRL) allows a found formalization of the problem of acting optimally while facing an unknown environment, i.e., avoiding the exploration-exploitation dilemma. However, algorithms explicitly addressing BRL suffer from such a combinatorial explosion that a large body of work relies on heuristic algorithms. This paper introduces BOLT, a simple and (almost) deterministic heuristic algorithm for BRL which is optimistic about the transition function. We analyze BOLT's sample complexity, and show that under certain parameters, the algorithm is near-optimal in the Bayesian sense with high probability. Then, experimental results highlight the key differences of this method compared to previous work.\n    ",
        "submission_date": "2012-06-18T00:00:00",
        "last_modified_date": "2012-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.4654",
        "title": "A Generalized Loop Correction Method for Approximate Inference in Graphical Models",
        "authors": [
            "Siamak Ravanbakhsh",
            "Chun-Nam Yu",
            "Russell Greiner"
        ],
        "abstract": "Belief Propagation (BP) is one of the most popular methods for inference in probabilistic graphical models. BP is guaranteed to return the correct answer for tree structures, but can be incorrect or non-convergent for loopy graphical models. Recently, several new approximate inference algorithms based on cavity distribution have been proposed. These methods can account for the effect of loops by incorporating the dependency between BP messages. Alternatively, region-based approximations (that lead to methods such as Generalized Belief Propagation) improve upon BP by considering interactions within small clusters of variables, thus taking small loops within these clusters into account. This paper introduces an approach, Generalized Loop Correction (GLC), that benefits from both of these types of loop correction. We show how GLC relates to these two families of inference methods, then provide empirical evidence that GLC works effectively in general, and can be significantly more accurate than both correction schemes.\n    ",
        "submission_date": "2012-06-18T00:00:00",
        "last_modified_date": "2012-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5242",
        "title": "Studies in Lower Bounding Probabilities of Evidence using the Markov Inequality",
        "authors": [
            "Vibhav Gogate",
            "Bozhena Bidyuk",
            "Rina Dechter"
        ],
        "abstract": "Computing the probability of evidence even with known error bounds is NP-hard. In this paper we address this hard problem by settling on an easier problem. We propose an approximation which provides high confidence lower bounds on probability of evidence but does not have any guarantees in terms of relative or absolute error. Our proposed approximation is a randomized importance sampling scheme that uses the Markov inequality. However, a straight-forward application of the Markov inequality may lead to poor lower bounds. We therefore propose several heuristic measures to improve its performance in practice. Empirical evaluation of our scheme with state-of- the-art lower bounding schemes reveals the promise of our approach.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5244",
        "title": "Search for Choquet-optimal paths under uncertainty",
        "authors": [
            "Lucie Galand",
            "Patrice Perny"
        ],
        "abstract": "Choquet expected utility (CEU) is one of the most sophisticated decision criteria used in decision theory under uncertainty. It provides a generalisation of expected utility enhancing both descriptive and prescriptive possibilities. In this paper, we investigate the use of CEU for path-planning under uncertainty with a special focus on robust solutions. We first recall the main features of the CEU model and introduce some examples showing its descriptive potential. Then we focus on the search for Choquet-optimal paths in multivalued implicit graphs where costs depend on different scenarios. After discussing complexity issues, we propose two different heuristic search algorithms to solve the problem. Finally, numerical experiments are reported, showing the practical efficiency of the proposed algorithms.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5245",
        "title": "A new parameter Learning Method for Bayesian Networks with Qualitative Influences",
        "authors": [
            "Ad Feelders"
        ],
        "abstract": "We propose a new method for parameter learning in Bayesian networks with qualitative influences. This method extends our previous work from networks of binary variables to networks of discrete variables with ordered values. The specified qualitative influences correspond to certain order restrictions on the parameters in the network. These parameters may therefore be estimated using constrained maximum likelihood estimation. We propose an alternative method, based on the isotonic regression. The constrained maximum likelihood estimates are fairly complicated to compute, whereas computation of the isotonic regression estimates only requires the repeated application of the Pool Adjacent Violators algorithm for linear orders. Therefore, the isotonic regression estimator is to be preferred from the viewpoint of computational complexity. Through experiments on simulated and real data, we show that the new learning method is competitive in performance to the constrained maximum likelihood estimator, and that both estimators improve on the standard estimator.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5249",
        "title": "Learning Probabilistic Relational Dynamics for Multiple Tasks",
        "authors": [
            "Ashwin Deshpande",
            "Brian Milch",
            "Luke S. Zettlemoyer",
            "Leslie Pack Kaelbling"
        ],
        "abstract": "The ways in which an agent's actions affect the world can often be modeled compactly using a set of relational probabilistic planning rules. This paper addresses the problem of learning such rule sets for multiple related tasks. We take a hierarchical Bayesian approach, in which the system learns a prior distribution over rule sets. We present a class of prior distributions parameterized by a rule set prototype that is stochastically modified to produce a task-specific rule set. We also describe a coordinate ascent algorithm that iteratively optimizes the task-specific rule sets and the prior distribution. Experiments using this algorithm show that transferring information from related tasks significantly reduces the amount of training data required to predict action effects in blocks-world domains.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5250",
        "title": "Probabilistic Models for Anomaly Detection in Remote Sensor Data Streams",
        "authors": [
            "Ethan W. Dereszynski",
            "Thomas G. Dietterich"
        ],
        "abstract": "Remote sensors are becoming the standard for observing and recording ecological data in the field. Such sensors can record data at fine temporal resolutions, and they can operate under extreme conditions prohibitive to human access. Unfortunately, sensor data streams exhibit many kinds of errors ranging from corrupt communications to partial or total sensor failures. This means that the raw data stream must be cleaned before it can be used by domain scientists. In our application environment|the H.J. Andrews Experimental Forest|this data cleaning is performed manually. This paper introduces a Dynamic Bayesian Network model for analyzing sensor observations and distinguishing sensor failures from valid data for the case of air temperature measured at 15 minute time resolution. The model combines an accurate distribution of long-term and short-term temperature variations with a single generalized fault model. Experiments with historical data show that the precision and recall of the method is comparable to that of the domain expert. The system is currently being deployed to perform real-time automated data cleaning.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5251",
        "title": "Node Splitting: A Scheme for Generating Upper Bounds in Bayesian Networks",
        "authors": [
            "Arthur Choi",
            "Mark Chavira",
            "Adnan Darwiche"
        ],
        "abstract": "We formulate in this paper the mini-bucket algorithm for approximate inference in terms of exact inference on an approximate model produced by splitting nodes in a Bayesian network. The new formulation leads to a number of theoretical and practical implications. First, we show that branchand- bound search algorithms that use minibucket bounds may operate in a drastically reduced search space. Second, we show that the proposed formulation inspires new minibucket heuristics and allows us to analyze existing heuristics from a new perspective. Finally, we show that this new formulation allows mini-bucket approximations to benefit from recent advances in exact inference, allowing one to significantly increase the reach of these approximations.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5255",
        "title": "Minimax regret based elicitation of generalized additive utilities",
        "authors": [
            "Darius Braziunas",
            "Craig Boutilier"
        ],
        "abstract": "We describe the semantic foundations for elicitation of generalized additively independent (GAI) utilities using the minimax regret criterion, and propose several new query types and strategies for this purpose. Computational feasibility is obtained by exploiting the local GAI structure in the model. Our results provide a practical approach for implementing preference-based constrained configuration optimization as well as effective search in multiattribute product databases.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5257",
        "title": "Evaluating influence diagrams with decision circuits",
        "authors": [
            "Debarun Bhattacharjya",
            "Ross D. Shachter"
        ],
        "abstract": "Although a number of related algorithms have been developed to evaluate influence diagrams, exploiting the conditional independence in the diagram, the exact solution has remained intractable for many important problems. In this paper we introduce decision circuits as a means to exploit the local structure usually found in decision problems and to improve the performance of influence diagram analysis. This work builds on the probabilistic inference algorithms using arithmetic circuits to represent Bayesian belief networks [Darwiche, 2003]. Once compiled, these arithmetic circuits efficiently evaluate probabilistic queries on the belief network, and methods have been developed to exploit both the global and local structure of the network. We show that decision circuits can be constructed in a similar fashion and promise similar benefits.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5258",
        "title": "Optimizing Memory-Bounded Controllers for Decentralized POMDPs",
        "authors": [
            "Christopher Amato",
            "Daniel S Bernstein",
            "Shlomo Zilberstein"
        ],
        "abstract": "We present a memory-bounded optimization approach for solving infinite-horizon decentralized POMDPs. Policies for each agent are represented by stochastic finite state controllers. We formulate the problem of optimizing these policies as a nonlinear program, leveraging powerful existing nonlinear optimization techniques for solving the problem. While existing solvers only guarantee locally optimal solutions, we show that our formulation produces higher quality controllers than the state-of-the-art approach. We also incorporate a shared source of randomness in the form of a correlation device to further increase solution quality with only a limited increase in space and time. Our experimental results show that nonlinear optimization can be used to provide high quality, concise solutions to decentralized decision problems under uncertainty.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5260",
        "title": "Reasoning at the Right Time Granularity",
        "authors": [
            "Suchi Saria",
            "Uri Nodelman",
            "Daphne Koller"
        ],
        "abstract": "Most real-world dynamic systems are composed of different components that often evolve at very different rates. In traditional temporal graphical models, such as dynamic Bayesian networks, time is modeled at a fixed granularity, generally selected based on the rate at which the fastest component evolves. Inference must then be performed at this fastest granularity, potentially at significant computational cost. Continuous Time Bayesian Networks (CTBNs) avoid time-slicing in the representation by modeling the system as evolving continuously over time. The expectation-propagation (EP) inference algorithm of Nodelman et al. (2005) can then vary the inference granularity over time, but the granularity is uniform across all parts of the system, and must be selected in advance. In this paper, we provide a new EP algorithm that utilizes a general cluster graph architecture where clusters contain distributions that can overlap in both space (set of variables) and time. This architecture allows different parts of the system to be modeled at very different time granularities, according to their current rate of evolution. We also provide an information-theoretic criterion for dynamically re-partitioning the clusters during inference to tune the level of approximation to the current rate of evolution. This avoids the need to hand-select the appropriate granularity, and allows the granularity to adapt as information is transmitted across the network. We present experiments demonstrating that this approach can result in significant computational savings.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5263",
        "title": "Reading Dependencies from Polytree-Like Bayesian Networks",
        "authors": [
            "Jose M. Pena"
        ],
        "abstract": "We present a graphical criterion for reading dependencies from the minimal directed independence map G of a graphoid p when G is a polytree and p satisfies composition and weak transitivity. We prove that the criterion is sound and complete. We argue that assuming composition and weak transitivity is not too restrictive.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5266",
        "title": "AND/OR Multi-Valued Decision Diagrams (AOMDDs) for Weighted Graphical Models",
        "authors": [
            "Robert Mateescu",
            "Rina Dechter"
        ],
        "abstract": "Compiling graphical models has recently been under intense investigation, especially for probabilistic modeling and processing. We present here a novel data structure for compiling weighted graphical models (in particular, probabilistic models), called AND/OR Multi-Valued Decision Diagram (AOMDD). This is a generalization of our previous work on constraint networks, to weighted models. The AOMDD is based on the frameworks of AND/OR search spaces for graphical models, and Ordered Binary Decision Diagrams (OBDD). The AOMDD is a canonical representation of a graphical model, and its size and compilation time are bounded exponentially by the treewidth of the graph, rather than pathwidth as is known for OBDDs. We discuss a Variable Elimination schedule for compilation, and present the general APPLY algorithm that combines two weighted AOMDDs, and also present a search based method for compilation method. The preliminary experimental evaluation is quite encouraging, showing the potential of the AOMDD data structure.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5268",
        "title": "Best-First AND/OR Search for Most Probable Explanations",
        "authors": [
            "Radu Marinescu",
            "Rina Dechter"
        ],
        "abstract": "The paper evaluates the power of best-first search over AND/OR search spaces for solving the Most Probable Explanation (MPE) task in Bayesian networks. The main virtue of the AND/OR representation of the search space is its sensitivity to the structure of the problem, which can translate into significant time savings. In recent years depth-first AND/OR Branch-and- Bound algorithms were shown to be very effective when exploring such search spaces, especially when using caching. Since best-first strategies are known to be superior to depth-first when memory is utilized, exploring the best-first control strategy is called for. The main contribution of this paper is in showing that a recent extension of AND/OR search algorithms from depth-first Branch-and-Bound to best-first is indeed very effective for computing the MPE in Bayesian networks. We demonstrate empirically the superiority of the best-first search approach on various probabilistic networks.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5271",
        "title": "Learning Bayesian Network Structure from Correlation-Immune Data",
        "authors": [
            "Eric Lantz",
            "Soumya Ray",
            "David Page"
        ],
        "abstract": "Searching the complete space of possible Bayesian networks is intractable for problems of interesting size, so Bayesian network structure learning algorithms, such as the commonly used Sparse Candidate algorithm, employ heuristics. However, these heuristics also restrict the types of relationships that can be learned exclusively from data. They are unable to learn relationships that exhibit \"correlation-immunity\", such as parity. To learn Bayesian networks in the presence of correlation-immune relationships, we extend the Sparse Candidate algorithm with a technique called \"skewing\". This technique uses the observation that relationships that are correlation-immune under a specific input distribution may not be correlation-immune under another, sufficiently different distribution. We show that by extending Sparse Candidate with this technique we are able to discover relationships between random variables that are approximately correlation-immune, with a significantly lower computational cost than the alternative of considering multiple parents of a node at a time.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5273",
        "title": "Survey Propagation Revisited",
        "authors": [
            "Lukas Kroc",
            "Ashish Sabharwal",
            "Bart Selman"
        ],
        "abstract": "Survey propagation (SP) is an exciting new technique that has been remarkably successful at solving very large hard combinatorial problems, such as determining the satisfiability of Boolean formulas. In a promising attempt at understanding the success of SP, it was recently shown that SP can be viewed as a form of belief propagation, computing marginal probabilities over certain objects called covers of a formula. This explanation was, however, shortly dismissed by experiments suggesting that non-trivial covers simply do not exist for large formulas. In this paper, we show that these experiments were misleading: not only do covers exist for large hard random formulas, SP is surprisingly accurate at computing marginals over these covers despite the existence of many cycles in the formulas. This re-opens a potentially simpler line of reasoning for understanding SP, in contrast to some alternative lines of explanation that have been proposed assuming covers do not exist.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5275",
        "title": "Polynomial Constraints in Causal Bayesian Networks",
        "authors": [
            "Changsung Kang",
            "Jin Tian"
        ],
        "abstract": "We use the implicitization procedure to generate polynomial equality constraints on the set of distributions induced by local interventions on variables governed by a causal Bayesian network with hidden variables. We show how we may reduce the complexity of the implicitization problem and make the problem tractable in certain causal Bayesian networks. We also show some preliminary results on the algebraic structure of polynomial constraints. The results have applications in distinguishing between causal models and in testing causal models with combined observational and experimental data.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5276",
        "title": "Template Based Inference in Symmetric Relational Markov Random Fields",
        "authors": [
            "Ariel Jaimovich",
            "Ofer Meshi",
            "Nir Friedman"
        ],
        "abstract": "Relational Markov Random Fields are a general and flexible framework for reasoning about the joint distribution over attributes of a large number of interacting entities. The main computational difficulty in learning such models is inference. Even when dealing with complete data, where one can summarize a large domain by sufficient statistics, learning requires one to compute the expectation of the sufficient statistics given different parameter choices. The typical solution to this problem is to resort to approximate inference procedures, such as loopy belief propagation. Although these procedures are quite efficient, they still require computation that is on the order of the number of interactions (or features) in the model. When learning a large relational model over a complex domain, even such approximations require unrealistic running time. In this paper we show that for a particular class of relational MRFs, which have inherent symmetry, we can perform the inference needed for learning procedures using a template-level belief propagation. This procedure's running time is proportional to the size of the relational model rather than the size of the domain. Moreover, we show that this computational procedure is equivalent to sychronous loopy belief propagation. This enables a dramatic speedup in inference and learning time. We use this procedure to learn relational MRFs for capturing the joint distribution of large protein-protein interaction networks.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5277",
        "title": "Accuracy Bounds for Belief Propagation",
        "authors": [
            "Alexander T. Ihler"
        ],
        "abstract": "The belief propagation (BP) algorithm is widely applied to perform approximate inference on arbitrary graphical models, in part due to its excellent empirical properties and performance. However, little is known theoretically about when this algorithm will perform well. Using recent analysis of convergence and stability properties in BP and new results on approximations in binary systems, we derive a bound on the error in BP's estimates for pairwise Markov random fields over discrete valued random variables. Our bound is relatively simple to compute, and compares favorably with a previous method of bounding the accuracy of BP.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5280",
        "title": "Ranking Under Uncertainty",
        "authors": [
            "Or Zuk",
            "Liat Ein-Dor",
            "Eytan Domany"
        ],
        "abstract": "Ranking objects is a simple and natural procedure for organizing data. It is often performed by assigning a quality score to each object according to its relevance to the problem at hand. Ranking is widely used for object selection, when resources are limited and it is necessary to select a subset of most relevant objects for further processing. In real world situations, the object's scores are often calculated from noisy measurements, casting doubt on the ranking reliability. We introduce an analytical method for assessing the influence of noise levels on the ranking reliability. We use two similarity measures for reliability evaluation, Top-K-List overlap and Kendall's tau measure, and show that the former is much more sensitive to noise than the latter. We apply our method to gene selection in a series of microarray experiments of several cancer types. The results indicate that the reliability of the lists obtained from these experiments is very poor, and that experiment sizes which are necessary for attaining reasonably stable Top-K-Lists are much larger than those currently available. Simulations support our analytical results.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5284",
        "title": "More-or-Less CP-Networks",
        "authors": [
            "Fusun Yaman",
            "Marie desJardins"
        ],
        "abstract": "Preferences play an important role in our everyday lives. CP-networks, or CP-nets in short, are graphical models for representing conditional qualitative preferences under ceteris paribus (\"all else being equal\") assumptions. Despite their intuitive nature and rich representation, dominance testing with CP-nets is computationally complex, even when the CP-nets are restricted to binary-valued preferences. Tractable algorithms exist for binary CP-nets, but these algorithms are incomplete for multi-valued CPnets. In this paper, we identify a class of multivalued CP-nets, which we call more-or-less CPnets, that have the same computational complexity as binary CP-nets. More-or-less CP-nets exploit the monotonicity of the attribute values and use intervals to aggregate values that induce similar preferences. We then present a search control rule for dominance testing that effectively prunes the search space while preserving completeness.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5286",
        "title": "MAP Estimation, Linear Programming and Belief Propagation with Convex Free Energies",
        "authors": [
            "Yair Weiss",
            "Chen Yanover",
            "Talya Meltzer"
        ],
        "abstract": "Finding the most probable assignment (MAP) in a general graphical model is known to be NP hard but good approximations have been attained with max-product belief propagation (BP) and its variants. In particular, it is known that using BP on a single-cycle graph or tree reweighted BP on an arbitrary graph will give the MAP solution if the beliefs have no ties. In this paper we extend the setting under which BP can be used to provably extract the MAP. We define Convex BP as BP algorithms based on a convex free energy approximation and show that this class includes ordinary BP with single-cycle, tree reweighted BP and many other BP variants. We show that when there are no ties, fixed-points of convex max-product BP will provably give the MAP solution. We also show that convex sum-product BP at sufficiently small temperatures can be used to solve linear programs that arise from relaxing the MAP problem. Finally, we derive a novel condition that allows us to derive the MAP solution even if some of the convex BP beliefs have ties. In experiments, we show that our theorems allow us to find the MAP in many real-world instances of graphical models where exact inference using junction-tree is impossible.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5287",
        "title": "Policy Iteration for Relational MDPs",
        "authors": [
            "Chenggang Wang",
            "Roni Khardon"
        ],
        "abstract": "Relational Markov Decision Processes are a useful abstraction for complex reinforcement learning problems and stochastic planning problems. Recent work developed representation schemes and algorithms for planning in such problems using the value iteration algorithm. However, exact versions of more complex algorithms, including policy iteration, have not been developed or analyzed. The paper investigates this potential and makes several contributions. First we observe two anomalies for relational representations showing that the value of some policies is not well defined or cannot be calculated for restricted representation schemes used in the literature. On the other hand, we develop a variant of policy iteration that can get around these anomalies. The algorithm includes an aspect of policy improvement in the process of policy evaluation and thus differs from the original algorithm. We show that despite this difference the algorithm converges to the optimal policy.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5292",
        "title": "Markov Logic in Infinite Domains",
        "authors": [
            "Parag Singla",
            "Pedro Domingos"
        ],
        "abstract": "Combining first-order logic and probability has long been a goal of AI. Markov logic (Richardson & Domingos, 2006) accomplishes this by attaching weights to first-order formulas and viewing them as templates for features of Markov networks. Unfortunately, it does not have the full power of first-order logic, because it is only defined for finite domains. This paper extends Markov logic to infinite domains, by casting it in the framework of Gibbs measures (Georgii, 1988). We show that a Markov logic network (MLN) admits a Gibbs measure as long as each ground atom has a finite number of neighbors. Many interesting cases fall in this category. We also show that an MLN admits a unique measure if the weights of its non-unit clauses are small enough. We then examine the structure of the set of consistent measures in the non-unique case. Many important phenomena, including systems with phase transitions, are represented by MLNs with non-unique measures. We relate the problem of satisfiability in first-order logic to the properties of MLN measures, and discuss how Markov logic relates to previous infinite models.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5294",
        "title": "What Counterfactuals Can Be Tested",
        "authors": [
            "Ilya Shpitser",
            "Judea Pearl"
        ],
        "abstract": "Counterfactual statements, e.g., \"my headache would be gone had I taken an aspirin\" are central to scientific discourse, and are formally interpreted as statements derived from \"alternative worlds\". However, since they invoke hypothetical states of affairs, often incompatible with what is actually known or observed, testing counterfactuals is fraught with conceptual and practical difficulties. In this paper, we provide a complete characterization of \"testable counterfactuals,\" namely, counterfactual statements whose probabilities can be inferred from physical experiments. We provide complete procedures for discerning whether a given counterfactual is testable and, if so, expressing its probability in terms of experimental data.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5295",
        "title": "Improved Memory-Bounded Dynamic Programming for Decentralized POMDPs",
        "authors": [
            "Sven Seuken",
            "Shlomo Zilberstein"
        ],
        "abstract": "Memory-Bounded Dynamic Programming (MBDP) has proved extremely effective in solving decentralized POMDPs with large horizons. We generalize the algorithm and improve its scalability by reducing the complexity with respect to the number of observations from exponential to polynomial. We derive error bounds on solution quality with respect to this new approximation and analyze the convergence behavior. To evaluate the effectiveness of the improvements, we introduce a new, larger benchmark problem. Experimental results show that despite the high complexity of decentralized POMDPs, scalable solution techniques such as MBDP perform surprisingly well.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5360",
        "title": "Analysis of a Nature Inspired Firefly Algorithm based Back-propagation Neural Network Training",
        "authors": [
            "Sudarshan Nandy",
            "Partha Pratim Sarkar",
            "Achintya Das"
        ],
        "abstract": "Optimization algorithms are normally influenced by meta-heuristic approach. In recent years several hybrid methods for optimization are developed to find out a better solution. The proposed work using meta-heuristic Nature Inspired algorithm is applied with back-propagation method to train a feed-forward neural network. Firefly algorithm is a nature inspired meta-heuristic algorithm, and it is incorporated into back-propagation algorithm to achieve fast and improved convergence rate in training feed-forward neural network. The proposed technique is tested over some standard data set. It is found that proposed method produces an improved convergence within very few iteration. This performance is also analyzed and compared to genetic algorithm based back-propagation. It is observed that proposed method consumes less time to converge and providing improved convergence rate with minimum feed-forward neural network design.\n    ",
        "submission_date": "2012-06-23T00:00:00",
        "last_modified_date": "2012-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5396",
        "title": "Markov Chains on Orbits of Permutation Groups",
        "authors": [
            "Mathias Niepert"
        ],
        "abstract": "We present a novel approach to detecting and utilizing symmetries in probabilistic graphical models with two main contributions. First, we present a scalable approach to computing generating sets of permutation groups representing the symmetries of graphical models. Second, we introduce orbital Markov chains, a novel family of Markov chains leveraging model symmetries to reduce mixing times. We establish an insightful connection between model symmetries and rapid mixing of orbital Markov chains. Thus, we present the first lifted MCMC algorithm for probabilistic graphical models. Both analytical and empirical results demonstrate the effectiveness and efficiency of the approach.\n    ",
        "submission_date": "2012-06-23T00:00:00",
        "last_modified_date": "2012-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5698",
        "title": "Relational Approach to Knowledge Engineering for POMDP-based Assistance Systems as a Translation of a Psychological Model",
        "authors": [
            "Marek Grzes",
            "Jesse Hoey",
            "Shehroz Khan",
            "Alex Mihailidis",
            "Stephen Czarnuch",
            "Dan Jackson",
            "Andrew Monk"
        ],
        "abstract": "Assistive systems for persons with cognitive disabilities (e.g. dementia) are difficult to build due to the wide range of different approaches people can take to accomplishing the same task, and the significant uncertainties that arise from both the unpredictability of client's behaviours and from noise in sensor readings. Partially observable Markov decision process (POMDP) models have been used successfully as the reasoning engine behind such assistive systems for small multi-step tasks such as hand washing. POMDP models are a powerful, yet flexible framework for modelling assistance that can deal with uncertainty and utility. Unfortunately, POMDPs usually require a very labour intensive, manual procedure for their definition and construction. Our previous work has described a knowledge driven method for automatically generating POMDP activity recognition and context sensitive prompting systems for complex tasks. We call the resulting POMDP a SNAP (SyNdetic Assistance Process). The spreadsheet-like result of the analysis does not correspond to the POMDP model directly and the translation to a formal POMDP representation is required. To date, this translation had to be performed manually by a trained POMDP expert. In this paper, we formalise and automate this translation process using a probabilistic relational model (PRM) encoded in a relational database. We demonstrate the method by eliciting three assistance tasks from non-experts. We validate the resulting POMDP models using case-based simulations to show that they are reasonable for the domains. We also show a complete case study of a designer specifying one database, including an evaluation in a real-life experiment with a human actor.\n    ",
        "submission_date": "2012-06-25T00:00:00",
        "last_modified_date": "2012-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5833",
        "title": "Revision of Defeasible Logic Preferences",
        "authors": [
            "Guido Governatori",
            "Francesco Olivieri",
            "Simone Scannapieco",
            "Matteo Cristani"
        ],
        "abstract": "There are several contexts of non-monotonic reasoning where a priority between rules is established whose purpose is preventing conflicts.\n",
        "submission_date": "2012-06-25T00:00:00",
        "last_modified_date": "2012-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5928",
        "title": "CAPIR: Collaborative Action Planning with Intention Recognition",
        "authors": [
            "Truong-Huy Dinh Nguyen",
            "David Hsu",
            "Wee-Sun Lee",
            "Tze-Yun Leong",
            "Leslie Pack Kaelbling",
            "Tomas Lozano-Perez",
            "Andrew Haydn Grant"
        ],
        "abstract": "We apply decision theoretic techniques to construct non-player characters that are able to assist a human player in collaborative games. The method is based on solving Markov decision processes, which can be difficult when the game state is described by many variables. To scale to more complex games, the method allows decomposition of a game task into subtasks, each of which can be modelled by a Markov decision process. Intention recognition is used to infer the subtask that the human is currently performing, allowing the helper to assist the human in performing the correct task. Experiments show that the method can be effective, giving near-human level performance in helping a human in a collaborative game.\n    ",
        "submission_date": "2012-06-26T00:00:00",
        "last_modified_date": "2012-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5940",
        "title": "Bootstrapping Monte Carlo Tree Search with an Imperfect Heuristic",
        "authors": [
            "Truong-Huy Dinh Nguyen",
            "Wee-Sun Lee",
            "Tze-Yun Leong"
        ],
        "abstract": "We consider the problem of using a heuristic policy to improve the value approximation by the Upper Confidence Bound applied in Trees (UCT) algorithm in non-adversarial settings such as planning with large-state space Markov Decision Processes. Current improvements to UCT focus on either changing the action selection formula at the internal nodes or the rollout policy at the leaf nodes of the search tree. In this work, we propose to add an auxiliary arm to each of the internal nodes, and always use the heuristic policy to roll out simulations at the auxiliary arms. The method aims to get fast convergence to optimal values at states where the heuristic policy is optimal, while retaining similar approximation as the original UCT in other states. We show that bootstrapping with the proposed method in the new algorithm, UCT-Aux, performs better compared to the original UCT algorithm and its variants in two benchmark experiment settings. We also examine conditions under which UCT-Aux works well.\n    ",
        "submission_date": "2012-06-26T00:00:00",
        "last_modified_date": "2012-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6080",
        "title": "Predicting the behavior of interacting humans by fusing data from multiple sources",
        "authors": [
            "Erik J. Schlicht",
            "Ritchie Lee",
            "David H. Wolpert",
            "Mykel J. Kochenderfer",
            "Brendan Tracey"
        ],
        "abstract": "Multi-fidelity methods combine inexpensive low-fidelity simulations with costly but high-fidelity simulations to produce an accurate model of a system of interest at minimal cost. They have proven useful in modeling physical systems and have been applied to engineering problems such as wing-design optimization. During human-in-the-loop experimentation, it has become increasingly common to use online platforms, like Mechanical Turk, to run low-fidelity experiments to gather human performance data in an efficient manner. One concern with these experiments is that the results obtained from the online environment generalize poorly to the actual domain of interest. To address this limitation, we extend traditional multi-fidelity approaches to allow us to combine fewer data points from high-fidelity human-in-the-loop experiments with plentiful but less accurate data from low-fidelity experiments to produce accurate models of how humans interact. We present both model-based and model-free methods, and summarize the predictive performance of each method under different conditions.\n    ",
        "submission_date": "2012-06-26T00:00:00",
        "last_modified_date": "2012-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6262",
        "title": "Scaling Life-long Off-policy Learning",
        "authors": [
            "Adam White",
            "Joseph Modayil",
            "Richard S. Sutton"
        ],
        "abstract": "We pursue a life-long learning approach to artificial intelligence that makes extensive use of reinforcement learning algorithms. We build on our prior work with general value functions (GVFs) and the Horde architecture. GVFs have been shown able to represent a wide variety of facts about the world's dynamics that may be useful to a long-lived agent (Sutton et al. 2011). We have also previously shown scaling - that thousands of on-policy GVFs can be learned accurately in real-time on a mobile robot (Modayil, White & Sutton 2011). That work was limited in that it learned about only one policy at a time, whereas the greatest potential benefits of life-long learning come from learning about many policies in parallel, as we explore in this paper. Many new challenges arise in this off-policy learning setting. To deal with convergence and efficiency challenges, we utilize the recently introduced GTD({\\lambda}) algorithm. We show that GTD({\\lambda}) with tile coding can simultaneously learn hundreds of predictions for five simple target policies while following a single random behavior policy, assessing accuracy with interspersed on-policy tests. To escape the need for the tests, which preclude further scaling, we introduce and empirically vali- date two online estimators of the off-policy objective (MSPBE). Finally, we use the more efficient of the two estimators to demonstrate off-policy learning at scale - the learning of value functions for one thousand policies in real time on a physical robot. This ability constitutes a significant step towards scaling life-long off-policy learning.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6347",
        "title": "The observational roots of reference of the semantic web",
        "authors": [
            "Simon Scheider",
            "Krzysztof Janowicz",
            "Benjamin Adams"
        ],
        "abstract": "Shared reference is an essential aspect of meaning. It is also indispensable for the semantic web, since it enables to weave the global graph, i.e., it allows different users to contribute to an identical referent. For example, an essential kind of referent is a geographic place, to which users may contribute observations. We argue for a human-centric, operational approach towards reference, based on respective human competences. These competences encompass perceptual, cognitive as well as technical ones, and together they allow humans to inter-subjectively refer to a phenomenon in their environment. The technology stack of the semantic web should be extended by such operations. This would allow establishing new kinds of observation-based reference systems that help constrain and integrate the semantic web bottom-up.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6390",
        "title": "Incorporating Causal Prior Knowledge as Path-Constraints in Bayesian Networks and Maximal Ancestral Graphs",
        "authors": [
            "Giorgos Borboudakis",
            "Ioannis Tsamardinos"
        ],
        "abstract": "We consider the incorporation of causal knowledge about the presence or absence of (possibly indirect) causal relations into a causal model. Such causal relations correspond to directed paths in a causal model. This type of knowledge naturally arises from experimental data, among others. Specifically, we consider the formalisms of Causal Bayesian Networks and Maximal Ancestral Graphs and their Markov equivalence classes: Partially Directed Acyclic Graphs and Partially Oriented Ancestral Graphs. We introduce sound and complete procedures which are able to incorporate causal prior knowledge in such models. In simulated experiments, we show that often considering even a few causal facts leads to a significant number of new inferences. In a case study, we also show how to use real experimental data to infer causal knowledge and incorporate it into a real biological causal network. The code is available at ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6424",
        "title": "Anytime Marginal MAP Inference",
        "authors": [
            "Denis Maua",
            "Cassio De Campos"
        ],
        "abstract": "This paper presents a new anytime algorithm for the marginal MAP problem in graphical models. The algorithm is described in detail, its complexity and convergence rate are studied, and relations to previous theoretical results for the problem are discussed. It is shown that the algorithm runs in polynomial-time if the underlying graph of the model has bounded tree-width, and that it provides guarantees to the lower and upper bounds obtained within a fixed amount of computational resources. Experiments with both real and synthetic generated models highlight its main characteristics and show that it compares favorably against Park and Darwiche's systematic search, particularly in the case of problems with many MAP variables and moderate tree-width.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6473",
        "title": "Compositional Planning Using Optimal Option Models",
        "authors": [
            "David Silver",
            "Kamil Ciosek"
        ],
        "abstract": "In this paper we introduce a framework for option model composition. Option models are temporal abstractions that, like macro-operators in classical planning, jump directly from a start state to an end state. Prior work has focused on constructing option models from primitive actions, by intra-option model learning; or on using option models to construct a value function, by inter-option planning. We present a unified view of intra- and inter-option model learning, based on a major generalisation of the Bellman equation. Our fundamental operation is the recursive composition of option models into other option models. This key idea enables compositional planning over many levels of abstraction. We illustrate our framework using a dynamic programming algorithm that simultaneously constructs optimal option models for multiple subgoals, and also searches over those option models to provide rapid progress towards other subgoals.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6814",
        "title": "An Empirical Comparison of Algorithms for Aggregating Expert Predictions",
        "authors": [
            "Varsha Dani",
            "Omid Madani",
            "David M Pennock",
            "Sumit Sanghai",
            "Brian Galebach"
        ],
        "abstract": "Predicting the outcomes of future events is a challenging problem for which a variety of solution methods have been explored and attempted. We present an empirical comparison of a variety of online and offline adaptive algorithms for aggregating experts' predictions of the outcomes of five years of US National Football League games (1319 games) using expert probability elicitations obtained from an Internet contest called ProbabilitySports. We find that it is difficult to improve over simple averaging of the predictions in terms of prediction accuracy, but that there is room for improvement in quadratic loss. Somewhat surprisingly, a Bayesian estimation algorithm which estimates the variance of each expert's prediction exhibits the most consistent superior performance over simple averaging among our collection of algorithms.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6816",
        "title": "MAIES: A Tool for DNA Mixture Analysis",
        "authors": [
            "Robert G. Cowell",
            "Steffen L. Lauritzen",
            "Julia Mortera"
        ],
        "abstract": "We describe an expert system, MAIES, developed for analysing forensic identification problems involving DNA mixture traces using quantitative peak area information. Peak area information is represented by conditional Gaussian distributions, and inference based on exact junction tree propagation ascertains whether individuals, whose profiles have been measured, have contributed to the mixture. The system can also be used to predict DNA profiles of unknown contributors by separating the mixture into its individual components. The use of the system is illustrated with an application to a real world example. The system implements a novel MAP (maximum a posteriori) search algorithm that is described in an appendix.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6817",
        "title": "A Variational Approach for Approximating Bayesian Networks by Edge Deletion",
        "authors": [
            "Arthur Choi",
            "Adnan Darwiche"
        ],
        "abstract": "We consider in this paper the formulation of approximate inference in Bayesian networks as a problem of exact inference on an approximate network that results from deleting edges (to reduce treewidth). We have shown in earlier work that deleting edges calls for introducing auxiliary network parameters to compensate for lost dependencies, and proposed intuitive conditions for determining these parameters. We have also shown that our method corresponds to IBP when enough edges are deleted to yield a polytree, and corresponds to some generalizations of IBP when fewer edges are deleted. In this paper, we propose a different criteria for determining auxiliary parameters based on optimizing the KL-divergence between the original and approximate networks. We discuss the relationship between the two methods for selecting parameters, shedding new light on IBP and its generalizations. We also discuss the application of our new method to approximating inference problems which are exponential in constrained treewidth, including MAP and nonmyopic value of information.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6818",
        "title": "Sensitivity Analysis for Threshold Decision Making with Dynamic Networks",
        "authors": [
            "Theodore Charitos",
            "Linda C. van der Gaag"
        ],
        "abstract": "The effect of inaccuracies in the parameters of a dynamic Bayesian network can be investigated by subjecting the network to a sensitivity analysis. Having detailed the resulting sensitivity functions in our previous work, we now study the effect of parameter inaccuracies on a recommended decision in view of a threshold decision-making model. We detail the effect of varying a single and multiple parameters from a conditional probability table and present a computational procedure for establishing bounds between which assessments for these parameters can be varied without inducing a change in the recommended decision. We illustrate the various concepts involved by means of a real-life dynamic network in the field of infectious disease.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6819",
        "title": "On the Robustness of Most Probable Explanations",
        "authors": [
            "Hei Chan",
            "Adnan Darwiche"
        ],
        "abstract": "In Bayesian networks, a Most Probable Explanation (MPE) is a complete variable instantiation with a highest probability given the current evidence. In this paper, we discuss the problem of finding robustness conditions of the MPE under single parameter changes. Specifically, we ask the question: How much change in a single network parameter can we afford to apply while keeping the MPE unchanged? We will describe a procedure, which is the first of its kind, that computes this answer for each parameter in the Bayesian network variable in time O(n exp(w)), where n is the number of network variables and w is its treewidth.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6821",
        "title": "Graphical Condition for Identification in recursive SEM",
        "authors": [
            "Carlos Brito",
            "Judea Pearl"
        ],
        "abstract": "The paper concerns the problem of predicting the effect of actions or interventions on a system from a combination of (i) statistical data on a set of observed variables, and (ii) qualitative causal knowledge encoded in the form of a directed acyclic graph (DAG). The DAG represents a set of linear equations called Structural Equations Model (SEM), whose coefficients are parameters representing direct causal effects. Reliable quantitative conclusions can only be obtained from the model if the causal effects are uniquely determined by the data. That is, if there exists a unique parametrization for the model that makes it compatible with the data. If this is the case, the model is called identified. The main result of the paper is a general sufficient condition for identification of recursive SEM models.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6822",
        "title": "Cutset Sampling with Likelihood Weighting",
        "authors": [
            "Bozhena Bidyuk",
            "Rina Dechter"
        ],
        "abstract": "The paper analyzes theoretically and empirically the performance of likelihood weighting (LW) on a subset of nodes in Bayesian networks. The proposed scheme requires fewer samples to converge due to reduction in sampling variance. The method exploits the structure of the network to bound the complexity of exact inference used to compute sampling distributions, similar to Gibbs cutset sampling. Yet, the extension of the previosly proposed cutset sampling principles to likelihood weighting is non-trivial due to differences in the sampling processes of Gibbs sampler and LW. We demonstrate empirically that likelihood weighting on a cutset (LWLC) is effective time-wise and has a lower rejection rate than LW when applied to networks with many deterministic probabilities. Finally, we show that the performance of likelihood weighting on a cutset can be improved further by caching computed sampling distributions and, consequently, learning 'zeros' of the target distribution.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6823",
        "title": "An Efficient Triplet-based Algorithm for Evidential Reasoning",
        "authors": [
            "Yaxin Bi",
            "Jiwen W. Guan"
        ],
        "abstract": "Linear-time computational techniques have been developed for combining evidence which is available on a number of contending hypotheses. They offer a means of making the computation-intensive calculations involved more efficient in certain circumstances. Unfortunately, they restrict the orthogonal sum of evidential functions to the dichotomous structure applies only to elements and their complements. In this paper, we present a novel evidence structure in terms of a triplet and a set of algorithms for evidential reasoning. The merit of this structure is that it divides a set of evidence into three subsets, distinguishing trivial evidential elements from important ones focusing some particular elements. It avoids the deficits of the dichotomous structure in representing the preference of evidence and estimating the basic probability assignment of evidence. We have established a formalism for this structure and the general formulae for combining pieces of evidence in the form of the triplet, which have been theoretically justified.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6825",
        "title": "Non-Minimal Triangulations for Mixed Stochastic/Deterministic Graphical Models",
        "authors": [
            "Chris Bartels",
            "Jeff A. Bilmes"
        ],
        "abstract": "We observe that certain large-clique graph triangulations can be useful to reduce both computational and space requirements when making queries on mixed stochastic/deterministic graphical models. We demonstrate that many of these large-clique triangulations are non-minimal and are thus unattainable via the variable elimination algorithm. We introduce ancestral pairs as the basis for novel triangulation heuristics and prove that no more than the addition of edges between ancestral pairs need be considered when searching for state space optimal triangulations in such graphs. Empirical results on random and real world graphs show that the resulting triangulations that yield significant speedups are almost always non-minimal. We also give an algorithm and correctness proof for determining if a triangulation can be obtained via elimination, and we show that the decision problem associated with finding optimal state space triangulations in this mixed stochastic/deterministic setting is NP-complete.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6827",
        "title": "Linear Algebra Approach to Separable Bayesian Networks",
        "authors": [
            "Chalee Asavathiratham"
        ],
        "abstract": "Separable Bayesian Networks, or the Influence Model, are dynamic Bayesian Networks in which the conditional probability distribution can be separated into a function of only the marginal distribution of a node's neighbors, instead of the joint distributions. In terms of modeling, separable networks has rendered possible siginificant reduction in complexity, as the state space is only linear in the number of variables on the network, in contrast to a typical state space which is exponential. In this work, We describe the connection between an arbitrary Conditional Probability Table (CPT) and separable systems using linear algebra. We give an alternate proof on the equivalence of sufficiency and separability. We present a computational method for testing whether a given CPT is separable.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6829",
        "title": "Inequality Constraints in Causal Models with Hidden Variables",
        "authors": [
            "Changsung Kang",
            "Jin Tian"
        ],
        "abstract": "We present a class of inequality constraints on the set of distributions induced by local interventions on variables governed by a causal Bayesian network, in which some of the variables remain unmeasured. We derive bounds on causal effects that are not directly measured in randomized experiments. We derive instrumental inequality type of constraints on nonexperimental distributions. The results have applications in testing causal models with observational or experimental data.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6831",
        "title": "Pearl's Calculus of Intervention Is Complete",
        "authors": [
            "Yimin Huang",
            "Marco Valtorta"
        ],
        "abstract": "This paper is concerned with graphical criteria that can be used to solve the problem of identifying casual effects from nonexperimental data in a causal Bayesian network structure, i.e., a directed acyclic graph that represents causal relationships. We first review Pearl's work on this topic [Pearl, 1995], in which several useful graphical criteria are presented. Then we present a complete algorithm [Huang and Valtorta, 2006b] for the identifiability problem. By exploiting the completeness of this algorithm, we prove that the three basic do-calculus rules that Pearl presents are complete, in the sense that, if a causal effect is identifiable, there exists a sequence of applications of the rules of the do-calculus that transforms the causal effect formula into a formula that only includes observational quantities.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6834",
        "title": "A new axiomatization for likelihood gambles",
        "authors": [
            "Phan H. Giang"
        ],
        "abstract": "This paper studies a new and more general axiomatization than one presented previously for preference on likelihood gambles. Likelihood gambles describe actions in a situation where a decision maker knows multiple probabilistic models and a random sample generated from one of those models but does not know prior probability of models. This new axiom system is inspired by Jensen's axiomatization of probabilistic gambles. Our approach provides a new perspective to the role of data in decision making under ambiguity. It avoids one of the most controversial issue of Bayesian methodology namely the assumption of prior probability.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6835",
        "title": "Dimension Reduction in Singularly Perturbed Continuous-Time Bayesian Networks",
        "authors": [
            "Nir Friedman",
            "Raz Kupferman"
        ],
        "abstract": "Continuous-time Bayesian networks (CTBNs) are graphical representations of multi-component continuous-time Markov processes as directed graphs. The edges in the network represent direct influences among components. The joint rate matrix of the multi-component process is specified by means of conditional rate matrices for each component separately. This paper addresses the situation where some of the components evolve on a time scale that is much shorter compared to the time scale of the other components. In this paper, we prove that in the limit where the separation of scales is infinite, the Markov process converges (in distribution, or weakly) to a reduced, or effective Markov process that only involves the slow components. We also demonstrate that for reasonable separation of scale (an order of magnitude) the reduced process is a good approximation of the marginal process over the slow components. We provide a simple procedure for building a reduced CTBN for this effective process, with conditional rate matrices that can be directly calculated from the original CTBN, and discuss the implications for approximate reasoning in large systems.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6836",
        "title": "Methods for computing state similarity in Markov Decision Processes",
        "authors": [
            "Norman Ferns",
            "Pablo Samuel Castro",
            "Doina Precup",
            "Prakash Panangaden"
        ],
        "abstract": "A popular approach to solving large probabilistic systems relies on aggregating states based on a measure of similarity. Many approaches in the literature are heuristic. A number of recent methods rely instead on metrics based on the notion of bisimulation, or behavioral equivalence between states (Givan et al, 2001, 2003; Ferns et al, 2004). An integral component of such metrics is the Kantorovich metric between probability distributions. However, while this metric enables many satisfying theoretical properties, it is costly to compute in practice. In this paper, we use techniques from network optimization and statistical sampling to overcome this problem. We obtain in this manner a variety of distance functions for MDP state aggregation, which differ in the tradeoff between time and space complexity, as well as the quality of the aggregation. We provide an empirical evaluation of these trade-offs.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6837",
        "title": "Residual Belief Propagation: Informed Scheduling for Asynchronous Message Passing",
        "authors": [
            "Gal Elidan",
            "Ian McGraw",
            "Daphne Koller"
        ],
        "abstract": "Inference for probabilistic graphical models is still very much a practical challenge in large domains. The commonly used and effective belief propagation (BP) algorithm and its generalizations often do not converge when applied to hard, real-life inference tasks. While it is widely recognized that the scheduling of messages in these algorithms may have significant consequences, this issue remains largely unexplored. In this work, we address the question of how to schedule messages for asynchronous propagation so that a fixed point is reached faster and more often. We first show that any reasonable asynchronous BP converges to a unique fixed point under conditions similar to those that guarantee convergence of synchronous BP. In addition, we show that the convergence rate of a simple round-robin schedule is at least as good as that of synchronous propagation. We then propose residual belief propagation (RBP), a novel, easy-to-implement, asynchronous propagation algorithm that schedules messages in an informed way, that pushes down a bound on the distance from the fixed point. Finally, we demonstrate the superiority of RBP over state-of-the-art methods for a variety of challenging synthetic and real-life problems: RBP converges significantly more often than other methods; and it significantly reduces running time until convergence, even when other methods converge.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6838",
        "title": "Continuous Time Markov Networks",
        "authors": [
            "Tal El-Hay",
            "Nir Friedman",
            "Daphne Koller",
            "Raz Kupferman"
        ],
        "abstract": "A central task in many applications is reasoning about processes that change in a continuous time. The mathematical framework of Continuous Time Markov Processes provides the basic foundations for modeling such systems. Recently, Nodelman et al introduced continuous time Bayesian networks (CTBNs), which allow a compact representation of continuous-time processes over a factored state space. In this paper, we introduce continuous time Markov networks (CTMNs), an alternative representation language that represents a different type of continuous-time dynamics. In many real life processes, such as biological and chemical systems, the dynamics of the process can be naturally described as an interplay between two forces - the tendency of each entity to change its state, and the overall fitness or energy function of the entire system. In our model, the first force is described by a continuous-time proposal process that suggests possible local changes to the state of the system at different rates. The second force is represented by a Markov network that encodes the fitness, or desirability, of different states; a proposed local change is then accepted with a probability that is a function of the change in the fitness distribution. We show that the fitness distribution is also the stationary distribution of the Markov process, so that this representation provides a characterization of a temporal process whose stationary distribution has a compact graphical representation. This allows us to naturally capture a different type of structure in complex dynamical processes, such as evolving biological sequences. We describe the semantics of the representation, its basic properties, and how it compares to CTBNs. We also provide algorithms for learning such models from data, and discuss its applicability to biological sequence evolution.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6841",
        "title": "Asymmetric separation for local independence graphs",
        "authors": [
            "Vanessa Didelez"
        ],
        "abstract": "Directed possibly cyclic graphs have been proposed by Didelez (2000) and Nodelmann et al. (2002) in order to represent the dynamic dependencies among stochastic processes. These dependencies are based on a generalization of Granger-causality to continuous time, first developed by Schweder (1970) for Markov processes, who called them local dependencies. They deserve special attention as they are asymmetric unlike stochastic (in)dependence. In this paper we focus on their graphical representation and develop a suitable, i.e. asymmetric notion of separation, called delta-separation. The properties of this graph separation as well as of local independence are investigated in detail within a framework of asymmetric (semi)graphoids allowing a deeper insight into what information can be read off these graphs.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6843",
        "title": "Adjacency-Faithfulness and Conservative Causal Inference",
        "authors": [
            "Joseph Ramsey",
            "Jiji Zhang",
            "Peter L. Spirtes"
        ],
        "abstract": "Most causal inference algorithms in the literature (e.g., Pearl (2000), Spirtes et al. (2000), Heckerman et al. (1999)) exploit an assumption usually referred to as the causal Faithfulness or Stability condition. In this paper, we highlight two components of the condition used in constraint-based algorithms, which we call \"Adjacency-Faithfulness\" and \"Orientation-Faithfulness\". We point out that assuming Adjacency-Faithfulness is true, it is in principle possible to test the validity of Orientation-Faithfulness. Based on this observation, we explore the consequence of making only the Adjacency-Faithfulness assumption. We show that the familiar PC algorithm has to be modified to be (asymptotically) correct under the weaker, Adjacency-Faithfulness assumption. Roughly the modified algorithm, called Conservative PC (CPC), checks whether Orientation-Faithfulness holds in the orientation phase, and if not, avoids drawing certain causal conclusions the PC algorithm would draw. However, if the stronger, standard causal Faithfulness condition actually obtains, the CPC algorithm is shown to output the same pattern as the PC algorithm does in the large sample limit. We also present a simulation study showing that the CPC algorithm runs almost as fast as the PC algorithm, and outputs significantly fewer false causal arrowheads than the PC algorithm does on realistic sample sizes. We end our paper by discussing how score-based algorithms such as GES perform when the Adjacency-Faithfulness but not the standard causal Faithfulness condition holds, and how to extend our work to the FCI algorithm, which allows for the possibility of latent variables.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6844",
        "title": "From influence diagrams to multi-operator cluster DAGs",
        "authors": [
            "Cedric Pralet",
            "Thomas Schiex",
            "Gerard Verfaillie"
        ],
        "abstract": "There exist several architectures to solve influence diagrams using local computations, such as the Shenoy-Shafer, the HUGIN, or the Lazy Propagation architectures. They all extend usual variable elimination algorithms thanks to the use of so-called 'potentials'. In this paper, we introduce a new architecture, called the Multi-operator Cluster DAG architecture, which can produce decompositions with an improved constrained induced-width, and therefore induce potentially exponential gains. Its principle is to benefit from the composite nature of influence diagrams, instead of using uniform potentials, in order to better analyze the problem structure.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6849",
        "title": "General-Purpose MCMC Inference over Relational Structures",
        "authors": [
            "Brian Milch",
            "Stuart Russell"
        ],
        "abstract": "Tasks such as record linkage and multi-target tracking, which involve reconstructing the set of objects that underlie some observed data, are particularly challenging for probabilistic inference. Recent work has achieved efficient and accurate inference on such problems using Markov chain Monte Carlo (MCMC) techniques with customized proposal distributions. Currently, implementing such a system requires coding MCMC state representations and acceptance probability calculations that are specific to a particular application. An alternative approach, which we pursue in this paper, is to use a general-purpose probabilistic modeling language (such as BLOG) and a generic Metropolis-Hastings MCMC algorithm that supports user-supplied proposal distributions. Our algorithm gains flexibility by using MCMC states that are only partial descriptions of possible worlds; we provide conditions under which MCMC over partial worlds yields correct answers to queries. We also show how to use a context-specific Bayes net to identify the factors in the acceptance probability that need to be computed for a given proposed move. Experimental results on a citation matching task show that our general-purpose MCMC engine compares favorably with an application-specific system.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6853",
        "title": "A theoretical study of Y structures for causal discovery",
        "authors": [
            "Subramani Mani",
            "Peter L. Spirtes",
            "Gregory F. Cooper"
        ],
        "abstract": "There are several existing algorithms that under appropriate assumptions can reliably identify a subset of the underlying causal relationships from observational data. This paper introduces the first computationally feasible score-based algorithm that can reliably identify causal relationships in the large sample limit for discrete models, while allowing for the possibility that there are unobserved common causes. In doing so, the algorithm does not ever need to assign scores to causal structures with unobserved common causes. The algorithm is based on the identification of so called Y substructures within Bayesian network structures that can be learned from observational data. An example of a Y substructure is A -> C, B -> C, C -> D. After providing background on causal discovery, the paper proves the conditions under which the algorithm is reliable in the large sample limit.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6854",
        "title": "Belief Update in CLG Bayesian Networks With Lazy Propagation",
        "authors": [
            "Anders L. Madsen"
        ],
        "abstract": "In recent years Bayesian networks (BNs) with a mixture of continuous and discrete variables have received an increasing level of attention. We present an architecture for exact belief update in Conditional Linear Gaussian BNs (CLG BNs). The architecture is an extension of lazy propagation using operations of Lauritzen & Jensen [6] and Cowell [2]. By decomposing clique and separator potentials into sets of factors, the proposed architecture takes advantage of independence and irrelevance properties induced by the structure of the graph and the evidence. The resulting benefits are illustrated by examples. Results of a preliminary empirical performance evaluation indicate a significant potential of the proposed architecture.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6856",
        "title": "Reasoning about Uncertainty in Metric Spaces",
        "authors": [
            "Seunghwan Lee"
        ],
        "abstract": "We set up a model for reasoning about metric spaces with belief theoretic measures. The uncertainty in these spaces stems from both probability and metric. To represent both aspect of uncertainty, we choose an expected distance function as a measure of uncertainty. A formal logical system is constructed for the reasoning about expected distance. Soundness and completeness are shown for this logic. For reasoning on product metric space with uncertainty, a new metric is defined and shown to have good properties.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6859",
        "title": "Propagation of Delays in the National Airspace System",
        "authors": [
            "Kathryn Blackmond Laskey",
            "Ning Xu",
            "Chun-Hung Chen"
        ],
        "abstract": "The National Airspace System (NAS) is a large and complex system with thousands of interrelated components: administration, control centers, airports, airlines, aircraft, passengers, etc. The complexity of the NAS creates many difficulties in management and control. One of the most pressing problems is flight delay. Delay creates high cost to airlines, complaints from passengers, and difficulties for airport operations. As demand on the system increases, the delay problem becomes more and more prominent. For this reason, it is essential for the Federal Aviation Administration to understand the causes of delay and to find ways to reduce delay. Major contributing factors to delay are congestion at the origin airport, weather, increasing demand, and air traffic management (ATM) decisions such as the Ground Delay Programs (GDP). Delay is an inherently stochastic phenomenon. Even if all known causal factors could be accounted for, macro-level national airspace system (NAS) delays could not be predicted with certainty from micro-level aircraft information. This paper presents a stochastic model that uses Bayesian Networks (BNs) to model the relationships among different components of aircraft delay and the causal factors that affect delays. A case study on delays of departure flights from Chicago O'Hare international airport (ORD) to Hartsfield-Jackson Atlanta International Airport (ATL) reveals how local and system level environmental and human-caused factors combine to affect components of delay, and how these components contribute to the final arrival delay at the destination airport.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6864",
        "title": "Infinite Hidden Relational Models",
        "authors": [
            "Zhao Xu",
            "Volker Tresp",
            "Kai Yu",
            "Hans-Peter Kriegel"
        ],
        "abstract": "In many cases it makes sense to model a relationship symmetrically, not implying any particular directionality. Consider the classical example of a recommendation system where the rating of an item by a user should symmetrically be dependent on the attributes of both the user and the item. The attributes of the (known) relationships are also relevant for predicting attributes of entities and for predicting attributes of new relations. In recommendation systems, the exploitation of relational attributes is often referred to as collaborative filtering. Again, in many applications one might prefer to model the collaborative effect in a symmetrical way. In this paper we present a relational model, which is completely symmetrical. The key innovation is that we introduce for each entity (or object) an infinite-dimensional latent variable as part of a Dirichlet process (DP) model. We discuss inference in the model, which is based on a DP Gibbs sampler, i.e., the Chinese restaurant process. We extend the Chinese restaurant process to be applicable to relational modeling. Our approach is evaluated in three applications. One is a recommendation system based on the MovieLens data set. The second application concerns the prediction of the function of yeast genes/proteins on the data set of KDD Cup 2001 using a multi-relational model. The third application involves a relational medical domain. The experimental results show that our model gives significantly improved estimates of attributes describing relationships or entities in complex relational models.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6867",
        "title": "Axiomatic Foundations for a Class of Generalized Expected Utility: Algebraic Expected Utility",
        "authors": [
            "Paul Weng"
        ],
        "abstract": "Expected Utility: Algebraic Expected Utility In this paper, we provide two axiomatizations of algebraic expected utility, which is a particular generalized expected utility, in a von Neumann-Morgenstern setting, i.e. uncertainty representation is supposed to be given and here to be described by a plausibility measure valued on a semiring, which could be partially ordered. We show that axioms identical to those for expected utility entail that preferences are represented by an algebraic expected utility. This algebraic approach allows many previous propositions (expected utility, binary possibilistic utility,...) to be unified in a same general framework and proves that the obtained utility enjoys the same nice features as expected utility: linearity, dynamic consistency, autoduality of the underlying uncertainty measure, autoduality of the decision criterion and possibility of modeling decision maker's attitude toward uncertainty.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6869",
        "title": "Recognizing Activities and Spatial Context Using Wearable Sensors",
        "authors": [
            "Amarnag Subramanya",
            "Alvin Raj",
            "Jeff A. Bilmes",
            "Dieter Fox"
        ],
        "abstract": "We introduce a new dynamic model with the capability of recognizing both activities that an individual is performing as well as where that ndividual is located. Our model is novel in that it utilizes a dynamic graphical model to jointly estimate both activity and spatial context over time based on the simultaneous use of asynchronous observations consisting of GPS measurements, and measurements from a small mountable sensor board. Joint inference is quite desirable as it has the ability to improve accuracy of the model. A key goal, however, in designing our overall system is to be able to perform accurate inference decisions while minimizing the amount of hardware an individual must wear. This minimization leads to greater comfort and flexibility, decreased power requirements and therefore increased battery life, and reduced cost. We show results indicating that our joint measurement model outperforms measurements from either the sensor board or GPS alone, using two types of probabilistic inference procedures, namely particle filtering and pruned exact inference.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6875",
        "title": "A simple approach for finding the globally optimal Bayesian network structure",
        "authors": [
            "Tomi Silander",
            "Petri Myllymaki"
        ],
        "abstract": "We study the problem of learning the best Bayesian network structure with respect to a decomposable score such as BDe, BIC or AIC. This problem is known to be NP-hard, which means that solving it becomes quickly infeasible as the number of variables increases. Nevertheless, in this paper we show that it is possible to learn the best Bayesian network structure with over 30 variables, which covers many practically interesting cases. Our algorithm is less complicated and more efficient than the techniques presented earlier. It can be easily parallelized, and offers a possibility for efficient exploration of the best networks consistent with different variable orderings. In the experimental part of the paper we compare the performance of the algorithm to the previous state-of-the-art algorithm. Free source-code and an online-demo can be found at ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6876",
        "title": "Identification of Conditional Interventional Distributions",
        "authors": [
            "Ilya Shpitser",
            "Judea Pearl"
        ],
        "abstract": "The subject of this paper is the elucidation of effects of actions from causal assumptions represented as a directed graph, and statistical knowledge given as a probability distribution. In particular, we are interested in predicting conditional distributions resulting from performing an action on a set of variables and, subsequently, taking measurements of another set. We provide a necessary and sufficient graphical condition for the cases where such distributions can be uniquely computed from the available information, as well as an algorithm which performs this computation whenever the condition holds. Furthermore, we use our results to prove completeness of do-calculus [Pearl, 1995] for the same identification problem.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6877",
        "title": "Inference in Hybrid Bayesian Networks Using Mixtures of Gaussians",
        "authors": [
            "Prakash P. Shenoy"
        ],
        "abstract": "The main goal of this paper is to describe a method for exact inference in general hybrid Bayesian networks (BNs) (with a mixture of discrete and continuous chance variables). Our method consists of approximating general hybrid Bayesian networks by a mixture of Gaussians (MoG) BNs. There exists a fast algorithm by Lauritzen-Jensen (LJ) for making exact inferences in MoG Bayesian networks, and there exists a commercial implementation of this algorithm. However, this algorithm can only be used for MoG BNs. Some limitations of such networks are as follows. All continuous chance variables must have conditional linear Gaussian distributions, and discrete chance nodes cannot have continuous parents. The methods described in this paper will enable us to use the LJ algorithm for a bigger class of hybrid Bayesian networks. This includes networks with continuous chance nodes with non-Gaussian distributions, networks with no restrictions on the topology of discrete and continuous variables, networks with conditionally deterministic variables that are a nonlinear function of their continuous parents, and networks with continuous chance variables whose variances are functions of their parents.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6879",
        "title": "Practical Linear Value-approximation Techniques for First-order MDPs",
        "authors": [
            "Scott Sanner",
            "Craig Boutilier"
        ],
        "abstract": "Recent work on approximate linear programming (ALP) techniques for first-order Markov Decision Processes (FOMDPs) represents the value function linearly w.r.t. a set of first-order basis functions and uses linear programming techniques to determine suitable weights. This approach offers the advantage that it does not require simplification of the first-order value function, and allows one to solve FOMDPs independent of a specific domain instantiation. In this paper, we address several questions to enhance the applicability of this work: (1) Can we extend the first-order ALP framework to approximate policy iteration to address performance deficiencies of previous approaches? (2) Can we automatically generate basis functions and evaluate their impact on value function quality? (3) How can we decompose intractable problems with universally quantified rewards into tractable subproblems? We propose answers to these questions along with a number of novel optimizations and provide a comparative empirical evaluation on logistics problems from the ICAPS 2004 Probabilistic Planning Competition.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.7064",
        "title": "Software Verification and Graph Similarity for Automated Evaluation of Students' Assignments",
        "authors": [
            "Milena Vujosevic-Janicic",
            "Mladen Nikolic",
            "Dusan Tosic",
            "Viktor Kuncak"
        ],
        "abstract": "In this paper we promote introducing software verification and control flow graph similarity measurement in automated evaluation of students' programs. We present a new grading framework that merges results obtained by combination of these two approaches with results obtained by automated testing, leading to improved quality and precision of automated grading. These two approaches are also useful in providing a comprehensible feedback that can help students to improve the quality of their programs We also present our corresponding tools that are publicly available and open source. The tools are based on LLVM low-level intermediate code representation, so they could be applied to a number of programming languages. Experimental evaluation of the proposed grading framework is performed on a corpus of university students' programs written in programming language C. Results of the experiments show that automatically generated grades are highly correlated with manually determined grades suggesting that the presented tools can find real-world applications in studying and grading.\n    ",
        "submission_date": "2012-06-29T00:00:00",
        "last_modified_date": "2012-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.0117",
        "title": "Rule Based Expert System for Cerebral Palsy Diagnosis",
        "authors": [
            "Rajdeep Borgohain",
            "Sugata Sanyal"
        ],
        "abstract": "The use of Artificial Intelligence is finding prominence not only in core computer areas, but also in cross disciplinary areas including medical diagnosis. In this paper, we present a rule based Expert System used in diagnosis of Cerebral Palsy. The expert system takes user input and depending on the symptoms of the patient, diagnoses if the patient is suffering from Cerebral Palsy. The Expert System also classifies the Cerebral Palsy as mild, moderate or severe based on the presented symptoms.\n    ",
        "submission_date": "2012-06-30T00:00:00",
        "last_modified_date": "2012-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.0206",
        "title": "Alternative Restart Strategies for CMA-ES",
        "authors": [
            "Ilya Loshchilov",
            "Marc Schoenauer",
            "Mich\u00e8le Sebag"
        ],
        "abstract": "This paper focuses on the restart strategy of CMA-ES on multi-modal functions. A first alternative strategy proceeds by decreasing the initial step-size of the mutation while doubling the population size at each restart. A second strategy adaptively allocates the computational budget among the restart settings in the BIPOP scheme. Both restart strategies are validated on the BBOB benchmark; their generality is also demonstrated on an independent real-world problem suite related to spacecraft trajectory optimization.\n    ",
        "submission_date": "2012-07-01T00:00:00",
        "last_modified_date": "2012-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.0262",
        "title": "Characteristic matrix of covering and its application to boolean matrix decomposition and axiomatization",
        "authors": [
            "Shiping Wang",
            "Qingxin Zhu",
            "William Zhu",
            "Fan Min"
        ],
        "abstract": "Covering is an important type of data structure while covering-based rough sets provide an efficient and systematic theory to deal with covering data. In this paper, we use boolean matrices to represent and axiomatize three types of covering approximation operators. First, we define two types of characteristic matrices of a covering which are essentially square boolean ones, and their properties are studied. Through the characteristic matrices, three important types of covering approximation operators are concisely equivalently represented. Second, matrix representations of covering approximation operators are used in boolean matrix decomposition. We provide a sufficient and necessary condition for a square boolean matrix to decompose into the boolean product of another one and its transpose. And we develop an algorithm for this boolean matrix decomposition. Finally, based on the above results, these three types of covering approximation operators are axiomatized using boolean matrices. In a word, this work borrows extensively from boolean matrices and present a new view to study covering-based rough sets.\n    ",
        "submission_date": "2012-07-02T00:00:00",
        "last_modified_date": "2013-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.0403",
        "title": "Robust Principal Component Analysis Using Statistical Estimators",
        "authors": [
            "Peratham Wiriyathammabhum",
            "Boonserm Kijsirikul"
        ],
        "abstract": "Principal Component Analysis (PCA) finds a linear mapping and maximizes the variance of the data which makes PCA sensitive to outliers and may cause wrong eigendirection. In this paper, we propose techniques to solve this problem; we use the data-centering method and reestimate the covariance matrix using robust statistic techniques such as median, robust scaling which is a booster to data-centering and Huber M-estimator which measures the presentation of outliers and reweight them with small values. The results on several real world data sets show that our proposed method handles outliers and gains better results than the original PCA and provides the same accuracy with lower computation cost than the Kernel PCA using the polynomial kernel in classification tasks.\n    ",
        "submission_date": "2012-07-02T00:00:00",
        "last_modified_date": "2012-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.0742",
        "title": "The OS* Algorithm: a Joint Approach to Exact Optimization and Sampling",
        "authors": [
            "Marc Dymetman",
            "Guillaume Bouchard",
            "Simon Carter"
        ],
        "abstract": "Most current sampling algorithms for high-dimensional distributions are based on MCMC techniques and are approximate in the sense that they are valid only asymptotically. Rejection sampling, on the other hand, produces valid samples, but is unrealistically slow in high-dimension spaces. The OS* algorithm that we propose is a unified approach to exact optimization and sampling, based on incremental refinements of a functional upper bound, which combines ideas of adaptive rejection sampling and of A* optimization search. We show that the choice of the refinement can be done in a way that ensures tractability in high-dimension spaces, and we present first experiments in two different settings: inference in high-order HMMs and in large discrete graphical models.\n    ",
        "submission_date": "2012-07-03T00:00:00",
        "last_modified_date": "2012-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.0833",
        "title": "Relational Data Mining Through Extraction of Representative Exemplars",
        "authors": [
            "Fr\u00e9d\u00e9ric Blanchard",
            "Michel Herbin"
        ],
        "abstract": "With the growing interest on Network Analysis, Relational Data Mining is becoming an emphasized domain of Data Mining. This paper addresses the problem of extracting representative elements from a relational dataset. After defining the notion of degree of representativeness, computed using the Borda aggregation procedure, we present the extraction of exemplars which are the representative elements of the dataset. We use these concepts to build a network on the dataset. We expose the main properties of these notions and we propose two typical applications of our framework. The first application consists in resuming and structuring a set of binary images and the second in mining co-authoring relation in a research team.\n    ",
        "submission_date": "2012-07-03T00:00:00",
        "last_modified_date": "2012-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1230",
        "title": "Higher-Order Partial Least Squares (HOPLS): A Generalized Multi-Linear Regression Method",
        "authors": [
            "Qibin Zhao",
            "Cesar F. Caiafa",
            "Danilo P. Mandic",
            "Zenas C. Chao",
            "Yasuo Nagasaka",
            "Naotaka Fujii",
            "Liqing Zhang",
            "Andrzej Cichocki"
        ],
        "abstract": "A new generalized multilinear regression model, termed the Higher-Order Partial Least Squares (HOPLS), is introduced with the aim to predict a tensor (multiway array) $\\tensor{Y}$ from a tensor $\\tensor{X}$ through projecting the data onto the latent space and performing regression on the corresponding latent variables. HOPLS differs substantially from other regression models in that it explains the data by a sum of orthogonal Tucker tensors, while the number of orthogonal loadings serves as a parameter to control model complexity and prevent overfitting. The low dimensional latent space is optimized sequentially via a deflation operation, yielding the best joint subspace approximation for both $\\tensor{X}$ and $\\tensor{Y}$. Instead of decomposing $\\tensor{X}$ and $\\tensor{Y}$ individually, higher order singular value decomposition on a newly defined generalized cross-covariance tensor is employed to optimize the orthogonal loadings. A systematic comparison on both synthetic data and real-world decoding of 3D movement trajectories from electrocorticogram (ECoG) signals demonstrate the advantages of HOPLS over the existing methods in terms of better predictive ability, suitability to handle small sample sizes, and robustness to noise.\n    ",
        "submission_date": "2012-07-05T00:00:00",
        "last_modified_date": "2012-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1350",
        "title": "Cost Sensitive Reachability Heuristics for Handling State Uncertainty",
        "authors": [
            "Daniel Bryce",
            "Subbarao Kambhampati"
        ],
        "abstract": "While POMDPs provide a general platform for non-deterministic conditional planning under a variety of quality metrics they have limited scalability. On the other hand, non-deterministic conditional planners scale very well, but many lack the ability to optimize plan quality metrics. We present a novel generalization of planning graph based heuristics that helps conditional planners both scale and generate high quality plans when using actions with nonuniform costs. We make empirical comparisons with two state of the art planners to show the benefit of our techniques.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1351",
        "title": "Stable Independence in Perfect Maps",
        "authors": [
            "Peter de Waal",
            "Linda C. van der Gaag"
        ],
        "abstract": "With the aid of the concept of stable independence we can construct, in an efficient way, a compact representation of a semi-graphoid independence relation. We show that this representation provides a new necessary condition for the existence of a directed perfect map for the relation. The test for this condition is based to a large extent on the transitivity property of a special form of d-separation. The complexity of the test is linear in the size of the representation. The test, moreover, brings the additional benefit that it can be used to guide the early stages of network construction.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1352",
        "title": "Prediction, Expectation, and Surprise: Methods, Designs, and Study of a Deployed Traffic Forecasting Service",
        "authors": [
            "Eric J. Horvitz",
            "Johnson Apacible",
            "Raman Sarin",
            "Lin Liao"
        ],
        "abstract": "We present research on developing models that forecast traffic flow and congestion in the Greater Seattle area. The research has led to the deployment of a service named JamBayes, that is being actively used by over 2,500 users via smartphones and desktop versions of the system. We review the modeling effort and describe experiments probing the predictive accuracy of the models. Finally, we present research on building models that can identify current and future surprises, via efforts on modeling and forecasting unexpected situations.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1353",
        "title": "'Say EM' for Selecting Probabilistic Models for Logical Sequences",
        "authors": [
            "Kristian Kersting",
            "Tapani Raiko"
        ],
        "abstract": "Many real world sequences such as protein secondary structures or shell logs exhibit a rich internal structures. Traditional probabilistic models of sequences, however, consider sequences of flat symbols only. Logical hidden Markov models have been proposed as one solution. They deal with logical sequences, i.e., sequences over an alphabet of logical atoms. This comes at the expense of a more complex model selection problem. Indeed, different abstraction levels have to be explored. In this paper, we propose a novel method for selecting logical hidden Markov models from data called SAGEM. SAGEM combines generalized expectation maximization, which optimizes parameters, with structure search for model selection using inductive logic programming refinement operators. We provide convergence and experimental results that show SAGEM's effectiveness.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1354",
        "title": "Of Starships and Klingons: Bayesian Logic for the 23rd Century",
        "authors": [
            "Kathryn Blackmond Laskey",
            "Paulo da Costa"
        ],
        "abstract": "Intelligent systems in an open world must reason about many interacting entities related to each other in diverse ways and having uncertain features and relationships. Traditional probabilistic languages lack the expressive power to handle relational domains. Classical first-order logic is sufficiently expressive, but lacks a coherent plausible reasoning capability. Recent years have seen the emergence of a variety of approaches to integrating first-order logic, probability, and machine learning. This paper presents Multi-entity Bayesian networks (MEBN), a formal system that integrates First Order Logic (FOL) with Bayesian probability theory. MEBN extends ordinary Bayesian networks to allow representation of graphical models with repeated sub-structures, and can express a probability distribution over models of any consistent, finitely axiomatizable first-order theory. We present the logic using an example inspired by the Paramount Series StarTrek.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1355",
        "title": "A Differential Semantics of Lazy AR Propagation",
        "authors": [
            "Anders L. Madsen"
        ],
        "abstract": "In this paper we present a differential semantics of Lazy AR Propagation (LARP) in discrete Bayesian networks. We describe how both single and multi dimensional partial derivatives of the evidence may easily be calculated from a junction tree in LARP equilibrium. We show that the simplicity of the calculations stems from the nature of LARP. Based on the differential semantics we describe how variable propagation in the LARP architecture may give access to additional partial derivatives. The cautious LARP (cLARP) scheme is derived to produce a flexible cLARP equilibrium that offers additional opportunities for calculating single and multidimensional partial derivatives of the evidence and subsets of the evidence from a single propagation. The results of an empirical evaluation illustrates how the access to a largely increased number of partial derivatives comes at a low computational cost.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1356",
        "title": "Modifying Bayesian Networks by Probability Constraints",
        "authors": [
            "Yun Peng",
            "Zhongli Ding"
        ],
        "abstract": "This paper deals with the following problem: modify a Bayesian network to satisfy a given set of probability constraints by only change its conditional probability tables, and the probability distribution of the resulting network should be as close as possible to that of the original network. We propose to solve this problem by extending IPFP (iterative proportional fitting procedure) to probability distributions represented by Bayesian networks. The resulting algorithm E-IPFP is further developed to D-IPFP, which reduces the computational cost by decomposing a global EIPFP into a set of smaller local E-IPFP problems. Limited analysis is provided, including the convergence proofs of the two algorithms. Computer experiments were conducted to validate the algorithms. The results are consistent with the theoretical analysis.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1357",
        "title": "Exploiting Evidence-dependent Sensitivity Bounds",
        "authors": [
            "Silja Renooij",
            "Linda C. van der Gaag"
        ],
        "abstract": "Studying the effects of one-way variation of any number of parameters on any number of output probabilities quickly becomes infeasible in practice, especially if various evidence profiles are to be taken into consideration. To provide for identifying the parameters that have a potentially large effect prior to actually performing the analysis, we need properties of sensitivity functions that are independent of the network under study, of the available evidence, or of both. In this paper, we study properties that depend upon just the probability of the entered evidence. We demonstrate that these properties provide for establishing an upper bound on the sensitivity value for a parameter; they further provide for establishing the region in which the vertex of the sensitivity function resides, thereby serving to identify parameters with a low sensitivity value that may still have a large impact on the probability of interest for relatively small parameter variations.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1359",
        "title": "MAA*: A Heuristic Search Algorithm for Solving Decentralized POMDPs",
        "authors": [
            "Daniel Szer",
            "Francois Charpillet",
            "Shlomo Zilberstein"
        ],
        "abstract": "We present multi-agent A* (MAA*), the first complete and optimal heuristic search algorithm for solving decentralized partially-observable Markov decision problems (DEC-POMDPs) with finite horizon. The algorithm is suitable for computing optimal plans for a cooperative group of agents that operate in a stochastic environment such as multirobot coordination, network traffic control, `or distributed resource allocation. Solving such problems efiectively is a major challenge in the area of planning under uncertainty. Our solution is based on a synthesis of classical heuristic search and decentralized control theory. Experimental results show that MAA* has significant advantages. We introduce an anytime variant of MAA* and conclude with a discussion of promising extensions such as an approach to solving infinite horizon problems.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1363",
        "title": "A unified setting for inference and decision: An argumentation-based approach",
        "authors": [
            "Leila Amgoud"
        ],
        "abstract": "Inferring from inconsistency and making decisions are two problems which have always been treated separately by researchers in Artificial Intelligence. Consequently, different models have been proposed for each category. Different argumentation systems [2, 7, 10, 11] have been developed for handling inconsistency in knowledge bases. Recently, other argumentation systems [3, 4, 8] have been defined for making decisions under uncertainty. The aim of this paper is to present a general argumentation framework in which both inferring from inconsistency and decision making are captured. The proposed framework can be used for decision under uncertainty, multiple criteria decision, rule-based decision and finally case-based decision. Moreover, works on classical decision suppose that the information about environment is coherent, and this no longer required by this general framework.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1367",
        "title": "Belief Updating and Learning in Semi-Qualitative Probabilistic Networks",
        "authors": [
            "Cassio Polpo de Campos",
            "Fabio Gagliardi Cozman"
        ],
        "abstract": "This paper explores semi-qualitative probabilistic networks (SQPNs) that combine numeric and qualitative information. We first show that exact inferences with SQPNs are NPPP-Complete. We then show that existing qualitative relations in SQPNs (plus probabilistic logic and imprecise assessments) can be dealt effectively through multilinear programming. We then discuss learning: we consider a maximum likelihood method that generates point estimates given a SQPN and empirical data, and we describe a Bayesian-minded method that employs the Imprecise Dirichlet Model to generate set-valued estimates.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1369",
        "title": "Hybrid Bayesian Networks with Linear Deterministic Variables",
        "authors": [
            "Barry Cobb",
            "Prakash P. Shenoy"
        ],
        "abstract": "When a hybrid Bayesian network has conditionally deterministic variables with continuous parents, the joint density function for the continuous variables does not exist. Conditional linear Gaussian distributions can handle such cases when the continuous variables have a multi-variate normal distribution and the discrete variables do not have continuous parents. In this paper, operations required for performing inference with conditionally deterministic variables in hybrid Bayesian networks are developed. These methods allow inference in networks with deterministic variables where continuous variables may be non-Gaussian, and their density functions can be approximated by mixtures of truncated exponentials. There are no constraints on the placement of continuous and discrete nodes in the network.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1370",
        "title": "On Bayesian Network Approximation by Edge Deletion",
        "authors": [
            "Arthur Choi",
            "Hei Chan",
            "Adnan Darwiche"
        ],
        "abstract": "We consider the problem of deleting edges from a Bayesian network for the purpose of simplifying models in probabilistic inference. In particular, we propose a new method for deleting network edges, which is based on the evidence at hand. We provide some interesting bounds on the KL-divergence between original and approximate networks, which highlight the impact of given evidence on the quality of approximation and shed some light on good and bad candidates for edge deletion. We finally demonstrate empirically the promise of the proposed edge deletion technique as a basis for approximate inference.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1372",
        "title": "Exploiting Evidence in Probabilistic Inference",
        "authors": [
            "Mark Chavira",
            "David Allen",
            "Adnan Darwiche"
        ],
        "abstract": "We define the notion of compiling a Bayesian network with evidence and provide a specific approach for evidence-based compilation, which makes use of logical processing. The approach is practical and advantageous in a number of application areas-including maximum likelihood estimation, sensitivity analysis, and MAP computations-and we provide specific empirical results in the domain of genetic linkage analysis. We also show that the approach is applicable for networks that do not contain determinism, and show that it empirically subsumes the performance of the quickscore algorithm when applied to noisy-or networks.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1373",
        "title": "Counterexample-guided Planning",
        "authors": [
            "Krishnendu Chatterjee",
            "Thomas A. Henzinger",
            "Ranjit Jhala",
            "Rupak Majumdar"
        ],
        "abstract": "Planning in adversarial and uncertain environments can be modeled as the problem of devising strategies in stochastic perfect information games. These games are generalizations of Markov decision processes (MDPs): there are two (adversarial) players, and a source of randomness. The main practical obstacle to computing winning strategies in such games is the size of the state space. In practice therefore, one typically works with abstractions of the model. The diffculty is to come up with an abstraction that is neither too coarse to remove all winning strategies (plans), nor too fine to be intractable. In verification, the paradigm of counterexample-guided abstraction refinement has been successful to construct useful but parsimonious abstractions automatically. We extend this paradigm to probabilistic models (namely, perfect information games and, as a special case, MDPs). This allows us to apply the counterexample-guided abstraction paradigm to the AI planning problem. As special cases, we get planning algorithms for MDPs and deterministic systems that automatically construct system abstractions.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1374",
        "title": "Use of Dempster-Shafer Conflict Metric to Detect Interpretation Inconsistency",
        "authors": [
            "Jennifer Carlson",
            "Robin R. Murphy"
        ],
        "abstract": "A model of the world built from sensor data may be incorrect even if the sensors are functioning correctly. Possible causes include the use of inappropriate sensors (e.g. a laser looking through glass walls), sensor inaccuracies accumulate (e.g. localization errors), the a priori models are wrong, or the internal representation does not match the world (e.g. a static occupancy grid used with dynamically moving objects). We are interested in the case where the constructed model of the world is flawed, but there is no access to the ground truth that would allow the system to see the discrepancy, such as a robot entering an unknown environment. This paper considers the problem of determining when something is wrong using only the sensor data used to construct the world model. It proposes 11 interpretation inconsistency indicators based on the Dempster-Shafer conflict metric, Con, and evaluates these indicators according to three criteria: ability to distinguish true inconsistency from sensor noise (classification), estimate the magnitude of discrepancies (estimation), and determine the source(s) (if any) of sensing problems in the environment (isolation). The evaluation is conducted using data from a mobile robot with sonar and laser range sensors navigating indoor environments under controlled conditions. The evaluation shows that the Gambino indicator performed best in terms of estimation (at best 0.77 correlation), isolation, and classification of the sensing situation as degraded (7% false negative rate) or normal (0% false positive rate).\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1375",
        "title": "Nonparametric Bayesian Logic",
        "authors": [
            "Peter Carbonetto",
            "Jacek Kisynski",
            "Nando de Freitas",
            "David L Poole"
        ],
        "abstract": "The Bayesian Logic (BLOG) language was recently developed for defining first-order probability models over worlds with unknown numbers of objects. It handles important problems in AI, including data association and population estimation. This paper extends BLOG by adopting generative processes over function spaces - known as nonparametrics in the Bayesian literature. We introduce syntax for reasoning about arbitrary collections of objects, and their properties, in an intuitive manner. By exploiting exchangeability, distributions over unknown objects and their attributes are cast as Dirichlet processes, which resolve difficulties in model selection and inference caused by varying numbers of objects. We demonstrate these concepts with application to citation matching.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1376",
        "title": "Counterfactual Reasoning in Linear Structural Equation Models",
        "authors": [
            "Zhihong Cai",
            "Manabu Kuroki"
        ],
        "abstract": "Consider the case where causal relations among variables can be described as a Gaussian linear structural equation model. This paper deals with the problem of clarifying how the variance of a response variable would have changed if a treatment variable were assigned to some value (counterfactually), given that a set of variables is observed (actually). In order to achieve this aim, we reformulate the formulas of the counterfactual distribution proposed by Balke and Pearl (1995) through both the total effects and a covariance matrix of observed variables. We further extend the framework of Balke and Pearl (1995) from point observations to interval observations, and from an unconditional plan to a conditional plan. The results of this paper enable us to clarify the properties of counterfactual distribution and establish an optimal plan.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1377",
        "title": "Efficient algorithm for estimation of qualitative expected utility in possibilistic case-based reasoning",
        "authors": [
            "Jakub Brzostowski",
            "Ryszard Kowalczyk"
        ],
        "abstract": "We propose an efficient algorithm for estimation of possibility based qualitative expected utility. It is useful for decision making mechanisms where each possible decision is assigned a multi-attribute possibility distribution. The computational complexity of ordinary methods calculating the expected utility based on discretization is growing exponentially with the number of attributes, and may become infeasible with a high number of these attributes. We present series of theorems and lemmas proving the correctness of our algorithm that exibits a linear computational complexity. Our algorithm has been applied in the context of selecting the most prospective partners in multi-party multi-attribute negotiation, and can also be used in making decisions about potential offers during the negotiation as other similar problems.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1378",
        "title": "Local Markov Property for Models Satisfying Composition Axiom",
        "authors": [
            "Changsung Kang",
            "Jin Tian"
        ],
        "abstract": "The local Markov condition for a DAG to be an independence map of a probability distribution is well known. For DAGs with latent variables, represented as bi-directed edges in the graph, the local Markov property may invoke exponential number of conditional independencies. This paper shows that the number of conditional independence relations required may be reduced if the probability distributions satisfy the composition axiom. In certain types of graphs, only linear number of conditional independencies are required. The result has applications in testing linear structural equation models with correlated errors.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1381",
        "title": "Unsupervised Activity Discovery and Characterization From Event-Streams",
        "authors": [
            "Rafay Hammid",
            "Siddhartha Maddi",
            "Amos Johnson",
            "Aaron Bobick",
            "Irfan Essa",
            "Charles Lee Isbell"
        ],
        "abstract": "We present a framework to discover and characterize different classes of everyday activities from event-streams. We begin by representing activities as bags of event n-grams. This allows us to analyze the global structural information of activities, using their local event statistics. We demonstrate how maximal cliques in an undirected edge-weighted graph of activities, can be used for activity-class discovery in an unsupervised manner. We show how modeling an activity as a variable length Markov process, can be used to discover recurrent event-motifs to characterize the discovered activity-classes. We present results over extensive data-sets, collected from multiple active environments, to show the competence and generalizability of our proposed framework.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1384",
        "title": "Modeling Transportation Routines using Hybrid Dynamic Mixed Networks",
        "authors": [
            "Vibhav Gogate",
            "Rina Dechter",
            "Bozhena Bidyuk",
            "Craig Rindt",
            "James Marca"
        ],
        "abstract": "This paper describes a general framework called Hybrid Dynamic Mixed Networks (HDMNs) which are Hybrid Dynamic Bayesian Networks that allow representation of discrete deterministic information in the form of constraints. We propose approximate inference algorithms that integrate and adjust well known algorithmic principles such as Generalized Belief Propagation, Rao-Blackwellised Particle Filtering and Constraint Propagation to address the complexity of modeling and reasoning in HDMNs. We use this framework to model a person's travel activity over time and to predict destination and routes given the current location. We present a preliminary empirical evaluation demonstrating the effectiveness of our modeling framework and algorithms using several variants of the activity model.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1385",
        "title": "Approximate Inference Algorithms for Hybrid Bayesian Networks with Discrete Constraints",
        "authors": [
            "Vibhav Gogate",
            "Rina Dechter"
        ],
        "abstract": "In this paper, we consider Hybrid Mixed Networks (HMN) which are Hybrid Bayesian Networks that allow discrete deterministic information to be modeled explicitly in the form of constraints. We present two approximate inference algorithms for HMNs that integrate and adjust well known algorithmic principles such as Generalized Belief Propagation, Rao-Blackwellised Importance Sampling and Constraint Propagation to address the complexity of modeling and reasoning in HMNs. We demonstrate the performance of our approximate inference algorithms on randomly generated HMNs.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1386",
        "title": "Metrics for Markov Decision Processes with Infinite State Spaces",
        "authors": [
            "Norman Ferns",
            "Prakash Panangaden",
            "Doina Precup"
        ],
        "abstract": "We present metrics for measuring state similarity in Markov decision processes (MDPs) with infinitely many states, including MDPs with continuous state spaces. Such metrics provide a stable quantitative analogue of the notion of bisimulation for MDPs, and are suitable for use in MDP approximation. We show that the optimal value function associated with a discounted infinite horizon planning task varies continuously with respect to our metric distances.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1387",
        "title": "Learning Bayesian Network Parameters with Prior Knowledge about Context-Specific Qualitative Influences",
        "authors": [
            "Ad Feelders",
            "Linda C. van der Gaag"
        ],
        "abstract": "We present a method for learning the parameters of a Bayesian network with prior knowledge about the signs of influences between variables. Our method accommodates not just the standard signs, but provides for context-specific signs as well. We show how the various signs translate into order constraints on the network parameters and how isotonic regression can be used to compute order-constrained estimates from the available data. Our experimental results show that taking prior knowledge about the signs of influences into account leads to an improved fit of the true distribution, especially when only a small sample of data is available. Moreover, the computed estimates are guaranteed to be consistent with the specified signs, thereby resulting in a network that is more likely to be accepted by experts in its domain of application.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1388",
        "title": "Planning in POMDPs Using Multiplicity Automata",
        "authors": [
            "Eyal Even-Dar",
            "Sham M. Kakade",
            "Yishay Mansour"
        ],
        "abstract": "Planning and learning in Partially Observable MDPs (POMDPs) are among the most challenging tasks in both the AI and Operation Research communities. Although solutions to these problems are intractable in general, there might be special cases, such as structured POMDPs, which can be solved efficiently. A natural and possibly efficient way to represent a POMDP is through the predictive state representation (PSR) - a representation which recently has been receiving increasing attention. In this work, we relate POMDPs to multiplicity automata- showing that POMDPs can be represented by multiplicity automata with no increase in the representation size. Furthermore, we show that the size of the multiplicity automaton is equal to the rank of the predictive state representation. Therefore, we relate both the predictive state representation and POMDPs to the well-founded multiplicity automata literature. Based on the multiplicity automata representation, we provide a planning algorithm which is exponential only in the multiplicity automata rank rather than the number of states of the POMDP. As a result, whenever the predictive state representation is logarithmic in the standard POMDP representation, our planning algorithm is efficient.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1389",
        "title": "On the Number of Experiments Sufficient and in the Worst Case Necessary to Identify All Causal Relations Among N Variables",
        "authors": [
            "Frederick Eberhardt",
            "Clark Glymour",
            "Richard Scheines"
        ],
        "abstract": "We show that if any number of variables are allowed to be simultaneously and independently randomized in any one experiment, log2(N) + 1 experiments are sufficient and in the worst case necessary to determine the causal relations among N >= 2 variables when no latent variables, no sample selection bias and no feedback cycles are present. For all K, 0 < K < 1/(2N) we provide an upper bound on the number experiments required to determine causal structure when each experiment simultaneously randomizes K variables. For large N, these bounds are significantly lower than the N - 1 bound required when each experiment randomizes at most one variable. For kmax < N/2, we show that (N/kmax-1)+N/(2kmax)log2(kmax) experiments aresufficient and in the worst case necessary. We over a conjecture as to the minimal number of experiments that are in the worst case sufficient to identify all causal relations among N observed variables that are a subset of the vertices of a DAG.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1390",
        "title": "Unstructuring User Preferences: Efficient Non-Parametric Utility Revelation",
        "authors": [
            "Carmel Domshlak",
            "Thorsten Joachims"
        ],
        "abstract": "Tackling the problem of ordinal preference revelation and reasoning, we propose a novel methodology for generating an ordinal utility function from a set of qualitative preference statements. To the best of our knowledge, our proposal constitutes the first nonparametric solution for this problem that is both efficient and semantically sound. Our initial experiments provide strong evidence for practical effectiveness of our approach.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1391",
        "title": "Existence and Finiteness Conditions for Risk-Sensitive Planning: Results and Conjectures",
        "authors": [
            "Yaxin Liu",
            "Sven Koenig"
        ],
        "abstract": "Decision-theoretic planning with risk-sensitive planning objectives is important for building autonomous agents or decision-support systems for real-world applications. However, this line of research has been largely ignored in the artificial intelligence and operations research communities since planning with risk-sensitive planning objectives is more complicated than planning with risk-neutral planning objectives. To remedy this situation, we derive conditions that guarantee that the optimal expected utilities of the total plan-execution reward exist and are finite for fully observable Markov decision process models with non-linear utility functions. In case of Markov decision process models with both positive and negative rewards, most of our results hold for stationary policies only, but we conjecture that they can be generalized to non stationary policies.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1394",
        "title": "Near-optimal Nonmyopic Value of Information in Graphical Models",
        "authors": [
            "Andreas Krause",
            "Carlos E. Guestrin"
        ],
        "abstract": "A fundamental issue in real-world systems, such as sensor networks, is the selection of observations which most effectively reduce uncertainty. More specifically, we address the long standing problem of nonmyopically selecting the most informative subset of variables in a graphical model. We present the first efficient randomized algorithm providing a constant factor (1-1/e-epsilon) approximation guarantee for any epsilon > 0 with high confidence. The algorithm leverages the theory of submodular functions, in combination with a polynomial bound on sample complexity. We furthermore prove that no polynomial time algorithm can provide a constant factor approximation better than (1 - 1/e) unless P = NP. Finally, we provide extensive evidence of the effectiveness of our method on two complex real-world datasets.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1395",
        "title": "On the optimality of tree-reweighted max-product message-passing",
        "authors": [
            "Vladimir Kolmogorov",
            "Martin Wainwright"
        ],
        "abstract": "Tree-reweighted max-product (TRW) message passing is a modified form of the ordinary max-product algorithm for attempting to find minimal energy configurations in Markov random field with cycles. For a TRW fixed point satisfying the strong tree agreement condition, the algorithm outputs a configuration that is provably optimal. In this paper, we focus on the case of binary variables with pairwise couplings, and establish stronger properties of TRW fixed points that satisfy only the milder condition of weak tree agreement (WTA). First, we demonstrate how it is possible to identify part of the optimal solution|i.e., a provably optimal solution for a subset of nodes| without knowing a complete solution. Second, we show that for submodular functions, a WTA fixed point always yields a globally optimal solution. We establish that for binary variables, any WTA fixed point always achieves the global maximum of the linear programming relaxation underlying the TRW method.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1397",
        "title": "A Revision-Based Approach to Resolving Conflicting Information",
        "authors": [
            "Guilin Qi",
            "Weiru Liu",
            "David A. Bell"
        ],
        "abstract": "In this paper, we propose a revision-based approach for conflict resolution by generalizing the Disjunctive Maxi-Adjustment (DMA) approach (Benferhat et al. 2004). Revision operators can be classified into two different families: the model-based ones and the formula-based ones. So the revision-based approach has two different versions according to which family of revision operators is chosen. Two particular revision operators are considered, one is the Dalal's revision operator, which is a model-based revision operator, and the other is the cardinality-maximal based revision operator, which is a formulabased revision operator. When the Dalal's revision operator is chosen, the revision-based approach is independent of the syntactic form in each stratum and it captures some notion of minimal change. When the cardinalitymaximal based revision operator is chosen, the revision-based approach is equivalent to the DMA approach. We also show that both approaches are computationally easier than the DMA approach.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1398",
        "title": "Asynchronous Dynamic Bayesian Networks",
        "authors": [
            "Avi Pfeffer",
            "Terry Tai"
        ],
        "abstract": "Systems such as sensor networks and teams of autonomous robots consist of multiple autonomous entities that interact with each other in a distributed, asynchronous manner. These entities need to keep track of the state of the system as it evolves. Asynchronous systems lead to special challenges for monitoring, as nodes must update their beliefs independently of each other and no central coordination is possible. Furthermore, the state of the system continues to change as beliefs are being updated. Previous approaches to developing distributed asynchronous probabilistic reasoning systems have used static models. We present an approach using dynamic models, that take into account the way the system changes state over time. Our approach, which is based on belief propagation, is fully distributed and asynchronous, and allows the world to keep on changing as messages are being sent around. Experimental results show that our approach compares favorably to the factored frontier algorithm.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1401",
        "title": "Expectation Propagation for Continuous Time Bayesian Networks",
        "authors": [
            "Uri Nodelman",
            "Daphne Koller",
            "Christian R. Shelton"
        ],
        "abstract": "Continuous time Bayesian networks (CTBNs) describe structured stochastic processes with finitely many states that evolve over continuous time. A CTBN is a directed (possibly cyclic) dependency graph over a set of variables, each of which represents a finite state continuous time Markov process whose transition model is a function of its parents. As shown previously, exact inference in CTBNs is intractable. We address the problem of approximate inference, allowing for general queries conditioned on evidence over continuous time intervals and at discrete time points. We show how CTBNs can be parameterized within the exponential family, and use that insight to develop a message passing scheme in cluster graphs and allows us to apply expectation propagation to CTBNs. The clusters in our cluster graph do not contain distributions over the cluster variables at individual time points, but distributions over trajectories of the variables throughout a duration. Thus, unlike discrete time temporal models such as dynamic Bayesian networks, we can adapt the time granularity at which we reason for different variables and in different conditions.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1402",
        "title": "Expectation Maximization and Complex Duration Distributions for Continuous Time Bayesian Networks",
        "authors": [
            "Uri Nodelman",
            "Christian R. Shelton",
            "Daphne Koller"
        ],
        "abstract": "Continuous time Bayesian networks (CTBNs) describe structured stochastic processes with finitely many states that evolve over continuous time. A CTBN is a directed (possibly cyclic) dependency graph over a set of variables, each of which represents a finite state continuous time Markov process whose transition model is a function of its parents. We address the problem of learning the parameters and structure of a CTBN from partially observed data. We show how to apply expectation maximization (EM) and structural expectation maximization (SEM) to CTBNs. The availability of the EM algorithm allows us to extend the representation of CTBNs to allow a much richer class of transition durations distributions, known as phase distributions. This class is a highly expressive semi-parametric representation, which can approximate any duration distribution arbitrarily closely. This extension to the CTBN framework addresses one of the main limitations of both CTBNs and DBNs - the restriction to exponentially / geometrically distributed duration. We present experimental results on a real data set of people's life spans, showing that our algorithm learns reasonable models - structure and parameters - from partially observed data, and, with the use of phase distributions, achieves better performance than DBNs.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1405",
        "title": "Sufficient conditions for convergence of Loopy Belief Propagation",
        "authors": [
            "Joris Mooij",
            "Hilbert Kappen"
        ],
        "abstract": "We derive novel sufficient conditions for convergence of Loopy Belief Propagation (also known as the Sum-Product algorithm) to a unique fixed point. Our results improve upon previously known conditions. For binary variables with (anti-)ferromagnetic interactions, our conditions seem to be sharp.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1407",
        "title": "The Relationship Between AND/OR Search and Variable Elimination",
        "authors": [
            "Robert Mateescu",
            "Rina Dechter"
        ],
        "abstract": "In this paper we compare search and inference in graphical models through the new framework of AND/OR search. Specifically, we compare Variable Elimination (VE) and memoryintensive AND/OR Search (AO) and place algorithms such as graph-based backjumping and no-good and good learning, as well as Recursive Conditioning [7] and Value Elimination [2] within the AND/OR search framework.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1408",
        "title": "Representation Policy Iteration",
        "authors": [
            "Sridhar Mahadevan"
        ],
        "abstract": "This paper addresses a fundamental issue central to approximation methods for solving large Markov decision processes (MDPs): how to automatically learn the underlying representation for value function approximation? A novel theoretically rigorous framework is proposed that automatically generates geometrically customized orthonormal sets of basis functions, which can be used with any approximate MDP solver like least squares policy iteration (LSPI). The key innovation is a coordinate-free representation of value functions, using the theory of smooth functions on a Riemannian manifold. Hodge theory yields a constructive method for generating basis functions for approximating value functions based on the eigenfunctions of the self-adjoint (Laplace-Beltrami) operator on manifolds. In effect, this approach performs a global Fourier analysis on the state space graph to approximate value functions, where the basis functions reflect the largescale topology of the underlying state space. A new class of algorithms called Representation Policy Iteration (RPI) are presented that automatically learn both basis functions and approximately optimal policies. Illustrative experiments compare the performance of RPI with that of LSPI using two handcoded basis functions (RBF and polynomial state encodings).\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1410",
        "title": "Description Logics with Fuzzy Concrete Domains",
        "authors": [
            "Umberto Straccia"
        ],
        "abstract": "We present a fuzzy version of description logics with concrete domains. Main features are: (i) concept constructors are based on t-norm, t-conorm, negation and implication; (ii) concrete domains are fuzzy sets; (iii) fuzzy modifiers are allowed; and (iv) the reasoning algorithm is based on a mixture of completion rules and bounded mixed integer programming.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1412",
        "title": "Point-Based POMDP Algorithms: Improved Analysis and Implementation",
        "authors": [
            "Trey Smith",
            "Reid Simmons"
        ],
        "abstract": "Existing complexity bounds for point-based POMDP value iteration algorithms focus either on the curse of dimensionality or the curse of history. We derive a new bound that relies on both and uses the concept of discounted reachability; our conclusions may help guide future algorithm design. We also discuss recent improvements to our (point-based) heuristic search value iteration algorithm. Our new implementation calculates tighter initial bounds, avoids solving linear programs, and makes more effective use of sparsity.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1415",
        "title": "Approximate Linear Programming for First-order MDPs",
        "authors": [
            "Scott Sanner",
            "Craig Boutilier"
        ],
        "abstract": "We introduce a new approximate solution technique for first-order Markov decision processes (FOMDPs). Representing the value function linearly w.r.t. a set of first-order basis functions, we compute suitable weights by casting the corresponding optimization as a first-order linear program and show how off-the-shelf theorem prover and LP software can be effectively used. This technique allows one to solve FOMDPs independent of a specific domain instantiation; furthermore, it allows one to determine bounds on approximation error that apply equally to all domain instantiations. We apply this solution technique to the task of elevator scheduling with a rich feature space and multi-criteria additive reward, and demonstrate that it outperforms a number of intuitive, heuristicallyguided policies.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1416",
        "title": "Predictive Linear-Gaussian Models of Stochastic Dynamical Systems",
        "authors": [
            "Matthew Rudary",
            "Satinder Singh",
            "David Wingate"
        ],
        "abstract": "Models of dynamical systems based on predictive state representations (PSRs) are defined strictly in terms of observable quantities, in contrast with traditional models (such as Hidden Markov Models) that use latent variables or statespace representations. In addition, PSRs have an effectively infinite memory, allowing them to model some systems that finite memory-based models cannot. Thus far, PSR models have primarily been developed for domains with discrete observations. Here, we develop the Predictive Linear-Gaussian (PLG) model, a class of PSR models for domains with continuous observations. We show that PLG models subsume Linear Dynamical System models (also called Kalman filter models or state-space models) while using fewer parameters. We also introduce an algorithm to estimate PLG parameters from data, and contrast it with standard Expectation Maximization (EM) algorithms used to estimate Kalman filter parameters. We show that our algorithm is a consistent estimation procedure and present preliminary empirical results suggesting that our algorithm outperforms EM, particularly as the model dimension increases.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1418",
        "title": "Efficient Test Selection in Active Diagnosis via Entropy Approximation",
        "authors": [
            "Alice X. Zheng",
            "Irina Rish",
            "Alina Beygelzimer"
        ],
        "abstract": "We consider the problem of diagnosing faults in a system represented by a Bayesian network, where diagnosis corresponds to recovering the most likely state of unobserved nodes given the outcomes of tests (observed nodes). Finding an optimal subset of tests in this setting is intractable in general. We show that it is difficult even to compute the next most-informative test using greedy test selection, as it involves several entropy terms whose exact computation is intractable. We propose an approximate approach that utilizes the loopy belief propagation infrastructure to simultaneously compute approximations of marginal and conditional entropies on multiple subsets of nodes. We apply our method to fault diagnosis in computer networks, and show the algorithm to be very effective on realistic Internet-like topologies. We also provide theoretical justification for the greedy test selection approach, along with some performance guarantees.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1419",
        "title": "A Transformational Characterization of Markov Equivalence for Directed Acyclic Graphs with Latent Variables",
        "authors": [
            "Jiji Zhang",
            "Peter L. Spirtes"
        ],
        "abstract": "Different directed acyclic graphs (DAGs) may be Markov equivalent in the sense that they entail the same conditional independence relations among the observed variables. Chickering (1995) provided a transformational characterization of Markov equivalence for DAGs (with no latent variables), which is useful in deriving properties shared by Markov equivalent DAGs, and, with certain generalization, is needed to prove the asymptotic correctness of a search procedure over Markov equivalence classes, known as the GES algorithm. For DAG models with latent variables, maximal ancestral graphs (MAGs) provide a neat representation that facilitates model search. However, no transformational characterization -- analogous to Chickering's -- of Markov equivalent MAGs is yet available. This paper establishes such a characterization for directed MAGs, which we expect will have similar uses as it does for DAGs.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1422",
        "title": "Importance Sampling in Bayesian Networks: An Influence-Based Approximation Strategy for Importance Functions",
        "authors": [
            "Changhe Yuan",
            "Marek J. Druzdzel"
        ],
        "abstract": "One of the main problems of importance sampling in Bayesian networks is representation of the importance function, which should ideally be as close as possible to the posterior joint distribution. Typically, we represent an importance function as a factorization, i.e., product of conditional probability tables (CPTs). Given diagnostic evidence, we do not have explicit forms for the CPTs in the networks. We first derive the exact form for the CPTs of the optimal importance function. Since the calculation is hard, we usually only use their approximations. We review several popular strategies and point out their limitations. Based on an analysis of the influence of evidence, we propose a method for approximating the exact form of importance function by explicitly modeling the most important additional dependence relations introduced by evidence. Our experimental results show that the new approximation strategy offers an immediate improvement in the quality of the importance function.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1425",
        "title": "Qualitative Decision Making Under Possibilistic Uncertainty: Toward more discriminating criteria",
        "authors": [
            "Paul Weng"
        ],
        "abstract": "The aim of this paper is to propose a generalization of previous approaches in qualitative decision making. Our work is based on the binary possibilistic utility (PU), which is a possibilistic counterpart of Expected Utility (EU).We first provide a new axiomatization of PU and study its relation with the lexicographic aggregation of pessimistic and optimistic utilities. Then we explain the reasons of the coarseness of qualitative decision criteria. Finally, thanks to a redefinition of possibilistic lotteries and mixtures, we present the refined binary possibilistic utility, which is more discriminating than previously proposed criteria.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1426",
        "title": "Structured Region Graphs: Morphing EP into GBP",
        "authors": [
            "Max Welling",
            "Thomas P. Minka",
            "Yee Whye Teh"
        ],
        "abstract": "GBP and EP are two successful algorithms for approximate probabilistic inference, which are based on different approximation strategies. An open problem in both algorithms has been how to choose an appropriate approximation structure. We introduce 'structured region graphs', a formalism which marries these two strategies, reveals a deep connection between them, and suggests how to choose good approximation structures. In this formalism, each region has an internal structure which defines an exponential family, whose sufficient statistics must be matched by the parent region. Reduction operators on these structures allow conversion between EP and GBP free energies. Thus it is revealed that all EP approximations on discrete variables are special cases of GBP, and conversely that some wellknown GBP approximations, such as overlapping squares, are special cases of EP. Furthermore, region graphs derived from EP have a number of good structural properties, including maxent-normality and overall counting number of one. The result is a convenient framework for producing high-quality approximations with a user-adjustable level of complexity\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1427",
        "title": "A Model for Reasoning with Uncertain Rules in Event Composition Systems",
        "authors": [
            "Segev Wasserkrug",
            "Avigdor Gal",
            "Opher Etzion"
        ],
        "abstract": "In recent years, there has been an increased need for the use of active systems - systems required to act automatically based on events, or changes in the environment. Such systems span many areas, from active databases to applications that drive the core business processes of today's enterprises. However, in many cases, the events to which the system must respond are not generated by monitoring tools, but must be inferred from other events based on complex temporal predicates. In addition, in many applications, such inference is inherently uncertain. In this paper, we introduce a formal framework for knowledge representation and reasoning enabling such event inference. Based on probability theory, we define the representation of the associated uncertainty. In addition, we formally define the probability space, and show how the relevant probabilities can be calculated by dynamically constructing a Bayesian network. To the best of our knowledge, this is the first work that enables taking such uncertainty into account in the context of active systems. herefore, our contribution is twofold: We formally define the representation and semantics of event composition for probabilistic settings, and show how to apply these extensions to the quantification of the occurrence probability of events. These results enable any active system to handle such uncertainty.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1501",
        "title": "Super-Mixed Multiple Attribute Group Decision Making Method Based on Hybrid Fuzzy Grey Relation Approach Degree",
        "authors": [
            "Gol Kim",
            "Fei Ye"
        ],
        "abstract": "The feature of our method different from other fuzzy grey relation method for supermixed multiple attribute group decision-making is that all of the subjective and objective weights are obtained by interval grey number and that the group decisionmaking is performed based on the relative approach degree of grey TOPSIS, the relative approach degree of grey incidence and the relative membership degree of grey incidence using 4-dimensional Euclidean distance. The weighted Borda method is used to obtain final rank by using the results of four methods. An example shows the applicability of the proposed approach.\n    ",
        "submission_date": "2012-07-06T00:00:00",
        "last_modified_date": "2012-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1534",
        "title": "Generalized Hybrid Grey Relation Method for Multiple Attribute Mixed Type Decision Making",
        "authors": [
            "Gol Kim",
            "Yunchol Jong",
            "Sifeng Liu"
        ],
        "abstract": "The multiple attribute mixed type decision making is performed by four methods, that is, the relative approach degree of grey TOPSIS method, the relative approach degree of grey incidence, the relative membership degree of grey incidence and the grey relation relative approach degree method using the maximum entropy estimation, respectively. In these decision making methods, the grey incidence degree in four-dimensional Euclidean space is used. The final arrangement result is obtained by weighted Borda method. An example illustrates the applicability of the proposed approach.\n    ",
        "submission_date": "2012-07-06T00:00:00",
        "last_modified_date": "2012-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1641",
        "title": "Syntactic vs. Semantic Locality: How Good Is a Cheap Approximation?",
        "authors": [
            "Chiara Del Vescovo",
            "Pavel Klinov",
            "Bijan Parsia",
            "Uli Sattler",
            "Thomas Schneider",
            "Dmitry Tsarkov"
        ],
        "abstract": "Extracting a subset of a given OWL ontology that captures all the ontology's knowledge about a specified set of terms is a well-understood task. This task can be based, for instance, on locality-based modules (LBMs). These come in two flavours, syntactic and semantic, and a syntactic LBM is known to contain the corresponding semantic LBM. For syntactic LBMs, polynomial extraction algorithms are known, implemented in the OWL API, and being used. In contrast, extracting semantic LBMs involves reasoning, which is intractable for OWL 2 DL, and these algorithms had not been implemented yet for expressive ontology languages. We present the first implementation of semantic LBMs and report on experiments that compare them with syntactic LBMs extracted from real-life ontologies. Our study reveals whether semantic LBMs are worth the additional extraction effort, compared with syntactic LBMs.\n    ",
        "submission_date": "2012-07-06T00:00:00",
        "last_modified_date": "2012-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1811",
        "title": "The SeqBin Constraint Revisited",
        "authors": [
            "George Katsirelos",
            "Nina Narodytska",
            "Toby Walsh"
        ],
        "abstract": "We revisit the SeqBin constraint. This meta-constraint subsumes a number of important global constraints like Change, Smooth and IncreasingNValue. We show that the previously proposed filtering algorithm for SeqBin has two drawbacks even under strong restrictions: it does not detect bounds disentailment and it is not idempotent. We identify the cause for these problems, and propose a new propagator that overcomes both issues. Our algorithm is based on a connection to the problem of finding a path of a given cost in a restricted $n$-partite graph. Our propagator enforces domain consistency in O(nd^2) and, for special cases of SeqBin that include Change, Smooth and IncreasingNValue, in O(nd) time.\n    ",
        "submission_date": "2012-07-07T00:00:00",
        "last_modified_date": "2012-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1832",
        "title": "Minimal Proof Search for Modal Logic K Model Checking",
        "authors": [
            "Abdallah Saffidine"
        ],
        "abstract": "Most modal logics such as S5, LTL, or ATL are extensions of Modal Logic K. While the model checking problems for LTL and to a lesser extent ATL have been very active research areas for the past decades, the model checking problem for the more basic Multi-agent Modal Logic K (MMLK) has important applications as a formal framework for perfect information multi-player games on its own.\n",
        "submission_date": "2012-07-08T00:00:00",
        "last_modified_date": "2012-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.2232",
        "title": "Effective Enabling of Sharing and Reuse of Knowledge On Semantic Web by Ontology in Date Fruit Model",
        "authors": [
            "P. C. Sherimon",
            "Reshmy Krishnan",
            "P. V. Vinu"
        ],
        "abstract": "Since Organizations have recognized that knowledge constitutes a valuable intangible asset for creating and sustaining competitive advantages, knowledge sharing has a vital role in present society. It is an activity through which information is exchanged among people through different media. Many problems face the area of knowledge sharing and knowledge reuse. Currently, knowledge sharing between entities is achieved in a very ad-hoc fashion, lacking proper understanding of the meaning of the data. Ontologies can potentially solve these problems by facilitating knowledge sharing and reuse through formal and real-world semantics. Ontologies, through formal semantics, are machine-understandable. A computer can process data, annotated with references to ontologies, and through the knowledge encapsulated in the ontology, deduce facts from the original data. The date fruit is the most enduring symbol of the Sultanate's rich heritage. Creating ontology for dates will enrich the farming group and research scholars in the agro farm area.\n    ",
        "submission_date": "2012-07-10T00:00:00",
        "last_modified_date": "2012-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.2373",
        "title": "Arabic CALL system based on pedagogically indexed text",
        "authors": [
            "Mohamed Achraf Ben Mohamed",
            "Dhaou El Ghoul",
            "Mohamed Amine Nahdi",
            "Mourad Mars",
            "Mounir Zrigui"
        ],
        "abstract": "This article introduces the benefits of using computer as a tool for foreign language teaching and learning. It describes the effect of using Natural Language Processing (NLP) tools for learning Arabic. The technique explored in this particular case is the employment of pedagogically indexed corpora. This text-based method provides the teacher the advantage of building activities based on texts adapted to a particular pedagogical situation. This paper also presents ARAC: a Platform dedicated to language educators allowing them to create activities within their own pedagogical area of interest.\n    ",
        "submission_date": "2012-07-10T00:00:00",
        "last_modified_date": "2012-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.2459",
        "title": "Etude de Mod\u00e8les \u00e0 base de r\u00e9seaux Bay\u00e9siens pour l'aide au diagnostic de tumeurs c\u00e9r\u00e9brales",
        "authors": [
            "Fradj Ben Lamine",
            "Karim Kalti",
            "Mohamed Ali Mahjoub"
        ],
        "abstract": "This article describes different models based on Bayesian networks RB modeling expertise in the diagnosis of brain tumors. Indeed, they are well adapted to the representation of the uncertainty in the process of diagnosis of these tumors. In our work, we first tested several structures derived from the Bayesian network reasoning performed by doctors on the one hand and structures generated automatically on the other. This step aims to find the best structure that increases diagnostic accuracy. The machine learning algorithms relate MWST-EM algorithms, SEM and SEM + T. To estimate the parameters of the Bayesian network from a database incomplete, we have proposed an extension of the EM algorithm by adding a priori knowledge in the form of the thresholds calculated by the first phase of the algorithm RBE . The very encouraging results obtained are discussed at the end of the paper\n    ",
        "submission_date": "2012-07-10T00:00:00",
        "last_modified_date": "2012-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.2592",
        "title": "Novel Grey Interval Weight Determining and Hybrid Grey Interval Relation Method in Multiple Attribute Decision-Making",
        "authors": [
            "Gol Kim"
        ],
        "abstract": "This paper proposes a grey interval relation TOPSIS for the decision making in which all of the attribute weights and attribute values are given by the interval grey numbers. The feature of our method different from other grey relation decision-making is that all of the subjective and objective weights are obtained by interval grey number and that decisionmaking is performed based on the relative approach degree of grey TOPSIS, the relative approach degree of grey incidence and the relative membership degree of grey incidence using 2-dimensional Euclidean distance. The weighted Borda method is used for combining the results of three methods. An example shows the applicability of the proposed approach.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.2619",
        "title": "Conceptual Modelling and The Quality of Ontologies: Endurantism Vs. Perdurantism",
        "authors": [
            "Mutaz M. Al-Debei",
            "Mohammad Mourhaf Al Asswad",
            "Sergio de Cesare",
            "Mark Lycett"
        ],
        "abstract": "Ontologies are key enablers for sharing precise and machine-understandable semantics among different applications and parties. Yet, for ontologies to meet these expectations, their quality must be of a good standard. The quality of an ontology is strongly based on the design method employed. This paper addresses the design problems related to the modelling of ontologies, with specific concentration on the issues related to the quality of the conceptualisations produced. The paper aims to demonstrate the impact of the modelling paradigm adopted on the quality of ontological models and, consequently, the potential impact that such a decision can have in relation to the development of software applications. To this aim, an ontology that is conceptualised based on the Object-Role Modelling (ORM) approach (a representative of endurantism) is re-engineered into a one modelled on the basis of the Object Paradigm (OP) (a representative of perdurantism). Next, the two ontologies are analytically compared using the specified criteria. The conducted comparison highlights that using the OP for ontology conceptualisation can provide more expressive, reusable, objective and temporal ontologies than those conceptualised on the basis of the ORM approach.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.2761",
        "title": "A GPS Pseudorange Based Cooperative Vehicular Distance Measurement Technique",
        "authors": [
            "Daiqin Yang",
            "Fang Zhao",
            "Kai Liu",
            "Hock Beng Lim",
            "Emilio Frazzoli",
            "Daniela Rus"
        ],
        "abstract": "Accurate vehicular localization is important for various cooperative vehicle safety (CVS) applications such as collision avoidance, turning assistant, etc. In this paper, we propose a cooperative vehicular distance measurement technique based on the sharing of GPS pseudorange measurements and a weighted least squares method. The classic double difference pseudorange solution, which was originally designed for high-end survey level GPS systems, is adapted to low-end navigation level GPS receivers for its wide availability in ground vehicles. The Carrier to Noise Ratio (CNR) of raw pseudorange measurements are taken into account for noise mitigation. We present a Dedicated Short Range Communications (DSRC) based mechanism to implement the exchange of pseudorange information among neighboring vehicles. As demonstrated in field tests, our proposed technique increases the accuracy of the distance measurement significantly compared with the distance obtained from the GPS fixes.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.3270",
        "title": "Probabilistic Event Calculus for Event Recognition",
        "authors": [
            "Anastasios Skarlatidis",
            "Georgios Paliouras",
            "Alexander Artikis",
            "George A. Vouros"
        ],
        "abstract": "Symbolic event recognition systems have been successfully applied to a variety of application domains, extracting useful information in the form of events, allowing experts or other systems to monitor and respond when significant events are recognised. In a typical event recognition application, however, these systems often have to deal with a significant amount of uncertainty. In this paper, we address the issue of uncertainty in logic-based event recognition by extending the Event Calculus with probabilistic reasoning. Markov Logic Networks are a natural candidate for our logic-based formalism. However, the temporal semantics of the Event Calculus introduce a number of challenges for the proposed model. We show how and under what assumptions we can overcome these problems. Additionally, we study how probabilistic modelling changes the behaviour of the formalism, affecting its key property, the inertia of fluents. Furthermore, we demonstrate the advantages of the probabilistic Event Calculus through examples and experiments in the domain of activity recognition, using a publicly available dataset for video surveillance.\n    ",
        "submission_date": "2012-07-13T00:00:00",
        "last_modified_date": "2013-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.3315",
        "title": "Verifying an algorithm computing Discrete Vector Fields for digital imaging",
        "authors": [
            "J\u00f3nathan Heras",
            "Mar\u00eda Poza",
            "Julio Rubio"
        ],
        "abstract": "In this paper, we present a formalization of an algorithm to construct admissible discrete vector fields in the Coq theorem prover taking advantage of the SSReflect library. Discrete vector fields are a tool which has been welcomed in the homological analysis of digital images since it provides a procedure to reduce the amount of information but preserving the homological properties. In particular, thanks to discrete vector fields, we are able to compute, inside Coq, homological properties of biomedical images which otherwise are out of the reach of this system.\n    ",
        "submission_date": "2012-07-13T00:00:00",
        "last_modified_date": "2012-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.3434",
        "title": "An Approach to Model Interest for Planetary Rover through Dezert-Smarandache Theory",
        "authors": [
            "Matteo Ceriotti",
            "Massimiliano Vasile",
            "Giovanni Giardini",
            "Mauro Massari"
        ],
        "abstract": "In this paper, we propose an approach for assigning an interest level to the goals of a planetary rover. Assigning an interest level to goals, allows the rover autonomously to transform and reallocate the goals. The interest level is defined by data-fusing payload and navigation information. The fusion yields an \"interest map\", that quantifies the level of interest of each area around the rover. In this way the planner can choose the most interesting scientific objectives to be analyzed, with limited human intervention, and reallocates its goals autonomously. The Dezert-Smarandache Theory of Plausible and Paradoxical Reasoning was used for information fusion: this theory allows dealing with vague and conflicting data. In particular, it allows us directly to model the behavior of the scientists that have to evaluate the relevance of a particular set of goals. The paper shows an application of the proposed approach to the generation of a reliable interest map.\n    ",
        "submission_date": "2012-07-14T00:00:00",
        "last_modified_date": "2012-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.3543",
        "title": "Classification of Approaches and Challenges of Frequent Subgraphs Mining in Biological Networks",
        "authors": [
            "Mohammadreza Keyvanpour",
            "Fereshteh Azizani"
        ],
        "abstract": "Understanding the structure and dynamics of biological networks is one of the important challenges in system biology. In addition, increasing amount of experimental data in biological networks necessitate the use of efficient methods to analyze these huge amounts of data. Such methods require to recognize common patterns to analyze data. As biological networks can be modeled by graphs, the problem of common patterns recognition is equivalent with frequent sub graph mining in a set of graphs. In this paper, at first the challenges of frequent subgrpahs mining in biological networks are introduced and the existing approaches are classified for each challenge. then the algorithms are analyzed on the basis of the type of the approach they apply for each of the challenges.\n    ",
        "submission_date": "2012-07-15T00:00:00",
        "last_modified_date": "2012-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.3855",
        "title": "Hybrid Grey Interval Relation Decision-Making in Artistic Talent Evaluation of Player",
        "authors": [
            "Gol Kim",
            "Yunchol Jong",
            "Sifeng Liu",
            "Choe Rim Shong"
        ],
        "abstract": "This paper proposes a grey interval relation TOPSIS method for the decision making in which all of the attribute weights and attribute values are given by the interval grey numbers. In this paper, all of the subjective and objective weights are obtained by interval grey number and decision-making is based on four methods such as the relative approach degree of grey TOPSIS, the relative approach degree of grey incidence and the relative approach degree method using the maximum entropy estimation using 2-dimensional Euclidean distance. A multiple attribute decision-making example for evaluation of artistic talent of Kayagum (stringed Korean harp) players is given to show practicability of the proposed approach.\n    ",
        "submission_date": "2012-07-17T00:00:00",
        "last_modified_date": "2012-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.3863",
        "title": "Qualitative Approximate Behavior Composition",
        "authors": [
            "Nitin Yadav",
            "Sebastian Sardina"
        ],
        "abstract": "The behavior composition problem involves automatically building a controller that is able to realize a desired, but unavailable, target system (e.g., a house surveillance) by suitably coordinating a set of available components (e.g., video cameras, blinds, lamps, a vacuum cleaner, phones, etc.) Previous work has almost exclusively aimed at bringing about the desired component in its totality, which is highly unsatisfactory for unsolvable problems. In this work, we develop an approach for approximate behavior composition without departing from the classical setting, thus making the problem applicable to a much wider range of cases. Based on the notion of simulation, we characterize what a maximal controller and the \"closest\" implementable target module (optimal approximation) are, and show how these can be computed using ATL model checking technology for a special case. We show the uniqueness of optimal approximations, and prove their soundness and completeness with respect to their imported controllers.\n    ",
        "submission_date": "2012-07-17T00:00:00",
        "last_modified_date": "2012-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.3874",
        "title": "Reasoning about Agent Programs using ATL-like Logics",
        "authors": [
            "Nitin Yadav",
            "Sebastian Sardina"
        ],
        "abstract": "We propose a variant of Alternating-time Temporal Logic (ATL) grounded in the agents' operational know-how, as defined by their libraries of abstract plans. Inspired by ATLES, a variant itself of ATL, it is possible in our logic to explicitly refer to \"rational\" strategies for agents developed under the Belief-Desire-Intention agent programming paradigm. This allows us to express and verify properties of BDI systems using ATL-type logical frameworks.\n    ",
        "submission_date": "2012-07-17T00:00:00",
        "last_modified_date": "2012-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4107",
        "title": "Exploiting First-Order Regression in Inductive Policy Selection",
        "authors": [
            "Charles Gretton",
            "Sylvie Thiebaux"
        ],
        "abstract": "We consider the problem of computing optimal generalised policies for relational Markov decision processes. We describe an approach combining some of the benefits of purely inductive techniques with those of symbolic dynamic programming methods. The latter reason about the optimal value function using first-order decision theoretic regression and formula rewriting, while the former, when provided with a suitable hypotheses language, are capable of generalising value functions or policies for small instances. Our idea is to use reasoning and in particular classical first-order regression to automatically generate a hypotheses language dedicated to the domain at hand, which is then used as input by an inductive solver. This approach avoids the more complex reasoning of symbolic dynamic programming while focusing the inductive solver's attention on concepts that are specifically relevant to the optimal value function for the domain considered.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4111",
        "title": "Decision Making for Symbolic Probability",
        "authors": [
            "Phan H. Giang",
            "Sathyakama Sandilya"
        ],
        "abstract": "This paper proposes a decision theory for a symbolic generalization of probability theory (SP). Darwiche and Ginsberg [2,3] proposed SP to relax the requirement of using numbers for uncertainty while preserving desirable patterns of Bayesian reasoning. SP represents uncertainty by symbolic supports that are ordered partially rather than completely as in the case of standard probability. We show that a preference relation on acts that satisfies a number of intuitive postulates is represented by a utility function whose domain is a set of pairs of supports. We argue that a subjective interpretation is as useful and appropriate for SP as it is for numerical probability. It is useful because the subjective interpretation provides a basis for uncertainty elicitation. It is appropriate because we can provide a decision theory that explains how preference on acts is based on support comparison.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4114",
        "title": "Metrics for Finite Markov Decision Processes",
        "authors": [
            "Norman Ferns",
            "Prakash Panangaden",
            "Doina Precup"
        ],
        "abstract": "We present metrics for measuring the similarity of states in a finite Markov decision process (MDP). The formulation of our metrics is based on the notion of bisimulation for MDPs, with an aim towards solving discounted infinite horizon reinforcement learning tasks. Such metrics can be used to aggregate states, as well as to better structure other value function approximators (e.g., memory-based or nearest-neighbor approximators). We provide bounds that relate our metric distances to the optimal values of states in the given MDP.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4115",
        "title": "Dynamic Programming for Structured Continuous Markov Decision Problems",
        "authors": [
            "Zhengzhu Feng",
            "Richard Dearden",
            "Nicolas Meuleau",
            "Richard Washington"
        ],
        "abstract": "We describe an approach for exploiting structure in Markov Decision Processes with continuous state variables. At each step of the dynamic programming, the state space is dynamically partitioned into regions where the value function is the same throughout the region. We first describe the algorithm for piecewise constant representations. We then extend it to piecewise linear representations, using techniques from POMDPs to represent and reason about linear surfaces efficiently. We show that for complex, structured problems, our approach exploits the natural structure so that optimal solutions can be computed efficiently.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4116",
        "title": "Region-Based Incremental Pruning for POMDPs",
        "authors": [
            "Zhengzhu Feng",
            "Shlomo Zilberstein"
        ],
        "abstract": "We present a major improvement to the incremental pruning algorithm for solving partially observable Markov decision processes. Our technique targets the cross-sum step of the dynamic programming (DP) update, a key source of complexity in POMDP algorithms. Instead of reasoning about the whole belief space when pruning the cross-sums, our algorithm divides the belief space into smaller regions and performs independent pruning in each region. We evaluate the benefits of the new technique both analytically and experimentally, and show that it produces very significant performance gains. The results contribute to the scalability of POMDP algorithms to domains that cannot be handled by the best existing techniques.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4117",
        "title": "A Unified framework for order-of-magnitude confidence relations",
        "authors": [
            "Didier Dubois",
            "Helene Fargier"
        ],
        "abstract": "The aim of this work is to provide a unified framework for ordinal representations of uncertainty lying at the crosswords between possibility and probability theories. Such confidence relations between events are commonly found in monotonic reasoning, inconsistency management, or qualitative decision theory. They start either from probability theory, making it more qualitative, or from possibility theory, making it more expressive. We show these two trends converge to a class of genuine probability theories. We provide characterization results for these useful tools that preserve the qualitative nature of possibility rankings, while enjoying the power of expressivity of additive representations.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4119",
        "title": "Mixtures of Deterministic-Probabilistic Networks and their AND/OR Search Space",
        "authors": [
            "Rina Dechter",
            "Robert Mateescu"
        ],
        "abstract": "The paper introduces mixed networks, a new framework for expressing and reasoning with probabilistic and deterministic information. The framework combines belief networks with constraint networks, defining the semantics and graphical representation. We also introduce the AND/OR search space for graphical models, and develop a new linear space search algorithm. This provides the basis for understanding the benefits of processing the constraint information separately, resulting in the pruning of the search space. When the constraint part is tractable or has a small number of solutions, using the mixed representation can be exponentially more effective than using pure belief networks which odel constraints as conditional probability tables.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4120",
        "title": "Stable Independance and Complexity of Representation",
        "authors": [
            "Peter de Waal",
            "Linda C. van der Gaag"
        ],
        "abstract": "The representation of independence relations generally builds upon the well-known semigraphoid axioms of independence. Recently, a representation has been proposed that captures a set of dominant statements of an independence relation from which any other statement can be generated by means of the axioms; the cardinality of this set is taken to indicate the complexity of the relation. Building upon the idea of dominance, we introduce the concept of stability to provide for a more compact representation of independence. We give an associated algorithm for establishing such a ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4121",
        "title": "Propositional and Relational Bayesian Networks Associated with Imprecise and Qualitative Probabilistic Assesments",
        "authors": [
            "Fabio Gagliardi Cozman",
            "Cassio Polpo de Campos",
            "Jaime Ide",
            "Jose Carlos Ferreira da Rocha"
        ],
        "abstract": "This paper investigates a representation language with flexibility inspired by probabilistic logic and compactness inspired by relational Bayesian networks. The goal is to handle propositional and first-order constructs together with precise, imprecise, indeterminate and qualitative probabilistic assessments. The paper shows how this can be achieved through the theory of credal networks. New exact and approximate inference algorithms based on multilinear programming and iterated/loopy propagation of interval probabilities are presented; their superior performance, compared to existing ones, is shown empirically.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4123",
        "title": "A Logic Programming Framework for Possibilistic Argumentation with Vague Knowledge",
        "authors": [
            "Carlos Chesnevar",
            "Guillermo Simari",
            "Teresa Alsinet",
            "Lluis Godo"
        ],
        "abstract": "Defeasible argumentation frameworks have evolved to become a sound setting to formalize commonsense, qualitative reasoning from incomplete and potentially inconsistent knowledge. Defeasible Logic Programming (DeLP) is a defeasible argumentation formalism based on an extension of logic programming. Although DeLP has been successfully integrated in a number of different real-world applications, DeLP cannot deal with explicit uncertainty, nor with vague knowledge, as defeasibility is directly encoded in the object language. This paper introduces P-DeLP, a new logic programming language that extends original DeLP capabilities for qualitative reasoning by incorporating the treatment of possibilistic uncertainty and fuzzy knowledge. Such features will be formalized on the basis of PGL, a possibilistic logic based on Godel fuzzy logic.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4124",
        "title": "Sensitivity Analysis in Bayesian Networks: From Single to Multiple Parameters",
        "authors": [
            "Hei Chan",
            "Adnan Darwiche"
        ],
        "abstract": "Previous work on sensitivity analysis in Bayesian networks has focused on single parameters, where the goal is to understand the sensitivity of queries to single parameter changes, and to identify single parameter changes that would enforce a certain query constraint. In this paper, we expand the work to multiple parameters which may be in the CPT of a single variable, or the CPTs of multiple variables. Not only do we identify the solution space of multiple parameter changes that would be needed to enforce a query constraint, but we also show how to find the optimal solution, that is, the one which disturbs the current probability distribution the least (with respect to a specific measure of disturbance). We characterize the computational complexity of our new techniques and discuss their applications to developing and debugging Bayesian networks, and to the problem of reasoning about the value (reliability) of new information.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4126",
        "title": "Compact Value-Function Representations for Qualitative Preferences",
        "authors": [
            "Ronen I. Brafman",
            "Carmel Domshlak",
            "Tanya Kogan"
        ],
        "abstract": "We consider the challenge of preference elicitation in systems that help users discover the most desirable item(s) within a given database. Past work on preference elicitation focused on structured models that provide a factored representation of users' preferences. Such models require less information to construct and support efficient reasoning algorithms. This paper makes two substantial contributions to this area: (1) Strong representation theorems for factored value functions. (2) A methodology that utilizes our representation results to address the problem of optimal item selection.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4130",
        "title": "Using arguments for making decisions: A possibilistic logic approach",
        "authors": [
            "Leila Amgoud",
            "Henri Prade"
        ],
        "abstract": "Humans currently use arguments for explaining choices which are already made, or for evaluating potential choices. Each potential choice has usually pros and cons of various strengths. In spite of the usefulness of arguments in a decision making process, there have been few formal proposals handling this idea if we except works by Fox and Parsons and by Bonet and Geffner. In this paper we propose a possibilistic logic framework where arguments are built from an uncertain knowledge base and a set of prioritized goals. The proposed approach can compute two kinds of decisions by distinguishing between pessimistic and optimistic attitudes. When the available, maybe uncertain, knowledge is consistent, as well as the set of prioritized goals (which have to be fulfilled as far as possible), the method for evaluating decisions on the basis of arguments agrees with the possibility theory-based approach to decision-making under uncertainty. Taking advantage of its relation with formal approaches to defeasible argumentation, the proposed framework can be generalized in case of partially inconsistent knowledge, or goal bases.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4135",
        "title": "Case-Factor Diagrams for Structured Probabilistic Modeling",
        "authors": [
            "David A. McAllester",
            "Michael Collins",
            "Fernando Pereira"
        ],
        "abstract": "We introduce a probabilistic formalism subsuming Markov random fields of bounded tree width and probabilistic context free grammars. Our models are based on a representation of Boolean formulas that we call case-factor diagrams (CFDs). CFDs are similar to binary decision diagrams (BDDs) but are concise for circuits of bounded tree width (unlike BDDs) and can concisely represent the set of parse trees over a given string undera given context free grammar (also unlike BDDs). A probabilistic model consists of aCFD defining a feasible set of Boolean assignments and a weight (or cost) for each individual Boolean variable. We give an insideoutside algorithm for simultaneously computing the marginal of each Boolean variable, and a Viterbi algorithm for finding the mininum cost variable assignment. Both algorithms run in time proportional to the size of the CFD.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4136",
        "title": "Convolutional Factor Graphs as Probabilistic Models",
        "authors": [
            "Yongyi Mao",
            "Frank Kschischang",
            "Brendan J. Frey"
        ],
        "abstract": "Based on a recent development in the area of error control coding, we introduce the notion of convolutional factor graphs (CFGs) as a new class of probabilistic graphical models. In this context, the conventional factor graphs are referred to as multiplicative factor graphs (MFGs). This paper shows that CFGs are natural models for probability functions when summation of independent latent random variables is involved. In particular, CFGs capture a large class of linear models, where the linearity is in the sense that the observed variables are obtained as a linear ransformation of the latent variables taking arbitrary distributions. We use Gaussian models and independent factor models as examples to emonstrate the use of CFGs. The requirement of a linear transformation between latent variables (with certain independence restriction) and the bserved variables, to an extent, limits the modelling flexibility of CFGs. This structural restriction however provides a powerful analytic tool to the framework of CFGs; that is, upon taking the Fourier transform of the function represented by the CFG, the resulting function is represented by a FG with identical structure. This Fourier transform duality allows inference problems on a CFG to be solved on the corresponding dual MFG.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4137",
        "title": "An Empirical Evaluation of Possible Variations of Lazy Propagation",
        "authors": [
            "Anders L. Madsen"
        ],
        "abstract": "As real-world Bayesian networks continue to grow larger and more complex, it is important to investigate the possibilities for improving the performance of existing algorithms of probabilistic inference. Motivated by examples, we investigate the dependency of the performance of Lazy propagation on the message computation algorithm. We show how Symbolic Probabilistic Inference (SPI) and Arc-Reversal (AR) can be used for computation of clique to clique messages in the addition to the traditional use of Variable Elimination (VE). In addition, the paper resents the results of an empirical evaluation of the performance of Lazy propagation using VE, SPI, and AR as the message computation algorithm. The results of the empirical evaluation show that for most networks, the performance of inference did not depend on the choice of message computation algorithm, but for some randomly generated networks the choice had an impact on both space and time performance. In the cases where the choice had an impact, AR produced the best results.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4141",
        "title": "Pre-Selection of Independent Binary Features: An Application to Diagnosing Scrapie in Sheep",
        "authors": [
            "Ludmila Kuncheva",
            "C. Whitaker",
            "P. Cockcroft",
            "Z. S. Hoare"
        ],
        "abstract": "Suppose that the only available information in a multi-class problem are expert estimates of the conditional probabilities of occurrence for a set of binary features. The aim is to select a subset of features to be measured in subsequent data collection experiments. In the lack of any information about the dependencies between the features, we assume that all features are conditionally independent and hence choose the Naive Bayes classifier as the optimal classifier for the problem. Even in this (seemingly trivial) case of complete knowledge of the distributions, choosing an optimal feature subset is not straightforward. We discuss the properties and implementation details of Sequential Forward Selection (SFS) as a feature selection procedure for the current problem. A sensitivity analysis was carried out to investigate whether the same features are selected when the probabilities vary around the estimated values. The procedure is illustrated with a set of probability estimates for Scrapie in sheep.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4150",
        "title": "Solving Factored MDPs with Continuous and Discrete Variables",
        "authors": [
            "Carlos E. Guestrin",
            "Milos Hauskrecht",
            "Branislav Kveton"
        ],
        "abstract": "Although many real-world stochastic planning problems are more naturally formulated by hybrid models with both discrete and continuous variables, current state-of-the-art methods cannot adequately address these problems. We present the first framework that can exploit problem structure for modeling and solving hybrid problems efficiently. We formulate these problems as hybrid Markov decision processes (MDPs with continuous and discrete state and action variables), which we assume can be represented in a factored way using a hybrid dynamic Bayesian network (hybrid DBN). This formulation also allows us to apply our methods to collaborative multiagent settings. We present a new linear program approximation method that exploits the structure of the hybrid MDP and lets us compute approximate value functions more efficiently. In particular, we describe a new factored discretization of continuous variables that avoids the exponential blow-up of traditional approaches. We provide theoretical bounds on the quality of such an approximation and on its scale-up potential. We support our theoretical arguments with experiments on a set of control problems with up to 28-dimensional continuous state space and 22-dimensional action space.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4153",
        "title": "Annealed MAP",
        "authors": [
            "Changhe Yuan",
            "Tsai-Ching Lu",
            "Marek J. Druzdzel"
        ],
        "abstract": "Maximum a Posteriori assignment (MAP) is the problem of finding the most probable instantiation of a set of variables given the partial evidence on the other variables in a Bayesian network. MAP has been shown to be a NP-hard problem [22], even for constrained networks, such as polytrees [18]. Hence, previous approaches often fail to yield any results for MAP problems in large complex Bayesian networks. To address this problem, we propose AnnealedMAP algorithm, a simulated annealing-based MAP algorithm. The AnnealedMAP algorithm simulates a non-homogeneous Markov chain whose invariant function is a probability density that concentrates itself on the modes of the target density. We tested this algorithm on several real Bayesian networks. The results show that, while maintaining good quality of the MAP solutions, the AnnealedMAP algorithm is also able to solve many problems that are beyond the reach of previous approaches.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4154",
        "title": "Discretized Approximations for POMDP with Average Cost",
        "authors": [
            "Huizhen Yu",
            "Dimitri Bertsekas"
        ],
        "abstract": "In this paper, we propose a new lower approximation scheme for POMDP with discounted and average cost criterion. The approximating functions are determined by their values at a finite number of belief points, and can be computed efficiently using value iteration algorithms for finite-state MDP. While for discounted problems several lower approximation schemes have been proposed earlier, ours seems the first of its kind for average cost problems. We focus primarily on the average cost case, and we show that the corresponding approximation can be computed efficiently using multi-chain algorithms for finite-state MDP. We give a preliminary analysis showing that regardless of the existence of the optimal average cost J in the POMDP, the approximation obtained is a lower bound of the liminf optimal average cost function, and can also be used to calculate an upper bound on the limsup optimal average cost function, as well as bounds on the cost of executing the stationary policy associated with the approximation. Weshow the convergence of the cost approximation, when the optimal average cost is constant and the optimal differential cost is continuous.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4158",
        "title": "On the Choice of Regions for Generalized Belief Propagation",
        "authors": [
            "Max Welling"
        ],
        "abstract": "Generalized belief propagation (GBP) has proven to be a promising technique for approximate inference tasks in AI and machine learning. However, the choice of a good set of clusters to be used in GBP has remained more of an art then a science until this day. This paper proposes a sequential approach to adding new clusters of nodes and their interactions (i.e. \"regions\") to the approximation. We first review and analyze the recently introduced region graphs and find that three kinds of operations (\"split\", \"merge\" and \"death\") leave the free energy and (under some conditions) the fixed points of GBP invariant. This leads to the notion of \"weakly irreducible\" regions as the natural candidates to be added to the approximation. Computational complexity of the GBP algorithm is controlled by restricting attention to regions with small \"region-width\". Combining the above with an efficient (i.e. local in the graph) measure to predict the improved accuracy of GBP leads to the sequential \"region pursuit\" algorithm for adding new regions bottom-up to the region graph. Experiments show that this algorithm can indeed perform close to optimally.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4160",
        "title": "Monotonicity in Bayesian Networks",
        "authors": [
            "Linda C. van der Gaag",
            "Hans L. Bodlaender",
            "Ad Feelders"
        ],
        "abstract": "For many real-life Bayesian networks, common knowledge dictates that the output established for the main variable of interest increases with higher values for the observable variables. We define two concepts of monotonicity to capture this type of knowledge. We say that a network is isotone in distribution if the probability distribution computed for the output variable given specific observations is stochastically dominated by any such distribution given higher-ordered observations; a network is isotone in mode if a probability distribution given higher observations has a higher mode. We show that establishing whether a network exhibits any of these properties of monotonicity is coNPPP-complete in general, and remains coNP-complete for polytrees. We present an approximate algorithm for deciding whether a network is monotone in distribution and illustrate its application to a real-life network in oncology.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4161",
        "title": "Identifying Conditional Causal Effects",
        "authors": [
            "Jin Tian"
        ],
        "abstract": "This paper concerns the assessment of the effects of actions from a combination of nonexperimental data and causal assumptions encoded in the form of a directed acyclic graph in which some variables are presumed to be unobserved. We provide a procedure that systematically identifies cause effects between two sets of variables conditioned on some other variables, in time polynomial in the number of variables in the graph. The identifiable conditional causal effects are expressed in terms of the observed joint distribution.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4166",
        "title": "Heuristic Search Value Iteration for POMDPs",
        "authors": [
            "Trey Smith",
            "Reid Simmons"
        ],
        "abstract": "We present a novel POMDP planning algorithm called heuristic search value iteration (HSVI).HSVI is an anytime algorithm that returns a policy and a provable bound on its regret with respect to the optimal policy. HSVI gets its power by combining two well-known techniques: attention-focusing search heuristics and piecewise linear convex representations of the value function. HSVI's soundness and convergence have been proven. On some benchmark problems from the literature, HSVI displays speedups of greater than 100 with respect to other state-of-the-art POMDP value iteration algorithms. We also apply HSVI to a new rover exploration problem 10 times larger than most POMDP problems in the literature.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4167",
        "title": "Predictive State Representations: A New Theory for Modeling Dynamical Systems",
        "authors": [
            "Satinder Singh",
            "Michael James",
            "Matthew Rudary"
        ],
        "abstract": "Modeling dynamical systems, both for control purposes and to make predictions about their behavior, is ubiquitous in science and engineering. Predictive state representations (PSRs) are a recently introduced class of models for discrete-time dynamical systems. The key idea behind PSRs and the closely related OOMs (Jaeger's observable operator models) is to represent the state of the system as a set of predictions of observable outcomes of experiments one can do in the system. This makes PSRs rather different from history-based models such as nth-order Markov models and hidden-state-based models such as HMMs and POMDPs. We introduce an interesting construct, the systemdynamics matrix, and show how PSRs can be derived simply from it. We also use this construct to show formally that PSRs are more general than both nth-order Markov models and HMMs/POMDPs. Finally, we discuss the main difference between PSRs and OOMs and conclude with directions for future work.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4168",
        "title": "A New Characterization of Probabilities in Bayesian Networks",
        "authors": [
            "Lenhart Schubert"
        ],
        "abstract": "We characterize probabilities in Bayesian networks in terms of algebraic expressions called quasi-probabilities. These are arrived at by casting Bayesian networks as noisy AND-OR-NOT networks, and viewing the subnetworks that lead to a node as arguments for or against a node. Quasi-probabilities are in a sense the \"natural\" algebra of Bayesian networks: we can easily compute the marginal quasi-probability of any node recursively, in a compact form; and we can obtain the joint quasi-probability of any set of nodes by multiplying their marginals (using an idempotent product operator). Quasi-probabilities are easily manipulated to improve the efficiency of probabilistic inference. They also turn out to be representable as square-wave pulse trains, and joint and marginal distributions can be computed by multiplication and complementation of pulse trains.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4170",
        "title": "Evidence-invariant Sensitivity Bounds",
        "authors": [
            "Silja Renooij",
            "Linda C. van der Gaag"
        ],
        "abstract": "The sensitivities revealed by a sensitivity analysis of a probabilistic network typically depend on the entered evidence. For a real-life network therefore, the analysis is performed a number of times, with different evidence. Although efficient algorithms for sensitivity analysis exist, a complete analysis is often infeasible because of the large range of possible combinations of observations. In this paper we present a method for studying sensitivities that are invariant to the evidence entered. Our method builds upon the idea of establishing bounds between which a parameter can be varied without ever inducing a change in the most likely value of a variable of interest.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4173",
        "title": "Robustness of Causal Claims",
        "authors": [
            "Judea Pearl"
        ],
        "abstract": "A causal claim is any assertion that invokes causal relationships between variables, for example that a drug has a certain effect on preventing a disease. Causal claims are established through a combination of data and a set of causal assumptions called a causal model. A claim is robust when it is insensitive to violations of some of the causal assumptions embodied in the model. This paper gives a formal definition of this notion of robustness and establishes a graphical condition for quantifying the degree of robustness of a given causal claim. Algorithms for computing the degree of robustness are also presented.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4174",
        "title": "Robust Probabilistic Inference in Distributed Systems",
        "authors": [
            "Mark Paskin",
            "Carlos E. Guestrin"
        ],
        "abstract": "Probabilistic inference problems arise naturally in distributed systems such as sensor networks and teams of mobile robots. Inference algorithms that use message passing are a natural fit for distributed systems, but they must be robust to the failure situations that arise in real-world settings, such as unreliable communication and node failures. Unfortunately, the popular sum-product algorithm can yield very poor estimates in these settings because the nodes' beliefs before convergence can be arbitrarily different from the correct posteriors. In this paper, we present a new message passing algorithm for probabilistic inference which provides several crucial guarantees that the standard sum-product algorithm does not. Not only does it converge to the correct posteriors, but it is also guaranteed to yield a principled approximation at any point before convergence. In addition, the computational complexity of the message passing updates depends only upon the model, and is dependent of the network topology of the distributed system. We demonstrate the approach with detailed experimental results on a distributed sensor calibration task using data from an actual sensor network deployment.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4175",
        "title": "On Modeling Profiles instead of Values",
        "authors": [
            "Alon Orlitsky",
            "Narayana Santhanam",
            "Krishnamurthy Viswanathan",
            "Junan Zhang"
        ],
        "abstract": "We consider the problem of estimating the distribution underlying an observed sample of data. Instead of maximum likelihood, which maximizes the probability of the ob served values, we propose a different estimate, the high-profile distribution, which maximizes the probability of the observed profile the number of symbols appearing any given number of times. We determine the high-profile distribution of several data samples, establish some of its general properties, and show that when the number of distinct symbols observed is small compared to the data size, the high-profile and maximum-likelihood distributions are roughly the same, but when the number of symbols is large, the distributions differ, and high-profile better explains the data.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4176",
        "title": "Learning Diagnostic Policies from Examples by Systematic Search",
        "authors": [
            "Valentina Bayer-Zubek"
        ],
        "abstract": "A diagnostic policy specifies what test to perform next, based on the results of previous tests, and when to stop and make a diagnosis. Cost-sensitive diagnostic policies perform tradeoffs between (a) the cost of tests and (b) the cost of misdiagnoses. An optimal diagnostic policy minimizes the expected total cost. We formalize this diagnosis process as a Markov Decision Process (MDP). We investigate two types of algorithms for solving this MDP: systematic search based on AO* algorithm and greedy search (particularly the Value of Information method). We investigate the issue of learning the MDP probabilities from examples, but only as they are relevant to the search for good policies. We do not learn nor assume a Bayesian network for the diagnosis process. Regularizers are developed to control overfitting and speed up the search. This research is the first that integrates overfitting prevention into systematic search. The paper has two contributions: it discusses the factors that make systematic search feasible for diagnosis, and it shows experimentally, on benchmark data sets, that systematic search methods produce better diagnostic policies than greedy methods.\n    ",
        "submission_date": "2012-07-12T00:00:00",
        "last_modified_date": "2012-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4177",
        "title": "Hybrid Influence Diagrams Using Mixtures of Truncated Exponentials",
        "authors": [
            "Barry Cobb",
            "Prakash P. Shenoy"
        ],
        "abstract": "Mixtures of truncated exponentials (MTE) potentials are an alternative to discretization for representing continuous chance variables in influence diagrams. Also, MTE potentials can be used to approximate utility functions. This paper introduces MTE influence diagrams, which can represent decision problems without restrictions on the relationships between continuous and discrete chance variables, without limitations on the distributions of continuous chance variables, and without limitations on the nature of the utility functions. In MTE influence diagrams, all probability distributions and the joint utility function (or its multiplicative factors) are represented by MTE potentials and decision nodes are assumed to have discrete state spaces. MTE influence diagrams are solved by variable elimination using a fusion algorithm.\n    ",
        "submission_date": "2012-07-12T00:00:00",
        "last_modified_date": "2012-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4432",
        "title": "Towards Understanding Triangle Construction Problems",
        "authors": [
            "Vesna Marinkovic",
            "Predrag Janicic"
        ],
        "abstract": "Straightedge and compass construction problems are one of the oldest and most challenging problems in elementary mathematics. The central challenge, for a human or for a computer program, in solving construction problems is a huge search space. In this paper we analyze one family of triangle construction problems, aiming at detecting a small core of the underlying geometry knowledge. The analysis leads to a small set of needed definitions, lemmas and primitive construction steps, and consequently, to a simple algorithm for automated solving of problems from this family. The same approach can be applied to other families of construction problems.\n    ",
        "submission_date": "2012-07-18T00:00:00",
        "last_modified_date": "2012-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4525",
        "title": "SiGMa: Simple Greedy Matching for Aligning Large Knowledge Bases",
        "authors": [
            "Simon Lacoste-Julien",
            "Konstantina Palla",
            "Alex Davies",
            "Gjergji Kasneci",
            "Thore Graepel",
            "Zoubin Ghahramani"
        ],
        "abstract": "The Internet has enabled the creation of a growing number of large-scale knowledge bases in a variety of domains containing complementary information. Tools for automatically aligning these knowledge bases would make it possible to unify many sources of structured knowledge and answer complex queries. However, the efficient alignment of large-scale knowledge bases still poses a considerable challenge. Here, we present Simple Greedy Matching (SiGMa), a simple algorithm for aligning knowledge bases with millions of entities and facts. SiGMa is an iterative propagation algorithm which leverages both the structural information from the relationship graph as well as flexible similarity measures between entity properties in a greedy local search, thus making it scalable. Despite its greedy nature, our experiments indicate that SiGMa can efficiently match some of the world's largest knowledge bases with high precision. We provide additional experiments on benchmark datasets which demonstrate that SiGMa can outperform state-of-the-art approaches both in accuracy and efficiency.\n    ",
        "submission_date": "2012-07-19T00:00:00",
        "last_modified_date": "2012-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4708",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
        "authors": [
            "Marc G. Bellemare",
            "Yavar Naddaf",
            "Joel Veness",
            "Michael Bowling"
        ],
        "abstract": "In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.\n    ",
        "submission_date": "2012-07-19T00:00:00",
        "last_modified_date": "2013-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4813",
        "title": "Exploring the rationality of some syntactic merging operators (extended version)",
        "authors": [
            "Jos\u00e9 Luis Chac\u00f3n",
            "Ram\u00f3n Pino P\u00e9rez"
        ],
        "abstract": "Most merging operators are defined by semantics methods which have very high computational complexity. In order to have operators with a lower computational complexity, some merging operators defined in a syntactical way have be proposed. In this work we define some syntactical merging operators and exploring its rationality properties. To do that we constrain the belief bases to be sets of formulas very close to logic programs and the underlying logic is defined through forward chaining rule (Modus Ponens). We propose two types of operators: arbitration operators when the inputs are only two bases and fusion with integrity constraints operators. We introduce a set of postulates inspired of postulates LS, proposed by Liberatore and Shaerf and then we analyzed the first class of operators through these postulates. We also introduce a set of postulates inspired of postulates KP, proposed by Konieczny and Pino P\u00e9rez and then we analyzed the second class of operators through these postulates.\n    ",
        "submission_date": "2012-07-19T00:00:00",
        "last_modified_date": "2012-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4814",
        "title": "Automorphism Groups of Graphical Models and Lifted Variational Inference",
        "authors": [
            "Hung Hai Bui",
            "Tuyen N. Huynh",
            "Sebastian Riedel"
        ],
        "abstract": "Using the theory of group action, we first introduce the concept of the automorphism group of an exponential family or a graphical model, thus formalizing the general notion of symmetry of a probabilistic model. This automorphism group provides a precise mathematical framework for lifted inference in the general exponential family. Its group action partitions the set of random variables and feature functions into equivalent classes (called orbits) having identical marginals and expectations. Then the inference problem is effectively reduced to that of computing marginals or expectations for each class, thus avoiding the need to deal with each individual variable or feature. We demonstrate the usefulness of this general framework in lifting two classes of variational approximation for MAP inference: local LP relaxation and local LP relaxation with cycle constraints; the latter yields the first lifted inference that operate on a bound tighter than local constraints. Initial experimental results demonstrate that lifted MAP inference with cycle constraints achieved the state of the art performance, obtaining much better objective function values than local approximation while remaining relatively efficient.\n    ",
        "submission_date": "2012-07-19T00:00:00",
        "last_modified_date": "2012-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.5152",
        "title": "Stator flux optimization on direct torque control with fuzzy logic",
        "authors": [
            "Fatih Korkmaz",
            "M. Faruk Cakir",
            "Yilmaz Korkmaz",
            "Ismail Topaloglu"
        ],
        "abstract": "The Direct Torque Control (DTC) is well known as an effective control technique for high performance drives in a wide variety of industrial applications and conventional DTC technique uses two constant reference value: torque and stator flux. In this paper, fuzzy logic based stator flux optimization technique for DTC drives that has been proposed. The proposed fuzzy logic based stator flux optimizer self-regulates the stator flux reference using induction motor load situation without need of any motor parameters. Simulation studies have been carried out with Matlab/Simulink to compare the proposed system behaviors at vary load conditions. Simulation results show that the performance of the proposed DTC technique has been improved and especially at low-load conditions torque ripple are greatly reduced with respect to the conventional DTC.\n    ",
        "submission_date": "2012-07-21T00:00:00",
        "last_modified_date": "2012-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.5208",
        "title": "Meta-Learning of Exploration/Exploitation Strategies: The Multi-Armed Bandit Case",
        "authors": [
            "Francis Maes",
            "Damien Ernst",
            "Louis Wehenkel"
        ],
        "abstract": "The exploration/exploitation (E/E) dilemma arises naturally in many subfields of Science. Multi-armed bandit problems formalize this dilemma in its canonical form. Most current research in this field focuses on generic solutions that can be applied to a wide range of problems. However, in practice, it is often the case that a form of prior information is available about the specific class of target problems. Prior knowledge is rarely used in current solutions due to the lack of a systematic approach to incorporate it into the E/E strategy.\n",
        "submission_date": "2012-07-22T00:00:00",
        "last_modified_date": "2012-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.5293",
        "title": "Probability Bracket Notation: Multivariable Systems and Static Bayesian Networks",
        "authors": [
            "Xing M. Wang"
        ],
        "abstract": "We expand the Probability Bracket Notation (PBN), a symbolic framework inspired by the Dirac notation in quantum mechanics, to multivariable probability systems and static Bayesian networks (BNs). By introducing PBN for joint, marginal, and conditional probability distributions (PDs), as well as marginal and conditional expectations, we demonstrate how to express dependencies among multiple random variables concisely and manipulate them algebraically. Using the well-known Student BN as an example of probabilistic graphical models (PGMs), we show how to apply PBN to analyze predictions, inferences (using both bottom-up and top-down approaches), and expectations. We also extend PBN to BNs with continuous variables. After reviewing linear Gaussian networks, we introduce a customized Healthcare BN that includes both continuous and discrete random variables, utilizes user-specific data, and provides tailored predictions through discrete-display (DD) nodes as proxies for their continuous variable parents. Compared to traditional probability notation, PBN offers a unifying operator-like framework that simplifies the analysis of probabilistic models. This work highlights the potential of PBN as both an educational tool and a practical framework for probabilistic modeling, paving the way for applications in causal reasoning, inferences, expectations, data analytics, machine learning, and artificial intelligence.\n    ",
        "submission_date": "2012-07-23T00:00:00",
        "last_modified_date": "2025-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.5536",
        "title": "MCTS Based on Simple Regret",
        "authors": [
            "David Tolpin",
            "Solomon Eyal Shimony"
        ],
        "abstract": "UCT, a state-of-the art algorithm for Monte Carlo tree search (MCTS) in games and Markov decision processes, is based on UCB, a sampling policy for the Multi-armed Bandit problem (MAB) that minimizes the cumulative regret. However, search differs from MAB in that in MCTS it is usually only the final \"arm pull\" (the actual move selection) that collects a reward, rather than all \"arm pulls\". Therefore, it makes more sense to minimize the simple regret, as opposed to the cumulative regret. We begin by introducing policies for multi-armed bandits with lower finite-time and asymptotic simple regret than UCB, using it to develop a two-stage scheme (SR+CR) for MCTS which outperforms UCT empirically.\n",
        "submission_date": "2012-07-23T00:00:00",
        "last_modified_date": "2012-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.5589",
        "title": "VOI-aware MCTS",
        "authors": [
            "David Tolpin",
            "Solomon Eyal Shimony"
        ],
        "abstract": "UCT, a state-of-the art algorithm for Monte Carlo tree search (MCTS) in games and Markov decision processes, is based on UCB1, a sampling policy for the Multi-armed Bandit problem (MAB) that minimizes the cumulative regret. However, search differs from MAB in that in MCTS it is usually only the final \"arm pull\" (the actual move selection) that collects a reward, rather than all \"arm pulls\". In this paper, an MCTS sampling policy based on Value of Information (VOI) estimates of rollouts is suggested. Empirical evaluation of the policy and comparison to UCB1 and UCT is performed on random MAB instances as well as on Computer Go.\n    ",
        "submission_date": "2012-07-24T00:00:00",
        "last_modified_date": "2012-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.5879",
        "title": "Selecting Computations: Theory and Applications",
        "authors": [
            "Nicholas Hay",
            "Stuart Russell",
            "David Tolpin",
            "Solomon Eyal Shimony"
        ],
        "abstract": "Sequential decision problems are often approximately solvable by simulating possible future action sequences. {\\em Metalevel} decision procedures have been developed for selecting {\\em which} action sequences to simulate, based on estimating the expected improvement in decision quality that would result from any particular simulation; an example is the recent work on using bandit algorithms to control Monte Carlo tree search in the game of Go. In this paper we develop a theoretical basis for metalevel decisions in the statistical framework of Bayesian {\\em selection problems}, arguing (as others have done) that this is more appropriate than the bandit framework. We derive a number of basic results applicable to Monte Carlo selection problems, including the first finite sampling bounds for optimal policies in certain cases; we also provide a simple counterexample to the intuitive conjecture that an optimal policy will necessarily reach a decision in all cases. We then derive heuristic approximations in both Bayesian and distribution-free settings and demonstrate their superiority to bandit-based heuristics in one-shot decision problems and in Go.\n    ",
        "submission_date": "2012-07-25T00:00:00",
        "last_modified_date": "2012-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.5926",
        "title": "Redundant Sudoku Rules",
        "authors": [
            "Bart Demoen",
            "Maria Garcia de la Banda"
        ],
        "abstract": "The rules of Sudoku are often specified using twenty seven \\texttt{all\\_different} constraints, referred to as the {\\em big} \\mrules. Using graphical proofs and exploratory logic programming, the following main and new result is obtained: many subsets of six of these big \\mrules are redundant (i.e., they are entailed by the remaining twenty one \\mrules), and six is maximal (i.e., removing more than six \\mrules is not possible while maintaining equivalence). The corresponding result for binary inequality constraints, referred to as the {\\em small} \\mrules, is stated as a conjecture.\n    ",
        "submission_date": "2012-07-25T00:00:00",
        "last_modified_date": "2012-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.6224",
        "title": "Evolving knowledge through negotiation",
        "authors": [
            "J\u00e9r\u00f4me Euzenat"
        ],
        "abstract": "Semantic web information is at the extremities of long pipelines held by human beings. They are at the origin of information and they will consume it either explicitly because the information will be delivered to them in a readable way, or implicitly because the computer processes consuming this information will affect them. Computers are particularly capable of dealing with information the way it is provided to them. However, people may assign to the information they provide a narrower meaning than semantic technologies may consider. This is typically what happens when people do not think their assertions as ambiguous. Model theory, used to provide semantics to the information on the semantic web, is particularly apt at preserving ambiguity and delivering it to the other side of the pipeline. Indeed, it preserves as much interpretations as possible. This quality for reasoning efficiency, becomes a deficiency for accurate communication and meaning preservation. Overcoming it may require either interactive feedback or preservation of the source context. Work from social science and humanities may help solving this particular problem.\n    ",
        "submission_date": "2012-07-26T00:00:00",
        "last_modified_date": "2012-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.6253",
        "title": "On When and How to use SAT to Mine Frequent Itemsets",
        "authors": [
            "Rui Henriques",
            "In\u00eas Lynce",
            "Vasco Manquinho"
        ],
        "abstract": "A new stream of research was born in the last decade with the goal of mining itemsets of interest using Constraint Programming (CP). This has promoted a natural way to combine complex constraints in a highly flexible manner. Although CP state-of-the-art solutions formulate the task using Boolean variables, the few attempts to adopt propositional Satisfiability (SAT) provided an unsatisfactory performance. This work deepens the study on when and how to use SAT for the frequent itemset mining (FIM) problem by defining different encodings with multiple task-driven enumeration options and search strategies. Although for the majority of the scenarios SAT-based solutions appear to be non-competitive with CP peers, results show a variety of interesting cases where SAT encodings are the best option.\n    ",
        "submission_date": "2012-07-26T00:00:00",
        "last_modified_date": "2012-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.6514",
        "title": "Earthquake Scenario Reduction by Symmetry Reasoning",
        "authors": [
            "Steven Prestwich"
        ],
        "abstract": "A recently identified problem is that of finding an optimal investment plan for a transportation network, given that a disaster such as an earthquake may destroy links in the network. The aim is to strengthen key links to preserve the expected network connectivity. A network based on the Istanbul highway system has thirty links and therefore a billion scenarios, but it has been estimated that sampling a million scenarios gives reasonable accuracy. In this paper we use symmetry reasoning to reduce the number of scenarios to a much smaller number, making sampling unnecessary. This result can be used to facilitate metaheuristic and exact approaches to the problem.\n    ",
        "submission_date": "2012-07-27T00:00:00",
        "last_modified_date": "2012-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.6713",
        "title": "Model-Lite Case-Based Planning",
        "authors": [
            "Hankz Hankui Zhuo",
            "Subbarao Kambhampati",
            "Tuan Nguyen"
        ],
        "abstract": "There is increasing awareness in the planning community that depending on complete models impedes the applicability of planning technology in many real world domains where the burden of specifying complete domain models is too high. In this paper, we consider a novel solution for this challenge that combines generative planning on incomplete domain models with a library of plan cases that are known to be correct. While this was arguably the original motivation for case-based planning, most existing case-based planners assume (and depend on) from-scratch planners that work on complete domain models. In contrast, our approach views the plan generated with respect to the incomplete model as a \"skeletal plan\" and augments it with directed mining of plan fragments from library cases. We will present the details of our approach and present an empirical evaluation of our method in comparison to a state-of-the-art case-based planner that depends on complete domain models.\n    ",
        "submission_date": "2012-07-28T00:00:00",
        "last_modified_date": "2012-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.0293",
        "title": "The Distributed Ontology Language (DOL): Use Cases, Syntax, and Extensibility",
        "authors": [
            "Christoph Lange",
            "Till Mossakowski",
            "Oliver Kutz",
            "Christian Galinski",
            "Michael Gr\u00fcninger",
            "Daniel Couto Vale"
        ],
        "abstract": "The Distributed Ontology Language (DOL) is currently being standardized within the OntoIOp (Ontology Integration and Interoperability) activity of ISO/TC 37/SC 3. It aims at providing a unified framework for (1) ontologies formalized in heterogeneous logics, (2) modular ontologies, (3) links between ontologies, and (4) annotation of ontologies. This paper presents the current state of DOL's standardization. It focuses on use cases where distributed ontologies enable interoperability and reusability. We demonstrate relevant features of the DOL syntax and semantics and explain how these integrate into existing knowledge engineering environments.\n    ",
        "submission_date": "2012-08-01T00:00:00",
        "last_modified_date": "2012-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.1103",
        "title": "System identification and modeling for interacting and non-interacting tank systems using intelligent techniques",
        "authors": [
            "N. S. Bhuvaneswari",
            "R. Praveena",
            "R. Divya"
        ],
        "abstract": "System identification from the experimental data plays a vital role for model based controller design. Derivation of process model from first principles is often difficult due to its complexity. The first stage in the development of any control and monitoring system is the identification and modeling of the system. Each model is developed within the context of a specific control problem. Thus, the need for a general system identification framework is warranted. The proposed framework should be able to adapt and emphasize different properties based on the control objective and the nature of the behavior of the system. Therefore, system identification has been a valuable tool in identifying the model of the system based on the input and output data for the design of the controller. The present work is concerned with the identification of transfer function models using statistical model identification, process reaction curve method, ARX model, genetic algorithm and modeling using neural network and fuzzy logic for interacting and non interacting tank process. The identification technique and modeling used is prone to parameter change & disturbance. The proposed methods are used for identifying the mathematical model and intelligent model of interacting and non interacting process from the real time experimental data.\n    ",
        "submission_date": "2012-08-06T00:00:00",
        "last_modified_date": "2012-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.1136",
        "title": "Credal nets under epistemic irrelevance",
        "authors": [
            "Jasper De Bock",
            "Gert de Cooman"
        ],
        "abstract": "We present a new approach to credal nets, which are graphical models that generalise Bayesian nets to imprecise probability. Instead of applying the commonly used notion of strong independence, we replace it by the weaker notion of epistemic irrelevance. We show how assessments of epistemic irrelevance allow us to construct a global model out of given local uncertainty models and mention some useful properties. The main results and proofs are presented using the language of sets of desirable gambles, which provides a very general and expressive way of representing imprecise probability models.\n    ",
        "submission_date": "2012-08-06T00:00:00",
        "last_modified_date": "2012-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.1743",
        "title": "Hybrid systems modeling for gas transmission network",
        "authors": [
            "Amir Noori",
            "Mohammad Bagher Menhaj",
            "Masoud Shafiee"
        ],
        "abstract": "Gas Transmission Networks are large-scale complex systems, and corresponding design and control problems are challenging. In this paper, we consider the problem of control and management of these systems in crisis situations. We present these networks by a hybrid systems framework that provides required analysis models. Further, we discuss decision-making using computational discrete and hybrid optimization methods. In particular, several reinforcement learning methods are employed to explore decision space and achieve the best policy in a specific crisis situation. Simulations are presented to illustrate the efficiency of the method.\n    ",
        "submission_date": "2012-08-08T00:00:00",
        "last_modified_date": "2012-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.1921",
        "title": "Algorithmic Simplicity and Relevance",
        "authors": [
            "Jean-Louis Dessalles"
        ],
        "abstract": "The human mind is known to be sensitive to complexity. For instance, the visual system reconstructs hidden parts of objects following a principle of maximum simplicity. We suggest here that higher cognitive processes, such as the selection of relevant situations, are sensitive to variations of complexity. Situations are relevant to human beings when they appear simpler to describe than to generate. This definition offers a predictive (i.e. falsifiable) model for the selection of situations worth reporting (interestingness) and for what individuals consider an appropriate move in conversation.\n    ",
        "submission_date": "2012-08-09T00:00:00",
        "last_modified_date": "2012-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.1940",
        "title": "Experiments with Game Tree Search in Real-Time Strategy Games",
        "authors": [
            "Santiago Ontanon"
        ],
        "abstract": "Game tree search algorithms such as minimax have been used with enormous success in turn-based adversarial games such as Chess or Checkers. However, such algorithms cannot be directly applied to real-time strategy (RTS) games because a number of reasons. For example, minimax assumes a turn-taking game mechanics, not present in RTS games. In this paper we present RTMM, a real-time variant of the standard minimax algorithm, and discuss its applicability in the context of RTS games. We discuss its strengths and weaknesses, and evaluate it in two real-time games.\n    ",
        "submission_date": "2012-08-09T00:00:00",
        "last_modified_date": "2012-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.1955",
        "title": "Comparison of different T-norm operators in classification problems",
        "authors": [
            "Fahimeh Farahbod",
            "Mahdi Eftekhari"
        ],
        "abstract": "Fuzzy rule based classification systems are one of the most popular fuzzy modeling systems used in pattern classification problems. This paper investigates the effect of applying nine different T-norms in fuzzy rule based classification systems. In the recent researches, fuzzy versions of confidence and support merits from the field of data mining have been widely used for both rules selecting and weighting in the construction of fuzzy rule based classification systems. For calculating these merits the product has been usually used as a T-norm. In this paper different T-norms have been used for calculating the confidence and support measures. Therefore, the calculations in rule selection and rule weighting steps (in the process of constructing the fuzzy rule based classification systems) are modified by employing these T-norms. Consequently, these changes in calculation results in altering the overall accuracy of rule based classification systems. Experimental results obtained on some well-known data sets show that the best performance is produced by employing the Aczel-Alsina operator in terms of the classification accuracy, the second best operator is Dubois-Prade and the third best operator is Dombi. In experiments, we have used 12 data sets with numerical attributes from the University of California, Irvine machine learning repository (UCI).\n    ",
        "submission_date": "2012-08-09T00:00:00",
        "last_modified_date": "2012-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.2102",
        "title": "A Novel Fuzzy Logic Based Adaptive Supertwisting Sliding Mode Control Algorithm for Dynamic Uncertain Systems",
        "authors": [
            "Abdul Kareem",
            "Mohammad Fazle Azeem"
        ],
        "abstract": "This paper presents a novel fuzzy logic based Adaptive Super-twisting Sliding Mode Controller for the control of dynamic uncertain systems. The proposed controller combines the advantages of Second order Sliding Mode Control, Fuzzy Logic Control and Adaptive Control. The reaching conditions, stability and robustness of the system with the proposed controller are guaranteed. In addition, the proposed controller is well suited for simple design and implementation. The effectiveness of the proposed controller over the first order Sliding Mode Fuzzy Logic controller is illustrated by Matlab based simulations performed on a DC-DC Buck converter. Based on this comparison, the proposed controller is shown to obtain the desired transient response without causing chattering and error under steady-state conditions. The proposed controller is able to give robust performance in terms of rejection to input voltage variations and load variations.\n    ",
        "submission_date": "2012-08-10T00:00:00",
        "last_modified_date": "2012-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.2199",
        "title": "Elimination of ISI Using Improved LMS Based Decision Feedback Equalizer",
        "authors": [
            "Mohammad Havaei",
            "Nandivada Krishna Prasad",
            "Velleshala Sudheer"
        ],
        "abstract": "This paper deals with the implementation of Least Mean Square (LMS) algorithm in Decision Feedback Equalizer (DFE) for removal of Inter Symbol Interference (ISI) at the receiver. The channel disrupts the transmitted signal by spreading it in time. Although, the LMS algorithm is robust and reliable, it is slow in convergence. In order to increase the speed of convergence, modifications have been made in the algorithm where the weights get updated depending on the severity of disturbance.\n    ",
        "submission_date": "2012-08-10T00:00:00",
        "last_modified_date": "2012-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.2362",
        "title": "The Guppy Effect as Interference",
        "authors": [
            "Diederik Aerts",
            "Jan Broekaert",
            "Liane Gabora",
            "Tomas Veloz"
        ],
        "abstract": "People use conjunctions and disjunctions of concepts in ways that violate the rules of classical logic, such as the law of compositionality. Specifically, they overextend conjunctions of concepts, a phenomenon referred to as the Guppy Effect. We build on previous efforts to develop a quantum model that explains the Guppy Effect in terms of interference. Using a well-studied data set with 16 exemplars that exhibit the Guppy Effect, we developed a 17-dimensional complex Hilbert space H that models the data and demonstrates the relationship between overextension and interference. We view the interference effect as, not a logical fallacy on the conjunction, but a signal that out of the two constituent concepts, a new concept has emerged.\n    ",
        "submission_date": "2012-08-11T00:00:00",
        "last_modified_date": "2012-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.2566",
        "title": "The Complexity of Planning Revisited - A Parameterized Analysis",
        "authors": [
            "Christer Baeckstroem",
            "Yue Chen",
            "Peter Jonsson",
            "Sebastian Ordyniak",
            "Stefan Szeider"
        ],
        "abstract": "The early classifications of the computational complexity of planning under various restrictions in STRIPS (Bylander) and SAS+ (Baeckstroem and Nebel) have influenced following research in planning in many ways. We go back and reanalyse their subclasses, but this time using the more modern tool of parameterized complexity analysis. This provides new results that together with the old results give a more detailed picture of the complexity landscape. We demonstrate separation results not possible with standard complexity theory, which contributes to explaining why certain cases of planning have seemed simpler in practice than theory has predicted. In particular, we show that certain restrictions of practical interest are tractable in the parameterized sense of the term, and that a simple heuristic is sufficient to make a well-known partial-order planner exploit this fact.\n    ",
        "submission_date": "2012-08-13T00:00:00",
        "last_modified_date": "2012-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.2852",
        "title": "Ordered {AND, OR}-Decomposition and Binary-Decision Diagram",
        "authors": [
            "Yong Lai",
            "Dayou Liu"
        ],
        "abstract": "In the context of knowledge compilation (KC), we study the effect of augmenting Ordered Binary Decision Diagrams (OBDD) with two kinds of decomposition nodes, i.e., AND-vertices and OR-vertices which denote conjunctive and disjunctive decomposition of propositional knowledge bases, respectively. The resulting knowledge compilation language is called Ordered {AND, OR}-decomposition and binary-Decision Diagram (OAODD). Roughly speaking, several previous languages can be seen as special types of OAODD, including OBDD, AND/OR Binary Decision Diagram (AOBDD), OBDD with implied Literals (OBDD-L), Multi-Level Decomposition Diagrams (MLDD). On the one hand, we propose some families of algorithms which can convert some fragments of OAODD into others; on the other hand, we present a rich set of polynomial-time algorithms that perform logical operations. According to these algorithms, as well as theoretical analysis, we characterize the space efficiency and tractability of OAODD and its some fragments with respect to the evaluating criteria in the KC map. Finally, we present a compilation algorithm which can convert formulas in negative normal form into OAODD.\n    ",
        "submission_date": "2012-08-14T00:00:00",
        "last_modified_date": "2014-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.3015",
        "title": "Explaining Time-Table-Edge-Finding Propagation for the Cumulative Resource Constraint",
        "authors": [
            "Andreas Schutt",
            "Thibaut Feydy",
            "Peter J. Stuckey"
        ],
        "abstract": "Cumulative resource constraints can model scarce resources in scheduling problems or a dimension in packing and cutting problems. In order to efficiently solve such problems with a constraint programming solver, it is important to have strong and fast propagators for cumulative resource constraints. One such propagator is the recently developed time-table-edge-finding propagator, which considers the current resource profile during the edge-finding propagation. Recently, lazy clause generation solvers, i.e. constraint programming solvers incorporating nogood learning, have proved to be excellent at solving scheduling and cutting problems. For such solvers, concise and accurate explanations of the reasons for propagation are essential for strong nogood learning. In this paper, we develop the first explaining version of time-table-edge-finding propagation and show preliminary results on resource-constrained project scheduling problems from various standard benchmark suites. On the standard benchmark suite PSPLib, we were able to close one open instance and to improve the lower bound of about 60% of the remaining open instances. Moreover, 6 of those instances were closed.\n    ",
        "submission_date": "2012-08-15T00:00:00",
        "last_modified_date": "2012-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.3148",
        "title": "Evaluating Ontology Matching Systems on Large, Multilingual and Real-world Test Cases",
        "authors": [
            "Christian Meilicke",
            "Ondrej Sv\u00e1b-Zamazal",
            "C\u00e1ssia Trojahn",
            "Ernesto Jim\u00e9nez-Ruiz",
            "Jos\u00e9-Luis Aguirre",
            "Heiner Stuckenschmidt",
            "Bernardo Cuenca Grau"
        ],
        "abstract": "In the field of ontology matching, the most systematic evaluation of matching systems is established by the Ontology Alignment Evaluation Initiative (OAEI), which is an annual campaign for evaluating ontology matching systems organized by different groups of researchers. In this paper, we report on the results of an intermediary OAEI campaign called OAEI 2011.5. The evaluations of this campaign are divided in five tracks. Three of these tracks are new or have been improved compared to previous OAEI campaigns. Overall, we evaluated 18 matching systems. We discuss lessons learned, in terms of scalability, multilingual issues and the ability do deal with real world cases from different domains.\n    ",
        "submission_date": "2012-08-15T00:00:00",
        "last_modified_date": "2012-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.3600",
        "title": "Modeling and Control of CSTR using Model based Neural Network Predictive Control",
        "authors": [
            "Piyush Shrivastava"
        ],
        "abstract": "This paper presents a predictive control strategy based on neural network model of the plant is applied to Continuous Stirred Tank Reactor (CSTR). This system is a highly nonlinear process; therefore, a nonlinear predictive method, e.g., neural network predictive control, can be a better match to govern the system dynamics. In the paper, the NN model and the way in which it can be used to predict the behavior of the CSTR process over a certain prediction horizon are described, and some comments about the optimization procedure are made. Predictive control algorithm is applied to control the concentration in a continuous stirred tank reactor (CSTR), whose parameters are optimally determined by solving quadratic performance index using the optimization algorithm. An efficient control of the product concentration in cstr can be achieved only through accurate model. Here an attempt is made to alleviate the modeling difficulties using Artificial Intelligent technique such as Neural Network. Simulation results demonstrate the feasibility and effectiveness of the NNMPC technique.\n    ",
        "submission_date": "2012-08-17T00:00:00",
        "last_modified_date": "2012-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.3802",
        "title": "OntoAna: Domain Ontology for Human Anatomy",
        "authors": [
            "Archana Vashisth",
            "Iti Mathur",
            "Nisheeth Joshi"
        ],
        "abstract": "Today, we can find many search engines which provide us with information which is more operational in nature. None of the search engines provide domain specific information. This becomes very troublesome to a novice user who wishes to have information in a particular domain. In this paper, we have developed an ontology which can be used by a domain specific search engine. We have developed an ontology on human anatomy, which captures information regarding cardiovascular system, digestive system, skeleton and nervous system. This information can be used by people working in medical and health care domain.\n    ",
        "submission_date": "2012-08-19T00:00:00",
        "last_modified_date": "2012-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.3809",
        "title": "Lifted Variable Elimination: A Novel Operator and Completeness Results",
        "authors": [
            "Nima Taghipour",
            "Daan Fierens",
            "Guy Van den Broeck",
            "Jesse Davis",
            "Hendrik Blockeel"
        ],
        "abstract": "Various methods for lifted probabilistic inference have been proposed, but our understanding of these methods and the relationships between them is still limited, compared to their propositional counterparts. The only existing theoretical characterization of lifting is for weighted first-order model counting (WFOMC), which was shown to be complete domain-lifted for the class of 2-logvar models. This paper makes two contributions to lifted variable elimination (LVE). First, we introduce a novel inference operator called group inversion. Second, we prove that LVE augmented with this operator is complete in the same sense as WFOMC.\n    ",
        "submission_date": "2012-08-19T00:00:00",
        "last_modified_date": "2012-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.4692",
        "title": "Monte Carlo Search Algorithm Discovery for One Player Games",
        "authors": [
            "Francis Maes",
            "David Lupien St-Pierre",
            "Damien Ernst"
        ],
        "abstract": "Much current research in AI and games is being devoted to Monte Carlo search (MCS) algorithms. While the quest for a single unified MCS algorithm that would perform well on all problems is of major interest for AI, practitioners often know in advance the problem they want to solve, and spend plenty of time exploiting this knowledge to customize their MCS algorithm in a problem-driven way. We propose an MCS algorithm discovery scheme to perform this in an automatic and reproducible way. We first introduce a grammar over MCS algorithms that enables inducing a rich space of candidate algorithms. Afterwards, we search in this space for the algorithm that performs best on average for a given distribution of training problems. We rely on multi-armed bandits to approximately solve this optimization problem. The experiments, generated on three different domains, show that our approach enables discovering algorithms that outperform several well-known MCS algorithms such as Upper Confidence bounds applied to Trees and Nested Monte Carlo search. We also show that the discovered algorithms are generally quite robust with respect to changes in the distribution over the training problems.\n    ",
        "submission_date": "2012-08-23T00:00:00",
        "last_modified_date": "2012-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.4942",
        "title": "A Unifying Survey of Reinforced, Sensitive and Stigmergic Agent-Based Approaches for E-GTSP",
        "authors": [
            "Camelia-M. Pintea"
        ],
        "abstract": "The Generalized Traveling Salesman Problem (GTSP) is one of the NP-hard combinatorial optimization problems. A variant of GTSP is E-GTSP where E, meaning equality, has the constraint: exactly one node from a cluster of a graph partition is visited. The main objective of the E-GTSP is to find a minimum cost tour passing through exactly one node from each cluster of an undirected graph. Agent-based approaches involving are successfully used nowadays for solving real life complex problems. The aim of the current paper is to illustrate some variants of agent-based algorithms including ant-based models with specific properties for solving E-GTSP.\n    ",
        "submission_date": "2012-08-24T00:00:00",
        "last_modified_date": "2014-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.4945",
        "title": "Parallel ACO with a Ring Neighborhood for Dynamic TSP",
        "authors": [
            "Camelia-M. Pintea",
            "Gloria Cerasela Crisan",
            "Mihai Manea"
        ],
        "abstract": "The current paper introduces a new parallel computing technique based on ant colony optimization for a dynamic routing problem. In the dynamic traveling salesman problem the distances between cities as travel times are no longer fixed. The new technique uses a parallel model for a problem variant that allows a slight movement of nodes within their Neighborhoods. The algorithm is tested with success on several large data sets.\n    ",
        "submission_date": "2012-08-24T00:00:00",
        "last_modified_date": "2012-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.5154",
        "title": "Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (2008)",
        "authors": [
            "David McAllester",
            "Petri Myllymaki"
        ],
        "abstract": "This is the Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence, which was held in Helsinki, Finland, July 9 - 12 2008.\n    ",
        "submission_date": "2012-08-25T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.5155",
        "title": "Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence (2007)",
        "authors": [
            "Ronald Parr",
            "Linda S. van der Gaag"
        ],
        "abstract": "This is the Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence, which was held in Vancouver, British Columbia, July 19 - 22 2007.\n    ",
        "submission_date": "2012-08-25T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.5159",
        "title": "Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence (2005)",
        "authors": [
            "Fahiem Bacchus",
            "Tommi Jaakkola"
        ],
        "abstract": "This is the Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence, which was held in Edinburgh, Scotland July 26 - 29 2005.\n    ",
        "submission_date": "2012-08-25T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.5160",
        "title": "Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (2006)",
        "authors": [
            "Rina Dechter",
            "Thomas S. Richardson"
        ],
        "abstract": "This is the Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence, which was held in Cambridge, MA, July 13 - 16 2006.\n    ",
        "submission_date": "2012-08-25T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.5161",
        "title": "Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence (2004)",
        "authors": [
            "Max Chickering",
            "Joseph Halpern"
        ],
        "abstract": "This is the Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence, which was held in Banff, Canada, July 7 - 11 2004.\n    ",
        "submission_date": "2012-08-25T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.5333",
        "title": "A hybrid ACO approach to the Matrix Bandwidth Minimization Problem",
        "authors": [
            "Camelia-M. Pintea",
            "Camelia Chira",
            "Gloria-C. Crisan"
        ],
        "abstract": "The evolution of the human society raises more and more difficult endeavors. For some of the real-life problems, the computing time-restriction enhances their complexity. The Matrix Bandwidth Minimization Problem (MBMP) seeks for a simultaneous permutation of the rows and the columns of a square matrix in order to keep its nonzero entries close to the main diagonal. The MBMP is a highly investigated P-complete problem, as it has broad applications in industry, logistics, artificial intelligence or information recovery. This paper describes a new attempt to use the Ant Colony Optimization framework in tackling MBMP. The introduced model is based on the hybridization of the Ant Colony System technique with new local search mechanisms. Computational experiments confirm a good performance of the proposed algorithm for the considered set of MBMP instances.\n    ",
        "submission_date": "2012-08-27T00:00:00",
        "last_modified_date": "2012-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.5340",
        "title": "New results of ant algorithms for the Linear Ordering Problem",
        "authors": [
            "Camelia-M. Pintea",
            "Camelia Chira",
            "D. Dumitrescu"
        ],
        "abstract": "Ant-based algorithms are successful tools for solving complex problems. One of these problems is the Linear Ordering Problem (LOP). The paper shows new results on some LOP instances, using Ant Colony System (ACS) and the Step-Back Sensitive Ant Model (SB-SAM).\n    ",
        "submission_date": "2012-08-27T00:00:00",
        "last_modified_date": "2012-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.5341",
        "title": "Sensitive Ants in Solving the Generalized Vehicle Routing Problem",
        "authors": [
            "Camelia-M. Pintea",
            "Camelia Chira",
            "D. Dumitrescu",
            "Petrica C. Pop"
        ],
        "abstract": "The idea of sensitivity in ant colony systems has been exploited in hybrid ant-based models with promising results for many combinatorial optimization problems. Heterogeneity is induced in the ant population by endowing individual ants with a certain level of sensitivity to the pheromone trail. The variable pheromone sensitivity within the same population of ants can potentially intensify the search while in the same time inducing diversity for the exploration of the environment. The performance of sensitive ant models is investigated for solving the generalized vehicle routing problem. Numerical results and comparisons are discussed and analysed with a focus on emphasizing any particular aspects and potential benefits related to hybrid ant-based models.\n    ",
        "submission_date": "2012-08-27T00:00:00",
        "last_modified_date": "2012-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.5373",
        "title": "Distributed Pharaoh System for Network Routing",
        "authors": [
            "Camelia-M. Pintea",
            "D. Dumitrescu"
        ],
        "abstract": "In this paper it is introduced a biobjective ant algorithm for constructing low cost routing networks. The new algorithm is called the Distributed Pharaoh System (DPS). DPS is based on AntNet algorithm. The algorithm is using Pharaoh Ant System (PAS) with an extra-exploration phase and a 'no-entry' condition in order to improve the solutions for the Low Cost Network Routing problem. Additionally it is used a cost model for overlay network construction that includes network traffic demands. The Pharaoh ants (Monomorium pharaonis) includes negative pheromones with signals concentrated at decision points where trails fork. The negative pheromones may complement positive pheromone or could help ants to escape from an unnecessarily long route to food that is being reinforced by attractive signals. Numerical experiments were made for a random 10-node network. The average node degree of the network tested was 4.0. The results are encouraging. The algorithm converges to the shortest path while converging on a low cost overlay routing network topology.\n    ",
        "submission_date": "2012-08-27T00:00:00",
        "last_modified_date": "2012-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.5554",
        "title": "Soft Computing approaches on the Bandwidth Problem",
        "authors": [
            "Gabriela Czibula",
            "Gloria Cerasela Crisan",
            "Camelia-M. Pintea",
            "Istvan-Gergely Czibula"
        ],
        "abstract": "The Matrix Bandwidth Minimization Problem (MBMP) seeks for a simultaneous reordering of the rows and the columns of a square matrix such that the nonzero entries are collected within a band of small width close to the main diagonal. The MBMP is a NP-complete problem, with applications in many scientific domains, linear systems, artificial intelligence, and real-life situations in industry, logistics, information recovery. The complex problems are hard to solve, that is why any attempt to improve their solutions is beneficent. Genetic algorithms and ant-based systems are Soft Computing methods used in this paper in order to solve some MBMP instances. Our approach is based on a learning agent-based model involving a local search procedure. The algorithm is compared with the classical Cuthill-McKee algorithm, and with a hybrid genetic algorithm, using several instances from Matrix Market collection. Computational experiments confirm a good performance of the proposed algorithms for the considered set of MBMP instances. On Soft Computing basis, we also propose a new theoretical Reinforcement Learning model for solving the MBMP problem.\n    ",
        "submission_date": "2012-08-28T00:00:00",
        "last_modified_date": "2012-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.0056",
        "title": "Learning implicitly in reasoning in PAC-Semantics",
        "authors": [
            "Brendan Juba"
        ],
        "abstract": "We consider the problem of answering queries about formulas of propositional logic based on background knowledge partially represented explicitly as other formulas, and partially represented as partially obscured examples independently drawn from a fixed probability distribution, where the queries are answered with respect to a weaker semantics than usual -- PAC-Semantics, introduced by Valiant (2000) -- that is defined using the distribution of examples. We describe a fairly general, efficient reduction to limited versions of the decision problem for a proof system (e.g., bounded space treelike resolution, bounded degree polynomial calculus, etc.) from corresponding versions of the reasoning problem where some of the background knowledge is not explicitly given as formulas, only learnable from the examples. Crucially, we do not generate an explicit representation of the knowledge extracted from the examples, and so the \"learning\" of the background knowledge is only done implicitly. As a consequence, this approach can utilize formulas as background knowledge that are not perfectly valid over the distribution---essentially the analogue of agnostic learning here.\n    ",
        "submission_date": "2012-09-01T00:00:00",
        "last_modified_date": "2012-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.0852",
        "title": "Automatic firewall rules generator for anomaly detection systems with Apriori algorithm",
        "authors": [
            "Ehsan Saboori",
            "Shafigh Parsazad",
            "Yasaman Sanatkhani"
        ],
        "abstract": "Network intrusion detection systems have become a crucial issue for computer systems security infrastructures. Different methods and algorithms are developed and proposed in recent years to improve intrusion detection systems. The most important issue in current systems is that they are poor at detecting novel anomaly attacks. These kinds of attacks refer to any action that significantly deviates from the normal behaviour which is considered intrusion. This paper proposed a model to improve this problem based on data mining techniques. Apriori algorithm is used to predict novel attacks and generate real-time rules for firewall. Apriori algorithm extracts interesting correlation relationships among large set of data items. This paper illustrates how to use Apriori algorithm in intrusion detection systems to cerate a automatic firewall rules generator to detect novel anomaly attack. Apriori is the best-known algorithm to mine association rules. This is an innovative way to find association rules on large scale.\n    ",
        "submission_date": "2012-09-05T00:00:00",
        "last_modified_date": "2012-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.0880",
        "title": "On Solving the Oriented Two-Dimensional Bin Packing Problem under Free Guillotine Cutting: Exploiting the Power of Probabilistic Solution Construction",
        "authors": [
            "Christian Blum",
            "Verena Schmid",
            "Lukas Baumgartner"
        ],
        "abstract": "Two-dimensional bin packing problems are highly relevant combinatorial optimization problems. They find a large number of applications, for example, in the context of transportation or warehousing, and for the cutting of different materials such as glass, wood or metal. In this work we deal with the oriented two-dimensional bin packing problem under free guillotine cutting. In this specific problem a set of oriented rectangular items is given which must be packed into a minimum number of bins of equal size. The first algorithm proposed in this work is a randomized multi-start version of a constructive one-pass heuristic from the literature. Additionally we propose the use of this randomized one-pass heuristic within an evolutionary algorithm. The results of the two proposed algorithms are compared to the best approaches from the literature. In particular the evolutionary algorithm compares very favorably to current state-of-the-art approaches. The optimal solution for 4 previously unsolved instances could be found.\n    ",
        "submission_date": "2012-09-05T00:00:00",
        "last_modified_date": "2012-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.0997",
        "title": "Direct computation of diagnoses for ontology debugging",
        "authors": [
            "Kostyantyn Shchekotykhin",
            "Philipp Fleiss",
            "Patrick Rodler",
            "Gerhard Friedrich"
        ],
        "abstract": "Modern ontology debugging methods allow efficient identification and localization of faulty axioms defined by a user while developing an ontology. The ontology development process in this case is characterized by rather frequent and regular calls to a reasoner resulting in an early user awareness of modeling errors. In such a scenario an ontology usually includes only a small number of conflict sets, i.e. sets of axioms preserving the faults. This property allows efficient use of standard model-based diagnosis techniques based on the application of hitting set algorithms to a number of given conflict sets. However, in many use cases such as ontology alignment the ontologies might include many more conflict sets than in usual ontology development settings, thus making precomputation of conflict sets and consequently ontology diagnosis infeasible. In this paper we suggest a debugging approach based on a direct computation of diagnoses that omits calculation of conflict sets. Embedded in an ontology debugger, the proposed algorithm is able to identify diagnoses for an ontology which includes a large number of faults and for which application of standard diagnosis methods fails. The evaluation results show that the approach is practicable and is able to identify a fault in adequate time.\n    ",
        "submission_date": "2012-09-05T00:00:00",
        "last_modified_date": "2012-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.1899",
        "title": "A matrix approach for computing extensions of argumentation frameworks",
        "authors": [
            "Xu Yuming"
        ],
        "abstract": "The matrices and their sub-blocks are introduced into the study of determining various extensions in the sense of Dung's theory of argumentation frameworks. It is showed that each argumentation framework has its matrix representations, and the core semantics defined by Dung can be characterized by specific sub-blocks of the matrix. Furthermore, the elementary permutations of a matrix are employed by which an efficient matrix approach for finding out all extensions under a given semantics is obtained. Different from several established approaches, such as the graph labelling algorithm, Constraint Satisfaction Problem algorithm, the matrix approach not only put the mathematic idea into the investigation for finding out various extensions, but also completely achieve the goal to compute all the extensions needed.\n    ",
        "submission_date": "2012-09-10T00:00:00",
        "last_modified_date": "2012-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.2322",
        "title": "On firm specific characteristics of pharmaceutical generics and incentives to permanence under fuzzy conditions",
        "authors": [
            "Javier Puente",
            "David de la Fuente",
            "Jesus Lozano",
            "Fernando Gascon"
        ],
        "abstract": "The aim of this paper is to develop a methodology that is useful for analysing from a microeconomic perspective the incentives to entry, permanence and exit in the market for pharmaceutical generics under fuzzy conditions. In an empirical application of our proposed methodology, the potential towards permanence of labs with different characteristics has been estimated. The case we deal with is set in an open market where global players diversify into different national markets of pharmaceutical generics. Risk issues are significantly important in deterring decision makers from expanding in the generic pharmaceutical business. However, not all players are affected in the same way and/or to the same extent. Small, non-diversified generics labs are in the worse position. We have highlighted that the expected NPV and the number of generics in the portfolio of a pharmaceutical lab are important variables, but that it is also important to consider the degree of diversification. Labs with a higher potential for diversification across markets have an advantage over smaller labs. We have described a fuzzy decision support system based on the Mamdani model in order to determine the incentives for a laboratory to remain in the market both when it is stable and when it is growing.\n    ",
        "submission_date": "2012-09-11T00:00:00",
        "last_modified_date": "2012-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.3419",
        "title": "Tractable Optimization Problems through Hypergraph-Based Structural Restrictions",
        "authors": [
            "Georg Gottlob",
            "Gianluigi Greco",
            "Francesco Scarcello"
        ],
        "abstract": "Several variants of the Constraint Satisfaction Problem have been proposed and investigated in the literature for modelling those scenarios where solutions are associated with some given costs. Within these frameworks computing an optimal solution is an NP-hard problem in general; yet, when restricted over classes of instances whose constraint interactions can be modelled via (nearly-)acyclic graphs, this problem is known to be solvable in polynomial time. In this paper, larger classes of tractable instances are singled out, by discussing solution approaches based on exploiting hypergraph acyclicity and, more generally, structural decomposition methods, such as (hyper)tree decompositions.\n    ",
        "submission_date": "2012-09-15T00:00:00",
        "last_modified_date": "2012-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.3734",
        "title": "RIO: Minimizing User Interaction in Ontology Debugging",
        "authors": [
            "Patrick Rodler",
            "Kostyantyn Shchekotykhin",
            "Philipp Fleiss",
            "Gerhard Friedrich"
        ],
        "abstract": "Efficient ontology debugging is a cornerstone for many activities in the context of the Semantic Web, especially when automatic tools produce (parts of) ontologies such as in the field of ontology matching. The best currently known interactive debugging systems rely upon some meta information in terms of fault probabilities, which can speed up the debugging procedure in the good case, but can also have negative impact on the performance in the bad case. The problem is that assessment of the meta information is only possible a-posteriori. Consequently, as long as the actual fault is unknown, there is always some risk of suboptimal interactive diagnoses discrimination. As an alternative, one might prefer to rely on a tool which pursues a no-risk strategy. In this case, however, possibly well-chosen meta information cannot be exploited, resulting again in inefficient debugging actions. In this work we present a reinforcement learning strategy that continuously adapts its behavior depending on the performance achieved and minimizes the risk of using low-quality meta information. Therefore, this method is suitable for application scenarios where reliable a-priori fault estimates are difficult to obtain. Using problematic ontologies in the field of ontology matching, we show that the proposed risk-aware query strategy outperforms both active learning approaches and no-risk strategies on average in terms of required amount of user interaction.\n    ",
        "submission_date": "2012-09-17T00:00:00",
        "last_modified_date": "2012-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.3811",
        "title": "Textual Features for Programming by Example",
        "authors": [
            "Aditya Krishna Menon",
            "Omer Tamuz",
            "Sumit Gulwani",
            "Butler Lampson",
            "Adam Tauman Kalai"
        ],
        "abstract": "In Programming by Example, a system attempts to infer a program from input and output examples, generally by searching for a composition of certain base functions. Performing a naive brute force search is infeasible for even mildly involved tasks. We note that the examples themselves often present clues as to which functions to compose, and how to rank the resulting programs. In text processing, which is our domain of interest, clues arise from simple textual features: for example, if parts of the input and output strings are permutations of one another, this suggests that sorting may be useful. We describe a system that learns the reliability of such clues, allowing for faster search and a principled ranking over programs. Experiments on a prototype of this system show that this learning scheme facilitates efficient inference on a range of text processing tasks.\n    ",
        "submission_date": "2012-09-17T00:00:00",
        "last_modified_date": "2012-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.3818",
        "title": "Evolution and the structure of learning agents",
        "authors": [
            "Alok Raj"
        ],
        "abstract": "This paper presents the thesis that all learning agents of finite information size are limited by their informational structure in what goals they can efficiently learn to achieve in a complex environment. Evolutionary change is critical for creating the required structure for all learning agents in any complex environment. The thesis implies that there is no efficient universal learning algorithm. An agent can go past the learning limits imposed by its structure only by slow evolutionary change or blind search which in a very complex environment can only give an agent an inefficient universal learning capability that can work only in evolutionary timescales or improbable luck.\n    ",
        "submission_date": "2012-09-18T00:00:00",
        "last_modified_date": "2013-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.3869",
        "title": "Hybrid technique for effective knowledge representation & a comparative study",
        "authors": [
            "Poonam Tanwar",
            "T. V. Prasad",
            "Dr. Kamlesh Datta"
        ],
        "abstract": "Knowledge representation (KR) and inference mechanism are most desirable thing to make the system intelligent. System is known to an intelligent if its intelligence is equivalent to the intelligence of human being for a particular domain or general. Because of incomplete ambiguous and uncertain information the task of making intelligent system is very difficult. The objective of this paper is to present the hybrid KR technique for making the system effective & Optimistic. The requirement for (effective & optimistic) is because the system must be able to reply the answer with a confidence of some factor. This paper also presents the comparison between various hybrid KR techniques with the proposed one.\n    ",
        "submission_date": "2012-09-18T00:00:00",
        "last_modified_date": "2012-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.3914",
        "title": "Theorem Proving in Large Formal Mathematics as an Emerging AI Field",
        "authors": [
            "Josef Urban",
            "Jiri Vyskocil"
        ],
        "abstract": "In the recent years, we have linked a large corpus of formal mathematics with automated theorem proving (ATP) tools, and started to develop combined AI/ATP systems working in this setting. In this paper we first relate this project to the earlier large-scale automated developments done by Quaife with McCune's Otter system, and to the discussions of the QED project about formalizing a significant part of mathematics. Then we summarize our adventure so far, argue that the QED dreams were right in anticipating the creation of a very interesting semantic AI field, and discuss its further research directions.\n    ",
        "submission_date": "2012-09-18T00:00:00",
        "last_modified_date": "2012-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.4275",
        "title": "Decision-Theoretic Coordination and Control for Active Multi-Camera Surveillance in Uncertain, Partially Observable Environments",
        "authors": [
            "Prabhu Natarajan",
            "Trong Nghia Hoang",
            "Kian Hsiang Low",
            "Mohan Kankanhalli"
        ],
        "abstract": "A central problem of surveillance is to monitor multiple targets moving in a large-scale, obstacle-ridden environment with occlusions. This paper presents a novel principled Partially Observable Markov Decision Process-based approach to coordinating and controlling a network of active cameras for tracking and observing multiple mobile targets at high resolution in such surveillance environments. Our proposed approach is capable of (a) maintaining a belief over the targets' states (i.e., locations, directions, and velocities) to track them, even when they may not be observed directly by the cameras at all times, (b) coordinating the cameras' actions to simultaneously improve the belief over the targets' states and maximize the expected number of targets observed with a guaranteed resolution, and (c) exploiting the inherent structure of our surveillance problem to improve its scalability (i.e., linear time) in the number of targets to be observed. Quantitative comparisons with state-of-the-art multi-camera coordination and control techniques show that our approach can achieve higher surveillance quality in real time. The practical feasibility of our approach is also demonstrated using real AXIS 214 PTZ cameras\n    ",
        "submission_date": "2012-09-19T00:00:00",
        "last_modified_date": "2012-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.4290",
        "title": "Cognitive Bias for Universal Algorithmic Intelligence",
        "authors": [
            "Alexey Potapov",
            "Sergey Rodionov",
            "Andrew Myasnikov",
            "Galymzhan Begimov"
        ],
        "abstract": "Existing theoretical universal algorithmic intelligence models are not practically realizable. More pragmatic approach to artificial general intelligence is based on cognitive architectures, which are, however, non-universal in sense that they can construct and use models of the environment only from Turing-incomplete model spaces. We believe that the way to the real AGI consists in bridging the gap between these two approaches. This is possible if one considers cognitive functions as a \"cognitive bias\" (priors and search heuristics) that should be incorporated into the models of universal algorithmic intelligence without violating their universality. Earlier reported results suiting this approach and its overall feasibility are discussed on the example of perception, planning, knowledge representation, attention, theory of mind, language, and some others.\n    ",
        "submission_date": "2012-09-19T00:00:00",
        "last_modified_date": "2012-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.4330",
        "title": "Modeling and Verification of a Multi-Agent Argumentation System using NuSMV",
        "authors": [
            "Supriya D'Souza",
            "Abhishek Rao",
            "Amit Sharma",
            "Sanjay Singh"
        ],
        "abstract": "Autonomous intelligent agent research is a domain situated at the forefront of artificial intelligence. Interest-based negotiation (IBN) is a form of negotiation in which agents exchange information about their underlying goals, with a view to improve the likelihood and quality of a offer. In this paper we model and verify a multi-agent argumentation scenario of resource sharing mechanism to enable resource sharing in a distributed system. We use IBN in our model wherein agents express their interests to the others in the society to gain certain resources.\n    ",
        "submission_date": "2012-09-19T00:00:00",
        "last_modified_date": "2012-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.4445",
        "title": "Speech Signal Filters based on Soft Computing Techniques: A Comparison",
        "authors": [
            "Sachin Lakra",
            "T.V. Prasad",
            "G. Ramakrishna"
        ],
        "abstract": "The paper presents a comparison of various soft computing techniques used for filtering and enhancing speech signals. The three major techniques that fall under soft computing are neural networks, fuzzy systems and genetic algorithms. Other hybrid techniques such as neuro-fuzzy systems are also available. In general, soft computing techniques have been experimentally observed to give far superior performance as compared to non-soft computing techniques in terms of robustness and accuracy.\n    ",
        "submission_date": "2012-09-20T00:00:00",
        "last_modified_date": "2012-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.4532",
        "title": "Applicability of Crisp and Fuzzy Logic in Intelligent Response Generation",
        "authors": [
            "T.V. Prasad",
            "Sachin Lakra",
            "G. Ramakrishna"
        ],
        "abstract": "This paper discusses the merits and demerits of crisp logic and fuzzy logic with respect to their applicability in intelligent response generation by a human being and by a robot. Intelligent systems must have the capability of taking decisions that are wise and handle situations intelligently. A direct relationship exists between the level of perfection in handling a situation and the level of completeness of the available knowledge or information or data required to handle the situation. The paper concludes that the use of crisp logic with complete knowledge leads to perfection in handling situations whereas fuzzy logic can handle situations imperfectly only. However, in the light of availability of incomplete knowledge fuzzy theory is more effective but may be disadvantageous as compared to crisp logic.\n    ",
        "submission_date": "2012-09-20T00:00:00",
        "last_modified_date": "2012-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.4535",
        "title": "Application of Fuzzy Mathematics to Speech-to-Text Conversion by Elimination of Paralinguistic Content",
        "authors": [
            "Sachin Lakra",
            "T.V. Prasad",
            "Deepak Kumar Sharma",
            "Shree Harsh Atrey",
            "Anubhav Kumar Sharma"
        ],
        "abstract": "For the past few decades, man has been trying to create an intelligent computer which can talk and respond like he can. The task of creating a system that can talk like a human being is the primary objective of Automatic Speech Recognition. Various Speech Recognition techniques have been developed in theory and have been applied in practice. This paper discusses the problems that have been encountered in developing Speech Recognition, the techniques that have been applied to automate the task, and a representation of the core problems of present day Speech Recognition by using Fuzzy Mathematics.\n    ",
        "submission_date": "2012-09-20T00:00:00",
        "last_modified_date": "2012-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.4838",
        "title": "Formal Definition of AI",
        "authors": [
            "Dimiter Dobrev"
        ],
        "abstract": "A definition of Artificial Intelligence was proposed in [1] but this definition was not absolutely formal at least because the word \"Human\" was used. In this paper we will formalize the definition from [1]. The biggest problem in this definition was that the level of intelligence of AI is compared to the intelligence of a human being. In order to change this we will introduce some parameters to which AI will depend. One of this parameters will be the level of intelligence and we will define one AI to each level of intelligence. We assume that for some level of intelligence the respective AI will be more intelligent than a human being. Nevertheless, we cannot say which is this level because we cannot calculate its exact value.\n    ",
        "submission_date": "2012-09-21T00:00:00",
        "last_modified_date": "2012-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.4975",
        "title": "Parametric matroid of rough set",
        "authors": [
            "Yanfang Liu",
            "William Zhu"
        ],
        "abstract": "Rough set is mainly concerned with the approximations of objects through an equivalence relation on a universe. Matroid is a combinatorial generalization of linear independence in vector spaces. In this paper, we define a parametric set family, with any subset of a universe as its parameter, to connect rough sets and matroids. On the one hand, for a universe and an equivalence relation on the universe, a parametric set family is defined through the lower approximation operator. This parametric set family is proved to satisfy the independent set axiom of matroids, therefore it can generate a matroid, called a parametric matroid of the rough set. Three equivalent representations of the parametric set family are obtained. Moreover, the parametric matroid of the rough set is proved to be the direct sum of a partition-circuit matroid and a free matroid. On the other hand, since partition-circuit matroids were well studied through the lower approximation number, we use it to investigate the parametric matroid of the rough set. Several characteristics of the parametric matroid of the rough set, such as independent sets, bases, circuits, the rank function and the closure operator, are expressed by the lower approximation number.\n    ",
        "submission_date": "2012-09-22T00:00:00",
        "last_modified_date": "2012-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.4976",
        "title": "Matroidal structure of rough sets based on serial and transitive relations",
        "authors": [
            "Yanfang Liu",
            "William Zhu"
        ],
        "abstract": "The theory of rough sets is concerned with the lower and upper approximations of objects through a binary relation on a universe. It has been applied to machine learning, knowledge discovery and data mining. The theory of matroids is a generalization of linear independence in vector spaces. It has been used in combinatorial optimization and algorithm design. In order to take advantages of both rough sets and matroids, in this paper we propose a matroidal structure of rough sets based on a serial and transitive relation on a universe. We define the family of all minimal neighborhoods of a relation on a universe, and prove it satisfy the circuit axioms of matroids when the relation is serial and transitive. In order to further study this matroidal structure, we investigate the inverse of this construction: inducing a relation by a matroid. The relationships between the upper approximation operators of rough sets based on relations and the closure operators of matroids in the above two constructions are studied. Moreover, we investigate the connections between the above two constructions.\n    ",
        "submission_date": "2012-09-22T00:00:00",
        "last_modified_date": "2012-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.4978",
        "title": "Covering matroid",
        "authors": [
            "Yanfang Liu",
            "William Zhu"
        ],
        "abstract": "In this paper, we propose a new type of matroids, namely covering matroids, and investigate the connections with the second type of covering-based rough sets and some existing special matroids. Firstly, as an extension of partitions, coverings are more natural combinatorial objects and can sometimes be more efficient to deal with problems in the real world. Through extending partitions to coverings, we propose a new type of matroids called covering matroids and prove them to be an extension of partition matroids. Secondly, since some researchers have successfully applied partition matroids to classical rough sets, we study the relationships between covering matroids and covering-based rough sets which are an extension of classical rough sets. Thirdly, in matroid theory, there are many special matroids, such as transversal matroids, partition matroids, 2-circuit matroid and partition-circuit matroids. The relationships among several special matroids and covering matroids are studied.\n    ",
        "submission_date": "2012-09-22T00:00:00",
        "last_modified_date": "2012-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.5251",
        "title": "On Move Pattern Trends in a Large Go Games Corpus",
        "authors": [
            "Petr Baudi\u0161",
            "Josef Moud\u0159\u00edk"
        ],
        "abstract": "We process a large corpus of game records of the board game of Go and propose a way of extracting summary information on played moves. We then apply several basic data-mining methods on the summary information to identify the most differentiating features within the summary information, and discuss their correspondence with traditional Go knowledge. We show statistically significant mappings of the features to player attributes such as playing strength or informally perceived \"playing style\" (e.g. territoriality or aggressivity), describe accurate classifiers for these attributes, and propose applications including seeding real-work ranks of internet players, aiding in Go study and tuning of Go-playing programs, or contribution to Go-theoretical discussion on the scope of \"playing style\".\n    ",
        "submission_date": "2012-09-24T00:00:00",
        "last_modified_date": "2012-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.5345",
        "title": "Mining Social Data to Extract Intellectual Knowledge",
        "authors": [
            "Muhammad Mahbubur Rahman"
        ],
        "abstract": "Social data mining is an interesting phe-nomenon which colligates different sources of social data to extract information. This information can be used in relationship prediction, decision making, pat-tern recognition, social mapping, responsibility distri-bution and many other applications. This paper presents a systematical data mining architecture to mine intellectual knowledge from social data. In this research, we use social networking site facebook as primary data source. We collect different attributes such as about me, comments, wall post and age from facebook as raw data and use advanced data mining approaches to excavate intellectual knowledge. We also analyze our mined knowledge with comparison for possible usages like as human behavior prediction, pattern recognition, job responsibility distribution, decision making and product promoting.\n    ",
        "submission_date": "2012-09-24T00:00:00",
        "last_modified_date": "2012-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.5456",
        "title": "Relation matroid and its relationship with generalized rough set based on relation",
        "authors": [
            "Yanfang Liu",
            "William Zhu"
        ],
        "abstract": "Recently, the relationship between matroids and generalized rough sets based on relations has been studied from the viewpoint of linear independence of matrices. In this paper, we reveal more relationships by the predecessor and successor neighborhoods from relations. First, through these two neighborhoods, we propose a pair of matroids, namely predecessor relation matroid and successor relation matroid, respectively. Basic characteristics of this pair of matroids, such as dependent sets, circuits, the rank function and the closure operator, are described by the predecessor and successor neighborhoods from relations. Second, we induce a relation from a matroid through the circuits of the matroid. We prove that the induced relation is always an equivalence relation. With these two inductions, a relation induces a relation matroid, and the relation matroid induces an equivalence relation, then the connection between the original relation and the induced equivalence relation is studied. Moreover, the relationships between the upper approximation operator in generalized rough sets and the closure operator in matroids are investigated.\n    ",
        "submission_date": "2012-09-24T00:00:00",
        "last_modified_date": "2012-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.5470",
        "title": "Matroidal structure of generalized rough sets based on symmetric and transitive relations",
        "authors": [
            "Bin Yang",
            "William Zhu"
        ],
        "abstract": "Rough sets are efficient for data pre-process in data mining. Lower and upper approximations are two core concepts of rough sets. This paper studies generalized rough sets based on symmetric and transitive relations from the operator-oriented view by matroidal approaches. We firstly construct a matroidal structure of generalized rough sets based on symmetric and transitive relations, and provide an approach to study the matroid induced by a symmetric and transitive relation. Secondly, this paper establishes a close relationship between matroids and generalized rough sets. Approximation quality and roughness of generalized rough sets can be computed by the circuit of matroid theory. At last, a symmetric and transitive relation can be constructed by a matroid with some special properties.\n    ",
        "submission_date": "2012-09-25T00:00:00",
        "last_modified_date": "2012-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.5473",
        "title": "Some characteristics of matroids through rough sets",
        "authors": [
            "Lirun Su",
            "William Zhu"
        ],
        "abstract": "At present, practical application and theoretical discussion of rough sets are two hot problems in computer science. The core concepts of rough set theory are upper and lower approximation operators based on equivalence relations. Matroid, as a branch of mathematics, is a structure that generalizes linear independence in vector spaces. Further, matroid theory borrows extensively from the terminology of linear algebra and graph theory. We can combine rough set theory with matroid theory through using rough sets to study some characteristics of matroids. In this paper, we apply rough sets to matroids through defining a family of sets which are constructed from the upper approximation operator with respect to an equivalence relation. First, we prove the family of sets satisfies the support set axioms of matroids, and then we obtain a matroid. We say the matroids induced by the equivalence relation and a type of matroid, namely support matroid, is induced. Second, through rough sets, some characteristics of matroids such as independent sets, support sets, bases, hyperplanes and closed sets are investigated.\n    ",
        "submission_date": "2012-09-25T00:00:00",
        "last_modified_date": "2012-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.5480",
        "title": "Condition for neighborhoods in covering based rough sets to form a partition",
        "authors": [
            "Hua Yao",
            "William Zhu"
        ],
        "abstract": "Neighborhood is an important concept in covering based rough sets. That under what condition neighborhoods form a partition is a meaningful issue induced by this concept. Many scholars have paid attention to this issue and presented some necessary and sufficient conditions. However, there exists one common trait among these conditions, that is they are established on the basis of all neighborhoods have been obtained. In this paper, we provide a necessary and sufficient condition directly based on the covering itself. First, we investigate the influence of that there are reducible elements in the covering on neighborhoods. Second, we propose the definition of uniform block and obtain a sufficient condition from it. Third, we propose the definitions of repeat degree and excluded number. By means of the two concepts, we obtain a necessary and sufficient condition for neighborhoods to form a partition. In a word, we have gained a deeper and more direct understanding of the essence over that neighborhoods form a partition.\n    ",
        "submission_date": "2012-09-25T00:00:00",
        "last_modified_date": "2012-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.5482",
        "title": "Rough sets and matroidal contraction",
        "authors": [
            "Jingqian Wang",
            "William Zhu"
        ],
        "abstract": "Rough sets are efficient for data pre-processing in data mining. As a generalization of the linear independence in vector spaces, matroids provide well-established platforms for greedy algorithms. In this paper, we apply rough sets to matroids and study the contraction of the dual of the corresponding matroid. First, for an equivalence relation on a universe, a matroidal structure of the rough set is established through the lower approximation operator. Second, the dual of the matroid and its properties such as independent sets, bases and rank function are investigated. Finally, the relationships between the contraction of the dual matroid to the complement of a single point set and the contraction of the dual matroid to the complement of the equivalence class of this point are studied.\n    ",
        "submission_date": "2012-09-25T00:00:00",
        "last_modified_date": "2012-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.5484",
        "title": "Condition for neighborhoods induced by a covering to be equal to the covering itself",
        "authors": [
            "Hua Yao",
            "William Zhu"
        ],
        "abstract": "It is a meaningful issue that under what condition neighborhoods induced by a covering are equal to the covering itself. A necessary and sufficient condition for this issue has been provided by some scholars. In this paper, through a counter-example, we firstly point out the necessary and sufficient condition is false. Second, we present a necessary and sufficient condition for this issue. Third, we concentrate on the inverse issue of computing neighborhoods by a covering, namely giving an arbitrary covering, whether or not there exists another covering such that the neighborhoods induced by it is just the former covering. We present a necessary and sufficient condition for this issue as well. In a word, through the study on the two fundamental issues induced by neighborhoods, we have gained a deeper understanding of the relationship between neighborhoods and the covering which induce the neighborhoods.\n    ",
        "submission_date": "2012-09-25T00:00:00",
        "last_modified_date": "2012-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.5567",
        "title": "Closed-set lattice of regular sets based on a serial and transitive relation through matroids",
        "authors": [
            "Qingyin Li",
            "William Zhu"
        ],
        "abstract": "Rough sets are efficient for data pre-processing in data mining. Matroids are based on linear algebra and graph theory, and have a variety of applications in many fields. Both rough sets and matroids are closely related to lattices. For a serial and transitive relation on a universe, the collection of all the regular sets of the generalized rough set is a lattice. In this paper, we use the lattice to construct a matroid and then study relationships between the lattice and the closed-set lattice of the matroid. First, the collection of all the regular sets based on a serial and transitive relation is proved to be a semimodular lattice. Then, a matroid is constructed through the height function of the semimodular lattice. Finally, we propose an approach to obtain all the closed sets of the matroid from the semimodular lattice. Borrowing from matroids, results show that lattice theory provides an interesting view to investigate rough sets.\n    ",
        "submission_date": "2012-09-25T00:00:00",
        "last_modified_date": "2013-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.5569",
        "title": "Lattice structures of fixed points of the lower approximations of two types of covering-based rough sets",
        "authors": [
            "Qingyin Li",
            "William Zhu"
        ],
        "abstract": "Covering is a common type of data structure and covering-based rough set theory is an efficient tool to process this data. Lattice is an important algebraic structure and used extensively in investigating some types of generalized rough sets. In this paper, we propose two family of sets and study the conditions that these two sets become some lattice structures. These two sets are consisted by the fixed point of the lower approximations of the first type and the sixth type of covering-based rough sets, respectively. These two sets are called the fixed point set of neighborhoods and the fixed point set of covering, respectively. First, for any covering, the fixed point set of neighborhoods is a complete and distributive lattice, at the same time, it is also a double p-algebra. Especially, when the neighborhood forms a partition of the universe, the fixed point set of neighborhoods is both a boolean lattice and a double Stone algebra. Second, for any covering, the fixed point set of covering is a complete ",
        "submission_date": "2012-09-25T00:00:00",
        "last_modified_date": "2012-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.5601",
        "title": "Feature selection with test cost constraint",
        "authors": [
            "Fan Min",
            "Qinghua Hu",
            "William Zhu"
        ],
        "abstract": "Feature selection is an important preprocessing step in machine learning and data mining. In real-world applications, costs, including money, time and other resources, are required to acquire the features. In some cases, there is a test cost constraint due to limited resources. We shall deliberately select an informative and cheap feature subset for classification. This paper proposes the feature selection with test cost constraint problem for this issue. The new problem has a simple form while described as a constraint satisfaction problem (CSP). Backtracking is a general algorithm for CSP, and it is efficient in solving the new problem on medium-sized data. As the backtracking algorithm is not scalable to large datasets, a heuristic algorithm is also developed. Experimental results show that the heuristic algorithm can find the optimal solution in most cases. We also redefine some existing feature selection problems in rough sets, especially in decision-theoretic rough sets, from the viewpoint of CSP. These new definitions provide insight to some new research directions.\n    ",
        "submission_date": "2012-09-25T00:00:00",
        "last_modified_date": "2012-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.5663",
        "title": "Semi-automatic annotation process for procedural texts: An application on cooking recipes",
        "authors": [
            "Valmi Dufour-Lussier",
            "Florence Le Ber",
            "Jean Lieber",
            "Thomas Meilender",
            "Emmanuel Nauer"
        ],
        "abstract": "Taaable is a case-based reasoning system that adapts cooking recipes to user constraints. Within it, the preparation part of recipes is formalised as a graph. This graph is a semantic representation of the sequence of instructions composing the cooking process and is used to compute the procedure adaptation, conjointly with the textual adaptation. It is composed of cooking actions and ingredients, among others, represented as vertices, and semantic relations between those, shown as arcs, and is built automatically thanks to natural language processing. The results of the automatic annotation process is often a disconnected graph, representing an incomplete annotation, or may contain errors. Therefore, a validating and correcting step is required. In this paper, we present an existing graphic tool named \\kcatos, conceived for representing and editing decision trees, and show how it has been adapted and integrated in WikiTaaable, the semantic wiki in which the knowledge used by Taaable is stored. This interface provides the wiki users with a way to correct the case representation of the cooking process, improving at the same time the quality of the knowledge about cooking procedures stored in WikiTaaable.\n    ",
        "submission_date": "2012-09-25T00:00:00",
        "last_modified_date": "2012-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.5664",
        "title": "Extension du formalisme des flux op\u00e9rationnels par une alg\u00e8bre temporelle",
        "authors": [
            "Valmi Dufour-Lussier",
            "Florence Le Ber",
            "Jean Lieber"
        ],
        "abstract": "Workflows constitute an important language to represent knowledge about processes, but also increasingly to reason on such knowledge. On the other hand, there is a limit to which time constraints between activities can be expressed. Qualitative interval algebras can model processes using finer temporal relations, but they cannot reproduce all workflow patterns. This paper defines a common ground model-theoretical semantics for both workflows and interval algebras, making it possible for reasoning systems working with either to interoperate. Thanks to this, interesting properties and inferences can be defined, both on workflows and on an extended formalism combining workflows with interval algebras. Finally, similar formalisms proposing a sound formal basis for workflows and extending them are discussed.\n    ",
        "submission_date": "2012-09-25T00:00:00",
        "last_modified_date": "2012-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.5853",
        "title": "Efficient Natural Evolution Strategies",
        "authors": [
            "Yi Sun",
            "Daan Wierstra",
            "Tom Schaul",
            "Juergen Schmidhuber"
        ],
        "abstract": "Efficient Natural Evolution Strategies (eNES) is a novel alternative to conventional evolutionary algorithms, using the natural gradient to adapt the mutation distribution. Unlike previous methods based on natural gradients, eNES uses a fast algorithm to calculate the inverse of the exact Fisher information matrix, thus increasing both robustness and performance of its evolution gradient estimation, even in higher dimensions. Additional novel aspects of eNES include optimal fitness baselines and importance mixing (a procedure for updating the population with very few fitness evaluations). The algorithm yields competitive results on both unimodal and multimodal benchmarks.\n    ",
        "submission_date": "2012-09-26T00:00:00",
        "last_modified_date": "2012-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.6195",
        "title": "Examples of Artificial Perceptions in Optical Character Recognition and Iris Recognition",
        "authors": [
            "Cristina M. Noaica",
            "Robert Badea",
            "Iulia M. Motoc",
            "Claudiu G. Ghica",
            "Alin C. Rosoiu",
            "Nicolaie Popescu-Bodorin"
        ],
        "abstract": "This paper assumes the hypothesis that human learning is perception based, and consequently, the learning process and perceptions should not be represented and investigated independently or modeled in different simulation spaces. In order to keep the analogy between the artificial and human learning, the former is assumed here as being based on the artificial perception. Hence, instead of choosing to apply or develop a Computational Theory of (human) Perceptions, we choose to mirror the human perceptions in a numeric (computational) space as artificial perceptions and to analyze the interdependence between artificial learning and artificial perception in the same numeric space, using one of the simplest tools of Artificial Intelligence and Soft Computing, namely the perceptrons. As practical applications, we choose to work around two examples: Optical Character Recognition and Iris Recognition. In both cases a simple Turing test shows that artificial perceptions of the difference between two characters and between two irides are fuzzy, whereas the corresponding human perceptions are, in fact, crisp.\n    ",
        "submission_date": "2012-09-27T00:00:00",
        "last_modified_date": "2012-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.6299",
        "title": "Approximate evaluation of marginal association probabilities with belief propagation",
        "authors": [
            "Jason L. Williams",
            "Roslyn A. Lau"
        ],
        "abstract": "Data association, the problem of reasoning over correspondence between targets and measurements, is a fundamental problem in tracking. This paper presents a graphical model formulation of data association and applies an approximate inference method, belief propagation (BP), to obtain estimates of marginal association probabilities. We prove that BP is guaranteed to converge, and bound the number of iterations necessary. Experiments reveal a favourable comparison to prior methods in terms of accuracy and computational complexity.\n    ",
        "submission_date": "2012-09-12T00:00:00",
        "last_modified_date": "2014-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.6395",
        "title": "Multi-Agents Dynamic Case Based Reasoning and The Inverse Longest Common Sub-Sequence And Individualized Follow-up of Learners in The CEHL",
        "authors": [
            "Abdelhamid Zouhair",
            "El Mokhtar En-Naimi",
            "Benaissa Amami",
            "Hadhoum Boukachour",
            "Patrick Person",
            "Cyrille Bertelle"
        ],
        "abstract": "In E-learning, there is still the problem of knowing how to ensure an individualized and continuous learner's follow-up during learning process, indeed among the numerous tools proposed, very few systems concentrate on a real time learner's follow-up. Our work in this field develops the design and implementation of a Multi-Agents System Based on Dynamic Case Based Reasoning which can initiate learning and provide an individualized follow-up of learner. When interacting with the platform, every learner leaves his/her traces in the machine. These traces are stored in a basis under the form of scenarios which enrich collective past experience. The system monitors, compares and analyses these traces to keep a constant intelligent watch and therefore detect difficulties hindering progress and/or avoid possible dropping out. The system can support any learning subject. The success of a case-based reasoning system depends critically on the performance of the retrieval step used and, more specifically, on similarity measure used to retrieve scenarios that are similar to the course of the learner (traces in progress). We propose a complementary similarity measure, named Inverse Longest Common Sub-Sequence (ILCSS). To help and guide the learner, the system is equipped with combined virtual and human tutors.\n    ",
        "submission_date": "2012-09-27T00:00:00",
        "last_modified_date": "2012-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.6561",
        "title": "Scoring and Searching over Bayesian Networks with Causal and Associative Priors",
        "authors": [
            "Giorgos Borboudakis",
            "Ioannis Tsamardinos"
        ],
        "abstract": "A significant theoretical advantage of search-and-score methods for learning Bayesian Networks is that they can accept informative prior beliefs for each possible network, thus complementing the data. In this paper, a method is presented for assigning priors based on beliefs on the presence or absence of certain paths in the true network. Such beliefs correspond to knowledge about the possible causal and associative relations between pairs of variables. This type of knowledge naturally arises from prior experimental and observational data, among others. In addition, a novel search-operator is proposed to take advantage of such prior knowledge. Experiments show that, using path beliefs improves the learning of the skeleton, as well as the edge directions in the network.\n    ",
        "submission_date": "2012-09-28T00:00:00",
        "last_modified_date": "2013-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.0074",
        "title": "Topological characterizations to three types of covering approximation operators",
        "authors": [
            "Aiping Huang",
            "William Zhu"
        ],
        "abstract": "Covering-based rough set theory is a useful tool to deal with inexact, uncertain or vague knowledge in information systems. Topology, one of the most important subjects in mathematics, provides mathematical tools and interesting topics in studying information systems and rough sets. In this paper, we present the topological characterizations to three types of covering approximation operators. First, we study the properties of topology induced by the sixth type of covering lower approximation operator. Second, some topological characterizations to the covering lower approximation operator to be an interior operator are established. We find that the topologies induced by this operator and by the sixth type of covering lower approximation operator are the same. Third, we study the conditions which make the first type of covering upper approximation operator be a closure operator, and find that the topology induced by the operator is the same as the topology induced by the fifth type of covering upper approximation operator. Forth, the conditions of the second type of covering upper approximation operator to be a closure operator and the properties of topology induced by it are established. Finally, these three topologies space are compared. In a word, topology provides a useful method to study the covering-based rough sets.\n    ",
        "submission_date": "2012-09-29T00:00:00",
        "last_modified_date": "2012-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.0075",
        "title": "Geometric lattice structure of covering-based rough sets through matroids",
        "authors": [
            "Aiping Huang",
            "William Zhu"
        ],
        "abstract": "Covering-based rough set theory is a useful tool to deal with inexact, uncertain or vague knowledge in information systems. Geometric lattice has widely used in diverse fields, especially search algorithm design which plays important role in covering reductions. In this paper, we construct four geometric lattice structures of covering-based rough sets through matroids, and compare their relationships. First, a geometric lattice structure of covering-based rough sets is established through the transversal matroid induced by the covering, and its characteristics including atoms, modular elements and modular pairs are studied. We also construct a one-to-one correspondence between this type of geometric lattices and transversal matroids in the context of covering-based rough sets. Second, sufficient and necessary conditions for three types of covering upper approximation operators to be closure operators of matroids are presented. We exhibit three types of matroids through closure axioms, and then obtain three geometric lattice structures of covering-based rough sets. Third, these four geometric lattice structures are compared. Some core concepts such as reducible elements in covering-based rough sets are investigated with geometric lattices. In a word, this work points out an interesting view, namely geometric lattice, to study covering-based rough sets.\n    ",
        "submission_date": "2012-09-29T00:00:00",
        "last_modified_date": "2012-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.0077",
        "title": "Optimistic Agents are Asymptotically Optimal",
        "authors": [
            "Peter Sunehag",
            "Marcus Hutter"
        ],
        "abstract": "We use optimism to introduce generic asymptotically optimal reinforcement learning agents. They achieve, with an arbitrary finite or compact class of environments, asymptotically optimal behavior. Furthermore, in the finite deterministic case we provide finite error bounds.\n    ",
        "submission_date": "2012-09-29T00:00:00",
        "last_modified_date": "2012-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.0091",
        "title": "Test-cost-sensitive attribute reduction of data with normal distribution measurement errors",
        "authors": [
            "Hong Zhao",
            "Fan Min",
            "William Zhu"
        ],
        "abstract": "The measurement error with normal distribution is universal in applications. Generally, smaller measurement error requires better instrument and higher test cost. In decision making based on attribute values of objects, we shall select an attribute subset with appropriate measurement error to minimize the total test cost. Recently, error-range-based covering rough set with uniform distribution error was proposed to investigate this issue. However, the measurement errors satisfy normal distribution instead of uniform distribution which is rather simple for most applications. In this paper, we introduce normal distribution measurement errors to covering-based rough set model, and deal with test-cost-sensitive attribute reduction problem in this new model. The major contributions of this paper are four-fold. First, we build a new data model based on normal distribution measurement errors. With the new data model, the error range is an ellipse in a two-dimension space. Second, the covering-based rough set with normal distribution measurement errors is constructed through the \"3-sigma\" rule. Third, the test-cost-sensitive attribute reduction problem is redefined on this covering-based rough set. Fourth, a heuristic algorithm is proposed to deal with this problem. The algorithm is tested on ten UCI (University of California - Irvine) datasets. The experimental results show that the algorithm is more effective and efficient than the existing one. This study is a step toward realistic applications of cost-sensitive learning.\n    ",
        "submission_date": "2012-09-29T00:00:00",
        "last_modified_date": "2013-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.0167",
        "title": "Exhaustive Search-based Model for Hybrid Sensor Network",
        "authors": [
            "A.A. Waskita",
            "H. Suhartanto",
            "Z. Akbar",
            "L.T. Handoko"
        ],
        "abstract": "A new model for a cluster of hybrid sensors network with multi sub-clusters is proposed. The model is in particular relevant to the early warning system in a large scale monitoring system in, for example, a nuclear power plant. It mainly addresses to a safety critical system which requires real-time processes with high accuracy. The mathematical model is based on the extended conventional search algorithm with certain interactions among the nearest neighborhood of sensors. It is argued that the model could realize a highly accurate decision support system with less number of parameters. A case of one dimensional interaction function is discussed, and a simple algorithm for the model is also given.\n    ",
        "submission_date": "2012-09-30T00:00:00",
        "last_modified_date": "2012-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.0772",
        "title": "Relationship between the second type of covering-based rough set and matroid via closure operator",
        "authors": [
            "Yanfang Liu",
            "William Zhu"
        ],
        "abstract": "Recently, in order to broad the application and theoretical areas of rough sets and matroids, some authors have combined them from many different viewpoints, such as circuits, rank function, spanning sets and so on. In this paper, we connect the second type of covering-based rough sets and matroids from the view of closure operators. On one hand, we establish a closure system through the fixed point family of the second type of covering lower approximation operator, and then construct a closure operator. For a covering of a universe, the closure operator is a closure one of a matroid if and only if the reduct of the covering is a partition of the universe. On the other hand, we investigate the sufficient and necessary condition that the second type of covering upper approximation operation is a closure one of a matroid.\n    ",
        "submission_date": "2012-10-02T00:00:00",
        "last_modified_date": "2012-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.0794",
        "title": "A Semantic Approach for Automatic Structuring and Analysis of Software Process Patterns",
        "authors": [
            "Nahla Jlaiel",
            "Khouloud Madhbouh",
            "Mohamed Ben Ahmed"
        ],
        "abstract": "The main contribution of this paper, is to propose a novel semantic approach based on a Natural Language Processing technique in order to ensure a semantic unification of unstructured process patterns which are expressed not only in different formats but also, in different forms. This approach is implemented using the GATE text engineering framework and then evaluated leading up to high-quality results motivating us to continue in this direction.\n    ",
        "submission_date": "2012-10-02T00:00:00",
        "last_modified_date": "2012-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.0887",
        "title": "The Definition of AI in Terms of Multi Agent Systems",
        "authors": [
            "Dimiter Dobrev"
        ],
        "abstract": "The questions which we will consider here are \"What is AI?\" and \"How can we make AI?\". Here we will present the definition of AI in terms of multi-agent systems. This means that here you will not find a new answer to the question \"What is AI?\", but an old answer in a new form.\n",
        "submission_date": "2012-10-02T00:00:00",
        "last_modified_date": "2012-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.1568",
        "title": "A Definition of Artificial Intelligence",
        "authors": [
            "Dimiter Dobrev"
        ],
        "abstract": "In this paper we offer a formal definition of Artificial Intelligence and this directly gives us an algorithm for construction of this object. Really, this algorithm is useless due to the combinatory explosion.\n",
        "submission_date": "2012-10-03T00:00:00",
        "last_modified_date": "2012-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.1649",
        "title": "Conflict-driven ASP Solving with External Sources",
        "authors": [
            "Thomas Eiter",
            "Michael Fink",
            "Thomas Krennwallner",
            "Christoph Redl"
        ],
        "abstract": "Answer Set Programming (ASP) is a well-known problem solving approach based on nonmonotonic logic programs and efficient solvers. To enable access to external information, HEX-programs extend programs with external atoms, which allow for a bidirectional communication between the logic program and external sources of computation (e.g., description logic reasoners and Web resources). Current solvers evaluate HEX-programs by a translation to ASP itself, in which values of external atoms are guessed and verified after the ordinary answer set computation. This elegant approach does not scale with the number of external accesses in general, in particular in presence of nondeterminism (which is instrumental for ASP). In this paper, we present a novel, native algorithm for evaluating HEX-programs which uses learning techniques. In particular, we extend conflict-driven ASP solving techniques, which prevent the solver from running into the same conflict again, from ordinary to HEX-programs. We show how to gain additional knowledge from external source evaluations and how to use it in a conflict-driven algorithm. We first target the uninformed case, i.e., when we have no extra information on external sources, and then extend our approach to the case where additional meta-information is available. Experiments show that learning from external sources can significantly decrease both the runtime and the number of considered candidate compatible sets.\n    ",
        "submission_date": "2012-10-05T00:00:00",
        "last_modified_date": "2012-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.1753",
        "title": "Intelligent Search Heuristics for Cost Based Scheduling",
        "authors": [
            "Murphy Choy",
            "Michelle Cheong"
        ],
        "abstract": "Nurse scheduling is a difficult optimization problem with multiple constraints. There is extensive research in the literature solving the problem using meta-heuristics approaches. In this paper, we will investigate an intelligent search heuristics that handles cost based scheduling problem. The heuristics demonstrated superior performances compared to the original algorithms used to solve the problems described in Li et. Al. (2003) and Ozkarahan (1989) in terms of time needed to establish a feasible solution. Both problems can be formulated as a cost problem. The search heuristic consists of several phrases of search and input based on the cost of each assignment and how the assignment will interact with the cost of the resources.\n    ",
        "submission_date": "2012-10-04T00:00:00",
        "last_modified_date": "2012-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.1785",
        "title": "Relative Expressiveness of Defeasible Logics",
        "authors": [
            "Michael Maher"
        ],
        "abstract": "We address the relative expressiveness of defeasible logics in the framework DL. Relative expressiveness is formulated as the ability to simulate the reasoning of one logic within another logic. We show that such simulations must be modular, in the sense that they also work if applied only to part of a theory, in order to achieve a useful notion of relative expressiveness. We present simulations showing that logics in DL with and without the capability of team defeat are equally expressive. We also show that logics that handle ambiguity differently -- ambiguity blocking versus ambiguity propagating -- have distinct expressiveness, with neither able to simulate the other under a different formulation of expressiveness.\n    ",
        "submission_date": "2012-10-05T00:00:00",
        "last_modified_date": "2012-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.1791",
        "title": "An efficient algorithm for estimating state sequences in imprecise hidden Markov models",
        "authors": [
            "Jasper De Bock",
            "Gert de Cooman"
        ],
        "abstract": "We present an efficient exact algorithm for estimating state sequences from outputs (or observations) in imprecise hidden Markov models (iHMM), where both the uncertainty linking one state to the next, and that linking a state to its output, are represented using coherent lower previsions. The notion of independence we associate with the credal network representing the iHMM is that of epistemic irrelevance. We consider as best estimates for state sequences the (Walley--Sen) maximal sequences for the posterior joint state model conditioned on the observed output sequence, associated with a gain function that is the indicator of the state sequence. This corresponds to (and generalises) finding the state sequence with the highest posterior probability in HMMs with precise transition and output probabilities (pHMMs). We argue that the computational complexity is at worst quadratic in the length of the Markov chain, cubic in the number of states, and essentially linear in the number of maximal state sequences. For binary iHMMs, we investigate experimentally how the number of maximal state sequences depends on the model parameters. We also present a simple toy application in optical character recognition, demonstrating that our algorithm can be used to robustify the inferences made by precise probability models.\n    ",
        "submission_date": "2012-10-05T00:00:00",
        "last_modified_date": "2012-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.1931",
        "title": "D-FLAT: Declarative Problem Solving Using Tree Decompositions and Answer-Set Programming",
        "authors": [
            "Bernhard Bliem",
            "Michael Morak",
            "Stefan Woltran"
        ],
        "abstract": "In this work, we propose Answer-Set Programming (ASP) as a tool for rapid prototyping of dynamic programming algorithms based on tree decompositions. In fact, many such algorithms have been designed, but only a few of them found their way into implementation. The main obstacle is the lack of easy-to-use systems which (i) take care of building a tree decomposition and (ii) provide an interface for declarative specifications of dynamic programming algorithms. In this paper, we present D-FLAT, a novel tool that relieves the user of having to handle all the technical details concerned with parsing, tree decomposition, the handling of data structures, etc. Instead, it is only the dynamic programming algorithm itself which has to be specified in the ASP language. D-FLAT employs an ASP solver in order to compute the local solutions in the dynamic programming algorithm. In the paper, we give a few examples illustrating the use of D-FLAT and describe the main features of the system. Moreover, we report experiments which show that ASP-based D-FLAT encodings for some problems outperform monolithic ASP encodings on instances of small treewidth.\n    ",
        "submission_date": "2012-10-06T00:00:00",
        "last_modified_date": "2012-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.2316",
        "title": "Disjunctive Datalog with Existential Quantifiers: Semantics, Decidability, and Complexity Issues",
        "authors": [
            "Mario Alviano",
            "Wolfgang Faber",
            "Nicola Leone",
            "Marco Manna"
        ],
        "abstract": "Datalog is one of the best-known rule-based languages, and extensions of it are used in a wide context of applications. An important Datalog extension is Disjunctive Datalog, which significantly increases the expressivity of the basic language. Disjunctive Datalog is useful in a wide range of applications, ranging from Databases (e.g., Data Integration) to Artificial Intelligence (e.g., diagnosis and planning under incomplete knowledge). However, in recent years an important shortcoming of Datalog-based languages became evident, e.g. in the context of data-integration (consistent query-answering, ontology-based data access) and Semantic Web applications: The language does not permit any generation of and reasoning with unnamed individuals in an obvious way. In general, it is weak in supporting many cases of existential quantification. To overcome this problem, Datalogex has recently been proposed, which extends traditional Datalog by existential quantification in rule heads. In this work, we propose a natural extension of Disjunctive Datalog and Datalogex, called Datalogexor, which allows both disjunctions and existential quantification in rule heads and is therefore an attractive language for knowledge representation and reasoning, especially in domains where ontology-based reasoning is needed. We formally define syntax and semantics of the language Datalogexor, and provide a notion of instantiation, which we prove to be adequate for Datalogexor. A main issue of Datalogex and hence also of Datalogexor is that decidability is no longer guaranteed for typical reasoning tasks. In order to address this issue, we identify many decidable fragments of the language, which extend, in a natural way, analog classes defined in the non-disjunctive case. Moreover, we carry out an in-depth complexity analysis, deriving interesting results which range from Logarithmic Space to Exponential Time.\n    ",
        "submission_date": "2012-10-08T00:00:00",
        "last_modified_date": "2012-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.2715",
        "title": "AI in arbitrary world",
        "authors": [
            "Dimiter Dobrev"
        ],
        "abstract": "In order to build AI we have to create a program which copes well in an arbitrary world. In this paper we will restrict our attention on one concrete world, which represents the game Tick-Tack-Toe. This world is a very simple one but it is sufficiently complicated for our task because most people cannot manage with it. The main difficulty in this world is that the player cannot see the entire internal state of the world so he has to build a model in order to understand the world. The model which we will offer will consist of final automata and first order formulas.\n    ",
        "submission_date": "2012-10-09T00:00:00",
        "last_modified_date": "2012-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.2984",
        "title": "Learning Onto-Relational Rules with Inductive Logic Programming",
        "authors": [
            "Francesca A. Lisi"
        ],
        "abstract": "Rules complement and extend ontologies on the Semantic Web. We refer to these rules as onto-relational since they combine DL-based ontology languages and Knowledge Representation formalisms supporting the relational data model within the tradition of Logic Programming and Deductive Databases. Rule authoring is a very demanding Knowledge Engineering task which can be automated though partially by applying Machine Learning algorithms. In this chapter we show how Inductive Logic Programming (ILP), born at the intersection of Machine Learning and Logic Programming and considered as a major approach to Relational Learning, can be adapted to Onto-Relational Learning. For the sake of illustration, we provide details of a specific Onto-Relational Learning solution to the problem of learning rule-based definitions of DL concepts and roles with ILP.\n    ",
        "submission_date": "2012-10-10T00:00:00",
        "last_modified_date": "2012-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.3241",
        "title": "Distributional Framework for Emergent Knowledge Acquisition and its Application to Automated Document Annotation",
        "authors": [
            "Vit Novacek"
        ],
        "abstract": "The paper introduces a framework for representation and acquisition of knowledge emerging from large samples of textual data. We utilise a tensor-based, distributional representation of simple statements extracted from text, and show how one can use the representation to infer emergent knowledge patterns from the textual data in an unsupervised manner. Examples of the patterns we investigate in the paper are implicit term relationships or conjunctive IF-THEN rules. To evaluate the practical relevance of our approach, we apply it to annotation of life science articles with terms from MeSH (a controlled biomedical vocabulary and thesaurus).\n    ",
        "submission_date": "2012-10-11T00:00:00",
        "last_modified_date": "2012-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.3375",
        "title": "An Agent-based framework for cooperation in Supply Chain",
        "authors": [
            "Benaissa Ezzeddine",
            "Benabdelhafid Abdellatif",
            "Benaissa Mounir"
        ],
        "abstract": "Supply Chain coordination has become a critical success factor for Supply Chain management (SCM) and effectively improving the performance of organizations in various industries. Companies are increasingly located at the intersection of one or more corporate networks which are designated by \"Supply Chain\". Managing this chain is mainly based on an 'information sharing' and redeployment activities between the various links that comprise it. Several attempts have been made by industrialists and researchers to educate policymakers about the gains to be made by the implementation of cooperative relationships. The approach presented in this paper here is among the works that aim to propose solutions related to information systems distributed Supply Chains to enable the different actors of the chain to improve their performance. We propose in particular solutions that focus on cooperation between actors in the Supply Chain.\n    ",
        "submission_date": "2012-10-11T00:00:00",
        "last_modified_date": "2012-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.3946",
        "title": "Local optima networks and the performance of iterated local search",
        "authors": [
            "Fabio Daolio",
            "S\u00e9bastien Verel",
            "Gabriela Ochoa",
            "Marco Tomassini"
        ],
        "abstract": "Local Optima Networks (LONs) have been recently proposed as an alternative model of combinatorial fitness landscapes. The model compresses the information given by the whole search space into a smaller mathematical object that is the graph having as vertices the local optima and as edges the possible weighted transitions between them. A new set of metrics can be derived from this model that capture the distribution and connectivity of the local optima in the underlying configuration space. This paper departs from the descriptive analysis of local optima networks, and actively studies the correlation between network features and the performance of a local search heuristic. The NK family of landscapes and the Iterated Local Search metaheuristic are considered. With a statistically-sound approach based on multiple linear regression, it is shown that some LONs' features strongly influence and can even partly predict the performance of a heuristic search algorithm. This study validates the expressive power of LONs as a model of combinatorial fitness landscapes.\n    ",
        "submission_date": "2012-10-15T00:00:00",
        "last_modified_date": "2012-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4021",
        "title": "Local Optima Networks, Landscape Autocorrelation and Heuristic Search Performance",
        "authors": [
            "Francisco Chicano",
            "Fabio Daolio",
            "Gabriela Ochoa",
            "S\u00e9bastien Verel",
            "Marco Tomassini",
            "Enrique Alba"
        ],
        "abstract": "Recent developments in fitness landscape analysis include the study of Local Optima Networks (LON) and applications of the Elementary Landscapes theory. This paper represents a first step at combining these two tools to explore their ability to forecast the performance of search algorithms. We base our analysis on the Quadratic Assignment Problem (QAP) and conduct a large statistical study over 600 generated instances of different types. Our results reveal interesting links between the network measures, the autocorrelation measures and the performance of heuristic search algorithms.\n    ",
        "submission_date": "2012-10-15T00:00:00",
        "last_modified_date": "2012-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4840",
        "title": "Lifted Relax, Compensate and then Recover: From Approximate to Exact Lifted Probabilistic Inference",
        "authors": [
            "Guy Van den Broeck",
            "Arthur Choi",
            "Adnan Darwiche"
        ],
        "abstract": "We propose an approach to lifted approximate inference for first-order probabilistic models, such as Markov logic networks. It is based on performing exact lifted inference in a simplified first-order model, which is found by relaxing first-order constraints, and then compensating for the relaxation. These simplified models can be incrementally improved by carefully recovering constraints that have been relaxed, also at the first-order level. This leads to a spectrum of approximations, with lifted belief propagation on one end, and exact lifted inference on the other. We discuss how relaxation, compensation, and recovery can be performed, all at the firstorder level, and show empirically that our approach substantially improves on the approximations of both propositional solvers and lifted belief propagation.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4841",
        "title": "An Efficient Message-Passing Algorithm for the M-Best MAP Problem",
        "authors": [
            "Dhruv Batra"
        ],
        "abstract": "Much effort has been directed at algorithms for obtaining the highest probability configuration in a probabilistic random field model known as the maximum a posteriori (MAP) inference problem. In many situations, one could benefit from having not just a single solution, but the top M most probable solutions known as the M-Best MAP problem. In this paper, we propose an efficient message-passing based algorithm for solving the M-Best MAP problem. Specifically, our algorithm solves the recently proposed Linear Programming (LP) formulation of M-Best MAP [7], while being orders of magnitude faster than a generic LP-solver. Our approach relies on studying a particular partial Lagrangian relaxation of the M-Best MAP LP which exposes a natural combinatorial structure of the problem that we exploit.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4842",
        "title": "Causal Inference by Surrogate Experiments: z-Identifiability",
        "authors": [
            "Elias Bareinboim",
            "Judea Pearl"
        ],
        "abstract": "We address the problem of estimating the effect of intervening on a set of variables X from experiments on a different set, Z, that is more accessible to manipulation. This problem, which we call z-identifiability, reduces to ordinary identifiability when Z = empty and, like the latter, can be given syntactic characterization using the do-calculus [Pearl, 1995; 2000]. We provide a graphical necessary and sufficient condition for z-identifiability for arbitrary sets X,Z, and Y (the outcomes). We further develop a complete algorithm for computing the causal effect of X on Y using information provided by experiments on Z. Finally, we use our results to prove completeness of do-calculus relative to z-identifiability, a result that does not follow from completeness relative to ordinary identifiability.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4845",
        "title": "Exploiting Uniform Assignments in First-Order MPE",
        "authors": [
            "Udi Apsel",
            "Ronen I. Brafman"
        ],
        "abstract": "The MPE (Most Probable Explanation) query plays an important role in probabilistic inference. MPE solution algorithms for probabilistic relational models essentially adapt existing belief assessment method, replacing summation with maximization. But the rich structure and symmetries captured by relational models together with the properties of the maximization operator offer an opportunity for additional simplification with potentially significant computational ramifications. Specifically, these models often have groups of variables that define symmetric distributions over some population of formulas. The maximizing choice for different elements of this group is the same. If we can realize this ahead of time, we can significantly reduce the size of the model by eliminating a potentially significant portion of random variables. This paper defines the notion of uniformly assigned and partially uniformly assigned sets of variables, shows how one can recognize these sets efficiently, and how the model can be greatly simplified once we recognize them, with little computational effort. We demonstrate the effectiveness of these ideas empirically on a number of models.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4852",
        "title": "The Do-Calculus Revisited",
        "authors": [
            "Judea Pearl"
        ],
        "abstract": "The do-calculus was developed in 1995 to facilitate the identification of causal effects in non-parametric models. The completeness proofs of [Huang and Valtorta, 2006] and [Shpitser and Pearl, 2006] and the graphical criteria of [Tian and Shpitser, 2010] have laid this identification problem to rest. Recent explorations unveil the usefulness of the do-calculus in three additional areas: mediation analysis [Pearl, 2012], transportability [Pearl and Bareinboim, 2011] and metasynthesis. Meta-synthesis (freshly coined) is the task of fusing empirical results from several diverse studies, conducted on heterogeneous populations and under different conditions, so as to synthesize an estimate of a causal relation in some target environment, potentially different from those under study. The talk surveys these results with emphasis on the challenges posed by meta-synthesis. For background material, see ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4857",
        "title": "Generalized Belief Propagation on Tree Robust Structured Region Graphs",
        "authors": [
            "Andrew E. Gelfand",
            "Max Welling"
        ],
        "abstract": "This paper provides some new guidance in the construction of region graphs for Generalized Belief Propagation (GBP). We connect the problem of choosing the outer regions of a LoopStructured Region Graph (SRG) to that of finding a fundamental cycle basis of the corresponding Markov network. We also define a new class of tree-robust Loop-SRG for which GBP on any induced (spanning) tree of the Markov network, obtained by setting to zero the off-tree interactions, is exact. This class of SRG is then mapped to an equivalent class of tree-robust cycle bases on the Markov network. We show that a treerobust cycle basis can be identified by proving that for every subset of cycles, the graph obtained from the edges that participate in a single cycle only, is multiply connected. Using this we identify two classes of tree-robust cycle bases: planar cycle bases and \"star\" cycle bases. In experiments we show that tree-robustness can be successfully exploited as a design principle to improve the accuracy and convergence of GBP.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4861",
        "title": "Uniform Solution Sampling Using a Constraint Solver As an Oracle",
        "authors": [
            "Stefano Ermon",
            "Carla P. Gomes",
            "Bart Selman"
        ],
        "abstract": "We consider the problem of sampling from solutions defined by a set of hard constraints on a combinatorial space. We propose a new sampling technique that, while enforcing a uniform exploration of the search space, leverages the reasoning power of a systematic constraint solver in a black-box scheme. We present a series of challenging domains, such as energy barriers and highly asymmetric spaces, that reveal the difficulties introduced by hard constraints. We demonstrate that standard approaches such as Simulated Annealing and Gibbs Sampling are greatly affected, while our new technique can overcome many of these difficulties. Finally, we show that our sampling scheme naturally defines a new approximate model counting technique, which we empirically show to be very accurate on a range of benchmark problems.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4865",
        "title": "Scaling Up Decentralized MDPs Through Heuristic Search",
        "authors": [
            "Jilles S. Dibangoye",
            "Christopher Amato",
            "Arnoud Doniec"
        ],
        "abstract": "Decentralized partially observable Markov decision processes (Dec-POMDPs) are rich models for cooperative decision-making under uncertainty, but are often intractable to solve optimally (NEXP-complete). The transition and observation independent Dec-MDP is a general subclass that has been shown to have complexity in NP, but optimal algorithms for this subclass are still inefficient in practice. In this paper, we first provide an updated proof that an optimal policy does not depend on the histories of the agents, but only the local observations. We then present a new algorithm based on heuristic search that is able to expand search nodes by using constraint optimization. We show experimental results comparing our approach with the state-of-the-art DecMDP and Dec-POMDP solvers. These results show a reduction in computation time and an increase in scalability by multiple orders of magnitude in a number of benchmarks.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4866",
        "title": "A Bayesian Approach to Constraint Based Causal Inference",
        "authors": [
            "Tom Claassen",
            "Tom Heskes"
        ],
        "abstract": "We target the problem of accuracy and robustness in causal inference from finite data sets. Some state-of-the-art algorithms produce clear output complete with solid theoretical guarantees but are susceptible to propagating erroneous decisions, while others are very adept at handling and representing uncertainty, but need to rely on undesirable assumptions. Our aim is to combine the inherent robustness of the Bayesian approach with the theoretical strength and clarity of constraint-based methods. We use a Bayesian score to obtain probability estimates on the input statements used in a constraint-based procedure. These are subsequently processed in decreasing order of reliability, letting more reliable decisions take precedence in case of con icts, until a single output model is obtained. Tests show that a basic implementation of the resulting Bayesian Constraint-based Causal Discovery (BCCD) algorithm already outperforms established procedures such as FCI and Conservative PC. It can also indicate which causal decisions in the output have high reliability and which do not.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4870",
        "title": "Crowdsourcing Control: Moving Beyond Multiple Choice",
        "authors": [
            "Christopher H. Lin",
            "Mausam",
            "Daniel Weld"
        ],
        "abstract": "To ensure quality results from crowdsourced tasks, requesters often aggregate worker responses and use one of a plethora of strategies to infer the correct answer from the set of noisy responses. However, all current models assume prior knowledge of all possible outcomes of the task. While not an unreasonable assumption for tasks that can be posited as multiple-choice questions (e.g. n-ary classification), we observe that many tasks do not naturally fit this paradigm, but instead demand a free-response formulation where the outcome space is of infinite size (e.g. audio transcription). We model such tasks with a novel probabilistic graphical model, and design and implement LazySusan, a decision-theoretic controller that dynamically requests responses as necessary in order to infer answers to these tasks. We also design an EM algorithm to jointly learn the parameters of our model while inferring the correct answers to multiple tasks at a time. Live experiments on Amazon Mechanical Turk demonstrate the superiority of LazySusan at solving SAT Math questions, eliminating 83.2% of the error and achieving greater net utility compared to the state-ofthe-art strategy, majority-voting. We also show in live experiments that our EM algorithm outperforms majority-voting on a visualization task that we design.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4874",
        "title": "Dynamic Stochastic Orienteering Problems for Risk-Aware Applications",
        "authors": [
            "Hoong Chuin Lau",
            "William Yeoh",
            "Pradeep Varakantham",
            "Duc Thien Nguyen",
            "Huaxing Chen"
        ],
        "abstract": "Orienteering problems (OPs) are a variant of the well-known prize-collecting traveling salesman problem, where the salesman needs to choose a subset of cities to visit within a given deadline. OPs and their extensions with stochastic travel times (SOPs) have been used to model vehicle routing problems and tourist trip design problems. However, they suffer from two limitations travel times between cities are assumed to be time independent and the route provided is independent of the risk preference (with respect to violating the deadline) of the user. To address these issues, we make the following contributions: We introduce (1) a dynamic SOP (DSOP) model, which is an extension of SOPs with dynamic (time-dependent) travel times; (2) a risk-sensitive criterion to allow for different risk preferences; and (3) a local search algorithm to solve DSOPs with this risk-sensitive criterion. We evaluated our algorithms on a real-world dataset for a theme park navigation problem as well as synthetic datasets employed in the literature.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4875",
        "title": "A Theory of Goal-Oriented MDPs with Dead Ends",
        "authors": [
            "Andrey Kolobov",
            "Mausam",
            "Daniel Weld"
        ],
        "abstract": "Stochastic Shortest Path (SSP) MDPs is a problem class widely studied in AI, especially in probabilistic planning. They describe a wide range of scenarios but make the restrictive assumption that the goal is reachable from any state, i.e., that dead-end states do not exist. Because of this, SSPs are unable to model various scenarios that may have catastrophic events (e.g., an airplane possibly crashing if it flies into a storm). Even though MDP algorithms have been used for solving problems with dead ends, a principled theory of SSP extensions that would allow dead ends, including theoretically sound algorithms for solving such MDPs, has been lacking. In this paper, we propose three new MDP classes that admit dead ends under increasingly weaker assumptions. We present Value Iteration-based as well as the more efficient heuristic search algorithms for optimally solving each class, and explore theoretical relationships between these classes. We also conduct a preliminary empirical study comparing the performance of our algorithms on different MDP classes, especially on scenarios with unavoidable dead ends.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4878",
        "title": "Join-graph based cost-shifting schemes",
        "authors": [
            "Alexander T. Ihler",
            "Natalia Flerova",
            "Rina Dechter",
            "Lars Otten"
        ],
        "abstract": "We develop several algorithms taking advantage of two common approaches for bounding MPE queries in graphical models: minibucket elimination and message-passing updates for linear programming relaxations. Both methods are quite similar, and offer useful perspectives for the other; our hybrid approaches attempt to balance the advantages of each. We demonstrate the power of our hybrid algorithms through extensive empirical evaluation. Most notably, a Branch and Bound search guided by the heuristic function calculated by one of our new algorithms has recently won first place in the PASCAL2 inference challenge.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4880",
        "title": "Inferring Strategies from Limited Reconnaissance in Real-time Strategy Games",
        "authors": [
            "Jesse Hostetler",
            "Ethan W. Dereszynski",
            "Thomas G. Dietterich",
            "Alan Fern"
        ],
        "abstract": "In typical real-time strategy (RTS) games, enemy units are visible only when they are within sight range of a friendly unit. Knowledge of an opponent's disposition is limited to what can be observed through scouting. Information is costly, since units dedicated to scouting are unavailable for other purposes, and the enemy will resist scouting attempts. It is important to infer as much as possible about the opponent's current and future strategy from the available observations. We present a dynamic Bayes net model of strategies in the RTS game Starcraft that combines a generative model of how strategies relate to observable quantities with a principled framework for incorporating evidence gained via scouting. We demonstrate the model's ability to infer unobserved aspects of the game from realistic observations.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4882",
        "title": "A Maximum Likelihood Approach For Selecting Sets of Alternatives",
        "authors": [
            "Ariel D. Procaccia",
            "Sashank J. Reddi",
            "Nisarg Shah"
        ],
        "abstract": "We consider the problem of selecting a subset of alternatives given noisy evaluations of the relative strength of different alternatives. We wish to select a k-subset (for a given k) that provides a maximum likelihood estimate for one of several objectives, e.g., containing the strongest alternative. Although this problem is NP-hard, we show that when the noise level is sufficiently high, intuitive methods provide the optimal solution. We thus generalize classical results about singling out one alternative and identifying the hidden ranking of alternatives by strength. Extensive experiments show that our methods perform well in practical settings.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4885",
        "title": "A Case Study in Complexity Estimation: Towards Parallel Branch-and-Bound over Graphical Models",
        "authors": [
            "Lars Otten",
            "Rina Dechter"
        ],
        "abstract": "We study the problem of complexity estimation in the context of parallelizing an advanced Branch and Bound-type algorithm over graphical models. The algorithm's pruning power makes load balancing, one crucial element of every distributed system, very challenging. We propose using a statistical regression model to identify and tackle disproportionally complex parallel subproblems, the cause of load imbalance, ahead of time. The proposed model is evaluated and analyzed on various levels and shown to yield robust predictions. We then demonstrate its effectiveness for load balancing in practice.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4890",
        "title": "The Complexity of Approximately Solving Influence Diagrams",
        "authors": [
            "Denis D. Maua",
            "Cassio Polpo de Campos",
            "Marco Zaffalon"
        ],
        "abstract": "Influence diagrams allow for intuitive and yet precise description of complex situations involving decision making under uncertainty. Unfortunately, most of the problems described by influence diagrams are hard to solve. In this paper we discuss the complexity of approximately solving influence diagrams. We do not assume no-forgetting or regularity, which makes the class of problems we address very broad. Remarkably, we show that when both the tree-width and the cardinality of the variables are bounded the problem admits a fully polynomial-time approximation scheme.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4894",
        "title": "Heuristic Ranking in Tightly Coupled Probabilistic Description Logics",
        "authors": [
            "Thomas Lukasiewicz",
            "Maria Vanina Martinez",
            "Giorgio Orsi",
            "Gerardo I. Simari"
        ],
        "abstract": "The Semantic Web effort has steadily been gaining traction in the recent years. In particular,Web search companies are recently realizing that their products need to evolve towards having richer semantic search capabilities. Description logics (DLs) have been adopted as the formal underpinnings for Semantic Web languages used in describing ontologies. Reasoning under uncertainty has recently taken a leading role in this arena, given the nature of data found on theWeb. In this paper, we present a probabilistic extension of the DL EL++ (which underlies the OWL2 EL profile) using Markov logic networks (MLNs) as probabilistic semantics. This extension is tightly coupled, meaning that probabilistic annotations in formulas can refer to objects in the ontology. We show that, even though the tightly coupled nature of our language means that many basic operations are data-intractable, we can leverage a sublanguage of MLNs that allows to rank the atomic consequences of an ontology relative to their probability values (called ranking queries) even when these values are not fully computed. We present an anytime algorithm to answer ranking queries, and provide an upper bound on the error that it incurs, as well as a criterion to decide when results are guaranteed to be correct.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4897",
        "title": "Belief Propagation for Structured Decision Making",
        "authors": [
            "Qiang Liu",
            "Alexander T. Ihler"
        ],
        "abstract": "Variational inference algorithms such as belief propagation have had tremendous impact on our ability to learn and use graphical models, and give many insights for developing or understanding exact and approximate inference. However, variational approaches have not been widely adoped for decision making in graphical models, often formulated through influence diagrams and including both centralized and decentralized (or multi-agent) decisions. In this work, we present a general variational framework for solving structured cooperative decision-making problems, use it to propose several belief propagation-like algorithms, and analyze them both theoretically and empirically.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4900",
        "title": "Probability and Asset Updating using Bayesian Networks for Combinatorial Prediction Markets",
        "authors": [
            "Wei Sun",
            "Robin Hanson",
            "Kathryn Blackmond Laskey",
            "Charles Twardy"
        ],
        "abstract": "A market-maker-based prediction market lets forecasters aggregate information by editing a consensus probability distribution either directly or by trading securities that pay off contingent on an event of interest. Combinatorial prediction markets allow trading on any event that can be specified as a combination of a base set of events. However, explicitly representing the full joint distribution is infeasible for markets with more than a few base events. A factored representation such as a Bayesian network (BN) can achieve tractable computation for problems with many related variables. Standard BN inference algorithms, such as the junction tree algorithm, can be used to update a representation of the entire joint distribution given a change to any local conditional probability. However, in order to let traders reuse assets from prior trades while never allowing assets to become negative, a BN based prediction market also needs to update a representation of each user's assets and find the conditional state in which a user has minimum assets. Users also find it useful to see their expected assets given an edit outcome. We show how to generalize the junction tree algorithm to perform all these computations.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4906",
        "title": "Efficient MRF Energy Minimization via Adaptive Diminishing Smoothing",
        "authors": [
            "Bogdan Savchynskyy",
            "Stefan Schmidt",
            "Joerg Kappes",
            "Christoph Schnoerr"
        ],
        "abstract": "We consider the linear programming relaxation of an energy minimization problem for Markov Random Fields. The dual objective of this problem can be treated as a concave and unconstrained, but non-smooth function. The idea of smoothing the objective prior to optimization was recently proposed in a series of papers. Some of them suggested the idea to decrease the amount of smoothing (so called temperature) while getting closer to the optimum. However, no theoretical substantiation was provided. We propose an adaptive smoothing diminishing algorithm based on the duality gap between relaxed primal and dual objectives and demonstrate the efficiency of our approach with a smoothed version of Sequential Tree-Reweighted Message Passing (TRW-S) algorithm. The strategy is applicable to other algorithms as well, avoids adhoc tuning of the smoothing during iterations, and provably guarantees convergence to the optimum.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4907",
        "title": "From imprecise probability assessments to conditional probabilities with quasi additive classes of conditioning events",
        "authors": [
            "Giuseppe Sanfilippo"
        ],
        "abstract": "In this paper, starting from a generalized coherent (i.e. avoiding uniform loss) intervalvalued probability assessment on a finite family of conditional events, we construct conditional probabilities with quasi additive classes of conditioning events which are consistent with the given initial assessment. Quasi additivity assures coherence for the obtained conditional probabilities. In order to reach our goal we define a finite sequence of conditional probabilities by exploiting some theoretical results on g-coherence. In particular, we use solutions of a finite sequence of linear systems.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4910",
        "title": "New Advances and Theoretical Insights into EDML",
        "authors": [
            "Khaled S. Refaat",
            "Arthur Choi",
            "Adnan Darwiche"
        ],
        "abstract": "EDML is a recently proposed algorithm for learning MAP parameters in Bayesian networks. In this paper, we present a number of new advances and insights on the EDML algorithm. First, we provide the multivalued extension of EDML, originally proposed for Bayesian networks over binary variables. Next, we identify a simplified characterization of EDML that further implies a simple fixed-point algorithm for the convex optimization problem that underlies it. This characterization further reveals a connection between EDML and EM: a fixed point of EDML is a fixed point of EM, and vice versa. We thus identify also a new characterization of EM fixed points, but in the semantics of EDML. Finally, we propose a hybrid EDML/EM algorithm that takes advantage of the improved empirical convergence behavior of EDML, while maintaining the monotonic improvement property of EM.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4911",
        "title": "Multi-objective Influence Diagrams",
        "authors": [
            "Radu Marinescu",
            "Abdul Razak",
            "Nic Wilson"
        ],
        "abstract": "We describe multi-objective influence diagrams, based on a set of p objectives, where utility values are vectors in Rp, and are typically only partially ordered. These can still be solved by a variable elimination algorithm, leading to a set of maximal values of expected utility. If the Pareto ordering is used this set can often be prohibitively large. We consider approximate representations of the Pareto set based on e-coverings, allowing much larger problems to be solved. In addition, we define a method for incorporating user tradeoffs, which also greatly improves the efficiency.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4912",
        "title": "FHHOP: A Factored Hybrid Heuristic Online Planning Algorithm for Large POMDPs",
        "authors": [
            "Zhongzhang Zhang",
            "Xiaoping Chen"
        ],
        "abstract": "Planning in partially observable Markov decision processes (POMDPs) remains a challenging topic in the artificial intelligence community, in spite of recent impressive progress in approximation techniques. Previous research has indicated that online planning approaches are promising in handling large-scale POMDP domains efficiently as they make decisions \"on demand\" instead of proactively for the entire state space. We present a Factored Hybrid Heuristic Online Planning (FHHOP) algorithm for large POMDPs. FHHOP gets its power by combining a novel hybrid heuristic search strategy with a recently developed factored state representation. On several benchmark problems, FHHOP substantially outperformed state-of-the-art online heuristic search approaches in terms of both scalability and quality.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4913",
        "title": "An Improved Admissible Heuristic for Learning Optimal Bayesian Networks",
        "authors": [
            "Changhe Yuan",
            "Brandon Malone"
        ],
        "abstract": "Recently two search algorithms, A* and breadth-first branch and bound (BFBnB), were developed based on a simple admissible heuristic for learning Bayesian network structures that optimize a scoring function. The heuristic represents a relaxation of the learning problem such that each variable chooses optimal parents independently. As a result, the heuristic may contain many directed cycles and result in a loose bound. This paper introduces an improved admissible heuristic that tries to avoid directed cycles within small groups of variables. A sparse representation is also introduced to store only the unique optimal parent choices. Empirical results show that the new techniques significantly improved the efficiency and scalability of A* and BFBnB on most of datasets tested in this paper.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4916",
        "title": "A Cluster-Cumulant Expansion at the Fixed Points of Belief Propagation",
        "authors": [
            "Max Welling",
            "Andrew E. Gelfand",
            "Alexander T. Ihler"
        ],
        "abstract": "We introduce a new cluster-cumulant expansion (CCE) based on the fixed points of iterative belief propagation (IBP). This expansion is similar in spirit to the loop-series (LS) recently introduced in [1]. However, in contrast to the latter, the CCE enjoys the following important qualities: 1) it is defined for arbitrary state spaces 2) it is easily extended to fixed points of generalized belief propagation (GBP), 3) disconnected groups of variables will not contribute to the CCE and 4) the accuracy of the expansion empirically improves upon that of the LS. The CCE is based on the same M\u00f6bius transform as the Kikuchi approximation, but unlike GBP does not require storing the beliefs of the GBP-clusters nor does it suffer from convergence issues during belief updating.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.5222",
        "title": "Module Theorem for The General Theory of Stable Models",
        "authors": [
            "Joseph Babb",
            "Joohyung Lee"
        ],
        "abstract": "The module theorem by Janhunen et al. demonstrates how to provide a modular structure in answer set programming, where each module has a well-defined input/output interface which can be used to establish the compositionality of answer sets. The theorem is useful in the analysis of answer set programs, and is a basis of incremental grounding and reactive answer set programming. We extend the module theorem to the general theory of stable models by Ferraris et al. The generalization applies to non-ground logic programs allowing useful constructs in answer set programming, such as choice rules, the count aggregate, and nested expressions. Our extension is based on relating the module theorem to the symmetric splitting theorem by Ferraris et al. Based on this result, we reformulate and extend the theory of incremental answer set computation to a more general class of programs.\n    ",
        "submission_date": "2012-10-18T00:00:00",
        "last_modified_date": "2012-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.5670",
        "title": "Typed Answer Set Programming and Inverse Lambda Algorithms",
        "authors": [
            "Chitta Baral",
            "Juraj Dzifcak",
            "Marcos A. Gonzalez",
            "Aaron Gottesman"
        ],
        "abstract": "Our broader goal is to automatically translate English sentences into formulas in appropriate knowledge representation languages as a step towards understanding and thus answering questions with respect to English text. Our focus in this paper is on the language of Answer Set Programming (ASP). Our approach to translate sentences to ASP rules is inspired by Montague's use of lambda calculus formulas as meaning of words and phrases. With ASP as the target language the meaning of words and phrases are ASP-lambda formulas. In an earlier work we illustrated our approach by manually developing a dictionary of words and their ASP-lambda formulas. However such an approach is not scalable. In this paper our focus is on two algorithms that allow one to construct ASP-lambda formulas in an inverse manner. In particular the two algorithms take as input two lambda-calculus expressions G and H and compute a lambda-calculus expression F such that F with input as G, denoted by F@G, is equal to H; and similarly G@F = H. We present correctness and complexity results about these algorithms. To do that we develop the notion of typed ASP-lambda calculus theories and their orders and use it in developing the completeness results. (To appear in Theory and Practice of Logic Programming.)\n    ",
        "submission_date": "2012-10-21T00:00:00",
        "last_modified_date": "2012-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.6128",
        "title": "Improved Local Search in Artificial Bee Colony using Golden Section Search",
        "authors": [
            "Tarun Kumar Sharma",
            "Millie Pant",
            "V.P.Singh"
        ],
        "abstract": "Artificial bee colony (ABC), an optimization algorithm is a recent addition to the family of population based search algorithm. ABC has taken its inspiration from the collective intelligent foraging behavior of honey bees. In this study we have incorporated golden section search mechanism in the structure of basic ABC to improve the global convergence and prevent to stick on a local solution. The proposed variant is termed as ILS-ABC. Comparative numerical results with the state-of-art algorithms show the performance of the proposal when applied to the set of unconstrained engineering design problems. The simulated results show that the proposed variant can be successfully applied to solve real life problems.\n    ",
        "submission_date": "2012-10-23T00:00:00",
        "last_modified_date": "2012-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.6209",
        "title": "Characteristic of partition-circuit matroid through approximation number",
        "authors": [
            "Yanfang Liu",
            "William Zhu"
        ],
        "abstract": "Rough set theory is a useful tool to deal with uncertain, granular and incomplete knowledge in information systems. And it is based on equivalence relations or partitions. Matroid theory is a structure that generalizes linear independence in vector spaces, and has a variety of applications in many fields. In this paper, we propose a new type of matroids, namely, partition-circuit matroids, which are induced by partitions. Firstly, a partition satisfies circuit axioms in matroid theory, then it can induce a matroid which is called a partition-circuit matroid. A partition and an equivalence relation on the same universe are one-to-one corresponding, then some characteristics of partition-circuit matroids are studied through rough sets. Secondly, similar to the upper approximation number which is proposed by Wang and Zhu, we define the lower approximation number. Some characteristics of partition-circuit matroids and the dual matroids of them are investigated through the lower approximation number and the upper approximation number.\n    ",
        "submission_date": "2012-10-23T00:00:00",
        "last_modified_date": "2012-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.6275",
        "title": "Ambiente de Planejamento Ip\u00ea",
        "authors": [
            "Jo\u00e3o Eugenio Marynowski"
        ],
        "abstract": "In this work we investigate the systems that implements algorithms for the planning problem in Artificial Intelligence, called planners, with especial attention to the planners based on the plan graph. We analyze the problem of comparing the performance of the different algorithms and we propose an environment for the development and analysis of planners.\n    ",
        "submission_date": "2012-10-23T00:00:00",
        "last_modified_date": "2012-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.6415",
        "title": "Lex-Partitioning: A New Option for BDD Search",
        "authors": [
            "Stefan Edelkamp",
            "Peter Kissmann",
            "\u00c1lvaro Torralba"
        ],
        "abstract": "For the exploration of large state spaces, symbolic search using binary decision diagrams (BDDs) can save huge amounts of memory and computation time. State sets are represented and modified by accessing and manipulating their characteristic functions. BDD partitioning is used to compute the image as the disjunction of smaller subimages.\n",
        "submission_date": "2012-10-24T00:00:00",
        "last_modified_date": "2012-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.6855",
        "title": "Asynchronous Decentralized Algorithm for Space-Time Cooperative Pathfinding",
        "authors": [
            "Michal \u010c\u00e1p",
            "Peter Nov\u00e1k",
            "Ji\u0159\u00ed Vok\u0159\u00ednek",
            "Michal P\u011bchou\u010dek"
        ],
        "abstract": "Cooperative pathfinding is a multi-agent path planning problem where a group of vehicles searches for a corresponding set of non-conflicting space-time trajectories. Many of the practical methods for centralized solving of cooperative pathfinding problems are based on the prioritized planning strategy. However, in some domains (e.g., multi-robot teams of unmanned aerial vehicles, autonomous underwater vehicles, or unmanned ground vehicles) a decentralized approach may be more desirable than a centralized one due to communication limitations imposed by the domain and/or privacy concerns.\n",
        "submission_date": "2012-10-25T00:00:00",
        "last_modified_date": "2012-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.7002",
        "title": "A Biomimetic Approach Based on Immune Systems for Classification of Unstructured Data",
        "authors": [
            "Mohamed Hamou",
            "Abdelmalek Amine",
            "Ahmed Chaouki Lokbani"
        ],
        "abstract": "In this paper we present the results of unstructured data clustering in this case a textual data from Reuters 21578 corpus with a new biomimetic approach using immune system. Before experimenting our immune system, we digitalized textual data by the n-grams approach. The novelty lies on hybridization of n-grams and immune systems for clustering. The experimental results show that the recommended ideas are promising and prove that this method can solve the text clustering problem.\n    ",
        "submission_date": "2012-10-25T00:00:00",
        "last_modified_date": "2012-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.7154",
        "title": "Get my pizza right: Repairing missing is-a relations in ALC ontologies (extended version)",
        "authors": [
            "Patrick Lambrix",
            "Zlatan Dragisic",
            "Valentina Ivanova"
        ],
        "abstract": "With the increased use of ontologies in semantically-enabled applications, the issue of debugging defects in ontologies has become increasingly important. These defects can lead to wrong or incomplete results for the applications. Debugging consists of the phases of detection and repairing. In this paper we focus on the repairing phase of a particular kind of defects, i.e. the missing relations in the is-a hierarchy. Previous work has dealt with the case of taxonomies. In this work we extend the scope to deal with ALC ontologies that can be represented using acyclic terminologies. We present algorithms and discuss a system.\n    ",
        "submission_date": "2012-10-26T00:00:00",
        "last_modified_date": "2012-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.7959",
        "title": "Algorithm Selection for Combinatorial Search Problems: A Survey",
        "authors": [
            "Lars Kotthoff"
        ],
        "abstract": "The Algorithm Selection Problem is concerned with selecting the best algorithm to solve a given problem on a case-by-case basis. It has become especially relevant in the last decade, as researchers are increasingly investigating how to identify the most suitable existing algorithm for solving a problem instead of developing new algorithms. This survey presents an overview of this work focusing on the contributions made in the area of combinatorial search problems, where Algorithm Selection techniques have achieved significant performance improvements. We unify and organise the vast literature according to criteria that determine Algorithm Selection systems in practice. The comprehensive classification of approaches identifies and analyses the different directions from which Algorithm Selection has been approached. This paper contrasts and compares different methods for solving the problem as well as ways of using these solutions. It closes by identifying directions of current and future research.\n    ",
        "submission_date": "2012-10-30T00:00:00",
        "last_modified_date": "2012-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.8385",
        "title": "First Experiments with PowerPlay",
        "authors": [
            "Rupesh Kumar Srivastava",
            "Bas R. Steunebrink",
            "J\u00fcrgen Schmidhuber"
        ],
        "abstract": "Like a scientist or a playing child, PowerPlay not only learns new skills to solve given problems, but also invents new interesting problems by itself. By design, it continually comes up with the fastest to find, initially novel, but eventually solvable tasks. It also continually simplifies or compresses or speeds up solutions to previous tasks. Here we describe first experiments with PowerPlay. A self-delimiting recurrent neural network SLIM RNN is used as a general computational problem solving architecture. Its connection weights can encode arbitrary, self-delimiting, halting or non-halting programs affecting both environment (through effectors) and internal states encoding abstractions of event sequences. Our PowerPlay-driven SLIM RNN learns to become an increasingly general solver of self-invented problems, continually adding new problem solving procedures to its growing skill repertoire. Extending a recent conference paper, we identify interesting, emerging, developmental stages of our open-ended system. We also show how it automatically self-modularizes, frequently re-using code for previously invented skills, always trying to invent novel tasks that can be quickly validated because they do not require too many weight changes affecting too many previous tasks.\n    ",
        "submission_date": "2012-10-31T00:00:00",
        "last_modified_date": "2012-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.8442",
        "title": "Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On Boltzmann Machines",
        "authors": [
            "Louis Yuanlong Shao"
        ],
        "abstract": "One conjecture in both deep learning and classical connectionist viewpoint is that the biological brain implements certain kinds of deep networks as its back-end. However, to our knowledge, a detailed correspondence has not yet been set up, which is important if we want to bridge between neuroscience and machine learning. Recent researches emphasized the biological plausibility of Linear-Nonlinear-Poisson (LNP) neuron model. We show that with neurally plausible settings, the whole network is capable of representing any Boltzmann machine and performing a semi-stochastic Bayesian inference algorithm lying between Gibbs sampling and variational inference.\n    ",
        "submission_date": "2012-10-31T00:00:00",
        "last_modified_date": "2013-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.0611",
        "title": "Matrix approach to rough sets through vector matroids over a field",
        "authors": [
            "Aiping Huang",
            "William Zhu"
        ],
        "abstract": "Rough sets were proposed to deal with the vagueness and incompleteness of knowledge in information systems. There are may optimization issues in this field such as attribute reduction. Matroids generalized from matrices are widely used in optimization. Therefore, it is necessary to connect matroids with rough sets. In this paper, we take field into consideration and introduce matrix to study rough sets through vector matroids. First, a matrix representation of an equivalence relation is proposed, and then a matroidal structure of rough sets over a field is presented by the matrix. Second, the properties of the matroidal structure including circuits, bases and so on are studied through two special matrix solution spaces, especially null space. Third, over a binary field, we construct an equivalence relation from matrix null space, and establish an algebra isomorphism from the collection of equivalence relations to the collection of sets, which any member is a family of the minimal non-empty sets that are supports of members of null space of a binary dependence matrix. In a word, matrix provides a new viewpoint to study rough sets.\n    ",
        "submission_date": "2012-11-03T00:00:00",
        "last_modified_date": "2013-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.0749",
        "title": "Student Modeling using Case-Based Reasoning in Conventional Learning System",
        "authors": [
            "Indriana Hidayah",
            "Alvi Syahrina",
            "Adhistya Erna Permanasari"
        ],
        "abstract": "Conventional face-to-face classrooms are still the main learning system applied in Indonesia. In assisting such conventional learning towards an optimal learning, formative evaluations are needed to monitor the progress of the class. This task can be very hard when the size of the class is large. Hence, this research attempted to create a classroom monitoring system based on student data of Department of Electrical Engineering and Information Technology. In order to achieve the goal, a student modeling using Case-Based Reasoning was proposed. A generic student model based on a framework was developed. The model represented student knowledge of a subject. The result showed that the system was able to store and retrieve student data for suggestion of the current situation and formative evaluation for one of the subject in the Department.\n    ",
        "submission_date": "2012-11-05T00:00:00",
        "last_modified_date": "2012-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.0906",
        "title": "Algorithm Runtime Prediction: Methods & Evaluation",
        "authors": [
            "Frank Hutter",
            "Lin Xu",
            "Holger H. Hoos",
            "Kevin Leyton-Brown"
        ],
        "abstract": "Perhaps surprisingly, it is possible to predict how long an algorithm will take to run on a previously unseen input, using machine learning techniques to build a model of the algorithm's runtime as a function of problem-specific instance features. Such models have important applications to algorithm analysis, portfolio-based algorithm selection, and the automatic configuration of parameterized algorithms. Over the past decade, a wide variety of techniques have been studied for building such models. Here, we describe extensions and improvements of existing models, new families of models, and -- perhaps most importantly -- a much more thorough treatment of algorithm parameters as model inputs. We also comprehensively describe new and existing features for predicting algorithm runtime for propositional satisfiability (SAT), travelling salesperson (TSP) and mixed integer programming (MIP) problems. We evaluate these innovations through the largest empirical analysis of its kind, comparing to a wide range of runtime modelling techniques from the literature. Our experiments consider 11 algorithms and 35 instance distributions; they also span a very wide range of SAT, MIP, and TSP instances, with the least structured having been generated uniformly at random and the most structured having emerged from real industrial applications. Overall, we demonstrate that our new models yield substantially better runtime predictions than previous approaches in terms of their generalization to new problem instances, to new algorithms from a parameterized space, and to both simultaneously.\n    ",
        "submission_date": "2012-11-05T00:00:00",
        "last_modified_date": "2013-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.2126",
        "title": "Dynamic Decision Support System Based on Bayesian Networks Application to fight against the Nosocomial Infections",
        "authors": [
            "Hela Ltifi",
            "Ghada Trabelsi",
            "Mounir Ben Ayed",
            "Adel M. Alimi"
        ],
        "abstract": "The improvement of medical care quality is a significant interest for the future years. The fight against nosocomial infections (NI) in the intensive care units (ICU) is a good example. We will focus on a set of observations which reflect the dynamic aspect of the decision, result of the application of a Medical Decision Support System (MDSS). This system has to make dynamic decision on temporal data. We use dynamic Bayesian network (DBN) to model this dynamic process. It is a temporal reasoning within a real-time environment; we are interested in the Dynamic Decision Support Systems in healthcare domain (MDDSS).\n    ",
        "submission_date": "2012-11-09T00:00:00",
        "last_modified_date": "2012-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.2512",
        "title": "Minimal cost feature selection of data with normal distribution measurement errors",
        "authors": [
            "Hong Zhao",
            "Fan Min",
            "William Zhu"
        ],
        "abstract": "Minimal cost feature selection is devoted to obtain a trade-off between test costs and misclassification costs. This issue has been addressed recently on nominal data. In this paper, we consider numerical data with measurement errors and study minimal cost feature selection in this model. First, we build a data model with normal distribution measurement errors. Second, the neighborhood of each data item is constructed through the confidence interval. Comparing with discretized intervals, neighborhoods are more reasonable to maintain the information of data. Third, we define a new minimal total cost feature selection problem through considering the trade-off between test costs and misclassification costs. Fourth, we proposed a backtracking algorithm with three effective pruning techniques to deal with this problem. The algorithm is tested on four UCI data sets. Experimental results indicate that the pruning techniques are effective, and the algorithm is efficient for data sets with nearly one thousand objects.\n    ",
        "submission_date": "2012-11-12T00:00:00",
        "last_modified_date": "2013-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.2719",
        "title": "Quantum Consciousness Soccer Simulator",
        "authors": [
            "N. B\u00e1tfai"
        ],
        "abstract": "In cognitive sciences it is not uncommon to use various games effectively. For example, in artificial intelligence, the RoboCup initiative was to set up to catalyse research on the field of autonomous agent technology. In this paper, we introduce a similar soccer simulation initiative to try to investigate a model of human consciousness and a notion of reality in the form of a cognitive problem. In addition, for example, the home pitch advantage and the objective role of the supporters could be naturally described and discussed in terms of this new soccer simulation model.\n    ",
        "submission_date": "2012-11-12T00:00:00",
        "last_modified_date": "2012-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.2736",
        "title": "Hybrid Systems for Knowledge Representation in Artificial Intelligence",
        "authors": [
            "Rajeswari P. V. N.",
            "T. V. Prasad"
        ],
        "abstract": "There are few knowledge representation (KR) techniques available for efficiently representing knowledge. However, with the increase in complexity, better methods are needed. Some researchers came up with hybrid mechanisms by combining two or more methods. In an effort to construct an intelligent computer system, a primary consideration is to represent large amounts of knowledge in a way that allows effective use and efficiently organizing information to facilitate making the recommended inferences. There are merits and demerits of combinations, and standardized method of KR is needed. In this paper, various hybrid schemes of KR were explored at length and details presented.\n    ",
        "submission_date": "2012-11-12T00:00:00",
        "last_modified_date": "2012-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.2972",
        "title": "Segregating event streams and noise with a Markov renewal process model",
        "authors": [
            "Dan Stowell",
            "Mark D. Plumbley"
        ],
        "abstract": "We describe an inference task in which a set of timestamped event observations must be clustered into an unknown number of temporal sequences with independent and varying rates of observations. Various existing approaches to multi-object tracking assume a fixed number of sources and/or a fixed observation rate; we develop an approach to inferring structure in timestamped data produced by a mixture of an unknown and varying number of similar Markov renewal processes, plus independent clutter noise. The inference simultaneously distinguishes signal from noise as well as clustering signal observations into separate source streams. We illustrate the technique via a synthetic experiment as well as an experiment to track a mixture of singing birds.\n    ",
        "submission_date": "2012-11-13T00:00:00",
        "last_modified_date": "2012-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.3371",
        "title": "A Comparison of Meta-heuristic Search for Interactive Software Design",
        "authors": [
            "C. L. Simons",
            "J. E. Smith"
        ],
        "abstract": "Advances in processing capacity, coupled with the desire to tackle problems where a human subjective judgment plays an important role in determining the value of a proposed solution, has led to a dramatic rise in the number of applications of Interactive Artificial Intelligence. Of particular note is the coupling of meta-heuristic search engines with user-provided evaluation and rating of solutions, usually in the form of Interactive Evolutionary Algorithms (IEAs). These have a well-documented history of successes, but arguably the preponderance of IEAs stems from this history, rather than as a conscious design choice of meta-heuristic based on the characteristics of the problem at hand. This paper sets out to examine the basis for that assumption, taking as a case study the domain of interactive software design. We consider a range of factors that should affect the design choice including ease of use, scalability, and of course, performance, i.e. that ability to generate good solutions within the limited number of evaluations available in interactive work before humans lose focus. We then evaluate three methods, namely greedy local search, an evolutionary algorithm and ant colony optimization, with a variety of representations for candidate solutions. Results show that after suitable parameter tuning, ant colony optimization is highly effective within interactive search and out-performs evolutionary algorithms with respect to increasing numbers of attributes and methods in the software design problem. However, when larger numbers of classes are present in the software design, an evolutionary algorithm using a naive grouping integer-based representation appears more scalable.\n    ",
        "submission_date": "2012-11-14T00:00:00",
        "last_modified_date": "2012-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.3497",
        "title": "Ontology Based Information Extraction for Disease Intelligence",
        "authors": [
            "Prabath Chaminda Abeysiriwardana",
            "Saluka R Kodituwakku"
        ],
        "abstract": "Disease Intelligence (DI) is based on the acquisition and aggregation of fragmented knowledge of diseases at multiple sources all over the world to provide valuable information to doctors, researchers and information seeking community. Some diseases have their own characteristics changed rapidly at different places of the world and are reported on documents as unrelated and heterogeneous information which may be going unnoticed and may not be quickly available. This research presents an Ontology based theoretical framework in the context of medical intelligence and country/region. Ontology is designed for storing information about rapidly spreading and changing diseases with incorporating existing disease taxonomies to genetic information of both humans and infectious organisms. It further maps disease symptoms to diseases and drug effects to disease symptoms. The machine understandable disease ontology represented as a website thus allows the drug effects to be evaluated on disease symptoms and exposes genetic involvements in the human diseases. Infectious agents which have no known place in an existing classification but have data on genetics would still be identified as organisms through the intelligence of this system. It will further facilitate researchers on the subject to try out different solutions for curing diseases.\n    ",
        "submission_date": "2012-11-15T00:00:00",
        "last_modified_date": "2012-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.3882",
        "title": "Gliders2012: Development and Competition Results",
        "authors": [
            "Edward Moore",
            "Oliver Obst",
            "Mikhail Prokopenko",
            "Peter Wang",
            "Jason Held"
        ],
        "abstract": "The RoboCup 2D Simulation League incorporates several challenging features, setting a benchmark for Artificial Intelligence (AI). In this paper we describe some of the ideas and tools around the development of our team, Gliders2012. In our description, we focus on the evaluation function as one of our central mechanisms for action selection. We also point to a new framework for watching log files in a web browser that we release for use and further development by the RoboCup community. Finally, we also summarize results of the group and final matches we played during RoboCup 2012, with Gliders2012 finishing 4th out of 19 teams.\n    ",
        "submission_date": "2012-11-16T00:00:00",
        "last_modified_date": "2012-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.4122",
        "title": "Cost-sensitive C4.5 with post-pruning and competition",
        "authors": [
            "Zilong Xu",
            "Fan Min",
            "William Zhu"
        ],
        "abstract": "Decision tree is an effective classification approach in data mining and machine learning. In applications, test costs and misclassification costs should be considered while inducing decision trees. Recently, some cost-sensitive learning algorithms based on ID3 such as CS-ID3, IDX, \\lambda-ID3 have been proposed to deal with the issue. These algorithms deal with only symbolic data. In this paper, we develop a decision tree algorithm inspired by C4.5 for numeric data. There are two major issues for our algorithm. First, we develop the test cost weighted information gain ratio as the heuristic information. According to this heuristic information, our algorithm is to pick the attribute that provides more gain ratio and costs less for each selection. Second, we design a post-pruning strategy through considering the tradeoff between test costs and misclassification costs of the generated decision tree. In this way, the total cost is reduced. Experimental results indicate that (1) our algorithm is stable and effective; (2) the post-pruning technique reduces the total cost significantly; (3) the competition strategy is effective to obtain a cost-sensitive decision tree with low cost.\n    ",
        "submission_date": "2012-11-17T00:00:00",
        "last_modified_date": "2012-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.4133",
        "title": "A Logic and Adaptive Approach for Efficient Diagnosis Systems using CBR",
        "authors": [
            "Ibrahim El Bitar",
            "Fatima-Zahra Belouadha",
            "Ounsa Roudies"
        ],
        "abstract": "Case Based Reasoning (CBR) is an intelligent way of thinking based on experience and capitalization of already solved cases (source cases) to find a solution to a new problem (target case). Retrieval phase consists on identifying source cases that are similar to the target case. This phase may lead to erroneous results if the existing knowledge imperfections are not taken into account. This work presents a novel solution based on Fuzzy logic techniques and adaptation measures which aggregate weighted similarities to improve the retrieval results. To confirm the efficiency of our solution, we have applied it to the industrial diagnosis domain. The obtained results are more efficient results than those obtained by applying typical measures.\n    ",
        "submission_date": "2012-11-17T00:00:00",
        "last_modified_date": "2012-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.4552",
        "title": "A Dataset for StarCraft AI \\& an Example of Armies Clustering",
        "authors": [
            "Gabriel Synnaeve",
            "Pierre Bessiere"
        ],
        "abstract": "This paper advocates the exploration of the full state of recorded real-time strategy (RTS) games, by human or robotic players, to discover how to reason about tactics and strategy. We present a dataset of StarCraft games encompassing the most of the games' state (not only player's orders). We explain one of the possible usages of this dataset by clustering armies on their compositions. This reduction of armies compositions to mixtures of Gaussian allow for strategic reasoning at the level of the components. We evaluated this clustering method by predicting the outcomes of battles based on armies compositions' mixtures components\n    ",
        "submission_date": "2012-11-19T00:00:00",
        "last_modified_date": "2012-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.4709",
        "title": "A New Similarity Measure for Taxonomy Based on Edge Counting",
        "authors": [
            "Manjula Shenoy.K",
            "K.C.Shet",
            "U.Dinesh Acharya"
        ],
        "abstract": "This paper introduces a new similarity measure based on edge counting in a taxonomy like WorldNet or Ontology. Measurement of similarity between text segments or concepts is very useful for many applications like information retrieval, ontology matching, text mining, and question answering and so on. Several measures have been developed for measuring similarity between two concepts: out of these we see that the measure given by Wu and Palmer [1] is simple, and gives good performance. Our measure is based on their measure but strengthens it. Wu and Palmer [1] measure has a disadvantage that it does not consider how far the concepts are semantically. In our measure we include the shortest path between the concepts and the depth of whole taxonomy together with the distances used in Wu and Palmer [1]. Also the measure has following disadvantage i.e. in some situations, the similarity of two elements of an IS-A ontology contained in the neighborhood exceeds the similarity value of two elements contained in the same hierarchy. Our measure introduces a penalization factor for this case based upon shortest length between the concepts and depth of whole taxonomy.\n    ",
        "submission_date": "2012-11-20T00:00:00",
        "last_modified_date": "2012-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.4957",
        "title": "An Experiment on the Connection between the DLs' Family DL<ForAllPiZero> and the Real World",
        "authors": [
            "Antonio Pisasale",
            "Domenico Cantone"
        ],
        "abstract": "This paper describes the analysis of a selected testbed of Semantic Web ontologies, by a SPARQL query, which determines those ontologies that can be related to the description logic DL<ForAllPiZero>, introduced in [4] and studied in [9]. We will see that a reasonable number of them is expressible within such computationally efficient language. We expect that, in a long-term view, a temporalization of description logics, and consequently, of OWL(2), can open new perspectives for the inclusion in this language of a greater number of ontologies of the testbed and, hopefully, of the \"real world\".\n    ",
        "submission_date": "2012-11-21T00:00:00",
        "last_modified_date": "2012-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.5189",
        "title": "Optimally fuzzy temporal memory",
        "authors": [
            "Karthik H. Shankar",
            "Marc W. Howard"
        ],
        "abstract": "Any learner with the ability to predict the future of a structured time-varying signal must maintain a memory of the recent past. If the signal has a characteristic timescale relevant to future prediction, the memory can be a simple shift register---a moving window extending into the past, requiring storage resources that linearly grows with the timescale to be represented. However, an independent general purpose learner cannot a priori know the characteristic prediction-relevant timescale of the signal. Moreover, many naturally occurring signals show scale-free long range correlations implying that the natural prediction-relevant timescale is essentially unbounded. Hence the learner should maintain information from the longest possible timescale allowed by resource availability. Here we construct a fuzzy memory system that optimally sacrifices the temporal accuracy of information in a scale-free fashion in order to represent prediction-relevant information from exponentially long timescales. Using several illustrative examples, we demonstrate the advantage of the fuzzy memory system over a shift register in time series forecasting of natural signals. When the available storage resources are limited, we suggest that a general purpose learner would be better off committing to such a fuzzy memory system.\n    ",
        "submission_date": "2012-11-22T00:00:00",
        "last_modified_date": "2013-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.5643",
        "title": "Shadows and headless shadows: a worlds-based, autobiographical approach to reasoning",
        "authors": [
            "Ladislau Boloni"
        ],
        "abstract": "Many cognitive systems deploy multiple, closed, individually consistent models which can represent interpretations of the present state of the world, moments in the past, possible futures or alternate versions of reality. While they appear under different names, these structures can be grouped under the general term of worlds. The Xapagy architecture is a story-oriented cognitive system which relies exclusively on the autobiographical memory implemented as a raw collection of events organized into world-type structures called {\\em scenes}. The system performs reasoning by shadowing current events with events from the autobiography. The shadows are then extrapolated into headless shadows corresponding to predictions, hidden events or inferred relations.\n    ",
        "submission_date": "2012-11-24T00:00:00",
        "last_modified_date": "2012-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.5644",
        "title": "Modeling problems of identity in Little Red Riding Hood",
        "authors": [
            "Ladislau Boloni"
        ],
        "abstract": "This paper argues that the problem of identity is a critical challenge in agents which are able to reason about stories. The Xapagy architecture has been built from scratch to perform narrative reasoning and relies on a somewhat unusual approach to represent instances and identity. We illustrate the approach by a representation of the story of Little Red Riding Hood in the architecture, with a focus on the problem of identity raised by the narrative.\n    ",
        "submission_date": "2012-11-24T00:00:00",
        "last_modified_date": "2012-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.5766",
        "title": "Visualization and clustering by 3D cellular automata: Application to unstructured data",
        "authors": [
            "Reda Mohamed Hamou",
            "Abdelmalek Amine",
            "Ahmed Chaouki Lokbani",
            "Michel Simonet"
        ],
        "abstract": "Given the limited performance of 2D cellular automata in terms of space when the number of documents increases and in terms of visualization clusters, our motivation was to experiment these cellular automata by increasing the size to view the impact of size on quality of results. The representation of textual data was carried out by a vector model whose components are derived from the overall balancing of the used corpus, Term Frequency Inverse Document Frequency (TF-IDF). The WorldNet thesaurus has been used to address the problem of the lemmatization of the words because the representation used in this study is that of the bags of words. Another independent method of the language was used to represent textual records is that of the n-grams. Several measures of similarity have been tested. To validate the classification we have used two measures of assessment based on the recall and precision (f-measure and entropy). The results are promising and confirm the idea to increase the dimension to the problem of the spatiality of the classes. The results obtained in terms of purity class (i.e. the minimum value of entropy) shows that the number of documents over longer believes the results are better for 3D cellular automata, which was not obvious to the 2D dimension. In terms of spatial navigation, cellular automata provide very good 3D performance visualization than 2D cellular automata.\n    ",
        "submission_date": "2012-11-25T00:00:00",
        "last_modified_date": "2012-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.5829",
        "title": "An Automatic Algorithm for Object Recognition and Detection Based on ASIFT Keypoints",
        "authors": [
            "Reza Oji"
        ],
        "abstract": "Object recognition is an important task in image processing and computer vision. This paper presents a perfect method for object recognition with full boundary detection by combining affine scale invariant feature transform (ASIFT) and a region merging algorithm. ASIFT is a fully affine invariant algorithm that means features are invariant to six affine parameters namely translation (2 parameters), zoom, rotation and two camera axis orientations. The features are very reliable and give us strong keypoints that can be used for matching between different images of an object. We trained an object in several images with different aspects for finding best keypoints of it. Then, a robust region merging algorithm is used to recognize and detect the object with full boundary in the other images based on ASIFT keypoints and a similarity measure for merging regions in the image. Experimental results show that the presented method is very efficient and powerful to recognize the object and detect it with high accuracy.\n    ",
        "submission_date": "2012-11-26T00:00:00",
        "last_modified_date": "2012-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.6097",
        "title": "Shadows and Headless Shadows: an Autobiographical Approach to Narrative Reasoning",
        "authors": [
            "Ladislau Boloni"
        ],
        "abstract": "The Xapagy architecture is a story-oriented cognitive system which relies exclusively on the autobiographical memory implemented as a raw collection of events. Reasoning is performed by shadowing current events with events from the autobiography. The shadows are then extrapolated into headless shadows (HLSs). In a story following mood, HLSs can be used to track the level of surprise of the agent, to infer hidden actions or relations between the participants, and to summarize ongoing events. In recall mood, the HLSs can be used to create new stories ranging from exact recall to free-form confabulation.\n    ",
        "submission_date": "2012-11-24T00:00:00",
        "last_modified_date": "2012-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.6409",
        "title": "Obesity Heuristic, New Way On Artificial Immune Systems",
        "authors": [
            "Mohammed El-Dosuky",
            "Ahmed EL-Bassiouny",
            "Taher Hamza",
            "Magdy Rashad"
        ],
        "abstract": "There is a need for new metaphors from immunology to flourish the application areas of Artificial Immune Systems. A metaheuristic called Obesity Heuristic derived from advances in obesity treatment is proposed. The main forces of the algorithm are the generation omega-6 and omega-3 fatty acids. The algorithm works with Just-In-Time philosophy; by starting only when desired. A case study of data cleaning is provided. With experiments conducted on standard tables, results show that Obesity Heuristic outperforms other algorithms, with 100% recall. This is a great improvement over other algorithms\n    ",
        "submission_date": "2012-11-24T00:00:00",
        "last_modified_date": "2012-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.6727",
        "title": "Graph Laplacians on Singular Manifolds: Toward understanding complex spaces: graph Laplacians on manifolds with singularities and boundaries",
        "authors": [
            "Mikhail Belkin",
            "Qichao Que",
            "Yusu Wang",
            "Xueyuan Zhou"
        ],
        "abstract": "Recently, much of the existing work in manifold learning has been done under the assumption that the data is sampled from a manifold without boundaries and singularities or that the functions of interest are evaluated away from such points. At the same time, it can be argued that singularities and boundaries are an important aspect of the geometry of realistic data.\n",
        "submission_date": "2012-11-28T00:00:00",
        "last_modified_date": "2012-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.7012",
        "title": "Learning-Assisted Automated Reasoning with Flyspeck",
        "authors": [
            "Cezary Kaliszyk",
            "Josef Urban"
        ],
        "abstract": "The considerable mathematical knowledge encoded by the Flyspeck project is combined with external automated theorem provers (ATPs) and machine-learning premise selection methods trained on the proofs, producing an AI system capable of answering a wide range of mathematical queries automatically. The performance of this architecture is evaluated in a bootstrapping scenario emulating the development of Flyspeck from axioms to the last theorem, each time using only the previous theorems and proofs. It is shown that 39% of the 14185 theorems could be proved in a push-button mode (without any high-level advice and user interaction) in 30 seconds of real time on a fourteen-CPU workstation. The necessary work involves: (i) an implementation of sound translations of the HOL Light logic to ATP formalisms: untyped first-order, polymorphic typed first-order, and typed higher-order, (ii) export of the dependency information from HOL Light and ATP proofs for the machine learners, and (iii) choice of suitable representations and methods for learning from previous proofs, and their integration as advisors with HOL Light. This work is described and discussed here, and an initial analysis of the body of proofs that were found fully automatically is provided.\n    ",
        "submission_date": "2012-11-29T00:00:00",
        "last_modified_date": "2014-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.0229",
        "title": "Simplification and integration in computing and cognition: the SP theory and the multiple alignment concept",
        "authors": [
            "James Gerard Wolff"
        ],
        "abstract": "The main purpose of this article is to describe potential benefits and applications of the SP theory, a unique attempt to simplify and integrate ideas across artificial intelligence, mainstream computing and human cognition, with information compression as a unifying theme. The theory, including a concept of multiple alignment, combines conceptual simplicity with descriptive and explanatory power in several areas including representation of knowledge, natural language processing, pattern recognition, several kinds of reasoning, the storage and retrieval of information, planning and problem solving, unsupervised learning, information compression, and human perception and cognition. In the SP machine -- an expression of the SP theory which is currently realised in the form of computer models -- there is potential for an overall simplification of computing systems, including software. As a theory with a broad base of support, the SP theory promises useful insights in many areas and the integration of structures and functions, both within a given area and amongst different areas. There are potential benefits in natural language processing (with potential for the understanding and translation of natural languages), the need for a versatile intelligence in autonomous robots, computer vision, intelligent databases, maintaining multiple versions of documents or web pages, software engineering, criminal investigations, the management of big data and gaining benefits from it, the semantic web, medical diagnosis, the detection of computer viruses, the economical transmission of data, and data fusion. Further development of these ideas would be facilitated by the creation of a high-parallel, web-based, open-source version of the SP machine, with a good user interface. This would provide a means for researchers to explore what can be done with the system and to refine it.\n    ",
        "submission_date": "2012-12-02T00:00:00",
        "last_modified_date": "2012-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.0582",
        "title": "Compositional Stochastic Modeling and Probabilistic Programming",
        "authors": [
            "Eric Mjolsness"
        ],
        "abstract": "Probabilistic programming is related to a compositional approach to stochastic modeling by switching from discrete to continuous time dynamics. In continuous time, an operator-algebra semantics is available in which processes proceeding in parallel (and possibly interacting) have summed time-evolution operators. From this foundation, algorithms for simulation, inference and model reduction may be systematically derived. The useful consequences are potentially far-reaching in computational science, machine learning and beyond. Hybrid compositional stochastic modeling/probabilistic programming approaches may also be possible.\n    ",
        "submission_date": "2012-12-03T00:00:00",
        "last_modified_date": "2012-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.0692",
        "title": "An Empirical Evaluation of Portfolios Approaches for solving CSPs",
        "authors": [
            "Roberto Amadini",
            "Maurizio Gabbrielli",
            "Jacopo Mauro"
        ],
        "abstract": "Recent research in areas such as SAT solving and Integer Linear Programming has shown that the performances of a single arbitrarily efficient solver can be significantly outperformed by a portfolio of possibly slower on-average solvers. We report an empirical evaluation and comparison of portfolio approaches applied to Constraint Satisfaction Problems (CSPs). We compared models developed on top of off-the-shelf machine learning algorithms with respect to approaches used in the SAT field and adapted for CSPs, considering different portfolio sizes and using as evaluation metrics the number of solved problems and the time taken to solve them. Results indicate that the best SAT approaches have top performances also in the CSP field and are slightly more competitive than simple models built on top of classification algorithms.\n    ",
        "submission_date": "2012-12-04T00:00:00",
        "last_modified_date": "2014-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.0750",
        "title": "Problem Solving and Computational Thinking in a Learning Environment",
        "authors": [
            "Michael Gr. Voskoglou",
            "Sheryl Buckley"
        ],
        "abstract": "Computational thinking is a new problem soling method named for its extensive use of computer science techniques. It synthesizes critical thinking and existing knowledge and applies them in solving complex technological problems. The term was coined by J. Wing, but the relationship between computational and critical thinking, the two modes of thiking in solving problems, has not been yet learly established. This paper aims at shedding some light into this relationship. We also present two classroom experiments performed recently at the Graduate Technological Educational Institute of Patras in Greece. The results of these experiments give a strong indication that the use of computers as a tool for problem solving enchances the students' abilities in solving real world problems involving mathematical modelling. This is also crossed by earlier findings of other researchers for the problem solving process in general (not only for mathematical problems).\n    ",
        "submission_date": "2012-12-02T00:00:00",
        "last_modified_date": "2012-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.0768",
        "title": "An ontology-based approach to relax traffic regulation for autonomous vehicle assistance",
        "authors": [
            "Philippe Morignot",
            "Fawzi Nashashibi"
        ],
        "abstract": "Traffic regulation must be respected by all vehicles, either human- or computer- driven. However, extreme traffic situations might exhibit practical cases in which a vehicle should safely and reasonably relax traffic regulation, e.g., in order not to be indefinitely blocked and to keep circulating. In this paper, we propose a high-level representation of an automated vehicle, other vehicles and their environment, which can assist drivers in taking such \"illegal\" but practical relaxation decisions. This high-level representation (an ontology) includes topological knowledge and inference rules, in order to compute the next high-level motion an automated vehicle should take, as assistance to a driver. Results on practical cases are presented.\n    ",
        "submission_date": "2012-12-04T00:00:00",
        "last_modified_date": "2012-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.0967",
        "title": "Compiling Relational Database Schemata into Probabilistic Graphical Models",
        "authors": [
            "Sameer Singh",
            "Thore Graepel"
        ],
        "abstract": "Instead of requiring a domain expert to specify the probabilistic dependencies of the data, in this work we present an approach that uses the relational DB schema to automatically construct a Bayesian graphical model for a database. This resulting model contains customized distributions for columns, latent variables that cluster the data, and factors that reflect and represent the foreign key links. Experiments demonstrate the accuracy of the model and the scalability of inference on synthetic and real-world data.\n    ",
        "submission_date": "2012-12-05T00:00:00",
        "last_modified_date": "2012-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.1143",
        "title": "Multiscale Markov Decision Problems: Compression, Solution, and Transfer Learning",
        "authors": [
            "Jake Bouvrie",
            "Mauro Maggioni"
        ],
        "abstract": "Many problems in sequential decision making and stochastic control often have natural multiscale structure: sub-tasks are assembled together to accomplish complex goals. Systematically inferring and leveraging hierarchical structure, particularly beyond a single level of abstraction, has remained a longstanding challenge. We describe a fast multiscale procedure for repeatedly compressing, or homogenizing, Markov decision processes (MDPs), wherein a hierarchy of sub-problems at different scales is automatically determined. Coarsened MDPs are themselves independent, deterministic MDPs, and may be solved using existing algorithms. The multiscale representation delivered by this procedure decouples sub-tasks from each other and can lead to substantial improvements in convergence rates both locally within sub-problems and globally across sub-problems, yielding significant computational savings. A second fundamental aspect of this work is that these multiscale decompositions yield new transfer opportunities across different problems, where solutions of sub-tasks at different levels of the hierarchy may be amenable to transfer to new problems. Localized transfer of policies and potential operators at arbitrary scales is emphasized. Finally, we demonstrate compression and transfer in a collection of illustrative domains, including examples involving discrete and continuous statespaces.\n    ",
        "submission_date": "2012-12-05T00:00:00",
        "last_modified_date": "2012-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.1570",
        "title": "A simple method for decision making in robocup soccer simulation 3d environment",
        "authors": [
            "Khashayar Niki Maleki",
            "Mohammad Hadi Valipour",
            "Sadegh Mokari",
            "Roohollah Yeylaghi Ashrafi",
            "Mohammad Reza Jamali",
            "Caro Lucas"
        ],
        "abstract": "In this paper new hierarchical hybrid fuzzy-crisp methods for decision making and action selection of an agent in soccer simulation 3D environment are presented. First, the skills of an agent are introduced, implemented and classified in two layers, the basicskills and the highlevel skills. In the second layer, a twophase mechanism for decision making is introduced. In phase one, some useful methods are implemented which check the agent's situation for performing required skills. In the next phase, the team str ategy, team for mation, agent's role and the agent's positioning system are introduced. A fuzzy logical approach is employed to recognize the team strategy and further more to tell the player the best position to move. At last, we comprised our implemented algor ithm in the Robocup Soccer Simulation 3D environment and results showed th eefficiency of the introduced methodology.\n    ",
        "submission_date": "2012-12-07T00:00:00",
        "last_modified_date": "2012-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2005",
        "title": "The Dynamic Controllability of Conditional STNs with Uncertainty",
        "authors": [
            "Luke Hunsberger",
            "Roberto Posenato",
            "Carlo Combi"
        ],
        "abstract": "Recent attempts to automate business processes and medical-treatment processes have uncovered the need for a formal framework that can accommodate not only temporal constraints, but also observations and actions with uncontrollable durations. To meet this need, this paper defines a Conditional Simple Temporal Network with Uncertainty (CSTNU) that combines the simple temporal constraints from a Simple Temporal Network (STN) with the conditional nodes from a Conditional Simple Temporal Problem (CSTP) and the contingent links from a Simple Temporal Network with Uncertainty (STNU). A notion of dynamic controllability for a CSTNU is defined that generalizes the dynamic consistency of a CTP and the dynamic controllability of an STNU. The paper also presents some sound constraint-propagation rules for dynamic controllability that are expected to form the backbone of a dynamic-controllability-checking algorithm for CSTNUs.\n    ",
        "submission_date": "2012-12-10T00:00:00",
        "last_modified_date": "2012-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2056",
        "title": "Soft Constraint Logic Programming for Electric Vehicle Travel Optimization",
        "authors": [
            "Giacoma Valentina Monreale",
            "Ugo Montanari",
            "Nicklas Hoch"
        ],
        "abstract": "Soft Constraint Logic Programming is a natural and flexible declarative programming formalism, which allows to model and solve real-life problems involving constraints of different types.\n",
        "submission_date": "2012-12-10T00:00:00",
        "last_modified_date": "2012-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2444",
        "title": "On revising fuzzy belief bases",
        "authors": [
            "Richard Booth",
            "Eva Richter"
        ],
        "abstract": "We look at the problem of revising fuzzy belief bases, i.e., belief base     revision in which both formulas in the base as well as revision-input formulas     can come attached with varying truth-degrees. Working within a very general     framework for fuzzy logic which is able to capture a variety of types of     inference under uncertainty, such as truth-functional fuzzy logics and certain     types of probabilistic inference, we show how the idea of rational change from     'crisp' base revision, as embodied by the idea of partial meet revision, can     be faithfully extended to revising fuzzy belief bases. We present and     axiomatise an operation of partial meet fuzzy revision and illustrate how the     operation works in several important special instances of the framework.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2445",
        "title": "Upgrading Ambiguous Signs in QPNs",
        "authors": [
            "Janneke H. Bolt",
            "Silja Renooij",
            "Linda C. van der Gaag"
        ],
        "abstract": "WA qualitative probabilistic network models the probabilistic relationships     between its variables by means of signs. Non-monotonic influences have     associated an ambiguous sign. These ambiguous signs typically lead to     uninformative results upon inference. A non-monotonic influence can, however,     be associated with a, more informative, sign that indicates its effect in the     current state of the network. To capture this effect, we introduce the concept     of situational sign. Furthermore, if the network converts to a state in which     all variables that provoke the non-monotonicity have been observed, a     non-monotonic influence reduces to a monotonic influence. We study the     persistence and propagation of situational signs upon inference and give a     method to establish the sign of a reduced influence.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2446",
        "title": "Parametric Dependability Analysis through Probabilistic Horn Abduction",
        "authors": [
            "Andrea Bobbio",
            "Stefania Montani",
            "Luigi Portinale"
        ],
        "abstract": "Dependability modeling and evaluation is aimed at investigating that a system     performs its function correctly in time. A usual way to achieve a high     reliability, is to design redundant systems that contain several replicas of     the same subsystem or component. State space methods for dependability analysis     may suffer of the state space explosion problem in such a kind of situation.     Combinatorial models, on the other hand, require the simplified assumption of     statistical independence; however, in case of redundant systems, this does not     guarantee a reduced number of modeled elements. In order to provide a more     compact system representation, parametric system modeling has been investigated     in the literature, in such a way that a set of replicas of a given subsystem is     parameterized so that only one representative instance is explicitly included.     While modeling aspects can be suitably addressed by these approaches,     analytical tools working on parametric characterizations are often more     difficult to be defined and the standard approach is to 'unfold' the     parametric model, in order to exploit standard analysis algorithms working at     the unfolded 'ground' level. Moreover, parameterized combinatorial methods     still require the statistical independence assumption. In the present paper we     consider the formalism of Parametric Fault Tree (PFT) and we show how it can be     related to Probabilistic Horn Abduction (PHA). Since PHA is a framework where     both modeling and analysis can be performed in a restricted first-order     language, we aim at showing that converting a PFT into a PHA knowledge base     will allow an approach to dependability analysis directly exploiting parametric     representation. We will show that classical qualitative and quantitative     dependability measures can be characterized within PHA. Furthermore, additional     modeling aspects (such as noisy gates and local dependencies) as well as     additional reliability measures (such as posterior probability analysis) can be     naturally addressed by this conversion. A simple example of a multi-processor     system with several replicated units is used to illustrate the approach.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2448",
        "title": "On Triangulating Dynamic Graphical Models",
        "authors": [
            "Jeff A. Bilmes",
            "Chris Bartels"
        ],
        "abstract": "This paper introduces new methodology to triangulate dynamic Bayesian     networks (DBNs) and dynamic graphical models (DGMs). While most methods to     triangulate such networks use some form of constrained elimination scheme based     on properties of the underlying directed graph, we find it useful to view     triangulation and elimination using properties only of the resulting undirected     graph, obtained after the moralization step. We first briefly introduce the     Graphical model toolkit (GMTK) and its notion of dynamic graphical models, one     that slightly extends the standard notion of a DBN. We next introduce the     'boundary algorithm', a method to find the best boundary between partitions     in a dynamic model. We find that using this algorithm, the notions of forward-     and backward-interface become moot - namely, the size and fill-in of the best     forward- and backward- interface are identical. Moreover, we observe that     finding a good partition boundary allows for constrained elimination orders     (and therefore graph triangulations) that are not possible using standard     slice-by-slice constrained eliminations. More interestingly, with certain     boundaries it is possible to obtain constrained elimination schemes that lie     outside the space of possible triangulations using only unconstrained     elimination. Lastly, we report triangulation results on invented graphs,     standard DBNs from the literature, novel DBNs used in speech recognition     research systems, and also random graphs. Using a number of different     triangulation quality measures (max clique size, state-space, etc.), we find     that with our boundary algorithm the triangulation quality can dramatically     improve.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2449",
        "title": "An Empirical Study of w-Cutset Sampling for Bayesian Networks",
        "authors": [
            "Bozhena Bidyuk",
            "Rina Dechter"
        ],
        "abstract": "The paper studies empirically the time-space trade-off between sampling and     inference in a sl cutset sampling algorithm. The algorithm samples over a     subset of nodes in a Bayesian network and applies exact inference over the     rest. Consequently, while the size of the sampling space decreases, requiring     less samples for convergence, the time for generating each single sample     increases. The w-cutset sampling selects a sampling set such that the     induced-width of the network when the sampling set is observed is bounded by w,     thus requiring inference whose complexity is exponential in w. In this paper,     we investigate performance of w-cutset sampling over a range of w values and     measure the accuracy of w-cutset sampling as a function of w. Our experiments     demonstrate that the cutset sampling idea is quite powerful showing that an     optimal balance between inference and sampling benefits substantially from restricting the cutset size, even at the cost of more complex inference.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2450",
        "title": "A possibilistic handling of partially ordered information",
        "authors": [
            "Salem Benferhat",
            "Sylvain Lagrue",
            "Odile Papini"
        ],
        "abstract": "In a standard possibilistic logic, prioritized information are encoded by     means of weighted knowledge base. This paper proposes an extension of     possibilistic logic for dealing with partially ordered information. We Show     that all basic notions of standard possibilitic logic (sumbsumption, syntactic     and semantic inference, etc.) have natural couterparts when dealing with     partially ordered information. We also propose an algorithm which computes     possibilistic conclusions of a partial knowledge base of a partially ordered     knowlege base.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2452",
        "title": "Value Elimination: Bayesian Inference via Backtracking Search",
        "authors": [
            "Fahiem Bacchus",
            "Shannon Dalmao",
            "Toniann Pitassi"
        ],
        "abstract": "Backtracking search is a powerful algorithmic paradigm that can be used to     solve many problems. It is in a certain sense the dual of variable elimination;     but on many problems, e.g., SAT, it is vastly superior to variable elimination     in practice. Motivated by this we investigate the application of backtracking     search to the problem of Bayesian inference (Bayes). We show that natural     generalizations of known techniques allow backtracking search to achieve     performance guarantees similar to standard algorithms for Bayes, and that there     exist problems on which backtracking can in fact do much better. We also     demonstrate that these ideas can be applied to implement a Bayesian inference     engine whose performance is competitive with standard algorithms. Since     backtracking search can very naturally take advantage of context specific     structure, the potential exists for performance superior to standard algorithms     on many problems.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2455",
        "title": "New Advances in Inference by Recursive Conditioning",
        "authors": [
            "David Allen",
            "Adnan Darwiche"
        ],
        "abstract": "Recursive Conditioning (RC) was introduced recently as the first any-space     algorithm for inference in Bayesian networks which can trade time for space by     varying the size of its cache at the increment needed to store a floating point     number. Under full caching, RC has an asymptotic time and space complexity     which is comparable to mainstream algorithms based on variable elimination and     clustering (exponential in the network treewidth and linear in its size). We     show two main results about RC in this paper. First, we show that its actual     space requirements under full caching are much more modest than those needed by     mainstream methods and study the implications of this finding. Second, we show     that RC can effectively deal with determinism in Bayesian networks by employing     standard logical techniques, such as unit resolution, allowing a significant     reduction in its time requirements in certain cases. We illustrate our results     using a number of benchmark networks, including the very challenging ones that     arise in genetic linkage analysis.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2456",
        "title": "Incremental Compilation of Bayesian networks",
        "authors": [
            "Julia M. Flores",
            "Jose A. Gamez",
            "Kristian G. Olesen"
        ],
        "abstract": "Most methods of exact probability propagation in Bayesian networks do not carry out the inference directly over the network, but over a secondary structure known as a junction tree or a join tree (JT). The process of obtaining a JT is usually termed {sl compilation}. As compilation is usually viewed as a whole process; each time the network is modified, a new compilation process has to be carried out. The possibility of reusing an already existing JT, in order to obtain the new one regarding only the modifications in the network has received only little attention in the literature. In this paper we present a method for incremental compilation of a Bayesian network, following the classical scheme in which triangulation plays the key role. In order to perform incremental compilation we propose to recompile only those parts of the JT which can have been affected by the networks modifications. To do so, we exploit the technique OF maximal prime subgraph decomposition in determining the minimal subgraph(s) that have to be recompiled, and thereby the minimal subtree(s) of the JT that should be replaced by new subtree(s).We focus on structural modifications : addition and deletion of links and variables.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2457",
        "title": "Structure-Based Causes and Explanations in the Independent Choice Logic",
        "authors": [
            "Alberto Finzi",
            "Thomas Lukasiewicz"
        ],
        "abstract": "This paper is directed towards combining Pearl's structural-model approach to causal reasoning with high-level formalisms for reasoning about actions. More precisely, we present a combination of Pearl's structural-model approach with Poole's independent choice logic. We show how probabilistic theories in the independent choice logic can be mapped to probabilistic causal models. This mapping provides the independent choice logic with appealing concepts of causality and explanation from the structural-model approach. We illustrate this along Halpern and Pearl's sophisticated notions of actual cause, explanation, and partial explanation. This mapping also adds first-order modeling capabilities and explicit actions to the structural-model approach.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2458",
        "title": "Inference in Polytrees with Sets of Probabilities",
        "authors": [
            "Jose Carlos Ferreira da Rocha",
            "Fabio Gagliardi Cozman",
            "Cassio Polpo de Campos"
        ],
        "abstract": "Inferences in directed acyclic graphs associated with probability sets and     probability intervals are NP-hard, even for polytrees. In this paper we focus     on such inferences, and propose: 1) a substantial improvement on Tessems A / R algorithm FOR polytrees WITH probability intervals; 2)     a new algorithm FOR     direction - based local search(IN sets OF probability)     that improves ON existing methods; 3) a collection OF branch - AND - bound algorithms that combine the previous ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2459",
        "title": "Symbolic Generalization for On-line Planning",
        "authors": [
            "Zhengzhu Feng",
            "Eric A. Hansen",
            "Shlomo Zilberstein"
        ],
        "abstract": "Symbolic representations have been used successfully in off-line planning algorithms for Markov decision processes. We show that they can also improve the performance of on-line planners. In addition to reducing computation time, symbolic generalization can reduce the amount of costly real-world interactions required for convergence. We introduce Symbolic Real-Time Dynamic Programming (or sRTDP), an extension of RTDP. After each step of on-line interaction with an environment, sRTDP uses symbolic model-checking techniques to generalizes its experience by updating a group of states rather than a single state.  We examine two heuristic approaches to dynamic grouping of states and show that they accelerate the planning process significantly in terms of both CPU time and the number of steps of interaction with the environment.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2461",
        "title": "Probabilistic Reasoning about Actions in Nonmonotonic Causal Theories",
        "authors": [
            "Thomas Eiter",
            "Thomas Lukasiewicz"
        ],
        "abstract": "We present the language {m P}{cal C}+ for probabilistic reasoning about actions, which is a generalization of the action language {cal C}+ that allows to deal with probabilistic as well as nondeterministic effects of actions. We define a formal semantics of {m P}{cal C}+ in terms of probabilistic transitions between sets of states. Using a concept of a history and its belief state, we then show how several important problems in reasoning about actions can be concisely formulated in our formalism.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2463",
        "title": "A Simple Insight into Iterative Belief Propagation's Success",
        "authors": [
            "Rina Dechter",
            "Robert Mateescu"
        ],
        "abstract": "In Non - ergodic belief networks the posterior belief OF many queries given evidence may become ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2464",
        "title": "A Robust Independence Test for Constraint-Based Learning of Causal Structure",
        "authors": [
            "Denver Dash",
            "Marek J. Druzdzel"
        ],
        "abstract": "Constraint-based (CB) learning is a formalism for learning a causal network with a database D by performing a series of conditional-independence tests to infer structural information. This paper considers a new test of independence that combines ideas from Bayesian learning, Bayesian network inference, and classical hypothesis testing to produce a more reliable and robust test. The new test can be calculated in the same asymptotic time and space required for the standard tests such as the chi-squared test, but it allows the specification of a prior distribution over parameters and can be used when the database is incomplete. We prove that the test is correct, and we demonstrate empirically that, when used with a CB causal discovery algorithm with noninformative priors, it recovers structural features more reliably and it produces networks with smaller KL-Divergence, especially as the number of nodes increases or the number of records decreases. Another benefit is the dramatic reduction in the probability that a CB algorithm will stall during the search, providing a remedy for an annoying problem plaguing CB learning when the database is small.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2465",
        "title": "Loopy Belief Propagation as a Basis for Communication in Sensor Networks",
        "authors": [
            "Christopher Crick",
            "Avi Pfeffer"
        ],
        "abstract": "Sensor networks are an exciting new kind of computer system. Consisting of a     large number of tiny, cheap computational devices physically distributed in an     environment, they gather and process data about the environment in real time.     One of the central questions in sensor networks is what to do with the data,     i.e., how to reason with it and how to communicate it. This paper argues that     the lessons of the UAI community, in particular that one should produce and     communicate beliefs rather than raw sensor values, are highly relevant to     sensor networks. We contend that loopy belief propagation is particularly well     suited to communicating beliefs in sensor networks, due to its compact     implementation and distributed nature. We investigate the ability of loopy     belief propagation to function under the stressful conditions likely to prevail     in sensor networks. Our experiments show that it performs well and degrades     gracefully. It converges to appropriate beliefs even in highly asynchronous     settings where some nodes communicate far less frequently than others; it     continues to function if some nodes fail to participate in the propagation     process; and it can track changes in the environment that occur while beliefs     are propagating. As a result, we believe that sensor networks present an     important application opportunity for UAI.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2469",
        "title": "Using the structure of d-connecting paths as a qualitative measure of the strength of dependence",
        "authors": [
            "Sanjay Chaudhari",
            "Thomas S. Richardson"
        ],
        "abstract": "Pearls concept OF a d - connecting path IS one OF the foundations      OF the modern theory OF graphical models : the absence OF a d      - connecting path IN a DAG indicates that conditional independence      will hold IN ANY distribution factorising according TO that graph.      IN this paper we show that IN singly - connected Gaussian DAGs      it IS possible TO USE the form OF a d - connection TO obtain qualitative      information about the strength OF conditional ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2473",
        "title": "A Linear Belief Function Approach to Portfolio Evaluation",
        "authors": [
            "Liping Liu",
            "Catherine Shenoy",
            "Prakash P. Shenoy"
        ],
        "abstract": "By elaborating on the notion of linear belief functions (Dempster 1990; Liu 1996), we propose an elementary approach to knowledge representation for expert systems using linear belief functions.  We show how to use basic matrices to represent market information and financial knowledge, including complete ignorance, statistical observations, subjective speculations, distributional assumptions, linear relations, and empirical asset pricing models.  We then appeal to Dempster's rule of combination to integrate the knowledge for assessing an overall belief of portfolio performance, and updating the belief by incorporating additional information.  We use an example of three gold stocks to illustrate the approach.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2476",
        "title": "Approximate Decomposition: A Method for Bounding and Estimating Probabilistic and Deterministic Queries",
        "authors": [
            "David Ephraim Larkin"
        ],
        "abstract": " In this paper, we introduce a method for approximating the solution to inference and optimization tasks in uncertain and deterministic reasoning. Such tasks are in general intractable for exact algorithms because of the large number of dependency relationships in their structure. Our method effectively maps such a dense problem to a sparser one which is in some sense \"closest\". Exact methods can be run on the sparser problem to derive bounds on the original answer, which can be quite sharp. We present empirical results demonstrating that our method works well on the tasks of belief inference and finding the probability of the most probable explanation in belief networks, and finding the cost of the solution that violates the smallest number of constraints in constraint satisfaction problems. On one large CPCS network, for example, we were able to calculate upper and lower bounds on the conditional probability of a variable, given evidence, that were almost identical in the average case.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2481",
        "title": "Monte-Carlo optimizations for resource allocation problems in stochastic network systems",
        "authors": [
            "Milos Hauskrecht",
            "Tomas Singliar"
        ],
        "abstract": "Real-world distributed systems and networks are often unreliable and subject     to random failures of its components. Such a stochastic behavior affects     adversely the complexity of optimization tasks performed routinely upon such     systems, in particular, various resource allocation tasks. In this work we     investigate and develop Monte Carlo solutions for a class of two-stage     optimization problems in stochastic networks in which the expected value of     resource allocations before and after stochastic failures needs to be     optimized. The limitation of these problems is that their exact solutions are     exponential in the number of unreliable network components: thus, exact methods     do not scale-up well to large networks often seen in practice. We first prove     that Monte Carlo optimization methods can overcome the exponential bottleneck     of exact methods. Next we support our theoretical findings on resource     allocation experiments and show a very good scale-up potential of the new     methods to large stochastic networks.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2482",
        "title": "Implementation and Comparison of Solution Methods for Decision Processes with Non-Markovian Rewards",
        "authors": [
            "Charles Gretton",
            "David Price",
            "Sylvie Thiebaux"
        ],
        "abstract": "This paper examines a number of solution methods for decision processes with     non-Markovian rewards (NMRDPs). They all exploit a temporal logic specification     of the reward function to automatically translate the NMRDP into an equivalent     Markov decision process (MDP) amenable to well-known MDP solution methods. They     differ however in the representation of the target MDP and the class of MDP     solution methods to which they are suited. As a result, they adopt different     temporal logics and different translations. Unfortunately, no implementation of     these methods nor experimental let alone comparative results have ever been     reported. This paper is the first step towards filling this gap. We describe an     integrated system for solving NMRDPs which implements these methods and several     variants under a common interface; we use it to compare the various approaches     and identify the problem features favoring one over the other.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2484",
        "title": "Decision Making with Partially Consonant Belief Functions",
        "authors": [
            "Phan H. Giang",
            "Prakash P. Shenoy"
        ],
        "abstract": "This paper studies decision making for Walley's partially consonant belief functions (pcb). In a pcb, the set of foci are partitioned. Within each partition, the foci are nested. The pcb class includes probability functions and possibility functions as extreme cases. Unlike earlier proposals for a decision theory with belief functions, we employ an axiomatic approach. We adopt an axiom system similar in spirit to von Neumann - Morgenstern's linear utility theory for a preference relation on pcb lotteries. We prove a representation theorem for this relation. Utility for a pcb lottery is a combination of linear utility for probabilistic lottery and binary utility for possibilistic lottery.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2485",
        "title": "Phase Transition of Tractability in Constraint Satisfaction and Bayesian Network Inference",
        "authors": [
            "Yong Gao"
        ],
        "abstract": "There has been great interest in identifying tractable subclasses of NP     complete problems and designing efficient algorithms for these tractable     classes. Constraint satisfaction and Bayesian network inference are two     examples of such problems that are of great importance in AI and algorithms. In     this paper we study, under the frameworks of random constraint satisfaction     problems and random Bayesian networks, a typical tractable subclass     characterized by the treewidth of the problems. We show that the property of     having a bounded treewidth for CSPs and Bayesian network inference problem has     a phase transition that occurs while the underlying structures of problems are     still sparse. This implies that algorithms making use of treewidth based     structural knowledge only work efficiently in a limited range of random     instance.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2486",
        "title": "Extending Factor Graphs so as to Unify Directed and Undirected Graphical Models",
        "authors": [
            "Brendan J. Frey"
        ],
        "abstract": "The two most popular types of graphical model are directed models (Bayesian     networks) and undirected models (Markov random fields, or MRFs). Directed and     undirected models offer complementary properties in model construction,     expressing conditional independencies, expressing arbitrary factorizations of     joint distributions, and formulating message-passing inference algorithms. We     show that the strengths of these two representations can be combined in a     single type of graphical model called a 'factor graph'. Every Bayesian     network or MRF can be easily converted to a factor graph that expresses the     same conditional independencies, expresses the same factorization of the joint     distribution, and can be used for probabilistic inference through application     of a single, simple message-passing algorithm. In contrast to chain graphs,     where message-passing is implemented on a hypergraph, message-passing can be     directly implemented on the factor graph. We describe a modified 'Bayes-ball'     algorithm for establishing conditional independence in factor graphs, and we     show that factor graphs form a strict superset of Bayesian networks and MRFs.     In particular, we give an example of a commonly-used 'mixture of experts' model     fragment, whose independencies cannot be represented in a Bayesian network or     an MRF, but can be represented in a factor graph. We finish by giving examples     of real-world problems that are not well suited to representation in Bayesian     networks and MRFs, but are well-suited to representation in factor graphs.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2493",
        "title": "Decentralized Sensor Fusion With Distributed Particle Filters",
        "authors": [
            "Matthew Rosencrantz",
            "Geoffrey Gordon",
            "Sebastian Thrun"
        ],
        "abstract": "This paper presents a scalable Bayesian technique for     decentralized state estimation from multiple platforms in dynamic     environments. As has long been recognized, centralized     architectures impose severe scaling limitations for     distributed systems due to the enormous communication     overheads. We propose a strictly decentralized approach in which only     nearby platforms exchange information. They do so through an     interactive communication protocol aimed at maximizing     information flow. Our approach is evaluated in the context     of a distributed surveillance scenario that arises in a robotic system     for playing the game of laser tag. Our results, both from     simulation and using physical robots, illustrate an unprecedented     scaling capability to large teams of vehicles.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2496",
        "title": "An Axiomatic Approach to Robustness in Search Problems with Multiple Scenarios",
        "authors": [
            "Patrice Perny",
            "Olivier Spanjaard"
        ],
        "abstract": "This paper is devoted to the search of robust solutions in state space graphs when costs depend on scenarios. We first present axiomatic requirements for preference compatibility with the intuitive idea of ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2497",
        "title": "Solving MAP Exactly using Systematic Search",
        "authors": [
            "James D. Park",
            "Adnan Darwiche"
        ],
        "abstract": "MAP is the problem of finding a most probable instantiation of a set of     variables in a Bayesian network given some evidence. Unlike computing posterior     probabilities, or MPE (a special case of MAP), the time and space complexity of     structural solutions for MAP are not only exponential in the network treewidth,     but in a larger parameter known as the \"constrained\" treewidth. In practice,     this means that computing MAP can be orders of magnitude more expensive than     computing posterior probabilities or MPE. This paper introduces a new, simple     upper bound on the probability of a MAP solution, which admits a tradeoff     between the bound quality and the time needed to compute it. The bound is shown     to be generally much tighter than those of other methods of comparable     complexity. We use this proposed upper bound to develop a branch-and-bound     search algorithm for solving MAP exactly. Experimental results demonstrate that     the search algorithm is able to solve many problems that are far beyond the     reach of any structure-based method for MAP. For example, we show that the     proposed algorithm can compute MAP exactly and efficiently for some networks     whose constrained treewidth is more than 40.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2499",
        "title": "Marginalizing Out Future Passengers in Group Elevator Control",
        "authors": [
            "Daniel N. Nikovski",
            "Matthew Brand"
        ],
        "abstract": "Group elevator scheduling is an NP-hard sequential decision-making problem     with unbounded state spaces and substantial uncertainty. Decision-theoretic     reasoning plays a surprisingly limited role in fielded systems. A new     opportunity for probabilistic methods has opened with the recent discovery of a     tractable solution for the expected waiting times of all passengers in the     building, marginalized over all possible passenger itineraries. Though     commercially competitive, this solution does not contemplate future passengers.     Yet in up-peak traffic, the effects of future passengers arriving at the lobby     and entering elevator cars can dominate all waiting times. We develop a     probabilistic model of how these arrivals affect the behavior of elevator cars     at the lobby, and demonstrate how this model can be used to very significantly     reduce the average waiting time of all passengers.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2501",
        "title": "Dealing with uncertainty in fuzzy inductive reasoning methodology",
        "authors": [
            "Francisco Mugica",
            "Angela Nebot",
            "Pilar Gomez"
        ],
        "abstract": "The aim of this research is to develop a reasoning under uncertainty strategy     in the context of the Fuzzy Inductive Reasoning (FIR) methodology. FIR emerged     from the General Systems Problem Solving developed by G. Klir. It is a data     driven methodology based on systems behavior rather than on structural     knowledge. It is a very useful tool for both the modeling and the prediction of     those systems for which no previous structural knowledge is available. FIR     reasoning is based on pattern rules synthesized from the available data. The     size of the pattern rule base can be very large making the prediction process     quite difficult. In order to reduce the size of the pattern rule base, it is     possible to automatically extract classical Sugeno fuzzy rules starting from     the set of pattern rules. The Sugeno rule base preserves pattern rules     knowledge as much as possible. In this process some information is lost but     robustness is considerably increased. In the forecasting process either the     pattern rule base or the Sugeno fuzzy rule base can be used. The first option     is desirable when the computational resources make it possible to deal with the     overall pattern rule base or when the extracted fuzzy rules are not accurate     enough due to uncertainty associated to the original data. In the second     option, the prediction process is done by means of the classical Sugeno     inference system. If the amount of uncertainty associated to the data is small,     the predictions obtained using the Sugeno fuzzy rule base will be very     accurate. In this paper a mixed pattern/fuzzy rules strategy is proposed to     deal with uncertainty in such a way that the best of both perspectives is used.     Areas in the data space with a higher level of uncertainty are identified by     means of the so-called error models. The prediction process in these areas     makes use of a mixed pattern/fuzzy rules scheme, whereas areas identified with     a lower level of uncertainty only use the Sugeno fuzzy rule base. The proposed     strategy is applied to a real biomedical system, i.e., the central nervous     system control of the cardiovascular system.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2502",
        "title": "Optimal Limited Contingency Planning",
        "authors": [
            "Nicolas Meuleau",
            "David Smith"
        ],
        "abstract": "For a given problem, the optimal Markov policy can be considerred as a     conditional or contingent plan containing a (potentially large) number of     branches. Unfortunately, there are applications where it is desirable to     strictly limit the number of decision points and branches in a plan. For     example, it may be that plans must later undergo more detailed simulation to     verify correctness and safety, or that they must be simple enough to be     understood and analyzed by humans. As a result, it may be necessary to limit     consideration to plans with only a small number of branches. This raises the     question of how one goes about finding optimal plans containing only a limited     number of branches. In this paper, we present an any-time algorithm for optimal     k-contingency planning (OKP). It is the first optimal algorithm for limited     contingency planning that is not an explicit enumeration of possible contingent     plans. By modelling the problem as a Partially Observable Markov Decision     Process, it implements the Bellman optimality principle and prunes the solution     space. We present experimental results of applying this algorithm to some     simple test cases.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2503",
        "title": "Practically Perfect",
        "authors": [
            "Christopher Meek",
            "David Maxwell Chickering"
        ],
        "abstract": "The property of perfectness plays an important role in the theory of Bayesian     networks. First, the existence of perfect distributions for arbitrary sets of     variables and directed acyclic graphs implies that various methods for reading     independence from the structure of the graph (e.g., Pearl, 1988; Lauritzen,     Dawid, Larsen & Leimer, 1990) are complete. Second, the asymptotic     reliability of various search methods is guaranteed under the assumption that     the generating distribution is perfect (e.g., Spirtes, Glymour & Scheines,     2000; Chickering & Meek, 2002). We provide a lower-bound on the     probability of sampling a non-perfect distribution when using a fixed number of     bits to represent the parameters of the Bayesian network. This bound approaches     zero exponentially fast as one increases the number of bits used to represent     the parameters. This result implies that perfect distributions with     fixed-length representations exist. We also provide a lower-bound on the number     of bits needed to guarantee that a distribution sampled from a uniform     Dirichlet distribution is perfect with probability greater than 1/2. This     result is useful for constructing randomized reductions for hardness proofs.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2505",
        "title": "Systematic vs. Non-systematic Algorithms for Solving the MPE Task",
        "authors": [
            "Radu Marinescu",
            "Kalev Kask",
            "Rina Dechter"
        ],
        "abstract": "The paper continues the study of partitioning based inference of heuristics     for search in the context of solving the Most Probable Explanation task in     Bayesian Networks. We compare two systematic Branch and Bound search     algorithms, BBBT (for which the heuristic information is constructed during     search and allows dynamic variable/value ordering) and its predecessor BBMB     (for which the heuristic information is pre-compiled), against a number of     popular local search algorithms for the MPE problem. We show empirically that,     when viewed as approximation schemes, BBBT/BBMB are superior to all of these     best known SLS algorithms, especially when the domain sizes increase beyond 2.     This is in contrast with the performance of SLS vs. systematic search on     CSP/SAT problems, where SLS often significantly outperforms systematic     algorithms. As far as we know, BBBT/BBMB are currently the best performing     algorithms for solving the MPE task.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2506",
        "title": "Strong Faithfulness and Uniform Consistency in Causal Inference",
        "authors": [
            "Jiji Zhang",
            "Peter L. Spirtes"
        ],
        "abstract": "A fundamental question in causal inference is whether it     is possible to reliably infer manipulation effects from observational     data. There are a variety of senses of asymptotic reliability in the     statistical literature, among which the most commonly discussed     frequentist notions are pointwise consistency and uniform     consistency. Uniform consistency is in general preferred to pointwise     consistency because the former allows us to control the worst case     error bounds with a finite sample size. In the sense of pointwise     consistency, several reliable causal inference algorithms have been     established under the Markov and Faithfulness assumptions [Pearl 2000,     Spirtes et al. 2001]. In the sense of uniform consistency, however,     reliable causal inference is impossible under the two assumptions when     time order is unknown and/or latent confounders are present [Robins et     al. 2000]. In this paper we present two natural generalizations of the     Faithfulness assumption in the context of structural equation models,     under which we show that the typical algorithms in the literature (in     some cases with modifications) are uniformly consistent even when the     time order is unknown. We also discuss the situation where latent     confounders may be present and the sense in which the Faithfulness     assumption is a limiting case of the stronger assumptions.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2507",
        "title": "An Importance Sampling Algorithm Based on Evidence Pre-propagation",
        "authors": [
            "Changhe Yuan",
            "Marek J. Druzdzel"
        ],
        "abstract": "Precision achieved by stochastic sampling algorithms for Bayesian networks typically deteriorates in face of extremely unlikely evidence. To address this problem, we propose the Evidence Pre-propagation Importance Sampling algorithm (EPIS-BN), an importance sampling algorithm that computes an approximate importance function by the heuristic methods: loopy belief Propagation and e-cutoff. We tested the performance of e-cutoff on three large real Bayesian networks: ANDES, CPCS, and PATHFINDER. We observed that on each of these networks the EPIS-BN algorithm gives us a considerable improvement over the current state of the art algorithm, the AIS-BN algorithm. In addition, it avoids the costly learning stage of the AIS-BN algorithm.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2515",
        "title": "The Revisiting Problem in Mobile Robot Map Building: A Hierarchical Bayesian Approach",
        "authors": [
            "Benjamin Stewart",
            "Jonathan Ko",
            "Dieter Fox",
            "Kurt Konolige"
        ],
        "abstract": "We present an application of hierarchical Bayesian estimation to robot map building. The revisiting problem occurs when a robot has to decide whether it is seeing a previously-built portion of a map, or is exploring new territory. This is a difficult decision problem, requiring the probability of being outside of the current known map. To estimate this probability, we model the structure of a \"typical\" environment as a hidden Markov model that generates sequences of views observed by a robot navigating through the environment. A Dirichlet prior over structural models is learned from previously explored environments. Whenever a robot explores a new environment, the posterior over the model is estimated by Dirichlet hyperparameters. Our approach is implemented and tested in the context of multi-robot map merging, a particularly difficult instance of the revisiting problem. Experiments with robot data show that the technique yields strong improvements over alternative methods.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2518",
        "title": "Efficient Inference in Large Discrete Domains",
        "authors": [
            "Rita Sharma",
            "David L Poole"
        ],
        "abstract": "In this paper we examine the problem of inference in Bayesian Networks with discrete random variables that have very large or even unbounded domains. For example, in a domain where we are trying to identify a person, we may have variables that have as domains, the set of all names, the set of all postal codes, or the set of all credit card numbers. We cannot just have big tables of the conditional probabilities, but need compact representations.  We provide an inference algorithm, based on variable elimination, for belief networks containing both large domain and normal discrete random variables. We use intensional (i.e., in terms of procedures) and extensional (in terms of listing the elements) representations of conditional probabilities and of the intermediate factors. \n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2519",
        "title": "CLP(BN): Constraint Logic Programming for Probabilistic Knowledge",
        "authors": [
            "Vitor Santos Costa",
            "David Page",
            "Maleeha Qazi",
            "James Cussens"
        ],
        "abstract": "We present CLP(BN), a novel approach that aims at expressing Bayesian     networks through the constraint logic programming framework. Arguably, an     important limitation of traditional Bayesian networks is that they are     propositional, and thus cannot represent relations between multiple similar     objects in multiple contexts. Several researchers have thus proposed     first-order languages to describe such networks. Namely, one very successful     example of this approach are the Probabilistic Relational Models (PRMs), that     combine Bayesian networks with relational database technology. The key     difficulty that we had to address when designing CLP(cal{BN}) is that logic     based representations use ground terms to denote objects. With probabilitic     data, we need to be able to uniquely represent an object whose value we are not     sure about. We use {sl Skolem functions} as unique new symbols that uniquely     represent objects with unknown value. The semantics of CLP(cal{BN}) programs     then naturally follow from the general framework of constraint logic     programming, as applied to a specific domain where we have probabilistic data.     This paper introduces and defines CLP(cal{BN}), and it describes an     implementation and initial experiments. The paper also shows how     CLP(cal{BN}) relates to Probabilistic Relational Models (PRMs), Ngo and     Haddawys Probabilistic Logic Programs, AND Kersting AND     De Raedts Bayesian     Logic Programs.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2614",
        "title": "A Study on Fuzzy Systems",
        "authors": [
            "Michael Gr. Voskoglou"
        ],
        "abstract": "We use princiles of fuzzy logic to develop a general model representing several processes in a system's operation characterized by a degree of vagueness and/or uncertainy. Further, we introduce three altenative measures of a fuzzy system's effectiveness connected to the above model. An applcation is also developed for the Mathematical Modelling process illustrating our results.\n    ",
        "submission_date": "2012-12-11T00:00:00",
        "last_modified_date": "2012-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2657",
        "title": "Study: Symmetry breaking for ASP",
        "authors": [
            "Anna Ryabokon"
        ],
        "abstract": "In their nature configuration problems are combinatorial (optimization) problems. In order to find a configuration a solver has to instantiate a number of components of a some type and each of these components can be used in a relation defined for a type. Therefore, many solutions of a configuration problem have symmetric ones which can be obtained by replacing some component of a solution by another one of the same type. These symmetric solutions decrease performance of optimization algorithms because of two reasons: a) they satisfy all requirements and cannot be pruned out from the search space; and b) existence of symmetric optimal solutions does not allow to prove the optimum in a feasible time.\n    ",
        "submission_date": "2012-12-11T00:00:00",
        "last_modified_date": "2012-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2671",
        "title": "Performance Analysis of ANFIS in short term Wind Speed Prediction",
        "authors": [
            "Ernesto Cort\u00e9s P\u00e9rez",
            "Ignacio Algredo-Badillo",
            "V\u00edctor Hugo Garc\u00eda Rodr\u00edguez"
        ],
        "abstract": "Results are presented on the performance of Adaptive Neuro-Fuzzy Inference system (ANFIS) for wind velocity forecasts in the Isthmus of Tehuantepec region in the state of Oaxaca, Mexico. The data bank was provided by the meteorological station located at the University of Isthmus, Tehuantepec campus, and this data bank covers the period from 2008 to 2011. Three data models were constructed to carry out 16, 24 and 48 hours forecasts using the following variables: wind velocity, temperature, barometric pressure, and date. The performance measure for the three models is the mean standard error (MSE). In this work, performance analysis in short-term prediction is presented, because it is essential in order to define an adequate wind speed model for eolian parks, where a right planning provide economic benefits.\n    ",
        "submission_date": "2012-12-11T00:00:00",
        "last_modified_date": "2012-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2791",
        "title": "Understanding (dis)similarity measures",
        "authors": [
            "Llu\u00eds A. Belanche"
        ],
        "abstract": "Intuitively, the concept of similarity is the notion to measure an inexact matching between two entities of the same reference set. The notions of similarity and its close relative dissimilarity are widely used in many fields of Artificial Intelligence. Yet they have many different and often partial definitions or properties, usually restricted to one field of application and thus incompatible with other uses. This paper contributes to the design and understanding of similarity and dissimilarity measures for Artificial Intelligence. A formal dual definition for each concept is proposed, joined with a set of fundamental properties. The behavior of the properties under several transformations is studied and revealed as an important matter to bear in mind. We also develop several practical examples that work out the proposed approach.\n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2857",
        "title": "ConArg: a Tool to Solve (Weighted) Abstract Argumentation Frameworks with (Soft) Constraints",
        "authors": [
            "Stefano Bistarelli",
            "Francesco Santini"
        ],
        "abstract": "ConArg is a Constraint Programming-based tool that can be used to model and solve different problems related to Abstract Argumentation Frameworks (AFs). To implement this tool we have used JaCoP, a Java library that provides the user with a Finite Domain Constraint Programming paradigm. ConArg is able to randomly generate networks with small-world properties in order to find conflict-free, admissible, complete, stable grounded, preferred, semi-stable, stage and ideal extensions on such interaction graphs. We present the main features of ConArg and we report the performance in time, showing also a comparison with ASPARTIX [1], a similar tool using Answer Set Programming. The use of techniques for constraint solving can tackle the complexity of the problems presented in [2]. Moreover we suggest semiring-based soft constraints as a mean to parametrically represent and solve Weighted Argumentation Frameworks: different kinds of preference levels related to attacks, e.g., a score representing a \"fuzziness\", a \"cost\" or a probability, can be represented by choosing different instantiation of the semiring algebraic structure. The basic idea is to provide a common computational and quantitative framework.\n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2902",
        "title": "Modeling in OWL 2 without Restrictions",
        "authors": [
            "Michael Schneider",
            "Sebastian Rudolph",
            "Geoff Sutcliffe"
        ],
        "abstract": "The Semantic Web ontology language OWL 2 DL comes with a variety of language features that enable sophisticated and practically useful modeling. However, the use of these features has been severely restricted in order to retain decidability of the language. For example, OWL 2 DL does not allow a property to be both transitive and asymmetric, which would be desirable, e.g., for representing an ancestor relation. In this paper, we argue that the so-called global restrictions of OWL 2 DL preclude many useful forms of modeling, by providing a catalog of basic modeling patterns that would be available in OWL 2 DL if the global restrictions were discarded. We then report on the results of evaluating several state-of-the-art OWL 2 DL reasoners on problems that use combinations of features in a way that the global restrictions are violated. The systems turn out to rely heavily on the global restrictions and are thus largely incapable of coping with the modeling patterns. Next we show how off-the-shelf first-order logic theorem proving technology can be used to perform reasoning in the OWL 2 direct semantics, the semantics that underlies OWL 2 DL, but without requiring the global restrictions. Applying a naive proof-of-concept implementation of this approach to the test problems was successful in all cases. Based on our observations, we make suggestions for future lines of research on expressive description logic-style OWL reasoning.\n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2013-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.3618",
        "title": "Machine Learning in Proof General: Interfacing Interfaces",
        "authors": [
            "Ekaterina Komendantskaya",
            "J\u00f3nathan Heras",
            "Gudmund Grov"
        ],
        "abstract": "We present ML4PG - a machine learning extension for Proof General. It allows users to gather proof statistics related to shapes of goals, sequences of applied tactics, and proof tree structures from the libraries of interactive higher-order proofs written in Coq and SSReflect. The gathered data is clustered using the state-of-the-art machine learning algorithms available in MATLAB and Weka. ML4PG provides automated interfacing between Proof General and MATLAB/Weka. The results of clustering are used by ML4PG to provide proof hints in the process of interactive proof development.  \n    ",
        "submission_date": "2012-12-14T00:00:00",
        "last_modified_date": "2013-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.3817",
        "title": "Probability Bracket Notation: Markov Sequence Projector of Visible and Hidden Markov Models in Dynamic Bayesian Networks",
        "authors": [
            "Xing M. Wang"
        ],
        "abstract": "With the symbolic framework of Probability Bracket Notation (PBN), the Markov Sequence Projector (MSP) is introduced to expand the evolution formula of Homogeneous Markov Chains (HMCs). The well-known weather example, a Visible Markov Model (VMM), illustrates that the full joint probability of a VMM corresponds to a specifically projected Markov state sequence in the expanded evolution formula. In a Hidden Markov Model (HMM), the probability basis (P-basis) of the hidden Markov state sequence and the P-basis of the observation sequence exist in the sequential event space. The full joint probability of an HMM is the product of the (unknown) projected hidden sequence of Markov states and their transformations into the observation P-bases. The Viterbi algorithm is applied to the famous Weather-Stone HMM example to determine the most likely weather-state sequence given the observed stone-state sequence. Our results are verified using the Elvira software package. Using the PBN, we unify the evolution formulas for Markov models like VMMs, HMMs, and factorial HMMs (with discrete time). We briefly investigated the extended HMM, addressing the feedback issue, and the continuous-time VMM and HMM (with discrete or continuous states). All these models are subclasses of Dynamic Bayesian Networks (DBNs) essential for Machine Learning (ML) and Artificial Intelligence (AI).\n    ",
        "submission_date": "2012-12-16T00:00:00",
        "last_modified_date": "2025-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.3996",
        "title": "Increasing Air Traffic: What is the Problem?",
        "authors": [
            "Areski Hadjaz",
            "Ga\u00e9tan Marceau",
            "Pierre Sav\u00e9ant",
            "Marc Schoenauer"
        ],
        "abstract": "Nowadays, huge efforts are made to modernize the air traffic management systems to cope with uncertainty, complexity and sub-optimality. An answer is to enhance the information sharing between the stakeholders. This paper introduces a framework that bridges the gap between air traffic management and air traffic control on the one hand, and bridges the gap between the ground, the approach and the en-route centers on the other hand. An original system is presented, that has three essential components: the trajectory models, the optimization process, and the monitoring process. The uncertainty of the trajectory is modeled with a Bayesian Network, where the nodes are associated to two types of random variables: the time of overflight on metering points of the airspace, and the traveling time of the routes linking these points. The resulting Bayesian Network covers the complete airspace, and Monte- Carlo simulations are done to estimate the probabilities of sector congestion and delays. On top of this trajectory model, an optimization process minimizes these probabilities by tuning the parameters of the Bayesian trajectory model related to overflight times on metering points. The last component is the monitoring process, that continuously updates the situation of the airspace, modifying the trajectories uncertainties according to actual positions of aircraft. After each update, a new optimal set of overflight times is computed, and can be communicated to the controllers as clearances for the aircraft pilots. The paper presents a formal specification of this global optimization problem, whose underlying rationale was derived with the help of air traffic controllers at Thales Air Systems.\n    ",
        "submission_date": "2012-12-17T00:00:00",
        "last_modified_date": "2012-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.3998",
        "title": "Online Learning for Ground Trajectory Prediction",
        "authors": [
            "Areski Hadjaz",
            "Ga\u00e9tan Marceau",
            "Pierre Sav\u00e9ant",
            "Marc Schoenauer"
        ],
        "abstract": "This paper presents a model based on an hybrid system to numerically simulate the climbing phase of an aircraft. This model is then used within a trajectory prediction tool. Finally, the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) optimization algorithm is used to tune five selected parameters, and thus improve the accuracy of the model. Incorporated within a trajectory prediction tool, this model can be used to derive the order of magnitude of the prediction error over time, and thus the domain of validity of the trajectory prediction. A first validation experiment of the proposed model is based on the errors along time for a one-time trajectory prediction at the take off of the flight with respect to the default values of the theoretical BADA model. This experiment, assuming complete information, also shows the limit of the model. A second experiment part presents an on-line trajectory prediction, in which the prediction is continuously updated based on the current aircraft position. This approach raises several issues, for which improvements of the basic model are proposed, and the resulting trajectory prediction tool shows statistically significantly more accurate results than those of the default model.\n    ",
        "submission_date": "2012-12-17T00:00:00",
        "last_modified_date": "2012-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.4799",
        "title": "Towards common-sense reasoning via conditional simulation: legacies of Turing in Artificial Intelligence",
        "authors": [
            "Cameron E. Freer",
            "Daniel M. Roy",
            "Joshua B. Tenenbaum"
        ],
        "abstract": "The problem of replicating the flexibility of human common-sense reasoning has captured the imagination of computer scientists since the early days of Alan Turing's foundational work on computation and the philosophy of artificial intelligence. In the intervening years, the idea of cognition as computation has emerged as a fundamental tenet of Artificial Intelligence (AI) and cognitive science. But what kind of computation is cognition?\n",
        "submission_date": "2012-12-19T00:00:00",
        "last_modified_date": "2013-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.5276",
        "title": "Multi-Objective AI Planning: Evaluating DAE-YAHSP on a Tunable Benchmark",
        "authors": [
            "Mostepha Redouane Khouadjia",
            "Marc Schoenauer",
            "Vincent Vidal",
            "Johann Dr\u00e9o",
            "Pierre Sav\u00e9ant"
        ],
        "abstract": "All standard AI planners to-date can only handle a single objective, and the only way for them to take into account multiple objectives is by aggregation of the objectives. Furthermore, and in deep contrast with the single objective case, there exists no benchmark problems on which to test the algorithms for multi-objective planning. Divide and Evolve (DAE) is an evolutionary planner that won the (single-objective) deterministic temporal satisficing track in the last International Planning Competition. Even though it uses intensively the classical (and hence single-objective) planner YAHSP, it is possible to turn DAE-YAHSP into a multi-objective evolutionary planner. A tunable benchmark suite for multi-objective planning is first proposed, and the performances of several variants of multi-objective DAE-YAHSP are compared on different instances of this benchmark, hopefully paving the road to further multi-objective competitions in AI planning.\n    ",
        "submission_date": "2012-12-20T00:00:00",
        "last_modified_date": "2012-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.5776",
        "title": "Improving problem solving by exploiting the concept of symmetry",
        "authors": [
            "M. A. El-Dosuky",
            "M. Z. Rashad",
            "T. T. Hamza",
            "A.H. EL-Bassiouny"
        ],
        "abstract": "We investigate the concept of symmetry and its role in problem solving. This paper first defines precisely the elements that constitute a \"problem\" and its \"solution,\" and gives several examples to illustrate these definitions. Given precise definitions of problems, it is relatively straightforward to construct a search process for finding solutions. Finally this paper attempts to exploit the concept of symmetry in improving problem solving.\n    ",
        "submission_date": "2012-12-23T00:00:00",
        "last_modified_date": "2013-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.6207",
        "title": "Irrespective Priority-Based Regular Properties of High-Intensity Virtual Environments",
        "authors": [
            "Kirill A. Sorudeykin"
        ],
        "abstract": "We have a lot of relation to the encoding and the Theory of Information, when considering thinking. This is a natural process and, at once, the complex thing we investigate. This always was a challenge - to understand how our mind works, and we are trying to find some universal models for this. A lot of ways have been considered so far, but we are looking for Something, we seek for approaches. And the goal is to find a consistent, noncontradictory view, which should at once be enough flexible in any dimensions to allow to represent various kinds of processes and environments, matters of different nature and diverse objects. Developing of such a model is the destination of this article.\n    ",
        "submission_date": "2012-12-21T00:00:00",
        "last_modified_date": "2012-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.6216",
        "title": "Generating Motion Patterns Using Evolutionary Computation in Digital Soccer",
        "authors": [
            "Masoud Amoozgar",
            "Daniel Khashabi",
            "Milad Heydarian",
            "Mohammad Nokhbeh",
            "Saeed Bagheri Shouraki"
        ],
        "abstract": "Dribbling an opponent player in digital soccer environment is an important practical problem in motion planning. It has special complexities which can be generalized to most important problems in other similar Multi Agent Systems. In this paper, we propose a hybrid computational geometry and evolutionary computation approach for generating motion trajectories to avoid a mobile obstacle. In this case an opponent agent is not only an obstacle but also one who tries to harden dribbling procedure. One characteristic of this approach is reducing process cost of online stage by transferring it to offline stage which causes increment in agents' performance. This approach breaks the problem into two offline and online stages. During offline stage the goal is to find desired trajectory using evolutionary computation and saving it as a trajectory plan. A trajectory plan consists of nodes which approximate information of each trajectory plan. In online stage, a linear interpolation along with Delaunay triangulation in xy-plan is applied to trajectory plan to retrieve desired action.\n    ",
        "submission_date": "2012-12-26T00:00:00",
        "last_modified_date": "2013-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.6519",
        "title": "Dialectics of Knowledge Representation in a Granular Rough Set Theory",
        "authors": [
            "A. Mani"
        ],
        "abstract": "The concepts of rough and definite objects are relatively more determinate than those of granules and granulation in general rough set theory (RST) [1]. Representation of rough objects can however depend on the dialectical relation between granulation and definiteness. In this research, we make this exact in the context of RST over proto-transitive approximation spaces. This approach can be directly extended to many other types of RST. These are used for formulating an extended concept of knowledge interpretation (KI)(relative the situation for classical RST) and the problem of knowledge representation (KR) is solved. These will be of direct interest in granular KR in RST as developed by the present author [2] and of rough objects in general. In [3], these have already been used for five different semantics by the present author. This is an extended version of [4] with key examples and more results.\n    ",
        "submission_date": "2012-12-28T00:00:00",
        "last_modified_date": "2013-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.6521",
        "title": "A Frequency-Domain Encoding for Neuroevolution",
        "authors": [
            "Jan Koutn\u00edk",
            "Juergen Schmidhuber",
            "Faustino Gomez"
        ],
        "abstract": "Neuroevolution has yet to scale up to complex reinforcement learning tasks that require large networks. Networks with many inputs (e.g. raw video) imply a very high dimensional search space if encoded directly. Indirect methods use a more compact genotype representation that is transformed into networks of potentially arbitrary size. In this paper, we present an indirect method where networks are encoded by a set of Fourier coefficients which are transformed into network weight matrices via an inverse Fourier-type transform. Because there often exist network solutions whose weight matrices contain regularity (i.e. adjacent weights are correlated), the number of coefficients required to represent these networks in the frequency domain is much smaller than the number of weights (in the same way that natural images can be compressed by ignore high-frequency components). This \"compressed\" encoding is compared to the direct approach where search is conducted in the weight space on the high-dimensional octopus arm task. The results show that representing networks in the frequency domain can reduce the search-space dimensionality by as much as two orders of magnitude, both accelerating convergence and yielding more general solutions.\n    ",
        "submission_date": "2012-12-28T00:00:00",
        "last_modified_date": "2012-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.6527",
        "title": "Discovering Basic Emotion Sets via Semantic Clustering on a Twitter Corpus",
        "authors": [
            "Eugene Yuta Bann"
        ],
        "abstract": "A plethora of words are used to describe the spectrum of human emotions, but how many emotions are there really, and how do they interact? Over the past few decades, several theories of emotion have been proposed, each based around the existence of a set of 'basic emotions', and each supported by an extensive variety of research including studies in facial expression, ethology, neurology and physiology. Here we present research based on a theory that people transmit their understanding of emotions through the language they use surrounding emotion keywords. Using a labelled corpus of over 21,000 tweets, six of the basic emotion sets proposed in existing literature were analysed using Latent Semantic Clustering (LSC), evaluating the distinctiveness of the semantic meaning attached to the emotional label. We hypothesise that the more distinct the language is used to express a certain emotion, then the more distinct the perception (including proprioception) of that emotion is, and thus more 'basic'. This allows us to select the dimensions best representing the entire spectrum of emotion. We find that Ekman's set, arguably the most frequently used for classifying emotions, is in fact the most semantically distinct overall. Next, taking all analysed (that is, previously proposed) emotion terms into account, we determine the optimal semantically irreducible basic emotion set using an iterative LSC algorithm. Our newly-derived set (Accepting, Ashamed, Contempt, Interested, Joyful, Pleased, Sleepy, Stressed) generates a 6.1% increase in distinctiveness over Ekman's set (Angry, Disgusted, Joyful, Sad, Scared). We also demonstrate how using LSC data can help visualise emotions. We introduce the concept of an Emotion Profile and briefly analyse compound emotions both visually and mathematically.\n    ",
        "submission_date": "2012-12-28T00:00:00",
        "last_modified_date": "2012-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.6550",
        "title": "Alternating Directions Dual Decomposition",
        "authors": [
            "Andre F. T. Martins",
            "Mario A. T. Figueiredo",
            "Pedro M. Q. Aguiar",
            "Noah A. Smith",
            "Eric P. Xing"
        ],
        "abstract": "We propose AD3, a new algorithm for approximate maximum a posteriori (MAP) inference on factor graphs based on the alternating directions method of multipliers. Like dual decomposition algorithms, AD3 uses worker nodes to iteratively solve local subproblems and a controller node to combine these local solutions into a global update. The key characteristic of AD3 is that each local subproblem has a quadratic regularizer, leading to a faster consensus than subgradient-based dual decomposition, both theoretically and in practice. We provide closed-form solutions for these AD3 subproblems for binary pairwise factors and factors imposing first-order logic constraints. For arbitrary factors (large or combinatorial), we introduce an active set method which requires only an oracle for computing a local MAP configuration, making AD3 applicable to a wide range of problems. Experiments on synthetic and realworld problems show that AD3 compares favorably with the state-of-the-art.\n    ",
        "submission_date": "2012-12-28T00:00:00",
        "last_modified_date": "2012-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.0216",
        "title": "Building Smart Communities with Cyber-Physical Systems",
        "authors": [
            "Feng Xia",
            "Jianhua Ma"
        ],
        "abstract": "There is a growing trend towards the convergence of cyber-physical systems (CPS) and social computing, which will lead to the emergence of smart communities composed of various objects (including both human individuals and physical things) that interact and cooperate with each other. These smart communities promise to enable a number of innovative applications and services that will improve the quality of life. This position paper addresses some opportunities and challenges of building smart communities characterized by cyber-physical and social intelligence.\n    ",
        "submission_date": "2011-12-31T00:00:00",
        "last_modified_date": "2011-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.0856",
        "title": "Complexity Classification in Infinite-Domain Constraint Satisfaction",
        "authors": [
            "Manuel Bodirsky"
        ],
        "abstract": "A constraint satisfaction problem (CSP) is a computational problem where the input consists of a finite set of variables and a finite set of constraints, and where the task is to decide whether there exists a satisfying assignment of values to the variables. Depending on the type of constraints that we allow in the input, a CSP might be tractable, or computationally hard. In recent years, general criteria have been discovered that imply that a CSP is polynomial-time tractable, or that it is NP-hard. Finite-domain CSPs have become a major common research focus of graph theory, artificial intelligence, and finite model theory. It turned out that the key questions for complexity classification of CSPs are closely linked to central questions in universal algebra.\n",
        "submission_date": "2012-01-04T00:00:00",
        "last_modified_date": "2019-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.0979",
        "title": "Sciduction: Combining Induction, Deduction, and Structure for Verification and Synthesis",
        "authors": [
            "Sanjit A. Seshia"
        ],
        "abstract": "Even with impressive advances in automated formal methods, certain problems in system verification and synthesis remain challenging. Examples include the verification of quantitative properties of software involving constraints on timing and energy consumption, and the automatic synthesis of systems from specifications. The major challenges include environment modeling, incompleteness in specifications, and the complexity of underlying decision problems.\n",
        "submission_date": "2012-01-04T00:00:00",
        "last_modified_date": "2012-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.1409",
        "title": "Interactive Character Posing by Sparse Coding",
        "authors": [
            "Ranch Y.Q. Lai",
            "Pong C. Yuen",
            "K.W. Lee",
            "J.H. Lai"
        ],
        "abstract": "Character posing is of interest in computer animation. It is difficult due to its dependence on inverse kinematics (IK) techniques and articulate property of human characters . To solve the IK problem, classical methods that rely on numerical solutions often suffer from the under-determination problem and can not guarantee naturalness. Existing data-driven methods address this problem by learning from motion capture data. When facing a large variety of poses however, these methods may not be able to capture the pose styles or be applicable in real-time environment. Inspired from the low-rank motion de-noising and completion model in \\cite{lai2011motion}, we propose a novel model for character posing based on sparse coding. Unlike conventional approaches, our model directly captures the pose styles in Euclidean space to provide intuitive training error measurements and facilitate pose synthesis. A pose dictionary is learned in training stage and based on it natural poses are synthesized to satisfy users' constraints . We compare our model with existing models for tasks of pose de-noising and completion. Experiments show our model obtains lower de-noising and completion error. We also provide User Interface(UI) examples illustrating that our model is effective for interactive character posing.\n    ",
        "submission_date": "2012-01-06T00:00:00",
        "last_modified_date": "2012-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.1657",
        "title": "A Split-Merge MCMC Algorithm for the Hierarchical Dirichlet Process",
        "authors": [
            "Chong Wang",
            "David M. Blei"
        ],
        "abstract": "The hierarchical Dirichlet process (HDP) has become an important Bayesian nonparametric model for grouped data, such as document collections. The HDP is used to construct a flexible mixed-membership model where the number of components is determined by the data. As for most Bayesian nonparametric models, exact posterior inference is intractable---practitioners use Markov chain Monte Carlo (MCMC) or variational inference. Inspired by the split-merge MCMC algorithm for the Dirichlet process (DP) mixture model, we describe a novel split-merge MCMC sampling algorithm for posterior inference in the HDP. We study its properties on both synthetic data and text corpora. We find that split-merge MCMC for the HDP can provide significant improvements over traditional Gibbs sampling, and we give some understanding of the data properties that give rise to larger improvements.\n    ",
        "submission_date": "2012-01-08T00:00:00",
        "last_modified_date": "2012-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.2241",
        "title": "Distance-Based Bias in Model-Directed Optimization of Additively Decomposable Problems",
        "authors": [
            "Martin Pelikan",
            "Mark W. Hauschild"
        ],
        "abstract": "For many optimization problems it is possible to define a distance metric between problem variables that correlates with the likelihood and strength of interactions between the variables. For example, one may define a metric so that the dependencies between variables that are closer to each other with respect to the metric are expected to be stronger than the dependencies between variables that are further apart. The purpose of this paper is to describe a method that combines such a problem-specific distance metric with information mined from probabilistic models obtained in previous runs of estimation of distribution algorithms with the goal of solving future problem instances of similar type with increased speed, accuracy and reliability. While the focus of the paper is on additively decomposable problems and the hierarchical Bayesian optimization algorithm, it should be straightforward to generalize the approach to other model-directed optimization techniques and other problem classes. Compared to other techniques for learning from experience put forward in the past, the proposed technique is both more practical and more broadly applicable.\n    ",
        "submission_date": "2012-01-11T00:00:00",
        "last_modified_date": "2012-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.2430",
        "title": "A Well-typed Lightweight Situation Calculus",
        "authors": [
            "Li Tan"
        ],
        "abstract": "Situation calculus has been widely applied in Artificial Intelligence related fields. This formalism is considered as a dialect of logic programming language and mostly used in dynamic domain modeling. However, type systems are hardly deployed in situation calculus in the literature. To achieve a correct and sound typed program written in situation calculus, adding typing elements into the current situation calculus will be quite helpful. In this paper, we propose to add more typing mechanisms to the current version of situation calculus, especially for three basic elements in situation calculus: situations, actions and objects, and then perform rigid type checking for existing situation calculus programs to find out the well-typed and ill-typed ones. In this way, type correctness and soundness in situation calculus programs can be guaranteed by type checking based on our type system. This modified version of a lightweight situation calculus is proved to be a robust and well-typed system.\n    ",
        "submission_date": "2012-01-11T00:00:00",
        "last_modified_date": "2012-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.2630",
        "title": "Hybrid GPS-GSM Localization of Automobile Tracking System",
        "authors": [
            "Mohammad A. Al-Khedher"
        ],
        "abstract": "An integrated GPS-GSM system is proposed to track vehicles using Google Earth application. The remote module has a GPS mounted on the moving vehicle to identify its current position, and to be transferred by GSM with other parameters acquired by the automobile's data port as an SMS to a recipient station. The received GPS coordinates are filtered using a Kalman filter to enhance the accuracy of measured position. After data processing, Google Earth application is used to view the current location and status of each vehicle. This goal of this system is to manage fleet, police automobiles distribution and car theft cautions.\n    ",
        "submission_date": "2012-01-12T00:00:00",
        "last_modified_date": "2012-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.3117",
        "title": "Design of Emergent and Adaptive Virtual Players in a War RTS Game",
        "authors": [
            "Jos\u00e9 A. Garc\u00eda Guti\u00e9rrez",
            "Carlos Cotta",
            "Antonio J. Fern\u00e1ndez-Leiva"
        ],
        "abstract": "Basically, in (one-player) war Real Time Strategy (wRTS) games a human player controls, in real time, an army consisting of a number of soldiers and her aim is to destroy the opponent's assets where the opponent is a virtual (i.e., non-human player controlled) player that usually consists of a pre-programmed decision-making script. These scripts have usually associated some well-known problems (e.g., predictability, non-rationality, repetitive behaviors, and sensation of artificial stupidity among others). This paper describes a method for the automatic generation of virtual players that adapt to the player skills; this is done by building initially a model of the player behavior in real time during the game, and further evolving the virtual player via this model in-between two games. The paper also shows preliminary results obtained on a one player wRTS game constructed specifically for experimentation.\n    ",
        "submission_date": "2012-01-15T00:00:00",
        "last_modified_date": "2012-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.3880",
        "title": "Modelling and simulation of complex systems: an approach based on multi-level agents",
        "authors": [
            "Alain-J\u00e9r\u00f4me Foug\u00e8res"
        ],
        "abstract": "A complex system is made up of many components with many interactions. So the design of systems such as simulation systems, cooperative systems or assistance systems includes a very accurate modelling of interactional and communicational levels. The agent-based approach provides an adapted abstraction level for this problem. After having studied the organizational context and communicative capacities of agentbased systems, to simulate the reorganization of a flexible manufacturing, to regulate an urban transport system, and to simulate an epidemic detection system, our thoughts on the interactional level were inspired by human-machine interface models, especially those in \"cognitive engineering\". To provide a general framework for agent-based complex systems modelling, we then proposed a scale of four behaviours that agents may adopt in their complex systems (reactive, routine, cognitive, and collective). To complete the description of multi-level agent models, which is the focus of this paper, we illustrate our modelling and discuss our ongoing work on each level.\n    ",
        "submission_date": "2012-01-18T00:00:00",
        "last_modified_date": "2012-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.3883",
        "title": "Dynamic Shared Context Processing in an E-Collaborative Learning Environment",
        "authors": [
            "Jing Peng",
            "Alain-J\u00e9r\u00f4me Foug\u00e8res",
            "Samuel Deniaud",
            "Michel Ferney"
        ],
        "abstract": "In this paper, we propose a dynamic shared context processing method based on DSC (Dynamic Shared Context) model, applied in an e-collaborative learning environment. Firstly, we present the model. This is a way to measure the relevance between events and roles in collaborative environments. With this method, we can share the most appropriate event information for each role instead of sharing all information to all roles in a collaborative work environment. Then, we apply and verify this method in our project with Google App supported e-learning collaborative environment. During this experiment, we compared DSC method measured relevance of events and roles to manual measured relevance. And we describe the favorable points from this comparison and our finding. Finally, we discuss our future research of a hybrid DSC method to make dynamical information shared more effective in a collaborative work environment.\n    ",
        "submission_date": "2012-01-18T00:00:00",
        "last_modified_date": "2012-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.4210",
        "title": "Collaborative Personalized Web Recommender System using Entropy based Similarity Measure",
        "authors": [
            "Harita Mehta",
            "Shveta Kundra Bhatia",
            "Punam Bedi",
            "V. S. Dixit"
        ],
        "abstract": "On the internet, web surfers, in the search of information, always strive for recommendations. The solutions for generating recommendations become more difficult because of exponential increase in information domain day by day. In this paper, we have calculated entropy based similarity between users to achieve solution for scalability problem. Using this concept, we have implemented an online user based collaborative web recommender system. In this model based collaborative system, the user session is divided into two levels. Entropy is calculated at both the levels. It is shown that from the set of valuable recommenders obtained at level I; only those recommenders having lower entropy at level II than entropy at level I, served as trustworthy recommenders. Finally, top N recommendations are generated from such trustworthy recommenders for an online user.\n    ",
        "submission_date": "2012-01-20T00:00:00",
        "last_modified_date": "2012-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.4239",
        "title": "Dynamic Decision Making for Graphical Models Applied to Oil Exploration",
        "authors": [
            "Gabriele Martinelli",
            "Jo Eidsvik",
            "Ragnar Hauge"
        ],
        "abstract": "This paper has been withdrawn by the authors. We present a framework for sequential decision making in problems described by graphical models. The setting is given by dependent discrete random variables with associated costs or revenues. In our examples, the dependent variables are the potential outcomes (oil, gas or dry) when drilling a petroleum well. The goal is to develop an optimal selection strategy that incorporates a chosen utility function within an approximated dynamic programming scheme. We propose and compare different approximations, from simple heuristics to more complex iterative schemes, and we discuss their computational properties. We apply our strategies to oil exploration over multiple prospects modeled by a directed acyclic graph, and to a reservoir drilling decision problem modeled by a Markov random field. The results show that the suggested strategies clearly improve the simpler intuitive constructions, and this is useful when selecting exploration policies.\n    ",
        "submission_date": "2012-01-20T00:00:00",
        "last_modified_date": "2013-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.4342",
        "title": "A Pareto-metaheuristic for a bi-objective winner determination problem in a combinatorial reverse auction",
        "authors": [
            "Tobias Buer",
            "Herbert Kopfer"
        ],
        "abstract": "The bi-objective winner determination problem (2WDP-SC) of a combinatorial procurement auction for transport contracts is characterized by a set B of bundle bids, with each bundle bid b in B consisting of a bidding carrier c_b, a bid price p_b, and a set tau_b transport contracts which is a subset of the set T of tendered transport contracts. Additionally, the transport quality q_{t,c_b} is given which is expected to be realized when a transport contract t is executed by a carrier c_b. The task of the auctioneer is to find a set X of winning bids (X subset B), such that each transport contract is part of at least one winning bid, the total procurement costs are minimized, and the total transport quality is maximized. This article presents a metaheuristic approach for the 2WDP-SC which integrates the greedy randomized adaptive search procedure with a two-stage candidate component selection procedure, large neighborhood search, and self-adaptive parameter setting in order to find a competitive set of non-dominated solutions. The heuristic outperforms all existing approaches. For seven small benchmark instances, the heuristic is the sole approach that finds all Pareto-optimal solutions. For 28 out of 30 large instances, none of the existing approaches is able to compute a solution that dominates a solution found by the proposed heuristic.\n    ",
        "submission_date": "2012-01-20T00:00:00",
        "last_modified_date": "2013-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.5217",
        "title": "Unsupervised Classification Using Immune Algorithm",
        "authors": [
            "M. T. Al-Muallim",
            "R. El-Kouatly"
        ],
        "abstract": "Unsupervised classification algorithm based on clonal selection principle named Unsupervised Clonal Selection Classification (UCSC) is proposed in this paper. The new proposed algorithm is data driven and self-adaptive, it adjusts its parameters to the data to make the classification operation as fast as possible. The performance of UCSC is evaluated by comparing it with the well known K-means algorithm using several artificial and real-life data sets. The experiments show that the proposed UCSC algorithm is more reliable and has high classification precision comparing to traditional classification methods such as K-means.\n    ",
        "submission_date": "2012-01-25T00:00:00",
        "last_modified_date": "2012-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.5346",
        "title": "Tableau-based decision procedure for the multi-agent epistemic logic with all coalitional operators for common and distributed knowledge",
        "authors": [
            "Mai Ajspur",
            "Valentin Goranko",
            "Dmitry Shkatov"
        ],
        "abstract": "We develop a conceptually clear, intuitive, and feasible decision procedure for testing satisfiability in the full multi-agent epistemic logic CMAEL(CD) with operators for common and distributed knowledge for all coalitions of agents mentioned in the language. To that end, we introduce Hintikka structures for CMAEL(CD) and prove that satisfiability in such structures is equivalent to satisfiability in standard models. Using that result, we design an incremental tableau-building procedure that eventually constructs a satisfying Hintikka structure for every satisfiable input set of formulae of CMAEL(CD) and closes for every unsatisfiable input set of formulae.\n    ",
        "submission_date": "2012-01-25T00:00:00",
        "last_modified_date": "2012-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.5946",
        "title": "Feature selection using nearest attributes",
        "authors": [
            "Alex Pappachen James",
            "Sima Dimitrijev"
        ],
        "abstract": "Feature selection is an important problem in high-dimensional data analysis and classification. Conventional feature selection approaches focus on detecting the features based on a redundancy criterion using learning and feature searching schemes. In contrast, we present an approach that identifies the need to select features based on their discriminatory ability among classes. Area of overlap between inter-class and intra-class distances resulting from feature to feature comparison of an attribute is used as a measure of discriminatory ability of the feature. A set of nearest attributes in a pattern having the lowest area of overlap within a degree of tolerance defined by a selection threshold is selected to represent the best available discriminable features. State of the art recognition results are reported for pattern classification problems by using the proposed feature selection scheme with the nearest neighbour classifier. These results are reported with benchmark databases having high dimensional feature vectors in the problems involving images and micro array data.\n    ",
        "submission_date": "2012-01-28T00:00:00",
        "last_modified_date": "2012-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.5947",
        "title": "Examplers based image fusion features for face recognition",
        "authors": [
            "Alex Pappachen James",
            "Sima Dimitrijev"
        ],
        "abstract": "Examplers of a face are formed from multiple gallery images of a person and are used in the process of classification of a test image. We incorporate such examplers in forming a biologically inspired local binary decisions on similarity based face recognition method. As opposed to single model approaches such as face averages the exampler based approach results in higher recognition accu- racies and stability. Using multiple training samples per person, the method shows the following recognition accuracies: 99.0% on AR, 99.5% on FERET, 99.5% on ORL, 99.3% on EYALE, 100.0% on YALE and 100.0% on CALTECH face databases. In addition to face recognition, the method also detects the natural variability in the face images which can find application in automatic tagging of face images.\n    ",
        "submission_date": "2012-01-28T00:00:00",
        "last_modified_date": "2012-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.0255",
        "title": "Reasoning about Unreliable Actions",
        "authors": [
            "Graham White"
        ],
        "abstract": "We analyse the philosopher Davidson's semantics of actions, using a strongly typed logic with contexts given by sets of partial equations between the outcomes of actions. This provides a perspicuous and elegant treatment of reasoning about action, analogous to Reiter's work on artificial intelligence. We define a sequent calculus for this logic, prove cut elimination, and give a semantics based on fibrations over partial cartesian categories: we give a structure theory for such fibrations. The existence of lax comma objects is necessary for the proof of cut elimination, and we give conditions on the domain fibration of a partial cartesian category for such comma objects to exist.\n    ",
        "submission_date": "2012-02-01T00:00:00",
        "last_modified_date": "2012-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.0515",
        "title": "High-Dimensional Feature Selection by Feature-Wise Kernelized Lasso",
        "authors": [
            "Makoto Yamada",
            "Wittawat Jitkrittum",
            "Leonid Sigal",
            "Eric P. Xing",
            "Masashi Sugiyama"
        ],
        "abstract": "The goal of supervised feature selection is to find a subset of input features that are responsible for predicting output values. The least absolute shrinkage and selection operator (Lasso) allows computationally efficient feature selection based on linear dependency between input features and output values. In this paper, we consider a feature-wise kernelized Lasso for capturing non-linear input-output dependency. We first show that, with particular choices of kernel functions, non-redundant features with strong statistical dependence on output values can be found in terms of kernel-based independence measures. We then show that the globally optimal solution can be efficiently computed; this makes the approach scalable to high-dimensional problems. The effectiveness of the proposed method is demonstrated through feature selection experiments with thousands of features.\n    ",
        "submission_date": "2012-02-02T00:00:00",
        "last_modified_date": "2019-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.0862",
        "title": "e-Valuate: A Two-player Game on Arithmetic Expressions -- An Update",
        "authors": [
            "Sarang Aravamuthan",
            "Biswajit Ganguly"
        ],
        "abstract": "e-Valuate is a game on arithmetic expressions. The players have contrasting roles of maximizing and minimizing the given expression. The maximizer proposes values and the minimizer substitutes them for variables of his choice. When the expression is fully instantiated, its value is compared with a certain minimax value that would result if the players played to their optimal strategies. The winner is declared based on this comparison.\n",
        "submission_date": "2012-02-04T00:00:00",
        "last_modified_date": "2014-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.0914",
        "title": "Type-elimination-based reasoning for the description logic SHIQbs using decision diagrams and disjunctive datalog",
        "authors": [
            "Sebastian Rudolph",
            "Markus Kr\u00f6tzsch",
            "Pascal Hitzler"
        ],
        "abstract": " We propose a novel, type-elimination-based method for reasoning in the description logic SHIQbs including DL-safe rules. To this end, we first establish a knowledge compilation method converting the terminological part of an ALCIb knowledge base into an ordered binary decision diagram (OBDD) which represents a canonical model. This OBDD can in turn be transformed into disjunctive Datalog and merged with the assertional part of the knowledge base in order to perform combined reasoning. In order to leverage our technique for full SHIQbs, we provide a stepwise reduction from SHIQbs to ALCIb that preserves satisfiability and entailment of positive and negative ground facts. The proposed technique is shown to be worst case optimal w.r.t. combined and data complexity and easily admits extensions with ground conjunctive queries.\n    ",
        "submission_date": "2012-02-04T00:00:00",
        "last_modified_date": "2012-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.0984",
        "title": "OWL: Yet to arrive on the Web of Data?",
        "authors": [
            "Birte Glimm",
            "Aidan Hogan",
            "Markus Kr\u00f6tzsch",
            "Axel Polleres"
        ],
        "abstract": "Seven years on from OWL becoming a W3C recommendation, and two years on from the more recent OWL 2 W3C recommendation, OWL has still experienced only patchy uptake on the Web. Although certain OWL features (like owl:sameAs) are very popular, other features of OWL are largely neglected by publishers in the Linked Data world. This may suggest that despite the promise of easy implementations and the proposal of tractable profiles suggested in OWL's second version, there is still no \"right\" standard fragment for the Linked Data community. In this paper, we (1) analyse uptake of OWL on the Web of Data, (2) gain insights into the OWL fragment that is actually used/usable on the Web, where we arrive at the conclusion that this fragment is likely to be a simplified profile based on OWL RL, (3) propose and discuss such a new fragment, which we call OWL LD (for Linked Data).\n    ",
        "submission_date": "2012-02-01T00:00:00",
        "last_modified_date": "2012-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.2577",
        "title": "Citizen Science: Contributions to Astronomy Research",
        "authors": [
            "Carol Christian",
            "Chris Lintott",
            "Arfon Smith",
            "Lucy Fortson",
            "Steven Bamford"
        ],
        "abstract": "The contributions of everyday individuals to significant research has grown dramatically beyond the early days of classical birdwatching and endeavors of amateurs of the 19th century. Now people who are casually interested in science can participate directly in research covering diverse scientific fields. Regarding astronomy, volunteers, either as individuals or as networks of people, are involved in a variety of types of studies. Citizen Science is intuitive, engaging, yet necessarily robust in its adoption of sci-entific principles and methods. Herein, we discuss Citizen Science, focusing on fully participatory projects such as Zooniverse (by several of the au-thors CL, AS, LF, SB), with mention of other programs. In particular, we make the case that citizen science (CS) can be an important aspect of the scientific data analysis pipelines provided to scientists by observatories.\n    ",
        "submission_date": "2012-02-12T00:00:00",
        "last_modified_date": "2012-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.2745",
        "title": "Multi-column Deep Neural Networks for Image Classification",
        "authors": [
            "Dan Cire\u015fan",
            "Ueli Meier",
            "Juergen Schmidhuber"
        ],
        "abstract": "Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.\n    ",
        "submission_date": "2012-02-13T00:00:00",
        "last_modified_date": "2012-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.2770",
        "title": "Multi-Level Error-Resilient Neural Networks with Learning",
        "authors": [
            "Amir Hesam Salavati",
            "Amin Karbasi"
        ],
        "abstract": "The problem of neural network association is to retrieve a previously memorized pattern from its noisy version using a network of neurons. An ideal neural network should include three components simultaneously: a learning algorithm, a large pattern retrieval capacity and resilience against noise. Prior works in this area usually improve one or two aspects at the cost of the third.\n",
        "submission_date": "2012-02-13T00:00:00",
        "last_modified_date": "2012-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3046",
        "title": "Segmentation of Offline Handwritten Bengali Script",
        "authors": [
            "Subhadip Basu",
            "Chitrita Chaudhuri",
            "Mahantapas Kundu",
            "Mita Nasipuri",
            "Dipak K. Basu"
        ],
        "abstract": "Character segmentation has long been one of the most critical areas of optical character recognition process. Through this operation, an image of a sequence of characters, which may be connected in some cases, is decomposed into sub-images of individual alphabetic symbols. In this paper, segmentation of cursive handwritten script of world's fourth popular language, Bengali, is considered. Unlike English script, Bengali handwritten characters and its components often encircle the main character, making the conventional segmentation methodologies inapplicable. Experimental results, using the proposed segmentation technique, on sample cursive handwritten data containing 218 ideal segmentation points show a success rate of 97.7%. Further feature-analysis on these segments may lead to actual recognition of handwritten cursive Bengali script.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3538",
        "title": "Refinement Modal Logic",
        "authors": [
            "Laura Bozzelli",
            "Hans van Ditmarsch",
            "Tim French",
            "James Hales",
            "Sophie Pinchinat"
        ],
        "abstract": "In this paper we present {\\em refinement modal logic}. A refinement is like a bisimulation, except that from the three relational requirements only `atoms' and `back' need to be satisfied. Our logic contains a new operator 'all' in addition to the standard modalities 'box' for each agent. The operator 'all' acts as a quantifier over the set of all refinements of a given model. As a variation on a bisimulation quantifier, this refinement operator or refinement quantifier 'all' can be seen as quantifying over a variable not occurring in the formula bound by it. The logic combines the simplicity of multi-agent modal logic with some powers of monadic second-order quantification. We present a sound and complete axiomatization of multi-agent refinement modal logic. We also present an extension of the logic to the modal mu-calculus, and an axiomatization for the single-agent version of this logic. Examples and applications are also discussed: to software verification and design (the set of agents can also be seen as a set of actions), and to dynamic epistemic logic. We further give detailed results on the complexity of satisfiability, and on succinctness.\n    ",
        "submission_date": "2012-02-16T00:00:00",
        "last_modified_date": "2013-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3700",
        "title": "Solving Cooperative Reliability Games",
        "authors": [
            "Yoram Bachrach",
            "Reshef Meir",
            "Michal Feldman",
            "Moshe Tennenholtz"
        ],
        "abstract": "Cooperative games model the allocation of profit from joint actions, following considerations such as stability and fairness. We propose the reliability extension of such games, where agents may fail to participate in the game. In the reliability extension, each agent only \"survives\" with a certain probability, and a coalition's value is the probability that its surviving members would be a winning coalition in the base game. We study prominent solution concepts in such games, showing how to approximate the Shapley value and how to compute the core in games with few agent types. We also show that applying the reliability extension may stabilize the game, making the core non-empty even when the base game has an empty core.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3701",
        "title": "Active Diagnosis via AUC Maximization: An Efficient Approach for Multiple Fault Identification in Large Scale, Noisy Networks",
        "authors": [
            "Gowtham Bellala",
            "Jason Stanley",
            "Clayton Scott",
            "Suresh K. Bhavnani"
        ],
        "abstract": "The problem of active diagnosis arises in several applications such as disease diagnosis, and fault diagnosis in computer networks, where the goal is to rapidly identify the binary states of a set of objects (e.g., faulty or working) by sequentially selecting, and observing, (noisy) responses to binary valued queries. Current algorithms in this area rely on loopy belief propagation for active query selection. These algorithms have an exponential time complexity, making them slow and even intractable in large networks. We propose a rank-based greedy algorithm that sequentially chooses queries such that the area under the ROC curve of the rank-based output is maximized. The AUC criterion allows us to make a simplifying assumption that significantly reduces the complexity of active query selection (from exponential to near quadratic), with little or no compromise on the performance quality.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3703",
        "title": "Factored Filtering of Continuous-Time Systems",
        "authors": [
            "E. Busra Celikkaya",
            "Christian R. Shelton",
            "William Lam"
        ],
        "abstract": "We consider filtering for a continuous-time, or asynchronous, stochastic system where the full distribution over states is too large to be stored or calculated. We assume that the rate matrix of the system can be compactly represented and that the belief distribution is to be approximated as a product of marginals. The essential computation is the matrix exponential. We look at two different methods for its computation: ODE integration and uniformization of the Taylor expansion. For both we consider approximations in which only a factored belief state is maintained. For factored uniformization we demonstrate that the KL-divergence of the filtering is bounded. Our experimental results confirm our factored uniformization performs better than previously suggested uniformization methods and the mean field algorithm.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3705",
        "title": "Filtered Fictitious Play for Perturbed Observation Potential Games and Decentralised POMDPs",
        "authors": [
            "Archie C. Chapman",
            "Simon A. Williamson",
            "Nicholas R. Jennings"
        ],
        "abstract": "Potential games and decentralised partially observable MDPs (Dec-POMDPs) are two commonly used models of multi-agent interaction, for static optimisation and sequential decisionmaking settings, respectively. In this paper we introduce filtered fictitious play for solving repeated potential games in which each player's observations of others' actions are perturbed by random noise, and use this algorithm to construct an online learning method for solving Dec-POMDPs. Specifically, we prove that noise in observations prevents standard fictitious play from converging to Nash equilibrium in potential games, which also makes fictitious play impractical for solving Dec-POMDPs. To combat this, we derive filtered fictitious play, and provide conditions under which it converges to a Nash equilibrium in potential games with noisy observations. We then use filtered fictitious play to construct a solver for Dec-POMDPs, and demonstrate our new algorithm's performance in a box pushing problem. Our results show that we consistently outperform the state-of-the-art Dec-POMDP solver by an average of 100% across the range of noise in the observation function.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3706",
        "title": "A Framework for Optimizing Paper Matching",
        "authors": [
            "Laurent Charlin",
            "Richard S. Zemel",
            "Craig Boutilier"
        ],
        "abstract": "At the heart of many scientific conferences is the problem of matching submitted papers to suitable reviewers. Arriving at a good assignment is a major and important challenge for any conference organizer. In this paper we propose a framework to optimize paper-to-reviewer assignments. Our framework uses suitability scores to measure pairwise affinity between papers and reviewers. We show how learning can be used to infer suitability scores from a small set of provided scores, thereby reducing the burden on reviewers and organizers. We frame the assignment problem as an integer program and propose several variations for the paper-to-reviewer matching domain. We also explore how learning and matching interact. Experiments on two conference data sets examine the performance of several learning methods as well as the effectiveness of the matching formulations.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3710",
        "title": "Strictly Proper Mechanisms with Cooperating Players",
        "authors": [
            "SangIn Chun",
            "Ross D. Shachter"
        ],
        "abstract": "Prediction markets provide an efficient means to assess uncertain quantities from forecasters. Traditional and competitive strictly proper scoring rules have been shown to incentivize players to provide truthful probabilistic forecasts. However, we show that when those players can cooperate, these mechanisms can instead discourage them from reporting what they really believe. When players with different beliefs are able to cooperate and form a coalition, these mechanisms admit arbitrage and there is a report that will always pay coalition members more than their truthful forecasts. If the coalition were created by an intermediary, such as a web portal, the intermediary would be guaranteed a profit.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3720",
        "title": "Efficient Inference in Markov Control Problems",
        "authors": [
            "Thomas Furmston",
            "David Barber"
        ],
        "abstract": "Markov control algorithms that perform smooth, non-greedy updates of the policy have been shown to be very general and versatile, with policy gradient and Expectation Maximisation algorithms being particularly popular. For these algorithms, marginal inference of the reward weighted trajectory distribution is required to perform policy updates. We discuss a new exact inference algorithm for these marginals in the finite horizon case that is more efficient than the standard approach based on classical forward-backward recursions. We also provide a principled extension to infinite horizon Markov Decision Problems that explicitly accounts for an infinite horizon. This extension provides a novel algorithm for both policy gradients and Expectation Maximisation in infinite horizon problems.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3722",
        "title": "Hierarchical Affinity Propagation",
        "authors": [
            "Inmar Givoni",
            "Clement Chung",
            "Brendan J. Frey"
        ],
        "abstract": "Affinity propagation is an exemplar-based clustering algorithm that finds a set of data-points that best exemplify the data, and associates each datapoint with one exemplar. We extend affinity propagation in a principled way to solve the hierarchical clustering problem, which arises in a variety of domains including biology, sensor networks and decision making in operational research. We derive an inference algorithm that operates by propagating information up and down the hierarchy, and is efficient despite the high-order potentials required for the graphical model formulation.\nWe demonstrate that our method outperforms greedy techniques that cluster one layer at a time. We show that on an artificial dataset designed to mimic the HIV-strain mutation dynamics, our method outperforms related methods. For real HIV sequences, where the ground truth is not available, we show our method achieves better results, in terms of the underlying objective function, and show the results correspond meaningfully to geographical location and strain subtypes. Finally we report results on using the method for the analysis of mass spectra, showing it performs favorably compared to state-of-the-art methods.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3732",
        "title": "Sum-Product Networks: A New Deep Architecture",
        "authors": [
            "Hoifung Poon",
            "Pedro Domingos"
        ],
        "abstract": "The key limiting factor in graphical model inference and learning is the complexity of the partition function. We thus ask the question: what are general conditions under which the partition function is tractable? The answer leads to a new kind of deep architecture, which we call sum-product networks (SPNs). SPNs are directed acyclic graphs with variables as leaves, sums and products as internal nodes, and weighted edges. We show that if an SPN is complete and consistent it represents the partition function and all marginals of some graphical model, and give semantics to its nodes. Essentially all tractable graphical models can be cast as SPNs, but SPNs are also strictly more general. We then propose learning algorithms for SPNs, based on backpropagation and EM. Experiments show that inference and learning with SPNs can be both faster and more accurate than with standard deep networks. For example, SPNs perform image completion better than state-of-the-art deep networks for this task. SPNs also have intriguing potential connections to the architecture of the cortex.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3734",
        "title": "Efficient Probabilistic Inference with Partial Ranking Queries",
        "authors": [
            "Jonathan Huang",
            "Ashish Kapoor",
            "Carlos E. Guestrin"
        ],
        "abstract": "Distributions over rankings are used to model data in various settings such as preference analysis and political elections. The factorial size of the space of rankings, however, typically forces one to make structural assumptions, such as smoothness, sparsity, or probabilistic independence about these underlying distributions. We approach the modeling problem from the computational principle that one should make structural assumptions which allow for efficient calculation of typical probabilistic queries. For ranking models, \"typical\" queries predominantly take the form of partial ranking queries (e.g., given a user's top-k favorite movies, what are his preferences over remaining movies?). In this paper, we argue that riffled independence factorizations proposed in recent literature [7, 8] are a natural structural assumption for ranking distributions, allowing for particularly efficient processing of partial ranking queries.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3738",
        "title": "Learning Determinantal Point Processes",
        "authors": [
            "Alex Kulesza",
            "Ben Taskar"
        ],
        "abstract": "Determinantal point processes (DPPs), which arise in random matrix theory and quantum physics, are natural models for subset selection problems where diversity is preferred. Among many remarkable properties, DPPs offer tractable algorithms for exact inference, including computing marginal probabilities and sampling; however, an important open question has been how to learn a DPP from labeled training data. In this paper we propose a natural feature-based parameterization of conditional DPPs, and show how it leads to a convex and efficient learning formulation. We analyze the relationship between our model and binary Markov random fields with repulsive potentials, which are qualitatively similar but computationally intractable. Finally, we apply our approach to the task of extractive summarization, where the goal is to choose a small subset of sentences conveying the most important information from a set of documents. In this task there is a fundamental tradeoff between sentences that are highly relevant to the collection as a whole, and sentences that are diverse and not repetitive. Our parameterization allows us to naturally balance these two characteristics. We evaluate our system on data from the DUC 2003/04 multi-document summarization task, achieving state-of-the-art results.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3742",
        "title": "Variational Algorithms for Marginal MAP",
        "authors": [
            "Qiang Liu",
            "Alexander T. Ihler"
        ],
        "abstract": "Marginal MAP problems are notoriously difficult tasks for graphical models. We derive a general variational framework for solving marginal MAP problems, in which we apply analogues of the Bethe, tree-reweighted, and mean field approximations. We then derive a \"mixed\" message passing algorithm and a convergent alternative using CCCP to solve the BP-type approximations. Theoretically, we give conditions under which the decoded solution is a global or local optimum, and obtain novel upper bounds on solutions. Experimentally we demonstrate that our algorithms outperform related approaches. We also show that EM and variational EM comprise a special case of our framework.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3751",
        "title": "Dynamic Mechanism Design for Markets with Strategic Resources",
        "authors": [
            "Swaprava Nath",
            "Onno Zoeter",
            "Yadati Narahari",
            "Christopher R. Dance"
        ],
        "abstract": "The assignment of tasks to multiple resources becomes an interesting game theoretic problem, when both the task owner and the resources are strategic. In the classical, nonstrategic setting, where the states of the tasks and resources are observable by the controller, this problem is that of finding an optimal policy for a Markov decision process (MDP). When the states are held by strategic agents, the problem of an efficient task allocation extends beyond that of solving an MDP and becomes that of designing a mechanism. Motivated by this fact, we propose a general mechanism which decides on an allocation rule for the tasks and resources and a payment rule to incentivize agents' participation and truthful reports. In contrast to related dynamic strategic control problems studied in recent literature, the problem studied here has interdependent values: the benefit of an allocation to the task owner is not simply a function of the characteristics of the task itself and the allocation, but also of the state of the resources. We introduce a dynamic extension of Mezzetti's two phase mechanism for interdependent valuations. In this changed setting, the proposed dynamic mechanism is efficient, within period ex-post incentive compatible, and within period ex-post individually rational.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3755",
        "title": "Iterated risk measures for risk-sensitive Markov decision processes with discounted cost",
        "authors": [
            "Takayuki Osogami"
        ],
        "abstract": "We demonstrate a limitation of discounted expected utility, a standard approach for representing the preference to risk when future cost is discounted. Specifically, we provide an example of the preference of a decision maker that appears to be rational but cannot be represented with any discounted expected utility. A straightforward modification to discounted expected utility leads to inconsistent decision making over time. We will show that an iterated risk measure can represent the preference that cannot be represented by any discounted expected utility and that the decisions based on the iterated risk measure are consistent over time.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3756",
        "title": "Price Updating in Combinatorial Prediction Markets with Bayesian Networks",
        "authors": [
            "David M. Pennock",
            "Lirong Xia"
        ],
        "abstract": "To overcome the #P-hardness of computing/updating prices in logarithm market scoring rule-based (LMSR-based) combinatorial prediction markets, Chen et al. [5] recently used a simple Bayesian network to represent the prices of securities in combinatorial predictionmarkets for tournaments, and showed that two types of popular securities are structure preserving. In this paper, we significantly extend this idea by employing Bayesian networks in general combinatorial prediction markets. We reveal a very natural connection between LMSR-based combinatorial prediction markets and probabilistic belief aggregation,which leads to a complete characterization of all structure preserving securities for decomposable network structures. Notably, the main results by Chen et al. [5] are corollaries of our characterization. We then prove that in order for a very basic set of securities to be structure preserving, the graph of the Bayesian network must be decomposable. We also discuss some approximation techniques for securities that are not structure preserving.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3768",
        "title": "The Structure of Signals: Causal Interdependence Models for Games of Incomplete Information",
        "authors": [
            "Michael P. Wellman",
            "Lu Hong",
            "Scott E. Page"
        ],
        "abstract": "Traditional economic models typically treat private information, or signals, as generated from some underlying state. Recent work has explicated alternative models, where signals correspond to interpretations of available information. We show that the difference between these formulations can be sharply cast in terms of causal dependence structure, and employ graphical models to illustrate the distinguishing characteristics. The graphical representation supports inferences about signal patterns in the interpreted framework, and suggests how results based on the generated model can be extended to more general situations. Specific insights about bidding games in classical auction mechanisms derive from qualitative graphical models.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3782",
        "title": "Graphical Models for Bandit Problems",
        "authors": [
            "Kareem Amin",
            "Michael Kearns",
            "Umar Syed"
        ],
        "abstract": "We introduce a rich class of graphical models for multi-armed bandit problems that permit both the state or context space and the action space to be very large, yet succinctly specify the payoffs for any context-action pair. Our main result is an algorithm for such models whose regret is bounded by the number of parameters and whose running time depends only on the treewidth of the graph substructure induced by the action space.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.4144",
        "title": "Towards an efficient prover for the C1 paraconsistent logic",
        "authors": [
            "Adolfo Neto",
            "Celso A. A. Kaestner",
            "Marcelo Finger"
        ],
        "abstract": "The KE inference system is a tableau method developed by Marco Mondadori which was presented as an improvement, in the computational efficiency sense, over Analytic Tableaux. In the literature, there is no description of a theorem prover based on the KE method for the C1 paraconsistent logic. Paraconsistent logics have several applications, such as in robot control and medicine. These applications could benefit from the existence of such a prover. We present a sound and complete KE system for C1, an informal specification of a strategy for the C1 prover as well as problem families that can be used to evaluate provers for C1. The C1 KE system and the strategy described in this paper will be used to implement a KE based prover for C1, which will be useful for those who study and apply paraconsistent logics.\n    ",
        "submission_date": "2012-02-19T00:00:00",
        "last_modified_date": "2012-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.4174",
        "title": "Perception Lie Paradox: Mathematically Proved Uncertainty about Humans Perception Similarity",
        "authors": [
            "Ahmed M. Mahran"
        ],
        "abstract": "Agents' judgment depends on perception and previous knowledge. Assuming that previous knowledge depends on perception, we can say that judgment depends on perception. So, if judgment depends on perception, can agents judge that they have the same perception? In few words, this is the addressed paradox through this document. While illustrating on the paradox, it's found that to reach agreement in communication, it's not necessary for parties to have the same perception however the necessity is to have perception correspondence. The attempted solution to this paradox reveals a potential uncertainty in judging the matter thus supporting the skeptical view of the problem. Moreover, relating perception to intelligence, the same uncertainty is inherited by judging the level of intelligence of an agent compared to others not necessarily from the same kind (e.g. machine intelligence compared to human intelligence). Using a proposed simple mathematical model for perception and action, a tool is developed to construct scenarios, and the problem is addressed mathematically such that conclusions are drawn systematically based on mathematically defined properties. When it comes to formalization, philosophical arguments and views become more visible and explicit.\n    ",
        "submission_date": "2012-02-19T00:00:00",
        "last_modified_date": "2012-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.4177",
        "title": "$Q$- and $A$-Learning Methods for Estimating Optimal Dynamic Treatment Regimes",
        "authors": [
            "Phillip J. Schulte",
            "Anastasios A. Tsiatis",
            "Eric B. Laber",
            "Marie Davidian"
        ],
        "abstract": "In clinical practice, physicians make a series of treatment decisions over the course of a patient's disease based on his/her baseline and evolving characteristics. A dynamic treatment regime is a set of sequential decision rules that operationalizes this process. Each rule corresponds to a decision point and dictates the next treatment action based on the accrued information. Using existing data, a key goal is estimating the optimal regime, that, if followed by the patient population, would yield the most favorable outcome on average. Q- and A-learning are two main approaches for this purpose. We provide a detailed account of these methods, study their performance, and illustrate them using data from a depression study.\n    ",
        "submission_date": "2012-02-19T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.4331",
        "title": "Strong Backdoors to Nested Satisfiability",
        "authors": [
            "Serge Gaspers",
            "Stefan Szeider"
        ],
        "abstract": "Knuth (1990) introduced the class of nested formulas and showed that their satisfiability can be decided in polynomial time. We show that, parameterized by the size of a smallest strong backdoor set to the target class of nested formulas, checking the satisfiability of any CNF formula is fixed-parameter tractable. Thus, for any k>0, the satisfiability problem can be solved in polynomial time for any formula F for which there exists a variable set B of size at most k such that for every truth assignment t to B, the formula F[t] is nested; moreover, the degree of the polynomial is independent of k.\n",
        "submission_date": "2012-02-20T00:00:00",
        "last_modified_date": "2012-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.4465",
        "title": "MAV Stabilization using Machine Learning and Onboard Sensors",
        "authors": [
            "Jason Yosinski",
            "Cooper Bills"
        ],
        "abstract": "In many situations, Miniature Aerial Vehicles (MAVs) are limited to using only on-board sensors for navigation. This limits the data available to algorithms used for stabilization and localization, and current control methods are often insufficient to allow reliable hovering in place or trajectory following. In this research, we explore using machine learning to predict the drift (flight path errors) of an MAV while executing a desired flight path. This predicted drift will allow the MAV to adjust it's flightpath to maintain a desired course.\n    ",
        "submission_date": "2012-02-20T00:00:00",
        "last_modified_date": "2012-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.4478",
        "title": "(weak) Calibration is Computationally Hard",
        "authors": [
            "Elad Hazan",
            "Sham Kakade"
        ],
        "abstract": "We show that the existence of a computationally efficient calibration algorithm, with a low weak calibration rate, would imply the existence of an efficient algorithm for computing approximate Nash equilibria - thus implying the unlikely conclusion that every problem in PPAD is solvable in polynomial time.\n    ",
        "submission_date": "2012-02-20T00:00:00",
        "last_modified_date": "2012-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.4835",
        "title": "Isabelle/PIDE as Platform for Educational Tools",
        "authors": [
            "Makarius Wenzel",
            "Burkhart Wolff"
        ],
        "abstract": "The Isabelle/PIDE platform addresses the question whether proof assistants of the LCF family are suitable as technological basis for educational tools.  The traditionally strong logical foundations of systems like HOL, Coq, or Isabelle have so far been counter-balanced by somewhat inaccessible interaction via the TTY (or minor variations like the well-known Proof General / Emacs interface).  Thus the fundamental question of math education tools with fully-formal background theories has often been answered negatively due to accidental weaknesses of existing proof engines.\n",
        "submission_date": "2012-02-22T00:00:00",
        "last_modified_date": "2012-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.4905",
        "title": "A Bi-Directional Refinement Algorithm for the Calculus of (Co)Inductive Constructions",
        "authors": [
            "Andrea Asperti",
            "Wilmer Ricciotti",
            "Claudio Sacerdoti Coen",
            "Enrico Tassi"
        ],
        "abstract": " The paper describes the refinement algorithm for the Calculus of (Co)Inductive Constructions (CIC) implemented in the interactive theorem prover Matita. The refinement algorithm is in charge of giving a meaning to the terms, types and proof terms directly written by the user or generated by using tactics, decision procedures or general automation. The terms are written in an \"external syntax\" meant to be user friendly that allows omission of information, untyped binders and a certain liberal use of user defined sub-typing. The refiner modifies the terms to obtain related well typed terms in the internal syntax understood by the kernel of the ITP. In particular, it acts as a type inference algorithm when all the binders are untyped. The proposed algorithm is bi-directional: given a term in external syntax and a type expected for the term, it propagates as much typing information as possible towards the leaves of the term. Traditional mono-directional algorithms, instead, proceed in a bottom-up way by inferring the type of a sub-term and comparing (unifying) it with the type expected by its context only at the end. We propose some novel bi-directional rules for CIC that are particularly effective. Among the benefits of bi-directionality we have better error message reporting and better inference of dependent types. Moreover, thanks to bi-directionality, the coercion system for sub-typing is more effective and type inference generates simpler unification problems that are more likely to be solved by the inherently incomplete higher order unification algorithms implemented. Finally we introduce in the external syntax the notion of vector of placeholders that enables to omit at once an arbitrary number of arguments. Vectors of placeholders allow a trivial implementation of implicit arguments and greatly simplify the implementation of primitive and simple tactics. \n    ",
        "submission_date": "2012-02-22T00:00:00",
        "last_modified_date": "2012-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.5284",
        "title": "Elitism Levels Traverse Mechanism For The Derivation of Upper Bounds on Unimodal Functions",
        "authors": [
            "Aram Ter-Sarkisov"
        ],
        "abstract": "In this article we present an Elitism Levels Traverse Mechanism that we designed to find bounds on population-based Evolutionary algorithms solving unimodal functions. We prove its efficiency theoretically and test it on OneMax function deriving bounds c{\\mu}n log n - O({\\mu} n). This analysis can be generalized to any similar algorithm using variants of tournament selection and genetic operators that flip or swap only 1 bit in each string.\n    ",
        "submission_date": "2012-02-23T00:00:00",
        "last_modified_date": "2012-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.6157",
        "title": "Distributed Power Allocation with SINR Constraints Using Trial and Error Learning",
        "authors": [
            "Luca Rose",
            "Samir M. Perlaza",
            "M\u00e9rouane Debbah",
            "Christophe J. Le Martret"
        ],
        "abstract": "In this paper, we address the problem of global transmit power minimization in a self-congiguring network where radio devices are subject to operate at a minimum signal to interference plus noise ratio (SINR) level. We model the network as a parallel Gaussian interference channel and we introduce a fully decentralized algorithm (based on trial and error) able to statistically achieve a congiguration where the performance demands are met. Contrary to existing solutions, our algorithm requires only local information and can learn stable and efficient working points by using only one bit feedback. We model the network under two different game theoretical frameworks: normal form and satisfaction form. We show that the converging points correspond to equilibrium points, namely Nash and satisfaction equilibrium. Similarly, we provide sufficient conditions for the algorithm to converge in both formulations. Moreover, we provide analytical results to estimate the algorithm's performance, as a function of the network parameters. Finally, numerical results are provided to validate our theoretical conclusions. Keywords: Learning, power control, trial and error, Nash equilibrium, spectrum sharing.\n    ",
        "submission_date": "2012-02-28T00:00:00",
        "last_modified_date": "2012-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.0202",
        "title": "Pictures of Processes: Automated Graph Rewriting for Monoidal Categories and Applications to Quantum Computing",
        "authors": [
            "Aleks Kissinger"
        ],
        "abstract": "This work is about diagrammatic languages, how they can be represented, and what they in turn can be used to represent. More specifically, it focuses on representations and applications of string diagrams. String diagrams are used to represent a collection of processes, depicted as \"boxes\" with multiple (typed) inputs and outputs, depicted as \"wires\". If we allow plugging input and output wires together, we can intuitively represent complex compositions of processes, formalised as morphisms in a monoidal category.\n",
        "submission_date": "2012-03-01T00:00:00",
        "last_modified_date": "2012-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.0504",
        "title": "Modelling Social Structures and Hierarchies in Language Evolution",
        "authors": [
            "Martin Bachwerk",
            "Carl Vogel"
        ],
        "abstract": "Language evolution might have preferred certain prior social configurations over others. Experiments conducted with models of different social structures (varying subgroup interactions and the role of a dominant interlocutor) suggest that having isolated agent groups rather than an interconnected agent is more advantageous for the emergence of a social communication system. Distinctive groups that are closely connected by communication yield systems less like natural language than fully isolated groups inhabiting the same world. Furthermore, the addition of a dominant male who is asymmetrically favoured as a hearer, and equally likely to be a speaker has no positive influence on the disjoint groups.\n    ",
        "submission_date": "2012-03-02T00:00:00",
        "last_modified_date": "2012-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.0512",
        "title": "Establishing linguistic conventions in task-oriented primeval dialogue",
        "authors": [
            "Martin Bachwerk",
            "Carl Vogel"
        ],
        "abstract": "In this paper, we claim that language is likely to have emerged as a mechanism for coordinating the solution of complex tasks. To confirm this thesis, computer simulations are performed based on the coordination task presented by Garrod & Anderson (1987). The role of success in task-oriented dialogue is analytically evaluated with the help of performance measurements and a thorough lexical analysis of the emergent communication system. Simulation results confirm a strong effect of success mattering on both reliability and dispersion of linguistic conventions.\n    ",
        "submission_date": "2012-03-02T00:00:00",
        "last_modified_date": "2012-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.0550",
        "title": "Algorithms for Learning Kernels Based on Centered Alignment",
        "authors": [
            "Corinna Cortes",
            "Mehryar Mohri",
            "Afshin Rostamizadeh"
        ],
        "abstract": "This paper presents new and effective algorithms for learning kernels. In particular, as shown by our empirical results, these algorithms consistently outperform the so-called uniform combination solution that has proven to be difficult to improve upon in the past, as well as other algorithms for learning kernels based on convex combinations of base kernels in both classification and regression. Our algorithms are based on the notion of centered alignment which is used as a similarity measure between kernels or kernel matrices. We present a number of novel algorithmic, theoretical, and empirical results for learning kernels based on our notion of centered alignment. In particular, we describe efficient algorithms for learning a maximum alignment kernel by showing that the problem can be reduced to a simple QP and discuss a one-stage algorithm for learning both a kernel and a hypothesis based on that kernel using an alignment-based regularization. Our theoretical results include a novel concentration bound for centered alignment between kernel matrices, the proof of the existence of effective predictors for kernels with high alignment, both for classification and for regression, and the proof of stability-based generalization bounds for a broad family of algorithms for learning kernels based on centered alignment. We also report the results of experiments with our centered alignment-based algorithms in both classification and regression.\n    ",
        "submission_date": "2012-03-02T00:00:00",
        "last_modified_date": "2024-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.0648",
        "title": "Towards Electronic Shopping of Composite Product",
        "authors": [
            "Mark Sh. Levin"
        ],
        "abstract": "In the paper, frameworks for electronic shopping of composite (modular) products are described: (a) multicriteria selection (product is considered as a whole system, it is a traditional approach), (b) combinatorial synthesis (composition) of the product from its components, (c) aggregation of the product from several selected products/prototypes. The following product model is examined: (i) general tree-like structure, (ii) set of system parts/components (leaf nodes), (iii) design alternatives (DAs) for each component, (iv) ordinal priorities for DAs, and (v) estimates of compatibility between DAs for different components. The combinatorial synthesis is realized as morphological design of a composite (modular) product or an extended composite product (e.g., product and support services as financial instruments). Here the solving process is based on Hierarchical Morphological Multicriteria Design (HMMD): (i) multicriteria selection of alternatives for system parts, (ii) composing the selected alternatives into a resultant combination (while taking into account ordinal quality of the alternatives above and their compatibility). The aggregation framework is based on consideration of aggregation procedures, for example: (i) addition procedure: design of a products substructure or an extended substructure ('kernel') and addition of elements, and (ii) design procedure: design of the composite solution based on all elements of product superstructure. Applied numerical examples (e.g., composite product, extended composite product, product repair plan, and product trajectory) illustrate the proposed approaches.\n    ",
        "submission_date": "2012-03-03T00:00:00",
        "last_modified_date": "2012-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.0697",
        "title": "Learning High-Dimensional Mixtures of Graphical Models",
        "authors": [
            "A. Anandkumar",
            "D. Hsu",
            "F. Huang",
            "S. M. Kakade"
        ],
        "abstract": "We consider unsupervised estimation of mixtures of discrete graphical models, where the class variable corresponding to the mixture components is hidden and each mixture component over the observed variables can have a potentially different Markov graph structure and parameters. We propose a novel approach for estimating the mixture components, and our output is a tree-mixture model which serves as a good approximation to the underlying graphical model mixture. Our method is efficient when the union graph, which is the union of the Markov graphs of the mixture components, has sparse vertex separators between any pair of observed variables. This includes tree mixtures and mixtures of bounded degree graphs. For such models, we prove that our method correctly recovers the union graph structure and the tree structures corresponding to maximum-likelihood tree approximations of the mixture components. The sample and computational complexities of our method scale as $\\poly(p, r)$, for an $r$-component mixture of $p$-variate graphical models. We further extend our results to the case when the union graph has sparse local separators between any pair of observed variables, such as mixtures of locally tree-like graphs, and the mixture components are in the regime of correlation decay.\n    ",
        "submission_date": "2012-03-04T00:00:00",
        "last_modified_date": "2012-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.0876",
        "title": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals",
        "authors": [
            "Subhadip Basu",
            "Nibaran Das",
            "Ram Sarkar",
            "Mahantapas Kundu",
            "Mita Nasipuri",
            "Dipak Kumar Basu"
        ],
        "abstract": "The work presented here involves the design of a Multi Layer Perceptron (MLP) based pattern classifier for recognition of handwritten Bangla digits using a 76 element feature vector. Bangla is the second most popular script and language in the Indian subcontinent and the fifth most popular language in the world. The feature set developed for representing handwritten Bangla numerals here includes 24 shadow features, 16 centroid features and 36 longest-run features. On experimentation with a database of 6000 samples, the technique yields an average recognition rate of 96.67% evaluated after three-fold cross validation of results. It is useful for applications related to OCR of handwritten Bangla Digit and can also be extended to include OCR of handwritten characters of Bangla alphabet.\n    ",
        "submission_date": "2012-03-05T00:00:00",
        "last_modified_date": "2012-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.0882",
        "title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
        "authors": [
            "Subhadip Basu",
            "Nibaran Das",
            "Ram Sarkar",
            "Mahantapas Kundu",
            "Mita Nasipuri",
            "Dipak Kumar Basu"
        ],
        "abstract": "The work presented here involves the design of a Multi Layer Perceptron (MLP) based classifier for recognition of handwritten Bangla alphabet using a 76 element feature set Bangla is the second most popular script and language in the Indian subcontinent and the fifth most popular language in the world. The feature set developed for representing handwritten characters of Bangla alphabet includes 24 shadow features, 16 centroid features and 36 longest-run features. Recognition performances of the MLP designed to work with this feature set are experimentally observed as 86.46% and 75.05% on the samples of the training and the test sets respectively. The work has useful application in the development of a complete OCR system for handwritten Bangla text.\n    ",
        "submission_date": "2012-03-05T00:00:00",
        "last_modified_date": "2012-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.1007",
        "title": "Agnostic System Identification for Model-Based Reinforcement Learning",
        "authors": [
            "Stephane Ross",
            "J. Andrew Bagnell"
        ],
        "abstract": "A fundamental problem in control is to learn a model of a system from observations that is useful for controller synthesis. To provide good performance guarantees, existing methods must assume that the real system is in the class of models considered during learning. We present an iterative method with strong guarantees even in the agnostic case where the system is not in the class. In particular, we show that any no-regret online learning algorithm can be used to obtain a near-optimal policy, provided some model achieves low training error and access to a good exploration distribution. Our approach applies to both discrete and continuous domains. We demonstrate its efficacy and scalability on a challenging helicopter domain from the literature.\n    ",
        "submission_date": "2012-03-05T00:00:00",
        "last_modified_date": "2012-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.2200",
        "title": "Role-Dynamics: Fast Mining of Large Dynamic Networks",
        "authors": [
            "Ryan Rossi",
            "Brian Gallagher",
            "Jennifer Neville",
            "Keith Henderson"
        ],
        "abstract": "To understand the structural dynamics of a large-scale social, biological or technological network, it may be useful to discover behavioral roles representing the main connectivity patterns present over time. In this paper, we propose a scalable non-parametric approach to automatically learn the structural dynamics of the network and individual nodes. Roles may represent structural or behavioral patterns such as the center of a star, peripheral nodes, or bridge nodes that connect different communities. Our novel approach learns the appropriate structural role dynamics for any arbitrary network and tracks the changes over time. In particular, we uncover the specific global network dynamics and the local node dynamics of a technological, communication, and social network. We identify interesting node and network patterns such as stationary and non-stationary roles, spikes/steps in role-memberships (perhaps indicating anomalies), increasing/decreasing role trends, among many others. Our results indicate that the nodes in each of these networks have distinct connectivity patterns that are non-stationary and evolve considerably over time. Overall, the experiments demonstrate the effectiveness of our approach for fast mining and tracking of the dynamics in large networks. Furthermore, the dynamic structural representation provides a basis for building more sophisticated models and tools that are fast for exploring large dynamic networks.\n    ",
        "submission_date": "2012-03-09T00:00:00",
        "last_modified_date": "2012-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.2315",
        "title": "Modeling multistage decision processes with Reflexive Game Theory",
        "authors": [
            "Sergey Tarasenko"
        ],
        "abstract": "This paper introduces application of Reflexive Game Theory to the matter of multistage decision making processes. The idea behind is that each decision making session has certain parameters like \"when the session is taking place\", \"who are the group members to make decision\", \"how group members influence on each other\", etc. This study illustrates the consecutive or sequential decision making process, which consist of two stages. During the stage 1 decisions about the parameters of the ultimate decision making are made. Then stage 2 is implementation of Ultimate decision making itself. Since during stage 1 there can be multiple decision sessions. In such a case it takes more than two sessions to make ultimate (final) decision. Therefore the overall process of ultimate decision making becomes multistage decision making process consisting of consecutive decision making sessions.\n    ",
        "submission_date": "2012-03-11T00:00:00",
        "last_modified_date": "2012-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.2990",
        "title": "Evolving Culture vs Local Minima",
        "authors": [
            "Yoshua Bengio"
        ],
        "abstract": "We propose a theory that relates difficulty of learning in deep architectures to culture and language. It is articulated around the following hypotheses: (1) learning in an individual human brain is hampered by the presence of effective local minima; (2) this optimization difficulty is particularly important when it comes to learning higher-level abstractions, i.e., concepts that cover a vast and highly-nonlinear span of sensory configurations; (3) such high-level abstractions are best represented in brains by the composition of many levels of representation, i.e., by deep architectures; (4) a human brain can learn such high-level abstractions if guided by the signals produced by other humans, which act as hints or indirect supervision for these high-level abstractions; and (5), language and the recombination and optimization of mental concepts provide an efficient evolutionary recombination operator, and this gives rise to rapid search in the space of communicable ideas that help humans build up better high-level internal representations of their world. These hypotheses put together imply that human culture and the evolution of ideas have been crucial to counter an optimization difficulty: this optimization difficulty would otherwise make it very difficult for human brains to capture high-level knowledge of the world. The theory is grounded in experimental observations of the difficulties of training deep artificial neural networks. Plausible consequences of this theory for the efficiency of cultural evolutions are sketched.\n    ",
        "submission_date": "2012-03-14T00:00:00",
        "last_modified_date": "2012-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3471",
        "title": "An Online Learning-based Framework for Tracking",
        "authors": [
            "Kamalika Chaudhuri",
            "Yoav Freund",
            "Daniel Hsu"
        ],
        "abstract": "We study the tracking problem, namely, estimating the hidden state of an object over time, from unreliable and noisy measurements. The standard framework for the tracking problem is the generative framework, which is the basis of solutions such as the Bayesian algorithm and its approximation, the particle filters. However, these solutions can be very sensitive to model mismatches. In this paper, motivated by online learning, we introduce a new framework for tracking. We provide an efficient tracking algorithm for this framework. We provide experimental results comparing our algorithm to the Bayesian algorithm on simulated data. Our experiments show that when there are slight model mismatches, our algorithm outperforms the Bayesian algorithm.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3479",
        "title": "Maximum likelihood fitting of acyclic directed mixed graphs to binary data",
        "authors": [
            "Robin J. Evans",
            "Thomas S. Richardson"
        ],
        "abstract": "Acyclic directed mixed graphs, also known as semi-Markov models represent the conditional independence structure induced on an observed margin by a DAG model with latent variables. In this paper we present the first method for fitting these models to binary data using maximum likelihood estimation.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3480",
        "title": "Learning Game Representations from Data Using Rationality Constraints",
        "authors": [
            "Xi Alice Gao",
            "Avi Pfeffer"
        ],
        "abstract": "While game theory is widely used to model strategic interactions, a natural question is where do the game representations come from? One answer is to learn the representations from data. If one wants to learn both the payoffs and the players' strategies, a naive approach is to learn them both directly from the data. This approach ignores the fact the players might be playing reasonably good strategies, so there is a connection between the strategies and the data. The main contribution of this paper is to make this connection while learning. We formulate the learning problem as a weighted constraint satisfaction problem, including constraints both for the fit of the payoffs and strategies to the data and the fit of the strategies to the payoffs. We use quantal response equilibrium as our notion of rationality for quantifying the latter fit. Our results show that incorporating rationality constraints can improve learning when the amount of data is limited.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3481",
        "title": "Real-Time Scheduling via Reinforcement Learning",
        "authors": [
            "Robert Glaubius",
            "Terry Tidwell",
            "Christopher Gill",
            "William D. Smart"
        ],
        "abstract": "Cyber-physical systems, such as mobile robots, must respond adaptively to dynamic operating conditions. Effective operation of these systems requires that sensing and actuation tasks are performed in a timely manner. Additionally, execution of mission specific tasks such as imaging a room must be balanced against the need to perform more general tasks such as obstacle avoidance. This problem has been addressed by maintaining relative utilization of shared resources among tasks near a user-specified target level. Producing optimal scheduling strategies requires complete prior knowledge of task behavior, which is unlikely to be available in practice. Instead, suitable scheduling strategies must be learned online through interaction with the system. We consider the sample complexity of reinforcement learning in this domain, and demonstrate that while the problem state space is countably infinite, we may leverage the problem's structure to guarantee efficient learning.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3484",
        "title": "Intracluster Moves for Constrained Discrete-Space MCMC",
        "authors": [
            "Firas Hamze",
            "Nando de Freitas"
        ],
        "abstract": "This paper addresses the problem of sampling from binary distributions with constraints. In particular, it proposes an MCMC method to draw samples from a distribution of the set of all states at a specified distance from some reference state. For example, when the reference state is the vector of zeros, the algorithm can draw samples from a binary distribution with a constraint on the number of active variables, say the number of 1's. We motivate the need for this algorithm with examples from statistical physics and probabilistic inference. Unlike previous algorithms proposed to sample from binary distributions with these constraints, the new algorithm allows for large moves in state space and tends to propose them such that they are energetically favourable. The algorithm is demonstrated on three Boltzmann machines of varying difficulty: A ferromagnetic Ising model (with positive potentials), a restricted Boltzmann machine with learned Gabor-like filters as potentials, and a challenging three-dimensional spin-glass (with positive and negative potentials).\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3488",
        "title": "Causal Conclusions that Flip Repeatedly and Their Justification",
        "authors": [
            "Kevin T. Kelly",
            "Conor Mayo-Wilson"
        ],
        "abstract": "Over the past two decades, several consistent procedures have been designed to infer causal conclusions from observational data. We prove that if the true causal network might be an arbitrary, linear Gaussian network or a discrete Bayes network, then every unambiguous causal conclusion produced by a consistent method from non-experimental data is subject to reversal as the sample size increases any finite number of times. That result, called the causal flipping theorem, extends prior results to the effect that causal discovery cannot be reliable on a given sample size. We argue that since repeated flipping of causal conclusions is unavoidable in principle for consistent methods, the best possible discovery methods are consistent methods that retract their earlier conclusions no more than necessary. A series of simulations of various methods across a wide range of sample sizes illustrates concretely both the theorem and the principle of comparing methods in terms of retractions.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3498",
        "title": "Automated Planning in Repeated Adversarial Games",
        "authors": [
            "Enrique Munoz de Cote",
            "Archie C. Chapman",
            "Adam M. Sykulski",
            "Nicholas R. Jennings"
        ],
        "abstract": "Game theory's prescriptive power typically relies on full rationality and/or self-play interactions. In contrast, this work sets aside these fundamental premises and focuses instead on heterogeneous autonomous interactions between two or more agents. Specifically, we introduce a new and concise representation for repeated adversarial (constant-sum) games that highlight the necessary features that enable an automated planing agent to reason about how to score above the game's Nash equilibrium, when facing heterogeneous adversaries. To this end, we present TeamUP, a model-based RL algorithm designed for learning and planning such an abstraction. In essence, it is somewhat similar to R-max with a cleverly engineered reward shaping that treats exploration as an adversarial optimization problem. In practice, it attempts to find an ally with which to tacitly collude (in more than two-player games) and then collaborates on a joint plan of actions that can consistently score a high utility in adversarial repeated games. We use the inaugural Lemonade Stand Game Tournament to demonstrate the effectiveness of our approach, and find that TeamUP is the best performing agent, demoting the Tournament's actual winning strategy into second place. In our experimental analysis, we show hat our strategy successfully and consistently builds collaborations with many different heterogeneous (and sometimes very sophisticated) adversaries.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3503",
        "title": "On a Class of Bias-Amplifying Variables that Endanger Effect Estimates",
        "authors": [
            "Judea Pearl"
        ],
        "abstract": "This note deals with a class of variables that, if conditioned on, tends to amplify confounding bias in the analysis of causal effects. This class, independently discovered by Bhattacharya and Vogt (2007) and Wooldridge (2009), includes instrumental variables and variables that have greater influence on treatment selection than on the outcome. We offer a simple derivation and an intuitive explanation of this phenomenon and then extend the analysis to non linear models. We show that: 1. the bias-amplifying potential of instrumental variables extends over to non-linear models, though not as sweepingly as in linear models; 2. in non-linear models, conditioning on instrumental variables may introduce new bias where none existed before; 3. in both linear and non-linear models, instrumental variables have no effect on selection-induced bias.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3504",
        "title": "On Measurement Bias in Causal Inference",
        "authors": [
            "Judea Pearl"
        ],
        "abstract": "This paper addresses the problem of measurement errors in causal inference and highlights several algebraic and graphical methods for eliminating systematic bias induced by such errors. In particulars, the paper discusses the control of partially observable confounders in parametric and non parametric models and the computational problem of obtaining bias-free effect estimates in such models.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3505",
        "title": "Confounding Equivalence in Causal Inference",
        "authors": [
            "Judea Pearl",
            "Azaria Paz"
        ],
        "abstract": "The paper provides a simple test for deciding, from a given causal diagram, whether two sets of variables have the same bias-reducing potential under adjustment. The test requires that one of the following two conditions holds: either (1) both sets are admissible (i.e., satisfy the back-door criterion) or (2) the Markov boundaries surrounding the manipulated variable(s) are identical in both sets. Applications to covariate selection and model testing are discussed.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3515",
        "title": "On the Validity of Covariate Adjustment for Estimating Causal Effects",
        "authors": [
            "Ilya Shpitser",
            "Tyler VanderWeele",
            "James M. Robins"
        ],
        "abstract": "Identifying effects of actions (treatments) on outcome variables from observational data and causal assumptions is a fundamental problem in causal inference. This identification is made difficult by the presence of confounders which can be related to both treatment and outcome variables. Confounders are often handled, both in theory and in practice, by adjusting for covariates, in other words considering outcomes conditioned on treatment and covariate values, weighed by probability of observing those covariate values. In this paper, we give a complete graphical criterion for covariate adjustment, which we term the adjustment criterion, and derive some interesting corollaries of the completeness of this criterion.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3516",
        "title": "Modeling Events with Cascades of Poisson Processes",
        "authors": [
            "Aleksandr Simma",
            "Michael I. Jordan"
        ],
        "abstract": "We present a probabilistic model of events in continuous time in which each event triggers a Poisson process of successor events. The ensemble of observed events is thereby modeled as a superposition of Poisson processes. Efficient inference is feasible under this model with an EM algorithm. Moreover, the EM algorithm can be implemented as a distributed algorithm, permitting the model to be applied to very large datasets. We apply these techniques to the modeling of Twitter messages and the revision history of Wikipedia.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3518",
        "title": "Variance-Based Rewards for Approximate Bayesian Reinforcement Learning",
        "authors": [
            "Jonathan Sorg",
            "Satinder Singh",
            "Richard L. Lewis"
        ],
        "abstract": "The explore{exploit dilemma is one of the central challenges in Reinforcement Learning (RL). Bayesian RL solves the dilemma by providing the agent with information in the form of a prior distribution over environments; however, full Bayesian planning is intractable. Planning with the mean MDP is a common myopic approximation of Bayesian planning. We derive a novel reward bonus that is a function of the posterior distribution over environments, which, when added to the reward in planning with the mean MDP, results in an agent which explores efficiently and effectively. Although our method is similar to existing methods when given an uninformative or unstructured prior, unlike existing methods, our method can exploit structured priors. We prove that our method results in a polynomial sample complexity and empirically demonstrate its advantages in a structured exploration task.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3519",
        "title": "Bayesian Inference in Monte-Carlo Tree Search",
        "authors": [
            "Gerald Tesauro",
            "V T Rajan",
            "Richard Segal"
        ],
        "abstract": "Monte-Carlo Tree Search (MCTS) methods are drawing great interest after yielding breakthrough results in computer Go. This paper proposes a Bayesian approach to MCTS that is inspired by distributionfree approaches such as UCT [13], yet significantly differs in important respects. The Bayesian framework allows potentially much more accurate (Bayes-optimal) estimation of node values and node uncertainties from a limited number of simulation trials. We further propose propagating inference in the tree via fast analytic Gaussian approximation methods: this can make the overhead of Bayesian inference manageable in domains such as Go, while preserving high accuracy of expected-value estimates. We find substantial empirical outperformance of UCT in an idealized bandit-tree test environment, where we can obtain valuable insights by comparing with known ground truth. Additionally we rigorously prove on-policy and off-policy convergence of the proposed methods.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3520",
        "title": "Bayesian Model Averaging Using the k-best Bayesian Network Structures",
        "authors": [
            "Jin Tian",
            "Ru He",
            "Lavanya Ram"
        ],
        "abstract": "We study the problem of learning Bayesian network structures from data. We develop an algorithm for finding the k-best Bayesian network structures. We propose to compute the posterior probabilities of hypotheses of interest by Bayesian model averaging over the k-best Bayesian networks. We present empirical results on structural discovery over several real and synthetic data sets and show that the method outperforms the model selection method and the state of-the-art MCMC methods.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3526",
        "title": "Primal View on Belief Propagation",
        "authors": [
            "Tomas Werner"
        ],
        "abstract": "It is known that fixed points of loopy belief propagation (BP) correspond to stationary points of the Bethe variational problem, where we minimize the Bethe free energy subject to normalization and marginalization constraints. Unfortunately, this does not entirely explain BP because BP is a dual rather than primal algorithm to solve the Bethe variational problem -- beliefs are infeasible before convergence. Thus, we have no better understanding of BP than as an algorithm to seek for a common zero of a system of non-linear functions, not explicitly related to each other. In this theoretical paper, we show that these functions are in fact explicitly related -- they are the partial derivatives of a single function of reparameterizations. That means, BP seeks for a stationary point of a single function, without any constraints. This function has a very natural form: it is a linear combination of local log-partition functions, exactly as the Bethe entropy is the same linear combination of local entropies.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3527",
        "title": "Truthful Feedback for Sanctioning Reputation Mechanisms",
        "authors": [
            "Jens Witkowski"
        ],
        "abstract": "For product rating environments, similar to that of Amazon Reviews, it has been shown that the truthful elicitation of feedback is possible through mechanisms which pay buyer reports contingent on the reports of other buyers. We study whether similar mechanisms can be designed for reputation mechanisms at online auction sites where the buyers' experiences are partially determined by a strategic seller. We show that this is impossible for the basic setting. However, introducing a small prior belief that the seller is a cooperative commitment player leads to a payment scheme with a truthful perfect Bayesian equilibrium.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3529",
        "title": "Modeling Multiple Annotator Expertise in the Semi-Supervised Learning Scenario",
        "authors": [
            "Yan Yan",
            "Romer Rosales",
            "Glenn Fung",
            "Jennifer Dy"
        ],
        "abstract": "Learning algorithms normally assume that there is at most one annotation or label per data point. However, in some scenarios, such as medical diagnosis and on-line collaboration,multiple annotations may be available. In either case, obtaining labels for data points can be expensive and time-consuming (in some circumstances ground-truth may not exist). Semi-supervised learning approaches have shown that utilizing the unlabeled data is often beneficial in these cases. This paper presents a probabilistic semi-supervised model and algorithm that allows for learning from both unlabeled and labeled data in the presence of multiple annotators. We assume that it is known what annotator labeled which data points. The proposed approach produces annotator models that allow us to provide (1) estimates of the true label and (2) annotator variable expertise for both labeled and unlabeled data. We provide numerical comparisons under various scenarios and with respect to standard semi-supervised learning. Experiments showed that the presented approach provides clear advantages over multi-annotator methods that do not use the unlabeled data and over methods that do not use multi-labeler information.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3535",
        "title": "Multi-Domain Collaborative Filtering",
        "authors": [
            "Yu Zhang",
            "Bin Cao",
            "Dit-Yan Yeung"
        ],
        "abstract": "Collaborative filtering is an effective recommendation approach in which the preference of a user on an item is predicted based on the preferences of other users with similar interests. A big challenge in using collaborative filtering methods is the data sparsity problem which often arises because each user typically only rates very few items and hence the rating matrix is extremely sparse. In this paper, we address this problem by considering multiple collaborative filtering tasks in different domains simultaneously and exploiting the relationships between domains. We refer to it as a multi-domain collaborative filtering (MCF) problem. To solve the MCF problem, we propose a probabilistic framework which uses probabilistic matrix factorization to model the rating problem in each domain and allows the knowledge to be adaptively transferred across different domains by automatically learning the correlation between domains. We also introduce the link function for different domains to correct their biases. Experiments conducted on several real-world applications demonstrate the effectiveness of our methods when compared with some representative methods.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3536",
        "title": "A Convex Formulation for Learning Task Relationships in Multi-Task Learning",
        "authors": [
            "Yu Zhang",
            "Dit-Yan Yeung"
        ],
        "abstract": "Multi-task learning is a learning paradigm which  seeks to improve the generalization performance  of a learning task with the help of some other related  tasks. In this paper, we propose a regularization  formulation for learning the relationships  between tasks in multi-task learning. This formulation  can be viewed as a novel generalization  of the regularization framework for single-task  learning. Besides modeling positive task correlation,  our method, called multi-task relationship  learning (MTRL), can also describe negative  task correlation and identify outlier tasks  based on the same underlying principle. Under  this regularization framework, the objective  function of MTRL is convex. For efficiency,  we use an alternating method to learn the optimal  model parameters for each task as well  as the relationships between tasks. We study  MTRL in the symmetric multi-task learning setting  and then generalize it to the asymmetric setting  as well. We also study the relationships between  MTRL and some existing multi-task learning  methods. Experiments conducted on a toy  problem as well as several benchmark data sets  demonstrate the effectiveness of MTRL.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3725",
        "title": "Bayesian Parameter Estimation for Latent Markov Random Fields and Social Networks",
        "authors": [
            "Richard G. Everitt"
        ],
        "abstract": "Undirected graphical models are widely used in statistics, physics and machine vision. However Bayesian parameter estimation for undirected models is extremely challenging, since evaluation of the posterior typically involves the calculation of an intractable normalising constant. This problem has received much attention, but very little of this has focussed on the important practical case where the data consists of noisy or incomplete observations of the underlying hidden structure. This paper specifically addresses this problem, comparing two alternative methodologies. In the first of these approaches particle Markov chain Monte Carlo (Andrieu et al., 2010) is used to efficiently explore the parameter space, combined with the exchange algorithm (Murray et al., 2006) for avoiding the calculation of the intractable normalising constant (a proof showing that this combination targets the correct distribution in found in a supplementary appendix online). This approach is compared with approximate Bayesian computation (Pritchard et al., 1999). Applications to estimating the parameters of Ising models and exponential random graphs from noisy data are presented. Each algorithm used in the paper targets an approximation to the true posterior due to the use of MCMC to simulate from the latent graphical model, in lieu of being able to do this exactly in general. The supplementary appendix also describes the nature of the resulting approximation.\n    ",
        "submission_date": "2012-03-14T00:00:00",
        "last_modified_date": "2012-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3764",
        "title": "The Abzooba Smart Health Informatics Platform (SHIP) TM - From Patient Experiences to Big Data to Insights",
        "authors": [
            "Naveen Ashish",
            "Antarip Biswas",
            "Sumit Das",
            "Saurav Nag",
            "Rajiv Pratap"
        ],
        "abstract": "This paper describes a technology to connect patients to information in the experiences of other patients by using the power of structured big data. The approach, implemented in the Abzooba Smart Health Informatics Platform (SHIP),is to distill concepts of facts and expressions from conversations and discussions in health social media forums, and use those distilled concepts in connecting patients to experiences and insights that are highly relevant to them in particular. We envision our work, in progress, to provide new and effective tools to exploit the richness of content in social media in health for outcomes research.\n    ",
        "submission_date": "2012-03-16T00:00:00",
        "last_modified_date": "2012-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3783",
        "title": "Learning Feature Hierarchies with Centered Deep Boltzmann Machines",
        "authors": [
            "Gr\u00e9goire Montavon",
            "Klaus-Robert M\u00fcller"
        ],
        "abstract": "Deep Boltzmann machines are in principle powerful models for extracting the hierarchical structure of data. Unfortunately, attempts to train layers jointly (without greedy layer-wise pretraining) have been largely unsuccessful. We propose a modification of the learning algorithm that initially recenters the output of the activation functions to zero. This modification leads to a better conditioned Hessian and thus makes learning easier. We test the algorithm on real data and demonstrate that our suggestion, the centered deep Boltzmann machine, learns a hierarchy of increasingly abstract representations and a better generative model of data.\n    ",
        "submission_date": "2012-03-16T00:00:00",
        "last_modified_date": "2012-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3887",
        "title": "Learning loopy graphical models with latent variables: Efficient methods and guarantees",
        "authors": [
            "Animashree Anandkumar",
            "Ragupathyraj Valluvan"
        ],
        "abstract": "The problem of structure estimation in graphical models with latent variables is considered. We characterize conditions for tractable graph estimation and develop efficient methods with provable guarantees. We consider models where the underlying Markov graph is locally tree-like, and the model is in the regime of correlation decay. For the special case of the Ising model, the number of samples $n$ required for structural consistency of our method scales as $n=\\Omega(\\theta_{\\min}^{-\\delta\\eta(\\eta+1)-2}\\log p)$, where p is the number of variables, $\\theta_{\\min}$ is the minimum edge potential, $\\delta$ is the depth (i.e., distance from a hidden node to the nearest observed nodes), and $\\eta$ is a parameter which depends on the bounds on node and edge potentials in the Ising model. Necessary conditions for structural consistency under any algorithm are derived and our method nearly matches the lower bound on sample requirements. Further, the proposed method is practical to implement and provides flexibility to control the number of latent variables and the cycle lengths in the output graph.\n    ",
        "submission_date": "2012-03-17T00:00:00",
        "last_modified_date": "2013-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.4184",
        "title": "The Initial Conditions of the Universe from Constrained Simulations",
        "authors": [
            "Francisco-Shu Kitaura"
        ],
        "abstract": "I present a new approach to recover the primordial density fluctuations and the cosmic web structure underlying a galaxy distribution. The method is based on sampling Gaussian fields which are compatible with a galaxy distribution and a structure formation model. This is achieved by splitting the inversion problem into two Gibbs-sampling steps: the first being a Gaussianisation step transforming a distribution of point sources at Lagrangian positions -which are not a priori given- into a linear alias-free Gaussian field. This step is based on Hamiltonian sampling with a Gaussian-Poisson model. The second step consists on a likelihood comparison in which the set of matter tracers at the initial conditions is constrained on the galaxy distribution and the assumed structure formation model. For computational reasons second order Lagrangian Perturbation Theory is used. However, the presented approach is flexible to adopt any structure formation model. A semi-analytic halo-model based galaxy mock catalog is taken to demonstrate that the recovered initial conditions are closely unbiased with respect to the actual ones from the corresponding N-body simulation down to scales of a ~ 5 Mpc/h. The cross-correlation between them shows a substantial gain of information, being at k ~ 0.3 h/Mpc more than doubled. In addition the initial conditions are extremely well Gaussian distributed and the power-spectra follow the shape of the linear power-spectrum being very close to the actual one from the simulation down to scales of k ~ 1 h/Mpc.\n    ",
        "submission_date": "2012-03-19T00:00:00",
        "last_modified_date": "2012-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.4345",
        "title": "Robust Filtering and Smoothing with Gaussian Processes",
        "authors": [
            "Marc Peter Deisenroth",
            "Ryan Turner",
            "Marco F. Huber",
            "Uwe D. Hanebeck",
            "Carl Edward Rasmussen"
        ],
        "abstract": "We propose a principled algorithm for robust Bayesian filtering and smoothing in nonlinear stochastic dynamic systems when both the transition function and the measurement function are described by non-parametric Gaussian process (GP) models. GPs are gaining increasing importance in signal processing, machine learning, robotics, and control for representing unknown system functions by posterior probability distributions. This modern way of \"system identification\" is more robust than finding point estimates of a parametric function representation. In this article, we present a principled algorithm for robust analytic smoothing in GP dynamic systems, which are increasingly used in robotics and control. Our numerical evaluations demonstrate the robustness of the proposed approach in situations where other state-of-the-art Gaussian filters and smoothers can fail.\n    ",
        "submission_date": "2012-03-20T00:00:00",
        "last_modified_date": "2012-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.4416",
        "title": "On Training Deep Boltzmann Machines",
        "authors": [
            "Guillaume Desjardins",
            "Aaron Courville",
            "Yoshua Bengio"
        ],
        "abstract": "The deep Boltzmann machine (DBM) has been an important development in the quest for powerful \"deep\" probabilistic models. To date, simultaneous or joint training of all layers of the DBM has been largely unsuccessful with existing training methods. We introduce a simple regularization scheme that encourages the weight vectors associated with each hidden unit to have similar norms. We demonstrate that this regularization can be easily combined with standard stochastic maximum likelihood to yield an effective training strategy for the simultaneous training of all layers of the deep Boltzmann machine.\n    ",
        "submission_date": "2012-03-20T00:00:00",
        "last_modified_date": "2012-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.4855",
        "title": "Texture Classification Approach Based on Combination of Edge & Co-occurrence and Local Binary Pattern",
        "authors": [
            "Shervan Fekri Ershad"
        ],
        "abstract": "Texture classification is one of the problems which has been paid much attention on by computer scientists since late 90s. If texture classification is done correctly and accurately, it can be used in many cases such as Pattern recognition, object tracking, and shape recognition. So far, there have been so many methods offered to solve this problem. Near all these methods have tried to extract and define features to separate different labels of textures really well. This article has offered an approach which has an overall process on the images of textures based on Local binary pattern and Gray Level Co-occurrence matrix and then by edge detection, and finally, extracting the statistical features from the images would classify them. Although, this approach is a general one and is could be used in different applications, the method has been tested on the stone texture and the results have been compared with some of the previous approaches to prove the quality of proposed approach.\n    ",
        "submission_date": "2012-03-21T00:00:00",
        "last_modified_date": "2012-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.5443",
        "title": "Transfer Learning, Soft Distance-Based Bias, and the Hierarchical BOA",
        "authors": [
            "Martin Pelikan",
            "Mark W. Hauschild",
            "Pier Luca Lanzi"
        ],
        "abstract": "An automated technique has recently been proposed to transfer learning in the hierarchical Bayesian optimization algorithm (hBOA) based on distance-based statistics. The technique enables practitioners to improve hBOA efficiency by collecting statistics from probabilistic models obtained in previous hBOA runs and using the obtained statistics to bias future hBOA runs on similar problems. The purpose of this paper is threefold: (1) test the technique on several classes of NP-complete problems, including MAXSAT, spin glasses and minimum vertex cover; (2) demonstrate that the technique is effective even when previous runs were done on problems of different size; (3) provide empirical evidence that combining transfer learning with other efficiency enhancement techniques can often yield nearly multiplicative speedups.\n    ",
        "submission_date": "2012-03-24T00:00:00",
        "last_modified_date": "2012-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.0033",
        "title": "Transforming Graph Representations for Statistical Relational Learning",
        "authors": [
            "Ryan A. Rossi",
            "Luke K. McDowell",
            "David W. Aha",
            "Jennifer Neville"
        ],
        "abstract": "Relational data representations have become an increasingly important topic due to the recent proliferation of network datasets (e.g., social, biological, information networks) and a corresponding increase in the application of statistical relational learning (SRL) algorithms to these domains. In this article, we examine a range of representation issues for graph-based relational data. Since the choice of relational data representation for the nodes, links, and features can dramatically affect the capabilities of SRL algorithms, we survey approaches and opportunities for relational representation transformation designed to improve the performance of these algorithms. This leads us to introduce an intuitive taxonomy for data representation transformations in relational domains that incorporates link transformation and node transformation as symmetric representation tasks. In particular, the transformation tasks for both nodes and links include (i) predicting their existence, (ii) predicting their label or type, (iii) estimating their weight or importance, and (iv) systematically constructing their relevant features. We motivate our taxonomy through detailed examples and use it to survey and compare competing approaches for each of these tasks. We also discuss general conditions for transforming links, nodes, and features. Finally, we highlight challenges that remain to be addressed.\n    ",
        "submission_date": "2012-03-30T00:00:00",
        "last_modified_date": "2012-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.0274",
        "title": "Learning from Humans as an I-POMDP",
        "authors": [
            "Mark P. Woodward",
            "Robert J. Wood"
        ],
        "abstract": "The interactive partially observable Markov decision process (I-POMDP) is a recently developed framework which extends the POMDP to the multi-agent setting by including agent models in the state space. This paper argues for formulating the problem of an agent learning interactively from a human teacher as an I-POMDP, where the agent \\emph{programming} to be learned is captured by random variables in the agent's state space, all \\emph{signals} from the human teacher are treated as observed random variables, and the human teacher, modeled as a distinct agent, is explicitly represented in the agent's state space. The main benefits of this approach are: i. a principled action selection mechanism, ii. a principled belief update mechanism, iii. support for the most common teacher \\emph{signals}, and iv. the anticipated production of complex beneficial interactions. The proposed formulation, its benefits, and several open questions are presented.\n    ",
        "submission_date": "2012-04-01T00:00:00",
        "last_modified_date": "2012-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.0684",
        "title": "Validation of nonlinear PCA",
        "authors": [
            "Matthias Scholz"
        ],
        "abstract": "Linear principal component analysis (PCA) can be extended to a nonlinear PCA by using artificial neural networks. But the benefit of curved components requires a careful control of the model complexity. Moreover, standard techniques for model selection, including cross-validation and more generally the use of an independent test set, fail when applied to nonlinear PCA because of its inherent unsupervised characteristics. This paper presents a new approach for validating the complexity of nonlinear PCA models by using the error in missing data estimation as a criterion for model selection. It is motivated by the idea that only the model of optimal complexity is able to predict missing values with the highest accuracy. While standard test set validation usually favours over-fitted nonlinear PCA models, the proposed model validation approach correctly selects the optimal model complexity.\n    ",
        "submission_date": "2012-04-03T00:00:00",
        "last_modified_date": "2012-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.1581",
        "title": "A new approach of designing Multi-Agent Systems",
        "authors": [
            "Sara Maalal",
            "Malika Addou"
        ],
        "abstract": "Agent technology is a software paradigm that permits to implement large and complex distributed applications. In order to assist analyzing, conception and development or implementation phases of multi-agent systems, we've tried to present a practical application of a generic and scalable method of a MAS with a component-oriented architecture and agent-based approach that allows MDA to generate source code from a given model. We've designed on AUML the class diagrams as a class meta-model of different agents of a MAS. Then we generated the source code of the models developed using an open source tool called AndroMDA. This agent-based and evolutive approach enhances the modularity and genericity developments and promotes their reusability in future developments. This property distinguishes our design methodology of existing methodologies in that it is constrained by any particular agent-based model while providing a library of generic models\n    ",
        "submission_date": "2012-04-07T00:00:00",
        "last_modified_date": "2012-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.1596",
        "title": "An Intelligent Location Management approaches in GSM Mobile Network",
        "authors": [
            "N. Mallikharjuna Rao"
        ],
        "abstract": "Location management refers to the problem of updating and searching the current location of mobile nodes in a wireless network. To make it efficient, the sum of update costs of location database must be minimized. Previous work relying on fixed location databases is unable to fully exploit the knowledge of user mobility patterns in the system so as to achieve this minimization. The study presents an intelligent location management approach which has interacts between intelligent information system and knowledge-base technologies, so we can dynamically change the user patterns and reduce the transition between the VLR and HLR. The study provides algorithms are ability to handle location registration and call delivery\n    ",
        "submission_date": "2012-04-07T00:00:00",
        "last_modified_date": "2012-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.1679",
        "title": "Clustering and Bayesian network for image of faces classification",
        "authors": [
            "Khlifia Jayech",
            "Mohamed Ali Mahjoub"
        ],
        "abstract": "In a content based image classification system, target images are sorted by feature similarities with respect to the query (CBIR). In this paper, we propose to use new approach combining distance tangent, k-means algorithm and Bayesian network for image classification. First, we use the technique of tangent distance to calculate several tangent spaces representing the same image. The objective is to reduce the error in the classification phase. Second, we cut the image in a whole of blocks. For each block, we compute a vector of descriptors. Then, we use K-means to cluster the low-level features including color and texture information to build a vector of labels for each image. Finally, we apply five variants of Bayesian networks classifiers (Na\u00efve Bayes, Global Tree Augmented Na\u00efve Bayes (GTAN), Global Forest Augmented Na\u00efve Bayes (GFAN), Tree Augmented Na\u00efve Bayes for each class (TAN), and Forest Augmented Na\u00efve Bayes for each class (FAN) to classify the image of faces using the vector of labels. In order to validate the feasibility and effectively, we compare the results of GFAN to FAN and to the others classifiers (NB, GTAN, TAN). The results demonstrate FAN outperforms than GFAN, NB, GTAN and TAN in the overall classification accuracy.\n    ",
        "submission_date": "2012-04-07T00:00:00",
        "last_modified_date": "2012-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.1751",
        "title": "Automated Feedback Generation for Introductory Programming Assignments",
        "authors": [
            "Rishabh Singh",
            "Sumit Gulwani",
            "Armando Solar-Lezama"
        ],
        "abstract": "We present a new method for automatically providing feedback for introductory programming problems. In order to use this method, we need a reference implementation of the assignment, and an error model consisting of potential corrections to errors that students might make. Using this information, the system automatically derives minimal corrections to student's incorrect solutions, providing them with a quantifiable measure of exactly how incorrect a given solution was, as well as feedback about what they did wrong.\n",
        "submission_date": "2012-04-08T00:00:00",
        "last_modified_date": "2012-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.1811",
        "title": "Skin-color based videos categorization",
        "authors": [
            "Rehanullah Khan",
            "Asad Maqsood",
            "Zeeshan Khan",
            "Muhammad Ishaq",
            "Arsalan Arif"
        ],
        "abstract": "On dedicated websites, people can upload videos and share it with the rest of the world. Currently these videos are cat- egorized manually by the help of the user community. In this paper, we propose a combination of color spaces with the Bayesian network approach for robust detection of skin color followed by an automated video categorization. Exper- imental results show that our method can achieve satisfactory performance for categorizing videos based on skin color.\n    ",
        "submission_date": "2012-04-09T00:00:00",
        "last_modified_date": "2012-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.2003",
        "title": "Directed Information Graphs",
        "authors": [
            "Christopher J. Quinn",
            "Negar Kiyavash",
            "Todd P. Coleman"
        ],
        "abstract": "We propose a graphical model for representing networks of stochastic processes, the minimal generative model graph. It is based on reduced factorizations of the joint distribution over time. We show that under appropriate conditions, it is unique and consistent with another type of graphical model, the directed information graph, which is based on a generalization of Granger causality. We demonstrate how directed information quantifies Granger causality in a particular sequential prediction setting. We also develop efficient methods to estimate the topological structure from data that obviate estimating the joint statistics. One algorithm assumes upper-bounds on the degrees and uses the minimal dimension statistics necessary. In the event that the upper-bounds are not valid, the resulting graph is nonetheless an optimal approximation. Another algorithm uses near-minimal dimension statistics when no bounds are known but the distribution satisfies a certain criterion. Analogous to how structure learning algorithms for undirected graphical models use mutual information estimates, these algorithms use directed information estimates. We characterize the sample-complexity of two plug-in directed information estimators and obtain confidence intervals. For the setting when point estimates are unreliable, we propose an algorithm that uses confidence intervals to identify the best approximation that is robust to estimation error. Lastly, we demonstrate the effectiveness of the proposed algorithms through analysis of both synthetic data and real data from the Twitter network. In the latter case, we identify which news sources influence users in the network by merely analyzing tweet times.\n    ",
        "submission_date": "2012-04-09T00:00:00",
        "last_modified_date": "2015-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.2235",
        "title": "Publishing Identifiable Experiment Code And Configuration Is Important, Good and Easy",
        "authors": [
            "Richard Vaughan",
            "Jens Wawerla"
        ],
        "abstract": "We argue for the value of publishing the exact code, configuration and data processing scripts used to produce empirical work in robotics. In particular, we recommend publishing a unique identifier for the code package in the paper itself, as a promise to the reader that this is the relavant code. We review some recent discussion of best practice for reproducibility in various professional organisations and journals, and discuss the current reward structure for publishing code in robotics, along with some ideas for improvement.\n    ",
        "submission_date": "2012-04-10T00:00:00",
        "last_modified_date": "2012-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.2321",
        "title": "Derivation of Upper Bounds on Optimization Time of Population-Based Evolutionary Algorithm on a Function with Fitness Plateaus Using Elitism Levels Traverse Mechanism",
        "authors": [
            "Aram Ter-Sarkisov",
            "Stephen Marsland"
        ],
        "abstract": "In this article a tool for the analysis of population-based EAs is used to derive asymptotic upper bounds on the optimization time of the algorithm solving Royal Roads problem, a test function with plateaus of fitness. In addition to this, limiting distribution of a certain subset of the population is approximated.\n    ",
        "submission_date": "2012-04-11T00:00:00",
        "last_modified_date": "2013-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.2601",
        "title": "Detecting lateral genetic material transfer",
        "authors": [
            "C. Calder\u00f3n",
            "L. Delaye",
            "V. Mireles",
            "P. Miramontes"
        ],
        "abstract": "The bioinformatical methods to detect lateral gene transfer events are mainly based on functional coding DNA characteristics. In this paper, we propose the use of DNA traits not depending on protein coding requirements. We introduce several semilocal variables that depend on DNA primary sequence and that reflect thermodynamic as well as physico-chemical magnitudes that are able to tell apart the genome of different organisms. After combining these variables in a neural classificator, we obtain results whose power of resolution go as far as to detect the exchange of genomic material between bacteria that are phylogenetically close.\n    ",
        "submission_date": "2012-04-12T00:00:00",
        "last_modified_date": "2012-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.2741",
        "title": "Simultaneous Object Detection, Tracking, and Event Recognition",
        "authors": [
            "Andrei Barbu",
            "Aaron Michaux",
            "Siddharth Narayanaswamy",
            "Jeffrey Mark Siskind"
        ],
        "abstract": "The common internal structure and algorithmic organization of object detection, detection-based tracking, and event recognition facilitates a general approach to integrating these three components. This supports multidirectional information flow between these components allowing object detection to influence tracking and event recognition and event recognition to influence tracking and object detection. The performance of the combination can exceed the performance of the components in isolation. This can be done with linear asymptotic complexity.\n    ",
        "submission_date": "2012-04-12T00:00:00",
        "last_modified_date": "2012-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.2742",
        "title": "Video In Sentences Out",
        "authors": [
            "Andrei Barbu",
            "Alexander Bridge",
            "Zachary Burchill",
            "Dan Coroian",
            "Sven Dickinson",
            "Sanja Fidler",
            "Aaron Michaux",
            "Sam Mussman",
            "Siddharth Narayanaswamy",
            "Dhaval Salvi",
            "Lara Schmidt",
            "Jiangnan Shangguan",
            "Jeffrey Mark Siskind",
            "Jarrell Waggoner",
            "Song Wang",
            "Jinlian Wei",
            "Yifan Yin",
            "Zhiqi Zhang"
        ],
        "abstract": "We present a system that produces sentential descriptions of video: who did what to whom, and where and how they did it. Action class is rendered as a verb, participant objects as noun phrases, properties of those objects as adjectival modifiers in those noun phrases,spatial relations between those participants as prepositional phrases, and characteristics of the event as prepositional-phrase adjuncts and adverbial modifiers. Extracting the information needed to render these linguistic entities requires an approach to event recognition that recovers object tracks, the track-to-role assignments, and changing body posture.\n    ",
        "submission_date": "2012-04-12T00:00:00",
        "last_modified_date": "2012-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.2801",
        "title": "Seeing Unseeability to See the Unseeable",
        "authors": [
            "Siddharth Narayanaswamy",
            "Andrei Barbu",
            "Jeffrey Mark Siskind"
        ],
        "abstract": "We present a framework that allows an observer to determine occluded portions of a structure by finding the maximum-likelihood estimate of those occluded portions consistent with visible image evidence and a consistency model. Doing this requires determining which portions of the structure are occluded in the first place. Since each process relies on the other, we determine a solution to both problems in tandem. We extend our framework to determine confidence of one's assessment of which portions of an observed structure are occluded, and the estimate of that occluded structure, by determining the sensitivity of one's assessment to potential new observations. We further extend our framework to determine a robotic action whose execution would allow a new observation that would maximally increase one's confidence.\n    ",
        "submission_date": "2012-04-12T00:00:00",
        "last_modified_date": "2012-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.3040",
        "title": "Tractable Answer-Set Programming with Weight Constraints: Bounded Treewidth is not Enough",
        "authors": [
            "Reinhard Pichler",
            "Stefan R\u00fcmmele",
            "Stefan Szeider",
            "Stefan Woltran"
        ],
        "abstract": "Cardinality constraints or, more generally, weight constraints are well recognized as an important extension of answer-set programming. Clearly, all common algorithmic tasks related to programs with cardinality or weight constraints - like checking the consistency of a program - are intractable. Many intractable problems in the area of knowledge representation and reasoning have been shown to become linear time tractable if the treewidth of the programs or formulas under consideration is bounded by some constant. The goal of this paper is to apply the notion of treewidth to programs with cardinality or weight constraints and to identify tractable fragments. It will turn out that the straightforward application of treewidth to such class of programs does not suffice to obtain tractability. However, by imposing further restrictions, tractability can be achieved.\n    ",
        "submission_date": "2012-04-13T00:00:00",
        "last_modified_date": "2012-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.3221",
        "title": "Neuroevolution Results in Emergence of Short-Term Memory for Goal-Directed Behavior",
        "authors": [
            "Konstantin Lakhman",
            "Mikhail Burtsev"
        ],
        "abstract": "Animals behave adaptively in the environment with multiply competing goals. Understanding of the mechanisms underlying such goal-directed behavior remains a challenge for neuroscience as well for adaptive system research. To address this problem we developed an evolutionary model of adaptive behavior in the multigoal stochastic environment. Proposed neuroevolutionary algorithm is based on neuron's duplication as a basic mechanism of agent's recurrent neural network development. Results of simulation demonstrate that in the course of evolution agents acquire the ability to store the short-term memory and, therefore, use it in behavioral strategies with alternative actions. We found that evolution discovered two mechanisms for short-term memory. The first mechanism is integration of sensory signals and ongoing internal neural activity, resulting in emergence of cell groups specialized on alternative actions. And the second mechanism is slow neurodynamical processes that makes possible to code the previous behavioral choice.\n    ",
        "submission_date": "2012-04-14T00:00:00",
        "last_modified_date": "2012-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.3341",
        "title": "Patterns of Social Influence in a Network of Situated Cognitive Agents",
        "authors": [
            "Russell C. Thomas",
            "John S. Gero"
        ],
        "abstract": "This paper presents the results of computational experiments on the effects of social influence on individual and systemic behavior of situated cognitive agents in a product-consumer environment. Paired experiments were performed with identical initial conditions to compare social agents with non- social agents. Experiment results show that social agents are more productive in consuming available products, both in terms of aggregate unit consumption and aggregate utility. But this comes at a cost of individual average utility per unit consumed. In effect, social interaction achieved higher productivity by 'lowering the standards' of individual consumers. While still at an early stage of development, such an agent-based model laboratory is shown to be an effective research tool to investigate rich collective behavior in the context of demanding cognitive tasks.\n    ",
        "submission_date": "2012-04-16T00:00:00",
        "last_modified_date": "2012-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.3436",
        "title": "Explaining Adaptation in Genetic Algorithms With Uniform Crossover: The Hyperclimbing Hypothesis",
        "authors": [
            "Keki M. Burjorjee"
        ],
        "abstract": "The hyperclimbing hypothesis is a hypothetical explanation for adaptation in genetic algorithms with uniform crossover (UGAs). Hyperclimbing is an intuitive, general-purpose, non-local search heuristic applicable to discrete product spaces with rugged or stochastic cost functions. The strength of this heuristic lie in its insusceptibility to local optima when the cost function is deterministic, and its tolerance for noise when the cost function is stochastic. Hyperclimbing works by decimating a search space, i.e. by iteratively fixing the values of small numbers of variables. The hyperclimbing hypothesis holds that UGAs work by implementing efficient hyperclimbing. Proof of concept for this hypothesis comes from the use of a novel analytic technique involving the exploitation of algorithmic symmetry. We have also obtained experimental results that show that a simple tweak inspired by the hyperclimbing hypothesis dramatically improves the performance of a UGA on large, random instances of MAX-3SAT and the Sherrington Kirkpatrick Spin Glasses problem.\n    ",
        "submission_date": "2012-04-16T00:00:00",
        "last_modified_date": "2012-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.3516",
        "title": "When majority voting fails: Comparing quality assurance methods for noisy human computation environment",
        "authors": [
            "Yu-An Sun",
            "Christopher Dance"
        ],
        "abstract": "Quality assurance remains a key topic in human computation research. Prior work indicates that majority voting is effective for low difficulty tasks, but has limitations for harder tasks. This paper explores two methods of addressing this problem: tournament selection and elimination selection, which exploit 2-, 3- and 4-way comparisons between different answers to human computation tasks. Our experimental results and statistical analyses show that both methods produce the correct answer in noisy human computation environment more often than majority voting. Furthermore, we find that the use of 4-way comparisons can significantly reduce the cost of quality assurance relative to the use of 2-way comparisons.\n    ",
        "submission_date": "2012-04-16T00:00:00",
        "last_modified_date": "2012-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.3529",
        "title": "Hardness Results for Approximate Pure Horn CNF Formulae Minimization",
        "authors": [
            "Endre Boros",
            "Aritanan Gruber"
        ],
        "abstract": "We study the hardness of approximation of clause minimum and literal minimum representations of pure Horn functions in $n$ Boolean variables. We show that unless P=NP, it is not possible to approximate in polynomial time the minimum number of clauses and the minimum number of literals of pure Horn CNF representations to within a factor of $2^{\\log^{1-o(1)} n}$. This is the case even when the inputs are restricted to pure Horn 3-CNFs with $O(n^{1+\\varepsilon})$ clauses, for some small positive constant $\\varepsilon$. Furthermore, we show that even allowing sub-exponential time computation, it is still not possible to obtain constant factor approximations for such problems unless the Exponential Time Hypothesis turns out to be false.\n    ",
        "submission_date": "2012-04-16T00:00:00",
        "last_modified_date": "2014-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.3616",
        "title": "Large-Scale Automatic Labeling of Video Events with Verbs Based on Event-Participant Interaction",
        "authors": [
            "Andrei Barbu",
            "Alexander Bridge",
            "Dan Coroian",
            "Sven Dickinson",
            "Sam Mussman",
            "Siddharth Narayanaswamy",
            "Dhaval Salvi",
            "Lara Schmidt",
            "Jiangnan Shangguan",
            "Jeffrey Mark Siskind",
            "Jarrell Waggoner",
            "Song Wang",
            "Jinlian Wei",
            "Yifan Yin",
            "Zhiqi Zhang"
        ],
        "abstract": "We present an approach to labeling short video clips with English verbs as event descriptions. A key distinguishing aspect of this work is that it labels videos with verbs that describe the spatiotemporal interaction between event participants, humans and objects interacting with each other, abstracting away all object-class information and fine-grained image characteristics, and relying solely on the coarse-grained motion of the event participants. We apply our approach to a large set of 22 distinct verb classes and a corpus of 2,584 videos, yielding two surprising outcomes. First, a classification accuracy of greater than 70% on a 1-out-of-22 labeling task and greater than 85% on a variety of 1-out-of-10 subsets of this labeling task is independent of the choice of which of two different time-series classifiers we employ. Second, we achieve this level of accuracy using a highly impoverished intermediate representation consisting solely of the bounding boxes of one or two event participants as a function of time. This indicates that successful event recognition depends more on the choice of appropriate features that characterize the linguistic invariants of the event classes than on the particular classifier algorithms.\n    ",
        "submission_date": "2012-04-16T00:00:00",
        "last_modified_date": "2012-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.3820",
        "title": "Distance Optimal Formation Control on Graphs with a Tight Convergence Time Guarantee",
        "authors": [
            "Jingjin Yu",
            "Steven M. LaValle"
        ],
        "abstract": "For the task of moving a set of indistinguishable agents on a connected graph with unit edge distance to an arbitrary set of goal vertices, free of collisions, we propose a fast distance optimal control algorithm that guides the agents into the desired formation. Moreover, we show that the algorithm also provides a tight convergence time guarantee (time optimality and distance optimality cannot be simultaneously satisfied). Our generic graph formulation allows the algorithm to be applied to scenarios such as grids with holes (modeling obstacles) in arbitrary dimensions. Simulations, available online, confirm our theoretical developments.\n    ",
        "submission_date": "2012-04-17T00:00:00",
        "last_modified_date": "2012-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.3830",
        "title": "Planning Optimal Paths for Multiple Robots on Graphs",
        "authors": [
            "Jingjin Yu",
            "Steven M. LaValle"
        ],
        "abstract": "In this paper, we study the problem of optimal multi-robot path planning (MPP) on graphs. We propose two multiflow based integer linear programming (ILP) models that computes minimum last arrival time and minimum total distance solutions for our MPP formulation, respectively. The resulting algorithms from these ILP models are complete and guaranteed to yield true optimal solutions. In addition, our flexible framework can easily accommodate other variants of the MPP problem. Focusing on the time optimal algorithm, we evaluate its performance, both as a stand alone algorithm and as a generic heuristic for quickly solving large problem instances. Computational results confirm the effectiveness of our method.\n    ",
        "submission_date": "2012-04-17T00:00:00",
        "last_modified_date": "2013-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.4116",
        "title": "An existing, ecologically-successful genus of collectively intelligent artificial creatures",
        "authors": [
            "Benjamin Kuipers"
        ],
        "abstract": "People sometimes worry about the Singularity [Vinge, 1993; Kurzweil, 2005], or about the world being taken over by artificially intelligent robots. I believe the risks of these are very small. However, few people recognize that we already share our world with artificial creatures that participate as intelligent agents in our society: corporations. Our planet is inhabited by two distinct kinds of intelligent beings --- individual humans and corporate entities --- whose natures and interests are intimately linked. To co-exist well, we need to find ways to define the rights and responsibilities of both individual humans and corporate entities, and to find ways to ensure that corporate entities behave as responsible members of society.\n    ",
        "submission_date": "2012-04-18T00:00:00",
        "last_modified_date": "2012-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.4294",
        "title": "Learning in Riemannian Orbifolds",
        "authors": [
            "Brijnesh J. Jain",
            "Klaus Obermayer"
        ],
        "abstract": "Learning in Riemannian orbifolds is motivated by existing machine learning algorithms that directly operate on finite combinatorial structures such as point patterns, trees, and graphs. These methods, however, lack statistical justification. This contribution derives consistency results for learning problems in structured domains and thereby generalizes learning in vector spaces and manifolds.\n    ",
        "submission_date": "2012-04-19T00:00:00",
        "last_modified_date": "2012-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.4990",
        "title": "Objective Function Designing Led by User Preferences Acquisition",
        "authors": [
            "Patrick Taillandier",
            "Julien Gaffuri"
        ],
        "abstract": "Many real world problems can be defined as optimisation problems in which the aim is to maximise an objective function. The quality of obtained solution is directly linked to the pertinence of the used objective function. However, designing such function, which has to translate the user needs, is usually fastidious. In this paper, a method to help user objective functions designing is proposed. Our approach, which is highly interactive, is based on man machine dialogue and more particularly on the comparison of problem instance solutions by the user. We propose an experiment in the domain of cartographic generalisation that shows promising results.\n    ",
        "submission_date": "2012-04-23T00:00:00",
        "last_modified_date": "2012-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.5213",
        "title": "Solving Weighted Voting Game Design Problems Optimally: Representations, Synthesis, and Enumeration",
        "authors": [
            "Bart de Keijzer",
            "Tomas B. Klos",
            "Yingqian Zhang"
        ],
        "abstract": "We study the inverse power index problem for weighted voting games: the problem of finding a weighted voting game in which the power of the players is as close as possible to a certain target distribution. Our goal is to find algorithms that solve this problem exactly. Thereto, we study various subclasses of simple games, and their associated representation methods. We survey algorithms and impossibility results for the synthesis problem, i.e., converting a representation of a simple game into another representation.\n",
        "submission_date": "2012-04-23T00:00:00",
        "last_modified_date": "2013-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.5316",
        "title": "ILexicOn: toward an ECD-compliant interlingual lexical ontology described with semantic web formalisms",
        "authors": [
            "Maxime Lefran\u00e7ois",
            "Fabien Gandon"
        ],
        "abstract": "We are interested in bridging the world of natural language and the world of the semantic web in particular to support natural multilingual access to the web of data. In this paper we introduce a new type of lexical ontology called interlingual lexical ontology (ILexicOn), which uses semantic web formalisms to make each interlingual lexical unit class (ILUc) support the projection of its semantic decomposition on itself. After a short overview of existing lexical ontologies, we briefly introduce the semantic web formalisms we use. We then present the three layered architecture of our approach: i) the interlingual lexical meta-ontology (ILexiMOn); ii) the ILexicOn where ILUcs are formally defined; iii) the data layer. We illustrate our approach with a standalone ILexicOn, and introduce and explain a concise human-readable notation to represent ILexicOns. Finally, we show how semantic web formalisms enable the projection of a semantic decomposition on the decomposed ILUc.\n    ",
        "submission_date": "2012-04-24T00:00:00",
        "last_modified_date": "2012-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.5357",
        "title": "Learning AMP Chain Graphs under Faithfulness",
        "authors": [
            "Jose M. Pe\u00f1a"
        ],
        "abstract": "This paper deals with chain graphs under the alternative Andersson-Madigan-Perlman (AMP) interpretation. In particular, we present a constraint based algorithm for learning an AMP chain graph a given probability distribution is faithful to. We also show that the extension of Meek's conjecture to AMP chain graphs does not hold, which compromises the development of efficient and correct score+search learning algorithms under assumptions weaker than faithfulness.\n    ",
        "submission_date": "2012-04-24T00:00:00",
        "last_modified_date": "2012-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.5661",
        "title": "Transmission of distress in a bank credit network",
        "authors": [
            "Yoshiharu Maeno",
            "Satoshi Morinaga",
            "Hirokazu Matsushima",
            "Kenichi Amagai"
        ],
        "abstract": "The European sovereign debt crisis has impaired many European banks. The distress on the European banks may transmit worldwide, and result in a large-scale knock-on default of financial institutions. This study presents a computer simulation model to analyze the risk of insolvency of banks and defaults in a bank credit network. Simulation experiments reproduce the knock-on default, and quantify the impact which is imposed on the number of bank defaults by heterogeneity of the bank credit network, the equity capital ratio of banks, and the capital surcharge on big banks.\n    ",
        "submission_date": "2012-04-25T00:00:00",
        "last_modified_date": "2012-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.5805",
        "title": "Intelligent Automated Diagnosis of Client Device Bottlenecks in Private Clouds",
        "authors": [
            "C. Widanapathirana",
            "J. Li",
            "Y.A. Sekercioglu",
            "M. Ivanovich",
            "P. Fitzpatrick"
        ],
        "abstract": "We present an automated solution for rapid diagnosis of client device problems in private cloud environments: the Intelligent Automated Client Diagnostic (IACD) system. Clients are diagnosed with the aid of Transmission Control Protocol (TCP) packet traces, by (i) observation of anomalous artifacts occurring as a result of each fault and (ii) subsequent use of the inference capabilities of soft-margin Support Vector Machine (SVM) classifiers. The IACD system features a modular design and is extendible to new faults, with detection capability unaffected by the TCP variant used at the client. Experimental evaluation of the IACD system in a controlled environment demonstrated an overall diagnostic accuracy of 98%.\n    ",
        "submission_date": "2012-04-26T00:00:00",
        "last_modified_date": "2012-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.5859",
        "title": "On the Complexity of Finding Second-Best Abductive Explanations",
        "authors": [
            "Paolo Liberatore",
            "Marco Schaerf"
        ],
        "abstract": "While looking for abductive explanations of a given set of manifestations, an ordering between possible solutions is often assumed. The complexity of finding/verifying optimal solutions is already known. In this paper we consider the computational complexity of finding second-best solutions. We consider different orderings, and consider also different possible definitions of what a second-best solution is.\n    ",
        "submission_date": "2012-04-26T00:00:00",
        "last_modified_date": "2015-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.5981",
        "title": "Containment, Equivalence and Coreness from CSP to QCSP and beyond",
        "authors": [
            "Florent Madelaine",
            "Barnaby Martin"
        ],
        "abstract": "The constraint satisfaction problem (CSP) and its quantified extensions, whether without (QCSP) or with disjunction (QCSP_or), correspond naturally to the model checking problem for three increasingly stronger fragments of positive first-order logic. Their complexity is often studied when parameterised by a fixed model, the so-called template.\n",
        "submission_date": "2012-04-26T00:00:00",
        "last_modified_date": "2012-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.6233",
        "title": "Strong Backdoors to Bounded Treewidth SAT",
        "authors": [
            "Serge Gaspers",
            "Stefan Szeider"
        ],
        "abstract": "There are various approaches to exploiting \"hidden structure\" in instances of hard combinatorial problems to allow faster algorithms than for general unstructured or random instances. For SAT and its counting version #SAT, hidden structure has been exploited in terms of decomposability and strong backdoor sets. Decomposability can be considered in terms of the treewidth of a graph that is associated with the given CNF formula, for instance by considering clauses and variables as vertices of the graph, and making a variable adjacent with all the clauses it appears in. On the other hand, a strong backdoor set of a CNF formula is a set of variables such that each possible partial assignment to this set moves the formula into a fixed class for which (#)SAT can be solved in polynomial time.\n",
        "submission_date": "2012-04-27T00:00:00",
        "last_modified_date": "2012-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.6529",
        "title": "Generalising unit-refutation completeness and SLUR via nested input resolution",
        "authors": [
            "Matthew Gwynne",
            "Oliver Kullmann"
        ],
        "abstract": "We introduce two hierarchies of clause-sets, SLUR_k and UC_k, based on the classes SLUR (Single Lookahead Unit Refutation), introduced in 1995, and UC (Unit refutation Complete), introduced in 1994.\n",
        "submission_date": "2012-04-29T00:00:00",
        "last_modified_date": "2013-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.6552",
        "title": "A Game-Theoretic Model Motivated by the DARPA Network Challenge",
        "authors": [
            "Rajesh Chitnis",
            "MohammadTaghi Hajiaghayi",
            "Jonathan Katz",
            "Koyel Mukherjee"
        ],
        "abstract": "In this paper we propose a game-theoretic model to analyze events similar to the 2009 \\emph{DARPA Network Challenge}, which was organized by the Defense Advanced Research Projects Agency (DARPA) for exploring the roles that the Internet and social networks play in incentivizing wide-area collaborations. The challenge was to form a group that would be the first to find the locations of ten moored weather balloons across the United States. We consider a model in which $N$ people (who can form groups) are located in some topology with a fixed coverage volume around each person's geographical location. We consider various topologies where the players can be located such as the Euclidean $d$-dimension space and the vertices of a graph. A balloon is placed in the space and a group wins if it is the first one to report the location of the balloon. A larger team has a higher probability of finding the balloon, but we assume that the prize money is divided equally among the team members. Hence there is a competing tension to keep teams as small as possible.\n",
        "submission_date": "2012-04-30T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.0622",
        "title": "No-Regret Learning in Extensive-Form Games with Imperfect Recall",
        "authors": [
            "Marc Lanctot",
            "Richard Gibson",
            "Neil Burch",
            "Martin Zinkevich",
            "Michael Bowling"
        ],
        "abstract": "Counterfactual Regret Minimization (CFR) is an efficient no-regret learning algorithm for decision problems modeled as extensive games. CFR's regret bounds depend on the requirement of perfect recall: players always remember information that was revealed to them and the order in which it was revealed. In games without perfect recall, however, CFR's guarantees do not apply. In this paper, we present the first regret bound for CFR when applied to a general class of games with imperfect recall. In addition, we show that CFR applied to any abstraction belonging to our general class results in a regret bound not just for the abstract game, but for the full game as well. We verify our theory and show how imperfect recall can be used to trade a small increase in regret for a significant reduction in memory in three domains: die-roll poker, phantom tic-tac-toe, and Bluff.\n    ",
        "submission_date": "2012-05-03T00:00:00",
        "last_modified_date": "2012-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.0917",
        "title": "VIQI: A New Approach for Visual Interpretation of Deep Web Query Interfaces",
        "authors": [
            "Radhouane Boughamoura",
            "Lobna Hlaoua",
            "Mohamed Nazih Omri"
        ],
        "abstract": "Deep Web databases contain more than 90% of pertinent information of the Web. Despite their importance, users don't profit of this treasury. Many deep web services are offering competitive services in term of prices, quality of service, and facilities. As the number of services is growing rapidly, users have difficulty to ask many web services in the same time. In this paper, we imagine a system where users have the possibility to formulate one query using one query interface and then the system translates query to the rest of query interfaces. However, interfaces are created by designers in order to be interpreted visually by users, machines can not interpret query from a given interface. We propose a new approach which emulates capacity of interpretation of users and extracts query from deep web query interfaces. Our approach has proved good performances on two standard datasets.\n    ",
        "submission_date": "2012-05-04T00:00:00",
        "last_modified_date": "2012-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.1638",
        "title": "Document summarization using positive pointwise mutual information",
        "authors": [
            "Aji S",
            "Ramachandra Kaimal"
        ],
        "abstract": "The degree of success in document summarization processes depends on the performance of the method used in identifying significant sentences in the documents. The collection of unique words characterizes the major signature of the document, and forms the basis for Term-Sentence-Matrix (TSM). The Positive Pointwise Mutual Information, which works well for measuring semantic similarity in the Term-Sentence-Matrix, is used in our method to assign weights for each entry in the Term-Sentence-Matrix. The Sentence-Rank-Matrix generated from this weighted TSM, is then used to extract a summary from the document. Our experiments show that such a method would outperform most of the existing methods in producing summaries from large documents.\n    ",
        "submission_date": "2012-05-08T00:00:00",
        "last_modified_date": "2012-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2046",
        "title": "Multiset Estimates and Combinatorial Synthesis",
        "authors": [
            "Mark Sh. Levin"
        ],
        "abstract": "The paper addresses an approach to ordinal assessment of alternatives based on assignment of elements into an ordinal scale. Basic versions of the assessment problems are formulated while taking into account the number of levels at a basic ordinal scale [1,2,...,l] and the number of assigned elements (e.g., 1,2,3). The obtained estimates are multisets (or bags) (cardinality of the multiset equals a constant). Scale-posets for the examined assessment problems are presented. 'Interval multiset estimates' are suggested. Further, operations over multiset estimates are examined: (a) integration of multiset estimates, (b) proximity for multiset estimates, (c) comparison of multiset estimates, (d) aggregation of multiset estimates, and (e) alignment of multiset estimates. Combinatorial synthesis based on morphological approach is examined including the modified version of the approach with multiset estimates of design alternatives. Knapsack-like problems with multiset estimates are briefly described as well. The assessment approach, multiset-estimates, and corresponding combinatorial problems are illustrated by numerical examples.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2382",
        "title": "Mesh Learning for Classifying Cognitive Processes",
        "authors": [
            "Mete Ozay",
            "Ilke \u00d6ztekin",
            "Uygar \u00d6ztekin",
            "Fatos T. Yarman Vural"
        ],
        "abstract": "A relatively recent advance in cognitive neuroscience has been multi-voxel pattern analysis (MVPA), which enables researchers to decode brain states and/or the type of information represented in the brain during a cognitive operation. MVPA methods utilize machine learning algorithms to distinguish among types of information or cognitive states represented in the brain, based on distributed patterns of neural activity. In the current investigation, we propose a new approach for representation of neural data for pattern analysis, namely a Mesh Learning Model. In this approach, at each time instant, a star mesh is formed around each voxel, such that the voxel corresponding to the center node is surrounded by its p-nearest neighbors. The arc weights of each mesh are estimated from the voxel intensity values by least squares method. The estimated arc weights of all the meshes, called Mesh Arc Descriptors (MADs), are then used to train a classifier, such as Neural Networks, k-Nearest Neighbor, Na\u00efve Bayes and Support Vector Machines. The proposed Mesh Model was tested on neuroimaging data acquired via functional magnetic resonance imaging (fMRI) during a recognition memory experiment using categorized word lists, employing a previously established experimental paradigm (\u00d6ztekin & Badre, 2011). Results suggest that the proposed Mesh Learning approach can provide an effective algorithm for pattern analysis of brain activity during cognitive processing.\n    ",
        "submission_date": "2012-05-10T00:00:00",
        "last_modified_date": "2015-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2603",
        "title": "A Bayesian Framework for Community Detection Integrating Content and Link",
        "authors": [
            "Tianbao Yang",
            "Rong Jin",
            "Yun Chi",
            "Shenghuo Zhu"
        ],
        "abstract": "This paper addresses the problem of community detection in networked data that combines link and content analysis. Most existing work combines link and content information by a generative model. There are two major shortcomings with the existing approaches. First, they assume that the probability of creating a link between two nodes is determined only by the community memberships of the nodes; however other factors (e.g. popularity) could also affect the link pattern. Second, they use generative models to model the content of individual nodes, whereas these generative models are vulnerable to the content attributes that are irrelevant to communities. We propose a Bayesian framework for combining link and content information for community detection that explicitly addresses these shortcomings. A new link model is presented that introduces a random variable to capture the node popularity when deciding the link between two nodes; a discriminative model is used to determine the community membership of a node by its content. An approximate inference algorithm is presented for efficient Bayesian inference. Our empirical study shows that the proposed framework outperforms several state-of-theart approaches in combining link and content information for community detection.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2606",
        "title": "Exploring compact reinforcement-learning representations with linear regression",
        "authors": [
            "Thomas J. Walsh",
            "Istvan Szita",
            "Carlos Diuk",
            "Michael L. Littman"
        ],
        "abstract": "This paper presents a new algorithm for online linear regression whose efficiency guarantees satisfy the requirements of the KWIK (Knows What It Knows) framework. The algorithm improves on the complexity bounds of the current state-of-the-art procedure in this setting. We explore several applications of this algorithm for learning compact reinforcement-learning representations. We show that KWIK linear regression can be used to learn the reward function of a factored MDP and the probabilities of action outcomes in Stochastic STRIPS and Object Oriented MDPs, none of which have been proven to be efficiently learnable in the RL setting before. We also combine KWIK linear regression with other KWIK learners to learn larger portions of these models, including experiments on learning factored MDP transition and reward functions together.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2615",
        "title": "Effects of Treatment on the Treated: Identification and Generalization",
        "authors": [
            "Ilya Shpitser",
            "Judea Pearl"
        ],
        "abstract": "Many applications of causal analysis call for assessing, retrospectively, the effect of withholding an action that has in fact been implemented. This counterfactual quantity, sometimes called \"effect of treatment on the treated,\" (ETT) have been used to to evaluate educational programs, critic public policies, and justify individual decision making. In this paper we explore the conditions under which ETT can be estimated from (i.e., identified in) experimental and/or observational studies. We show that, when the action invokes a singleton variable, the conditions for ETT identification have simple characterizations in terms of causal diagrams. We further give a graphical characterization of the conditions under which the effects of multiple treatments on the treated can be identified, as well as ways in which the ETT estimand can be constructed from both interventional and observational distributions.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2636",
        "title": "Monolingual Probabilistic Programming Using Generalized Coroutines",
        "authors": [
            "Oleg Kiselyov",
            "Chung-chieh Shan"
        ],
        "abstract": "Probabilistic programming languages and modeling toolkits are two modular ways to build and reuse stochastic models and inference procedures. Combining strengths of both, we express models and inference as generalized coroutines in the same general-purpose language. We use existing facilities of the language, such as rich libraries, optimizing compilers, and types, to develop concise, declarative, and realistic models with competitive performance on exact and approximate inference. In particular, a wide range of models can be expressed using memoization. Because deterministic parts of models run at full speed, custom inference procedures are trivial to incorporate, and inference procedures can reason about themselves without interpretive overhead. Within this framework, we introduce a new, general algorithm for importance sampling with look-ahead.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2638",
        "title": "Temporal Action-Graph Games: A New Representation for Dynamic Games",
        "authors": [
            "Albert Xin Jiang",
            "Kevin Leyton-Brown",
            "Avi Pfeffer"
        ],
        "abstract": "In this paper we introduce temporal action graph games (TAGGs), a novel graphical representation of imperfect-information extensive form games. We show that when a game involves anonymity or context-specific utility independencies, its encoding as a TAGG can be much more compact than its direct encoding as a multiagent influence diagram (MAID).We also show that TAGGs can be understood as indirect MAID encodings in which many deterministic chance nodes are introduced. We provide an algorithm for computing with TAGGs, and show both theoretically and empirically that our approach improves significantly on the previous state of the art.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2644",
        "title": "First-Order Mixed Integer Linear Programming",
        "authors": [
            "Geoffrey Gordon",
            "Sue Ann Hong",
            "Miroslav Dudik"
        ],
        "abstract": "Mixed integer linear programming (MILP) is a powerful representation often used to formulate decision-making problems under uncertainty. However, it lacks a natural mechanism to reason about objects, classes of objects, and relations. First-order logic (FOL), on the other hand, excels at reasoning about classes of objects, but lacks a rich representation of uncertainty. While representing propositional logic in MILP has been extensively explored, no theory exists yet for fully combining FOL with MILP. We propose a new representation, called first-order programming or FOP, which subsumes both FOL and MILP. We establish formal methods for reasoning about first order programs, including a sound and complete lifted inference procedure for integer first order programs. Since FOP can offer exponential savings in representation and proof size compared to FOL, and since representations and proofs are never significantly longer in FOP than in FOL, we anticipate that inference in FOP will be more tractable than inference in FOL for corresponding problems.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.3109",
        "title": "Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search",
        "authors": [
            "Arthur Guez",
            "David Silver",
            "Peter Dayan"
        ],
        "abstract": "Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, finding the resulting Bayes-optimal policies is notoriously taxing, since the search space becomes enormous. In this paper we introduce a tractable, sample-based method for approximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our approach outperformed prior Bayesian model-based RL algorithms by a significant margin on several well-known benchmark problems -- because it avoids expensive applications of Bayes rule within the search tree by lazily sampling models from the current beliefs. We illustrate the advantages of our approach by showing it working in an infinite state space domain which is qualitatively out of reach of almost all previous work in Bayesian exploration.\n    ",
        "submission_date": "2012-05-14T00:00:00",
        "last_modified_date": "2013-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.3137",
        "title": "Unsupervised Discovery of Mid-Level Discriminative Patches",
        "authors": [
            "Saurabh Singh",
            "Abhinav Gupta",
            "Alexei A. Efros"
        ],
        "abstract": "The goal of this paper is to discover a set of discriminative patches which can serve as a fully unsupervised mid-level visual representation. The desired patches need to satisfy two requirements: 1) to be representative, they need to occur frequently enough in the visual world; 2) to be discriminative, they need to be different enough from the rest of the visual world. The patches could correspond to parts, objects, \"visual phrases\", etc. but are not restricted to be any one of them. We pose this as an unsupervised discriminative clustering problem on a huge dataset of image patches. We use an iterative procedure which alternates between clustering and training discriminative classifiers, while applying careful cross-validation at each step to prevent overfitting. The paper experimentally demonstrates the effectiveness of discriminative patches as an unsupervised mid-level visual representation, suggesting that it could be used in place of visual words for many tasks. Furthermore, discriminative patches can also be used in a supervised regime, such as scene classification, where they demonstrate state-of-the-art performance on the MIT Indoor-67 dataset.\n    ",
        "submission_date": "2012-05-14T00:00:00",
        "last_modified_date": "2012-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.3336",
        "title": "Distribution of the search of evolutionary product unit neural networks for classification",
        "authors": [
            "A.J. Tall\u00f3n-Ballesteros",
            "P.A. Guti\u00e9rrez-Pe\u00f1a",
            "C. Herv\u00e1s-Mart\u00ednez"
        ],
        "abstract": "This paper deals with the distributed processing in the search for an optimum classification model using evolutionary product unit neural networks. For this distributed search we used a cluster of computers. Our objective is to obtain a more efficient design than those net architectures which do not use a distributed process and which thus result in simpler designs. In order to get the best classification models we use evolutionary algorithms to train and design neural networks, which require a very time consuming computation. The reasons behind the need for this distribution are various. It is complicated to train this type of nets because of the difficulty entailed in determining their architecture due to the complex error surface. On the other hand, the use of evolutionary algorithms involves running a great number of tests with different seeds and parameters, thus resulting in a high computational cost\n    ",
        "submission_date": "2012-05-15T00:00:00",
        "last_modified_date": "2012-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.3997",
        "title": "Free Energy and the Generalized Optimality Equations for Sequential Decision Making",
        "authors": [
            "Pedro A. Ortega",
            "Daniel A. Braun"
        ],
        "abstract": "The free energy functional has recently been proposed as a variational principle for bounded rational decision-making, since it instantiates a natural trade-off between utility gains and information processing costs that can be axiomatically derived. Here we apply the free energy principle to general decision trees that include both adversarial and stochastic environments. We derive generalized sequential optimality equations that not only include the Bellman optimality equations as a limit case, but also lead to well-known decision-rules such as Expectimax, Minimax and Expectiminimax. We show how these decision-rules can be derived from a single free energy principle that assigns a resource parameter to each node in the decision tree. These resource parameters express a concrete computational cost that can be measured as the amount of samples that are needed from the distribution that belongs to each node. The free energy principle therefore provides the normative basis for generalized optimality equations that account for both adversarial and stochastic environments.\n    ",
        "submission_date": "2012-05-17T00:00:00",
        "last_modified_date": "2012-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.4213",
        "title": "Online Structured Prediction via Coactive Learning",
        "authors": [
            "Pannaga Shivaswamy",
            "Thorsten Joachims"
        ],
        "abstract": "We propose Coactive Learning as a model of interaction between a learning system and a human user, where both have the common goal of providing results of maximum utility to the user. At each step, the system (e.g. search engine) receives a context (e.g. query) and predicts an object (e.g. ranking). The user responds by correcting the system if necessary, providing a slightly improved -- but not necessarily optimal -- object as feedback. We argue that such feedback can often be inferred from observable user behavior, for example, from clicks in web-search. Evaluating predictions by their cardinal utility to the user, we propose efficient learning algorithms that have ${\\cal O}(\\frac{1}{\\sqrt{T}})$ average regret, even though the learning algorithm never observes cardinal utility values as in conventional online learning. We demonstrate the applicability of our model and learning algorithms on a movie recommendation task, as well as ranking for web-search.\n    ",
        "submission_date": "2012-05-18T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.4295",
        "title": "Efficient Methods for Unsupervised Learning of Probabilistic Models",
        "authors": [
            "Jascha Sohl-Dickstein"
        ],
        "abstract": "In this thesis I develop a variety of techniques to train, evaluate, and sample from intractable and high dimensional probabilistic models. Abstract exceeds arXiv space limitations -- see PDF.\n    ",
        "submission_date": "2012-05-19T00:00:00",
        "last_modified_date": "2012-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.4655",
        "title": "The View-Update Problem for Indefinite Databases",
        "authors": [
            "Luciano Caroprese",
            "Irina Trubitsyna",
            "Miroslaw Truszczynski",
            "Ester Zumpano"
        ],
        "abstract": "This paper introduces and studies a declarative framework for updating views over indefinite databases. An indefinite database is a database with null values that are represented, following the standard database approach, by a single null constant. The paper formalizes views over such databases as indefinite deductive databases, and defines for them several classes of database repairs that realize view-update requests. Most notable is the class of constrained repairs. Constrained repairs change the database \"minimally\" and avoid making arbitrary commitments. They narrow down the space of alternative ways to fulfill the view-update request to those that are grounded, in a certain strong sense, in the database, the view and the view-update request.\n    ",
        "submission_date": "2012-05-21T00:00:00",
        "last_modified_date": "2012-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.5823",
        "title": "Foreword: A Computable Universe, Understanding Computation and Exploring Nature As Computation",
        "authors": [
            "Roger Penrose"
        ],
        "abstract": "I am most honoured to have the privilege to present the Foreword to this fascinating and wonderfully varied collection of contributions, concerning the nature of computation and of its deep connection with the operation of those basic laws, known or yet unknown, governing the universe in which we live. Fundamentally deep questions are indeed being grappled with here, and the fact that we find so many different viewpoints is something to be expected, since, in truth, we know little about the foundational nature and origins of these basic laws, despite the immense precision that we so often find revealed in them. Accordingly, it is not surprising that within the viewpoints expressed here is some unabashed speculation, occasionally bordering on just partially justified guesswork, while elsewhere we find a good deal of precise reasoning, some in the form of rigorous mathematical theorems. Both of these are as should be, for without some inspired guesswork we cannot have new ideas as to where look in order to make genuinely new progress, and without precise mathematical reasoning, no less than in precise observation, we cannot know when we are right -- or, more usually, when we are wrong.\n    ",
        "submission_date": "2012-05-25T00:00:00",
        "last_modified_date": "2012-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.6179",
        "title": "A Mixed Integer Programming Model Formulation for Solving the Lot-Sizing Problem",
        "authors": [
            "Maryam Mohammadi",
            "Masine Md. Tap"
        ],
        "abstract": "This paper addresses a mixed integer programming (MIP) formulation for the multi-item uncapacitated lot-sizing problem that is inspired from the trailer manufacturer. The proposed MIP model has been utilized to find out the optimum order quantity, optimum order time, and the minimum total cost of purchasing, ordering, and holding over the predefined planning horizon. This problem is known as NP-hard problem. The model was presented in an optimal software form using LINGO 13.0.\n    ",
        "submission_date": "2012-05-28T00:00:00",
        "last_modified_date": "2012-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.1074",
        "title": "Memetic Artificial Bee Colony Algorithm for Large-Scale Global Optimization",
        "authors": [
            "Iztok Fister",
            "Iztok Fister Jr.",
            "Janez Brest",
            "Viljem \u017dumer"
        ],
        "abstract": "Memetic computation (MC) has emerged recently as a new paradigm of efficient algorithms for solving the hardest optimization problems. On the other hand, artificial bees colony (ABC) algorithms demonstrate good performances when solving continuous and combinatorial optimization problems. This study tries to use these technologies under the same roof. As a result, a memetic ABC (MABC) algorithm has been developed that is hybridized with two local search heuristics: the Nelder-Mead algorithm (NMA) and the random walk with direction exploitation (RWDE). The former is attended more towards exploration, while the latter more towards exploitation of the search space. The stochastic adaptation rule was employed in order to control the balancing between exploration and exploitation. This MABC algorithm was applied to a Special suite on Large Scale Continuous Global Optimization at the 2012 IEEE Congress on Evolutionary Computation. The obtained results the MABC are comparable with the results of DECC-G, DECC-G*, and MLCC.\n    ",
        "submission_date": "2012-06-05T00:00:00",
        "last_modified_date": "2012-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.1728",
        "title": "Aggregating Content and Network Information to Curate Twitter User Lists",
        "authors": [
            "Derek Greene",
            "Gavin Sheridan",
            "Barry Smyth",
            "P\u00e1draig Cunningham"
        ],
        "abstract": "Twitter introduced user lists in late 2009, allowing users to be grouped according to meaningful topics or themes. Lists have since been adopted by media outlets as a means of organising content around news stories. Thus the curation of these lists is important - they should contain the key information gatekeepers and present a balanced perspective on a story. Here we address this list curation process from a recommender systems perspective. We propose a variety of criteria for generating user list recommendations, based on content analysis, network analysis, and the \"crowdsourcing\" of existing user lists. We demonstrate that these types of criteria are often only successful for datasets with certain characteristics. To resolve this issue, we propose the aggregation of these different \"views\" of a news story on Twitter to produce more accurate user recommendations to support the curation process.\n    ",
        "submission_date": "2012-06-08T00:00:00",
        "last_modified_date": "2012-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.1898",
        "title": "A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function",
        "authors": [
            "Pedro A. Ortega",
            "Jordi Grau-Moya",
            "Tim Genewein",
            "David Balduzzi",
            "Daniel A. Braun"
        ],
        "abstract": "We propose a novel Bayesian approach to solve stochastic optimization problems that involve finding extrema of noisy, nonlinear functions. Previous work has focused on representing possible functions explicitly, which leads to a two-step procedure of first, doing inference over the function space and second, finding the extrema of these functions. Here we skip the representation step and directly model the distribution over extrema. To this end, we devise a non-parametric conjugate prior based on a kernel regressor. The resulting posterior distribution directly captures the uncertainty over the maximum of the unknown function. We illustrate the effectiveness of our model by optimizing a noisy, high-dimensional, non-convex objective function.\n    ",
        "submission_date": "2012-06-09T00:00:00",
        "last_modified_date": "2012-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.1926",
        "title": "The hardest logic puzzle ever becomes even tougher",
        "authors": [
            "Nikolay Novozhilov"
        ],
        "abstract": "\"The hardest logic puzzle ever\" presented by George Boolos became a target for philosophers and logicians who tried to modify it and make it even tougher. I propose further modification of the original puzzle where part of the available information is eliminated but the solution is still possible. The solution also gives interesting ideas on logic behind discovery of unknown language.\n    ",
        "submission_date": "2012-06-09T00:00:00",
        "last_modified_date": "2012-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.2082",
        "title": "Dimension Independent Similarity Computation",
        "authors": [
            "Reza Bosagh Zadeh",
            "Ashish Goel"
        ],
        "abstract": "We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO) to compute all pairwise similarities between very high dimensional sparse vectors. All of our results are provably independent of dimension, meaning apart from the initial cost of trivially reading in the data, all subsequent operations are independent of the dimension, thus the dimension can be very large. We study Cosine, Dice, Overlap, and the Jaccard similarity measures. For Jaccard similiarity we include an improved version of MinHash. Our results are geared toward the MapReduce framework. We empirically validate our theorems at large scale using data from the social networking site Twitter. At time of writing, our algorithms are live in production at ",
        "submission_date": "2012-06-11T00:00:00",
        "last_modified_date": "2013-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.2742",
        "title": "Online open neuroimaging mass meta-analysis",
        "authors": [
            "Finn \u00c5rup Nielsen",
            "Matthew J. Kempton",
            "Steven C. R. Williams"
        ],
        "abstract": "We describe a system for meta-analysis where a wiki stores numerical data in a simple format and a web service performs the numerical computation.\n",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.2802",
        "title": "Critical behavior in a cross-situational lexicon learning scenario",
        "authors": [
            "P. F. C. Tilles",
            "J. F. Fontanari"
        ],
        "abstract": "The associationist account for early word-learning is based on the co-occurrence between objects and words. Here we examine the performance of a simple associative learning algorithm for acquiring the referents of words in a cross-situational scenario affected by noise produced by out-of-context words. We find a critical value of the noise parameter $\\gamma_c$ above which learning is impossible. We use finite-size scaling to show that the sharpness of the transition persists across a region of order $\\tau^{-1/2}$ about $\\gamma_c$, where $\\tau$ is the number of learning trials, as well as to obtain the learning error (scaling function) in the critical region. In addition, we show that the distribution of durations of periods when the learning error is zero is a power law with exponent -3/2 at the critical point.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3234",
        "title": "Adaptive Inference on General Graphical Models",
        "authors": [
            "Umut A. Acar",
            "Alexander T. Ihler",
            "Ramgopal Mettu",
            "Ozgur Sumer"
        ],
        "abstract": "Many algorithms and applications involve repeatedly solving variations of the same inference problem; for example we may want to introduce new evidence to the model or perform updates to conditional dependencies. The goal of adaptive inference is to take advantage of what is preserved in the model and perform inference more rapidly than from scratch. In this paper, we describe techniques for adaptive inference on general graphs that support marginal computation and updates to the conditional probabilities and dependencies in logarithmic time. We give experimental results for an implementation of our algorithm, and demonstrate its potential performance benefit in the study of protein structure.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3235",
        "title": "Identifying reasoning patterns in games",
        "authors": [
            "Dimitrios Antos",
            "Avi Pfeffer"
        ],
        "abstract": "We present an algorithm that identifies the reasoning patterns of agents in a game, by iteratively examining the graph structure of its Multi-Agent Influence Diagram (MAID) representation. If the decision of an agent participates in no reasoning patterns, then we can effectively ignore that decision for the purpose of calculating a Nash equilibrium for the game. In some cases, this can lead to exponential time savings in the process of equilibrium calculation. Moreover, our algorithm can be used to enumerate the reasoning patterns in a game, which can be useful for constructing more effective computerized agents interacting with humans.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3239",
        "title": "On Identifying Total Effects in the Presence of Latent Variables and Selection bias",
        "authors": [
            "Zhihong Cai",
            "Manabu Kuroki"
        ],
        "abstract": "Assume that cause-effect relationships between variables can be described as a directed acyclic graph and the corresponding linear structural equation ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3240",
        "title": "Complexity of Inference in Graphical Models",
        "authors": [
            "Venkat Chandrasekaran",
            "Nathan Srebro",
            "Prahladh Harsha"
        ],
        "abstract": "It is well-known that inference in graphical models is hard in the worst case, but tractable for models with bounded treewidth. We ask whether treewidth is the only structural criterion of the underlying graph that enables tractable inference. In other words, is there some class of structures with unbounded treewidth in which inference is tractable? Subject to a combinatorial hypothesis due to Robertson et al. (1994), we show that low treewidth is indeed the only structural restriction that can ensure tractability. Thus, even for the \"best case\" graph structure, there is no inference algorithm with complexity polynomial in the treewidth.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3253",
        "title": "Learning and Solving Many-Player Games through a Cluster-Based Representation",
        "authors": [
            "Sevan G. Ficici",
            "David C. Parkes",
            "Avi Pfeffer"
        ],
        "abstract": "In addressing the challenge of exponential scaling with the number of agents we adopt a cluster-based representation to approximately solve asymmetric games of very many players. A cluster groups together agents with a similar \"strategic view\" of the game. We learn the clustered approximation from data consisting of strategy profiles and payoffs, which may be obtained from observations of play or access to a simulator. Using our clustering we construct a reduced \"twins\" game in which each cluster is associated with two players of the reduced game. This allows our representation to be individually- responsive because we align the interests of every individual agent with the strategy of its cluster. Our approach provides agents with higher payoffs and lower regret on average than model-free methods as well as previous cluster-based methods, and requires only few observations for learning to be successful. The \"twins\" approach is shown to be an important component of providing these low regret approximations.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3255",
        "title": "Church: a language for generative models",
        "authors": [
            "Noah Goodman",
            "Vikash Mansinghka",
            "Daniel M. Roy",
            "Keith Bonawitz",
            "Joshua B. Tenenbaum"
        ],
        "abstract": "We introduce Church, a universal language for describing stochastic generative processes. Church is based on the Lisp model of lambda calculus, containing a pure Lisp as its deterministic subset. The semantics of Church is defined in terms of evaluation histories and conditional distributions on such histories. Church also includes a novel language construct, the stochastic memoizer, which enables simple description of many complex non-parametric models. We illustrate language features through several examples, including: a generalized Bayes net in which parameters cluster over trials, infinite PCFGs, planning by inference, and various non-parametric clustering models. Finally, we show how to implement query on any Church program, exactly and approximately, using Monte Carlo techniques.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2014-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3260",
        "title": "Causal discovery of linear acyclic models with arbitrary distributions",
        "authors": [
            "Patrik O. Hoyer",
            "Aapo Hyvarinen",
            "Richard Scheines",
            "Peter L. Spirtes",
            "Joseph Ramsey",
            "Gustavo Lacerda",
            "Shohei Shimizu"
        ],
        "abstract": "An important task in data analysis is the discovery of causal relationships between observed variables. For continuous-valued data, linear acyclic causal models are commonly used to model the data-generating process, and the inference of such models is a well-studied problem. However, existing methods have significant limitations. Methods based on conditional independencies (Spirtes et al. 1993; Pearl 2000) cannot distinguish between independence-equivalent models, whereas approaches purely based on Independent Component Analysis (Shimizu et al. 2006) are inapplicable to data which is partially Gaussian. In this paper, we generalize and combine the two approaches, to yield a method able to learn the model structure in many cases for which the previous methods provide answers that are either incorrect or are not as informative as possible. We give exact graphical conditions for when two distinct models represent the same family of distributions, and empirically demonstrate the power of our method through thorough simulations.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3261",
        "title": "Learning When to Take Advice: A Statistical Test for Achieving A Correlated Equilibrium",
        "authors": [
            "Greg Hines",
            "Kate Larson"
        ],
        "abstract": "We study a multiagent learning problem where agents can either learn via repeated interactions, or can follow the advice of a mediator who suggests possible actions to take. We present an algorithmthat each agent can use so that, with high probability, they can verify whether or not the mediator's advice is useful. In particular, if the mediator's advice is useful then agents will reach a correlated equilibrium, but if the mediator's advice is not useful, then agents are not harmed by using our test, and can fall back to their original learning algorithm. We then generalize our algorithm and show that in the limit it always correctly verifies the mediator's advice.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3267",
        "title": "The Evaluation of Causal Effects in Studies with an Unobserved Exposure/Outcome Variable: Bounds and Identification",
        "authors": [
            "Manabu Kuroki",
            "Zhihong Cai"
        ],
        "abstract": "This paper deals with the problem of evaluating the causal effect using observational data in the presence of an unobserved exposure/ outcome variable, when cause-effect relationships between variables can be described as a directed acyclic graph and the corresponding recursive factorization of a joint distribution. First, we propose identifiability criteria for causal effects when an unobserved exposure/outcome variable is considered to contain more than two categories. Next, when unmeasured variables exist between an unobserved outcome variable and its proxy variables, we provide the tightest bounds based on the potential outcome approach. The results of this paper are helpful to evaluate causal effects in the case where it is difficult or expensive to observe an exposure/ outcome variable in many practical fields.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3288",
        "title": "Tightening LP Relaxations for MAP using Message Passing",
        "authors": [
            "David Sontag",
            "Talya Meltzer",
            "Amir Globerson",
            "Tommi S. Jaakkola",
            "Yair Weiss"
        ],
        "abstract": "Linear Programming (LP) relaxations have become powerful tools for finding the most probable (MAP) configuration in graphical models. These relaxations can be solved efficiently using message-passing algorithms such as belief propagation and, when the relaxation is tight, provably find the MAP configuration. The standard LP relaxation is not tight enough in many real-world problems, however, and this has lead to the use of higher order cluster-based LP relaxations. The computational cost increases exponentially with the size of the clusters and limits the number and type of clusters we can use. We propose to solve the cluster selection problem monotonically in the dual LP, iteratively selecting clusters with guaranteed improvement, and quickly re-solving with the added clusters by reusing the existing solution. Our dual message-passing algorithm finds the MAP configuration in protein sidechain placement, protein design, and stereo problems, in cases where the standard LP relaxation fails.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3437",
        "title": "Improving the Asymmetric TSP by Considering Graph Structure",
        "authors": [
            "Jean-Guillaume Fages",
            "Xavier Lorca"
        ],
        "abstract": "Recent works on cost based relaxations have improved Constraint Programming (CP) models for the Traveling Salesman Problem (TSP). We provide a short survey over solving asymmetric TSP with CP. Then, we suggest new implied propagators based on general graph properties. We experimentally show that such implied propagators bring robustness to pathological instances and highlight the fact that graph structure can significantly improve search heuristics behavior. Finally, we show that our approach outperforms current state of the art results.\n    ",
        "submission_date": "2012-06-15T00:00:00",
        "last_modified_date": "2012-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3633",
        "title": "Feature Based Fuzzy Rule Base Design for Image Extraction",
        "authors": [
            "Koushik Mondal",
            "Paramartha Dutta",
            "Siddhartha Bhattacharyya"
        ],
        "abstract": "In the recent advancement of multimedia technologies, it becomes a major concern of detecting visual attention regions in the field of image processing. The popularity of the terminal devices in a heterogeneous environment of the multimedia technology gives us enough scope for the betterment of image visualization. Although there exist numerous methods, feature based image extraction becomes a popular one in the field of image processing. The objective of image segmentation is the domain-independent partition of the image into a set of regions, which are visually distinct and uniform with respect to some property, such as grey level, texture or colour. Segmentation and subsequent extraction can be considered the first step and key issue in object recognition, scene understanding and image analysis. Its application area encompasses mobile devices, industrial quality control, medical appliances, robot navigation, geophysical exploration, military applications, etc. In all these areas, the quality of the final results depends largely on the quality of the preprocessing work. Most of the times, acquiring spurious-free preprocessing data requires a lot of application cum mathematical intensive background works. We propose a feature based fuzzy rule guided novel technique that is functionally devoid of any external intervention during execution. Experimental results suggest that this approach is an efficient one in comparison to different other techniques extensively addressed in literature. In order to justify the supremacy of performance of our proposed technique in respect of its competitors, we take recourse to effective metrics like Mean Squared Error (MSE), Mean Absolute Error (MAE) and Peak Signal to Noise Ratio (PSNR).\n    ",
        "submission_date": "2012-06-16T00:00:00",
        "last_modified_date": "2012-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3667",
        "title": "Information Retrieval in Intelligent Systems: Current Scenario & Issues",
        "authors": [
            "Sudhir Ahuja",
            "Mr. Rinkaj Goyal"
        ],
        "abstract": "Web space is the huge repository of data. Everyday lots of new information get added to this web space. The more the information, more is demand for tools to access that information. Answering users' queries about the online information intelligently is one of the great challenges in information retrieval in intelligent systems. In this paper, we will start with the brief introduction on information retrieval and intelligent systems and explain how swoogle, the semantic search engine, uses its algorithms and techniques to search for the desired contents in the web. We then continue with the clustering technique that is used to group the similar things together and discuss the machine learning technique called Self-organizing maps [6] or SOM, which is a data visualization technique that reduces the dimensions of data through the use of self-organizing neural networks. We then discuss how SOM is used to visualize the contents of the data, by following some lines of algorithm, in the form of maps. So, we could say that websites or machines can be used to retrieve the information that what exactly users want from them.\n    ",
        "submission_date": "2012-06-16T00:00:00",
        "last_modified_date": "2012-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3714",
        "title": "How important are Deformable Parts in the Deformable Parts Model?",
        "authors": [
            "Santosh K. Divvala",
            "Alexei A. Efros",
            "Martial Hebert"
        ],
        "abstract": "The main stated contribution of the Deformable Parts Model (DPM) detector of Felzenszwalb et al. (over the Histogram-of-Oriented-Gradients approach of Dalal and Triggs) is the use of deformable parts. A secondary contribution is the latent discriminative learning. Tertiary is the use of multiple components. A common belief in the vision community (including ours, before this study) is that their ordering of contributions reflects the performance of detector in practice. However, what we have experimentally found is that the ordering of importance might actually be the reverse. First, we show that by increasing the number of components, and switching the initialization step from their aspect-ratio, left-right flipping heuristics to appearance-based clustering, considerable improvement in performance is obtained. But more intriguingly, we show that with these new components, the part deformations can now be completely switched off, yet obtaining results that are almost on par with the original DPM detector. Finally, we also show initial results for using multiple components on a different problem -- scene classification, suggesting that this idea might have wider applications in addition to object detection.\n    ",
        "submission_date": "2012-06-16T00:00:00",
        "last_modified_date": "2012-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3902",
        "title": "On the Complexity of Existential Positive Queries",
        "authors": [
            "Hubie Chen"
        ],
        "abstract": "We systematically investigate the complexity of model checking the existential positive fragment of first-order logic. In particular, for a set of existential positive sentences, we consider model checking where the sentence is restricted to fall into the set; a natural question is then to classify which sentence sets are tractable and which are intractable. With respect to fixed-parameter tractability, we give a general theorem that reduces this classification question to the corresponding question for primitive positive logic, for a variety of representations of structures. This general theorem allows us to deduce that an existential positive sentence set having bounded arity is fixed-parameter tractable if and only if each sentence is equivalent to one in bounded-variable logic. We then use the lens of classical complexity to study these fixed-parameter tractable sentence sets. We show that such a set can be NP-complete, and consider the length needed by a translation from sentences in such a set to bounded-variable logic; we prove superpolynomial lower bounds on this length using the theory of compilability, obtaining an interesting type of formula size lower bound. Overall, the tools, concepts, and results of this article set the stage for the future consideration of the complexity of model checking on more expressive logics.\n    ",
        "submission_date": "2012-06-18T00:00:00",
        "last_modified_date": "2012-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.4116",
        "title": "Dependence Maximizing Temporal Alignment via Squared-Loss Mutual Information",
        "authors": [
            "Makoto Yamada",
            "Leonid Sigal",
            "Michalis Raptis",
            "Masashi Sugiyama"
        ],
        "abstract": "The goal of temporal alignment is to establish time correspondence between two sequences, which has many applications in a variety of areas such as speech processing, bioinformatics, computer vision, and computer graphics. In this paper, we propose a novel temporal alignment method called least-squares dynamic time warping (LSDTW). LSDTW finds an alignment that maximizes statistical dependency between sequences, measured by a squared-loss variant of mutual information. The benefit of this novel information-theoretic formulation is that LSDTW can align sequences with different lengths, different dimensionality, high non-linearity, and non-Gaussianity in a computationally efficient manner. In addition, model parameters such as an initial alignment matrix can be systematically optimized by cross-validation. We demonstrate the usefulness of LSDTW through experiments on synthetic and real-world Kinect action recognition datasets.\n    ",
        "submission_date": "2012-06-19T00:00:00",
        "last_modified_date": "2012-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.4185",
        "title": "Ant Robotics: Covering Continuous Domains by Multi-A(ge)nt Systems",
        "authors": [
            "Eliyahu Osherovich"
        ],
        "abstract": "In this work we present an algorithm for covering continuous connected domains by ant-like robots with very limited capabilities. The robots can mark visited places with pheromone marks and sense the level of the pheromone in their local neighborhood. In case of multiple robots these pheromone marks can be sensed by all robots and provide the only way of (indirect) communication between the robots. The robots are assumed to be memoryless, and to have no global information such as the domain map, their own position (either absolute or relative), total marked area percentage, maximal pheromone level, etc.. Despite the robots' simplicity, we show that they are able, by running a very simple rule of behavior, to ensure efficient covering of arbitrary connected domains, including non-planar and multidimensional ones. The novelty of our algorithm lies in the fact that, unlike previously proposed methods, our algorithm works on continuous domains without relying on some \"induced\" underlying graph, that effectively reduces the problem to a discrete case of graph covering. The algorithm guarantees complete coverage of any connected domain. We also prove that the algorithm is noise immune, i.e., it is able to cope with any initial pheromone profile (noise). In addition the algorithm provides a bounded constant time between two successive visits of the robot, and thus, is suitable for patrolling or surveillance applications.\n    ",
        "submission_date": "2012-06-19T00:00:00",
        "last_modified_date": "2012-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.4391",
        "title": "Gray Image extraction using Fuzzy Logic",
        "authors": [
            "Koushik Mondal",
            "Paramartha Dutta",
            "Siddhartha Bhattacharyya"
        ],
        "abstract": "Fuzzy systems concern fundamental methodology to represent and process uncertainty and imprecision in the linguistic information. The fuzzy systems that use fuzzy rules to represent the domain knowledge of the problem are known as Fuzzy Rule Base Systems (FRBS). On the other hand image segmentation and subsequent extraction from a noise-affected background, with the help of various soft computing methods, are relatively new and quite popular due to various reasons. These methods include various Artificial Neural Network (ANN) models (primarily supervised in nature), Genetic Algorithm (GA) based techniques, intensity histogram based methods etc. providing an extraction solution working in unsupervised mode happens to be even more interesting problem. Literature suggests that effort in this respect appears to be quite rudimentary. In the present article, we propose a fuzzy rule guided novel technique that is functional devoid of any external intervention during execution. Experimental results suggest that this approach is an efficient one in comparison to different other techniques extensively addressed in literature. In order to justify the supremacy of performance of our proposed technique in respect of its competitors, we take recourse to effective metrics like Mean Squared Error (MSE), Mean Absolute Error (MAE), Peak Signal to Noise Ratio (PSNR).\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.4603",
        "title": "Latent Collaborative Retrieval",
        "authors": [
            "Jason Weston",
            "Chong Wang",
            "Ron Weiss",
            "Adam Berenzweig"
        ],
        "abstract": "Retrieval tasks typically require a ranking of items given a query. Collaborative filtering tasks, on the other hand, learn to model user's preferences over items. In this paper we study the joint problem of recommending items to a user with respect to a given query, which is a surprisingly common task. This setup differs from the standard collaborative filtering one in that we are given a query x user x item tensor for training instead of the more traditional user x item matrix. Compared to document retrieval we do have a query, but we may or may not have content features (we will consider both cases) and we can also take account of the user's profile. We introduce a factorized model for this new task that optimizes the top-ranked items returned for the given query and user. We report empirical results where it outperforms several baselines.\n    ",
        "submission_date": "2012-06-18T00:00:00",
        "last_modified_date": "2012-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.4604",
        "title": "Learning the Experts for Online Sequence Prediction",
        "authors": [
            "Elad Eban",
            "Aharon Birnbaum",
            "Shai Shalev-Shwartz",
            "Amir Globerson"
        ],
        "abstract": "Online sequence prediction is the problem of predicting the next element of a sequence given previous elements. This problem has been extensively studied in the context of individual sequence prediction, where no prior assumptions are made on the origin of the sequence. Individual sequence prediction algorithms work quite well for long sequences, where the algorithm has enough time to learn the temporal structure of the sequence. However, they might give poor predictions for short sequences. A possible remedy is to rely on the general model of prediction with expert advice, where the learner has access to a set of $r$ experts, each of which makes its own predictions on the sequence. It is well known that it is possible to predict almost as well as the best expert if the sequence length is order of $\\log(r)$. But, without firm prior knowledge on the problem, it is not clear how to choose a small set of {\\em good} experts. In this paper we describe and analyze a new algorithm that learns a good set of experts using a training set of previously observed sequences. We demonstrate the merits of our approach by applying it on the task of click prediction on the web.\n    ",
        "submission_date": "2012-06-18T00:00:00",
        "last_modified_date": "2012-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.4606",
        "title": "TrueLabel + Confusions: A Spectrum of Probabilistic Models in Analyzing Multiple Ratings",
        "authors": [
            "Chao Liu",
            "Yi-Min Wang"
        ],
        "abstract": "This paper revisits the problem of analyzing multiple ratings given by different judges. Different from previous work that focuses on distilling the true labels from noisy crowdsourcing ratings, we emphasize gaining diagnostic insights into our in-house well-trained judges. We generalize the well-known DawidSkene model (Dawid & Skene, 1979) to a spectrum of probabilistic models under the same \"TrueLabel + Confusion\" paradigm, and show that our proposed hierarchical Bayesian model, called HybridConfusion, consistently outperforms DawidSkene on both synthetic and real-world data sets.\n    ",
        "submission_date": "2012-06-18T00:00:00",
        "last_modified_date": "2012-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.4617",
        "title": "Continuous Inverse Optimal Control with Locally Optimal Examples",
        "authors": [
            "Sergey Levine",
            "Vladlen Koltun"
        ],
        "abstract": "Inverse optimal control, also known as inverse reinforcement learning, is the problem of recovering an unknown reward function in a Markov decision process from expert demonstrations of the optimal policy. We introduce a probabilistic inverse optimal control algorithm that scales gracefully with task dimensionality, and is suitable for large, continuous domains where even computing a full policy is impractical. By using a local approximation of the reward function, our method can also drop the assumption that the demonstrations are globally optimal, requiring only local optimality. This allows it to learn from examples that are unsuitable for prior methods.\n    ",
        "submission_date": "2012-06-18T00:00:00",
        "last_modified_date": "2012-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.4636",
        "title": "Modeling Latent Variable Uncertainty for Loss-based Learning",
        "authors": [
            "M. Pawan Kumar",
            "Ben Packer",
            "Daphne Koller"
        ],
        "abstract": "We consider the problem of parameter estimation using weakly supervised datasets, where a training sample consists of the input and a partially specified annotation, which we refer to as the output. The missing information in the annotation is modeled using latent variables. Previous methods overburden a single distribution with two separate tasks: (i) modeling the uncertainty in the latent variables during training; and (ii) making accurate predictions for the output and the latent variables during testing. We propose a novel framework that separates the demands of the two tasks using two distributions: (i) a conditional distribution to model the uncertainty of the latent variables for a given input-output pair; and (ii) a delta distribution to predict the output and the latent variables for a given input. During learning, we encourage agreement between the two distributions by minimizing a loss-based dissimilarity coefficient. Our approach generalizes latent SVM in two important ways: (i) it models the uncertainty over latent variables instead of relying on a pointwise estimate; and (ii) it allows the use of loss functions that depend on latent variables, which greatly increases its applicability. We demonstrate the efficacy of our approach on two challenging problems---object detection and action detection---using publicly available datasets.\n    ",
        "submission_date": "2012-06-18T00:00:00",
        "last_modified_date": "2012-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.4639",
        "title": "Adaptive Regularization for Weight Matrices",
        "authors": [
            "Koby Crammer",
            "Gal Chechik"
        ],
        "abstract": "Algorithms for learning distributions over weight-vectors, such as AROW were recently shown empirically to achieve state-of-the-art performance at various problems, with strong theoretical guaranties. Extending these algorithms to matrix models pose challenges since the number of free parameters in the covariance of the distribution scales as $n^4$ with the dimension $n$ of the matrix, and $n$ tends to be large in real applications. We describe, analyze and experiment with two new algorithms for learning distribution of matrix models. Our first algorithm maintains a diagonal covariance over the parameters and can handle large covariance matrices. The second algorithm factors the covariance to capture inter-features correlation while keeping the number of parameters linear in the size of the original matrix. We analyze both algorithms in the mistake bound model and show a superior precision performance of our approach over other algorithms in two tasks: retrieving similar images, and ranking similar documents. The factored algorithm is shown to attain faster convergence rate.\n    ",
        "submission_date": "2012-06-18T00:00:00",
        "last_modified_date": "2012-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.4647",
        "title": "Active Learning for Matching Problems",
        "authors": [
            "Laurent Charlin",
            "Rich Zemel",
            "Craig Boutilier"
        ],
        "abstract": "Effective learning of user preferences is critical to easing user burden in various types of matching problems. Equally important is active query selection to further reduce the amount of preference information users must provide. We address the problem of active learning of user preferences for matching problems, introducing a novel method for determining probabilistic matchings, and developing several new active learning strategies that are sensitive to the specific matching objective. Experiments with real-world data sets spanning diverse domains demonstrate that matching-sensitive active learning\n    ",
        "submission_date": "2012-06-18T00:00:00",
        "last_modified_date": "2012-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.4652",
        "title": "The Most Persistent Soft-Clique in a Set of Sampled Graphs",
        "authors": [
            "Novi Quadrianto",
            "Chao Chen",
            "Christoph Lampert"
        ],
        "abstract": "When searching for characteristic subpatterns in potentially noisy graph data, it appears self-evident that having multiple observations would be better than having just one. However, it turns out that the inconsistencies introduced when different graph instances have different edge sets pose a serious challenge. In this work we address this challenge for the problem of finding maximum weighted cliques.\n",
        "submission_date": "2012-06-18T00:00:00",
        "last_modified_date": "2012-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.4656",
        "title": "Machine Learning that Matters",
        "authors": [
            "Kiri Wagstaff"
        ],
        "abstract": "Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field?s energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.\n    ",
        "submission_date": "2012-06-18T00:00:00",
        "last_modified_date": "2012-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.4667",
        "title": "Unachievable Region in Precision-Recall Space and Its Effect on Empirical Evaluation",
        "authors": [
            "Kendrick Boyd",
            "Vitor Santos Costa",
            "Jesse Davis",
            "David Page"
        ],
        "abstract": "Precision-recall (PR) curves and the areas under them are widely used to summarize machine learning results, especially for data sets exhibiting class skew. They are often used analogously to ROC curves and the area under ROC curves. It is known that PR curves vary as class skew changes. What was not recognized before this paper is that there is a region of PR space that is completely unachievable, and the size of this region depends only on the skew. This paper precisely characterizes the size of that region and discusses its implications for empirical evaluation methodology in machine learning.\n    ",
        "submission_date": "2012-06-18T00:00:00",
        "last_modified_date": "2012-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5157",
        "title": "Leaf vein segmentation using Odd Gabor filters and morphological operations",
        "authors": [
            "Vini Katyal",
            "Aviral"
        ],
        "abstract": "Leaf vein forms the basis of leaf characterization and classification. Different species have different leaf vein patterns. It is seen that leaf vein segmentation will help in maintaining a record of all the leaves according to their specific pattern of veins thus provide an effective way to retrieve and store information regarding various plant species in database as well as provide an effective means to characterize plants on the basis of leaf vein structure which is unique for every species. The algorithm proposes a new way of segmentation of leaf veins with the use of Odd Gabor filters and the use of morphological operations for producing a better output. The Odd Gabor filter gives an efficient output and is robust and scalable as compared with the existing techniques as it detects the fine fiber like veins present in leaves much more efficiently.\n    ",
        "submission_date": "2012-06-22T00:00:00",
        "last_modified_date": "2012-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5239",
        "title": "Large-Flip Importance Sampling",
        "authors": [
            "Firas Hamze",
            "Nando de Freitas"
        ],
        "abstract": "We propose a new Monte Carlo algorithm for complex discrete distributions. The algorithm is motivated by the N-Fold Way, which is an ingenious event-driven MCMC sampler that avoids rejection moves at any specific state. The N-Fold Way can however get \"trapped\" in cycles. We surmount this problem by modifying the sampling process. This correction does introduce bias, but the bias is subsequently corrected with a carefully engineered importance sampler.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5246",
        "title": "Causal Reasoning in Graphical Time Series Models",
        "authors": [
            "Michael Eichler",
            "Vanessa Didelez"
        ],
        "abstract": "We propose a definition of causality for time series in terms of the effect of an intervention in one component of a multivariate time series on another component at some later point in time. Conditions for identifiability, comparable to the back-door and front-door criteria, are presented and can also be verified graphically. Computation of the causal effect is derived and illustrated for the linear case.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5253",
        "title": "Reachability Under Uncertainty",
        "authors": [
            "Allen Chang",
            "Eyal Amir"
        ],
        "abstract": "In this paper we introduce a new network reachability problem where the goal is to find the most reliable path between two nodes in a network, represented as a directed acyclic graph. Individual edges within this network may fail according to certain probabilities, and these failure probabilities may depend on the values of one or more hidden variables. This problem may be viewed as a generalization of shortest-path problems for finding minimum cost paths or Viterbi-type problems for finding highest-probability sequences of states, where the addition of the hidden variables introduces correlations that are not handled by previous algorithms. We give theoretical results characterizing this problem including an NP-hardness proof. We also give an exact algorithm and a more efficient approximation algorithm for this problem.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5261",
        "title": "Mixture-of-Parents Maximum Entropy Markov Models",
        "authors": [
            "David S. Rosenberg",
            "Dan Klein",
            "Ben Taskar"
        ],
        "abstract": "We present the mixture-of-parents maximum entropy Markov model (MoP-MEMM), a class of directed graphical models extending MEMMs. The MoP-MEMM allows tractable incorporation of long-range dependencies between nodes by restricting the conditional distribution of each node to be a mixture of distributions given the parents. We show how to efficiently compute the exact marginal posterior node distributions, regardless of the range of the dependencies. This enables us to model non-sequential correlations present within text documents, as well as between interconnected documents, such as hyperlinked web pages. We apply the MoP-MEMM to a named entity recognition task and a web page classification task. In each, our model shows significant improvement over the basic MEMM, and is competitive with other long-range sequence models that use approximate inference.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5265",
        "title": "Consensus ranking under the exponential model",
        "authors": [
            "Marina Meila",
            "Kapil Phadnis",
            "Arthur Patterson",
            "Jeff A. Bilmes"
        ],
        "abstract": "We analyze the generalized Mallows model, a popular exponential model over rankings. Estimating the central (or consensus) ranking from data is NP-hard. We obtain the following new results: (1) We show that search methods can estimate both the central ranking pi0 and the model parameters theta exactly. The search is n! in the worst case, but is tractable when the true distribution is concentrated around its mode; (2) We show that the generalized Mallows model is jointly exponential in (pi0; theta), and introduce the conjugate prior for this model class; (3) The sufficient statistics are the pairwise marginal probabilities that item i is preferred to item j. Preliminary experiments confirm the theoretical predictions and compare the new algorithm and existing heuristics.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5272",
        "title": "Evaluation of the Causal Effect of Control Plans in Nonrecursive Structural Equation Models",
        "authors": [
            "Manabu Kuroki",
            "Zhihong Cai"
        ],
        "abstract": "When observational data is available from practical studies and a directed cyclic graph for how various variables affect each other is known based on substantive understanding of the process, we consider a problem in which a control plan of a treatment variable is conducted in order to bring a response variable close to a target value with variation reduction. We formulate an optimal control plan concerning a certain treatment variable through path coefficients in the framework of linear nonrecursive structural equation models. Based on the formulation, we clarify the properties of causal effects when conducting a control plan. The results enable us to evaluate the effect of a control plan on the variance from observational data.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5279",
        "title": "Making life better one large system at a time: Challenges for UAI research",
        "authors": [
            "Moises Goldszmidt"
        ],
        "abstract": "The rapid growth and diversity in service offerings and the ensuing complexity of information technology ecosystems present numerous management challenges (both operational and strategic). Instrumentation and measurement technology is, by and large, keeping pace with this development and growth. However, the algorithms, tools, and technology required to transform the data into relevant information for decision making are not. The claim in this paper (and the invited talk) is that the line of research conducted in Uncertainty in Artificial Intelligence is very well suited to address the challenges and close this gap. I will support this claim and discuss open problems using recent examples in diagnosis, model discovery, and policy optimization on three real life distributed systems.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5285",
        "title": "Importance Sampling via Variational Optimization",
        "authors": [
            "Ydo Wexler",
            "Dan Geiger"
        ],
        "abstract": "Computing the exact likelihood of data in large Bayesian networks consisting of thousands of vertices is often a difficult task. When these models contain many deterministic conditional probability tables and when the observed values are extremely unlikely even alternative algorithms such as variational methods and stochastic sampling often perform poorly. We present a new importance sampling algorithm for Bayesian networks which is based on variational techniques. We use the updates of the importance function to predict whether the stochastic sampling converged above or below the true likelihood, and change the proposal distribution accordingly. The validity of the method and its contribution to convergence is demonstrated on hard networks of large genetic linkage analysis tasks.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5288",
        "title": "Constrained Automated Mechanism Design for Infinite Games of Incomplete Information",
        "authors": [
            "Yevgeniy Vorobeychik",
            "Daniel Reeves",
            "Michael P. Wellman"
        ],
        "abstract": "We present a functional framework for automated mechanism design based on a two-stage game model of strategic interaction between the designer and the mechanism participants, and apply it to several classes of two-player infinite games of incomplete information. At the core of our framework is a black-box optimization algorithm which guides the selection process of candidate mechanisms. Our approach yields optimal or nearly optimal mechanisms in several application domains using various objective functions. By comparing our results with known optimal mechanisms, and in some cases improving on the best known mechanisms, we provide evidence that ours is a promising approach to parametric design of indirect mechanisms.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5289",
        "title": "A Criterion for Parameter Identification in Structural Equation Models",
        "authors": [
            "Jin Tian"
        ],
        "abstract": "This paper deals with the problem of identifying direct causal effects in recursive linear structural equation models. The paper establishes a sufficient criterion for identifying individual causal effects and provides a procedure computing identified causal effects in terms of observed covariance matrix.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5290",
        "title": "Imitation Learning with a Value-Based Prior",
        "authors": [
            "Umar Syed",
            "Robert E. Schapire"
        ],
        "abstract": "The goal of imitation learning is for an apprentice to learn how to behave in a stochastic environment by observing a mentor demonstrating the correct behavior. Accurate prior knowledge about the correct behavior can reduce the need for demonstrations from the mentor. We present a novel approach to encoding prior knowledge about the correct behavior, where we assume that this prior knowledge takes the form of a Markov Decision Process (MDP) that is used by the apprentice as a rough and imperfect model of the mentor's behavior. Specifically, taking a Bayesian approach, we treat the value of a policy in this modeling MDP as the log prior probability of the policy. In other words, we assume a priori that the mentor's behavior is likely to be a high value policy in the modeling MDP, though quite possibly different from the optimal policy. We describe an efficient algorithm that, given a modeling MDP and a set of demonstrations by a mentor, provably converges to a stationary point of the log posterior of the mentor's policy, where the posterior is computed with respect to the \"value based\" prior. We also present empirical evidence that this prior does in fact speed learning of the mentor's policy, and is an improvement in our experiments over similar previous methods.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5291",
        "title": "Improved Dynamic Schedules for Belief Propagation",
        "authors": [
            "Charles Sutton",
            "Andrew McCallum"
        ],
        "abstract": "Belief propagation and its variants are popular methods for approximate inference, but their running time and even their convergence depend greatly on the schedule used to send the messages. Recently, dynamic update schedules have been shown to converge much faster on hard networks than static schedules, namely the residual BP schedule of Elidan et al. [2006]. But that RBP algorithm wastes message updates: many messages are computed solely to determine their priority, and are never actually performed. In this paper, we show that estimating the residual, rather than calculating it directly, leads to significant decreases in the number of messages required for convergence, and in the total running time. The residual is estimated using an upper bound based on recent work on message errors in BP. On both synthetic and real-world networks, this dramatically decreases the running time of BP, in some cases by a factor of five, without affecting the quality of the solution.\n    ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5384",
        "title": "Keyphrase Based Arabic Summarizer (KPAS)",
        "authors": [
            "Tarek El-Shishtawy",
            "Fatma El-Ghannam"
        ],
        "abstract": "This paper describes a computationally inexpensive and efficient generic summarization algorithm for Arabic texts. The algorithm belongs to extractive summarization family, which reduces the problem into representative sentences identification and extraction sub-problems. Important keyphrases of the document to be summarized are identified employing combinations of statistical and linguistic features. The sentence extraction algorithm exploits keyphrases as the primary attributes to rank a sentence. The present experimental work, demonstrates different techniques for achieving various summarization goals including: informative richness, coverage of both main and auxiliary topics, and keeping redundancy to a minimum. A scoring scheme is then adopted that balances between these summarization goals. To evaluate the resulted Arabic summaries with well-established systems, aligned English/Arabic texts are used through the experiments.\n    ",
        "submission_date": "2012-06-23T00:00:00",
        "last_modified_date": "2012-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5754",
        "title": "Bayesian Modeling with Gaussian Processes using the GPstuff Toolbox",
        "authors": [
            "Jarno Vanhatalo",
            "Jaakko Riihim\u00e4ki",
            "Jouni Hartikainen",
            "Pasi Jyl\u00e4nki",
            "Ville Tolvanen",
            "Aki Vehtari"
        ],
        "abstract": "Gaussian processes (GP) are powerful tools for probabilistic modeling purposes. They can be used to define prior distributions over latent functions in hierarchical Bayesian models. The prior over functions is defined implicitly by the mean and covariance function, which determine the smoothness and variability of the function. The inference can then be conducted directly in the function space by evaluating or approximating the posterior process. Despite their attractive theoretical properties GPs provide practical challenges in their implementation. GPstuff is a versatile collection of computational tools for GP models compatible with Linux and Windows MATLAB and Octave. It includes, among others, various inference methods, sparse approximations and tools for model assessment. In this work, we review these tools and demonstrate the use of GPstuff in several models.\n    ",
        "submission_date": "2012-06-25T00:00:00",
        "last_modified_date": "2015-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6230",
        "title": "Decentralized Data Fusion and Active Sensing with Mobile Sensors for Modeling and Predicting Spatiotemporal Traffic Phenomena",
        "authors": [
            "Jie Chen",
            "Kian Hsiang Low",
            "Colin Keng-Yan Tan",
            "Ali Oran",
            "Patrick Jaillet",
            "John M. Dolan",
            "Gaurav S. Sukhatme"
        ],
        "abstract": "The problem of modeling and predicting spatiotemporal traffic phenomena over an urban road network is important to many traffic applications such as detecting and forecasting congestion hotspots. This paper presents a decentralized data fusion and active sensing (D2FAS) algorithm for mobile sensors to actively explore the road network to gather and assimilate the most informative data for predicting the traffic phenomenon. We analyze the time and communication complexity of D2FAS and demonstrate that it can scale well with a large number of observations and sensors. We provide a theoretical guarantee on its predictive performance to be equivalent to that of a sophisticated centralized sparse approximation for the Gaussian process (GP) model: The computation of such a sparse approximate GP model can thus be parallelized and distributed among the mobile sensors (in a Google-like MapReduce paradigm), thereby achieving efficient and scalable prediction. We also theoretically guarantee its active sensing performance that improves under various practical environmental conditions. Empirical evaluation on real-world urban road network data shows that our D2FAS algorithm is significantly more time-efficient and scalable than state-of-the-art centralized algorithms while achieving comparable predictive performance.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6386",
        "title": "How To Grade a Test Without Knowing the Answers --- A Bayesian Graphical Model for Adaptive Crowdsourcing and Aptitude Testing",
        "authors": [
            "Yoram Bachrach",
            "Thore Graepel",
            "Tom Minka",
            "John Guiver"
        ],
        "abstract": "We propose a new probabilistic graphical model that jointly models the difficulties of questions, the abilities of participants and the correct answers to questions in aptitude testing and crowdsourcing settings. We devise an active learning/adaptive testing scheme based on a greedy minimization of expected model entropy, which allows a more efficient resource allocation by dynamically choosing the next question to be asked based on the previous responses. We present experimental results that confirm the ability of our model to infer the required parameters and demonstrate that the adaptive testing scheme requires fewer questions to obtain the same accuracy as a static test scenario.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6399",
        "title": "Demand-Driven Clustering in Relational Domains for Predicting Adverse Drug Events",
        "authors": [
            "Jesse Davis",
            "Vitor Santos Costa",
            "Peggy Peissig",
            "Michael Caldwell",
            "Elizabeth Berg",
            "David Page"
        ],
        "abstract": "Learning from electronic medical records (EMR) is challenging due to their relational nature and the uncertain dependence between a patient's past and future health status. Statistical relational learning is a natural fit for analyzing EMRs but is less adept at handling their inherent latent structure, such as connections between related medications or diseases. One way to capture the latent structure is via a relational clustering of objects. We propose a novel approach that, instead of pre-clustering the objects, performs a demand-driven clustering during learning. We evaluate our algorithm on three real-world tasks where the goal is to use EMRs to predict whether a patient will have an adverse reaction to a medication. We find that our approach is more accurate than performing no clustering, pre-clustering, and using expert-constructed medical heterarchies.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6405",
        "title": "Bounded Planning in Passive POMDPs",
        "authors": [
            "Roy Fox",
            "Naftali Tishby"
        ],
        "abstract": "In Passive POMDPs actions do not affect the world state, but still incur costs. When the agent is bounded by information-processing constraints, it can only keep an approximation of the belief. We present a variational principle for the problem of maintaining the information which is most useful for minimizing the cost, and introduce an efficient and simple algorithm for finding an optimum.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6460",
        "title": "Output Space Search for Structured Prediction",
        "authors": [
            "Janardhan Rao Doppa",
            "Alan Fern",
            "Prasad Tadepalli"
        ],
        "abstract": "We consider a framework for structured prediction based on search in the space of complete structured outputs. Given a structured input, an output is produced by running a time-bounded search procedure guided by a learned cost function, and then returning the least cost output uncovered during the search. This framework can be instantiated for a wide range of search spaces and search procedures, and easily incorporates arbitrary structured-prediction loss functions. In this paper, we make two main technical contributions. First, we define the limited-discrepancy search space over structured outputs, which is able to leverage powerful classification learning algorithms to improve the search space quality. Second, we give a generic cost function learning approach, where the key idea is to learn a cost function that attempts to mimic the behavior of conducting searches guided by the true loss function. Our experiments on six benchmark domains demonstrate that using our framework with only a small amount of search is sufficient for significantly improving on state-of-the-art structured-prediction performance.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6476",
        "title": "Similarity Learning for Provably Accurate Sparse Linear Classification",
        "authors": [
            "Aurelien Bellet",
            "Amaury Habrard",
            "Marc Sebban"
        ],
        "abstract": "In recent years, the crucial importance of metrics in machine learning algorithms has led to an increasing interest for optimizing distance and similarity functions. Most of the state of the art focus on learning Mahalanobis distances (requiring to fulfill a constraint of positive semi-definiteness) for use in a local k-NN algorithm. However, no theoretical link is established between the learned metrics and their performance in classification. In this paper, we make use of the formal framework of good similarities introduced by Balcan et al. to design an algorithm for learning a non PSD linear similarity optimized in a nonlinear feature space, which is then used to build a global linear classifier. We show that our approach has uniform stability and derive a generalization bound on the classification error. Experiments performed on various datasets confirm the effectiveness of our approach compared to state-of-the-art methods and provide evidence that (i) it is fast, (ii) robust to overfitting and (iii) produces very sparse classifiers.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6484",
        "title": "Apprenticeship Learning for Model Parameters of Partially Observable Environments",
        "authors": [
            "Takaki Makino",
            "Johane Takeuchi"
        ],
        "abstract": "We consider apprenticeship learning, i.e., having an agent learn a task by observing an expert demonstrating the task in a partially observable environment when the model of the environment is uncertain. This setting is useful in applications where the explicit modeling of the environment is difficult, such as a dialogue system. We show that we can extract information about the environment model by inferring action selection process behind the demonstration, under the assumption that the expert is choosing optimal actions based on knowledge of the true model of the target environment. Proposed algorithms can achieve more accurate estimates of POMDP parameters and better policies from a short demonstration, compared to methods that learns only from the reaction from the environment.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6570",
        "title": "Extension of Three-Variable Counterfactual Casual Graphic Model: from Two-Value to Three-Value Random Variable",
        "authors": [
            "Jingwei Liu"
        ],
        "abstract": "The extension of counterfactual causal graphic model with three variables of vertex set in directed acyclic graph (DAG) is discussed in this paper by extending two- value distribution to three-value distribution of the variables involved in DAG. Using the conditional independence as ancillary information, 6 kinds of extension counterfactual causal graphic models with some variables are extended from two-value distribution to three-value distribution and the sufficient conditions of identifiability are derived.\n    ",
        "submission_date": "2012-06-28T00:00:00",
        "last_modified_date": "2012-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6735",
        "title": "Elimination of Spurious Ambiguity in Transition-Based Dependency Parsing",
        "authors": [
            "Shay B. Cohen",
            "Carlos G\u00f3mez-Rodr\u00edguez",
            "Giorgio Satta"
        ],
        "abstract": "We present a novel technique to remove spurious ambiguity from transition systems for dependency parsing. Our technique chooses a canonical sequence of transition operations (computation) for a given dependency tree. Our technique can be applied to a large class of bottom-up transition systems, including for instance Nivre (2004) and Attardi (2006).\n    ",
        "submission_date": "2012-06-28T00:00:00",
        "last_modified_date": "2012-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6820",
        "title": "Optimal Coordinated Planning Amongst Self-Interested Agents with Private State",
        "authors": [
            "Ruggiero Cavallo",
            "David C. Parkes",
            "Satinder Singh"
        ],
        "abstract": "Consider a multi-agent system in a dynamic and uncertain environment. Each agent's local decision problem is modeled as a Markov decision process (MDP) and agents must coordinate on a joint action in each period, which provides a reward to each agent and causes local state transitions. A social planner knows the model of every agent's MDP and wants to implement the optimal joint policy, but agents are self-interested and have private local state. We provide an incentive-compatible mechanism for eliciting state information that achieves the optimal joint plan in a Markov perfect equilibrium of the induced stochastic game. In the special case in which local problems are Markov chains and agents compete to take a single action in each period, we leverage Gittins allocation indices to provide an efficient factored algorithm and distribute computation of the optimal policy among the agents. Distributed, optimal coordinated learning in a multi-agent variant of the multi-armed bandit problem is obtained as a special case.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6828",
        "title": "Advances in exact Bayesian structure discovery in Bayesian networks",
        "authors": [
            "Mikko Koivisto"
        ],
        "abstract": "We consider a Bayesian method for learning the Bayesian network structure from complete data. Recently, Koivisto and Sood (2004) presented an algorithm that for any single edge computes its marginal posterior probability in O(n 2^n) time, where n is the number of attributes; the number of parents per attribute is bounded by a constant. In this paper we show that the posterior probabilities for all the n (n - 1) potential edges can be computed in O(n 2^n) total time. This result is achieved by a forward-backward technique and fast Moebius transform algorithms, which are of independent interest. The resulting speedup by a factor of about n^2 allows us to experimentally study the statistical power of learning moderate-size networks. We report results from a simulation study that covers data sets with 20 to 10,000 records over 5 to 25 discrete attributes\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6830",
        "title": "The AI&M Procedure for Learning from Incomplete Data",
        "authors": [
            "Manfred Jaeger"
        ],
        "abstract": "We investigate methods for parameter learning from incomplete data that is not missing at random. Likelihood-based methods then require the optimization of a profile likelihood that takes all possible missingness mechanisms into account. Optimzing this profile likelihood poses two main difficulties: multiple (local) maxima, and its very high-dimensional parameter space. In this paper a new method is presented for optimizing the profile likelihood that addresses the second difficulty: in the proposed AI&M (adjusting imputation and mazimization) procedure the optimization is performed by operations in the space of data completions, rather than directly in the parameter space of the profile likelihood. We apply the AI&M method to learning parameters for Bayesian networks. The method is compared against conservative inference, which takes into account each possible data completion, and against EM. The results indicate that likelihood-based inference is still feasible in the case of unknown missingness mechanisms, and that conservative inference is unnecessarily weak. On the other hand, our results also provide evidence that the EM algorithm is still quite effective when the data is not missing at random.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6842",
        "title": "Chi-square Tests Driven Method for Learning the Structure of Factored MDPs",
        "authors": [
            "Thomas Degris",
            "Olivier Sigaud",
            "Pierre-Henri Wuillemin"
        ],
        "abstract": "SDYNA is a general framework designed to address large stochastic reinforcement learning problems. Unlike previous model based methods in FMDPs, it incrementally learns the structure and the parameters of a RL problem using supervised learning techniques. Then, it integrates decision-theoric planning algorithms based on FMDPs to compute its policy. SPITI is an instanciation of SDYNA that exploits ITI, an incremental decision tree algorithm, to learn the reward function and the Dynamic Bayesian Networks with local structures representing the transition function of the problem. These representations are used by an incremental version of the Structured Value Iteration algorithm. In order to learn the structure, SPITI uses Chi-Square tests to detect the independence between two probability distributions. Thus, we study the relation between the threshold used in the Chi-Square test, the size of the model built and the relative error of the value function of the induced policy with respect to the optimal value. We show that, on stochastic problems, one can tune the threshold so as to generate both a compact model and an efficient policy. Then, we show that SPITI, while keeping its model compact, uses the generalization property of its learning method to perform better than a stochastic classical tabular algorithm in large RL problem with an unknown structure. We also introduce a new measure based on Chi-Square to qualify the accuracy of the model learned by SPITI. We qualitatively show that the generalization property in SPITI within the FMDP framework may prevent an exponential growth of the time required to learn the structure of large stochastic RL problems.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6846",
        "title": "Approximate Separability for Weak Interaction in Dynamic Systems",
        "authors": [
            "Avi Pfeffer"
        ],
        "abstract": "One approach to monitoring a dynamic system relies on decomposition of the system into weakly interacting subsystems. An earlier paper introduced a notion of weak interaction called separability, and showed that it leads to exact propagation of marginals for prediction. This paper addresses two questions left open by the earlier paper: can we define a notion of approximate separability that occurs naturally in practice, and do separability and approximate separability lead to accurate monitoring? The answer to both questions is afirmative. The paper also analyzes the structure of approximately separable decompositions, and provides some explanation as to why these models perform well.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6847",
        "title": "Identifying the Relevant Nodes Without Learning the Model",
        "authors": [
            "Jose M. Pena",
            "Roland Nilsson",
            "Johan Bj\u00f6rkegren",
            "Jesper Tegn\u00e9r"
        ],
        "abstract": "We propose a method to identify all the nodes that are relevant to compute all the conditional probability distributions for a given set of nodes. Our method is simple, effcient, consistent, and does not require learning a Bayesian network first. Therefore, our method can be applied to high-dimensional databases, e.g. gene expression databases.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6850",
        "title": "Visualization of Collaborative Data",
        "authors": [
            "Guobiao Mei",
            "Christian R. Shelton"
        ],
        "abstract": "Collaborative data consist of ratings relating two distinct sets of objects: users and items. Much of the work with such data focuses on filtering: predicting unknown ratings for pairs of users and items. In this paper we focus on the problem of visualizing the information. Given all of the ratings, our task is to embed all of the users and items as points in the same Euclidean space. We would like to place users near items that they have rated (or would rate) high, and far away from those they would give a low rating. We pose this problem as a real-valued non-linear Bayesian network and employ Markov chain Monte Carlo and expectation maximization to find an embedding. We present a metric by which to judge the quality of a visualization and compare our results to local linear embedding and Eigentaste on three real-world datasets.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6851",
        "title": "A compact, hierarchical Q-function decomposition",
        "authors": [
            "Bhaskara Marthi",
            "Stuart Russell",
            "David Andre"
        ],
        "abstract": "Previous work in hierarchical reinforcement learning has faced a dilemma: either ignore the values of different possible exit states from a subroutine, thereby risking suboptimal behavior, or represent those values explicitly thereby incurring a possibly large representation cost because exit values refer to nonlocal aspects of the world (i.e., all subsequent rewards). This paper shows that, in many cases, one can avoid both of these problems. The solution is based on recursively decomposing the exit value function in terms of Q-functions at higher levels of the hierarchy. This leads to an intuitively appealing runtime architecture in which a parent subroutine passes to its child a value function on the exit states and the child reasons about how its choices affect the exit value. We also identify structural conditions on the value function and transition distributions that allow much more concise representations of exit state distributions, leading to further state abstraction. In essence, the only variables whose exit values need be considered are those that the parent cares about and the child affects. We demonstrate the utility of our algorithms on a series of increasingly complex environments.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6852",
        "title": "Structured Priors for Structure Learning",
        "authors": [
            "Vikash Mansinghka",
            "Charles Kemp",
            "Thomas Griffiths",
            "Joshua Tenenbaum"
        ],
        "abstract": "Traditional approaches to Bayes net structure learning typically assume little regularity in graph structure other than sparseness. However, in many cases, we expect more systematicity: variables in real-world systems often group into classes that predict the kinds of probabilistic dependencies they participate in. Here we capture this form of prior knowledge in a hierarchical Bayesian framework, and exploit it to enable structure learning and type discovery from small datasets. Specifically, we present a nonparametric generative model for directed acyclic graphs as a prior for Bayes net structure learning. Our model assumes that variables come in one or more classes and that the prior probability of an edge existing between two variables is a function only of their classes. We derive an MCMC algorithm for simultaneous inference of the number of classes, the class assignments of variables, and the Bayes net structure over variables. For several realistic, sparse datasets, we show that the bias towards systematicity of connections provided by our model yields more accurate learned networks than a traditional, uniform prior approach, and that the classes found by our model are appropriate.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6861",
        "title": "Stratified Analysis of `Probabilities of Causation'",
        "authors": [
            "Manabu Kuroki",
            "Zhihong Cai"
        ],
        "abstract": "This paper proposes new formulas for the probabilities of causation difined by Pearl (2000). Tian and Pearl (2000a, 2000b) showed how to bound the quantities of the probabilities of causation from experimental and observational data, under the minimal assumptions about the data-generating process. We derive narrower bounds than Tian-Pearl bounds by making use of the covariate information measured in experimental and observational studies. In addition, we provide identifiable case under no-prevention assumption and discuss the covariate selection problem from the viewpoint of estimation accuracy. These results are helpful in providing more evidence for public policy assessment and dicision making problems.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6862",
        "title": "On the Number of Samples Needed to Learn the Correct Structure of a Bayesian Network",
        "authors": [
            "Or Zuk",
            "Shiri Margel",
            "Eytan Domany"
        ],
        "abstract": "Bayesian Networks (BNs) are useful tools giving a natural and compact representation of joint probability distributions. In many applications one needs to learn a Bayesian Network (BN) from data. In this context, it is important to understand the number of samples needed in order to guarantee a successful learning. Previous work have studied BNs sample complexity, yet it mainly focused on the requirement that the learned distribution will be close to the original distribution which generated the data. In this work, we study a different aspect of the learning, namely the number of samples needed in order to learn the correct structure of the network. We give both asymptotic results, valid in the large sample limit, and experimental results, demonstrating the learning behavior for feasible sample sizes. We show that structure learning is a more difficult task, compared to approximating the correct distribution, in the sense that it requires a much larger number of samples, regardless of the computational power available for the learner.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6865",
        "title": "A Non-Parametric Bayesian Method for Inferring Hidden Causes",
        "authors": [
            "Frank Wood",
            "Thomas Griffiths",
            "Zoubin Ghahramani"
        ],
        "abstract": "We present a non-parametric Bayesian approach to structure learning with hidden causes. Previous Bayesian treatments of this problem define a prior over the number of hidden causes and use algorithms such as reversible jump Markov chain Monte Carlo to move between solutions. In contrast, we assume that the number of hidden causes is unbounded, but only a finite number influence observable variables. This makes it possible to use a Gibbs sampler to approximate the distribution over causal structures. We evaluate the performance of both approaches in discovering hidden causes in simulated data, and use our non-parametric approach to discover hidden causes in a real medical dataset.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6870",
        "title": "Incremental Model-based Learners With Formal Learning-Time Guarantees",
        "authors": [
            "Alexander L. Strehl",
            "Lihong Li",
            "Michael L. Littman"
        ],
        "abstract": "Model-based learning algorithms have been shown to use experience efficiently when learning to solve Markov Decision Processes (MDPs) with finite state and action spaces. However, their high computational cost due to repeatedly solving an internal model inhibits their use in large-scale problems. We propose a method based on real-time dynamic programming (RTDP) to speed up two model-based algorithms, RMAX and MBIE (model-based interval estimation), resulting in computationally much faster algorithms with little loss compared to existing bounds. Specifically, our two new learning algorithms, RTDP-RMAX and RTDP-IE, have considerably smaller computational demands than RMAX and MBIE. We develop a general theoretical framework that allows us to prove that both are efficient learners in a PAC (probably approximately correct) sense. We also present an experimental evaluation of these new algorithms that helps quantify the tradeoff between computational and experience demands.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6874",
        "title": "Bayesian Inference for Gaussian Mixed Graph Models",
        "authors": [
            "Ricardo Silva",
            "Zoubin Ghahramani"
        ],
        "abstract": "We introduce priors and algorithms to perform Bayesian inference in Gaussian models defined by acyclic directed mixed graphs. Such a class of graphs, composed of directed and bi-directed edges, is a representation of conditional independencies that is closed under marginalization and arises naturally from causal models which allow for unmeasured confounding. Monte Carlo methods and a variational approximation for such models are presented. Our algorithms for Bayesian inference allow the evaluation of posterior distributions for several quantities of interest, including causal effects that are not identifiable from data alone but could otherwise be inferred where informative prior knowledge about confounding is available.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.7051",
        "title": "Stochastic Variational Inference",
        "authors": [
            "Matt Hoffman",
            "David M. Blei",
            "Chong Wang",
            "John Paisley"
        ],
        "abstract": "We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.\n    ",
        "submission_date": "2012-06-29T00:00:00",
        "last_modified_date": "2013-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.0869",
        "title": "Theory and Techniques for Synthesizing a Family of Graph Algorithms",
        "authors": [
            "Srinivas Nedunuri",
            "William R. Cook",
            "Douglas R. Smith"
        ],
        "abstract": "Although Breadth-First Search (BFS) has several advantages over Depth-First Search (DFS) its prohibitive space requirements have meant that algorithm designers often pass it over in favor of DFS. To address this shortcoming, we introduce a theory of Efficient BFS (EBFS) along with a simple recursive program schema for carrying out the search. The theory is based on dominance relations, a long standing technique from the field of search algorithms. We show how the theory can be used to systematically derive solutions to two graph algorithms, namely the Single Source Shortest Path problem and the Minimum Spanning Tree problem. The solutions are found by making small systematic changes to the derivation, revealing the connections between the two problems which are often obscured in textbook presentations of them.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1016",
        "title": "Map-aided Fusion Using Evidential Grids for Mobile Perception in Urban Environment",
        "authors": [
            "Marek Kurdej",
            "Julien Moras",
            "V\u00e9ronique Cherfaoui",
            "Philippe Bonnifait"
        ],
        "abstract": "Evidential grids have been recently used for mobile object perception. The novelty of this article is to propose a perception scheme using prior map knowledge. A geographic map is considered an additional source of information fused with a grid representing sensor data. Yager's rule is adapted to exploit the Dempster-Shafer conflict information at large. In order to distinguish stationary and mobile objects, a counter is introduced and used as a factor for mass function specialisation. Contextual discounting is used, since we assume that different pieces of information become obsolete at different rates. Tests on real-world data are also presented.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1257",
        "title": "Generalizing Redundancy in Propositional Logic: Foundations and Hitting Sets Duality",
        "authors": [
            "Anton Belov",
            "Joao Marques-Silva"
        ],
        "abstract": "Detection and elimination of redundant clauses from propositional formulas in Conjunctive Normal Form (CNF) is a fundamental problem with numerous application domains, including AI, and has been the subject of extensive research. Moreover, a number of recent applications motivated various extensions of this problem. For example, unsatisfiable formulas partitioned into disjoint subsets of clauses (so-called groups) often need to be simplified by removing redundant groups, or may contain redundant variables, rather than clauses. In this report we present a generalized theoretical framework of labelled CNF formulas that unifies various extensions of the redundancy detection and removal problem and allows to derive a number of results that subsume and extend previous work. The follow-up reports contain a number of additional theoretical results and algorithms for various computational problems in the context of the proposed framework.\n    ",
        "submission_date": "2012-07-05T00:00:00",
        "last_modified_date": "2012-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1361",
        "title": "Local Utility Elicitation in GAI Models",
        "authors": [
            "Darius Braziunas",
            "Craig Boutilier"
        ],
        "abstract": "Structured utility models are essential for the effective representation and elicitation of complex multiattribute utility functions. Generalized additive independence (GAI) models provide an attractive structural model of user preferences, offering a balanced tradeoff between simplicity and applicability. While representation and inference with such models is reasonably well understood, elicitation of the parameters of such models has been studied less from a practical perspective. We propose a procedure to elicit GAI model parameters using only \"local\" utility queries rather than \"global\" queries over full outcomes. Our local queries take full advantage of GAI structure and provide a sound framework for extending the elicitation procedure to settings where the uncertainty over utility parameters is represented probabilistically. We describe experiments using a myopic value-of-information approach to elicitation in a large GAI model.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1365",
        "title": "Towards Characterizing Markov Equivalence Classes for Directed Acyclic Graphs with Latent Variables",
        "authors": [
            "Ayesha R. Ali",
            "Thomas S. Richardson",
            "Peter L. Spirtes",
            "Jiji Zhang"
        ],
        "abstract": "It is well known that there may be many causal explanations that are consistent with a given set of data. Recent work has been done to represent the common aspects of these explanations into one representation. In this paper, we address what is less well known: how do the relationships common to every causal explanation among the observed variables of some DAG process change in the presence of latent variables? Ancestral graphs provide a class of graphs that can encode conditional independence relations that arise in DAG models with latent and selection variables. In this paper we present a set of orientation rules that construct the Markov equivalence class representative for ancestral graphs, given a member of the equivalence class. These rules are sound and complete. We also show that when the equivalence class includes a DAG, the equivalence class representative is the essential graph for the said DAG\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1368",
        "title": "Common Voting Rules as Maximum Likelihood Estimators",
        "authors": [
            "Vincent Conitzer",
            "Tuomas Sandholm"
        ],
        "abstract": "Voting is a very general method of preference aggregation. A voting rule takes as input every voter's vote (typically, a ranking of the alternatives), and produces as output either just the winning alternative or a ranking of the alternatives. One potential view of voting is the following. There exists a 'correct' outcome (winner/ranking), and each voter's vote corresponds to a noisy perception of this correct outcome. If we are given the noise model, then for any vector of votes, we can\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1392",
        "title": "The Graphical Identification for Total Effects by using Surrogate Variables",
        "authors": [
            "Manabu Kuroki",
            "Zhihong Cai",
            "Hiroki Motogaito"
        ],
        "abstract": "Consider the case where cause-effect relationships between variables can be described as a directed acyclic graph and the corresponding linear structural equation model. This paper provides graphical identifiability criteria for total effects by using surrogate variables in the case where it is difficult to observe a treatment/response variable. The results enable us to judge from graph structure whether a total effect can be identified through the observation of surrogate variables.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1399",
        "title": "Robotic Mapping with Polygonal Random Fields",
        "authors": [
            "Mark Paskin",
            "Sebastian Thrun"
        ],
        "abstract": "Two types of probabilistic maps are popular in the mobile robotics literature: occupancy grids and geometric maps. Occupancy grids have the advantages of simplicity and speed, but they represent only a restricted class of maps and they make incorrect independence assumptions. On the other hand, current geometric approaches, which characterize the environment by features such as line segments, can represent complex environments compactly. However, they do not reason explicitly about occupancy, a necessity for motion planning; and, they lack a complete probability model over environmental structures. In this paper we present a probabilistic mapping technique based on polygonal random fields (PRF), which combines the advantages of both approaches. Our approach explicitly represents occupancy using a geometric representation, and it is based upon a consistent probability distribution over environments which avoids the incorrect independence assumptions made by occupancy grids. We show how sampling techniques for PRFs can be applied to localized laser and sonar data, and we demonstrate significant improvements in mapping performance over occupancy grids.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1406",
        "title": "A Conditional Random Field for Discriminatively-trained Finite-state String Edit Distance",
        "authors": [
            "Andrew McCallum",
            "Kedar Bellare",
            "Fernando Pereira"
        ],
        "abstract": "The need to measure sequence similarity arises in information extraction, object identity, data mining, biological sequence analysis, and other domains. This paper presents discriminative string-edit CRFs, a finitestate conditional random field model for edit sequences between strings. Conditional random fields have advantages over generative approaches to this problem, such as pair HMMs or the work of Ristad and Yianilos, because as conditionally-trained methods, they enable the use of complex, arbitrary actions and features of the input strings. As in generative models, the training data does not have to specify the edit sequences between the given string pairs. Unlike generative models, however, our model is trained on both positive and negative instances of string pairs. We present positive experimental results on several data sets.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1411",
        "title": "Bayes' Bluff: Opponent Modelling in Poker",
        "authors": [
            "Finnegan Southey",
            "Michael P. Bowling",
            "Bryce Larson",
            "Carmelo Piccione",
            "Neil Burch",
            "Darse Billings",
            "Chris Rayner"
        ],
        "abstract": "Poker is a challenging problem for artificial intelligence, with non-deterministic dynamics, partial observability, and the added difficulty of unknown adversaries. Modelling all of the uncertainties in this domain is not an easy task. In this paper we present a Bayesian probabilistic model for a broad class of poker games, separating the uncertainty in the game dynamics from the uncertainty of the opponent's strategy. We then describe approaches to two key subproblems: (i) inferring a posterior over opponent strategies given a prior distribution and observations of their play, and (ii) playing an appropriate response to that distribution. We demonstrate the overall approach on a reduced version of poker using Dirichlet priors and then on the full game of Texas hold'em using a more informed prior. We demonstrate methods for playing effective responses to the opponent, based on the posterior.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1428",
        "title": "Generating Markov Equivalent Maximal Ancestral Graphs by Single Edge Replacement",
        "authors": [
            "Jin Tian"
        ],
        "abstract": "Maximal ancestral graphs (MAGs) are used to encode conditional independence relations in DAG models with hidden variables. Different MAGs may represent the same set of conditional independences and are called Markov equivalent. This paper considers MAGs without undirected edges and shows conditions under which an arrow in a MAG can be reversed or interchanged with a bi-directed edge so as to yield a Markov equivalent MAG.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1429",
        "title": "Ordering-Based Search: A Simple and Effective Algorithm for Learning Bayesian Networks",
        "authors": [
            "Marc Teyssier",
            "Daphne Koller"
        ],
        "abstract": "One of the basic tasks for Bayesian networks (BNs) is that of learning a network structure from data. The BN-learning problem is NP-hard, so the standard solution is heuristic search. Many approaches have been proposed for this task, but only a very small number outperform the baseline of greedy hill-climbing with tabu lists; moreover, many of the proposed algorithms are quite complex and hard to implement. In this paper, we propose a very simple and easy-to-implement method for addressing this task. Our approach is based on the well-known fact that the best network (of bounded in-degree) consistent with a given node ordering can be found very efficiently. We therefore propose a search not over the space of structures, but over the space of orderings, selecting for each ordering the best network consistent with it. This search space is much smaller, makes more global search steps, has a lower branching factor, and avoids costly acyclicity checks. We present results for this algorithm on both synthetic and real data sets, evaluating both the score of the network found and in the running time. We show that ordering-based search outperforms the standard baseline, and is competitive with recent algorithms that are much harder to implement.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1794",
        "title": "Design, Evaluation and Analysis of Combinatorial Optimization Heuristic Algorithms",
        "authors": [
            "Daniel Karapetyan"
        ],
        "abstract": "Combinatorial optimization is widely applied in a number of areas nowadays. Unfortunately, many combinatorial optimization problems are NP-hard which usually means that they are unsolvable in practice. However, it is often unnecessary to have an exact solution. In this case one may use heuristic approach to obtain a near-optimal solution in some reasonable time.\n",
        "submission_date": "2012-07-07T00:00:00",
        "last_modified_date": "2012-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.2104",
        "title": "Rule Based Expert System for Diagnosis of Neuromuscular Disorders",
        "authors": [
            "Rajdeep Borgohain",
            "Sugata Sanyal"
        ],
        "abstract": "In this paper, we discuss the implementation of a rule based expert system for diagnosing neuromuscular diseases. The proposed system is implemented as a rule based expert system in JESS for the diagnosis of Cerebral Palsy, Multiple Sclerosis, Muscular Dystrophy and Parkinson's disease. In the system, the user is presented with a list of questionnaires about the symptoms of the patients based on which the disease of the patient is diagnosed and possible treatment is suggested. The system can aid and support the patients suffering from neuromuscular diseases to get an idea of their disease and possible treatment for the disease.\n    ",
        "submission_date": "2012-07-09T00:00:00",
        "last_modified_date": "2012-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.2265",
        "title": "Challenges for Distributional Compositional Semantics",
        "authors": [
            "Daoud Clarke"
        ],
        "abstract": "This paper summarises the current state-of-the art in the study of compositionality in distributional semantics, and major challenges for this area. We single out generalised quantifiers and intensional semantics as areas on which to focus attention for the development of the theory. Once suitable theories have been developed, algorithms will be needed to apply the theory to tasks. Evaluation is a major problem; we single out application to recognising textual entailment and machine translation for this purpose.\n    ",
        "submission_date": "2012-07-10T00:00:00",
        "last_modified_date": "2012-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.2291",
        "title": "On Formal Specification of Maple Programs",
        "authors": [
            "Muhammad Taimoor Khan",
            "Wolfgang Schreiner"
        ],
        "abstract": "This paper is an example-based demonstration of our initial results on the formal specification of programs written in the computer algebra language MiniMaple (a substantial subset of Maple with slight extensions). The main goal of this work is to define a verification framework for MiniMaple. Formal specification of MiniMaple programs is rather complex task as it supports non-standard types of objects, e.g. symbols and unevaluated expressions, and additional functions and predicates, e.g. runtime type tests etc. We have used the specification language to specify various computer algebra concepts respective objects of the Maple package DifferenceDifferential developed at our institute.\n    ",
        "submission_date": "2012-07-10T00:00:00",
        "last_modified_date": "2012-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.2534",
        "title": "LPC(ID): A Sequent Calculus Proof System for Propositional Logic Extended with Inductive Definitions",
        "authors": [
            "Ping Hou",
            "Johan Wittocx",
            "Marc Denecker"
        ],
        "abstract": "The logic FO(ID) uses ideas from the field of logic programming to extend first order logic with non-monotone inductive definitions. Such logic formally extends logic programming, abductive logic programming and datalog, and thus formalizes the view on these formalisms as logics of (generalized) inductive definitions. The goal of this paper is to study a deductive inference method for PC(ID), which is the propositional fragment of FO(ID). We introduce a formal proof system based on the sequent calculus (Gentzen-style deductive system) for this logic. As PC(ID) is an integration of classical propositional logic and propositional inductive definitions, our sequent calculus proof system integrates inference rules for propositional calculus and definitions. We present the soundness and completeness of this proof system with respect to a slightly restricted fragment of PC(ID). We also provide some complexity results for PC(ID). By developing the proof system for PC(ID), it helps us to enhance the understanding of proof-theoretic foundations of FO(ID), and therefore to investigate useful proof systems for FO(ID).\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.3100",
        "title": "Set-valued dynamic treatment regimes for competing outcomes",
        "authors": [
            "Eric B. Laber",
            "Daniel J. Lizotte",
            "Bradley Ferguson"
        ],
        "abstract": "Dynamic treatment regimes operationalize the clinical decision process as a sequence of functions, one for each clinical decision, where each function takes as input up-to-date patient information and gives as output a single recommended treatment. Current methods for estimating optimal dynamic treatment regimes, for example Q-learning, require the specification of a single outcome by which the `goodness' of competing dynamic treatment regimes are measured. However, this is an over-simplification of the goal of clinical decision making, which aims to balance several potentially competing outcomes. For example, often a balance must be struck between treatment effectiveness and side-effect burden. We propose a method for constructing dynamic treatment regimes that accommodates competing outcomes by recommending sets of treatments at each decision point. Formally, we construct a sequence of set-valued functions that take as input up-to-date patient information and give as output a recommended subset of the possible treatments. For a given patient history, the recommended set of treatments contains all treatments that are not inferior according to any of the competing outcomes. When there is more than one decision point, constructing these set-valued functions requires solving a non-trivial enumeration problem. We offer an exact enumeration algorithm by recasting the problem as a linear mixed integer program. The proposed methods are illustrated using data from a depression study and the CATIE schizophrenia study.\n    ",
        "submission_date": "2012-07-12T00:00:00",
        "last_modified_date": "2012-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.3441",
        "title": "Isabelle/jEdit --- a Prover IDE within the PIDE framework",
        "authors": [
            "Makarius Wenzel"
        ],
        "abstract": "PIDE is a general framework for document-oriented prover interaction and integration, based on a bilingual architecture that combines ML and Scala. The overall aim is to connect LCF-style provers like Isabelle (or Coq or HOL) with sophisticated front-end technology on the JVM platform, overcoming command-line interaction at last.\n",
        "submission_date": "2012-07-14T00:00:00",
        "last_modified_date": "2012-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.3560",
        "title": "Diagnosing client faults using SVM-based intelligent inference from TCP packet traces",
        "authors": [
            "Chathuranga Widanapathirana",
            "Y. Ahmet Sekercioglu",
            "Paul G. Fitzpatrick",
            "Milosh V. Ivanovich",
            "Jonathan C. Li"
        ],
        "abstract": "We present the Intelligent Automated Client Diagnostic (IACD) system, which only relies on inference from Transmission Control Protocol (TCP) packet traces for rapid diagnosis of client device problems that cause network performance issues. Using soft-margin Support Vector Machine (SVM) classifiers, the system (i) distinguishes link problems from client problems, and (ii) identifies characteristics unique to client faults to report the root cause of the client device problem. Experimental evaluation demonstrated the capability of the IACD system to distinguish between faulty and healthy links and to diagnose the client faults with 98% accuracy in healthy links. The system can perform fault diagnosis independent of the client's specific TCP implementation, enabling diagnosis capability on diverse range of client computers.\n    ",
        "submission_date": "2012-07-16T00:00:00",
        "last_modified_date": "2012-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.3760",
        "title": "Towards a Self-Organized Agent-Based Simulation Model for Exploration of Human Synaptic Connections",
        "authors": [
            "\u00d6nder G\u00fcrcan",
            "Carole Bernon",
            "Kemal S. T\u00fcrker"
        ],
        "abstract": "In this paper, the early design of our self-organized agent-based simulation model for exploration of synaptic connections that faithfully generates what is observed in natural situation is given. While we take inspiration from neuroscience, our intent is not to create a veridical model of processes in neurodevelopmental biology, nor to represent a real biological system. Instead, our goal is to design a simulation model that learns acting in the same way of human nervous system by using findings on human subjects using reflex methodologies in order to estimate unknown connections.\n    ",
        "submission_date": "2012-07-16T00:00:00",
        "last_modified_date": "2012-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.3869",
        "title": "Automated Inference System for End-To-End Diagnosis of Network Performance Issues in Client-Terminal Devices",
        "authors": [
            "Chathuranga Widanapathirana",
            "Y. Ahmet \u015eekercio\u01e7lu",
            "Milosh V. Ivanovich",
            "Paul G. Fitzpatrick",
            "Jonathan C. Li"
        ],
        "abstract": "Traditional network diagnosis methods of Client-Terminal Device (CTD) problems tend to be laborintensive, time consuming, and contribute to increased customer dissatisfaction. In this paper, we propose an automated solution for rapidly diagnose the root causes of network performance issues in CTD. Based on a new intelligent inference technique, we create the Intelligent Automated Client Diagnostic (IACD) system, which only relies on collection of Transmission Control Protocol (TCP) packet traces. Using soft-margin Support Vector Machine (SVM) classifiers, the system (i) distinguishes link problems from client problems and (ii) identifies characteristics unique to the specific fault to report the root cause. The modular design of the system enables support for new access link and fault types. Experimental evaluation demonstrated the capability of the IACD system to distinguish between faulty and healthy links and to diagnose the client faults with 98% accuracy. The system can perform fault diagnosis independent of the user's specific TCP implementation, enabling diagnosis of diverse range of client devices\n    ",
        "submission_date": "2012-07-17T00:00:00",
        "last_modified_date": "2012-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4109",
        "title": "A Complete Anytime Algorithm for Treewidth",
        "authors": [
            "Vibhav Gogate",
            "Rina Dechter"
        ],
        "abstract": "In this paper, we present a Branch and Bound algorithm called QuickBB for computing the treewidth of an undirected graph. This algorithm performs a search in the space of perfect elimination ordering of vertices of the graph. The algorithm uses novel pruning and propagation techniques which are derived from the theory of graph minors and graph isomorphism. We present a new algorithm called minor-min-width for computing a lower bound on treewidth that is used within the branch and bound algorithm and which improves over earlier available lower bounds. Empirical evaluation of QuickBB on randomly generated graphs and benchmarks in Graph Coloring and Bayesian Networks shows that it is consistently better than complete algorithms like QuickTree [Shoikhet and Geiger, 1997] in terms of cpu time. QuickBB also has good anytime performance, being able to generate a better upper bound on treewidth of some graphs whose optimal treewidth could not be computed up to now.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4122",
        "title": "Bayesian Biosurveillance of Disease Outbreaks",
        "authors": [
            "Gregory F. Cooper",
            "Denver Dash",
            "John Levander",
            "Weng-Keen Wong",
            "William Hogan",
            "Michael Wagner"
        ],
        "abstract": "Early, reliable detection of disease outbreaks is a critical problem today. This paper reports an investigation of the use of causal Bayesian networks to model spatio-temporal patterns of a non-contagious disease (respiratory anthrax infection) in a population of people. The number of parameters in such a network can become enormous, if not carefully managed. Also, inference needs to be performed in real time as population data stream in. We describe techniques we have applied to address both the modeling and inference challenges. A key contribution of this paper is the explication of assumptions and techniques that are sufficient to allow the scaling of Bayesian network modeling and inference to millions of nodes for real-time surveillance applications. The results reported here provide a proof-of-concept that Bayesian networks can serve as the foundation of a system that effectively performs Bayesian biosurveillance of disease outbreaks.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4127",
        "title": "On finding minimal w-cutset",
        "authors": [
            "Bozhena Bidyuk",
            "Rina Dechter"
        ],
        "abstract": "The complexity of a reasoning task over a graphical model is tied to the induced width of the underlying graph. It is well-known that the conditioning (assigning values) on a subset of variables yields a subproblem of the reduced complexity where instantiated variables are removed. If the assigned variables constitute a cycle-cutset, the rest of the network is singly-connected and therefore can be solved by linear propagation algorithms. A w-cutset is a generalization of a cycle-cutset defined as a subset of nodes such that the subgraph with cutset nodes removed has induced-width of w or less. In this paper we address the problem of finding a minimal w-cutset in a graph. We relate the problem to that of finding the minimal w-cutset of a treedecomposition. The latter can be mapped to the well-known set multi-cover problem. This relationship yields a proof of NP-completeness on one hand and a greedy algorithm for finding a w-cutset of a tree decomposition on the other. Empirical evaluation of the algorithms is presented.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4132",
        "title": "MOB-ESP and other Improvements in Probability Estimation",
        "authors": [
            "Rodney Nielsen"
        ],
        "abstract": "A key prerequisite to optimal reasoning under uncertainty in intelligent systems is to start with good class probability estimates. This paper improves on the current best probability estimation trees (Bagged-PETs) and also presents a new ensemble-based algorithm (MOB-ESP). Comparisons are made using several benchmark datasets and multiple metrics. These experiments show that MOB-ESP outputs significantly more accurate class probabilities than either the baseline BPETs algorithm or the enhanced version presented here (EB-PETs). These results are based on metrics closely associated with the average accuracy of the predictions. MOB-ESP also provides much better probability rankings than B-PETs. The paper further suggests how these estimation techniques can be applied in concert with a broader category of classifiers.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4140",
        "title": "Selection of Identifiability Criteria for Total Effects by using Path Diagrams",
        "authors": [
            "Manabu Kuroki",
            "Zhihong Cai"
        ],
        "abstract": "Pearl has provided the back door criterion, the front door criterion and the conditional instrumental variable (IV) method as identifiability criteria for total effects. In some situations, these three criteria can be applied to identifying total effects simultaneously. For the purpose of increasing estimating accuracy, this paper compares the three ways of identifying total effects in terms of the asymptotic variance, and concludes that in some situations the superior of them can be recognized directly from the graph structure.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4445",
        "title": "Communities of Minima in Local Optima Networks of Combinatorial Spaces",
        "authors": [
            "Fabio Daolio",
            "Marco Tomassini",
            "S\u00e9bastien Verel",
            "Gabriela Ochoa"
        ],
        "abstract": "In this work we present a new methodology to study the structure of the configuration spaces of hard combinatorial problems. It consists in building the network that has as nodes the locally optimal configurations and as edges the weighted oriented transitions between their basins of attraction. We apply the approach to the detection of communities in the optima networks produced by two different classes of instances of a hard combinatorial optimization problem: the quadratic assignment problem (QAP). We provide evidence indicating that the two problem instance classes give rise to very different configuration spaces. For the so-called real-like class, the networks possess a clear modular structure, while the optima networks belonging to the class of random uniform instances are less well partitionable into clusters. This is convincingly supported by using several statistical tests. Finally, we shortly discuss the consequences of the findings for heuristically searching the corresponding problem spaces.\n    ",
        "submission_date": "2012-07-18T00:00:00",
        "last_modified_date": "2012-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4448",
        "title": "DAMS: Distributed Adaptive Metaheuristic Selection",
        "authors": [
            "Bilel Derbel",
            "S\u00e9bastien Verel"
        ],
        "abstract": "We present a distributed generic algorithm called DAMS dedicated to adaptive optimization in distributed environments. Given a set of metaheuristic, the goal of DAMS is to coordinate their local execution on distributed nodes in order to optimize the global performance of the distributed system. DAMS is based on three-layer architecture allowing node to decide distributively what local information to communicate, and what metaheuristic to apply while the optimization process is in progress. The adaptive features of DAMS are first addressed in a very general setting. A specific DAMS called SBM is then described and analyzed from both a parallel and an adaptive point of view. SBM is a simple, yet efficient, adaptive distributed algorithm using an exploitation component allowing nodes to select the metaheuristic with the best locally observed performance, and an exploration component allowing nodes to detect the metaheuristic with the actual best performance. The efficiency of BSM-DAMS is demonstrated through experimentations and comparisons with other adaptive strategies (sequential and distributed).\n    ",
        "submission_date": "2012-07-18T00:00:00",
        "last_modified_date": "2012-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4450",
        "title": "NILS: a Neutrality-based Iterated Local Search and its application to Flowshop Scheduling",
        "authors": [
            "Marie-Eleonore Marmion",
            "Clarisse Dhaenens",
            "Laetitia Jourdan",
            "Arnaud Liefooghe",
            "S\u00e9bastien Verel"
        ],
        "abstract": "This paper presents a new methodology that exploits specific characteristics from the fitness landscape. In particular, we are interested in the property of neutrality, that deals with the fact that the same fitness value is assigned to numerous solutions from the search space. Many combinatorial optimization problems share this property, that is generally very inhibiting for local search algorithms. A neutrality-based iterated local search, that allows neutral walks to move on the plateaus, is proposed and experimented on a permutation flowshop scheduling problem with the aim of minimizing the makespan. Our experiments show that the proposed approach is able to find improving solutions compared with a classical iterated local search. Moreover, the tradeoff between the exploitation of neutrality and the exploration of new parts of the search space is deeply analyzed.\n    ",
        "submission_date": "2012-07-18T00:00:00",
        "last_modified_date": "2012-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4451",
        "title": "Set-based Multiobjective Fitness Landscapes: A Preliminary Study",
        "authors": [
            "S\u00e9bastien Verel",
            "Arnaud Liefooghe",
            "Clarisse Dhaenens"
        ],
        "abstract": "Fitness landscape analysis aims to understand the geometry of a given optimization problem in order to design more efficient search algorithms. However, there is a very little knowledge on the landscape of multiobjective problems. In this work, following a recent proposal by Zitzler et al. (2010), we consider multiobjective optimization as a set problem. Then, we give a general definition of set-based multiobjective fitness landscapes. An experimental set-based fitness landscape analysis is conducted on the multiobjective NK-landscapes with objective correlation. The aim is to adapt and to enhance the comprehensive design of set-based multiobjective search approaches, motivated by an a priori analysis of the corresponding set problem properties.\n    ",
        "submission_date": "2012-07-18T00:00:00",
        "last_modified_date": "2012-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4452",
        "title": "Pareto Local Optima of Multiobjective NK-Landscapes with Correlated Objectives",
        "authors": [
            "S\u00e9bastien Verel",
            "Arnaud Liefooghe",
            "Laetitia Jourdan",
            "Clarisse Dhaenens"
        ],
        "abstract": "In this paper, we conduct a fitness landscape analysis for multiobjective combinatorial optimization, based on the local optima of multiobjective NK-landscapes with objective correlation. In single-objective optimization, it has become clear that local optima have a strong impact on the performance of metaheuristics. Here, we propose an extension to the multiobjective case, based on the Pareto dominance. We study the co-influence of the problem dimension, the degree of non-linearity, the number of objectives and the correlation degree between objective functions on the number of Pareto local optima.\n    ",
        "submission_date": "2012-07-18T00:00:00",
        "last_modified_date": "2012-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4455",
        "title": "First-improvement vs. Best-improvement Local Optima Networks of NK Landscapes",
        "authors": [
            "Gabriela Ochoa",
            "S\u00e9bastien Verel",
            "Marco Tomassini"
        ],
        "abstract": "This paper extends a recently proposed model for combinatorial landscapes: Local Optima Networks (LON), to incorporate a first-improvement (greedy-ascent) hill-climbing algorithm, instead of a best-improvement (steepest-ascent) one, for the definition and extraction of the basins of attraction of the landscape optima. A statistical analysis comparing best and first improvement network models for a set of NK landscapes, is presented and discussed. Our results suggest structural differences between the two models with respect to both the network connectivity, and the nature of the basins of attraction. The impact of these differences in the behavior of search heuristics based on first and best improvement local search is thoroughly discussed.\n    ",
        "submission_date": "2012-07-18T00:00:00",
        "last_modified_date": "2012-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4628",
        "title": "On the Effect of Connectedness for Biobjective Multiple and Long Path Problems",
        "authors": [
            "S\u00e9bastien Verel",
            "Arnaud Liefooghe",
            "J\u00e9r\u00e9mie Humeau",
            "Laetitia Jourdan",
            "Clarisse Dhaenens"
        ],
        "abstract": "Recently, the property of connectedness has been claimed to give a strong motivation on the design of local search techniques for multiobjective combinatorial optimization (MOCO). Indeed, when connectedness holds, a basic Pareto local search, initialized with at least one non-dominated solution, allows to identify the efficient set exhaustively. However, this becomes quickly infeasible in practice as the number of efficient solutions typically grows exponentially with the instance size. As a consequence, we generally have to deal with a limited-size approximation, where a good sample set has to be found. In this paper, we propose the biobjective multiple and long path problems to show experimentally that, on the first problems, even if the efficient set is connected, a local search may be outperformed by a simple evolutionary algorithm in the sampling of the efficient set. At the opposite, on the second problems, a local search algorithm may successfully approximate a disconnected efficient set. Then, we argue that connectedness is not the single property to study for the design of local search heuristics for MOCO. This work opens new discussions on a proper definition of the multiobjective fitness landscape.\n    ",
        "submission_date": "2012-07-19T00:00:00",
        "last_modified_date": "2012-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4629",
        "title": "On the Neutrality of Flowshop Scheduling Fitness Landscapes",
        "authors": [
            "Marie-Eleonore Marmion",
            "Clarisse Dhaenens",
            "Laetitia Jourdan",
            "Arnaud Liefooghe",
            "S\u00e9bastien Verel"
        ],
        "abstract": "Solving efficiently complex problems using metaheuristics, and in particular local searches, requires incorporating knowledge about the problem to solve. In this paper, the permutation flowshop problem is studied. It is well known that in such problems, several solutions may have the same fitness value. As this neutrality property is an important one, it should be taken into account during the design of optimization methods. Then in the context of the permutation flowshop, a deep landscape analysis focused on the neutrality property is driven and propositions on the way to use this neutrality to guide efficiently the search are given.\n    ",
        "submission_date": "2012-07-19T00:00:00",
        "last_modified_date": "2012-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4631",
        "title": "Analyzing the Effect of Objective Correlation on the Efficient Set of MNK-Landscapes",
        "authors": [
            "S\u00e9bastien Verel",
            "Arnaud Liefooghe",
            "Laetitia Jourdan",
            "Clarisse Dhaenens"
        ],
        "abstract": "In multiobjective combinatorial optimization, there exists two main classes of metaheuristics, based either on multiple aggregations, or on a dominance relation. As in the single objective case, the structure of the search space can explain the difficulty for multiobjective metaheuristics, and guide the design of such methods. In this work we analyze the properties of multiobjective combinatorial search spaces. In particular, we focus on the features related the efficient set, and we pay a particular attention to the correlation between objectives. Few benchmark takes such objective correlation into account. Here, we define a general method to design multiobjective problems with correlation. As an example, we extend the well-known multiobjective NK-landscapes. By measuring different properties of the search space, we show the importance of considering the objective correlation on the design of metaheuristics.\n    ",
        "submission_date": "2012-07-19T00:00:00",
        "last_modified_date": "2012-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4632",
        "title": "Clustering of Local Optima in Combinatorial Fitness Landscapes",
        "authors": [
            "Gabriela Ochoa",
            "S\u00e9bastien Verel",
            "Fabio Daolio",
            "Marco Tomassini"
        ],
        "abstract": "Using the recently proposed model of combinatorial landscapes: local optima networks, we study the distribution of local optima in two classes of instances of the quadratic assignment problem. Our results indicate that the two problem instance classes give rise to very different configuration spaces. For the so-called real-like class, the optima networks possess a clear modular structure, while the networks belonging to the class of random uniform instances are less well partitionable into clusters. We briefly discuss the consequences of the findings for heuristically searching the corresponding problem spaces.\n    ",
        "submission_date": "2012-07-19T00:00:00",
        "last_modified_date": "2012-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4931",
        "title": "Motion Planning Of an Autonomous Mobile Robot Using Artificial Neural Network",
        "authors": [
            "G. N. Tripathi",
            "V. Rihani"
        ],
        "abstract": "The paper presents the electronic design and motion planning of a robot based on decision making regarding its straight motion and precise turn using Artificial Neural Network (ANN). The ANN helps in learning of robot so that it performs motion autonomously. The weights calculated are implemented in microcontroller. The performance has been tested to be excellent.\n    ",
        "submission_date": "2012-07-20T00:00:00",
        "last_modified_date": "2012-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4940",
        "title": "Ontology for Cellular Communication",
        "authors": [
            "Hasni Neji"
        ],
        "abstract": "The lack of interoperability between mobile cellular access networks has long been a challenging obstacle, which telecommunication engineering is trying to overcome. In second generation networks for example, this problem lies in the fact that there are multiple standards. Each of these standards can operate in the same frequency range. However, each utilizes a different Radio Technology and Modulation Scheme, which are characteristics of the standard. Therefore, the lack of interoperability in 2G occurs because of the lack of standardization. Interoperability within 3G networks is limited to a few operating modes using different Radio Transmission Technologies that are not inter-operable. Thus, interoperability remains an issue for 3G. 4G technology even being successful in its various trials cannot guarantee the interoperability. This is within each network generation; meanwhile between heterogeneous network generations the situation seems to be worst. This approach is first to analyze the structure, inputs, and outputs of three different cellular technologies, performing a domain analysis (of this subset of technologies) and producing a feature model of the domain. Finally, we sought to build an ontology capable of providing a common view of the domain, providing an effective representation of relations between representations of corresponding concepts in different cellular technologies.\n    ",
        "submission_date": "2012-07-20T00:00:00",
        "last_modified_date": "2012-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.5560",
        "title": "Evolving Musical Counterpoint: The Chronopoint Musical Evolution System",
        "authors": [
            "Jeffrey Power Jacobs",
            "James Reggia"
        ],
        "abstract": "Musical counterpoint, a musical technique in which two or more independent melodies are played simultaneously with the goal of creating harmony, has been around since the baroque era. However, to our knowledge computational generation of aesthetically pleasing linear counterpoint based on subjective fitness assessment has not been explored by the evolutionary computation community (although generation using objective fitness has been attempted in quite a few cases). The independence of contrapuntal melodies and the subjective nature of musical aesthetics provide an excellent platform for the application of genetic algorithms. In this paper, a genetic algorithm approach to generating contrapuntal melodies is explained, with a description of the various musical heuristics used and of how variable-length chromosome strings are used to avoid generating \"jerky\" rhythms and melodic phrases, as well as how subjectivity is incorporated into the algorithm's fitness measures. Next, results from empirical testing of the algorithm are presented, with a focus on how a user's musical sophistication influences their experience. Lastly, further musical and compositional applications of the algorithm are discussed along with planned future work on the algorithm.\n    ",
        "submission_date": "2012-07-23T00:00:00",
        "last_modified_date": "2012-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.6051",
        "title": "Composition of Modular Telemetry System with Interval Multiset Estimates",
        "authors": [
            "Mark Sh. Levin"
        ],
        "abstract": "The paper describes combinatorial synthesis approach with interval multset estimates of system elements for modeling, analysis, design, and improvement of a modular telemetry system. Morphological (modular) system design and improvement are considered as composition of the telemetry system elements (components) configuration. The solving process is based on Hierarchical Morphological Multicriteria Design (HMMD): (i) multicriteria selection of alternatives for system components, (ii) synthesis of the selected alternatives into a resultant combination (while taking into account quality of the alternatives above and their compatibility). Interval multiset estimates are used for assessment of design alternatives for telemetry system elements. Two additional systems problems are examined: (a) improvement of the obtained solutions, (b) aggregation of the obtained solutions into a resultant system configuration. The improvement and aggregation processes are based on multiple choice problem with interval multiset estimates. Numerical examples for an on-board telemetry subsystem illustrate the design and improvement processes.\n    ",
        "submission_date": "2012-07-25T00:00:00",
        "last_modified_date": "2012-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.6199",
        "title": "Achieving Approximate Soft Clustering in Data Streams",
        "authors": [
            "Vaneet Aggarwal",
            "Shankar Krishnan"
        ],
        "abstract": "In recent years, data streaming has gained prominence due to advances in technologies that enable many applications to generate continuous flows of data. This increases the need to develop algorithms that are able to efficiently process data streams. Additionally, real-time requirements and evolving nature of data streams make stream mining problems, including clustering, challenging research problems.\n",
        "submission_date": "2012-07-26T00:00:00",
        "last_modified_date": "2012-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.6600",
        "title": "Diversity in Ranking using Negative Reinforcement",
        "authors": [
            "Rama Badrinath",
            "C. E. Veni Madhavan"
        ],
        "abstract": "In this paper, we consider the problem of diversity in ranking of the nodes in a graph. The task is to pick the top-k nodes in the graph which are both 'central' and 'diverse'. Many graph-based models of NLP like text summarization, opinion summarization involve the concept of diversity in generating the summaries. We develop a novel method which works in an iterative fashion based on random walks to achieve diversity. Specifically, we use negative reinforcement as a main tool to introduce diversity in the Personalized PageRank framework. Experiments on two benchmark datasets show that our algorithm is competitive to the existing methods.\n    ",
        "submission_date": "2012-07-27T00:00:00",
        "last_modified_date": "2012-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.6685",
        "title": "FMLtoHOL (version 1.0): Automating First-order Modal Logics with LEO-II and Friends",
        "authors": [
            "Christoph Benzmueller",
            "Thomas Raths"
        ],
        "abstract": "A converter from first-order modal logics to classical higher- order logic is presented. This tool enables the application of off-the-shelf higher-order theorem provers and model finders for reasoning within first- order modal logics. The tool supports logics K, K4, D, D4, T, S4, and S5 with respect to constant, varying and cumulative domain semantics.\n    ",
        "submission_date": "2012-07-28T00:00:00",
        "last_modified_date": "2012-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.7079",
        "title": "Improving multivariate Horner schemes with Monte Carlo tree search",
        "authors": [
            "J. Kuipers",
            "J. A. M. Vermaseren",
            "A. Plaat",
            "H. J. van den Herik"
        ],
        "abstract": "Optimizing the cost of evaluating a polynomial is a classic problem in computer science. For polynomials in one variable, Horner's method provides a scheme for producing a computationally efficient form. For multivariate polynomials it is possible to generalize Horner's method, but this leaves freedom in the order of the variables. Traditionally, greedy schemes like most-occurring variable first are used. This simple textbook algorithm has given remarkably efficient results. Finding better algorithms has proved difficult. In trying to improve upon the greedy scheme we have implemented Monte Carlo tree search, a recent search method from the field of artificial intelligence. This results in better Horner schemes and reduces the cost of evaluating polynomials, sometimes by factors up to two.\n    ",
        "submission_date": "2012-07-30T00:00:00",
        "last_modified_date": "2012-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.1184",
        "title": "Payment Rules through Discriminant-Based Classifiers",
        "authors": [
            "Paul Duetting",
            "Felix Fischer",
            "Pitchayut Jirapinyo",
            "John K. Lai",
            "Benjamin Lubin",
            "David C. Parkes"
        ],
        "abstract": "In mechanism design it is typical to impose incentive compatibility and then derive an optimal mechanism subject to this constraint. By replacing the incentive compatibility requirement with the goal of minimizing expected ex post regret, we are able to adapt statistical machine learning techniques to the design of payment rules. This computational approach to mechanism design is applicable to domains with multi-dimensional types and situations where computational efficiency is a concern. Specifically, given an outcome rule and access to a type distribution, we train a support vector machine with a special discriminant function structure such that it implicitly establishes a payment rule with desirable incentive properties. We discuss applications to a multi-minded combinatorial auction with a greedy winner-determination algorithm and to an assignment problem with egalitarian outcome rule. Experimental results demonstrate both that the construction produces payment rules with low ex post regret, and that penalizing classification errors is effective in preventing failures of ex post individual rationality.\n    ",
        "submission_date": "2012-08-06T00:00:00",
        "last_modified_date": "2012-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.1187",
        "title": "Toward an Integrated Framework for Automated Development and Optimization of Online Advertising Campaigns",
        "authors": [
            "Stamatina Thomaidou",
            "Michalis Vazirgiannis",
            "Kyriakos Liakopoulos"
        ],
        "abstract": "Creating and monitoring competitive and cost-effective pay-per-click advertisement campaigns through the web-search channel is a resource demanding task in terms of expertise and effort. Assisting or even automating the work of an advertising specialist will have an unrivaled commercial value. In this paper we propose a methodology, an architecture, and a fully functional framework for semi- and fully- automated creation, monitoring, and optimization of cost-efficient pay-per-click campaigns with budget constraints. The campaign creation module generates automatically keywords based on the content of the web page to be advertised extended with corresponding ad-texts. These keywords are used to create automatically the campaigns fully equipped with the appropriate values set. The campaigns are uploaded to the auctioneer platform and start running. The optimization module focuses on the learning process from existing campaign statistics and also from applied strategies of previous periods in order to invest optimally in the next period. The objective is to maximize the performance (i.e. clicks, actions) under the current budget constraint. The fully functional prototype is experimentally evaluated on real world Google AdWords campaigns and presents a promising behavior with regards to campaign performance statistics as it outperforms systematically the competing manually maintained campaigns.\n    ",
        "submission_date": "2012-08-06T00:00:00",
        "last_modified_date": "2012-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.1613",
        "title": "A Dynamic Phase Selection Strategy for Satisfiability Solvers",
        "authors": [
            "Jingchao Chen"
        ],
        "abstract": "The phase selection is an important of a SAT Solver based on conflict-driven DPLL. This paper presents a new phase selection strategy, in which the weight of each literal is defined as the sum of its implied-literals static weights. The implied literals of each literal is computed dynamically during the search. Therefore, it is call a dynamic phase selection strategy. In general, computing dynamically a weight is time-consuming. Hence, so far no SAT solver applies successfully a dynamic phase selection. Since the implied literal of our strategy conforms to that of the search process, the usual two watched-literals scheme can be applied here. Thus, the cost of our dynamic phase selection is very low. To improve Glucose 2.0 which won a Gold Medal for application category at SAT 2011 competition, we build five phase selection schemes using the dynamic phase selection policy. On application instances of SAT 2011, Glucose improved by the dynamic phase selection is significantly better than the original Glucose. We conduct also experiments on Lingeling, using the dynamic phase selection policy, and build two phase selection schemes. Experimental results show that the improved Lingeling is better than the original Lingeling.\n    ",
        "submission_date": "2012-08-08T00:00:00",
        "last_modified_date": "2012-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.1692",
        "title": "On Finding Optimal Polytrees",
        "authors": [
            "Serge Gaspers",
            "Mikko Koivisto",
            "Mathieu Liedloff",
            "Sebastian Ordyniak",
            "Stefan Szeider"
        ],
        "abstract": "Inferring probabilistic networks from data is a notoriously difficult task. Under various goodness-of-fit measures, finding an optimal network is NP-hard, even if restricted to polytrees of bounded in-degree. Polynomial-time algorithms are known only for rare special cases, perhaps most notably for branchings, that is, polytrees in which the in-degree of every node is at most one. Here, we study the complexity of finding an optimal polytree that can be turned into a branching by deleting some number of arcs or nodes, treated as a parameter.\n",
        "submission_date": "2012-08-08T00:00:00",
        "last_modified_date": "2012-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.1750",
        "title": "Guidelines for a Dynamic Ontology - Integrating Tools of Evolution and Versioning in Ontology",
        "authors": [
            "Perrine Pittet",
            "Christophe Nicolle",
            "Christophe Cruz"
        ],
        "abstract": "Ontologies are built on systems that conceptually evolve over time. In addition, techniques and languages for building ontologies evolve too. This has led to numerous studies in the field of ontology versioning and ontology evolution. This paper presents a new way to manage the lifecycle of an ontology incorporating both versioning tools and evolution process. This solution, called VersionGraph, is integrated in the source ontology since its creation in order to make it possible to evolve and to be versioned. Change management is strongly related to the model in which the ontology is represented. Therefore, we focus on the OWL language in order to take into account the impact of the changes on the logical consistency of the ontology like specified in OWL DL.\n    ",
        "submission_date": "2012-08-08T00:00:00",
        "last_modified_date": "2012-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.2261",
        "title": "Analysis of Statistical Hypothesis based Learning Mechanism for Faster Crawling",
        "authors": [
            "Sudarshan Nandy",
            "Partha Pratim Sarkar",
            "Achintya Das"
        ],
        "abstract": "The growth of world-wide-web (WWW) spreads its wings from an intangible quantities of web-pages to a gigantic hub of web information which gradually increases the complexity of crawling process in a search engine. A search engine handles a lot of queries from various parts of this world, and the answers of it solely depend on the knowledge that it gathers by means of crawling. The information sharing becomes a most common habit of the society, and it is done by means of publishing structured, semi-structured and unstructured resources on the web. This social practice leads to an exponential growth of web-resource, and hence it became essential to crawl for continuous updating of web-knowledge and modification of several existing resources in any situation. In this paper one statistical hypothesis based learning mechanism is incorporated for learning the behavior of crawling speed in different environment of network, and for intelligently control of the speed of crawler. The scaling technique is used to compare the performance proposed method with the standard crawler. The high speed performance is observed after scaling, and the retrieval of relevant web-resource in such a high speed is analyzed.\n    ",
        "submission_date": "2012-08-10T00:00:00",
        "last_modified_date": "2012-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.3432",
        "title": "A Novel Strategy Selection Method for Multi-Objective Clustering Algorithms Using Game Theory",
        "authors": [
            "Mahsa Badami",
            "Ali Hamzeh",
            "Sattar Hashemi"
        ],
        "abstract": "The most important factors which contribute to the efficiency of game-theoretical algorithms are time and game complexity. In this study, we have offered an elegant method to deal with high complexity of game theoretic multi-objective clustering methods in large-sized data sets. Here, we have developed a method which selects a subset of strategies from strategies profile for each player. In this case, the size of payoff matrices reduces significantly which has a remarkable impact on time complexity. Therefore, practical problems with more data are tractable with less computational complexity. Although strategies set may grow with increasing the number of data points, the presented model of strategy selection reduces the strategy space, considerably, where clusters are subdivided into several sub-clusters in each local game. The remarkable results demonstrate the efficiency of the presented approach in reducing computational complexity of the problem of concern.\n    ",
        "submission_date": "2012-08-15T00:00:00",
        "last_modified_date": "2012-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.3623",
        "title": "Content-based Text Categorization using Wikitology",
        "authors": [
            "Muhammad Rafi",
            "Sundus Hassan",
            "Mohammad Shahid Shaikh"
        ],
        "abstract": "A major computational burden, while performing document clustering, is the calculation of similarity measure between a pair of documents. Similarity measure is a function that assign a real number between 0 and 1 to a pair of documents, depending upon the degree of similarity between them. A value of zero means that the documents are completely dissimilar whereas a value of one indicates that the documents are practically identical. Traditionally, vector-based models have been used for computing the document similarity. The vector-based models represent several features present in documents. These approaches to similarity measures, in general, cannot account for the semantics of the document. Documents written in human languages contain contexts and the words used to describe these contexts are generally semantically related. Motivated by this fact, many researchers have proposed semantic-based similarity measures by utilizing text annotation through external thesauruses like WordNet (a lexical database). In this paper, we define a semantic similarity measure based on documents represented in topic maps. Topic maps are rapidly becoming an industrial standard for knowledge representation with a focus for later search and extraction. The documents are transformed into a topic map based coded knowledge and the similarity between a pair of documents is represented as a correlation between the common patterns. The experimental studies on the text mining datasets reveal that this new similarity measure is more effective as compared to commonly used similarity measures in text clustering.\n    ",
        "submission_date": "2012-08-17T00:00:00",
        "last_modified_date": "2012-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.4773",
        "title": "Optimized Look-Ahead Tree Policies: A Bridge Between Look-Ahead Tree Policies and Direct Policy Search",
        "authors": [
            "Tobias Jung",
            "Louis Wehenkel",
            "Damien Ernst",
            "Francis Maes"
        ],
        "abstract": "Direct policy search (DPS) and look-ahead tree (LT) policies are two widely used classes of techniques to produce high performance policies for sequential decision-making problems. To make DPS approaches work well, one crucial issue is to select an appropriate space of parameterized policies with respect to the targeted problem. A fundamental issue in LT approaches is that, to take good decisions, such policies must develop very large look-ahead trees which may require excessive online computational resources. In this paper, we propose a new hybrid policy learning scheme that lies at the intersection of DPS and LT, in which the policy is an algorithm that develops a small look-ahead tree in a directed way, guided by a node scoring function that is learned through DPS. The LT-based representation is shown to be a versatile way of representing policies in a DPS scheme, while at the same time, DPS enables to significantly reduce the size of the look-ahead trees that are required to take high-quality decisions.\n",
        "submission_date": "2012-08-23T00:00:00",
        "last_modified_date": "2012-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.5654",
        "title": "Document Clustering Evaluation: Divergence from a Random Baseline",
        "authors": [
            "Christopher M. De Vries",
            "Shlomo Geva",
            "Andrew Trotman"
        ],
        "abstract": "Divergence from a random baseline is a technique for the evaluation of document clustering. It ensures cluster quality measures are performing work that prevents ineffective clusterings from giving high scores to clusterings that provide no useful result. These concepts are defined and analysed using intrinsic and extrinsic approaches to the evaluation of document cluster quality. This includes the classical clusters to categories approach and a novel approach that uses ad hoc information retrieval. The divergence from a random baseline approach is able to differentiate ineffective clusterings encountered in the INEX XML Mining track. It also appears to perform a normalisation similar to the Normalised Mutual Information (NMI) measure but it can be applied to any measure of cluster quality. When it is applied to the intrinsic measure of distortion as measured by RMSE, subtraction from a random baseline provides a clear optimum that is not apparent otherwise. This approach can be applied to any clustering evaluation. This paper describes its use in the context of document clustering evaluation.\n    ",
        "submission_date": "2012-08-28T00:00:00",
        "last_modified_date": "2012-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.6335",
        "title": "Comparative Study and Optimization of Feature-Extraction Techniques for Content based Image Retrieval",
        "authors": [
            "Aman Chadha",
            "Sushmit Mallik",
            "Ravdeep Johar"
        ],
        "abstract": "The aim of a Content-Based Image Retrieval (CBIR) system, also known as Query by Image Content (QBIC), is to help users to retrieve relevant images based on their contents. CBIR technologies provide a method to find images in large databases by using unique descriptors from a trained image. The image descriptors include texture, color, intensity and shape of the object inside an image. Several feature-extraction techniques viz., Average RGB, Color Moments, Co-occurrence, Local Color Histogram, Global Color Histogram and Geometric Moment have been critically compared in this paper. However, individually these techniques result in poor performance. So, combinations of these techniques have also been evaluated and results for the most efficient combination of techniques have been presented and optimized for each class of image query. We also propose an improvement in image retrieval performance by introducing the idea of Query modification through image cropping. It enables the user to identify a region of interest and modify the initial query to refine and personalize the image retrieval results.\n    ",
        "submission_date": "2012-08-30T00:00:00",
        "last_modified_date": "2020-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.0308",
        "title": "Optimizing Supply Chain Management using Gravitational Search Algorithm and Multi Agent System",
        "authors": [
            "Muneendra Ojha"
        ],
        "abstract": "Supply chain management is a very dynamic operation research problem where one has to quickly adapt according to the changes perceived in environment in order to maximize the benefit or minimize the loss. Therefore we require a system which changes as per the changing requirements. Multi agent system technology in recent times has emerged as a possible way of efficient solution implementation for many such complex problems. Our research here focuses on building a Multi Agent System (MAS), which implements a modified version of Gravitational Search swarm intelligence Algorithm (GSA) to find out an optimal strategy in managing the demand supply chain. We target the grains distribution system among various centers of Food Corporation of India (FCI) as application domain. We assume centers with larger stocks as objects of greater mass and vice versa. Applying Newtonian law of gravity as suggested in GSA, larger objects attract objects of smaller mass towards itself, creating a virtual grain supply source. As heavier object sheds its mass by supplying some to the one in demand, it loses its gravitational pull and thus keeps the whole system of supply chain per-fectly in balance. The multi agent system helps in continuous updation of the whole system with the help of autonomous agents which react to the change in environment and act accordingly. This model also reduces the communication bottleneck to greater extents.\n    ",
        "submission_date": "2012-09-03T00:00:00",
        "last_modified_date": "2012-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.0911",
        "title": "Conquering the rating bound problem in neighborhood-based collaborative filtering: a function recovery approach",
        "authors": [
            "Junming Huang",
            "Xue-Qi Cheng",
            "Hua-Wei Shen",
            "Xiaoming Sun",
            "Tao Zhou",
            "Xiaolong Jin"
        ],
        "abstract": "As an important tool for information filtering in the era of socialized web, recommender systems have witnessed rapid development in the last decade. As benefited from the better interpretability, neighborhood-based collaborative filtering techniques, such as item-based collaborative filtering adopted by Amazon, have gained a great success in many practical recommender systems. However, the neighborhood-based collaborative filtering method suffers from the rating bound problem, i.e., the rating on a target item that this method estimates is bounded by the observed ratings of its all neighboring items. Therefore, it cannot accurately estimate the unobserved rating on a target item, if its ground truth rating is actually higher (lower) than the highest (lowest) rating over all items in its neighborhood. In this paper, we address this problem by formalizing rating estimation as a task of recovering a scalar rating function. With a linearity assumption, we infer all the ratings by optimizing the low-order norm, e.g., the $l_1/2$-norm, of the second derivative of the target scalar function, while remaining its observed ratings unchanged. Experimental results on three real datasets, namely Douban, Goodreads and MovieLens, demonstrate that the proposed approach can well overcome the rating bound problem. Particularly, it can significantly improve the accuracy of rating estimation by 37% than the conventional neighborhood-based methods.\n    ",
        "submission_date": "2012-09-05T00:00:00",
        "last_modified_date": "2012-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.1086",
        "title": "Robustness and Generalization for Metric Learning",
        "authors": [
            "Aur\u00e9lien Bellet",
            "Amaury Habrard"
        ],
        "abstract": "Metric learning has attracted a lot of interest over the last decade, but the generalization ability of such methods has not been thoroughly studied. In this paper, we introduce an adaptation of the notion of algorithmic robustness (previously introduced by Xu and Mannor) that can be used to derive generalization bounds for metric learning. We further show that a weak notion of robustness is in fact a necessary and sufficient condition for a metric learning algorithm to generalize. To illustrate the applicability of the proposed framework, we derive generalization results for a large family of existing metric learning algorithms, including some sparse formulations that are not covered by previous results.\n    ",
        "submission_date": "2012-09-05T00:00:00",
        "last_modified_date": "2014-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.1885",
        "title": "Parametric Constructive Kripke-Semantics for Standard Multi-Agent Belief and Knowledge (Knowledge As Unbiased Belief)",
        "authors": [
            "Simon Kramer",
            "Joshua Sack"
        ],
        "abstract": "We propose parametric constructive Kripke-semantics for multi-agent KD45-belief and S5-knowledge in terms of elementary set-theoretic constructions of two basic functional building blocks, namely bias (or viewpoint) and visibility, functioning also as the parameters of the doxastic and epistemic accessibility relation. The doxastic accessibility relates two possible worlds whenever the application of the composition of bias with visibility to the first world is equal to the application of visibility to the second world. The epistemic accessibility is the transitive closure of the union of our doxastic accessibility and its converse. Therefrom, accessibility relations for common and distributed belief and knowledge can be constructed in a standard way. As a result, we obtain a general definition of knowledge in terms of belief that enables us to view S5-knowledge as accurate (unbiased and thus true) KD45-belief, negation-complete belief and knowledge as exact KD45-belief and S5-knowledge, respectively, and perfect S5-knowledge as precise (exact and accurate) KD45-belief, and all this generically for arbitrary functions of bias and visibility. Our results can be seen as a semantic complement to previous foundational results by Halpern et al. about the (un)definability and (non-)reducibility of knowledge in terms of and to belief, respectively.\n    ",
        "submission_date": "2012-09-10T00:00:00",
        "last_modified_date": "2012-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.2204",
        "title": "How is non-knowledge represented in economic theory?",
        "authors": [
            "Ekaterina Svetlova",
            "Henk van Elst"
        ],
        "abstract": "In this article, we address the question of how non-knowledge about future events that influence economic agents' decisions in choice settings has been formally represented in economic theory up to date. To position our discussion within the ongoing debate on uncertainty, we provide a brief review of historical developments in economic theory and decision theory on the description of economic agents' choice behaviour under conditions of uncertainty, understood as either (i) ambiguity, or (ii) unawareness. Accordingly, we identify and discuss two approaches to the formalisation of non-knowledge: one based on decision-making in the context of a state space representing the exogenous world, as in Savage's axiomatisation and some successor concepts (ambiguity as situations with unknown probabilities), and one based on decision-making over a set of menus of potential future opportunities, providing the possibility of derivation of agents' subjective state spaces (unawareness as situation with imperfect subjective knowledge of all future events possible). We also discuss impeding challenges of the formalisation of non-knowledge.\n    ",
        "submission_date": "2012-09-10T00:00:00",
        "last_modified_date": "2012-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.2295",
        "title": "Multimodal diffusion geometry by joint diagonalization of Laplacians",
        "authors": [
            "Davide Eynard",
            "Klaus Glashoff",
            "Michael M. Bronstein",
            "Alexander M. Bronstein"
        ],
        "abstract": "We construct an extension of diffusion geometry to multiple modalities through joint approximate diagonalization of Laplacian matrices. This naturally extends classical data analysis tools based on spectral geometry, such as diffusion maps and spectral clustering. We provide several synthetic and real examples of manifold learning, retrieval, and clustering demonstrating that the joint diffusion geometry frequently better captures the inherent structure of multi-modal data. We also show that many previous attempts to construct multimodal spectral clustering can be seen as particular cases of joint approximate diagonalization of the Laplacians.\n    ",
        "submission_date": "2012-09-11T00:00:00",
        "last_modified_date": "2012-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.2355",
        "title": "Counterfactual Reasoning and Learning Systems",
        "authors": [
            "L\u00e9on Bottou",
            "Jonas Peters",
            "Joaquin Qui\u00f1onero-Candela",
            "Denis X. Charles",
            "D. Max Chickering",
            "Elon Portugaly",
            "Dipankar Ray",
            "Patrice Simard",
            "Ed Snelson"
        ],
        "abstract": "This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select changes that improve both the short-term and long-term performance of such systems. This work is illustrated by experiments carried out on the ad placement system associated with the Bing search engine.\n    ",
        "submission_date": "2012-09-11T00:00:00",
        "last_modified_date": "2013-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.2548",
        "title": "Training a Feed-forward Neural Network with Artificial Bee Colony Based Backpropagation Method",
        "authors": [
            "Sudarshan Nandy",
            "Partha Pratim Sarkar",
            "Achintya Das"
        ],
        "abstract": "Back-propagation algorithm is one of the most widely used and popular techniques to optimize the feed forward neural network training. Nature inspired meta-heuristic algorithms also provide derivative-free solution to optimize complex problem. Artificial bee colony algorithm is a nature inspired meta-heuristic algorithm, mimicking the foraging or food source searching behaviour of bees in a bee colony and this algorithm is implemented in several applications for an improved optimized outcome. The proposed method in this paper includes an improved artificial bee colony algorithm based back-propagation neural network training method for fast and improved convergence rate of the hybrid neural network learning method. The result is analysed with the genetic algorithm based back-propagation method, and it is another hybridized procedure of its kind. Analysis is performed over standard data sets, reflecting the light of efficiency of proposed method in terms of convergence speed and rate.\n    ",
        "submission_date": "2012-09-12T00:00:00",
        "last_modified_date": "2012-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.2620",
        "title": "Probabilities on Sentences in an Expressive Logic",
        "authors": [
            "Marcus Hutter",
            "John W. Lloyd",
            "Kee Siong Ng",
            "William T. B. Uther"
        ],
        "abstract": "Automated reasoning about uncertain knowledge has many applications. One difficulty when developing such systems is the lack of a completely satisfactory integration of logic and probability. We address this problem directly. Expressive languages like higher-order logic are ideally suited for representing and reasoning about structured knowledge. Uncertain knowledge can be modeled by using graded probabilities rather than binary truth-values. The main technical problem studied in this paper is the following: Given a set of sentences, each having some probability of being true, what probability should be ascribed to other (query) sentences? A natural wish-list, among others, is that the probability distribution (i) is consistent with the knowledge base, (ii) allows for a consistent inference procedure and in particular (iii) reduces to deductive logic in the limit of probabilities being 0 and 1, (iv) allows (Bayesian) inductive reasoning and (v) learning in the limit and in particular (vi) allows confirmation of universally quantified hypotheses/sentences. We translate this wish-list into technical requirements for a prior probability and show that probabilities satisfying all our criteria exist. We also give explicit constructions and several general characterizations of probabilities that satisfy some or all of the criteria and various (counter) examples. We also derive necessary and sufficient conditions for extending beliefs about finitely many sentences to suitable probabilities over all sentences, and in particular least dogmatic or least biased ones. We conclude with a brief outlook on how the developed theory might be used and approximated in autonomous reasoning agents. Our theory is a step towards a globally consistent and empirically satisfactory unification of probability and logic.\n    ",
        "submission_date": "2012-09-12T00:00:00",
        "last_modified_date": "2012-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.2948",
        "title": "Cultural Algorithm Toolkit for Multi-objective Rule Mining",
        "authors": [
            "Sujatha Srinivasan",
            "Sivakumar Ramakrishnan"
        ],
        "abstract": "Cultural algorithm is a kind of evolutionary algorithm inspired from societal evolution and is composed of a belief space, a population space and a protocol that enables exchange of knowledge between these sources. Knowledge created in the population space is accepted into the belief space while this collective knowledge from these sources is combined to influence the decisions of the individual agents in solving problems. Classification rules comes under descriptive knowledge discovery in data mining and are the most sought out by users since they represent highly comprehensible form of knowledge. The rules have certain properties which make them useful forms of actionable knowledge to users. The rules are evaluated using these properties namely the rule metrics. In the current study a Cultural Algorithm Toolkit for Classification Rule Mining (CAT-CRM) is proposed which allows the user to control three different set of parameters namely the evolutionary parameters, the rule parameters as well as agent parameters and hence can be used for experimenting with an evolutionary system, a rule mining system or an agent based social system. Results of experiments conducted to observe the effect of different number and type of metrics on the performance of the algorithm on bench mark data sets is reported.\n    ",
        "submission_date": "2012-09-12T00:00:00",
        "last_modified_date": "2012-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.3487",
        "title": "A framework for large-scale distributed AI search across disconnected heterogeneous infrastructures",
        "authors": [
            "Lars Kotthoff",
            "Tom Kelsey",
            "Martin McCaffery"
        ],
        "abstract": "We present a framework for a large-scale distributed eScience Artificial Intelligence search. Our approach is generic and can be used for many different problems. Unlike many other approaches, we do not require dedicated machines, homogeneous infrastructure or the ability to communicate between nodes. We give special consideration to the robustness of the framework, minimising the loss of effort even after total loss of infrastructure, and allowing easy verification of every step of the distribution process. In contrast to most eScience applications, the input data and specification of the problem is very small, being easily given in a paragraph of text. The unique challenges our framework tackles are related to the combinatorial explosion of the space that contains the possible solutions and the robustness of long-running computations. Not only is the time required to finish the computations unknown, but also the resource requirements may change during the course of the computation. We demonstrate the applicability of our framework by using it to solve a challenging and hitherto open problem in computational mathematics. The results demonstrate that our approach easily scales to computations of a size that would have been impossible to tackle in practice just a decade ago.\n    ",
        "submission_date": "2012-09-16T00:00:00",
        "last_modified_date": "2012-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.3694",
        "title": "Submodularity in Batch Active Learning and Survey Problems on Gaussian Random Fields",
        "authors": [
            "Yifei Ma",
            "Roman Garnett",
            "Jeff Schneider"
        ],
        "abstract": "Many real-world datasets can be represented in the form of a graph whose edge weights designate similarities between instances. A discrete Gaussian random field (GRF) model is a finite-dimensional Gaussian process (GP) whose prior covariance is the inverse of a graph Laplacian. Minimizing the trace of the predictive covariance Sigma (V-optimality) on GRFs has proven successful in batch active learning classification problems with budget constraints. However, its worst-case bound has been missing. We show that the V-optimality on GRFs as a function of the batch query set is submodular and hence its greedy selection algorithm guarantees an (1-1/e) approximation ratio. Moreover, GRF models have the absence-of-suppressor (AofS) condition. For active survey problems, we propose a similar survey criterion which minimizes 1'(Sigma)1. In practice, V-optimality criterion performs better than GPs with mutual information gain criteria and allows nonuniform costs for different nodes.\n    ",
        "submission_date": "2012-09-17T00:00:00",
        "last_modified_date": "2012-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.3916",
        "title": "Qualitative Modelling via Constraint Programming: Past, Present and Future",
        "authors": [
            "Thomas W. Kelsey",
            "Lars Kotthoff",
            "Christoffer A. Jefferson",
            "Stephen A. Linton",
            "Ian Miguel",
            "Peter Nightingale",
            "Ian P. Gent"
        ],
        "abstract": "Qualitative modelling is a technique integrating the fields of theoretical computer science, artificial intelligence and the physical and biological sciences. The aim is to be able to model the behaviour of systems without estimating parameter values and fixing the exact quantitative dynamics. Traditional applications are the study of the dynamics of physical and biological systems at a higher level of abstraction than that obtained by estimation of numerical parameter values for a fixed quantitative model. Qualitative modelling has been studied and implemented to varying degrees of sophistication in Petri nets, process calculi and constraint programming. In this paper we reflect on the strengths and weaknesses of existing frameworks, we demonstrate how recent advances in constraint programming can be leveraged to produce high quality qualitative models, and we describe the advances in theory and technology that would be needed to make constraint programming the best option for scientific investigation in the broadest sense.\n    ",
        "submission_date": "2012-09-18T00:00:00",
        "last_modified_date": "2012-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.5245",
        "title": "Spike Timing Dependent Competitive Learning in Recurrent Self Organizing Pulsed Neural Networks Case Study: Phoneme and Word Recognition",
        "authors": [
            "Tarek Behi",
            "Najet Arous",
            "Noureddine Ellouze"
        ],
        "abstract": "Synaptic plasticity seems to be a capital aspect of the dynamics of neural networks. It is about the physiological modifications of the synapse, which have like consequence a variation of the value of the synaptic weight. The information encoding is based on the precise timing of single spike events that is based on the relative timing of the pre- and post-synaptic spikes, local synapse competitions within a single neuron and global competition via lateral connections. In order to classify temporal sequences, we present in this paper how to use a local hebbian learning, spike-timing dependent plasticity for unsupervised competitive learning, preserving self-organizing maps of spiking neurons. In fact we present three variants of self-organizing maps (SOM) with spike-timing dependent Hebbian learning rule, the Leaky Integrators Neurons (LIN), the Spiking_SOM and the recurrent Spiking_SOM (RSSOM) models. The case study of the proposed SOM variants is phoneme classification and word recognition in continuous speech and speaker independent.\n    ",
        "submission_date": "2012-09-24T00:00:00",
        "last_modified_date": "2012-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.5571",
        "title": "A Cookbook for Temporal Conceptual Data Modelling with Description Logics",
        "authors": [
            "Alessandro Artale",
            "Roman Kontchakov",
            "Vladislav Ryzhikov",
            "Michael Zakharyaschev"
        ],
        "abstract": "We design temporal description logics suitable for reasoning about temporal conceptual data models and investigate their computational complexity. Our formalisms are based on DL-Lite logics with three types of concept inclusions (ranging from atomic concept inclusions and disjointness to the full Booleans), as well as cardinality constraints and role inclusions. In the temporal dimension, they capture future and past temporal operators on concepts, flexible and rigid roles, the operators `always' and `some time' on roles, data assertions for particular moments of time and global concept inclusions. The logics are interpreted over the Cartesian products of object domains and the flow of time (Z,<), satisfying the constant domain assumption. We prove that the most expressive of our temporal description logics (which can capture lifespan cardinalities and either qualitative or quantitative evolution constraints) turn out to be undecidable. However, by omitting some of the temporal operators on concepts/roles or by restricting the form of concept inclusions we obtain logics whose complexity ranges between PSpace and NLogSpace. These positive results were obtained by reduction to various clausal fragments of propositional temporal logic, which opens a way to employ propositional or first-order temporal provers for reasoning about temporal data models.\n    ",
        "submission_date": "2012-09-25T00:00:00",
        "last_modified_date": "2014-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.0052",
        "title": "Dimensionality Reduction and Classification feature using Mutual Information applied to Hyperspectral Images : A Filter strategy based algorithm",
        "authors": [
            "ELkebir Sarhrouni",
            "Ahmed Hammouch",
            "Driss Aboutajdine"
        ],
        "abstract": "Hyperspectral images (HIS) classification is a high technical remote sensing tool. The goal is to reproduce a thematic map that will be compared with a reference ground truth map (GT), constructed by expecting the region. The HIS contains more than a hundred bidirectional measures, called bands (or simply images), of the same region. They are taken at juxtaposed frequencies. Unfortunately, some bands contain redundant information, others are affected by the noise, and the high dimensionality of features made the accuracy of classification lower. The problematic is how to find the good bands to classify the pixels of regions. Some methods use Mutual Information (MI) and threshold, to select relevant bands, without treatment of redundancy. Others control and eliminate redundancy by selecting the band top ranking the MI, and if its neighbors have sensibly the same MI with the GT, they will be considered redundant and so discarded. This is the most inconvenient of this method, because this avoids the advantage of hyperspectral images: some precious information can be discarded. In this paper we'll accept the useful redundancy. A band contains useful redundancy if it contributes to produce an estimated reference map that has higher MI with the ",
        "submission_date": "2012-09-28T00:00:00",
        "last_modified_date": "2012-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.0252",
        "title": "A Linguistic Model for Terminology Extraction based Conditional Random Fields",
        "authors": [
            "Fethi Fkih",
            "Mohamed Nazih Omri",
            "Imen Toumia"
        ],
        "abstract": "In this paper, we show the possibility of using a linear Conditional Random Fields (CRF) for terminology extraction from a specialized text corpus.\n    ",
        "submission_date": "2012-09-30T00:00:00",
        "last_modified_date": "2014-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.0690",
        "title": "Revisiting the Training of Logic Models of Protein Signaling Networks with a Formal Approach based on Answer Set Programming",
        "authors": [
            "Santiago Videla",
            "Carito Guziolowski",
            "Federica Eduati",
            "Sven Thiele",
            "Niels Grabe",
            "Julio Saez-Rodriguez",
            "Anne Siegel"
        ],
        "abstract": "A fundamental question in systems biology is the construction and training to data of mathematical models. Logic formalisms have become very popular to model signaling networks because their simplicity allows us to model large systems encompassing hundreds of proteins. An approach to train (Boolean) logic models to high-throughput phospho-proteomics data was recently introduced and solved using optimization heuristics based on stochastic methods. Here we demonstrate how this problem can be solved using Answer Set Programming (ASP), a declarative problem solving paradigm, in which a problem is encoded as a logical program such that its answer sets represent solutions to the problem. ASP has significant improvements over heuristic methods in terms of efficiency and scalability, it guarantees global optimality of solutions as well as provides a complete set of solutions. We illustrate the application of ASP with in silico cases based on realistic networks and data.\n    ",
        "submission_date": "2012-10-02T00:00:00",
        "last_modified_date": "2012-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.1161",
        "title": "Feature Subset Selection for Software Cost Modelling and Estimation",
        "authors": [
            "Efi Papatheocharous",
            "Harris Papadopoulos",
            "Andreas S. Andreou"
        ],
        "abstract": "Feature selection has been recently used in the area of software engineering for improving the accuracy and robustness of software cost models. The idea behind selecting the most informative subset of features from a pool of available cost drivers stems from the hypothesis that reducing the dimensionality of datasets will significantly minimise the complexity and time required to reach to an estimation using a particular modelling technique. This work investigates the appropriateness of attributes, obtained from empirical project databases and aims to reduce the cost drivers used while preserving performance. Finding suitable subset selections that may cater improved predictions may be considered as a pre-processing step of a particular technique employed for cost estimation (filter or wrapper) or an internal (embedded) step to minimise the fitting error. This paper compares nine relatively popular feature selection methods and uses the empirical values of selected attributes recorded in the ISBSG and Desharnais datasets to estimate software development effort.\n    ",
        "submission_date": "2012-10-03T00:00:00",
        "last_modified_date": "2012-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.1184",
        "title": "Elegant Object-oriented Software Design via Interactive, Evolutionary Computation",
        "authors": [
            "Christopher L. Simons",
            "Ian C. Parmee"
        ],
        "abstract": "Design is fundamental to software development but can be demanding to perform. Thus to assist the software designer, evolutionary computing is being increasingly applied using machine-based, quantitative fitness functions to evolve software designs. However, in nature, elegance and symmetry play a crucial role in the reproductive fitness of various organisms. In addition, subjective evaluation has also been exploited in Interactive Evolutionary Computation (IEC). Therefore to investigate the role of elegance and symmetry in software design, four novel elegance measures are proposed based on the evenness of distribution of design elements. In controlled experiments in a dynamic interactive evolutionary computation environment, designers are presented with visualizations of object-oriented software designs, which they rank according to a subjective assessment of elegance. For three out of the four elegance measures proposed, it is found that a significant correlation exists between elegance values and reward elicited. These three elegance measures assess the evenness of distribution of (a) attributes and methods among classes, (b) external couples between classes, and (c) the ratio of attributes to methods. It is concluded that symmetrical elegance is in some way significant in software design, and that this can be exploited in dynamic, multi-objective interactive evolutionary computation to produce elegant software designs.\n    ",
        "submission_date": "2012-10-03T00:00:00",
        "last_modified_date": "2012-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.1207",
        "title": "Learning Human Activities and Object Affordances from RGB-D Videos",
        "authors": [
            "Hema Swetha Koppula",
            "Rudhir Gupta",
            "Ashutosh Saxena"
        ],
        "abstract": "Understanding human activities and object affordances are two very important skills, especially for personal robots which operate in human environments. In this work, we consider the problem of extracting a descriptive labeling of the sequence of sub-activities being performed by a human, and more importantly, of their interactions with the objects in the form of associated affordances. Given a RGB-D video, we jointly model the human activities and object affordances as a Markov random field where the nodes represent objects and sub-activities, and the edges represent the relationships between object affordances, their relations with sub-activities, and their evolution over time. We formulate the learning problem using a structural support vector machine (SSVM) approach, where labelings over various alternate temporal segmentations are considered as latent variables. We tested our method on a challenging dataset comprising 120 activity videos collected from 4 subjects, and obtained an accuracy of 79.4% for affordance, 63.4% for sub-activity and 75.0% for high-level activity labeling. We then demonstrate the use of such descriptive labeling in performing assistive tasks by a PR2 robot.\n    ",
        "submission_date": "2012-10-04T00:00:00",
        "last_modified_date": "2013-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.1317",
        "title": "Learning Heterogeneous Similarity Measures for Hybrid-Recommendations in Meta-Mining",
        "authors": [
            "Phong Nguyen",
            "Jun Wang",
            "Melanie Hilario",
            "Alexandros Kalousis"
        ],
        "abstract": "The notion of meta-mining has appeared recently and extends the traditional meta-learning in two ways. First it does not learn meta-models that provide support only for the learning algorithm selection task but ones that support the whole data-mining process. In addition it abandons the so called black-box approach to algorithm description followed in meta-learning. Now in addition to the datasets, algorithms also have descriptors, workflows as well. For the latter two these descriptions are semantic, describing properties of the algorithms. With the availability of descriptors both for datasets and data mining workflows the traditional modelling techniques followed in meta-learning, typically based on classification and regression algorithms, are no longer appropriate. Instead we are faced with a problem the nature of which is much more similar to the problems that appear in recommendation systems. The most important meta-mining requirements are that suggestions should use only datasets and workflows descriptors and the cold-start problem, e.g. providing workflow suggestions for new datasets.\n",
        "submission_date": "2012-10-04T00:00:00",
        "last_modified_date": "2012-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.1766",
        "title": "Bayesian Inference with Posterior Regularization and applications to Infinite Latent SVMs",
        "authors": [
            "Jun Zhu",
            "Ning Chen",
            "Eric P. Xing"
        ],
        "abstract": "Existing Bayesian models, especially nonparametric Bayesian methods, rely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations. While priors can affect posterior distributions through Bayes' rule, imposing posterior regularization is arguably more direct and in some cases more natural and general. In this paper, we present regularized Bayesian inference (RegBayes), a novel computational framework that performs posterior inference with a regularization term on the desired post-data posterior distribution under an information theoretical formulation. RegBayes is more flexible than the procedure that elicits expert knowledge via priors, and it covers both directed Bayesian networks and undirected Markov networks whose Bayesian formulation results in hybrid chain graph models. When the regularization is induced from a linear operator on the posterior distributions, such as the expectation operator, we present a general convex-analysis theorem to characterize the solution of RegBayes. Furthermore, we present two concrete examples of RegBayes, infinite latent support vector machines (iLSVM) and multi-task infinite latent support vector machines (MT-iLSVM), which explore the large-margin idea in combination with a nonparametric Bayesian model for discovering predictive latent features for classification and multi-task learning, respectively. We present efficient inference methods and report empirical studies on several benchmark datasets, which appear to demonstrate the merits inherited from both large-margin learning and Bayesian nonparametrics. Such results were not available until now, and contribute to push forward the interface between these two important subfields, which have been largely treated as isolated in the community.\n    ",
        "submission_date": "2012-10-05T00:00:00",
        "last_modified_date": "2014-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.1928",
        "title": "Information fusion in multi-task Gaussian processes",
        "authors": [
            "Shrihari Vasudevan",
            "Arman Melkumyan",
            "Steven Scheding"
        ],
        "abstract": "This paper evaluates heterogeneous information fusion using multi-task Gaussian processes in the context of geological resource modeling. Specifically, it empirically demonstrates that information integration across heterogeneous information sources leads to superior estimates of all the quantities being modeled, compared to modeling them individually. Multi-task Gaussian processes provide a powerful approach for simultaneous modeling of multiple quantities of interest while taking correlations between these quantities into consideration. Experiments are performed on large scale real sensor data.\n    ",
        "submission_date": "2012-10-06T00:00:00",
        "last_modified_date": "2013-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.2164",
        "title": "ET-LDA: Joint Topic Modeling For Aligning, Analyzing and Sensemaking of Public Events and Their Twitter Feeds",
        "authors": [
            "Yuheng Hu",
            "Ajita John",
            "Fei Wang",
            "Doree Duncan Seligmann",
            "Subbarao Kambhampati"
        ],
        "abstract": "Social media channels such as Twitter have emerged as popular platforms for crowds to respond to public events such as speeches, sports and debates. While this promises tremendous opportunities to understand and make sense of the reception of an event from the social media, the promises come entwined with significant technical challenges. In particular, given an event and an associated large scale collection of tweets, we need approaches to effectively align tweets and the parts of the event they refer to. This in turn raises questions about how to segment the event into smaller yet meaningful parts, and how to figure out whether a tweet is a general one about the entire event or specific one aimed at a particular segment of the event. In this work, we present ET-LDA, an effective method for aligning an event and its tweets through joint statistical modeling of topical influences from the events and their associated tweets. The model enables the automatic segmentation of the events and the characterization of tweets into two categories: (1) episodic tweets that respond specifically to the content in the segments of the events, and (2) steady tweets that respond generally about the events. We present an efficient inference method for this model, and a comprehensive evaluation of its effectiveness over existing methods. In particular, through a user study, we demonstrate that users find the topics, the segments, the alignment, and the episodic tweets discovered by ET-LDA to be of higher quality and more interesting as compared to the state-of-the-art, with improvements in the range of 18-41%.\n    ",
        "submission_date": "2012-10-08T00:00:00",
        "last_modified_date": "2012-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.2195",
        "title": "Annotating Answer-Set Programs in LANA?",
        "authors": [
            "Marina De Vos",
            "Do\u011fa Gizem K\u0131za",
            "Johannes Oetsch",
            "J\u00f6rg P\u00fchrer",
            "Hans Tompits"
        ],
        "abstract": "While past research in answer-set programming (ASP) mainly focused on theory, ASP solver technology, and applications, the present work situates itself in the context of a quite recent research trend: development support for ASP. In particular, we propose to augment answer-set programs with additional meta-information formulated in a dedicated annotation language, called LANA. This language allows the grouping of rules into coherent blocks and to specify language signatures, types, pre- and postconditions, as well as unit tests for such blocks. While these annotations are invisible to an ASP solver, as they take the form of program comments, they can be interpreted by tools for documentation, testing, and verification purposes, as well as to eliminate sources of common programming errors by realising syntax checking or code completion features. To demonstrate its versatility, we introduce two such tools, viz. (i) ASPDOC, for generating an HTML documentation for a program based on the annotated information, and (ii) ASPUNIT, for running and monitoring unit tests on program blocks. LANA is also exploited in the SeaLion system, an integrated development environment for ASP based on Eclipse. To appear in Theory and Practice of Logic Programming\n    ",
        "submission_date": "2012-10-08T00:00:00",
        "last_modified_date": "2012-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.2421",
        "title": "Simulated Tom Thumb, the Rule Of Thumb for Autonomous Robots",
        "authors": [
            "M. A. El-Dosuky",
            "M. Z. Rashad",
            "T. T. Hamza",
            "A.H. EL-Bassiouny"
        ],
        "abstract": "For a mobile robot to be truly autonomous, it must solve the simultaneous localization and mapping (SLAM) problem. We develop a new metaheuristic algorithm called Simulated Tom Thumb (STT), based on the detailed adventure of the clever Tom Thumb and advances in researches relating to path planning based on potential functions. Investigations show that it is very promising and could be seen as an optimization of the powerful solution of SLAM with data association and learning capabilities. STT outperform JCBB. The performance is 100 % match.\n    ",
        "submission_date": "2012-10-08T00:00:00",
        "last_modified_date": "2012-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.2429",
        "title": "Mining Permission Request Patterns from Android and Facebook Applications (extended author version)",
        "authors": [
            "Mario Frank",
            "Ben Dong",
            "Adrienne Porter Felt",
            "Dawn Song"
        ],
        "abstract": "Android and Facebook provide third-party applications with access to users' private data and the ability to perform potentially sensitive operations (e.g., post to a user's wall or place phone calls). As a security measure, these platforms restrict applications' privileges with permission systems: users must approve the permissions requested by applications before the applications can make privacy- or security-relevant API calls. However, recent studies have shown that users often do not understand permission requests and lack a notion of typicality of requests. As a first step towards simplifying permission systems, we cluster a corpus of 188,389 Android applications and 27,029 Facebook applications to find patterns in permission requests. Using a method for Boolean matrix factorization for finding overlapping clusters, we find that Facebook permission requests follow a clear structure that exhibits high stability when fitted with only five clusters, whereas Android applications demonstrate more complex permission requests. We also find that low-reputation applications often deviate from the permission request patterns that we identified for high-reputation applications suggesting that permission request patterns are indicative for user satisfaction or application quality.\n    ",
        "submission_date": "2012-10-08T00:00:00",
        "last_modified_date": "2012-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.2629",
        "title": "Optimization in Differentiable Manifolds in Order to Determine the Method of Construction of Prehistoric Wall-Paintings",
        "authors": [
            "Dimitris Arabadjis",
            "Panayiotis Rousopoulos",
            "Constantin Papaodysseus",
            "Michalis Exarhos",
            "Michalis Panagopoulos",
            "Lena Papazoglou-Manioudaki"
        ],
        "abstract": "In this paper a general methodology is introduced for the determination of potential prototype curves used for the drawing of prehistoric wall-paintings. The approach includes a) preprocessing of the wall-paintings contours to properly partition them, according to their curvature, b) choice of prototype curves families, c) analysis and optimization in 4-manifold for a first estimation of the form of these prototypes, d) clustering of the contour parts and the prototypes, to determine a minimal number of potential guides, e) further optimization in 4-manifold, applied to each cluster separately, in order to determine the exact functional form of the potential guides, together with the corresponding drawn contour parts. The introduced methodology simultaneously deals with two problems: a) the arbitrariness in data-points orientation and b) the determination of one proper form for a prototype curve that optimally fits the corresponding contour data. Arbitrariness in orientation has been dealt with a novel curvature based error, while the proper forms of curve prototypes have been exhaustively determined by embedding curvature deformations of the prototypes into 4-manifolds. Application of this methodology to celebrated wall-paintings excavated at Tyrins, Greece and the Greek island of Thera, manifests it is highly probable that these wall-paintings had been drawn by means of geometric guides that correspond to linear spirals and hyperbolae. These geometric forms fit the drawings' lines with an exceptionally low average error, less than 0.39mm. Hence, the approach suggests the existence of accurate realizations of complicated geometric entities, more than 1000 years before their axiomatic formulation in Classical Ages.\n    ",
        "submission_date": "2012-10-09T00:00:00",
        "last_modified_date": "2012-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.2640",
        "title": "Multi-view constrained clustering with an incomplete mapping between views",
        "authors": [
            "Eric Eaton",
            "Marie desJardins",
            "Sara Jacob"
        ],
        "abstract": "Multi-view learning algorithms typically assume a complete bipartite mapping between the different views in order to exchange information during the learning process. However, many applications provide only a partial mapping between the views, creating a challenge for current methods. To address this problem, we propose a multi-view algorithm based on constrained clustering that can operate with an incomplete mapping. Given a set of pairwise constraints in each view, our approach propagates these constraints using a local similarity measure to those instances that can be mapped to the other views, allowing the propagated constraints to be transferred across views via the partial mapping. It uses co-EM to iteratively estimate the propagation within each view based on the current clustering model, transfer the constraints across views, and then update the clustering model. By alternating the learning process between views, this approach produces a unified clustering model that is consistent with all views. We show that this approach significantly improves clustering performance over several other methods for transferring constraints and allows multi-view clustering to be reliably applied when given a limited mapping between the views. Our evaluation reveals that the propagated constraints have high precision with respect to the true clusters in the data, explaining their benefit to clustering performance in both single- and multi-view learning scenarios.\n    ",
        "submission_date": "2012-10-09T00:00:00",
        "last_modified_date": "2012-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.2646",
        "title": "A General Methodology for the Determination of 2D Bodies Elastic Deformation Invariants. Application to the Automatic Identification of Parasites",
        "authors": [
            "Dimitris Arabadjis",
            "Panayiotis Rousopoulos",
            "Constantin Papaodysseus",
            "Michalis Panagopoulos",
            "Panayiota Loumou",
            "Georgios Theodoropoulos"
        ],
        "abstract": "A novel methodology is introduced here that exploits 2D images of arbitrary elastic body deformation instances, so as to quantify mechano-elastic characteristics that are deformation invariant. Determination of such characteristics allows for developing methods offering an image of the undeformed body. General assumptions about the mechano-elastic properties of the bodies are stated, which lead to two different approaches for obtaining bodies' deformation invariants. One was developed to spot deformed body's neutral line and its cross sections, while the other solves deformation PDEs by performing a set of equivalent image operations on the deformed body images. Both these processes may furnish a body undeformed version from its deformed image. This was confirmed by obtaining the undeformed shape of deformed parasites, cells (protozoa), fibers and human lips. In addition, the method has been applied to the important problem of parasite automatic classification from their microscopic images. To achieve this, we first apply the previous method to straighten the highly deformed parasites and then we apply a dedicated curve classification method to the straightened parasite contours. It is demonstrated that essentially different deformations of the same parasite give rise to practically the same undeformed shape, thus confirming the consistency of the introduced methodology. Finally, the developed pattern recognition method classifies the unwrapped parasites into 6 families, with an accuracy rate of 97.6 %.\n    ",
        "submission_date": "2012-10-09T00:00:00",
        "last_modified_date": "2012-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.3265",
        "title": "Multi-threaded ASP Solving with clasp",
        "authors": [
            "Martin Gebser",
            "Benjamin Kaufmann",
            "Torsten Schaub"
        ],
        "abstract": "We present the new multi-threaded version of the state-of-the-art answer set solver clasp. We detail its component and communication architecture and illustrate how they support the principal functionalities of clasp. Also, we provide some insights into the data representation used for different constraint types handled by clasp. All this is accompanied by an extensive experimental analysis of the major features related to multi-threading in clasp.\n    ",
        "submission_date": "2012-10-11T00:00:00",
        "last_modified_date": "2012-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.3312",
        "title": "Artex is AnotheR TEXt summarizer",
        "authors": [
            "Juan-Manuel Torres-Moreno"
        ],
        "abstract": "This paper describes Artex, another algorithm for Automatic Text Summarization. In order to rank sentences, a simple inner product is calculated between each sentence, a document vector (text topic) and a lexical vector (vocabulary used by a sentence). Summaries are then generated by assembling the highest ranked sentences. No ruled-based linguistic post-processing is necessary in order to obtain summaries. Tests over several datasets (coming from Document Understanding Conferences (DUC), Text Analysis Conferences (TAC), evaluation campaigns, etc.) in French, English and Spanish have shown that summarizer achieves interesting results.\n    ",
        "submission_date": "2012-10-11T00:00:00",
        "last_modified_date": "2012-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.3587",
        "title": "Inferring the Underlying Structure of Information Cascades",
        "authors": [
            "Bo Zong",
            "Yinghui Wu",
            "Ambuj K. Singh",
            "Xifeng Yan"
        ],
        "abstract": "In social networks, information and influence diffuse among users as cascades. While the importance of studying cascades has been recognized in various applications, it is difficult to observe the complete structure of cascades in practice. Moreover, much less is known on how to infer cascades based on partial observations. In this paper we study the cascade inference problem following the independent cascade model, and provide a full treatment from complexity to algorithms: (a) We propose the idea of consistent trees as the inferred structures for cascades; these trees connect source nodes and observed nodes with paths satisfying the constraints from the observed temporal information. (b) We introduce metrics to measure the likelihood of consistent trees as inferred cascades, as well as several optimization problems for finding them. (c) We show that the decision problems for consistent trees are in general NP-complete, and that the optimization problems are hard to approximate. (d) We provide approximation algorithms with performance guarantees on the quality of the inferred cascades, as well as heuristics. We experimentally verify the efficiency and effectiveness of our inference algorithms, using real and synthetic data.\n    ",
        "submission_date": "2012-10-12T00:00:00",
        "last_modified_date": "2012-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.3634",
        "title": "Quick Summary",
        "authors": [
            "Robert Wahlstedt"
        ],
        "abstract": "Quick Summary is an innovate implementation of an automatic document summarizer that inputs a document in the English language and evaluates each sentence. The scanner or evaluator determines criteria based on its grammatical structure and place in the paragraph. The program then asks the user to specify the number of sentences the person wishes to highlight. For example should the user ask to have three of the most important sentences, it would highlight the first and most important sentence in green. Commonly this is the sentence containing the conclusion. Then Quick Summary finds the second most important sentence usually called a satellite and highlights it in yellow. This is usually the topic sentence. Then the program finds the third most important sentence and highlights it in red. The implementations of this technology are useful in a society of information overload when a person typically receives 42 emails a day (Microsoft). The paper also is a candid look at difficulty that machine learning has in textural translating. However, it speaks on how to overcome the obstacles that historically prevented progress. This paper proposes mathematical meta-data criteria that justify the place of importance of a sentence. Just as tools for the study of relational symmetry in bio-informatics, this tool seeks to classify words with greater clarity. \"Survey Finds Workers Average Only Three Productive Days per Week.\" Microsoft News Center. Microsoft. Web. 31 Mar. 2012.\n    ",
        "submission_date": "2012-10-12T00:00:00",
        "last_modified_date": "2012-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.3865",
        "title": "Opinion Mining for Relating Subjective Expressions and Annual Earnings in US Financial Statements",
        "authors": [
            "Chien-Liang Chen",
            "Chao-Lin Liu",
            "Yuan-Chen Chang",
            "Hsiang-Ping Tsai"
        ],
        "abstract": "Financial statements contain quantitative information and manager's subjective evaluation of firm's financial status. Using information released in U.S. 10-K filings. Both qualitative and quantitative appraisals are crucial for quality financial decisions. To extract such opinioned statements from the reports, we built tagging models based on the conditional random field (CRF) techniques, considering a variety of combinations of linguistic factors including morphology, orthography, predicate-argument structure, syntax, and simple semantics. Our results show that the CRF models are reasonably effective to find opinion holders in experiments when we adopted the popular MPQA corpus for training and testing. The contribution of our paper is to identify opinion patterns in multiword expressions (MWEs) forms rather than in single word forms.\n",
        "submission_date": "2012-10-15T00:00:00",
        "last_modified_date": "2012-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.3937",
        "title": "Introduction to the 28th International Conference on Logic Programming Special Issue",
        "authors": [
            "Agostino Dovier",
            "V\u00edtor Santos Costa"
        ],
        "abstract": "We are proud to introduce this special issue of the Journal of Theory and Practice of Logic Programming (TPLP), dedicated to the full papers accepted for the 28th International Conference on Logic Programming (ICLP). The ICLP meetings started in Marseille in 1982 and since then constitute the main venue for presenting and discussing work in the area of logic programming.\n    ",
        "submission_date": "2012-10-15T00:00:00",
        "last_modified_date": "2012-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4130",
        "title": "Relational Theories with Null Values and Non-Herbrand Stable Models",
        "authors": [
            "Vladimir Lifschitz",
            "Karl Pichotta",
            "Fangkai Yang"
        ],
        "abstract": "Generalized relational theories with null values in the sense of Reiter are first-order theories that provide a semantics for relational databases with incomplete information. In this paper we show that any such theory can be turned into an equivalent logic program, so that models of the theory can be generated using computational methods of answer set programming. As a step towards this goal, we develop a general method for calculating stable models under the domain closure assumption but without the unique name assumption.\n    ",
        "submission_date": "2012-10-15T00:00:00",
        "last_modified_date": "2012-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4184",
        "title": "The Kernel Pitman-Yor Process",
        "authors": [
            "Sotirios P. Chatzis",
            "Dimitrios Korkinof",
            "Yiannis Demiris"
        ],
        "abstract": "In this work, we propose the kernel Pitman-Yor process (KPYP) for nonparametric clustering of data with general spatial or temporal interdependencies. The KPYP is constructed by first introducing an infinite sequence of random locations. Then, based on the stick-breaking construction of the Pitman-Yor process, we define a predictor-dependent random probability measure by considering that the discount hyperparameters of the Beta-distributed random weights (stick variables) of the process are not uniform among the weights, but controlled by a kernel function expressing the proximity between the location assigned to each weight and the given predictors.\n    ",
        "submission_date": "2012-10-15T00:00:00",
        "last_modified_date": "2012-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4231",
        "title": "An example illustrating the imprecision of the efficient approach for diagnosis of Petri nets via integer linear programming",
        "authors": [
            "Alban Grastien"
        ],
        "abstract": "This document demonstrates that the efficient approach for diagnosis of Petri nets via integer linear programming may be unable to detect a fault even if the system is diagnosable.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4849",
        "title": "Toward Large-Scale Agent Guidance in an Urban Taxi Service",
        "authors": [
            "Lucas Agussurja",
            "Hoong Chuin Lau"
        ],
        "abstract": "Empty taxi cruising represents a wastage of resources in the context of urban taxi services. In this work, we seek to minimize such wastage. An analysis of a large trace of taxi operations reveals that the services' inefficiency is caused by drivers' greedy cruising behavior. We model the existing system as a continuous time Markov chain. To address the problem, we propose that each taxi be equipped with an intelligent agent that will guide the driver when cruising for passengers. Then, drawing from AI literature on multiagent planning, we explore two possible ways to compute such guidance. The first formulation assumes fully cooperative drivers. This allows us, in principle, to compute systemwide optimal cruising policy. This is modeled as a Markov decision process. The second formulation assumes rational drivers, seeking to maximize their own profit. This is modeled as a stochastic congestion game, a specialization of stochastic games. Nash equilibrium policy is proposed as the solution to the game, where no driver has the incentive to singly deviate from it. Empirical result shows that both formulations improve the efficiency of the service significantly.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4853",
        "title": "Weighted Sets of Probabilities and MinimaxWeighted Expected Regret: New Approaches for Representing Uncertainty and Making Decisions",
        "authors": [
            "Joseph Y. Halpern",
            "Samantha Leung"
        ],
        "abstract": "We consider a setting where an agent's uncertainty is represented by a set of probability measures, rather than a single measure. Measure-bymeasure updating of such a set of measures upon acquiring new information is well-known to suffer from problems; agents are not always able to learn appropriately. To deal with these problems, we propose using weighted sets of probabilities: a representation where each measure is associated with a weight, which denotes its significance. We describe a natural approach to updating in such a situation and a natural approach to determining the weights. We then show how this representation can be used in decision-making, by modifying a standard approach to decision making-minimizing expected regret-to obtain minimax weighted expected regret (MWER).We provide an axiomatization that characterizes preferences induced by MWER both in the static and dynamic case.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4854",
        "title": "Semantic Understanding of Professional Soccer Commentaries",
        "authors": [
            "Hannaneh Hajishirzi",
            "Mohammad Rastegari",
            "Ali Farhadi",
            "Jessica K. Hodgins"
        ],
        "abstract": "This paper presents a novel approach to the problem of semantic parsing via learning the correspondences between complex sentences and rich sets of events. Our main intuition is that correct correspondences tend to occur more frequently. Our model benefits from a discriminative notion of similarity to learn the correspondence between sentence and an event and a ranking machinery that scores the popularity of each correspondence. Our method can discover a group of events (called macro-events) that best describes a sentence. We evaluate our method on our novel dataset of professional soccer commentaries. The empirical results show that our method significantly outperforms the state-of-theart.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4879",
        "title": "Causal Discovery of Linear Cyclic Models from Multiple Experimental Data Sets with Overlapping Variables",
        "authors": [
            "Antti Hyttinen",
            "Frederick Eberhardt",
            "Patrik O. Hoyer"
        ],
        "abstract": "Much of scientific data is collected as randomized experiments intervening on some and observing other variables of interest. Quite often, a given phenomenon is investigated in several studies, and different sets of variables are involved in each study. In this article we consider the problem of integrating such knowledge, inferring as much as possible concerning the underlying causal structure with respect to the union of observed variables from such experimental or passive observational overlapping data sets. We do not assume acyclicity or joint causal sufficiency of the underlying data generating model, but we do restrict the causal relationships to be linear and use only second order statistics of the data. We derive conditions for full model identifiability in the most generic case, and provide novel techniques for incorporating an assumption of faithfulness to aid in inference. In each case we seek to establish what is and what is not determined by the data at hand.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4886",
        "title": "Exploiting Structure in Cooperative Bayesian Games",
        "authors": [
            "Frans A. Oliehoek",
            "Shimon Whiteson",
            "Matthijs T. J. Spaan"
        ],
        "abstract": "Cooperative Bayesian games (BGs) can model decision-making problems for teams of agents under imperfect information, but require space and computation time that is exponential in the number of agents. While agent independence has been used to mitigate these problems in perfect information settings, we propose a novel approach for BGs based on the observation that BGs additionally possess a different types of structure, which we call type independence. We propose a factor graph representation that captures both forms of independence and present a theoretical analysis showing that non-serial dynamic programming cannot effectively exploit type independence, while Max-Sum can. Experimental results demonstrate that our approach can tackle cooperative Bayesian games of unprecedented size.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4887",
        "title": "Hilbert Space Embeddings of POMDPs",
        "authors": [
            "Yu Nishiyama",
            "Abdeslam Boularias",
            "Arthur Gretton",
            "Kenji Fukumizu"
        ],
        "abstract": "A nonparametric approach for policy learning for POMDPs is proposed. The approach represents distributions over the states, observations, and actions as embeddings in feature spaces, which are reproducing kernel Hilbert spaces. Distributions over states given the observations are obtained by applying the kernel Bayes' rule to these distribution embeddings. Policies and value functions are defined on the feature space over states, which leads to a feature space expression for the Bellman equation. Value iteration may then be used to estimate the optimal value function and associated policy. Experimental results confirm that the correct policy is learned using the feature space representation.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4888",
        "title": "Local Structure Discovery in Bayesian Networks",
        "authors": [
            "Teppo Niinimaki",
            "Pekka Parviainen"
        ],
        "abstract": "Learning a Bayesian network structure from data is an NP-hard problem and thus exact algorithms are feasible only for small data sets. Therefore, network structures for larger networks are usually learned with various heuristics. Another approach to scaling up the structure learning is local learning. In local learning, the modeler has one or more target variables that are of special interest; he wants to learn the structure near the target variables and is not interested in the rest of the variables. In this paper, we present a score-based local learning algorithm called SLL. We conjecture that our algorithm is theoretically sound in the sense that it is optimal in the limit of large sample size. Empirical results suggest that SLL is competitive when compared to the constraint-based HITON algorithm. We also study the prospects of constructing the network structure for the whole node set based on local results by presenting two algorithms and comparing them to several heuristics.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4889",
        "title": "Learning STRIPS Operators from Noisy and Incomplete Observations",
        "authors": [
            "Kira Mourao",
            "Luke S. Zettlemoyer",
            "Ronald P. A. Petrick",
            "Mark Steedman"
        ],
        "abstract": "Agents learning to act autonomously in real-world domains must acquire a model of the dynamics of the domain in which they operate. Learning domain dynamics can be challenging, especially where an agent only has partial access to the world state, and/or noisy external sensors. Even in standard STRIPS domains, existing approaches cannot learn from noisy, incomplete observations typical of real-world domains. We propose a method which learns STRIPS action models in such domains, by decomposing the problem into first learning a transition function between states in the form of a set of classifiers, and then deriving explicit STRIPS rules from the classifiers' parameters. We evaluate our approach on simulated standard planning domains from the International Planning Competition, and show that it learns useful domain descriptions from noisy, incomplete observations.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4896",
        "title": "Closed-Form Learning of Markov Networks from Dependency Networks",
        "authors": [
            "Daniel Lowd"
        ],
        "abstract": "Markov networks (MNs) are a powerful way to compactly represent a joint probability distribution, but most MN structure learning methods are very slow, due to the high cost of evaluating candidates structures. Dependency networks (DNs) represent a probability distribution as a set of conditional probability distributions. DNs are very fast to learn, but the conditional distributions may be inconsistent with each other and few inference algorithms support DNs. In this paper, we present a closed-form method for converting a DN into an MN, allowing us to enjoy both the efficiency of DN learning and the convenience of the MN representation. When the DN is consistent, this conversion is exact. For inconsistent DNs, we present averaging methods that significantly improve the approximation. In experiments on 12 standard datasets, our methods are orders of magnitude faster than and often more accurate than combining conditional distributions using weight learning.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4901",
        "title": "An Approximate Solution Method for Large Risk-Averse Markov Decision Processes",
        "authors": [
            "Marek Petrik",
            "Dharmashankar Subramanian"
        ],
        "abstract": "Stochastic domains often involve risk-averse decision makers. While recent work has focused on how to model risk in Markov decision processes using risk measures, it has not addressed the problem of solving large risk-averse formulations. In this paper, we propose and analyze a new method for solving large risk-averse MDPs with hybrid continuous-discrete state spaces and continuous action spaces. The proposed method iteratively improves a bound on the value function using a linearity structure of the MDP. We demonstrate the utility and properties of the method on a portfolio optimization problem.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4918",
        "title": "Dynamic Teaching in Sequential Decision Making Environments",
        "authors": [
            "Thomas J. Walsh",
            "Sergiu Goschin"
        ],
        "abstract": "We describe theoretical bounds and a practical algorithm for teaching a model by demonstration in a sequential decision making environment. Unlike previous efforts that have optimized learners that watch a teacher demonstrate a static policy, we focus on the teacher as a decision maker who can dynamically choose different policies to teach different parts of the environment. We develop several teaching frameworks based on previously defined supervised protocols, such as Teaching Dimension, extending them to handle noise and sequences of inputs encountered in an ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.5118",
        "title": "Creating a level playing field for all symbols in a discretization",
        "authors": [
            "Matthew Butler",
            "Dimitar Kazakov"
        ],
        "abstract": "In time series analysis research there is a strong interest in discrete representations of real valued data streams. One approach that emerged over a decade ago and is still considered state-of-the-art is the Symbolic Aggregate Approximation algorithm. This discretization algorithm was the first symbolic approach that mapped a real-valued time series to a symbolic representation that was guaranteed to lower-bound Euclidean distance. The interest of this paper concerns the SAX assumption of data being highly Gaussian and the use of the standard normal curve to choose partitions to discretize the data. Though not necessarily, but generally, and certainly in its canonical form, the SAX approach chooses partitions on the standard normal curve that would produce an equal probability for each symbol in a finite alphabet to occur. This procedure is generally valid as a time series is normalized before the rest of the SAX algorithm is applied. However there exists a caveat to this assumption of equi-probability due to the intermediate step of Piecewise Aggregate Approximation (PAA). What we will show in this paper is that when PAA is applied the distribution of the data is indeed altered, resulting in a shrinking standard deviation that is proportional to the number of points used to create a segment of the PAA representation and the degree of auto-correlation within the series. Data that exhibits statistically significant auto-correlation is less affected by this shrinking distribution. As the standard deviation of the data contracts, the mean remains the same, however the distribution is no longer standard normal and therefore the partitions based on the standard normal curve are no longer valid for the assumption of equal probability.\n    ",
        "submission_date": "2012-10-18T00:00:00",
        "last_modified_date": "2012-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.5560",
        "title": "Wikipedia Vandalism Detection Through Machine Learning: Feature Review and New Proposals: Lab Report for PAN at CLEF 2010",
        "authors": [
            "Santiago M. Mola-Velasco"
        ],
        "abstract": "Wikipedia is an online encyclopedia that anyone can edit. In this open model, some people edits with the intent of harming the integrity of Wikipedia. This is known as vandalism. We extend the framework presented in (Potthast, Stein, and Gerling, 2008) for Wikipedia vandalism detection. In this approach, several vandalism indicating features are extracted from edits in a vandalism corpus and are fed to a supervised learning algorithm. The best performing classifiers were LogitBoost and Random Forest. Our classifier, a Random Forest, obtained an AUC of 0.92236, ranking in the first place of the PAN'10 Wikipedia vandalism detection task.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.5644",
        "title": "Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials",
        "authors": [
            "Philipp Kr\u00e4henb\u00fchl",
            "Vladlen Koltun"
        ],
        "abstract": "Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While region-level models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.\n    ",
        "submission_date": "2012-10-20T00:00:00",
        "last_modified_date": "2012-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.6539",
        "title": "Towards Swarm Calculus: Urn Models of Collective Decisions and Universal Properties of Swarm Performance",
        "authors": [
            "Heiko Hamann"
        ],
        "abstract": "Methods of general applicability are searched for in swarm intelligence with the aim of gaining new insights about natural swarms and to develop design methodologies for artificial swarms. An ideal solution could be a `swarm calculus' that allows to calculate key features of swarms such as expected swarm performance and robustness based on only a few parameters. To work towards this ideal, one needs to find methods and models with high degrees of generality. In this paper, we report two models that might be examples of exceptional generality. First, an abstract model is presented that describes swarm performance depending on swarm density based on the dichotomy between cooperation and interference. Typical swarm experiments are given as examples to show how the model fits to several different results. Second, we give an abstract model of collective decision making that is inspired by urn models. The effects of positive feedback probability, that is increasing over time in a decision making system, are understood by the help of a parameter that controls the feedback based on the swarm's current consensus. Several applicable methods, such as the description as Markov process, calculation of splitting probabilities, mean first passage times, and measurements of positive feedback, are discussed and applications to artificial and natural swarms are reported.\n    ",
        "submission_date": "2012-10-24T00:00:00",
        "last_modified_date": "2013-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.7038",
        "title": "Full Object Boundary Detection by Applying Scale Invariant Features in a Region Merging Segmentation Algorithm",
        "authors": [
            "Reza Oji",
            "Farshad Tajeripour"
        ],
        "abstract": "Object detection is a fundamental task in computer vision and has many applications in image processing. This paper proposes a new approach for object detection by applying scale invariant feature transform (SIFT) in an automatic segmentation algorithm. SIFT is an invariant algorithm respect to scale, translation and rotation. The features are very distinct and provide stable keypoints that can be used for matching an object in different images. At first, an object is trained with different aspects for finding best keypoints. The object can be recognized in the other images by using achieved keypoints. Then, a robust segmentation algorithm is used to detect the object with full boundary based on SIFT keypoints. In segmentation algorithm, a merging role is defined to merge the regions in image with the assistance of keypoints. The results show that the proposed approach is reliable for object detection and can extract object boundary well.\n    ",
        "submission_date": "2012-10-26T00:00:00",
        "last_modified_date": "2012-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.7053",
        "title": "Managing sparsity, time, and quality of inference in topic models",
        "authors": [
            "Khoat Than",
            "Tu Bao Ho"
        ],
        "abstract": "Inference is an integral part of probabilistic topic models, but is often non-trivial to derive an efficient algorithm for a specific model. It is even much more challenging when we want to find a fast inference algorithm which always yields sparse latent representations of documents. In this article, we introduce a simple framework for inference in probabilistic topic models, denoted by FW. This framework is general and flexible enough to be easily adapted to mixture models. It has a linear convergence rate, offers an easy way to incorporate prior knowledge, and provides us an easy way to directly trade off sparsity against quality and time. We demonstrate the goodness and flexibility of FW over existing inference methods by a number of tasks. Finally, we show how inference in topic models with nonconjugate priors can be done efficiently.\n    ",
        "submission_date": "2012-10-26T00:00:00",
        "last_modified_date": "2013-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.7495",
        "title": "Illustrating a neural model of logic computations: The case of Sherlock Holmes' old maxim",
        "authors": [
            "Eduardo Mizraji"
        ],
        "abstract": "Natural languages can express some logical propositions that humans are able to understand. We illustrate this fact with a famous text that Conan Doyle attributed to Holmes: 'It is an old maxim of mine that when you have excluded the impossible, whatever remains, however improbable, must be the truth'. This is a subtle logical statement usually felt as an evident truth. The problem we are trying to solve is the cognitive reason for such a feeling. We postulate here that we accept Holmes' maxim as true because our adult brains are equipped with neural modules that naturally perform modal logical computations.\n    ",
        "submission_date": "2012-10-28T00:00:00",
        "last_modified_date": "2016-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.7599",
        "title": "The automatic creation of concept maps from documents written using morphologically rich languages",
        "authors": [
            "Krunoslav Zubrinic",
            "Damir Kalpic",
            "Mario Milicevic"
        ],
        "abstract": "Concept map is a graphical tool for representing knowledge. They have been used in many different areas, including education, knowledge management, business and intelligence. Constructing of concept maps manually can be a complex task; an unskilled person may encounter difficulties in determining and positioning concepts relevant to the problem area. An application that recommends concept candidates and their position in a concept map can significantly help the user in that situation. This paper gives an overview of different approaches to automatic and semi-automatic creation of concept maps from textual and non-textual sources. The concept map mining process is defined, and one method suitable for the creation of concept maps from unstructured textual sources in highly inflected languages such as the Croatian language is described in detail. Proposed method uses statistical and data mining techniques enriched with linguistic tools. With minor adjustments, that method can also be used for concept map mining from textual sources in other morphologically rich languages.\n    ",
        "submission_date": "2012-10-29T00:00:00",
        "last_modified_date": "2014-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.8099",
        "title": "An Atypical Survey of Typical-Case Heuristic Algorithms",
        "authors": [
            "Lane A. Hemaspaandra",
            "Ryan Williams"
        ],
        "abstract": "Heuristic approaches often do so well that they seem to pretty much always give the right answer. How close can heuristic algorithms get to always giving the right answer, without inducing seismic complexity-theoretic consequences? This article first discusses how a series of results by Berman, Buhrman, Hartmanis, Homer, Longpr\u00e9, Ogiwara, Sch\u00f6ening, and Watanabe, from the early 1970s through the early 1990s, explicitly or implicitly limited how well heuristic algorithms can do on NP-hard problems. In particular, many desirable levels of heuristic success cannot be obtained unless severe, highly unlikely complexity class collapses occur. Second, we survey work initiated by Goldreich and Wigderson, who showed how under plausible assumptions deterministic heuristics for randomized computation can achieve a very high frequency of correctness. Finally, we consider formal ways in which theory can help explain the effectiveness of heuristics that solve NP-hard problems in practice.\n    ",
        "submission_date": "2012-10-30T00:00:00",
        "last_modified_date": "2012-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.8124",
        "title": "Hierarchical Learning Algorithm for the Beta Basis Function Neural Network",
        "authors": [
            "Habib Dhahri",
            "Mohamed Adel Alimi"
        ],
        "abstract": "The paper presents a two-level learning method for the design of the Beta Basis Function Neural Network BBFNN. A Genetic Algorithm is employed at the upper level to construct BBFNN, while the key learning parameters :the width, the centers and the Beta form are optimised using the gradient algorithm at the lower level. In order to demonstrate the effectiveness of this hierarchical learning algorithm HLABBFNN, we need to validate our algorithm for the approximation of non-linear function.\n    ",
        "submission_date": "2012-10-30T00:00:00",
        "last_modified_date": "2012-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.8260",
        "title": "Mean Field Theory of Dynamical Systems Driven by External Signals",
        "authors": [
            "Marc Massar",
            "Serge Massar"
        ],
        "abstract": "Dynamical systems driven by strong external signals are ubiquituous in nature and engineering. Here we study \"echo state networks\", networks of a large number of randomly connected nodes, which represent a simple model of a neural network, and have important applications in machine learning. We develop a mean field theory of echo state networks. The dynamics of the network is captured by the evolution law, similar to a logistic map, for a single collective variable. When the network is driven by many independent external signals, this collective variable reaches a steady state. But when the network is driven by a single external signal, the collective variable is nonstationnary but can be characterised by its time averaged distribution. The predictions of the mean field theory, including the value of the largest Lyaponuov exponent, are compared with the numerical integration of the equations of motion.\n    ",
        "submission_date": "2012-10-31T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.8291",
        "title": "Learning in the Model Space for Fault Diagnosis",
        "authors": [
            "Huanhuan Chen",
            "Peter Tino",
            "Xin Yao",
            "Ali Rodan"
        ],
        "abstract": "The emergence of large scaled sensor networks facilitates the collection of large amounts of real-time data to monitor and control complex engineering systems. However, in many cases the collected data may be incomplete or inconsistent, while the underlying environment may be time-varying or un-formulated. In this paper, we have developed an innovative cognitive fault diagnosis framework that tackles the above challenges. This framework investigates fault diagnosis in the model space instead of in the signal space. Learning in the model space is implemented by fitting a series of models using a series of signal segments selected with a rolling window. By investigating the learning techniques in the fitted model space, faulty models can be discriminated from healthy models using one-class learning algorithm. The framework enables us to construct fault library when unknown faults occur, which can be regarded as cognitive fault isolation. This paper also theoretically investigates how to measure the pairwise distance between two models in the model space and incorporates the model distance into the learning algorithm in the model space. The results on three benchmark applications and one simulated model for the Barcelona water distribution network have confirmed the effectiveness of the proposed framework.\n    ",
        "submission_date": "2012-10-31T00:00:00",
        "last_modified_date": "2012-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.8353",
        "title": "Temporal Autoencoding Restricted Boltzmann Machine",
        "authors": [
            "Chris H\u00e4usler",
            "Alex Susemihl"
        ],
        "abstract": "Much work has been done refining and characterizing the receptive fields learned by deep learning algorithms. A lot of this work has focused on the development of Gabor-like filters learned when enforcing sparsity constraints on a natural image dataset. Little work however has investigated how these filters might expand to the temporal domain, namely through training on natural movies. Here we investigate exactly this problem in established temporal deep learning algorithms as well as a new learning paradigm suggested here, the Temporal Autoencoding Restricted Boltzmann Machine (TARBM).\n    ",
        "submission_date": "2012-10-31T00:00:00",
        "last_modified_date": "2012-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.0418",
        "title": "Verbalizing Ontologies in Controlled Baltic Languages",
        "authors": [
            "Normunds Gr\u016bz\u012btis",
            "Gunta Ne\u0161pore",
            "Baiba Saul\u012bte"
        ],
        "abstract": "Controlled natural languages (mostly English-based) recently have emerged as seemingly informal supplementary means for OWL ontology authoring, if compared to the formal notations that are used by professional knowledge engineers. In this paper we present by examples controlled Latvian language that has been designed to be compliant with the state of the art Attempto Controlled English. We also discuss relation with controlled Lithuanian language that is being designed in parallel.\n    ",
        "submission_date": "2012-11-02T00:00:00",
        "last_modified_date": "2012-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.0424",
        "title": "Learning classifier systems with memory condition to solve non-Markov problems",
        "authors": [
            "Zhaoxiang Zang",
            "Dehua Li",
            "Junying Wang"
        ],
        "abstract": "In the family of Learning Classifier Systems, the classifier system XCS has been successfully used for many applications. However, the standard XCS has no memory mechanism and can only learn optimal policy in Markov environments, where the optimal action is determined solely by the state of current sensory input. In practice, most environments are partially observable environments on agent's sensation, which are also known as non-Markov environments. Within these environments, XCS either fails, or only develops a suboptimal policy, since it has no memory. In this work, we develop a new classifier system based on XCS to tackle this problem. It adds an internal message list to XCS as the memory list to record input sensation history, and extends a small number of classifiers with memory conditions. The classifier's memory condition, as a foothold to disambiguate non-Markov states, is used to sense a specified element in the memory list. Besides, a detection method is employed to recognize non-Markov states in environments, to avoid these states controlling over classifiers' memory conditions. Furthermore, four sets of different complex maze environments have been tested by the proposed method. Experimental results show that our system is one of the best techniques to solve partially observable environments, compared with some well-known classifier systems proposed for these environments.\n    ",
        "submission_date": "2012-11-02T00:00:00",
        "last_modified_date": "2012-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.0479",
        "title": "Parameterized Complexity and Kernel Bounds for Hard Planning Problems",
        "authors": [
            "Christer B\u00e4ckstr\u00f6m",
            "Peter Jonsson",
            "Sebastian Ordyniak",
            "Stefan Szeider"
        ],
        "abstract": "The propositional planning problem is a notoriously difficult computational problem. Downey et al. (1999) initiated the parameterized analysis of planning (with plan length as the parameter) and B\u00e4ckstr\u00f6m et al. (2012) picked up this line of research and provided an extensive parameterized analysis under various restrictions, leaving open only one stubborn case. We continue this work and provide a full classification. In particular, we show that the case when actions have no preconditions and at most $e$ postconditions is fixed-parameter tractable if $e\\leq 2$ and W[1]-complete otherwise. We show fixed-parameter tractability by a reduction to a variant of the Steiner Tree problem; this problem has been shown fixed-parameter tractable by Guo et al. (2007). If a problem is fixed-parameter tractable, then it admits a polynomial-time self-reduction to instances whose input size is bounded by a function of the parameter, called the kernel. For some problems, this function is even polynomial which has desirable computational implications. Recent research in parameterized complexity has focused on classifying fixed-parameter tractable problems on whether they admit polynomial kernels or not. We revisit all the previously obtained restrictions of planning that are fixed-parameter tractable and show that none of them admits a polynomial kernel unless the polynomial hierarchy collapses to its third level.\n    ",
        "submission_date": "2012-11-02T00:00:00",
        "last_modified_date": "2013-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.0501",
        "title": "Surprisingly Rational: Probability theory plus noise explains biases in judgment",
        "authors": [
            "Fintan Costello",
            "Paul Watts"
        ],
        "abstract": "The systematic biases seen in people's probability judgments are typically taken as evidence that people do not reason about probability using the rules of probability theory, but instead use heuristics which sometimes yield reasonable judgments and sometimes systematic biases. This view has had a major impact in economics, law, medicine, and other fields; indeed, the idea that people cannot reason with probabilities has become a widespread truism. We present a simple alternative to this view, where people reason about probability according to probability theory but are subject to random variation or noise in the reasoning process. In this account the effect of noise is cancelled for some probabilistic expressions: analysing data from two experiments we find that, for these expressions, people's probability judgments are strikingly close to those required by probability theory. For other expressions this account produces systematic deviations in probability estimates. These deviations explain four reliable biases in human probabilistic reasoning (conservatism, subadditivity, conjunction and disjunction fallacies). These results suggest that people's probability judgments embody the rules of probability theory, and that biases in those judgments are due to the effects of random noise.\n    ",
        "submission_date": "2012-11-01T00:00:00",
        "last_modified_date": "2014-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.0996",
        "title": "Learning using Local Membership Queries",
        "authors": [
            "Pranjal Awasthi",
            "Vitaly Feldman",
            "Varun Kanade"
        ],
        "abstract": "We introduce a new model of membership query (MQ) learning, where the learning algorithm is restricted to query points that are \\emph{close} to random examples drawn from the underlying distribution. The learning model is intermediate between the PAC model (Valiant, 1984) and the PAC+MQ model (where the queries are allowed to be arbitrary points).\n",
        "submission_date": "2012-11-05T00:00:00",
        "last_modified_date": "2013-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.2041",
        "title": "MaTrust: An Effective Multi-Aspect Trust Inference Model",
        "authors": [
            "Yuan Yao",
            "Hanghang Tong",
            "Xifeng Yan",
            "Feng Xu",
            "Jian Lu"
        ],
        "abstract": "Trust is a fundamental concept in many real-world applications such as e-commerce and peer-to-peer networks. In these applications, users can generate local opinions about the counterparts based on direct experiences, and these opinions can then be aggregated to build trust among unknown users. The mechanism to build new trust relationships based on existing ones is referred to as trust inference. State-of-the-art trust inference approaches employ the transitivity property of trust by propagating trust along connected users. In this paper, we propose a novel trust inference model (MaTrust) by exploring an equally important property of trust, i.e., the multi-aspect property. MaTrust directly characterizes multiple latent factors for each trustor and trustee from the locally-generated trust relationships. Furthermore, it can naturally incorporate prior knowledge as specified factors. These factors in turn serve as the basis to infer the unseen trustworthiness scores. Experimental evaluations on real data sets show that the proposed MaTrust significantly outperforms several benchmark trust inference models in both effectiveness and efficiency.\n    ",
        "submission_date": "2012-11-09T00:00:00",
        "last_modified_date": "2012-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.2087",
        "title": "Secured Wireless Communication using Fuzzy Logic based High Speed Public-Key Cryptography (FLHSPKC)",
        "authors": [
            "Arindam Sarkar",
            "J. K. Mandal"
        ],
        "abstract": "In this paper secured wireless communication using fuzzy logic based high speed public key cryptography (FLHSPKC) has been proposed by satisfying the major issues likes computational safety, power management and restricted usage of memory in wireless communication. Wireless Sensor Network (WSN) has several major constraints likes inadequate source of energy, restricted computational potentiality and limited memory. Though conventional Elliptic Curve Cryptography (ECC) which is a sort of public key cryptography used in wireless communication provides equivalent level of security like other existing public key algorithm using smaller parameters than other but this traditional ECC does not take care of all these major limitations in WSN. In conventional ECC consider Elliptic curve point p, an arbitrary integer k and modulus m, ECC carry out scalar multiplication kP mod m, which takes about 80% of key computation time on WSN. In this paper proposed FLHSPKC scheme provides some novel strategy including novel soft computing based strategy to speed up scalar multiplication in conventional ECC and which in turn takes shorter computational time and also satisfies power consumption restraint, limited usage of memory without hampering the security level. Performance analysis of the different strategies under FLHSPKC scheme and comparison study with existing conventional ECC methods has been done.\n    ",
        "submission_date": "2012-11-09T00:00:00",
        "last_modified_date": "2012-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.2245",
        "title": "Composite Strategy for Multicriteria Ranking/Sorting (methodological issues, examples)",
        "authors": [
            "Mark Sh. Levin"
        ],
        "abstract": "The paper addresses the modular design of composite solving strategies for multicriteria ranking (sorting). Here a 'scale of creativity' that is close to creative levels proposed by Altshuller is used as the reference viewpoint: (i) a basic object, (ii) a selected object, (iii) a modified object, and (iv) a designed object (e.g., composition of object components). These levels maybe used in various parts of decision support systems (DSS) (e.g., information, operations, user). The paper focuses on the more creative above-mentioned level (i.e., composition or combinatorial synthesis) for the operational part (i.e., composite solving strategy). This is important for a search/exploration mode of decision making process with usage of various procedures and techniques and analysis/integration of obtained results. The paper describes methodological issues of decision technology and synthesis of composite strategy for multicriteria ranking. The synthesis of composite strategies is based on 'hierarchical morphological multicriteria design' (HMMD) which is based on selection and combination of design alternatives (DAs) (here: local procedures or techniques) while taking into account their quality and quality of their interconnections (IC). A new version of HMMD with interval multiset estimates for DAs is used. The operational environment of DSS COMBI for multicriteria ranking, consisting of a morphology of local procedures or techniques (as design alternatives DAs), is examined as a basic one.\n    ",
        "submission_date": "2012-11-09T00:00:00",
        "last_modified_date": "2012-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.2290",
        "title": "Dating Texts without Explicit Temporal Cues",
        "authors": [
            "Abhimanu Kumar",
            "Jason Baldridge",
            "Matthew Lease",
            "Joydeep Ghosh"
        ],
        "abstract": "This paper tackles temporal resolution of documents, such as determining when a document is about or when it was written, based only on its text. We apply techniques from information retrieval that predict dates via language models over a discretized timeline. Unlike most previous works, we rely {\\it solely} on temporal cues implicit in the text. We consider both document-likelihood and divergence based techniques and several smoothing methods for both of them. Our best model predicts the mid-point of individuals' lives with a median of 22 and mean error of 36 years for Wikipedia biographies from 3800 B.C. to the present day. We also show that this approach works well when training on such biographies and predicting dates both for non-biographical Wikipedia pages about specific years (500 B.C. to 2010 A.D.) and for publication dates of short stories (1798 to 2008). Together, our work shows that, even in absence of temporal extraction resources, it is possible to achieve remarkable temporal locality across a diverse set of texts.\n    ",
        "submission_date": "2012-11-10T00:00:00",
        "last_modified_date": "2012-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.2399",
        "title": "Mining Determinism in Human Strategic Behavior",
        "authors": [
            "Rustam Tagiew"
        ],
        "abstract": "This work lies in the fusion of experimental economics and data mining. It continues author's previous work on mining behaviour rules of human subjects from experimental data, where game-theoretic predictions partially fail to work. Game-theoretic predictions aka equilibria only tend to success with experienced subjects on specific games, what is rarely given. Apart from game theory, contemporary experimental economics offers a number of alternative models. In relevant literature, these models are always biased by psychological and near-psychological theories and are claimed to be proven by the data. This work introduces a data mining approach to the problem without using vast psychological background. Apart from determinism, no other biases are regarded. Two datasets from different human subject experiments are taken for evaluation. The first one is a repeated mixed strategy zero sum game and the second - repeated ultimatum game. As result, the way of mining deterministic regularities in human strategic behaviour is described and evaluated. As future work, the design of a new representation formalism is discussed.\n    ",
        "submission_date": "2012-11-11T00:00:00",
        "last_modified_date": "2012-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.3089",
        "title": "ET-LDA: Joint Topic Modeling for Aligning Events and their Twitter Feedback",
        "authors": [
            "Yuheng Hu",
            "Ajita John",
            "Fei Wang",
            "Subbarao Kambhampati"
        ],
        "abstract": "During broadcast events such as the Superbowl, the U.S. Presidential and Primary debates, etc., Twitter has become the de facto platform for crowds to share perspectives and commentaries about them. Given an event and an associated large-scale collection of tweets, there are two fundamental research problems that have been receiving increasing attention in recent years. One is to extract the topics covered by the event and the tweets; the other is to segment the event. So far these problems have been viewed separately and studied in isolation. In this work, we argue that these problems are in fact inter-dependent and should be addressed together. We develop a joint Bayesian model that performs topic modeling and event segmentation in one unified framework. We evaluate the proposed model both quantitatively and qualitatively on two large-scale tweet datasets associated with two events from different domains to show that it improves significantly over baseline models.\n    ",
        "submission_date": "2012-11-13T00:00:00",
        "last_modified_date": "2012-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.3212",
        "title": "Distributed Non-Stochastic Experts",
        "authors": [
            "Varun Kanade",
            "Zhenming Liu",
            "Bozidar Radunovic"
        ],
        "abstract": "We consider the online distributed non-stochastic experts problem, where the distributed system consists of one coordinator node that is connected to $k$ sites, and the sites are required to communicate with each other via the coordinator. At each time-step $t$, one of the $k$ site nodes has to pick an expert from the set ${1, ..., n}$, and the same site receives information about payoffs of all experts for that round. The goal of the distributed system is to minimize regret at time horizon $T$, while simultaneously keeping communication to a minimum.\n",
        "submission_date": "2012-11-14T00:00:00",
        "last_modified_date": "2012-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.3831",
        "title": "Objective Improvement in Information-Geometric Optimization",
        "authors": [
            "Youhei Akimoto",
            "Yann Ollivier"
        ],
        "abstract": "Information-Geometric Optimization (IGO) is a unified framework of stochastic algorithms for optimization problems. Given a family of probability distributions, IGO turns the original optimization problem into a new maximization problem on the parameter space of the probability distributions. IGO updates the parameter of the probability distribution along the natural gradient, taken with respect to the Fisher metric on the parameter manifold, aiming at maximizing an adaptive transform of the objective function. IGO recovers several known algorithms as particular instances: for the family of Bernoulli distributions IGO recovers PBIL, for the family of Gaussian distributions the pure rank-mu CMA-ES update is recovered, and for exponential families in expectation parametrization the cross-entropy/ML method is recovered. This article provides a theoretical justification for the IGO framework, by proving that any step size not greater than 1 guarantees monotone improvement over the course of optimization, in terms of q-quantile values of the objective function f. The range of admissible step sizes is independent of f and its domain. We extend the result to cover the case of different step sizes for blocks of the parameters in the IGO algorithm. Moreover, we prove that expected fitness improves over time when fitness-proportional selection is applied, in which case the RPP algorithm is recovered.\n    ",
        "submission_date": "2012-11-16T00:00:00",
        "last_modified_date": "2013-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.4488",
        "title": "A Rule-Based Approach For Aligning Japanese-Spanish Sentences From A Comparable Corpora",
        "authors": [
            "Jessica C. Ram\u00edrez",
            "Yuji Matsumoto"
        ],
        "abstract": "The performance of a Statistical Machine Translation System (SMT) system is proportionally directed to the quality and length of the parallel corpus it uses. However for some pair of languages there is a considerable lack of them. The long term goal is to construct a Japanese-Spanish parallel corpus to be used for SMT, whereas, there are a lack of useful Japanese-Spanish parallel Corpus. To address this problem, In this study we proposed a method for extracting Japanese-Spanish Parallel Sentences from Wikipedia using POS tagging and Rule-Based approach. The main focus of this approach is the syntactic features of both languages. Human evaluation was performed over a sample and shows promising results, in comparison with the baseline.\n    ",
        "submission_date": "2012-11-19T00:00:00",
        "last_modified_date": "2012-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.4524",
        "title": "Applying Dynamic Model for Multiple Manoeuvring Target Tracking Using Particle Filtering",
        "authors": [
            "Mohammad Javad Parseh",
            "Saeid Pashazadeh"
        ],
        "abstract": "In this paper, we applied a dynamic model for manoeuvring targets in SIR particle filter algorithm for improving tracking accuracy of multiple manoeuvring targets. In our proposed approach, a color distribution model is used to detect changes of target's model . Our proposed approach controls deformation of target's model. If deformation of target's model is larger than a predetermined threshold, then the model will be updated. Global Nearest Neighbor (GNN) algorithm is used as data association algorithm. We named our proposed method as Deformation Detection Particle Filter (DDPF) . DDPF approach is compared with basic SIR-PF algorithm on real airshow videos. Comparisons results show that, the basic SIR-PF algorithm is not able to track the manoeuvring targets when the rotation or scaling is occurred in target' s model. However, DDPF approach updates target's model when the rotation or scaling is occurred. Thus, the proposed approach is able to track the manoeuvring targets more efficiently and accurately.\n    ",
        "submission_date": "2012-11-19T00:00:00",
        "last_modified_date": "2012-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.5371",
        "title": "A hybrid cross entropy algorithm for solving dynamic transit network design problem",
        "authors": [
            "Tai-Yu Ma"
        ],
        "abstract": "This paper proposes a hybrid multiagent learning algorithm for solving the dynamic simulation-based bilevel network design problem. The objective is to determine the op-timal frequency of a multimodal transit network, which minimizes total users' travel cost and operation cost of transit lines. The problem is formulated as a bilevel programming problem with equilibrium constraints describing non-cooperative Nash equilibrium in a dynamic simulation-based transit assignment context. A hybrid algorithm combing the cross entropy multiagent learning algorithm and Hooke-Jeeves algorithm is proposed. Computational results are provided on the Sioux Falls network to illustrate the perform-ance of the proposed algorithm.\n    ",
        "submission_date": "2012-11-22T00:00:00",
        "last_modified_date": "2012-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.6205",
        "title": "Neuro-Fuzzy Computing System with the Capacity of Implementation on Memristor-Crossbar and Optimization-Free Hardware Training",
        "authors": [
            "Farnood Merrikh-Bayat",
            "Farshad Merrikh-Bayat",
            "Saeed Bagheri Shouraki"
        ],
        "abstract": "In this paper, first we present a new explanation for the relation between logical circuits and artificial neural networks, logical circuits and fuzzy logic, and artificial neural networks and fuzzy inference systems. Then, based on these results, we propose a new neuro-fuzzy computing system which can effectively be implemented on the memristor-crossbar structure. One important feature of the proposed system is that its hardware can directly be trained using the Hebbian learning rule and without the need to any optimization. The system also has a very good capability to deal with huge number of input-out training data without facing problems like overtraining.\n    ",
        "submission_date": "2012-11-27T00:00:00",
        "last_modified_date": "2012-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.6410",
        "title": "New Hoopoe Heuristic Optimization",
        "authors": [
            "Mohammed El-Dosuky",
            "Ahmed EL-Bassiouny",
            "Taher Hamza",
            "Magdy Rashad"
        ],
        "abstract": "Most optimization problems in real life applications are often highly nonlinear. Local optimization algorithms do not give the desired performance. So, only global optimization algorithms should be used to obtain optimal solutions. This paper introduces a new nature-inspired metaheuristic optimization algorithm, called Hoopoe Heuristic (HH). In this paper, we will study HH and validate it against some test functions. Investigations show that it is very promising and could be seen as an optimization of the powerful algorithm of cuckoo search. Finally, we discuss the features of Hoopoe Heuristic and propose topics for further studies.\n    ",
        "submission_date": "2012-11-24T00:00:00",
        "last_modified_date": "2012-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.6411",
        "title": "New Heuristics for Interfacing Human Motor System using Brain Waves",
        "authors": [
            "Mohammed El-Dosuky",
            "Ahmed EL-Bassiouny",
            "Taher Hamza",
            "Magdy Rashad"
        ],
        "abstract": "There are many new forms of interfacing human users to machines. We persevere here electric mechanical form of interaction between human and machine. The emergence of brain-computer interface allows mind-to-movement systems. The story of the Pied Piper inspired us to devise some new heuristics for interfacing human motor system using brain waves by combining head helmet and LumbarMotionMonitor For the simulation we use java GridGain Brain responses of classified subjects during training indicates that Probe can be the best stimulus to rely on in distinguishing between knowledgeable and not knowledgeable\n    ",
        "submission_date": "2012-11-24T00:00:00",
        "last_modified_date": "2012-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.6496",
        "title": "TwitterPaul: Extracting and Aggregating Twitter Predictions",
        "authors": [
            "Naushad UzZaman",
            "Roi Blanco",
            "Michael Matthews"
        ],
        "abstract": "This paper introduces TwitterPaul, a system designed to make use of Social Media data to help to predict game outcomes for the 2010 FIFA World Cup tournament. To this end, we extracted over 538K mentions to football games from a large sample of tweets that occurred during the World Cup, and we classified into different types with a precision of up to 88%. The different mentions were aggregated in order to make predictions about the outcomes of the actual games. We attempt to learn which Twitter users are accurate predictors and explore several techniques in order to exploit this information to make more accurate predictions. We compare our results to strong baselines and against the betting line (prediction market) and found that the quality of extractions is more important than the quantity, suggesting that high precision methods working on a medium-sized dataset are preferable over low precision methods that use a larger amount of data. Finally, by aggregating some classes of predictions, the system performance is close to the one of the betting line. Furthermore, we believe that this domain independent framework can help to predict other sports, elections, product release dates and other future events that people talk about in social media.\n    ",
        "submission_date": "2012-11-28T00:00:00",
        "last_modified_date": "2012-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.6610",
        "title": "Intrusion Detection on Smartphones",
        "authors": [
            "Muhamed Halilovic",
            "Abdulhamit Subasi"
        ],
        "abstract": "Smartphone technology is more and more becoming the predominant communication tool for people across the world. People use their smartphones to keep their contact data, to browse the internet, to exchange messages, to keep notes, carry their personal files and documents, etc. Users while browsing are also capable of shopping online, thus provoking a need to type their credit card numbers and security codes. As the smartphones are becoming widespread so do the security threats and vulnerabilities facing this technology. Recent news and articles indicate huge increase in malware and viruses for operating systems employed on smartphones (primarily Android and iOS). Major limitations of smartphone technology are its processing power and its scarce energy source since smartphones rely on battery usage. Since smartphones are devices which change their network location as the user moves between different places, intrusion detection systems for smartphone technology are most often classified as IDSs designed for mobile ad-hoc networks. The aim of this research is to give a brief overview of IDS technology, give an overview of major machine learning and pattern recognition algorithms used in IDS technologies, give an overview of security models of iOS and Android and propose a new host-based IDS model for smartphones and create proof-of-concept application for Android platform for the newly proposed model. Keywords: IDS, SVM, Android, iOS;\n    ",
        "submission_date": "2012-11-28T00:00:00",
        "last_modified_date": "2012-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.6616",
        "title": "TACT: A Transfer Actor-Critic Learning Framework for Energy Saving in Cellular Radio Access Networks",
        "authors": [
            "Rongpeng Li",
            "Zhifeng Zhao",
            "Xianfu Chen",
            "Jacques Palicot",
            "Honggang Zhang"
        ],
        "abstract": "Recent works have validated the possibility of improving energy efficiency in radio access networks (RANs), achieved by dynamically turning on/off some base stations (BSs). In this paper, we extend the research over BS switching operations, which should match up with traffic load variations. Instead of depending on the dynamic traffic loads which are still quite challenging to precisely forecast, we firstly formulate the traffic variations as a Markov decision process. Afterwards, in order to foresightedly minimize the energy consumption of RANs, we design a reinforcement learning framework based BS switching operation scheme. Furthermore, to avoid the underlying curse of dimensionality in reinforcement learning, a transfer actor-critic algorithm (TACT), which utilizes the transferred learning expertise in historical periods or neighboring regions, is proposed and provably converges. In the end, we evaluate our proposed scheme by extensive simulations under various practical configurations and show that the proposed TACT algorithm contributes to a performance jumpstart and demonstrates the feasibility of significant energy efficiency improvement at the expense of tolerable delay performance.\n    ",
        "submission_date": "2012-11-28T00:00:00",
        "last_modified_date": "2014-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.6898",
        "title": "On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes",
        "authors": [
            "Bruno Scherrer",
            "Boris Lesner"
        ],
        "abstract": "We consider infinite-horizon stationary $\\gamma$-discounted Markov Decision Processes, for which it is known that there exists a stationary optimal policy. Using Value and Policy Iteration with some error $\\epsilon$ at each iteration, it is well-known that one can compute stationary policies that are $\\frac{2\\gamma}{(1-\\gamma)^2}\\epsilon$-optimal. After arguing that this guarantee is tight, we develop variations of Value and Policy Iteration for computing non-stationary policies that can be up to $\\frac{2\\gamma}{1-\\gamma}\\epsilon$-optimal, which constitutes a significant improvement in the usual situation when $\\gamma$ is close to 1. Surprisingly, this shows that the problem of \"computing near-optimal non-stationary policies\" is much simpler than that of \"computing near-optimal stationary policies\".\n    ",
        "submission_date": "2012-11-29T00:00:00",
        "last_modified_date": "2012-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.6971",
        "title": "A New Automatic Method to Adjust Parameters for Object Recognition",
        "authors": [
            "Issam Qaffou",
            "Mohamed Sadgal",
            "Aziz Elfazziki"
        ],
        "abstract": "To recognize an object in an image, the user must apply a combination of operators, where each operator has a set of parameters. These parameters must be well adjusted in order to reach good results. Usually, this adjustment is made manually by the user. In this paper we propose a new method to automate the process of parameter adjustment for an object recognition task. Our method is based on reinforcement learning, we use two types of agents: User Agent that gives the necessary information and Parameter Agent that adjusts the parameters of each operator. Due to the nature of reinforcement learning the results do not depend only on the system characteristics but also on the user favorite choices.\n    ",
        "submission_date": "2012-11-29T00:00:00",
        "last_modified_date": "2012-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.7210",
        "title": "Evolutionarily Stable Sets in Quantum Penny Flip Games",
        "authors": [
            "Tina Yu",
            "Radel Ben-Av"
        ],
        "abstract": "In game theory, an Evolutionarily Stable Set (ES set) is a set of Nash Equilibrium (NE) strategies that give the same payoffs. Similar to an Evolutionarily Stable Strategy (ES strategy), an ES set is also a strict NE. This work investigates the evolutionary stability of classical and quantum strategies in the quantum penny flip games. In particular, we developed an evolutionary game theory model to conduct a series of simulations where a population of mixed classical strategies from the ES set of the game were invaded by quantum strategies. We found that when only one of the two players' mixed classical strategies were invaded, the results were different. In one case, due to the interference phenomenon of superposition, quantum strategies provided more payoff, hence successfully replaced the mixed classical strategies in the ES set. In the other case, the mixed classical strategies were able to sustain the invasion of quantum strategies and remained in the ES set. Moreover, when both players' mixed classical strategies were invaded by quantum strategies, a new quantum ES set emerged. The strategies in the quantum ES set give both players payoff 0, which is the same as the payoff of the strategies in the mixed classical ES set of this game.\n    ",
        "submission_date": "2012-11-30T00:00:00",
        "last_modified_date": "2012-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.0059",
        "title": "Artificial Neural Network Fuzzy Inference System (ANFIS) For Brain Tumor Detection",
        "authors": [
            "Minakshi Sharma"
        ],
        "abstract": "Detection and segmentation of Brain tumor is very important because it provides anatomical information of normal and abnormal tissues which helps in treatment planning and patient follow-up. There are number of techniques for image segmentation. Proposed research work uses ANFIS (Artificial Neural Network Fuzzy Inference System) for image classification and then compares the results with FCM (Fuzzy C means) and K-NN (K-nearest neighbor). ANFIS includes benefits of both ANN and the fuzzy logic systems. A comprehensive feature set and fuzzy rules are selected to classify an abnormal image to the corresponding tumor type. Experimental results illustrate promising results in terms of classification accuracy. A comparative analysis is performed with the FCM and K-NN to show the superior nature of ANFIS systems.\n    ",
        "submission_date": "2012-12-01T00:00:00",
        "last_modified_date": "2012-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.0079",
        "title": "Computing Strong and Weak Permissions in Defeasible Logic",
        "authors": [
            "Guido Governatori",
            "Francesco Olivieri",
            "Antonino Rotolo",
            "Simone Scannapieco"
        ],
        "abstract": "In this paper we propose an extension of Defeasible Logic to represent and compute three concepts of defeasible permission. In particular, we discuss different types of explicit permissive norms that work as exceptions to opposite obligations. Moreover, we show how strong permissions can be represented both with, and without introducing a new consequence relation for inferring conclusions from explicit permissive norms. Finally, we illustrate how a preference operator applicable to contrary-to-duty obligations can be combined with a new operator representing ordered sequences of strong permissions which derogate from prohibitions. The logical system is studied from a computational standpoint and is shown to have liner computational complexity.\n    ",
        "submission_date": "2012-12-01T00:00:00",
        "last_modified_date": "2012-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.0873",
        "title": "Parallel Coordinate Descent Methods for Big Data Optimization",
        "authors": [
            "Peter Richt\u00e1rik",
            "Martin Tak\u00e1\u010d"
        ],
        "abstract": "In this work we show that randomized (block) coordinate descent methods can be accelerated by parallelization when applied to the problem of minimizing the sum of a partially separable smooth convex function and a simple separable convex function. The theoretical speedup, as compared to the serial method, and referring to the number of iterations needed to approximately solve the problem with high probability, is a simple expression depending on the number of parallel processors and a natural and easily computable measure of separability of the smooth component of the objective function. In the worst case, when no degree of separability is present, there may be no speedup; in the best case, when the problem is separable, the speedup is equal to the number of processors. Our analysis also works in the mode when the number of blocks being updated at each iteration is random, which allows for modeling situations with busy or unreliable processors. We show that our algorithm is able to solve a LASSO problem involving a matrix with 20 billion nonzeros in 2 hours on a large memory node with 24 cores.\n    ",
        "submission_date": "2012-12-04T00:00:00",
        "last_modified_date": "2013-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.1100",
        "title": "Making Early Predictions of the Accuracy of Machine Learning Applications",
        "authors": [
            "J. E. Smith",
            "P. Caleb-Solly",
            "M. A. Tahir",
            "D. Sannen",
            "H. van-Brussel"
        ],
        "abstract": "The accuracy of machine learning systems is a widely studied research topic. Established techniques such as cross-validation predict the accuracy on unseen data of the classifier produced by applying a given learning method to a given training data set. However, they do not predict whether incurring the cost of obtaining more data and undergoing further training will lead to higher accuracy. In this paper we investigate techniques for making such early predictions. We note that when a machine learning algorithm is presented with a training set the classifier produced, and hence its error, will depend on the characteristics of the algorithm, on training set's size, and also on its specific composition. In particular we hypothesise that if a number of classifiers are produced, and their observed error is decomposed into bias and variance terms, then although these components may behave differently, their behaviour may be predictable.\n",
        "submission_date": "2012-12-05T00:00:00",
        "last_modified_date": "2012-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.1108",
        "title": "On the Convergence Properties of Optimal AdaBoost",
        "authors": [
            "Joshua Belanich",
            "Luis E. Ortiz"
        ],
        "abstract": "AdaBoost is one of the most popular ML algorithms. It is simple to implement and often found very effective by practitioners, while still being mathematically elegant and theoretically sound. AdaBoost's interesting behavior in practice still puzzles the ML community. We address the algorithm's stability and establish multiple convergence properties of \"Optimal AdaBoost,\" a term coined by Rudin, Daubechies, and Schapire in 2004. We prove, in a reasonably strong computational sense, the almost universal existence of time averages, and with that, the convergence of the classifier itself, its generalization error, and its resulting margins, among many other objects, for fixed data sets under arguably reasonable conditions. Specifically, we frame Optimal AdaBoost as a dynamical system and, employing tools from ergodic theory, prove that, under a condition that Optimal AdaBoost does not have ties for best weak classifier eventually, a condition for which we provide empirical evidence from high dimensional real-world datasets, the algorithm's update behaves like a continuous map. We provide constructive proofs of several arbitrarily accurate approximations of Optimal AdaBoost; prove that they exhibit certain cycling behavior in finite time, and that the resulting dynamical system is ergodic; and establish sufficient conditions for the same to hold for the actual Optimal-AdaBoost update. We believe that our results provide reasonably strong evidence for the affirmative answer to two open conjectures, at least from a broad computational-theory perspective: AdaBoost always cycles and is an ergodic dynamical system. We present empirical evidence that cycles are hard to detect while time averages stabilize quickly. Our results ground future convergence-rate analysis and may help optimize generalization ability and alleviate a practitioner's burden of deciding how long to run the algorithm.\n    ",
        "submission_date": "2012-12-05T00:00:00",
        "last_modified_date": "2023-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.1313",
        "title": "Autonomous Navigation by Robust Scan Matching Technique",
        "authors": [
            "Debajyoti Banerji",
            "Ranjit Ray",
            "Jhankar Basu",
            "Indrajit Basak"
        ],
        "abstract": "For effective autonomous navigation,estimation of the pose of the robot is essential at every sampling time. For computing an accurate estimation,odometric error needs to be reduced with the help of data from external sensor. In this work, a technique has been developed for accurate pose estimation of mobile robot by using Laser Range data. The technique is robust to noisy data, which may contain considerable amount of outliers. A grey image is formed from laser range data and the key points from this image are extracted by Harris corner detector. The matching of the key points from consecutive data sets have been done while outliers have been rejected by RANSAC method. Robot state is measured by the correspondence between the two sets of keypoints. Finally, optimal robot state is estimated by Extended Kalman Filter. The technique has been applied to an operational robot in the laboratory environment to show the robustness of the technique in presence of noisy sensor data. The performance of this new technique has been compared with that of conventional ICP method. Through this method, effective and accurate navigation has been achieved even in presence of substantial noise in the sensor data at the cost of a small amount of additional computational complexity.\n    ",
        "submission_date": "2012-12-06T00:00:00",
        "last_modified_date": "2012-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.1625",
        "title": "Testing the AgreementMaker System in the Anatomy Task of OAEI 2012",
        "authors": [
            "Daniel Faria",
            "Catia Pesquita",
            "Emanuel Santos",
            "Francisco M. Couto",
            "Cosmin Stroe",
            "Isabel F. Cruz"
        ],
        "abstract": "The AgreementMaker system was the leading system in the anatomy task of the Ontology Alignment Evaluation Initiative (OAEI) competition in 2011. While AgreementMaker did not compete in OAEI 2012, here we report on its performance in the 2012 anatomy task, using the same configurations of AgreementMaker submitted to OAEI 2011. Additionally, we also test AgreementMaker using an updated version of the UBERON ontology as a mediating ontology, and otherwise identical configurations. AgreementMaker achieved an F-measure of 91.8% with the 2011 configurations, and an F-measure of 92.2% with the updated UBERON ontology. Thus, AgreementMaker would have been the second best system had it competed in the anatomy task of OAEI 2012, and only 0.1% below the F-measure of the best system.\n    ",
        "submission_date": "2012-12-07T00:00:00",
        "last_modified_date": "2012-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.1735",
        "title": "Towards Design of System Hierarchy (research survey)",
        "authors": [
            "Mark Sh. Levin"
        ],
        "abstract": "The paper addresses design/building frameworks for some kinds of tree-like and hierarchical structures of systems. The following approaches are examined: (1) expert-based procedures, (2) hierarchical clustering; (3) spanning problems (e.g., minimum spanning tree, minimum Steiner tree, maximum leaf spanning tree problem; (4) design of organizational 'optimal' hierarchies; (5) design of multi-layer (e.g., three-layer) k-connected network; (6) modification of hierarchies or networks: (i) modification of tree via condensing of neighbor nodes, (ii) hotlink assignment, (iii) transformation of tree into Steiner tree, (iv) restructuring as modification of an initial structural solution into a solution that is the most close to a goal solution while taking into account a cost of the modification. Combinatorial optimization problems are considered as basic ones (e.g., classification, knapsack problem, multiple choice problem, assignment problem). Some numerical examples illustrate the suggested problems and solving frameworks.\n    ",
        "submission_date": "2012-12-07T00:00:00",
        "last_modified_date": "2012-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.1798",
        "title": "IK-PSO, PSO Inverse Kinematics Solver with Application to Biped Gait Generation",
        "authors": [
            "Nizar Rokbani",
            "Adel M Alimi"
        ],
        "abstract": "This paper describes a new approach allowing the generation of a simplified Biped gait. This approach combines a classical dynamic modeling with an inverse kinematics' solver based on particle swarm optimization, PSO. First, an inverted pendulum, IP, is used to obtain a simplified dynamic model of the robot and to compute the target position of a key point in biped locomotion, the Centre Of Mass, COM. The proposed algorithm, called IK-PSO, Inverse Kinematics PSO, returns and inverse kinematics solution corresponding to that COM respecting the joints constraints. In This paper the inertia weight PSO variant is used to generate a possible solution according to the stability based fitness function and a set of joints motions constraints. The method is applied with success to a leg motion generation. Since based on a pre-calculated COM, that satisfied the biped stability, the proposal allowed also to plan a walk with application on a small size biped robot.\n    ",
        "submission_date": "2012-12-08T00:00:00",
        "last_modified_date": "2012-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.1881",
        "title": "Deciding Monotone Duality and Identifying Frequent Itemsets in Quadratic Logspace",
        "authors": [
            "Georg Gottlob"
        ],
        "abstract": "The monotone duality problem is defined as follows: Given two monotone formulas f and g in iredundant DNF, decide whether f and g are dual. This problem is the same as duality testing for hypergraphs, that is, checking whether a hypergraph H consists of precisely all minimal transversals of a simple hypergraph G. By exploiting a recent problem-decomposition method by Boros and Makino (ICALP 2009), we show that duality testing for hypergraphs, and thus for monotone DNFs, is feasible in DSPACE[log^2 n], i.e., in quadratic logspace. As the monotone duality problem is equivalent to a number of problems in the areas of databases, data mining, and knowledge discovery, the results presented here yield new complexity results for those problems, too. For example, it follows from our results that whenever for a Boolean-valued relation (whose attributes represent items), a number of maximal frequent itemsets and a number of minimal infrequent itemsets are known, then it can be decided in quadratic logspace whether there exist additional frequent or infrequent itemsets.\n    ",
        "submission_date": "2012-12-09T00:00:00",
        "last_modified_date": "2013-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.1942",
        "title": "Balanced K-SAT and Biased random K-SAT on trees",
        "authors": [
            "Sumedha",
            "Supriya Krishnamurthy",
            "Sharmistha Sahoo"
        ],
        "abstract": "We study and solve some variations of the random K-satisfiability problem - balanced K-SAT and biased random K-SAT - on a regular tree, using techniques we have developed earlier(",
        "submission_date": "2012-12-10T00:00:00",
        "last_modified_date": "2012-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2129",
        "title": "Online Portfolio Selection: A Survey",
        "authors": [
            "Bin Li",
            "Steven C. H. Hoi"
        ],
        "abstract": "Online portfolio selection is a fundamental problem in computational finance, which has been extensively studied across several research communities, including finance, statistics, artificial intelligence, machine learning, and data mining, etc. This article aims to provide a comprehensive survey and a structural understanding of published online portfolio selection techniques. From an online machine learning perspective, we first formulate online portfolio selection as a sequential decision problem, and then survey a variety of state-of-the-art approaches, which are grouped into several major categories, including benchmarks, \"Follow-the-Winner\" approaches, \"Follow-the-Loser\" approaches, \"Pattern-Matching\" based approaches, and \"Meta-Learning Algorithms\". In addition to the problem formulation and related algorithms, we also discuss the relationship of these algorithms with the Capital Growth theory in order to better understand the similarities and differences of their underlying trading ideas. This article aims to provide a timely and comprehensive survey for both machine learning and data mining researchers in academia and quantitative portfolio managers in the financial industry to help them understand the state-of-the-art and facilitate their research and practical applications. We also discuss some open issues and evaluate some emerging new trends for future research directions.\n    ",
        "submission_date": "2012-12-10T00:00:00",
        "last_modified_date": "2013-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2262",
        "title": "Bag-of-Words Representation for Biomedical Time Series Classification",
        "authors": [
            "Jin Wang",
            "Ping Liu",
            "Mary F.H.She",
            "Saeid Nahavandi",
            "and Abbas Kouzani"
        ],
        "abstract": "Automatic analysis of biomedical time series such as electroencephalogram (EEG) and electrocardiographic (ECG) signals has attracted great interest in the community of biomedical engineering due to its important applications in medicine. In this work, a simple yet effective bag-of-words representation that is able to capture both local and global structure similarity information is proposed for biomedical time series representation. In particular, similar to the bag-of-words model used in text document domain, the proposed method treats a time series as a text document and extracts local segments from the time series as words. The biomedical time series is then represented as a histogram of codewords, each entry of which is the count of a codeword appeared in the time series. Although the temporal order of the local segments is ignored, the bag-of-words representation is able to capture high-level structural information because both local and global structural information are well utilized. The performance of the bag-of-words model is validated on three datasets extracted from real EEG and ECG signals. The experimental results demonstrate that the proposed method is not only insensitive to parameters of the bag-of-words model such as local segment length and codebook size, but also robust to noise.\n    ",
        "submission_date": "2012-12-11T00:00:00",
        "last_modified_date": "2012-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2314",
        "title": "Tree Projections and Structural Decomposition Methods: Minimality and Game-Theoretic Characterization",
        "authors": [
            "Gianluigi Greco",
            "Francesco Scarcello"
        ],
        "abstract": "Tree projections provide a mathematical framework that encompasses all the various (purely) structural decomposition methods that have been proposed in the literature to single out classes of nearly-acyclic (hyper)graphs, such as the tree decomposition method, which is the most powerful decomposition method on graphs, and the (generalized) hypertree decomposition method, which is its natural counterpart on arbitrary hypergraphs. The paper analyzes this framework, by focusing in particular on \"minimal\" tree projections, that is, on tree projections without useless redundancies. First, it is shown that minimal tree projections enjoy a number of properties that are usually required for normal form decompositions in various structural decomposition methods. In particular, they enjoy the same kind of connection properties as (minimal) tree decompositions of graphs, with the result being tight in the light of the negative answer that is provided to the open question about whether they enjoy a slightly stronger notion of connection property, defined to speed-up the computation of hypertree decompositions. Second, it is shown that tree projections admit a natural game-theoretic characterization in terms of the Captain and Robber game. In this game, as for the Robber and Cops game characterizing tree decompositions, the existence of winning strategies implies the existence of monotone ones. As a special case, the Captain and Robber game can be used to characterize the generalized hypertree decomposition method, where such a game-theoretic characterization was missing and asked for. Besides their theoretical interest, these results have immediate algorithmic applications both for the general setting and for structural decomposition methods that can be recast in terms of tree projections.\n    ",
        "submission_date": "2012-12-11T00:00:00",
        "last_modified_date": "2012-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2468",
        "title": "Large-Sample Learning of Bayesian Networks is NP-Hard",
        "authors": [
            "David Maxwell Chickering",
            "Christopher Meek",
            "David Heckerman"
        ],
        "abstract": "In this paper, we provide new complexity results for algorithms that learn     discrete-variable Bayesian networks from data. Our results apply whenever the     learning algorithm uses a scoring criterion that favors the simplest model able     to represent the generative distribution exactly. Our results therefore hold     whenever the learning algorithm uses a consistent scoring criterion and is     applied to a sufficiently large dataset. We show that identifying high-scoring     structures is hard, even when we are given an independence oracle, an inference     oracle, and/or an information oracle. Our negative results also apply to the     learning of discrete-variable Bayesian networks in which each node has at most     k parents, for all k > 3.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2470",
        "title": "Reasoning about Bayesian Network Classifiers",
        "authors": [
            "Hei Chan",
            "Adnan Darwiche"
        ],
        "abstract": "Bayesian network classifiers are used in many fields, and one common class of classifiers are naive Bayes classifiers. In this paper, we introduce an approach for reasoning about Bayesian network classifiers in which we explicitly convert them into Ordered Decision Diagrams (ODDs), which are then used to reason about the properties of these classifiers. Specifically, we present an algorithm for converting any naive Bayes classifier into an ODD, and we show theoretically and experimentally that this algorithm can give us an ODD that is tractable in size even given an intractable number of instances. Since ODDs are tractable representations of classifiers, our algorithm allows us to efficiently test the equivalence of two naive Bayes classifiers and characterize discrepancies between them. We also show a number of additional results including a count of distinct classifiers that can be induced by changing some CPT in a naive Bayes classifier, and the range of allowable changes to a CPT which keeps the current classifier unchanged.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2471",
        "title": "Monte Carlo Matrix Inversion Policy Evaluation",
        "authors": [
            "Fletcher Lu",
            "Dale Schuurmans"
        ],
        "abstract": "In 1950, Forsythe and Leibler (1950) introduced a statistical technique for     finding the inverse of a matrix by characterizing the elements of the matrix     inverse as expected values of a sequence of random walks. Barto and Duff (1994)     subsequently showed relations between this technique and standard dynamic     programming and temporal differencing methods. The advantage of the Monte Carlo     matrix inversion (MCMI) approach is that it scales better with respect to     state-space size than alternative techniques. In this paper, we introduce an     algorithm for performing reinforcement learning policy evaluation using MCMI.     We demonstrate that MCMI improves on runtime over a maximum likelihood     model-based policy evaluation approach and on both runtime and accuracy over     the temporal differencing (TD) policy evaluation approach. We further improve     on MCMI policy evaluation by adding an importance sampling technique to our     algorithm to reduce the variance of our estimator. Lastly, we illustrate     techniques for scaling up MCMI to large state spaces in order to perform policy     improvement.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2479",
        "title": "LAYERWIDTH: Analysis of a New Metric for Directed Acyclic Graphs",
        "authors": [
            "Mark Hopkins"
        ],
        "abstract": "We analyze a new property of directed acyclic graphs (DAGs), called     layerwidth, arising from a class of DAGs proposed by Eiter and Lukasiewicz.     This class of DAGs permits certain problems of structural model-based causality     and explanation to be tractably solved. In this paper, we first address an open     question raised by Eiter and Lukasiewicz - the computational complexity of     deciding whether a given graph has a bounded layerwidth. After proving that     this problem is NP-complete, we proceed by proving numerous important     properties of layerwidth that are helpful in efficiently computing the optimal     layerwidth. Finally, we compare this new DAG property to two other important     DAG properties: treewidth and bandwidth.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2480",
        "title": "Approximate Inference and Constrained Optimization",
        "authors": [
            "Tom Heskes",
            "Kees Albers",
            "Hilbert Kappen"
        ],
        "abstract": "Loopy and generalized belief propagation are popular algorithms for approximate inference in Markov random fields and Bayesian networks. Fixed     points of these algorithms correspond to extrema of the Bethe and Kikuchi free     energy. However, belief propagation does not always converge, which explains     the need for approaches that explicitly minimize the Kikuchi/Bethe free energy,     such as CCCP and UPS. Here we describe a class of algorithms that solves this     typically nonconvex constrained minimization of the Kikuchi free energy through     a sequence of convex constrained minimizations of upper bounds on the Kikuchi     free energy. Intuitively one would expect tighter bounds to lead to faster     algorithms, which is indeed convincingly demonstrated in our simulations.     Several ideas are applied to obtain tight convex bounds that yield dramatic     speed-ups over CCCP.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2495",
        "title": "Policy-contingent abstraction for robust robot control",
        "authors": [
            "Joelle Pineau",
            "Geoffrey Gordon",
            "Sebastian Thrun"
        ],
        "abstract": "This paper presents a scalable control algorithm that enables a deployed     mobile robot system to make high-level decisions under full consideration of     its probabilistic belief. Our approach is based on insights from the rich     literature of hierarchical controllers and hierarchical MDPs. The resulting     controller has been successfully deployed in a nursing facility near     Pittsburgh, PA. To the best of our knowledge, this work is a unique instance of     applying POMDPs to high-level robotic control problems.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2500",
        "title": "On Local Optima in Learning Bayesian Networks",
        "authors": [
            "Jens D. Nielsen",
            "Tomas Kocka",
            "Jose M. Pena"
        ],
        "abstract": "This paper proposes and evaluates the k-greedy equivalence search algorithm (KES) for learning Bayesian networks (BNs) from complete data. The main characteristic of KES is that it allows a trade-off between greediness and randomness, thus exploring different good local optima. When greediness is set at maximum, KES corresponds to the greedy equivalence search algorithm (GES). When greediness is kept at minimum, we prove that under mild assumptions KES  asymptotically returns any inclusion optimal BN with nonzero probability. Experimental results for both synthetic and real data are reported showing that KES often finds a better local optima than GES. Moreover, we use KES to experimentally confirm that the number of different local optima is often huge.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2509",
        "title": "Exploiting Locality in Searching the Web",
        "authors": [
            "Joel Young",
            "Thomas L. Dean"
        ],
        "abstract": "Published experiments on spidering the Web suggest that, given training data     in the form of a (relatively small) subgraph of the Web containing a subset of     a selected class of target pages, it is possible to conduct a directed search     and find additional target pages significantly faster (with fewer page     retrievals) than by performing a blind or uninformed random or systematic     search, e.g., breadth-first search. If true, this claim motivates a number of     practical applications. Unfortunately, these experiments were carried out in     specialized domains or under conditions that are difficult to replicate. We     present and apply an experimental framework designed to reexamine and resolve     the basic claims of the earlier work, so that the supporting experiments can be     replicated and built upon. We provide high-performance tools for building     experimental spiders, make use of the ground truth and static nature of the     WT10g TREC Web corpus, and rely on simple well understand machine learning     techniques to conduct our experiments. In this paper, we describe the basic     framework, motivate the experimental design, and report on our findings     supporting and qualifying the conclusions of the earlier research.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2866",
        "title": "Speed Optimization In Unplanned Traffic Using Bio-Inspired Computing And Population Knowledge Base",
        "authors": [
            "Prasun Ghosal",
            "Arijit Chakraborty",
            "Sabyasachee Banerjee",
            "Satabdi Barman"
        ],
        "abstract": "Bio-Inspired Algorithms on Road Traffic Congestion and safety is a very promising research problem. Searching for an efficient optimization method to increase the degree of speed optimization and thereby increasing the traffic Flow in an unplanned zone is a widely concerning issue. However, there has been a limited research effort on the optimization of the lane usage with speed optimization. The main objective of this article is to find avenues or techniques in a novel way to solve the problem optimally using the knowledge from analysis of speeds of vehicles, which, in turn will act as a guide for design of lanes optimally to provide better optimized traffic. The accident factors adjust the base model estimates for individual geometric design element dimensions and for traffic control features. The application of these algorithms in partially modified form in accordance of this novel Speed Optimization Technique in an Unplanned Traffic analysis technique is applied to the proposed design and speed optimization plan. The experimental results based on real life data are quite encouraging.\n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2958",
        "title": "Spike and Tyke, the Quantized Neuron Model",
        "authors": [
            "M. A. El-Dosuky",
            "M. Z. Rashad",
            "T. T. Hamza",
            "A. H. EL-Bassiouny"
        ],
        "abstract": "Modeling spike firing assumes that spiking statistics are Poisson, but real data violates this assumption. To capture non-Poissonian features, in order to fix the inevitable inherent irregularity, researchers rescale the time axis with tedious computational overhead instead of searching for another distribution. Spikes or action potentials are precisely-timed changes in the ionic transport through synapses adjusting the synaptic weight, successfully modeled and developed as a memristor. Memristance value is multiples of initial resistance. This reminds us with the foundations of quantum mechanics. We try to quantize potential and resistance, as done with energy. After reviewing Planck curve for blackbody radiation, we propose the quantization equations. We introduce and prove a theorem that quantizes the resistance. Then we define the tyke showing its basic characteristics. Finally we give the basic transformations to model spiking and link an energy quantum to a tyke. Investigation shows how this perfectly models the neuron spiking, with over 97% match.\n    ",
        "submission_date": "2012-12-07T00:00:00",
        "last_modified_date": "2013-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2991",
        "title": "Accelerating Inference: towards a full Language, Compiler and Hardware stack",
        "authors": [
            "Shawn Hershey",
            "Jeff Bernstein",
            "Bill Bradley",
            "Andrew Schweitzer",
            "Noah Stein",
            "Theo Weber",
            "Ben Vigoda"
        ],
        "abstract": "We introduce Dimple, a fully open-source API for probabilistic modeling. Dimple allows the user to specify probabilistic models in the form of graphical models, Bayesian networks, or factor graphs, and performs inference (by automatically deriving an inference engine from a variety of algorithms) on the model. Dimple also serves as a compiler for GP5, a hardware accelerator for inference.\n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.3013",
        "title": "Product/Brand extraction from WikiPedia",
        "authors": [
            "K. Massoudi",
            "G. Modena"
        ],
        "abstract": "In this paper we describe the task of extracting product and brand pages from wikipedia. We present an experimental environment and setup built on top of a dataset of wikipedia pages we collected. We introduce a method for recognition of product pages modelled as a boolean probabilistic classification task. We show that this approach can lead to promising results and we discuss alternative approaches we considered.\n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.4287",
        "title": "Prediction of Parallel Speed-ups for Las Vegas Algorithms",
        "authors": [
            "Charlotte Truchet",
            "Florian Richoux",
            "Philippe Codognet"
        ],
        "abstract": "We propose a probabilistic model for the parallel execution of Las Vegas algorithms, i.e., randomized algorithms whose runtime might vary from one execution to another, even with the same input. This model aims at predicting the parallel performances (i.e., speedups) by analysis the runtime distribution of the sequential runs of the algorithm. Then, we study in practice the case of a particular Las Vegas algorithm for combinatorial optimization, on three classical problems, and compare with an actual parallel implementation up to 256 cores. We show that the prediction can be quite accurate, matching the actual speedups very well up to 100 parallel cores and then with a deviation of about 20% up to 256 cores.\n    ",
        "submission_date": "2012-12-18T00:00:00",
        "last_modified_date": "2012-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.4373",
        "title": "A trust-based security mechanism for nomadic users in pervasive systems",
        "authors": [
            "Mohammed Nadir Djedid"
        ],
        "abstract": "The emergence of network technologies and the appearance of new varied applications in terms of services and resources, has created new security problems for which existing solutions and mechanisms are inadequate, especially problems of identification and authentication. In a highly distributed and pervasive system, a uniform and centralized security management is not an option. It then becomes necessary to give more autonomy to security systems by providing them with mechanisms that allows a dynamic and flexible cooperation and collaboration between the actors in the system.\n    ",
        "submission_date": "2012-12-18T00:00:00",
        "last_modified_date": "2012-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.5271",
        "title": "Towards the Evolution of Novel Vertical-Axis Wind Turbines",
        "authors": [
            "Richard J. Preen",
            "Larry Bull"
        ],
        "abstract": "Renewable and sustainable energy is one of the most important challenges currently facing mankind. Wind has made an increasing contribution to the world's energy supply mix, but still remains a long way from reaching its full potential. In this paper, we investigate the use of artificial evolution to design vertical-axis wind turbine prototypes that are physically instantiated and evaluated under approximated wind tunnel conditions. An artificial neural network is used as a surrogate model to assist learning and found to reduce the number of fabrications required to reach a higher aerodynamic efficiency, resulting in an important cost reduction. Unlike in other approaches, such as computational fluid dynamics simulations, no mathematical formulations are used and no model assumptions are made.\n    ",
        "submission_date": "2012-12-20T00:00:00",
        "last_modified_date": "2012-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.5461",
        "title": "Interactive Ant Colony Optimisation (iACO) for Early Lifecycle Software Design",
        "authors": [
            "Christopher L. Simons",
            "Jim Smith",
            "Paul White"
        ],
        "abstract": "Software design is crucial to successful software development, yet is a demanding multi-objective problem for software engineers. In an attempt to assist the software designer, interactive (i.e. human in-the-loop) meta-heuristic search techniques such as evolutionary computing have been applied and show promising results. Recent investigations have also shown that Ant Colony Optimization (ACO) can outperform evolutionary computing as a potential search engine for interactive software design. With a limited computational budget, ACO produces superior candidate design solutions in a smaller number of iterations. Building on these findings, we propose a novel interactive ACO (iACO) approach to assist the designer in early lifecycle software design, in which the search is steered jointly by subjective designer evaluation as well as machine fitness functions relating the structural integrity and surrogate elegance of software designs. Results show that iACO is speedy, responsive and highly effective in enabling interactive, dynamic multi-objective search in early lifecycle software design. Study participants rate the iACO search experience as compelling. Results of machine learning of fitness measure weightings indicate that software design elegance does indeed play a significant role in designer evaluation of candidate software design. We conclude that the evenness of the number of attributes and methods among classes (NAC) is a significant surrogate elegance measure, which in turn suggests that this evenness of distribution, when combined with structural integrity, is an implicit but crucial component of effective early lifecycle software design.\n    ",
        "submission_date": "2012-12-21T00:00:00",
        "last_modified_date": "2014-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.6273",
        "title": "Human-Recognizable Robotic Gestures",
        "authors": [
            "John-John Cabibihan",
            "Wing-Chee So",
            "Soumo Pramanik"
        ],
        "abstract": "For robots to be accommodated in human spaces and in humans daily activities, robots should be able to understand messages from the human conversation partner. In the same light, humans must also understand the messages that are being communicated by robots, including the non-verbal ones. We conducted a web-based video study wherein participants gave interpretations on the iconic gestures and emblems that were produced by an anthropomorphic robot. Out of the 15 gestures presented, we found 6 robotic gestures that can be accurately recognized by the human observer. These were nodding, clapping, hugging, expressing anger, walking, and flying. We reviewed these gestures for their meaning from literatures in human and animal behavior. We conclude by discussing the possible implications of these gestures for the design of social robots that are aimed to have engaging interactions with humans.\n    ",
        "submission_date": "2012-12-26T00:00:00",
        "last_modified_date": "2012-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.6276",
        "title": "Echo State Queueing Network: a new reservoir computing learning tool",
        "authors": [
            "Sebasti\u00e1n Basterrech",
            "Gerardo Rubino"
        ],
        "abstract": "In the last decade, a new computational paradigm was introduced in the field of Machine Learning, under the name of Reservoir Computing (RC). RC models are neural networks which a recurrent part (the reservoir) that does not participate in the learning process, and the rest of the system where no recurrence (no neural circuit) occurs. This approach has grown rapidly due to its success in solving learning tasks and other computational applications. Some success was also observed with another recently proposed neural network designed using Queueing Theory, the Random Neural Network (RandNN). Both approaches have good properties and identified drawbacks. In this paper, we propose a new RC model called Echo State Queueing Network (ESQN), where we use ideas coming from RandNNs for the design of the reservoir. ESQNs consist in ESNs where the reservoir has a new dynamics inspired by recurrent RandNNs. The paper positions ESQNs in the global Machine Learning area, and provides examples of their use and performances. We show on largely used benchmarks that ESQNs are very accurate tools, and we illustrate how they compare with standard ESNs.\n    ",
        "submission_date": "2012-12-26T00:00:00",
        "last_modified_date": "2012-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.6298",
        "title": "Design of Intelligent Agents Based System for Commodity Market Simulation with JADE",
        "authors": [
            "R. Refianti",
            "A. B. Mutiara",
            "H. Gunawan"
        ],
        "abstract": "A market of potato commodity for industry scale usage is engaging several types of actors. They are farmers, middlemen, and industries. A multi-agent system has been built to simulate these actors into agent entities, based on manually given parameters within a simulation scenario file. Each type of agents has its own fuzzy logic representing actual actors' knowledge, to be used to interpreting values and take appropriated decision of it while on simulation. The system will simulate market activities with programmed behaviors then produce the results as spreadsheet and chart graph files. These results consist of each agent's yearly finance and commodity data. The system will also predict each of next value from these outputs.\n    ",
        "submission_date": "2012-12-27T00:00:00",
        "last_modified_date": "2012-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.6659",
        "title": "Focus of Attention for Linear Predictors",
        "authors": [
            "Raphael Pelossof",
            "Zhiliang Ying"
        ],
        "abstract": "We present a method to stop the evaluation of a prediction process when the result of the full evaluation is obvious. This trait is highly desirable in prediction tasks where a predictor evaluates all its features for every example in large datasets. We observe that some examples are easier to classify than others, a phenomenon which is characterized by the event when most of the features agree on the class of an example. By stopping the feature evaluation when encountering an easy- to-classify example, the predictor can achieve substantial gains in computation. Our method provides a natural attention mechanism for linear predictors where the predictor concentrates most of its computation on hard-to-classify examples and quickly discards easy-to-classify ones. By modifying a linear prediction algorithm such as an SVM or AdaBoost to include our attentive method we prove that the average number of features computed is O(sqrt(n log 1/sqrt(delta))) where n is the original number of features, and delta is the error rate incurred due to early stopping. We demonstrate the effectiveness of Attentive Prediction on MNIST, Real-sim, Gisette, and synthetic datasets.\n    ",
        "submission_date": "2012-12-29T00:00:00",
        "last_modified_date": "2012-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.6837",
        "title": "Autonomously Learning to Visually Detect Where Manipulation Will Succeed",
        "authors": [
            "Hai Nguyen",
            "Charles C. Kemp"
        ],
        "abstract": "Visual features can help predict if a manipulation behavior will succeed at a given location. For example, the success of a behavior that flips light switches depends on the location of the switch. Within this paper, we present methods that enable a mobile manipulator to autonomously learn a function that takes an RGB image and a registered 3D point cloud as input and returns a 3D location at which a manipulation behavior is likely to succeed. Given a pair of manipulation behaviors that can change the state of the world between two sets (e.g., light switch up and light switch down), classifiers that detect when each behavior has been successful, and an initial hint as to where one of the behaviors will be successful, the robot autonomously trains a pair of support vector machine (SVM) classifiers by trying out the behaviors at locations in the world and observing the results. When an image feature vector associated with a 3D location is provided as input to one of the SVMs, the SVM predicts if the associated manipulation behavior will be successful at the 3D location. To evaluate our approach, we performed experiments with a PR2 robot from Willow Garage in a simulated home using behaviors that flip a light switch, push a rocker-type light switch, and operate a drawer. By using active learning, the robot efficiently learned SVMs that enabled it to consistently succeed at these tasks. After training, the robot also continued to learn in order to adapt in the event of failure.\n    ",
        "submission_date": "2012-12-31T00:00:00",
        "last_modified_date": "2012-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.6846",
        "title": "Maximizing a Nonnegative, Monotone, Submodular Function Constrained to Matchings",
        "authors": [
            "Sagar Kale"
        ],
        "abstract": "Submodular functions have many applications. Matchings have many applications. The bitext word alignment problem can be modeled as the problem of maximizing a nonnegative, monotone, submodular function constrained to matchings in a complete bipartite graph where each vertex corresponds to a word in the two input sentences and each edge represents a potential word-to-word translation. We propose a more general problem of maximizing a nonnegative, monotone, submodular function defined on the edge set of a complete graph constrained to matchings; we call this problem the CSM-Matching problem. CSM-Matching also generalizes the maximum-weight matching problem, which has a polynomial-time algorithm; however, we show that it is NP-hard to approximate CSM-Matching within a factor of e/(e-1) by reducing the max k-cover problem to it. Our main result is a simple, greedy, 3-approximation algorithm for CSM-Matching. Then we reduce CSM-Matching to maximizing a nonnegative, monotone, submodular function over two matroids, i.e., CSM-2-Matroids. CSM-2-Matroids has a (2+epsilon)-approximation algorithm - called LSV2. We show that we can find a (4+epsilon)-approximate solution to CSM-Matching using LSV2. We extend this approach to similar problems.\n    ",
        "submission_date": "2012-12-31T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    }
]