[
    {
        "url": "https://arxiv.org/abs/0901.0318",
        "title": "Thoughts on an Unified Framework for Artificial Chemistries",
        "authors": [
            "Janrdan Misra"
        ],
        "abstract": "  Artificial Chemistries (ACs) are symbolic chemical metaphors for the exploration of Artificial Life, with specific focus on the problem of biogenesis or the origin of life. This paper presents authors thoughts towards defining a unified framework to characterize and classify symbolic artificial chemistries by devising appropriate formalism to capture semantic and organizational information. We identify three basic high level abstractions in initial proposal for this framework viz., information, computation, and communication. We present an analysis of two important notions of information, namely, Shannon's Entropy and Algorithmic Information, and discuss inductive and deductive approaches for defining the framework.\n    ",
        "submission_date": "2009-01-03T00:00:00",
        "last_modified_date": "2009-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0901.0786",
        "title": "Approximate inference on planar graphs using Loop Calculus and Belief Propagation",
        "authors": [
            "V. G\u00f3mez",
            "H. J. Kappen",
            "M. Chertkov"
        ],
        "abstract": "  We introduce novel results for approximate inference on planar graphical models using the loop calculus framework. The loop calculus (Chertkov and Chernyak, 2006) allows to express the exact partition function of a graphical model as a finite sum of terms that can be evaluated once the belief propagation (BP) solution is known. In general, full summation over all correction terms is intractable. We develop an algorithm for the approach presented in (Certkov et al., 2008) which represents an efficient truncation scheme on planar graphs and a new representation of the series in terms of Pfaffians of matrices. We analyze the performance of the algorithm for the partition function approximation for models with binary variables and pairwise interactions on grids and other planar graphs. We study in detail both the loop series and the equivalent Pfaffian series and show that the first term of the Pfaffian series for the general, intractable planar model, can provide very accurate approximations. The algorithm outperforms previous truncation schemes of the loop series and is competitive with other state-of-the-art methods for approximate inference.\n    ",
        "submission_date": "2009-01-07T00:00:00",
        "last_modified_date": "2009-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0901.1152",
        "title": "A nonclassical symbolic theory of working memory, mental computations, and mental set",
        "authors": [
            "Victor Eliashberg"
        ],
        "abstract": "  The paper tackles four basic questions associated with human brain as a learning system. How can the brain learn to (1) mentally simulate different external memory aids, (2) perform, in principle, any mental computations using imaginary memory aids, (3) recall the real sensory and motor events and synthesize a combinatorial number of imaginary events, (4) dynamically change its mental set to match a combinatorial number of contexts? We propose a uniform answer to (1)-(4) based on the general postulate that the human neocortex processes symbolic information in a \"nonclassical\" way. Instead of manipulating symbols in a read/write memory, as the classical symbolic systems do, it manipulates the states of dynamical memory representing different temporary attributes of immovable symbolic structures stored in a long-term memory. The approach is formalized as the concept of E-machine. Intuitively, an E-machine is a system that deals mainly with characteristic functions representing subsets of memory pointers rather than the pointers themselves. This nonclassical symbolic paradigm is Turing universal, and, unlike the classical one, is efficiently implementable in homogeneous neural networks with temporal modulation topologically resembling that of the neocortex.\n    ",
        "submission_date": "2009-01-08T00:00:00",
        "last_modified_date": "2009-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0901.1289",
        "title": "N-norm and N-conorm in Neutrosophic Logic and Set, and the Neutrosophic Topologies",
        "authors": [
            "Florentin Smarandache"
        ],
        "abstract": "  In this paper we present the N-norms/N-conorms in neutrosophic logic and set as extensions of T-norms/T-conorms in fuzzy logic and set. Also, as an extension of the Intuitionistic Fuzzy Topology we present the Neutrosophic Topologies.\n    ",
        "submission_date": "2009-01-09T00:00:00",
        "last_modified_date": "2009-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0901.2850",
        "title": "On finitely recursive programs",
        "authors": [
            "Sabrina Baselice",
            "Piero A. Bonatti",
            "Giovanni Criscuolo"
        ],
        "abstract": "  Disjunctive finitary programs are a class of logic programs admitting function symbols and hence infinite domains. They have very good computational properties, for example ground queries are decidable while in the general case the stable model semantics is highly undecidable. In this paper we prove that a larger class of programs, called finitely recursive programs, preserves most of the good properties of finitary programs under the stable model semantics, namely: (i) finitely recursive programs enjoy a compactness property; (ii) inconsistency checking and skeptical reasoning are semidecidable; (iii) skeptical resolution is complete for normal finitely recursive programs. Moreover, we show how to check inconsistency and answer skeptical queries using finite subsets of the ground program instantiation. We achieve this by extending the splitting sequence theorem by Lifschitz and Turner: We prove that if the input program P is finitely recursive, then the partial stable models determined by any smooth splitting omega-sequence converge to a stable model of P.\n    ",
        "submission_date": "2009-01-19T00:00:00",
        "last_modified_date": "2009-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0901.3608",
        "title": "A remark on higher order RUE-resolution with EXTRUE",
        "authors": [
            "Christoph Benzmueller"
        ],
        "abstract": "  We show that a prominent counterexample for the completeness of first order RUE-resolution does not apply to the higher order RUE-resolution approach EXTRUE.\n    ",
        "submission_date": "2009-01-23T00:00:00",
        "last_modified_date": "2009-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0901.3769",
        "title": "Deceptiveness and Neutrality - the ND family of fitness landscapes",
        "authors": [
            "William Beaudoin",
            "S\u00e9bastien Verel",
            "Philippe Collard",
            "Cathy Escazut"
        ],
        "abstract": "  When a considerable number of mutations have no effects on fitness values, the fitness landscape is said neutral. In order to study the interplay between neutrality, which exists in many real-world applications, and performances of metaheuristics, it is useful to design landscapes which make it possible to tune precisely neutral degree distribution. Even though many neutral landscape models have already been designed, none of them are general enough to create landscapes with specific neutral degree distributions. We propose three steps to design such landscapes: first using an algorithm we construct a landscape whose distribution roughly fits the target one, then we use a simulated annealing heuristic to bring closer the two distributions and finally we affect fitness values to each neutral network. Then using this new family of fitness landscapes we are able to highlight the interplay between deceptiveness and neutrality.\n    ",
        "submission_date": "2009-01-23T00:00:00",
        "last_modified_date": "2009-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0901.4004",
        "title": "Mining for adverse drug events with formal concept analysis",
        "authors": [
            "Alexander Estacio-Moreno",
            "Yannick Toussaint",
            "C\u00e9dric Bousquet"
        ],
        "abstract": "  The pharmacovigilance databases consist of several case reports involving drugs and adverse events (AEs). Some methods are applied consistently to highlight all signals, i.e. all statistically significant associations between a drug and an AE. These methods are appropriate for verification of more complex relationships involving one or several drug(s) and AE(s) (e.g; syndromes or interactions) but do not address the identification of them. We propose a method for the extraction of these relationships based on Formal Concept Analysis (FCA) associated with disproportionality measures. This method identifies all sets of drugs and AEs which are potential signals, syndromes or interactions. Compared to a previous experience of disproportionality analysis without FCA, the addition of FCA was more efficient for identifying false positives related to concomitant drugs.\n    ",
        "submission_date": "2009-01-26T00:00:00",
        "last_modified_date": "2009-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0901.4224",
        "title": "Geospatial semantics: beyond ontologies, towards an enactive approach",
        "authors": [
            "Pasquale Di Donato"
        ],
        "abstract": "  Current approaches to semantics in the geospatial domain are mainly based on ontologies, but ontologies, since continue to build entirely on the symbolic methodology, suffers from the classical problems, e.g. the symbol grounding problem, affecting representational theories. We claim for an enactive approach to semantics, where meaning is considered to be an emergent feature arising context-dependently in action. Since representational theories are unable to deal with context, a new formalism is required toward a contextual theory of concepts. SCOP is considered a promising formalism in this sense and is briefly described.\n    ",
        "submission_date": "2009-01-27T00:00:00",
        "last_modified_date": "2009-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0901.4761",
        "title": "A Knowledge Discovery Framework for Learning Task Models from User Interactions in Intelligent Tutoring Systems",
        "authors": [
            "P. Fournier-Viger",
            "R. Nkambou",
            "E. Mephu Nguifo"
        ],
        "abstract": "  Domain experts should provide relevant domain knowledge to an Intelligent Tutoring System (ITS) so that it can guide a learner during problemsolving learning activities. However, for many ill-defined domains, the domain knowledge is hard to define explicitly. In previous works, we showed how sequential pattern mining can be used to extract a partial problem space from logged user interactions, and how it can support tutoring services during problem-solving exercises. This article describes an extension of this approach to extract a problem space that is richer and more adapted for supporting tutoring services. We combined sequential pattern mining with (1) dimensional pattern mining (2) time intervals, (3) the automatic clustering of valued actions and (4) closed sequences mining. Some tutoring services have been implemented and an experiment has been conducted in a tutoring system.\n    ",
        "submission_date": "2009-01-29T00:00:00",
        "last_modified_date": "2009-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0901.4963",
        "title": "How Emotional Mechanism Helps Episodic Learning in a Cognitive Agent",
        "authors": [
            "Usef Faghihi",
            "Philippe Fournier-Viger",
            "Roger Nkambou",
            "Pierre Poirier",
            "Andre Mayers"
        ],
        "abstract": "  In this paper we propose the CTS (Concious Tutoring System) technology, a biologically plausible cognitive agent based on human brain ",
        "submission_date": "2009-01-30T00:00:00",
        "last_modified_date": "2009-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.0744",
        "title": "Embedding Data within Knowledge Spaces",
        "authors": [
            "James D. Myers",
            "Joe Futrelle",
            "Jeff Gaynor",
            "Joel Plutchak",
            "Peter Bajcsy",
            "Jason Kastner",
            "Kailash Kotwani",
            "Jong Sung Lee",
            "Luigi Marini",
            "Rob Kooper",
            "Robert E. McGrath",
            "Terry McLaren",
            "Alejandro Rodriguez",
            "Yong Liu"
        ],
        "abstract": "  The promise of e-Science will only be realized when data is discoverable, accessible, and comprehensible within distributed teams, across disciplines, and over the long-term--without reliance on out-of-band (non-digital) means. We have developed the open-source Tupelo semantic content management framework and are employing it to manage a wide range of e-Science entities (including data, documents, workflows, people, and projects) and a broad range of metadata (including provenance, social networks, geospatial relationships, temporal relations, and domain descriptions). Tupelo couples the use of global identifiers and resource description framework (RDF) statements with an aggregatable content repository model to provide a unified space for securely managing distributed heterogeneous content and relationships.\n    ",
        "submission_date": "2009-02-04T00:00:00",
        "last_modified_date": "2009-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.0798",
        "title": "Alleviating Media Bias Through Intelligent Agent Blogging",
        "authors": [
            "Ernesto Diaz-Aviles"
        ],
        "abstract": "  Consumers of mass media must have a comprehensive, balanced and plural selection of news to get an unbiased perspective; but achieving this goal can be very challenging, laborious and time consuming. News stories development over time, its (in)consistency, and different level of coverage across the media outlets are challenges that a conscientious reader has to overcome in order to alleviate bias.\n",
        "submission_date": "2009-02-04T00:00:00",
        "last_modified_date": "2009-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.0899",
        "title": "Comparative concept similarity over Minspaces: Axiomatisation and Tableaux Calculus",
        "authors": [
            "R\u00e9gis Alenda",
            "Nicola Olivetti",
            "Camilla Schwind"
        ],
        "abstract": "  We study the logic of comparative concept similarity $\\CSL$ introduced by Sheremet, Tishkovsky, Wolter and Zakharyaschev to capture a form of qualitative similarity comparison. In this logic we can formulate assertions of the form \" objects A are more similar to B than to C\". The semantics of this logic is defined by structures equipped by distance functions evaluating the similarity degree of objects. We consider here the particular case of the semantics induced by \\emph{minspaces}, the latter being distance spaces where the minimum of a set of distances always exists. It turns out that the semantics over arbitrary minspaces can be equivalently specified in terms of preferential structures, typical of conditional logics. We first give a direct axiomatisation of this logic over Minspaces. We next define a decision procedure in the form of a tableaux calculus. Both the calculus and the axiomatisation take advantage of the reformulation of the semantics in terms of preferential structures.\n    ",
        "submission_date": "2009-02-05T00:00:00",
        "last_modified_date": "2009-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.1080",
        "title": "A Model for Managing Collections of Patterns",
        "authors": [
            "Baptiste Jeudy",
            "Christine Largeron",
            "Fran\u00e7ois Jacquenet"
        ],
        "abstract": "  Data mining algorithms are now able to efficiently deal with huge amount of data. Various kinds of patterns may be discovered and may have some great impact on the general development of knowledge. In many domains, end users may want to have their data mined by data mining tools in order to extract patterns that could impact their business. Nevertheless, those users are often overwhelmed by the large quantity of patterns extracted in such a situation. Moreover, some privacy issues, or some commercial one may lead the users not to be able to mine the data by themselves. Thus, the users may not have the possibility to perform many experiments integrating various constraints in order to focus on specific patterns they would like to extract. Post processing of patterns may be an answer to that drawback. Thus, in this paper we present a framework that could allow end users to manage collections of patterns. We propose to use an efficient data structure on which some algebraic operators may be used in order to retrieve or access patterns in pattern bases.\n    ",
        "submission_date": "2009-02-06T00:00:00",
        "last_modified_date": "2009-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.1227",
        "title": "Discovering general partial orders in event streams",
        "authors": [
            "Avinash Achar",
            "Srivatsan Laxman",
            "Raajay Viswanathan",
            "P. S. Sastry"
        ],
        "abstract": "  Frequent episode discovery is a popular framework for pattern discovery in event streams. An episode is a partially ordered set of nodes with each node associated with an event type. Efficient (and separate) algorithms exist for episode discovery when the associated partial order is total (serial episode) and trivial (parallel episode). In this paper, we propose efficient algorithms for discovering frequent episodes with general partial orders. These algorithms can be easily specialized to discover serial or parallel episodes. Also, the algorithms are flexible enough to be specialized for mining in the space of certain interesting subclasses of partial orders. We point out that there is an inherent combinatorial explosion in frequent partial order mining and most importantly, frequency alone is not a sufficient measure of interestingness. We propose a new interestingness measure for general partial order episodes and a discovery method based on this measure, for filtering out uninteresting partial orders. Simulations demonstrate the effectiveness of our algorithms.\n    ",
        "submission_date": "2009-02-07T00:00:00",
        "last_modified_date": "2009-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.2206",
        "title": "Feature Hashing for Large Scale Multitask Learning",
        "authors": [
            "Kilian Weinberger",
            "Anirban Dasgupta",
            "Josh Attenberg",
            "John Langford",
            "Alex Smola"
        ],
        "abstract": "  Empirical evidence suggests that hashing is an effective strategy for dimensionality reduction and practical nonparametric estimation. In this paper we provide exponential tail bounds for feature hashing and show that the interaction between random subspaces is negligible with high probability. We demonstrate the feasibility of this approach with experimental results for a new use case -- multitask learning with hundreds of thousands of tasks.\n    ",
        "submission_date": "2009-02-12T00:00:00",
        "last_modified_date": "2010-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.2362",
        "title": "XML Representation of Constraint Networks: Format XCSP 2.1",
        "authors": [
            "Olivier Roussel",
            "Christophe Lecoutre"
        ],
        "abstract": "  We propose a new extended format to represent constraint networks using XML. This format allows us to represent constraints defined either in extension or in intension. It also allows us to reference global constraints. Any instance of the problems CSP (Constraint Satisfaction Problem), QCSP (Quantified CSP) and WCSP (Weighted CSP) can be represented using this format.\n    ",
        "submission_date": "2009-02-13T00:00:00",
        "last_modified_date": "2009-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.2871",
        "title": "The Semantics of Kalah Game",
        "authors": [
            "Kaninda Musumbu"
        ],
        "abstract": "  The present work consisted in developing a plateau game. There are the traditional ones (monopoly, cluedo, ect.) but those which interest us leave less place at the chance (luck) than to the strategy such that the chess game. Kallah is an old African game, its rules are simple but the strategies to be used are very complex to implement. Of course, they are based on a strongly mathematical basis as in the film \"Rain-Man\" where one can see that gambling can be payed with strategies based on mathematical theories. The Artificial Intelligence gives the possibility \"of thinking\" to a machine and, therefore, allows it to make decisions. In our work, we use it to give the means to the computer choosing its best movement.\n    ",
        "submission_date": "2009-02-17T00:00:00",
        "last_modified_date": "2009-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.2975",
        "title": "Writing Positive/Negative-Conditional Equations Conveniently",
        "authors": [
            "Claus-Peter Wirth",
            "Ruediger Lunde"
        ],
        "abstract": "  We present a convenient notation for positive/negative-conditional equations. The idea is to merge rules specifying the same function by using case-, if-, match-, and let-expressions. Based on the presented macro-rule-construct, positive/negative-conditional equational specifications can be written on a higher level. A rewrite system translates the macro-rule-constructs into positive/negative-conditional equations.\n    ",
        "submission_date": "2009-02-17T00:00:00",
        "last_modified_date": "2009-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.2995",
        "title": "ASF+ --- eine ASF-aehnliche Spezifikationssprache",
        "authors": [
            "Ruediger Lunde",
            "Claus-Peter Wirth"
        ],
        "abstract": "  Maintaining the main aspects of the algebraic specification language ASF as presented in [Bergstra&al.89] we have extend ASF with the following concepts: While once exported names in ASF must stay visible up to the top the module hierarchy, ASF+ permits a more sophisticated hiding of signature names. The erroneous merging of distinct structures that occurs when importing different actualizations of the same parameterized module in ASF is avoided in ASF+ by a more adequate form of parameter binding. The new ``Namensraum''-concept of ASF+ permits the specifier on the one hand directly to identify the origin of hidden names and on the other to decide whether an imported module is only to be accessed or whether an important property of it is to be modified. In the first case he can access one single globally provided version; in the second he has to import a copy of the module. Finally ASF+ permits semantic conditions on parameters and the specification of tasks for a theorem prover.\n    ",
        "submission_date": "2009-02-17T00:00:00",
        "last_modified_date": "2009-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.3176",
        "title": "Error-Correcting Tournaments",
        "authors": [
            "Alina Beygelzimer",
            "John Langford",
            "Pradeep Ravikumar"
        ],
        "abstract": "  We present a family of pairwise tournaments reducing $k$-class classification to binary classification. These reductions are provably robust against a constant fraction of binary errors. The results improve on the PECOC construction \\cite{SECOC} with an exponential improvement in computation, from $O(k)$ to $O(\\log_2 k)$, and the removal of a square root in the regret dependence, matching the best possible computation and regret up to a constant.\n    ",
        "submission_date": "2009-02-18T00:00:00",
        "last_modified_date": "2010-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.3294",
        "title": "Progress in Computer-Assisted Inductive Theorem Proving by Human-Orientedness and Descente Infinie?",
        "authors": [
            "Claus-Peter Wirth"
        ],
        "abstract": "In this short position paper we briefly review the development history of automated inductive theorem proving and computer-assisted mathematical induction. We think that the current low expectations on progress in this field result from a faulty narrow-scope historical projection. Our main motivation is to explain--on an abstract but hopefully sufficiently descriptive level--why we believe that future progress in the field is to result from human-orientedness and descente infinie.\n    ",
        "submission_date": "2009-02-17T00:00:00",
        "last_modified_date": "2010-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.3513",
        "title": "A Systematic Approach to Artificial Agents",
        "authors": [
            "Mark Burgin",
            "Gordana Dodig-Crnkovic"
        ],
        "abstract": "  Agents and agent systems are becoming more and more important in the development of a variety of fields such as ubiquitous computing, ambient intelligence, autonomous computing, intelligent systems and intelligent robotics. The need for improvement of our basic knowledge on agents is very essential. We take a systematic approach and present extended classification of artificial agents which can be useful for understanding of what artificial agents are and what they can be in the future. The aim of this classification is to give us insights in what kind of agents can be created and what type of problems demand a specific kind of agents for their solution.\n    ",
        "submission_date": "2009-02-20T00:00:00",
        "last_modified_date": "2009-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.3614",
        "title": "Syntactic Confluence Criteria for Positive/Negative-Conditional Term Rewriting Systems",
        "authors": [
            "Claus-Peter Wirth"
        ],
        "abstract": "  We study the combination of the following already known ideas for showing confluence of unconditional or conditional term rewriting systems into practically more useful confluence criteria for conditional systems: Our syntactical separation into constructor and non-constructor symbols, Huet's introduction and Toyama's generalization of parallel closedness for non-noetherian unconditional systems, the use of shallow confluence for proving confluence of noetherian and non-noetherian conditional systems, the idea that certain kinds of limited confluence can be assumed for checking the fulfilledness or infeasibility of the conditions of conditional critical pairs, and the idea that (when termination is given) only prime superpositions have to be considered and certain normalization restrictions can be applied for the substitutions fulfilling the conditions of conditional critical pairs. Besides combining and improving already known methods, we present the following new ideas and results: We strengthen the criterion for overlay joinable noetherian systems, and, by using the expressiveness of our syntactical separation into constructor and non-constructor symbols, we are able to present criteria for level confluence that are not criteria for shallow confluence actually and also able to weaken the severe requirement of normality (stiffened with left-linearity) in the criteria for shallow confluence of noetherian and non-noetherian conditional systems to the easily satisfied requirement of quasi-normality. Finally, the whole paper may also give a practically useful overview of the syntactical means for showing confluence of conditional term rewriting systems.\n    ",
        "submission_date": "2009-02-20T00:00:00",
        "last_modified_date": "2009-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.3623",
        "title": "A Self-Contained and Easily Accessible Discussion of the Method of Descente Infinie and Fermat's Only Explicitly Known Proof by Descente Infinie",
        "authors": [
            "Claus-Peter Wirth"
        ],
        "abstract": "We present the only proof of Pierre Fermat by descente infinie that is known to exist today. As the text of its Latin original requires active mathematical interpretation, it is more a proof sketch than a proper mathematical proof. We discuss descente infinie from the mathematical, logical, historical, linguistic, and refined logic-historical points of view. We provide the required preliminaries from number theory and develop a self-contained proof in a modern form, which nevertheless is intended to follow Fermat's ideas closely. We then annotate an English translation of Fermat's original proof with terms from the modern proof. Including all important facts, we present a concise and self-contained discussion of Fermat's proof sketch, which is easily accessible to laymen in number theory as well as to laymen in the history of mathematics, and which provides new clarification of the Method of Descente Infinie to the experts in these fields. Last but not least, this paper fills a gap regarding the easy accessibility of the subject.\n    ",
        "submission_date": "2009-02-20T00:00:00",
        "last_modified_date": "2010-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.3635",
        "title": "lim+, delta+, and Non-Permutability of beta-Steps",
        "authors": [
            "Claus-Peter Wirth"
        ],
        "abstract": "  Using a human-oriented formal example proof of the (lim+) theorem, i.e. that the sum of limits is the limit of the sum, which is of value for reference on its own, we exhibit a non-permutability of beta-steps and delta+-steps (according to Smullyan's classification), which is not visible with non-liberalized delta-rules and not serious with further liberalized delta-rules, such as the delta++-rule. Besides a careful presentation of the search for a proof of (lim+) with several pedagogical intentions, the main subject is to explain why the order of beta-steps plays such a practically important role in some calculi.\n    ",
        "submission_date": "2009-02-20T00:00:00",
        "last_modified_date": "2009-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.3648",
        "title": "An Algebraic Dexter-Based Hypertext Reference Model",
        "authors": [
            "Volker Mattick",
            "Claus-Peter Wirth"
        ],
        "abstract": "  We present the first formal algebraic specification of a hypertext reference model. It is based on the well-known Dexter Hypertext Reference Model and includes modifications with respect to the development of hypertext since the WWW came up. Our hypertext model was developed as a product model with the aim to automatically support the design process and is extended to a model of hypertext-systems in order to be able to describe the state transitions in this process. While the specification should be easy to read for non-experts in algebraic specification, it guarantees a unique understanding and enables a close connection to logic-based development and verification.\n    ",
        "submission_date": "2009-02-20T00:00:00",
        "last_modified_date": "2009-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.3730",
        "title": "Full First-Order Sequent and Tableau Calculi With Preservation of Solutions and the Liberalized delta-Rule but Without Skolemization",
        "authors": [
            "Claus-Peter Wirth"
        ],
        "abstract": "  We present a combination of raising, explicit variable dependency representation, the liberalized delta-rule, and preservation of solutions for first-order deductive theorem proving. Our main motivation is to provide the foundation for our work on inductive theorem proving, where the preservation of solutions is indispensable.\n    ",
        "submission_date": "2009-02-21T00:00:00",
        "last_modified_date": "2009-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.3749",
        "title": "Hilbert's epsilon as an Operator of Indefinite Committed Choice",
        "authors": [
            "Claus-Peter Wirth"
        ],
        "abstract": "Paul Bernays and David Hilbert carefully avoided overspecification of Hilbert's epsilon-operator and axiomatized only what was relevant for their proof-theoretic investigations. Semantically, this left the epsilon-operator underspecified. In the meanwhile, there have been several suggestions for semantics of the epsilon as a choice operator. After reviewing the literature on semantics of Hilbert's epsilon operator, we propose a new semantics with the following features: We avoid overspecification (such as right-uniqueness), but admit indefinite choice, committed choice, and classical logics. Moreover, our semantics for the epsilon supports proof search optimally and is natural in the sense that it does not only mirror some cases of referential interpretation of indefinite articles in natural language, but may also contribute to philosophy of language. Finally, we ask the question whether our epsilon within our free-variable framework can serve as a paradigm useful in the specification and computation of semantics of discourses in natural language.\n    ",
        "submission_date": "2009-02-21T00:00:00",
        "last_modified_date": "2012-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0041",
        "title": "Learning DTW Global Constraint for Time Series Classification",
        "authors": [
            "Vit Niennattrakul",
            "Chotirat Ann Ratanamahatana"
        ],
        "abstract": "  1-Nearest Neighbor with the Dynamic Time Warping (DTW) distance is one of the most effective classifiers on time series domain. Since the global constraint has been introduced in speech community, many global constraint models have been proposed including Sakoe-Chiba (S-C) band, Itakura Parallelogram, and Ratanamahatana-Keogh (R-K) band. The R-K band is a general global constraint model that can represent any global constraints with arbitrary shape and size effectively. However, we need a good learning algorithm to discover the most suitable set of R-K bands, and the current R-K band learning algorithm still suffers from an 'overfitting' phenomenon. In this paper, we propose two new learning algorithms, i.e., band boundary extraction algorithm and iterative learning algorithm. The band boundary extraction is calculated from the bound of all possible warping paths in each class, and the iterative learning is adjusted from the original R-K band learning. We also use a Silhouette index, a well-known clustering validation technique, as a heuristic function, and the lower bound function, LB_Keogh, to enhance the prediction speed. Twenty datasets, from the Workshop and Challenge on Time Series Classification, held in conjunction of the SIGKDD 2007, are used to evaluate our approach.\n    ",
        "submission_date": "2009-02-28T00:00:00",
        "last_modified_date": "2009-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0174",
        "title": "Accelerating and Evaluation of Syntactic Parsing in Natural Language Question Answering Systems",
        "authors": [
            "Zhe Chen",
            "Dunwei Wen"
        ],
        "abstract": "  With the development of Natural Language Processing (NLP), more and more systems want to adopt NLP in User Interface Module to process user input, in order to communicate with user in a natural way. However, this raises a speed problem. That is, if NLP module can not process sentences in durable time delay, users will never use the system. As a result, systems which are strict with processing time, such as dialogue systems, web search systems, automatic customer service systems, especially real-time systems, have to abandon NLP module in order to get a faster system response. This paper aims to solve the speed problem. In this paper, at first, the construction of a syntactic parser which is based on corpus machine learning and statistics model is introduced, and then a speed problem analysis is performed on the parser and its algorithms. Based on the analysis, two accelerating methods, Compressed POS Set and Syntactic Patterns Pruning, are proposed, which can effectively improve the time efficiency of parsing in NLP module. To evaluate different parameters in the accelerating algorithms, two new factors, PT and RT, are introduced and explained in detail. Experiments are also completed to prove and test these methods, which will surely contribute to the application of NLP.\n    ",
        "submission_date": "2009-03-01T00:00:00",
        "last_modified_date": "2009-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0211",
        "title": "Range and Roots: Two Common Patterns for Specifying and Propagating Counting and Occurrence Constraints",
        "authors": [
            "Christian Bessiere",
            "Emmanuel Hebrard",
            "Brahim Hnich",
            "Zeynep Kiziltan",
            "Toby Walsh"
        ],
        "abstract": "  We propose Range and Roots which are two common patterns useful for specifying a wide range of counting and occurrence constraints. We design specialised propagation algorithms for these two patterns. Counting and occurrence constraints specified using these patterns thus directly inherit a propagation algorithm. To illustrate the capabilities of the Range and Roots constraints, we specify a number of global constraints taken from the literature. Preliminary experiments demonstrate that propagating counting and occurrence constraints using these two patterns leads to a small loss in performance when compared to specialised global constraints and is competitive with alternative decompositions using elementary constraints.\n    ",
        "submission_date": "2009-03-02T00:00:00",
        "last_modified_date": "2009-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0276",
        "title": "Impact of Cognitive Radio on Future Management of Spectrum",
        "authors": [
            "Maziar Nekovee"
        ],
        "abstract": "  Cognitive radio is a breakthrough technology which is expected to have a profound impact on the way radio spectrum will be accessed, managed and shared in the future. In this paper I examine some of the implications of cognitive radio for future management of spectrum. Both a near-term view involving the opportunistic spectrum access model and a longer-term view involving a self-regulating dynamic spectrum access model within a society of cognitive radios are discussed.\n    ",
        "submission_date": "2009-03-02T00:00:00",
        "last_modified_date": "2009-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0279",
        "title": "An introduction to DSmT",
        "authors": [
            "Jean Dezert",
            "Florentin Smarandache"
        ],
        "abstract": "  The management and combination of uncertain, imprecise, fuzzy and even paradoxical or high conflicting sources of information has always been, and still remains today, of primal importance for the development of reliable modern information systems involving artificial reasoning. In this introduction, we present a survey of our recent theory of plausible and paradoxical reasoning, known as Dezert-Smarandache Theory (DSmT), developed for dealing with imprecise, uncertain and conflicting sources of information. We focus our presentation on the foundations of DSmT and on its most important rules of combination, rather than on browsing specific applications of DSmT available in literature. Several simple examples are given throughout this presentation to show the efficiency and the generality of this new approach.\n    ",
        "submission_date": "2009-03-02T00:00:00",
        "last_modified_date": "2009-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0314",
        "title": "Granularity-Adaptive Proof Presentation",
        "authors": [
            "Marvin Schiller",
            "Christoph Benzmueller"
        ],
        "abstract": "  When mathematicians present proofs they usually adapt their explanations to their didactic goals and to the (assumed) knowledge of their addressees. Modern automated theorem provers, in contrast, present proofs usually at a fixed level of detail (also called granularity). Often these presentations are neither intended nor suitable for human use. A challenge therefore is to develop user- and goal-adaptive proof presentation techniques that obey common mathematical practice. We present a flexible and adaptive approach to proof presentation that exploits machine learning techniques to extract a model of the specific granularity of proof examples and employs this model for the automated generation of further proofs at an adapted level of granularity.\n    ",
        "submission_date": "2009-03-02T00:00:00",
        "last_modified_date": "2009-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0422",
        "title": "Deductive Inference for the Interiors and Exteriors of Horn Theories",
        "authors": [
            "Kazuhisa Makino",
            "Hirotaka Ono"
        ],
        "abstract": "  In this paper, we investigate the deductive inference for the interiors and exteriors of Horn knowledge bases, where the interiors and exteriors were introduced by Makino and Ibaraki to study stability properties of knowledge bases. We present a linear time algorithm for the deduction for the interiors and show that it is co-NP-complete for the deduction for the exteriors. Under model-based representation, we show that the deduction problem for interiors is NP-complete while the one for exteriors is co-NP-complete. As for Horn envelopes of the exteriors, we show that it is linearly solvable under model-based representation, while it is co-NP-complete under formula-based representation. We also discuss the polynomially solvable cases for all the intractable problems.\n    ",
        "submission_date": "2009-03-03T00:00:00",
        "last_modified_date": "2009-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0460",
        "title": "Filtering Algorithms for the Multiset Ordering Constraint",
        "authors": [
            "Alan Frisch",
            "Brahim Hnich",
            "Zeynep Kiziltan",
            "Ian Miguel",
            "Toby Walsh"
        ],
        "abstract": "  Constraint programming (CP) has been used with great success to tackle a wide variety of constraint satisfaction problems which are computationally intractable in general. Global constraints are one of the important factors behind the success of CP. In this paper, we study a new global constraint, the multiset ordering constraint, which is shown to be useful in symmetry breaking and searching for leximin optimal solutions in CP. We propose efficient and effective filtering algorithms for propagating this global constraint. We show that the algorithms are sound and complete and we discuss possible extensions. We also consider alternative propagation methods based on existing constraints in CP toolkits. Our experimental results on a number of benchmark problems demonstrate that propagating the multiset ordering constraint via a dedicated algorithm can be very beneficial.\n    ",
        "submission_date": "2009-03-03T00:00:00",
        "last_modified_date": "2009-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0465",
        "title": "Breaking Value Symmetry",
        "authors": [
            "Toby Walsh"
        ],
        "abstract": "  Symmetry is an important factor in solving many constraint satisfaction problems. One common type of symmetry is when we have symmetric values. In a recent series of papers, we have studied methods to break value symmetries. Our results identify computational limits on eliminating value symmetry. For instance, we prove that pruning all symmetric values is NP-hard in general. Nevertheless, experiments show that much value symmetry can be broken in practice. These results may be useful to researchers in planning, scheduling and other areas as value symmetry occurs in many different domains.\n    ",
        "submission_date": "2009-03-03T00:00:00",
        "last_modified_date": "2009-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0467",
        "title": "The Parameterized Complexity of Global Constraints",
        "authors": [
            "Christian Bessiere",
            "Emmanuel Hebrard",
            "Brahim Hnich",
            "Zeynep Kiziltan",
            "Toby Walsh"
        ],
        "abstract": "  We argue that parameterized complexity is a useful tool with which to study global constraints. In particular, we show that many global constraints which are intractable to propagate completely have natural parameters which make them fixed-parameter tractable and which are easy to compute. This tractability tends either to be the result of a simple dynamic program or of a decomposition which has a strong backdoor of bounded size. This strong backdoor is often a cycle cutset. We also show that parameterized complexity can be used to study other aspects of constraint programming like symmetry breaking. For instance, we prove that value symmetry is fixed-parameter tractable to break in the number of symmetries. Finally, we argue that parameterized complexity can be used to derive results about the approximability of constraint propagation.\n    ",
        "submission_date": "2009-03-03T00:00:00",
        "last_modified_date": "2009-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0470",
        "title": "Decompositions of Grammar Constraints",
        "authors": [
            "Claude-Guy Quimper",
            "Toby Walsh"
        ],
        "abstract": "  A wide range of constraints can be compactly specified using automata or formal languages. In a sequence of recent papers, we have shown that an effective means to reason with such specifications is to decompose them into primitive constraints. We can then, for instance, use state of the art SAT solvers and profit from their advanced features like fast unit propagation, clause learning, and conflict-based search heuristics. This approach holds promise for solving combinatorial problems in scheduling, rostering, and configuration, as well as problems in more diverse areas like bioinformatics, software testing and natural language processing. In addition, decomposition may be an effective method to propagate other global constraints.\n    ",
        "submission_date": "2009-03-03T00:00:00",
        "last_modified_date": "2009-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0471",
        "title": "SLIDE: A Useful Special Case of the CARDPATH Constraint",
        "authors": [
            "Christian Bessiere",
            "Emmanuel Hebrard",
            "Brahim Hnich",
            "Zeynep Kiziltan",
            "Toby Walsh"
        ],
        "abstract": "  We study the CardPath constraint. This ensures a given constraint holds a number of times down a sequence of variables. We show that SLIDE, a special case of CardPath where the slid constraint must hold always, can be used to encode a wide range of sliding sequence constraints including CardPath itself. We consider how to propagate SLIDE and provide a complete propagator for CardPath. Since propagation is NP-hard in general, we identify special cases where propagation takes polynomial time. Our experiments demonstrate that using SLIDE to encode global constraints can be as efficient and effective as specialised propagators.\n    ",
        "submission_date": "2009-03-03T00:00:00",
        "last_modified_date": "2009-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0475",
        "title": "Reformulating Global Grammar Constraints",
        "authors": [
            "George Katsirelos",
            "Nina Narodytska",
            "Toby Walsh"
        ],
        "abstract": "  An attractive mechanism to specify global constraints in rostering and other domains is via formal languages. For instance, the Regular and Grammar constraints specify constraints in terms of the languages accepted by an automaton and a context-free grammar respectively. Taking advantage of the fixed length of the constraint, we give an algorithm to transform a context-free grammar into an automaton. We then study the use of minimization techniques to reduce the size of such automata and speed up propagation. We show that minimizing such automata after they have been unfolded and domains initially reduced can give automata that are more compact than minimizing before unfolding and reducing. Experimental results show that such transformations can improve the size of rostering problems that we can 'model and run'.\n    ",
        "submission_date": "2009-03-03T00:00:00",
        "last_modified_date": "2009-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0479",
        "title": "Combining Symmetry Breaking and Global Constraints",
        "authors": [
            "George Katsirelos",
            "Nina Narodytska",
            "Toby Walsh"
        ],
        "abstract": "  We propose a new family of constraints which combine together lexicographical ordering constraints for symmetry breaking with other common global constraints. We give a general purpose propagator for this family of constraints, and show how to improve its complexity by exploiting properties of the included global constraints.\n    ",
        "submission_date": "2009-03-03T00:00:00",
        "last_modified_date": "2009-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0695",
        "title": "Online Estimation of SAT Solving Runtime",
        "authors": [
            "Shai Haim",
            "Toby Walsh"
        ],
        "abstract": "  We present an online method for estimating the cost of solving SAT problems. Modern SAT solvers present several challenges to estimate search cost including non-chronological backtracking, learning and restarts. Our method uses a linear model trained on data gathered at the start of search. We show the effectiveness of this method using random and structured problems. We demonstrate that predictions made in early restarts can be used to improve later predictions. We also show that we can use such cost estimations to select a solver from a portfolio.\n    ",
        "submission_date": "2009-03-04T00:00:00",
        "last_modified_date": "2009-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0735",
        "title": "Modeling the Experience of Emotion",
        "authors": [
            "Joost Broekens"
        ],
        "abstract": "  Affective computing has proven to be a viable field of research comprised of a large number of multidisciplinary researchers resulting in work that is widely published. The majority of this work consists of computational models of emotion recognition, computational modeling of causal factors of emotion and emotion expression through rendered and robotic faces. A smaller part is concerned with modeling the effects of emotion, formal modeling of cognitive appraisal theory and models of emergent emotions. Part of the motivation for affective computing as a field is to better understand emotional processes through computational modeling. One of the four major topics in affective computing is computers that have emotions (the others are recognizing, expressing and understanding emotions). A critical and neglected aspect of having emotions is the experience of emotion (Barrett, Mesquita, Ochsner, and Gross, 2007): what does the content of an emotional episode look like, how does this content change over time and when do we call the episode emotional. Few modeling efforts have these topics as primary focus. The launch of a journal on synthetic emotions should motivate research initiatives in this direction, and this research should have a measurable impact on emotion research in psychology. I show that a good way to do so is to investigate the psychological core of what an emotion is: an experience. I present ideas on how the experience of emotion could be modeled and provide evidence that several computational models of emotion are already addressing the issue.\n    ",
        "submission_date": "2009-03-04T00:00:00",
        "last_modified_date": "2009-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0786",
        "title": "On Requirements for Programming Exercises from an E-learning Perspective",
        "authors": [
            "Carlos Loria-Saenz"
        ],
        "abstract": "  In this work, we deal with the question of modeling programming exercises for novices pointing to an e-learning scenario. Our purpose is to identify basic requirements, raise some key questions and propose potential answers from a conceptual perspective. Presented as a general picture, we hypothetically situate our work in a general context where e-learning instructional material needs to be adapted to form part of an introductory Computer Science (CS) e-learning course at the CS1-level. Meant is a potential course which aims at improving novices skills and knowledge on the essentials of programming by using e-learning based approaches in connection (at least conceptually) with a general host framework like Activemath (",
        "submission_date": "2009-03-04T00:00:00",
        "last_modified_date": "2009-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0829",
        "title": "Tagging multimedia stimuli with ontologies",
        "authors": [
            "Marko Horvat",
            "Sinisa Popovic",
            "Nikola Bogunovic",
            "Kresimir Cosic"
        ],
        "abstract": "  Successful management of emotional stimuli is a pivotal issue concerning Affective Computing (AC) and the related research. As a subfield of Artificial Intelligence, AC is concerned not only with the design of computer systems and the accompanying hardware that can recognize, interpret, and process human emotions, but also with the development of systems that can trigger human emotional response in an ordered and controlled manner. This requires the maximum attainable precision and efficiency in the extraction of data from emotionally annotated databases While these databases do use keywords or tags for description of the semantic content, they do not provide either the necessary flexibility or leverage needed to efficiently extract the pertinent emotional content. Therefore, to this extent we propose an introduction of ontologies as a new paradigm for description of emotionally annotated data. The ability to select and sequence data based on their semantic attributes is vital for any study involving metadata, semantics and ontological sorting like the Semantic Web or the Social Semantic Desktop, and the approach described in the paper facilitates reuse in these areas as well.\n    ",
        "submission_date": "2009-03-04T00:00:00",
        "last_modified_date": "2009-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0843",
        "title": "Algorithms for Weighted Boolean Optimization",
        "authors": [
            "Vasco Manquinho",
            "Joao Marques-Silva",
            "Jordi Planes"
        ],
        "abstract": "  The Pseudo-Boolean Optimization (PBO) and Maximum Satisfiability (MaxSAT) problems are natural optimization extensions of Boolean Satisfiability (SAT).\n",
        "submission_date": "2009-03-04T00:00:00",
        "last_modified_date": "2009-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.1136",
        "title": "Symmetry Breaking Using Value Precedence",
        "authors": [
            "Toby Walsh"
        ],
        "abstract": "  We present a comprehensive study of the use of value precedence constraints to break value symmetry. We first give a simple encoding of value precedence into ternary constraints that is both efficient and effective at breaking symmetry. We then extend value precedence to deal with a number of generalizations like wreath value and partial interchangeability. We also show that value precedence is closely related to lexicographical ordering. Finally, we consider the interaction between value precedence and symmetry breaking constraints for variable symmetries.\n    ",
        "submission_date": "2009-03-06T00:00:00",
        "last_modified_date": "2009-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.1137",
        "title": "Complexity of Terminating Preference Elicitation",
        "authors": [
            "Toby Walsh"
        ],
        "abstract": "  Complexity theory is a useful tool to study computational issues surrounding the elicitation of preferences, as well as the strategic manipulation of elections aggregating together preferences of multiple agents. We study here the complexity of determining when we can terminate eliciting preferences, and prove that the complexity depends on the elicitation strategy. We show, for instance, that it may be better from a computational perspective to elicit all preferences from one agent at a time than to elicit individual preferences from multiple agents. We also study the connection between the strategic manipulation of an election and preference elicitation. We show that what we can manipulate affects the computational complexity of manipulation. In particular, we prove that there are voting rules which are easy to manipulate if we can change all of an agent's vote, but computationally intractable if we can change only some of their preferences. This suggests that, as with preference elicitation, a fine-grained view of manipulation may be informative. Finally, we study the connection between predicting the winner of an election and preference elicitation. Based on this connection, we identify a voting rule where it is computationally difficult to decide the probability of a candidate winning given a probability distribution over the votes.\n    ",
        "submission_date": "2009-03-06T00:00:00",
        "last_modified_date": "2009-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.1139",
        "title": "The Complexity of Reasoning with Global Constraints",
        "authors": [
            "Christian Bessiere",
            "Emmanuel Hebrard",
            "Brahim Hnich",
            "Toby Walsh"
        ],
        "abstract": "  Constraint propagation is one of the techniques central to the success of constraint programming. To reduce search, fast algorithms associated with each constraint prune the domains of variables. With global (or non-binary) constraints, the cost of such propagation may be much greater than the quadratic cost for binary constraints. We therefore study the computational complexity of reasoning with global constraints. We first characterise a number of important questions related to constraint propagation. We show that such questions are intractable in general, and identify dependencies between the tractability and intractability of the different questions. We then demonstrate how the tools of computational complexity can be used in the design and analysis of specific global constraints. In particular, we illustrate how computational complexity can be used to determine when a lesser level of local consistency should be enforced, when constraints can be safely generalized, when decomposing constraints will reduce the amount of pruning, and when combining constraints is tractable.\n    ",
        "submission_date": "2009-03-06T00:00:00",
        "last_modified_date": "2009-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.1146",
        "title": "Breaking Value Symmetry",
        "authors": [
            "Toby Walsh"
        ],
        "abstract": "  One common type of symmetry is when values are symmetric. For example, if we are assigning colours (values) to nodes (variables) in a graph colouring problem then we can uniformly interchange the colours throughout a colouring. For a problem with value symmetries, all symmetric solutions can be eliminated in polynomial time. However, as we show here, both static and dynamic methods to deal with symmetry have computational limitations. With static methods, pruning all symmetric values is NP-hard in general. With dynamic methods, we can take exponential time on problems which static methods solve without search.\n    ",
        "submission_date": "2009-03-06T00:00:00",
        "last_modified_date": "2009-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.1150",
        "title": "Stochastic Constraint Programming: A Scenario-Based Approach",
        "authors": [
            "S. Armagan Tarim",
            "Suresh Manandhar",
            "Toby Walsh"
        ],
        "abstract": "  To model combinatorial decision problems involving uncertainty and probability, we introduce scenario based stochastic constraint programming. Stochastic constraint programs contain both decision variables, which we can set, and stochastic variables, which follow a discrete probability distribution. We provide a semantics for stochastic constraint programs based on scenario trees. Using this semantics, we can compile stochastic constraint programs down into conventional (non-stochastic) constraint programs. This allows us to exploit the full power of existing constraint solvers. We have implemented this framework for decision making under uncertainty in stochastic OPL, a language which is based on the OPL constraint modelling language [Hentenryck et al., 1999]. To illustrate the potential of this framework, we model a wide range of problems in areas as diverse as portfolio diversification, agricultural planning and production/inventory management.\n    ",
        "submission_date": "2009-03-06T00:00:00",
        "last_modified_date": "2009-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.1152",
        "title": "Stochastic Constraint Programming",
        "authors": [
            "Toby Walsh"
        ],
        "abstract": "  To model combinatorial decision problems involving uncertainty and probability, we introduce stochastic constraint programming. Stochastic constraint programs contain both decision variables (which we can set) and stochastic variables (which follow a probability distribution). They combine together the best features of traditional constraint satisfaction, stochastic integer programming, and stochastic satisfiability. We give a semantics for stochastic constraint programs, and propose a number of complete algorithms and approximation procedures. Finally, we discuss a number of extensions of stochastic constraint programming to relax various assumptions like the independence between stochastic variables, and compare with other approaches for decision making under uncertainty.\n    ",
        "submission_date": "2009-03-06T00:00:00",
        "last_modified_date": "2009-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.1451",
        "title": "Definition of evidence fusion rules on the basis of Referee Functions",
        "authors": [
            "Frederic Dambreville"
        ],
        "abstract": "  This chapter defines a new concept and framework for constructing fusion rules for evidences. This framework is based on a referee function, which does a decisional arbitrament conditionally to basic decisions provided by the several sources of information. A simple sampling method is derived from this framework. The purpose of this sampling approach is to avoid the combinatorics which are inherent to the definition of fusion rules of evidences. This definition of the fusion rule by the means of a sampling process makes possible the construction of several rules on the basis of an algorithmic implementation of the referee function, instead of a mathematical formulation. Incidentally, it is a versatile and intuitive way for defining rules. The framework is implemented for various well known evidence rules. On the basis of this framework, new rules for combining evidences are proposed, which takes into account a consensual evaluation of the sources of information.\n    ",
        "submission_date": "2009-03-08T00:00:00",
        "last_modified_date": "2009-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.1659",
        "title": "Heuristic Reasoning on Graph and Game Complexity of Sudoku",
        "authors": [
            "Zhe Chen"
        ],
        "abstract": "  The Sudoku puzzle has achieved worldwide popularity recently, and attracted great attention of the computational intelligence community. Sudoku is always considered as Satisfiability Problem or Constraint Satisfaction Problem. In this paper, we propose to focus on the essential graph structure underlying the Sudoku puzzle. First, we formalize Sudoku as a graph. Then a solving algorithm based on heuristic reasoning on the graph is proposed. The related r-Reduction theorem, inference theorem and their properties are proved, providing the formal basis for developments of Sudoku solving systems. In order to evaluate the difficulty levels of puzzles, a quantitative measurement of the complexity level of Sudoku puzzles based on the graph structure and information theory is proposed. Experimental results show that all the puzzles can be solved fast using the proposed heuristic reasoning, and that the proposed game complexity metrics can discriminate difficulty levels of puzzles perfectly.\n    ",
        "submission_date": "2009-03-09T00:00:00",
        "last_modified_date": "2009-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.1878",
        "title": "Contracting preference relations for database applications",
        "authors": [
            "Denis Mindolin",
            "Jan Chomicki"
        ],
        "abstract": "  The binary relation framework has been shown to be applicable to many real-life preference handling scenarios. Here we study preference contraction: the problem of discarding selected preferences. We argue that the property of minimality and the preservation of strict partial orders are crucial for contractions. Contractions can be further constrained by specifying which preferences should be protected. We consider two classes of preference relations: finite and finitely representable. We present algorithms for computing minimal and preference-protecting minimal contractions for finite as well as finitely representable preference relations. We study relationships between preference change in the binary relation framework and belief change in the belief revision theory. We also introduce some preference query optimization techniques which can be used in the presence of contraction. We evaluate the proposed algorithms experimentally and present the results.\n    ",
        "submission_date": "2009-03-10T00:00:00",
        "last_modified_date": "2009-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.2528",
        "title": "Airport Gate Assignment A Hybrid Model and Implementation",
        "authors": [
            "Chendong Li"
        ],
        "abstract": "  With the rapid development of airlines, airports today become much busier and more complicated than previous days. During airlines daily operations, assigning the available gates to the arriving aircrafts based on the fixed schedule is a very important issue, which motivates researchers to study and solve Airport Gate Assignment Problems (AGAP) with all kinds of state-of-the-art combinatorial optimization techniques. In this paper, we study the AGAP and propose a novel hybrid mathematical model based on the method of constraint programming and 0 - 1 mixed-integer programming. With the objective to minimize the number of gate conflicts of any two adjacent aircrafts assigned to the same gate, we build a mathematical model with logical constraints and the binary constraints. For practical considerations, the potential objective of the model is also to minimize the number of gates that airlines must lease or purchase in order to run their business smoothly. We implement the model in the Optimization Programming Language (OPL) and carry out empirical studies with the data obtained from online timetable of Continental Airlines, Houston Gorge Bush Intercontinental Airport IAH, which demonstrate that our model can provide an efficient evaluation criteria for the airline companies to estimate the efficiency of their current gate assignments.\n    ",
        "submission_date": "2009-03-14T00:00:00",
        "last_modified_date": "2009-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.3127",
        "title": "Norm-Product Belief Propagation: Primal-Dual Message-Passing for Approximate Inference",
        "authors": [
            "Tamir Hazan",
            "Amnon Shashua"
        ],
        "abstract": "In this paper we treat both forms of probabilistic inference, estimating marginal probabilities of the joint distribution and finding the most probable assignment, through a unified message-passing algorithm architecture. We generalize the Belief Propagation (BP) algorithms of sum-product and max-product and tree-rewaighted (TRW) sum and max product algorithms (TRBP) and introduce a new set of convergent algorithms based on \"convex-free-energy\" and Linear-Programming (LP) relaxation as a zero-temprature of a convex-free-energy. The main idea of this work arises from taking a general perspective on the existing BP and TRBP algorithms while observing that they all are reductions from the basic optimization formula of $f + \\sum_i h_i$ where the function $f$ is an extended-valued, strictly convex but non-smooth and the functions $h_i$ are extended-valued functions (not necessarily convex). We use tools from convex duality to present the \"primal-dual ascent\" algorithm which is an extension of the Bregman successive projection scheme and is designed to handle optimization of the general type $f + \\sum_i h_i$. Mapping the fractional-free-energy variational principle to this framework introduces the \"norm-product\" message-passing. Special cases include sum-product and max-product (BP algorithms) and the TRBP algorithms. When the fractional-free-energy is set to be convex (convex-free-energy) the norm-product is globally convergent for estimating of marginal probabilities and for approximating the LP-relaxation. We also introduce another branch of the norm-product, the \"convex-max-product\". The convex-max-product is convergent (unlike max-product) and aims at solving the LP-relaxation.\n    ",
        "submission_date": "2009-03-18T00:00:00",
        "last_modified_date": "2010-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.3669",
        "title": "Comment on \"Language Trees and Zipping\" ",
        "authors": [
            "Xiuli Wang"
        ],
        "abstract": "  Every encoding has priori information if the encoding represents any semantic information of the unverse or object. Encoding means mapping from the unverse to the string or strings of digits. The semantic here is used in the model-theoretic sense or denotation of the object. If encoding or strings of symbols is the adequate and true mapping of model or object, and the mapping is recursive or computable, the distance between two strings (text) is mapping the distance between models. We then are able to measure the distance by computing the distance between the two strings. Otherwise, we may take a misleading course. \"Language tree\" may not be a family tree in the sense of historical linguistics. Rather it just means the similarity.\n    ",
        "submission_date": "2009-03-21T00:00:00",
        "last_modified_date": "2009-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.3926",
        "title": "Designing a GUI for Proofs - Evaluation of an HCI Experiment",
        "authors": [
            "Martin Homik",
            "Andreas Meier"
        ],
        "abstract": "  Often user interfaces of theorem proving systems focus on assisting particularly trained and skilled users, i.e., proof experts. As a result, the systems are difficult to use for non-expert users. This paper describes a paper and pencil HCI experiment, in which (non-expert) students were asked to make suggestions for a GUI for an interactive system for mathematical proofs. They had to explain the usage of the GUI by applying it to construct a proof sketch for a given theorem. The evaluation of the experiment provides insights for the interaction design for non-expert users and the needs and wants of this user group.\n    ",
        "submission_date": "2009-03-23T00:00:00",
        "last_modified_date": "2009-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.4132",
        "title": "Switcher-random-walks: a cognitive-inspired mechanism for network exploration",
        "authors": [
            "Joaqu\u00edn Go\u00f1i",
            "I\u00f1igo Martincorena",
            "Bernat Corominas-Murtra",
            "Gonzalo Arrondo",
            "Sergio Ardanza-Trevijano",
            "Pablo Villoslada"
        ],
        "abstract": "  Semantic memory is the subsystem of human memory that stores knowledge of concepts or meanings, as opposed to life specific experiences. The organization of concepts within semantic memory can be understood as a semantic network, where the concepts (nodes) are associated (linked) to others depending on perceptions, similarities, etc. Lexical access is the complementary part of this system and allows the retrieval of such organized knowledge. While conceptual information is stored under certain underlying organization (and thus gives rise to a specific topology), it is crucial to have an accurate access to any of the information units, e.g. the concepts, for efficiently retrieving semantic information for real-time needings. An example of an information retrieval process occurs in verbal fluency tasks, and it is known to involve two different mechanisms: -clustering-, or generating words within a subcategory, and, when a subcategory is exhausted, -switching- to a new subcategory. We extended this approach to random-walking on a network (clustering) in combination to jumping (switching) to any node with certain probability and derived its analytical expression based on Markov chains. Results show that this dual mechanism contributes to optimize the exploration of different network models in terms of the mean first passage time. Additionally, this cognitive inspired dual mechanism opens a new framework to better understand and evaluate exploration, propagation and transport phenomena in other complex systems where switching-like phenomena are feasible.\n    ",
        "submission_date": "2009-03-24T00:00:00",
        "last_modified_date": "2009-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.4930",
        "title": "Time manipulation technique for speeding up reinforcement learning in simulations",
        "authors": [
            "Petar Kormushev",
            "Kohei Nomoto",
            "Fangyan Dong",
            "Kaoru Hirota"
        ],
        "abstract": "  A technique for speeding up reinforcement learning algorithms by using time manipulation is proposed. It is applicable to failure-avoidance control problems running in a computer simulation. Turning the time of the simulation backwards on failure events is shown to speed up the learning by 260% and improve the state space exploration by 12% on the cart-pole balancing task, compared to the conventional Q-learning and Actor-Critic algorithms.\n    ",
        "submission_date": "2009-03-28T00:00:00",
        "last_modified_date": "2009-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.5054",
        "title": "Flow of Activity in the Ouroboros Model",
        "authors": [
            "Knud Thomsen"
        ],
        "abstract": "  The Ouroboros Model is a new conceptual proposal for an algorithmic structure for efficient data processing in living beings as well as for artificial agents. Its central feature is a general repetitive loop where one iteration cycle sets the stage for the next. Sensory input activates data structures (schemata) with similar constituents encountered before, thus expectations are kindled. This corresponds to the highlighting of empty slots in the selected schema, and these expectations are compared with the actually encountered input. Depending on the outcome of this consumption analysis different next steps like search for further data or a reset, i.e. a new attempt employing another schema, are triggered. Monitoring of the whole process, and in particular of the flow of activation directed by the consumption analysis, yields valuable feedback for the optimum allocation of attention and resources including the selective establishment of useful new memory entries.\n    ",
        "submission_date": "2009-03-29T00:00:00",
        "last_modified_date": "2009-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.5289",
        "title": "Heterogeneous knowledge representation using a finite automaton and first order logic: a case study in electromyography",
        "authors": [
            "Vincent Rialle",
            "Annick Vila",
            "Yves Besnard"
        ],
        "abstract": "  In a certain number of situations, human cognitive functioning is difficult to represent with classical artificial intelligence structures. Such a difficulty arises in the polyneuropathy diagnosis which is based on the spatial distribution, along the nerve fibres, of lesions, together with the synthesis of several partial diagnoses. Faced with this problem while building up an expert system (NEUROP), we developed a heterogeneous knowledge representation associating a finite automaton with first order logic. A number of knowledge representation problems raised by the electromyography test features are examined in this study and the expert system architecture allowing such a knowledge modeling are laid out.\n    ",
        "submission_date": "2009-03-30T00:00:00",
        "last_modified_date": "2009-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.0029",
        "title": "Learning for Dynamic subsumption",
        "authors": [
            "Youssef Hamadi",
            "Said Jabbour",
            "Lakhdar Sais"
        ],
        "abstract": "  In this paper a new dynamic subsumption technique for Boolean CNF formulae is proposed. It exploits simple and sufficient conditions to detect during conflict analysis, clauses from the original formula that can be reduced by subsumption. During the learnt clause derivation, and at each step of the resolution process, we simply check for backward subsumption between the current resolvent and clauses from the original formula and encoded in the implication graph. Our approach give rise to a strong and dynamic simplification technique that exploits learning to eliminate literals from the original clauses. Experimental results show that the integration of our dynamic subsumption approach within the state-of-the-art SAT solvers Minisat and Rsat achieves interesting improvements particularly on crafted instances.\n    ",
        "submission_date": "2009-03-31T00:00:00",
        "last_modified_date": "2009-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.0228",
        "title": "Safe Reasoning Over Ontologies",
        "authors": [
            "Genady Grabarnik",
            "Aaron Kershenbaum"
        ],
        "abstract": "  As ontologies proliferate and automatic reasoners become more powerful, the problem of protecting sensitive information becomes more serious. In particular, as facts can be inferred from other facts, it becomes increasingly likely that information included in an ontology, while not itself deemed sensitive, may be able to be used to infer other sensitive information.\n",
        "submission_date": "2009-04-01T00:00:00",
        "last_modified_date": "2009-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.0300",
        "title": "Design, development and implementation of a tool for construction of declarative functional descriptions of semantic web services based on WSMO methodology",
        "authors": [
            "Petar Kormushev"
        ],
        "abstract": "  Semantic web services (SWS) are self-contained, self-describing, semantically marked-up software resources that can be published, discovered, composed and executed across the Web in a semi-automatic way. They are a key component of the future Semantic Web, in which networked computer programs become providers and users of information at the same time. This work focuses on developing a full-life-cycle software toolset for creating and maintaining Semantic Web Services (SWSs) based on the Web Service Modelling Ontology (WSMO) framework. A main part of WSMO-based SWS is service capability - a declarative description of Web service functionality. A formal syntax and semantics for such a description is provided by Web Service Modeling Language (WSML), which is based on different logical formalisms, namely, Description Logics, First-Order Logic and Logic Programming. A WSML description of a Web service capability is represented as a set of complex logical expressions (axioms). We develop a specialized user-friendly tool for constructing and editing WSMO-based SWS capabilities. Since the users of this tool are not specialists in first-order logic, a graphical way for constricting and editing axioms is proposed. The designed process for constructing logical expressions is ontology-driven, which abstracts away as much as possible from any concrete syntax of logical language. We propose several mechanisms to guarantees the semantic consistency of the produced logical expressions. The tool is implemented in Java using Eclipse for IDE and GEF (Graphical Editing Framework) for visualization.\n    ",
        "submission_date": "2009-04-02T00:00:00",
        "last_modified_date": "2009-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.0545",
        "title": "Time Hopping technique for faster reinforcement learning in simulations",
        "authors": [
            "Petar Kormushev",
            "Kohei Nomoto",
            "Fangyan Dong",
            "Kaoru Hirota"
        ],
        "abstract": "This preprint has been withdrawn by the author for revision\n    ",
        "submission_date": "2009-04-03T00:00:00",
        "last_modified_date": "2011-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.0546",
        "title": "Eligibility Propagation to Speed up Time Hopping for Reinforcement Learning",
        "authors": [
            "Petar Kormushev",
            "Kohei Nomoto",
            "Fangyan Dong",
            "Kaoru Hirota"
        ],
        "abstract": "  A mechanism called Eligibility Propagation is proposed to speed up the Time Hopping technique used for faster Reinforcement Learning in simulations. Eligibility Propagation provides for Time Hopping similar abilities to what eligibility traces provide for conventional Reinforcement Learning. It propagates values from one state to all of its temporal predecessors using a state transitions graph. Experiments on a simulated biped crawling robot confirm that Eligibility Propagation accelerates the learning process more than 3 times.\n    ",
        "submission_date": "2009-04-03T00:00:00",
        "last_modified_date": "2009-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.0643",
        "title": "Performing Nonlinear Blind Source Separation with Signal Invariants",
        "authors": [
            "David N. Levin"
        ],
        "abstract": "  Given a time series of multicomponent measurements x(t), the usual objective of nonlinear blind source separation (BSS) is to find a \"source\" time series s(t), comprised of statistically independent combinations of the measured components. In this paper, the source time series is required to have a density function in (s,ds/dt)-space that is equal to the product of density functions of individual components. This formulation of the BSS problem has a solution that is unique, up to permutations and component-wise transformations. Separability is shown to impose constraints on certain locally invariant (scalar) functions of x, which are derived from local higher-order correlations of the data's velocity dx/dt. The data are separable if and only if they satisfy these constraints, and, if the constraints are satisfied, the sources can be explicitly constructed from the data. The method is illustrated by using it to separate two speech-like sounds recorded with a single microphone.\n    ",
        "submission_date": "2009-04-03T00:00:00",
        "last_modified_date": "2009-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.1258",
        "title": "An Investigation Report on Auction Mechanism Design",
        "authors": [
            "Jinzhong Niu",
            "Simon Parsons"
        ],
        "abstract": "  Auctions are markets with strict regulations governing the information available to traders in the market and the possible actions they can take. Since well designed auctions achieve desirable economic outcomes, they have been widely used in solving real-world optimization problems, and in structuring stock or futures exchanges. Auctions also provide a very valuable testing-ground for economic theory, and they play an important role in computer-based control systems.\n",
        "submission_date": "2009-04-08T00:00:00",
        "last_modified_date": "2009-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.1579",
        "title": "Online prediction of ovarian cancer",
        "authors": [
            "Fedor Zhdanov",
            "Vladimir Vovk",
            "Brian Burford",
            "Dmitry Devetyarov",
            "Ilia Nouretdinov",
            "Alex Gammerman"
        ],
        "abstract": "  In this paper we apply computer learning methods to diagnosing ovarian cancer using the level of the standard biomarker CA125 in conjunction with information provided by mass-spectrometry. We are working with a new data set collected over a period of 7 years. Using the level of CA125 and mass-spectrometry peaks, our algorithm gives probability predictions for the disease. To estimate classification accuracy we convert probability predictions into strict predictions. Our algorithm makes fewer errors than almost any linear combination of the CA125 level and one peak's intensity (taken on the log scale). To check the power of our algorithm we use it to test the hypothesis that CA125 and the peaks do not contain useful information for the prediction of the disease at a particular time before the diagnosis. Our algorithm produces $p$-values that are better than those produced by the algorithm that has been previously applied to this data set. Our conclusion is that the proposed algorithm is more reliable for prediction on new data.\n    ",
        "submission_date": "2009-04-09T00:00:00",
        "last_modified_date": "2009-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.1672",
        "title": "CP-logic: A Language of Causal Probabilistic Events and Its Relation to Logic Programming",
        "authors": [
            "Joost Vennekens",
            "Marc Denecker",
            "Maurice Bruynooghe"
        ],
        "abstract": "  This papers develops a logical language for representing probabilistic causal laws. Our interest in such a language is twofold. First, it can be motivated as a fundamental study of the representation of causal knowledge. Causality has an inherent dynamic aspect, which has been studied at the semantical level by Shafer in his framework of probability trees. In such a dynamic context, where the evolution of a domain over time is considered, the idea of a causal law as something which guides this evolution is quite natural. In our formalization, a set of probabilistic causal laws can be used to represent a class of probability trees in a concise, flexible and modular way. In this way, our work extends Shafer's by offering a convenient logical representation for his semantical objects.\n",
        "submission_date": "2009-04-10T00:00:00",
        "last_modified_date": "2009-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.2595",
        "title": "A Methodology for Learning Players' Styles from Game Records",
        "authors": [
            "Mark Levene",
            "Trevor Fenner"
        ],
        "abstract": "  We describe a preliminary investigation into learning a Chess player's style from game records. The method is based on attempting to learn features of a player's individual evaluation function using the method of temporal differences, with the aid of a conventional Chess engine architecture. Some encouraging results were obtained in learning the styles of two recent Chess world champions, and we report on our attempt to use the learnt styles to discriminate between the players from game records by trying to detect who was playing white and who was playing black. We also discuss some limitations of our approach and propose possible directions for future research. The method we have presented may also be applicable to other strategic games, and may even be generalisable to other domains where sequences of agents' actions are recorded.\n    ",
        "submission_date": "2009-04-16T00:00:00",
        "last_modified_date": "2009-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.2827",
        "title": "Principle of development",
        "authors": [
            "Elena S. Vishnevksaya"
        ],
        "abstract": "  Today, science have a powerful tool for the description of reality - the numbers. However, the concept of number was not immediately, lets try to trace the evolution of the concept. The numbers emerged as the need for accurate estimates of the amount in order to permit a comparison of some objects. So if you see to it how many times a day a person uses the numbers and compare, it becomes evident that the comparison is used much more frequently. However, the comparison is not possible without two opposite basic standards. Thus, to introduce the concept of comparison, must have two opposing standards, in turn, the operation of comparison is necessary to introduce the concept of number. Arguably, the scientific description of reality is impossible without the concept of opposites.\n",
        "submission_date": "2009-04-18T00:00:00",
        "last_modified_date": "2011-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.2953",
        "title": "Towards an Intelligent System for Risk Prevention and Management",
        "authors": [
            "Fahem Kebair",
            "Frederic Serin"
        ],
        "abstract": "  Making a decision in a changeable and dynamic environment is an arduous task owing to the lack of information, their uncertainties and the unawareness of planners about the future evolution of incidents. The use of a decision support system is an efficient solution of this issue. Such a system can help emergency planners and responders to detect possible emergencies, as well as to suggest and evaluate possible courses of action to deal with the emergency. We are interested in our work to the modeling of a monitoring preventive and emergency management system, wherein we stress the generic aspect. In this paper we propose an agent-based architecture of this system and we describe a first step of our approach which is the modeling of information and their representation using a multiagent system.\n    ",
        "submission_date": "2009-04-20T00:00:00",
        "last_modified_date": "2009-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.2954",
        "title": "Agent-Based Decision Support System to Prevent and Manage Risk Situations",
        "authors": [
            "Fahem Kebair",
            "Frederic Serin"
        ],
        "abstract": "  The topic of risk prevention and emergency response has become a key social and political concern. One approach to address this challenge is to develop Decision Support Systems (DSS) that can help emergency planners and responders to detect emergencies, as well as to suggest possible course of actions to deal with the emergency. Our research work comes in this framework and aims to develop a DSS that must be generic as much as possible and independent from the case study.\n    ",
        "submission_date": "2009-04-20T00:00:00",
        "last_modified_date": "2009-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.3352",
        "title": "Optimistic Initialization and Greediness Lead to Polynomial Time Learning in Factored MDPs - Extended Version",
        "authors": [
            "Istvan Szita",
            "Andras Lorincz"
        ],
        "abstract": "  In this paper we propose an algorithm for polynomial-time reinforcement learning in factored Markov decision processes (FMDPs). The factored optimistic initial model (FOIM) algorithm, maintains an empirical model of the FMDP in a conventional way, and always follows a greedy policy with respect to its model. The only trick of the algorithm is that the model is initialized optimistically. We prove that with suitable initialization (i) FOIM converges to the fixed point of approximate value iteration (AVI); (ii) the number of steps when the agent makes non-near-optimal decisions (with respect to the solution of AVI) is polynomial in all relevant quantities; (iii) the per-step costs of the algorithm are also polynomial. To our best knowledge, FOIM is the first algorithm with these properties. This extended version contains the rigorous proofs of the main theorem. A version of this paper appeared in ICML'09.\n    ",
        "submission_date": "2009-04-21T00:00:00",
        "last_modified_date": "2009-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.3612",
        "title": "Variations of the Turing Test in the Age of Internet and Virtual Reality",
        "authors": [
            "Florentin Neumann",
            "Andrea Reichenberger",
            "Martin Ziegler"
        ],
        "abstract": "  Inspired by Hofstadter's Coffee-House Conversation (1982) and by the science fiction short story SAM by Schattschneider (1988), we propose and discuss criteria for non-mechanical intelligence. Firstly, we emphasize the practical need for such tests in view of massively multiuser online role-playing games (MMORPGs) and virtual reality systems like Second Life. Secondly, we demonstrate Second Life as a useful framework for implementing (some iterations of) that test.\n    ",
        "submission_date": "2009-04-23T00:00:00",
        "last_modified_date": "2009-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.3701",
        "title": "Semantic Social Network Analysis",
        "authors": [
            "Guillaume Er\u00e9t\u00e9o",
            "Fabien Gandon",
            "Olivier Corby",
            "Michel Buffa"
        ],
        "abstract": "  Social Network Analysis (SNA) tries to understand and exploit the key features of social networks in order to manage their life cycle and predict their evolution. Increasingly popular web 2.0 sites are forming huge social network. Classical methods from social network analysis (SNA) have been applied to such online networks. In this paper, we propose leveraging semantic web technologies to merge and exploit the best features of each domain. We present how to facilitate and enhance the analysis of online social networks, exploiting the power of semantic social network analysis.\n    ",
        "submission_date": "2009-04-23T00:00:00",
        "last_modified_date": "2009-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.3808",
        "title": "Automated Epilepsy Diagnosis Using Interictal Scalp EEG",
        "authors": [
            "Forrest Sheng Bao",
            "Jue-Ming Gao",
            "Jing Hu",
            "Donald Y.-C. Lie",
            "Yuanlin Zhang",
            "K. J. Oommen"
        ],
        "abstract": "  Approximately over 50 million people worldwide suffer from epilepsy. Traditional diagnosis of epilepsy relies on tedious visual screening by highly trained clinicians from lengthy EEG recording that contains the presence of seizure (ictal) activities. Nowadays, there are many automatic systems that can recognize seizure-related EEG signals to help the diagnosis. However, it is very costly and inconvenient to obtain long-term EEG data with seizure activities, especially in areas short of medical resources. We demonstrate in this paper that we can use the interictal scalp EEG data, which is much easier to collect than the ictal data, to automatically diagnose whether a person is epileptic. In our automated EEG recognition system, we extract three classes of features from the EEG data and build Probabilistic Neural Networks (PNNs) fed with these features. We optimize the feature extraction parameters and combine these PNNs through a voting mechanism. As a result, our system achieves an impressive 94.07% accuracy, which is very close to reported human recognition accuracy by experienced medical professionals.\n    ",
        "submission_date": "2009-04-24T00:00:00",
        "last_modified_date": "2009-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.3953",
        "title": "Guarded resolution for answer set programming",
        "authors": [
            "V.W. Marek",
            "J.B. Remmel"
        ],
        "abstract": "  We describe a variant of resolution rule of proof and show that it is complete for stable semantics of logic programs. We show applications of this result.\n    ",
        "submission_date": "2009-04-25T00:00:00",
        "last_modified_date": "2010-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.4587",
        "title": "Adaptive Learning with Binary Neurons",
        "authors": [
            "Juan-Manuel Torres-Moreno",
            "Mirta B. Gordon"
        ],
        "abstract": "  A efficient incremental learning algorithm for classification tasks, called NetLines, well adapted for both binary and real-valued input patterns is presented. It generates small compact feedforward neural networks with one hidden layer of binary units and binary output units. A convergence theorem ensures that solutions with a finite number of hidden units exist for both binary and real-valued input patterns. An implementation for problems with more than two classes, valid for any binary classifier, is proposed. The generalization error and the size of the resulting networks are compared to the best published results on well-known classification benchmarks. Early stopping is shown to decrease overfitting, without improving the generalization performance.\n    ",
        "submission_date": "2009-04-29T00:00:00",
        "last_modified_date": "2009-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.4727",
        "title": "Characterizations of Stable Model Semantics for Logic Programs with Arbitrary Constraint Atoms",
        "authors": [
            "Yi-Dong Shen",
            "Jia-Huai You",
            "Li-Yan Yuan"
        ],
        "abstract": "  This paper studies the stable model semantics of logic programs with (abstract) constraint atoms and their properties. We introduce a succinct abstract representation of these constraint atoms in which a constraint atom is represented compactly. We show two applications. First, under this representation of constraint atoms, we generalize the Gelfond-Lifschitz transformation and apply it to define stable models (also called answer sets) for logic programs with arbitrary constraint atoms. The resulting semantics turns out to coincide with the one defined by Son et al., which is based on a fixpoint approach. One advantage of our approach is that it can be applied, in a natural way, to define stable models for disjunctive logic programs with constraint atoms, which may appear in the disjunctive head as well as in the body of a rule. As a result, our approach to the stable model semantics for logic programs with constraint atoms generalizes a number of previous approaches. Second, we show that our abstract representation of constraint atoms provides a means to characterize dependencies of atoms in a program with constraint atoms, so that some standard characterizations and properties relying on these dependencies in the past for logic programs with ordinary atoms can be extended to logic programs with constraint atoms.\n    ",
        "submission_date": "2009-04-30T00:00:00",
        "last_modified_date": "2009-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.0192",
        "title": "Fuzzy Mnesors",
        "authors": [
            "Gilles Champenois"
        ],
        "abstract": "  A fuzzy mnesor space is a semimodule over the positive real numbers. It can be used as theoretical framework for fuzzy sets. Hence we can prove a great number of properties for fuzzy sets without refering to the membership functions.\n    ",
        "submission_date": "2009-05-02T00:00:00",
        "last_modified_date": "2009-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.0197",
        "title": "An Application of Proof-Theory in Answer Set Programming",
        "authors": [
            "V.W. Marek",
            "J.B. Remmel"
        ],
        "abstract": "  We apply proof-theoretic techniques in answer Set Programming. The main results include: 1. A characterization of continuity properties of Gelfond-Lifschitz operator for logic program. 2. A propositional characterization of stable models of logic programs (without referring to loop formulas.\n    ",
        "submission_date": "2009-05-02T00:00:00",
        "last_modified_date": "2010-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.0266",
        "title": "Gaussian Belief with dynamic data and in dynamic network",
        "authors": [
            "Erik Aurell",
            "Ren\u00e9 Pfitzner"
        ],
        "abstract": "  In this paper we analyse Belief Propagation over a Gaussian model in a dynamic environment. Recently, this has been proposed as a method to average local measurement values by a distributed protocol (\"Consensus Propagation\", Moallemi & Van Roy, 2006), where the average is available for read-out at every single node. In the case that the underlying network is constant but the values to be averaged fluctuate (\"dynamic data\"), convergence and accuracy are determined by the spectral properties of an associated Ruelle-Perron-Frobenius operator. For Gaussian models on Erdos-Renyi graphs, numerical computation points to a spectral gap remaining in the large-size limit, implying exceptionally good scalability. In a model where the underlying network also fluctuates (\"dynamic network\"), averaging is more effective than in the dynamic data case. Altogether, this implies very good performance of these methods in very large systems, and opens a new field of statistical physics of large (and dynamic) information systems.\n    ",
        "submission_date": "2009-05-03T00:00:00",
        "last_modified_date": "2009-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.2435",
        "title": "Quantified Multimodal Logics in Simple Type Theory",
        "authors": [
            "Christoph Benzmueller",
            "Lawrence C. Paulson"
        ],
        "abstract": "  We present a straightforward embedding of quantified multimodal logic in simple type theory and prove its soundness and completeness. Modal operators are replaced by quantification over a type of possible worlds. We present simple experiments, using existing higher-order theorem provers, to demonstrate that the embedding allows automated proofs of statements in these logics, as well as meta properties of them.\n    ",
        "submission_date": "2009-05-14T00:00:00",
        "last_modified_date": "2009-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.3369",
        "title": "Learning Nonlinear Dynamic Models",
        "authors": [
            "John Langford",
            "Ruslan Salakhutdinov",
            "Tong Zhang"
        ],
        "abstract": "  We present a novel approach for learning nonlinear dynamic models, which leads to a new set of tools capable of solving problems that are otherwise difficult. We provide theory showing this new approach is consistent for models with long range structure, and apply the approach to motion capture and high-dimensional video data, yielding results superior to standard alternatives.\n    ",
        "submission_date": "2009-05-20T00:00:00",
        "last_modified_date": "2009-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.3378",
        "title": "Interpretations of the Web of Data",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "abstract": "  The emerging Web of Data utilizes the web infrastructure to represent and interrelate data. The foundational standards of the Web of Data include the Uniform Resource Identifier (URI) and the Resource Description Framework (RDF). URIs are used to identify resources and RDF is used to relate resources. While RDF has been posited as a logic language designed specifically for knowledge representation and reasoning, it is more generally useful if it can conveniently support other models of computing. In order to realize the Web of Data as a general-purpose medium for storing and processing the world's data, it is necessary to separate RDF from its logic language legacy and frame it simply as a data model. Moreover, there is significant advantage in seeing the Semantic Web as a particular interpretation of the Web of Data that is focused specifically on knowledge representation and reasoning. By doing so, other interpretations of the Web of Data are exposed that realize RDF in different capacities and in support of different computing models.\n    ",
        "submission_date": "2009-05-20T00:00:00",
        "last_modified_date": "2009-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.3582",
        "title": "Profiling of a network behind an infectious disease outbreak",
        "authors": [
            "Yoshiharu Maeno"
        ],
        "abstract": "Stochasticity and spatial heterogeneity are of great interest recently in studying the spread of an infectious disease. The presented method solves an inverse problem to discover the effectively decisive topology of a heterogeneous network and reveal the transmission parameters which govern the stochastic spreads over the network from a dataset on an infectious disease outbreak in the early growth phase. Populations in a combination of epidemiological compartment models and a meta-population network model are described by stochastic differential equations. Probability density functions are derived from the equations and used for the maximal likelihood estimation of the topology and parameters. The method is tested with computationally synthesized datasets and the WHO dataset on SARS outbreak.\n    ",
        "submission_date": "2009-05-21T00:00:00",
        "last_modified_date": "2010-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.3720",
        "title": "Where are the really hard manipulation problems? The phase transition in manipulating the veto rule",
        "authors": [
            "Toby Walsh"
        ],
        "abstract": "  Voting is a simple mechanism to aggregate the preferences of agents. Many voting rules have been shown to be NP-hard to manipulate. However, a number of recent theoretical results suggest that this complexity may only be in the worst-case since manipulation is often easy in practice. In this paper, we show that empirical studies are useful in improving our understanding of this issue. We demonstrate that there is a smooth transition in the probability that a coalition can elect a desired candidate using the veto rule as the size of the manipulating coalition increases. We show that a rescaled probability curve displays a simple and universal form independent of the size of the problem. We argue that manipulation of the veto rule is asymptotically easy for many independent and identically distributed votes even when the coalition of manipulators is critical in size. Based on this argument, we identify a situation in which manipulation is computationally hard. This is when votes are highly correlated and the election is \"hung\". We show, however, that even a single uncorrelated voter is enough to make manipulation easy again.\n    ",
        "submission_date": "2009-05-22T00:00:00",
        "last_modified_date": "2009-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.3755",
        "title": "Decompositions of All Different, Global Cardinality and Related Constraints",
        "authors": [
            "Christian Bessiere",
            "George Katsirelos",
            "Nina Narodytska",
            "Claude-Guy Quimper",
            "Toby Walsh"
        ],
        "abstract": "  We show that some common and important global constraints like ALL-DIFFERENT and GCC can be decomposed into simple arithmetic constraints on which we achieve bound or range consistency, and in some cases even greater pruning. These decompositions can be easily added to new solvers. They also provide other constraints with access to the state of the propagator by sharing of variables. Such sharing can be used to improve propagation between constraints. We report experiments with our decomposition in a pseudo-Boolean solver.\n    ",
        "submission_date": "2009-05-22T00:00:00",
        "last_modified_date": "2009-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.3757",
        "title": "Circuit Complexity and Decompositions of Global Constraints",
        "authors": [
            "Christian Bessiere",
            "George Katsirelos",
            "Nina Narodytska",
            "Toby Walsh"
        ],
        "abstract": "  We show that tools from circuit complexity can be used to study decompositions of global constraints. In particular, we study decompositions of global constraints into conjunctive normal form with the property that unit propagation on the decomposition enforces the same level of consistency as a specialized propagation algorithm. We prove that a constraint propagator has a a polynomial size decomposition if and only if it can be computed by a polynomial size monotone Boolean circuit. Lower bounds on the size of monotone Boolean circuits thus translate to lower bounds on the size of decompositions of global constraints. For instance, we prove that there is no polynomial sized decomposition of the domain consistency propagator for the ALLDIFFERENT constraint.\n    ",
        "submission_date": "2009-05-22T00:00:00",
        "last_modified_date": "2009-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.3763",
        "title": "Scenario-based Stochastic Constraint Programming",
        "authors": [
            "Suresh Manandhar",
            "Armagan Tarim",
            "Toby Walsh"
        ],
        "abstract": "  To model combinatorial decision problems involving uncertainty and probability, we extend the stochastic constraint programming framework proposed in [Walsh, 2002] along a number of important dimensions (e.g. to multiple chance constraints and to a range of new objectives). We also provide a new (but equivalent) semantics based on scenarios. Using this semantics, we can compile stochastic constraint programs down into conventional (nonstochastic) constraint programs. This allows us to exploit the full power of existing constraint solvers. We have implemented this framework for decision making under uncertainty in stochastic OPL, a language which is based on the OPL constraint modelling language [Hentenryck et al., 1999]. To illustrate the potential of this framework, we model a wide range of problems in areas as diverse as finance, agriculture and production.\n    ",
        "submission_date": "2009-05-22T00:00:00",
        "last_modified_date": "2009-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.3766",
        "title": "Reasoning about soft constraints and conditional preferences: complexity results and approximation techniques",
        "authors": [
            "Carmel Domshlak",
            "Francesca Rossi",
            "Kristen Brent Venable",
            "Toby Walsh"
        ],
        "abstract": "  Many real life optimization problems contain both hard and soft constraints, as well as qualitative conditional preferences. However, there is no single formalism to specify all three kinds of information. We therefore propose a framework, based on both CP-nets and soft constraints, that handles both hard and soft constraints as well as conditional preferences efficiently and uniformly. We study the complexity of testing the consistency of preference statements, and show how soft constraints can faithfully approximate the semantics of conditional preference statements whilst improving the computational complexity\n    ",
        "submission_date": "2009-05-22T00:00:00",
        "last_modified_date": "2009-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.3769",
        "title": "Multiset Ordering Constraints",
        "authors": [
            "Alan M. Frisch",
            "Ian Miguel",
            "Zeynep Kiziltan",
            "Brahim Hnich",
            "Toby Walsh"
        ],
        "abstract": "  We identify a new and important global (or non-binary) constraint. This constraint ensures that the values taken by two vectors of variables, when viewed as multisets, are ordered. This constraint is useful for a number of different applications including breaking symmetry and fuzzy constraint satisfaction. We propose and implement an efficient linear time algorithm for enforcing generalised arc consistency on such a multiset ordering constraint. Experimental results on several problem domains show considerable promise.\n    ",
        "submission_date": "2009-05-22T00:00:00",
        "last_modified_date": "2009-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.3830",
        "title": "Tag Clouds for Displaying Semantics: The Case of Filmscripts",
        "authors": [
            "F. Murtagh",
            "A. Ganz",
            "S. McKie",
            "J. Mothe",
            "K. Englmeier"
        ],
        "abstract": "  We relate tag clouds to other forms of visualization, including planar or reduced dimensionality mapping, and Kohonen self-organizing maps. Using a modified tag cloud visualization, we incorporate other information into it, including text sequence and most pertinent words. Our notion of word pertinence goes beyond just word frequency and instead takes a word in a mathematical sense as located at the average of all of its pairwise relationships. We capture semantics through context, taken as all pairwise relationships. Our domain of application is that of filmscript analysis. The analysis of filmscripts, always important for cinema, is experiencing a major gain in importance in the context of television. Our objective in this work is to visualize the semantics of filmscript, and beyond filmscript any other partially structured, time-ordered, sequence of text segments. In particular we develop an innovative approach to plot characterization.\n    ",
        "submission_date": "2009-05-23T00:00:00",
        "last_modified_date": "2009-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.4341",
        "title": "Characterizing predictable classes of processes",
        "authors": [
            "Daniil Ryabko"
        ],
        "abstract": "  The problem is sequence prediction in the following setting. A sequence $x_1,...,x_n,...$ of discrete-valued observations is generated according to some unknown probabilistic law (measure) $\\mu$. After observing each outcome, it is required to give the conditional probabilities of the next observation. The measure $\\mu$ belongs to an arbitrary class $\\C$ of stochastic processes. We are interested in predictors $\\rho$ whose conditional probabilities converge to the \"true\" $\\mu$-conditional probabilities if any $\\mu\\in\\C$ is chosen to generate the data. We show that if such a predictor exists, then a predictor can also be obtained as a convex combination of a countably many elements of $\\C$. In other words, it can be obtained as a Bayesian predictor whose prior is concentrated on a countable set. This result is established for two very different measures of performance of prediction, one of which is very strong, namely, total variation, and the other is very weak, namely, prediction in expected average Kullback-Leibler divergence.\n    ",
        "submission_date": "2009-05-27T00:00:00",
        "last_modified_date": "2009-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.4369",
        "title": "Automating Quantified Multimodal Logics in Simple Type Theory -- A Case Study",
        "authors": [
            "Christoph Benzmueller"
        ],
        "abstract": "  In a case study we investigate whether off the shelf higher-order theorem provers and model generators can be employed to automate reasoning in and about quantified multimodal logics. In our experiments we exploit the new TPTP infrastructure for classical higher-order logic.\n    ",
        "submission_date": "2009-05-27T00:00:00",
        "last_modified_date": "2009-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.4387",
        "title": "Information Modeling for a Dynamic Representation of an Emergency Situation",
        "authors": [
            "Fahem Kebair",
            "Frederic Serin"
        ],
        "abstract": "  In this paper we propose an approach to build a decision support system that can help emergency planners and responders to detect and manage emergency situations. The internal mechanism of the system is independent from the treated application. Therefore, we think the system may be used or adapted easily to different case studies. We focus here on a first step in the decision-support process which concerns the modeling of information issued from the perceived environment and their representation dynamically using a multiagent system. This modeling was applied on the RoboCupRescue Simulation System. An implementation and some results are presented here.\n    ",
        "submission_date": "2009-05-27T00:00:00",
        "last_modified_date": "2009-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.4570",
        "title": "Weak Evolvability Equals Strong Evolvability",
        "authors": [
            "Yang Yu",
            "Zhi-Hua Zhou"
        ],
        "abstract": "  An updated version will be uploaded later.\n    ",
        "submission_date": "2009-05-28T00:00:00",
        "last_modified_date": "2010-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.4601",
        "title": "Considerations on Construction Ontologies",
        "authors": [
            "Alexandru Cicortas",
            "Victoria Stana Iordan",
            "Alexandra Emilia Fortis"
        ],
        "abstract": "  The paper proposes an analysis on some existent ontologies, in order to point out ways to resolve semantic heterogeneity in information systems. Authors are highlighting the tasks in a Knowledge Acquisiton System and identifying aspects related to the addition of new information to an intelligent system. A solution is proposed, as a combination of ontology reasoning services and natural languages generation. A multi-agent system will be conceived with an extractor agent, a reasoner agent and a competence management agent.\n    ",
        "submission_date": "2009-05-28T00:00:00",
        "last_modified_date": "2009-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.4614",
        "title": "A Logic Programming Approach to Activity Recognition",
        "authors": [
            "A. Artikis",
            "M. Sergot",
            "G. Paliouras"
        ],
        "abstract": "We have been developing a system for recognising human activity given a symbolic representation of video content. The input of our system is a set of time-stamped short-term activities detected on video frames. The output of our system is a set of recognised long-term activities, which are pre-defined temporal combinations of short-term activities. The constraints on the short-term activities that, if satisfied, lead to the recognition of a long-term activity, are expressed using a dialect of the Event Calculus. We illustrate the expressiveness of the dialect by showing the representation of several typical complex activities. Furthermore, we present a detailed evaluation of the system through experimentation on a benchmark dataset of surveillance videos.\n    ",
        "submission_date": "2009-05-28T00:00:00",
        "last_modified_date": "2013-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.4713",
        "title": "Mining Generalized Patterns from Large Databases using Ontologies",
        "authors": [
            "Leonard Kwuida",
            "Rokia Missaoui",
            "Lahcen Boumedjout",
            "Jean Vaillancourt"
        ],
        "abstract": "  Formal Concept Analysis (FCA) is a mathematical theory based on the formalization of the notions of concept and concept hierarchies. It has been successfully applied to several Computer Science fields such as data mining,software engineering, and knowledge engineering, and in many domains like medicine, psychology, linguistics and ecology. For instance, it has been exploited for the design, mapping and refinement of ontologies. In this paper, we show how FCA can benefit from a given domain ontology by analyzing the impact of a taxonomy (on objects and/or attributes) on the resulting concept lattice. We willmainly concentrate on the usage of a taxonomy to extract generalized patterns (i.e., knowledge generated from data when elements of a given domain ontology are used) in the form of concepts and rules, and improve navigation through these patterns. To that end, we analyze three generalization cases and show their impact on the size of the generalized pattern set. Different scenarios of simultaneous generalizations on both objects and attributes are also discussed\n    ",
        "submission_date": "2009-05-28T00:00:00",
        "last_modified_date": "2009-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.0311",
        "title": "Solar radiation forecasting using ad-hoc time series preprocessing and neural networks",
        "authors": [
            "Christophe Paoli",
            "Cyril Voyant",
            "Marc Muselli",
            "Marie-Laure Nivet"
        ],
        "abstract": "  In this paper, we present an application of neural networks in the renewable energy domain. We have developed a methodology for the daily prediction of global solar radiation on a horizontal surface. We use an ad-hoc time series preprocessing and a Multi-Layer Perceptron (MLP) in order to predict solar radiation at daily horizon. First results are promising with nRMSE < 21% and RMSE < 998 Wh/m2. Our optimized MLP presents prediction similar to or even better than conventional methods such as ARIMA techniques, Bayesian inference, Markov chains and k-Nearest-Neighbors approximators. Moreover we found that our data preprocessing approach can reduce significantly forecasting errors.\n    ",
        "submission_date": "2009-06-01T00:00:00",
        "last_modified_date": "2009-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.1182",
        "title": "The CIFF Proof Procedure for Abductive Logic Programming with Constraints: Theory, Implementation and Experiments",
        "authors": [
            "P. Mancarella",
            "G. Terreni",
            "F. Sadri",
            "F. Toni",
            "U. Endriss"
        ],
        "abstract": "  We present the CIFF proof procedure for abductive logic programming with constraints, and we prove its correctness. CIFF is an extension of the IFF proof procedure for abductive logic programming, relaxing the original restrictions over variable quantification (allowedness conditions) and incorporating a constraint solver to deal with numerical constraints as in constraint logic programming. Finally, we describe the CIFF system, comparing it with state of the art abductive systems and answer set solvers and showing how to use it to program some applications. (To appear in Theory and Practice of Logic Programming - TPLP).\n    ",
        "submission_date": "2009-06-05T00:00:00",
        "last_modified_date": "2009-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.1593",
        "title": "On Defining 'I' \"I logy\"",
        "authors": [
            "Farzad Didehvar"
        ],
        "abstract": "  Could we define I? Throughout this article we give a negative answer to this question. More exactly, we show that there is no definition for I in a certain way. But this negative answer depends on our definition of definability. Here, we try to consider sufficient generalized definition of definability. In the middle of paper a paradox will arise which makes us to modify the way we use the concept of property and definability.\n    ",
        "submission_date": "2009-06-06T00:00:00",
        "last_modified_date": "2009-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.1673",
        "title": "Knowledge Management in Economic Intelligence with Reasoning on Temporal Attributes",
        "authors": [
            "Bolanle Oladejo",
            "Adenike Osofisan",
            "Victor Odumuyiwa"
        ],
        "abstract": "  People have to make important decisions within a time frame. Hence, it is imperative to employ means or strategy to aid effective decision making. Consequently, Economic Intelligence (EI) has emerged as a field to aid strategic and timely decision making in an organization. In the course of attaining this goal: it is indispensable to be more optimistic towards provision for conservation of intellectual resource invested into the process of decision making. This intellectual resource is nothing else but the knowledge of the actors as well as that of the various processes for effecting decision making. Knowledge has been recognized as a strategic economic resource for enhancing productivity and a key for innovation in any organization or community. Thus, its adequate management with cognizance of its temporal properties is highly indispensable. Temporal properties of knowledge refer to the date and time (known as timestamp) such knowledge is created as well as the duration or interval between related knowledge. This paper focuses on the needs for a user-centered knowledge management approach as well as exploitation of associated temporal properties. Our perspective of knowledge is with respect to decision-problems projects in EI. Our hypothesis is that the possibility of reasoning about temporal properties in exploitation of knowledge in EI projects should foster timely decision making through generation of useful inferences from available and reusable knowledge for a new project.\n    ",
        "submission_date": "2009-06-09T00:00:00",
        "last_modified_date": "2009-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.1694",
        "title": "Toward a Category Theory Design of Ontological Knowledge Bases",
        "authors": [
            "Nikolaj Glazunov"
        ],
        "abstract": "  I discuss (ontologies_and_ontological_knowledge_bases / formal_methods_and_theories) duality and its category theory extensions as a step toward a solution to Knowledge-Based Systems Theory. In particular I focus on the example of the design of elements of ontologies and ontological knowledge bases of next three electronic courses: Foundations of Research Activities, Virtual Modeling of Complex Systems and Introduction to String Theory.\n    ",
        "submission_date": "2009-06-09T00:00:00",
        "last_modified_date": "2009-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.1842",
        "title": "Managing Requirement Volatility in an Ontology-Driven Clinical LIMS Using Category Theory. International Journal of Telemedicine and Applications",
        "authors": [
            "Arash Shaban-Nejad",
            "Olga Ormandjieva",
            "Mohamad Kassab",
            "Volker Haarslev"
        ],
        "abstract": "  Requirement volatility is an issue in software engineering in general, and in Web-based clinical applications in particular, which often originates from an incomplete knowledge of the domain of interest. With advances in the health science, many features and functionalities need to be added to, or removed from, existing software applications in the biomedical domain. At the same time, the increasing complexity of biomedical systems makes them more difficult to understand, and consequently it is more difficult to define their requirements, which contributes considerably to their volatility. In this paper, we present a novel agent-based approach for analyzing and managing volatile and dynamic requirements in an ontology-driven laboratory information management system (LIMS) designed for Web-based case reporting in medical mycology. The proposed framework is empowered with ontologies and formalized using category theory to provide a deep and common understanding of the functional and nonfunctional requirement hierarchies and their interrelations, and to trace the effects of a change on the conceptual framework.\n    ",
        "submission_date": "2009-06-10T00:00:00",
        "last_modified_date": "2009-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.1980",
        "title": "On Maximum a Posteriori Estimation of Hidden Markov Processes",
        "authors": [
            "Armen Allahverdyan",
            "Aram Galstyan"
        ],
        "abstract": "  We present a theoretical analysis of Maximum a Posteriori (MAP) sequence estimation for binary symmetric hidden Markov processes. We reduce the MAP estimation to the energy minimization of an appropriately defined Ising spin model, and focus on the performance of MAP as characterized by its accuracy and the number of solutions corresponding to a typical observed sequence. It is shown that for a finite range of sufficiently low noise levels, the solution is uniquely related to the observed sequence, while the accuracy degrades linearly with increasing the noise strength. For intermediate noise values, the accuracy is nearly noise-independent, but now there are exponentially many solutions to the estimation problem, which is reflected in non-zero ground-state entropy for the Ising model. Finally, for even larger noise intensities, the number of solutions reduces again, but the accuracy is poor. It is shown that these regimes are different thermodynamic phases of the Ising model that are related to each other via first-order phase transitions.\n    ",
        "submission_date": "2009-06-10T00:00:00",
        "last_modified_date": "2009-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.2824",
        "title": "What Does Artificial Life Tell Us About Death?",
        "authors": [
            "Carlos Gershenson"
        ],
        "abstract": "  Short philosophical essay\n    ",
        "submission_date": "2009-06-15T00:00:00",
        "last_modified_date": "2009-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.3036",
        "title": "Mnesors for automatic control",
        "authors": [
            "Gilles Champenois"
        ],
        "abstract": "  Mnesors are defined as elements of a semimodule over the min-plus integers. This two-sorted structure is able to merge graduation properties of vectors and idempotent properties of boolean numbers, which makes it appropriate for hybrid systems. We apply it to the control of an inverted pendulum and design a full logical controller, that is, without the usual algebra of real numbers.\n    ",
        "submission_date": "2009-06-16T00:00:00",
        "last_modified_date": "2009-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.3149",
        "title": "Semi-Myopic Sensing Plans for Value Optimization",
        "authors": [
            "David Tolpin",
            "Solomon Eyal Shimony"
        ],
        "abstract": "  We consider the following sequential decision problem. Given a set of items of unknown utility, we need to select one of as high a utility as possible (``the selection problem''). Measurements (possibly noisy) of item values prior to selection are allowed, at a known cost. The goal is to optimize the overall sequential decision process of measurements and selection.\n",
        "submission_date": "2009-06-17T00:00:00",
        "last_modified_date": "2009-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.3722",
        "title": "Two-Dimensional ARMA Modeling for Breast Cancer Detection and Classification",
        "authors": [
            "Nidhal Bouaynaya",
            "Jerzy Zielinski",
            "Dan Schonfeld"
        ],
        "abstract": "  We propose a new model-based computer-aided diagnosis (CAD) system for tumor detection and classification (cancerous v.s. benign) in breast images. Specifically, we show that (x-ray, ultrasound and MRI) images can be accurately modeled by two-dimensional autoregressive-moving average (ARMA) random fields. We derive a two-stage Yule-Walker Least-Squares estimates of the model parameters, which are subsequently used as the basis for statistical inference and biophysical interpretation of the breast image. We use a k-means classifier to segment the breast image into three regions: healthy tissue, benign tumor, and cancerous tumor. Our simulation results on ultrasound breast images illustrate the power of the proposed approach.\n    ",
        "submission_date": "2009-06-19T00:00:00",
        "last_modified_date": "2009-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.3926",
        "title": "Soft Constraints for Quality Aspects in Service Oriented Architectures",
        "authors": [
            "Stefano Bistarelli",
            "Francesco Santini"
        ],
        "abstract": "  We propose the use of Soft Constraints as a natural way to model Service Oriented Architecture. In the framework, constraints are used to model components and connectors and constraint aggregation is used to represent their interactions. The \"quality of a service\" is measured and considered when performing queries to service providers. Some examples consist in the levels of cost, performance and availability required by clients. In our framework, the QoS scores are represented by the softness level of the constraint and the measure of complex (web) services is computed by combining the levels of the components.\n    ",
        "submission_date": "2009-06-22T00:00:00",
        "last_modified_date": "2009-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.4321",
        "title": "Reasoning About Knowledge of Unawareness Revisited",
        "authors": [
            "Joseph Y. Halpern",
            "Leandro Rego"
        ],
        "abstract": "  In earlier work, we proposed a logic that extends the Logic of General Awareness of Fagin and Halpern [1988] by allowing quantification over primitive propositions. This makes it possible to express the fact that an agent knows that there are some facts of which he is unaware. In that logic, it is not possible to model an agent who is uncertain about whether he is aware of all formulas. To overcome this problem, we keep the syntax of the earlier paper, but allow models where, with each world, a possibly different language is associated. We provide a sound and complete axiomatization for this logic and show that, under natural assumptions, the quantifier-free fragment of the logic is characterized by exactly the same axioms as the logic of Heifetz, Meier, and Schipper [2008].\n    ",
        "submission_date": "2009-06-23T00:00:00",
        "last_modified_date": "2009-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.4326",
        "title": "A Logical Characterization of Iterated Admissibility",
        "authors": [
            "Joseph Y. Halpern",
            "Rafael Pass"
        ],
        "abstract": "  Brandenburger, Friedenberg, and Keisler provide an epistemic characterization of iterated admissibility (i.e., iterated deletion of weakly dominated strategies) where uncertainty is represented using LPSs (lexicographic probability sequences). Their characterization holds in a rich structure called a complete structure, where all types are possible. Here, a logical charaacterization of iterated admisibility is given that involves only standard probability and holds in all structures, not just complete structures. A stronger notion of strong admissibility is then defined. Roughly speaking, strong admissibility is meant to capture the intuition that \"all the agent knows\" is that the other agents satisfy the appropriate rationality assumptions. Strong admissibility makes it possible to relate admissibility, canonical structures (as typically considered in completeness proofs in modal logic), complete structures, and the notion of ``all I know''.\n    ",
        "submission_date": "2009-06-23T00:00:00",
        "last_modified_date": "2009-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.4332",
        "title": "Updating Sets of Probabilities",
        "authors": [
            "Adam J. Grove",
            "Joseph Y. Halpern"
        ],
        "abstract": "There are several well-known justifications for conditioning as the appropriate method for updating a single probability measure, given an observation. However, there is a significant body of work arguing for sets of probability measures, rather than single measures, as a more realistic model of uncertainty. Conditioning still makes sense in this context--we can simply condition each measure in the set individually, then combine the results--and, indeed, it seems to be the preferred updating procedure in the literature. But how justified is conditioning in this richer setting? Here we show, by considering an axiomatic account of conditioning given by van Fraassen, that the single-measure and sets-of-measures cases are very different. We show that van Fraassen's axiomatization for the former case is nowhere near sufficient for updating sets of measures. We give a considerably longer (and not as compelling) list of axioms that together force conditioning in this setting, and describe other update methods that are allowed once any of these axioms is dropped.\n    ",
        "submission_date": "2009-06-23T00:00:00",
        "last_modified_date": "2014-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.4982",
        "title": "Concept-based Recommendations for Internet Advertisement",
        "authors": [
            "Dmitry I. Ignatov",
            "Sergei O. Kuznetsov"
        ],
        "abstract": "  The problem of detecting terms that can be interesting to the advertiser is considered. If a company has already bought some advertising terms which describe certain services, it is reasonable to find out the terms bought by competing companies. A part of them can be recommended as future advertising terms to the company. The goal of this work is to propose better interpretable recommendations based on FCA and association rules.\n    ",
        "submission_date": "2009-06-26T00:00:00",
        "last_modified_date": "2009-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.5038",
        "title": "A Novel Two-Stage Dynamic Decision Support based Optimal Threat Evaluation and Defensive Resource Scheduling Algorithm for Multi Air-borne threats",
        "authors": [
            "Huma Naeem",
            "Asif Masood",
            "Mukhtar Hussain",
            "Shoab A. Khan"
        ],
        "abstract": "  This paper presents a novel two-stage flexible dynamic decision support based optimal threat evaluation and defensive resource scheduling algorithm for multi-target air-borne threats. The algorithm provides flexibility and optimality by swapping between two objective functions, i.e. the preferential and subtractive defense strategies as and when required. To further enhance the solution quality, it outlines and divides the critical parameters used in Threat Evaluation and Weapon Assignment (TEWA) into three broad categories (Triggering, Scheduling and Ranking parameters). Proposed algorithm uses a variant of many-to-many Stable Marriage Algorithm (SMA) to solve Threat Evaluation (TE) and Weapon Assignment (WA) problem. In TE stage, Threat Ranking and Threat-Asset pairing is done. Stage two is based on a new flexible dynamic weapon scheduling algorithm, allowing multiple engagements using shoot-look-shoot strategy, to compute near-optimal solution for a range of scenarios. Analysis part of this paper presents the strengths and weaknesses of the proposed algorithm over an alternative greedy algorithm as applied to different offline scenarios.\n    ",
        "submission_date": "2009-06-27T00:00:00",
        "last_modified_date": "2009-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.5119",
        "title": "General combination rules for qualitative and quantitative beliefs",
        "authors": [
            "Arnaud Martin",
            "Christophe Osswald",
            "Jean Dezert",
            "Florentin Smarandache"
        ],
        "abstract": "  Martin and Osswald \\cite{Martin07} have recently proposed many generalizations of combination rules on quantitative beliefs in order to manage the conflict and to consider the specificity of the responses of the experts. Since the experts express themselves usually in natural language with linguistic labels, Smarandache and Dezert \\cite{Li07} have introduced a mathematical framework for dealing directly also with qualitative beliefs. In this paper we recall some element of our previous works and propose the new combination rules, developed for the fusion of both qualitative or quantitative beliefs.\n    ",
        "submission_date": "2009-06-28T00:00:00",
        "last_modified_date": "2009-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.5148",
        "title": "Explicit probabilistic models for databases and networks",
        "authors": [
            "Tijl De Bie"
        ],
        "abstract": "  Recent work in data mining and related areas has highlighted the importance of the statistical assessment of data mining results. Crucial to this endeavour is the choice of a non-trivial null model for the data, to which the found patterns can be contrasted. The most influential null models proposed so far are defined in terms of invariants of the null distribution. Such null models can be used by computation intensive randomization approaches in estimating the statistical significance of data mining results.\n",
        "submission_date": "2009-06-29T00:00:00",
        "last_modified_date": "2009-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.5233",
        "title": "Restricted Global Grammar Constraints",
        "authors": [
            "George Katsirelos",
            "Sebastian Maneth",
            "Nina Narodytska",
            "Toby Walsh"
        ],
        "abstract": "  We investigate the global GRAMMAR constraint over restricted classes of context free grammars like deterministic and unambiguous context-free grammars. We show that detecting disentailment for the GRAMMAR constraint in these cases is as hard as parsing an unrestricted context free ",
        "submission_date": "2009-06-29T00:00:00",
        "last_modified_date": "2009-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.0067",
        "title": "A Novel Two-Staged Decision Support based Threat Evaluation and Weapon Assignment Algorithm, Asset-based Dynamic Weapon Scheduling using Artificial Intelligence Techinques",
        "authors": [
            "Huma Naeem",
            "Asif Masood",
            "Mukhtar Hussain",
            "Shoab A. Khan"
        ],
        "abstract": "  Surveillance control and reporting (SCR) system for air threats play an important role in the defense of a country. SCR system corresponds to air and ground situation management/processing along with information fusion, communication, coordination, simulation and other critical defense oriented tasks. Threat Evaluation and Weapon Assignment (TEWA) sits at the core of SCR system. In such a system, maximal or near maximal utilization of constrained resources is of extreme importance. Manual TEWA systems cannot provide optimality because of different limitations ",
        "submission_date": "2009-07-01T00:00:00",
        "last_modified_date": "2009-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.0499",
        "title": "Agent-Oriented Approach for Detecting and Managing Risks in Emergency Situations",
        "authors": [
            "Fahem Kebair",
            "Frederic Serin"
        ],
        "abstract": "  This paper presents an agent-oriented approach to build a decision support system aimed at helping emergency managers to detect and to manage risks. We stress the flexibility and the adaptivity characteristics that are crucial to build a robust and efficient system, able to resolve complex problems. The system should be independent as much as possible from the subject of study. Thereby, an original approach based on a mechanism of perception, representation, characterisation and assessment is proposed. The work described here is applied on the RoboCupRescue application. Experimentations and results are provided.\n    ",
        "submission_date": "2009-07-03T00:00:00",
        "last_modified_date": "2009-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.0589",
        "title": "Generalized Collective Inference with Symmetric Clique Potentials",
        "authors": [
            "Rahul Gupta",
            "Sunita Sarawagi",
            "Ajit A. Diwan"
        ],
        "abstract": "  Collective graphical models exploit inter-instance associative dependence to output more accurate labelings. However existing models support very limited kind of associativity which restricts accuracy gains. This paper makes two major contributions. First, we propose a general collective inference framework that biases data instances to agree on a set of {\\em properties} of their labelings. Agreement is encouraged through symmetric clique potentials. We show that rich properties leads to bigger gains, and present a systematic inference procedure for a large class of such properties. The procedure performs message passing on the cluster graph, where property-aware messages are computed with cluster specific algorithms. This provides an inference-only solution for domain adaptation. Our experiments on bibliographic information extraction illustrate significant test error reduction over unseen domains. Our second major contribution consists of algorithms for computing outgoing messages from clique clusters with symmetric clique potentials. Our algorithms are exact for arbitrary symmetric potentials on binary labels and for max-like and majority-like potentials on multiple labels. For majority potentials, we also provide an efficient Lagrangian Relaxation based algorithm that compares favorably with the exact algorithm. We present a 13/15-approximation algorithm for the NP-hard Potts potential, with runtime sub-quadratic in the clique size. In contrast, the best known previous guarantee for graphs with Potts potentials is only 1/2. We empirically show that our method for Potts potentials is an order of magnitude faster than the best alternatives, and our Lagrangian Relaxation based algorithm for majority potentials beats the best applicable heuristic -- ICM.\n    ",
        "submission_date": "2009-07-03T00:00:00",
        "last_modified_date": "2009-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.0746",
        "title": "Open Problems in Universal Induction & Intelligence",
        "authors": [
            "Marcus Hutter"
        ],
        "abstract": "  Specialized intelligent systems can be found everywhere: finger print, handwriting, speech, and face recognition, spam filtering, chess and other game programs, robots, et al. This decade the first presumably complete mathematical theory of artificial intelligence based on universal induction-prediction-decision-action has been proposed. This information-theoretic approach solidifies the foundations of inductive inference and artificial intelligence. Getting the foundations right usually marks a significant progress and maturing of a field. The theory provides a gold standard and guidance for researchers working on intelligent algorithms. The roots of universal induction have been laid exactly half-a-century ago and the roots of universal intelligence exactly one decade ago. So it is timely to take stock of what has been achieved and what remains to be done. Since there are already good recent surveys, I describe the state-of-the-art only in passing and refer the reader to the literature. This article concentrates on the open problems in universal induction and its extension to universal intelligence.\n    ",
        "submission_date": "2009-07-04T00:00:00",
        "last_modified_date": "2009-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.0939",
        "title": "The Soft Cumulative Constraint",
        "authors": [
            "Thierry Petit",
            "Emmanuel Poder"
        ],
        "abstract": "  This research report presents an extension of Cumulative of Choco constraint solver, which is useful to encode over-constrained cumulative problems. This new global constraint uses sweep and task interval violation-based algorithms.\n    ",
        "submission_date": "2009-07-06T00:00:00",
        "last_modified_date": "2009-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.2775",
        "title": "Modelling Concurrent Behaviors in the Process Specification Language",
        "authors": [
            "Dai Tri Man Le"
        ],
        "abstract": "  In this paper, we propose a first-order ontology for generalized stratified order structure. We then classify the models of the theory using model-theoretic techniques. An ontology mapping from this ontology to the core theory of Process Specification Language is also discussed.\n    ",
        "submission_date": "2009-07-16T00:00:00",
        "last_modified_date": "2009-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.2990",
        "title": "The Single Machine Total Weighted Tardiness Problem - Is it (for Metaheuristics) a Solved Problem ?",
        "authors": [
            "Martin Josef Geiger"
        ],
        "abstract": "  The article presents a study of rather simple local search heuristics for the single machine total weighted tardiness problem (SMTWTP), namely hillclimbing and Variable Neighborhood Search. In particular, we revisit these approaches for the SMTWTP as there appears to be a lack of appropriate/challenging benchmark instances in this case. The obtained results are impressive indeed. Only few instances remain unsolved, and even those are approximated within 1% of the optimal/best known solutions. Our experiments support the claim that metaheuristics for the SMTWTP are very likely to lead to good results, and that, before refining search strategies, more work must be done with regard to the proposition of benchmark data. Some recommendations for the construction of such data sets are derived from our investigations.\n    ",
        "submission_date": "2009-07-17T00:00:00",
        "last_modified_date": "2009-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.2993",
        "title": "Improvements for multi-objective flow shop scheduling by Pareto Iterated Local Search",
        "authors": [
            "Martin Josef Geiger"
        ],
        "abstract": "  The article describes the proposition and application of a local search metaheuristic for multi-objective optimization problems. It is based on two main principles of heuristic search, intensification through variable neighborhoods, and diversification through perturbations and successive iterations in favorable regions of the search space. The concept is successfully tested on permutation flow shop scheduling problems under multiple objectives and compared to other local search approaches. While the obtained results are encouraging in terms of their quality, another positive attribute of the approach is its simplicity as it does require the setting of only very few parameters.\n    ",
        "submission_date": "2009-07-17T00:00:00",
        "last_modified_date": "2009-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.3867",
        "title": "Artificial Dendritic Cells: Multi-faceted Perspectives",
        "authors": [
            "Julie Greensmith",
            "Uwe Aickelin"
        ],
        "abstract": "  Dendritic cells are the crime scene investigators of the human immune system. Their function is to correlate potentially anomalous invading entities with observed damage to the body. The detection of such invaders by dendritic cells results in the activation of the adaptive immune system, eventually leading to the removal of the invader from the host body. This mechanism has provided inspiration for the development of a novel bio-inspired algorithm, the Dendritic Cell Algorithm. This algorithm processes information at multiple levels of resolution, resulting in the creation of information granules of variable structure. In this chapter we examine the multi-faceted nature of immunology and how research in this field has shaped the function of the resulting Dendritic Cell Algorithm. A brief overview of the algorithm is given in combination with the details of the processes used for its development. The chapter is concluded with a discussion of the parallels between our understanding of the human immune system and how such knowledge influences the design of artificial immune systems.\n    ",
        "submission_date": "2009-07-22T00:00:00",
        "last_modified_date": "2009-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.4100",
        "title": "Beyond Turing Machines",
        "authors": [
            "Kurt Ammon"
        ],
        "abstract": "  This paper discusses \"computational\" systems capable of \"computing\" functions not computable by predefined Turing machines if the systems are not isolated from their environment. Roughly speaking, these systems can change their finite descriptions by interacting with their environment.\n    ",
        "submission_date": "2009-07-23T00:00:00",
        "last_modified_date": "2009-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.4128",
        "title": "Relativized hyperequivalence of logic programs for modular programming",
        "authors": [
            "Miroslaw Truszczy\u0144ski",
            "Stefan Woltran"
        ],
        "abstract": "  A recent framework of relativized hyperequivalence of programs offers a unifying generalization of strong and uniform equivalence. It seems to be especially well suited for applications in program optimization and modular programming due to its flexibility that allows us to restrict, independently of each other, the head and body alphabets in context programs. We study relativized hyperequivalence for the three semantics of logic programs given by stable, supported and supported minimal models. For each semantics, we identify four types of contexts, depending on whether the head and body alphabets are given directly or as the complement of a given set. Hyperequivalence relative to contexts where the head and body alphabets are specified directly has been studied before. In this paper, we establish the complexity of deciding relativized hyperequivalence with respect to the three other types of context programs.\n",
        "submission_date": "2009-07-23T00:00:00",
        "last_modified_date": "2009-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.4509",
        "title": "Pattern Recognition Theory of Mind",
        "authors": [
            "Gilberto de Paiva"
        ],
        "abstract": "  I propose that pattern recognition, memorization and processing are key concepts that can be a principle set for the theoretical modeling of the mind function. Most of the questions about the mind functioning can be answered by a descriptive modeling and definitions from these principles. An understandable consciousness definition can be drawn based on the assumption that a pattern recognition system can recognize its own patterns of activity. The principles, descriptive modeling and definitions can be a basis for theoretical and applied research on cognitive sciences, particularly at artificial intelligence studies.\n    ",
        "submission_date": "2009-07-26T00:00:00",
        "last_modified_date": "2009-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.4561",
        "title": "Fact Sheet on Semantic Web",
        "authors": [
            "York Sure"
        ],
        "abstract": "  The report gives an overview about activities on the topic Semantic Web. It has been released as technical report for the project \"KTweb -- Connecting Knowledge Technologies Communities\" in 2003.\n    ",
        "submission_date": "2009-07-27T00:00:00",
        "last_modified_date": "2009-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.5032",
        "title": "Restart Strategy Selection using Machine Learning Techniques",
        "authors": [
            "Shai Haim",
            "Toby Walsh"
        ],
        "abstract": "  Restart strategies are an important factor in the performance of conflict-driven Davis Putnam style SAT solvers. Selecting a good restart strategy for a problem instance can enhance the performance of a solver. Inspired by recent success applying machine learning techniques to predict the runtime of SAT solvers, we present a method which uses machine learning to boost solver performance through a smart selection of the restart strategy. Based on easy to compute features, we train both a satisfiability classifier and runtime models. We use these models to choose between restart strategies. We present experimental results comparing this technique with the most commonly used restart strategies. Our results demonstrate that machine learning is effective in improving solver performance.\n    ",
        "submission_date": "2009-07-29T00:00:00",
        "last_modified_date": "2009-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.5033",
        "title": "Online Search Cost Estimation for SAT Solvers",
        "authors": [
            "Shai Haim",
            "Toby Walsh"
        ],
        "abstract": "  We present two different methods for estimating the cost of solving SAT problems. The methods focus on the online behaviour of the backtracking solver, as well as the structure of the problem. Modern SAT solvers present several challenges to estimate search cost including coping with nonchronological backtracking, learning and restarts. Our first method adapt an existing algorithm for estimating the size of a search tree to deal with these challenges. We then suggest a second method that uses a linear model trained on data gathered online at the start of search. We compare the effectiveness of these two methods using random and structured problems. We also demonstrate that predictions made in early restarts can be used to improve later predictions. We conclude by showing that the cost of solving a set of problems can be reduced by selecting a solver from a portfolio based on such cost estimations.\n    ",
        "submission_date": "2009-07-29T00:00:00",
        "last_modified_date": "2009-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.5155",
        "title": "On Classification from Outlier View",
        "authors": [
            "C. A. Hsiao"
        ],
        "abstract": "Classification is the basis of cognition. Unlike other solutions, this study approaches it from the view of outliers. We present an expanding algorithm to detect outliers in univariate datasets, together with the underlying foundation. The expanding algorithm runs in a holistic way, making it a rather robust solution. Synthetic and real data experiments show its power. Furthermore, an application for multi-class problems leads to the introduction of the oscillator algorithm. The corresponding result implies the potential wide use of the expanding algorithm.\n    ",
        "submission_date": "2009-07-29T00:00:00",
        "last_modified_date": "2012-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.5598",
        "title": "Convergence of Expected Utility for Universal AI",
        "authors": [
            "Peter de Blanc"
        ],
        "abstract": "  We consider a sequence of repeated interactions between an agent and an environment. Uncertainty about the environment is captured by a probability distribution over a space of hypotheses, which includes all computable functions. Given a utility function, we can evaluate the expected utility of any computational policy for interaction with the environment. After making some plausible assumptions (and maybe one not-so-plausible assumption), we show that if the utility function is unbounded, then the expected utility of any policy is undefined.\n    ",
        "submission_date": "2009-07-31T00:00:00",
        "last_modified_date": "2009-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0908.0089",
        "title": "Knowledge Discovery of Hydrocyclone s Circuit Based on SONFIS and SORST",
        "authors": [
            "H. O. Ghaffari",
            "M. Ejtemaei",
            "M. Irannajad"
        ],
        "abstract": "  This study describes application of some approximate reasoning methods to analysis of hydrocyclone performance. In this manner, using a combining of Self Organizing Map (SOM), Neuro-Fuzzy Inference System (NFIS)-SONFIS- and Rough Set Theory (RST)-SORST-crisp and fuzzy granules are obtained. Balancing of crisp granules and non-crisp granules can be implemented in close-open iteration. Using different criteria and based on granulation level balance point (interval) or a pseudo-balance point is estimated. Validation of the proposed methods, on the data set of the hydrocyclone is rendered.\n    ",
        "submission_date": "2009-08-01T00:00:00",
        "last_modified_date": "2009-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0908.0100",
        "title": "A Class of DSm Conditional Rules",
        "authors": [
            "Florentin Smarandache",
            "Mark Alford"
        ],
        "abstract": "  In this paper we introduce two new DSm fusion conditioning rules with example, and as a generalization of them a class of DSm fusion conditioning rules, and then extend them to a class of DSm conditioning rules.\n    ",
        "submission_date": "2009-08-01T00:00:00",
        "last_modified_date": "2009-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0908.0373",
        "title": "A Reflection on the Structure and Process of the Web of Data",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "abstract": "  The Web community has introduced a set of standards and technologies for representing, querying, and manipulating a globally distributed data structure known as the Web of Data. The proponents of the Web of Data envision much of the world's data being interrelated and openly accessible to the general public. This vision is analogous in many ways to the Web of Documents of common knowledge, but instead of making documents and media openly accessible, the focus is on making data openly accessible. In providing data for public use, there has been a stimulated interest in a movement dubbed Open Data. Open Data is analogous in many ways to the Open Source movement. However, instead of focusing on software, Open Data is focused on the legal and licensing issues around publicly exposed data. Together, various technological and legal tools are laying the groundwork for the future of global-scale data management on the Web. As of today, in its early form, the Web of Data hosts a variety of data sets that include encyclopedic facts, drug and protein data, metadata on music, books and scholarly articles, social network representations, geospatial information, and many other types of information. The size and diversity of the Web of Data is a demonstration of the flexibility of the underlying standards and the overall feasibility of the project as a whole. The purpose of this article is to provide a review of the technological underpinnings of the Web of Data as well as some of the hurdles that need to be overcome if the Web of Data is to emerge as the defacto medium for data representation, distribution, and ultimately, processing.\n    ",
        "submission_date": "2009-08-04T00:00:00",
        "last_modified_date": "2009-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0908.2050",
        "title": "View-based Propagator Derivation",
        "authors": [
            "Christian Schulte",
            "Guido Tack"
        ],
        "abstract": "  When implementing a propagator for a constraint, one must decide about variants: When implementing min, should one also implement max? Should one implement linear constraints both with unit and non-unit coefficients? Constraint variants are ubiquitous: implementing them requires considerable (if not prohibitive) effort and decreases maintainability, but will deliver better performance than resorting to constraint decomposition.\n",
        "submission_date": "2009-08-14T00:00:00",
        "last_modified_date": "2009-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0908.3091",
        "title": "Computational Understanding and Manipulation of Symmetries",
        "authors": [
            "Attila Egri-Nagy",
            "Chrystopher L. Nehaniv"
        ],
        "abstract": "For natural and artificial systems with some symmetry structure, computational understanding and manipulation can be achieved without learning by exploiting the algebraic structure. Here we describe this algebraic coordinatization method and apply it to permutation puzzles. Coordinatization yields a structural understanding, not just solutions for the puzzles.\n    ",
        "submission_date": "2009-08-21T00:00:00",
        "last_modified_date": "2014-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0908.3394",
        "title": "A Cognitive Mind-map Framework to Foster Trust",
        "authors": [
            "Jayanta Poray",
            "Christoph Schommer"
        ],
        "abstract": "  The explorative mind-map is a dynamic framework, that emerges automatically from the input, it gets. It is unlike a verificative modeling system where existing (human) thoughts are placed and connected together. In this regard, explorative mind-maps change their size continuously, being adaptive with connectionist cells inside; mind-maps process data input incrementally and offer lots of possibilities to interact with the user through an appropriate communication interface. With respect to a cognitive motivated situation like a conversation between partners, mind-maps become interesting as they are able to process stimulating signals whenever they occur. If these signals are close to an own understanding of the world, then the conversational partner becomes automatically more trustful than if the signals do not or less match the own knowledge scheme. In this (position) paper, we therefore motivate explorative mind-maps as a cognitive engine and propose these as a decision support engine to foster trust.\n    ",
        "submission_date": "2009-08-24T00:00:00",
        "last_modified_date": "2009-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0908.3999",
        "title": "An improved axiomatic definition of information granulation",
        "authors": [
            "Ping Zhu"
        ],
        "abstract": "  To capture the uncertainty of information or knowledge in information systems, various information granulations, also known as knowledge granulations, have been proposed. Recently, several axiomatic definitions of information granulation have been introduced. In this paper, we try to improve these axiomatic definitions and give a universal construction of information granulation by relating information granulations with a class of functions of multiple variables. We show that the improved axiomatic definition has some concrete information granulations in the literature as instances.\n    ",
        "submission_date": "2009-08-27T00:00:00",
        "last_modified_date": "2009-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.0109",
        "title": "On the Internal Topological Structure of Plane Regions",
        "authors": [
            "Sanjiang Li"
        ],
        "abstract": "The study of topological information of spatial objects has for a long time been a focus of research in disciplines like computational geometry, spatial reasoning, cognitive science, and robotics. While the majority of these researches emphasised the topological relations between spatial objects, this work studies the internal topological structure of bounded plane regions, which could consist of multiple pieces and/or have holes and islands to any finite level. The insufficiency of simple regions (regions homeomorphic to closed disks) to cope with the variety and complexity of spatial entities and phenomena has been widely acknowledged. Another significant drawback of simple regions is that they are not closed under set operations union, intersection, and difference. This paper considers bounded semi-algebraic regions, which are closed under set operations and can closely approximate most plane regions arising in practice.\n    ",
        "submission_date": "2009-09-01T00:00:00",
        "last_modified_date": "2013-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.0122",
        "title": "Reasoning with Topological and Directional Spatial Information",
        "authors": [
            "Sanjiang Li",
            "Anthony G. Cohn"
        ],
        "abstract": "  Current research on qualitative spatial representation and reasoning mainly focuses on one single aspect of space. In real world applications, however, multiple spatial aspects are often involved simultaneously.\n",
        "submission_date": "2009-09-01T00:00:00",
        "last_modified_date": "2009-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.0138",
        "title": "Reasoning about Cardinal Directions between Extended Objects",
        "authors": [
            "Xiaotong Zhang",
            "Weiming Liu",
            "Sanjiang Li",
            "Mingsheng Ying"
        ],
        "abstract": "  Direction relations between extended spatial objects are important commonsense knowledge. Recently, Goyal and Egenhofer proposed a formal model, known as Cardinal Direction Calculus (CDC), for representing direction relations between connected plane regions. CDC is perhaps the most expressive qualitative calculus for directional information, and has attracted increasing interest from areas such as artificial intelligence, geographical information science, and image retrieval. Given a network of CDC constraints, the consistency problem is deciding if the network is realizable by connected regions in the real plane. This paper provides a cubic algorithm for checking consistency of basic CDC constraint networks, and proves that reasoning with CDC is in general an NP-Complete problem. For a consistent network of basic CDC constraints, our algorithm also returns a 'canonical' solution in cubic time. This cubic algorithm is also adapted to cope with cardinal directions between possibly disconnected regions, in which case currently the best algorithm is of time complexity O(n^5).\n    ",
        "submission_date": "2009-09-01T00:00:00",
        "last_modified_date": "2009-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.0173",
        "title": "A theory of intelligence: networked problem solving in animal societies",
        "authors": [
            "Robert Shour"
        ],
        "abstract": "A society's single emergent, increasing intelligence arises partly from the thermodynamic advantages of networking the innate intelligence of different individuals, and partly from the accumulation of solved problems. Economic growth is proportional to the square of the network entropy of a society's population times the network entropy of the number of the society's solved problems.\n    ",
        "submission_date": "2009-09-01T00:00:00",
        "last_modified_date": "2012-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.0682",
        "title": "On Planning with Preferences in HTN",
        "authors": [
            "Shirin Sohrabi",
            "Sheila A. McIlraith"
        ],
        "abstract": "  In this paper, we address the problem of generating preferred plans by combining the procedural control knowledge specified by Hierarchical Task Networks (HTNs) with rich qualitative user preferences. The outcome of our work is a language for specifyin user preferences, tailored to HTN planning, together with a provably optimal preference-based planner, HTNPLAN, that is implemented as an extension of SHOP2. To compute preferred plans, we propose an approach based on forward-chaining heuristic search. Our heuristic uses an admissible evaluation function measuring the satisfaction of preferences over partial plans. Our empirical evaluation demonstrates the effectiveness of our HTNPLAN heuristics. We prove our approach sound and optimal with respect to the plans it generates by appealing to a situation calculus semantics of our preference language and of HTN planning. While our implementation builds on SHOP2, the language and techniques proposed here are relevant to a broad range of HTN planners.\n    ",
        "submission_date": "2009-09-03T00:00:00",
        "last_modified_date": "2009-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.0801",
        "title": "A Monte Carlo AIXI Approximation",
        "authors": [
            "Joel Veness",
            "Kee Siong Ng",
            "Marcus Hutter",
            "William Uther",
            "David Silver"
        ],
        "abstract": "This paper introduces a principled approach for the design of a scalable general reinforcement learning agent. Our approach is based on a direct approximation of AIXI, a Bayesian optimality notion for general reinforcement learning agents. Previously, it has been unclear whether the theory of AIXI could motivate the design of practical algorithms. We answer this hitherto open question in the affirmative, by providing the first computationally feasible approximation to the AIXI agent. To develop our approximation, we introduce a new Monte-Carlo Tree Search algorithm along with an agent-specific extension to the Context Tree Weighting algorithm. Empirically, we present a set of encouraging results on a variety of stochastic and partially observable domains. We conclude by proposing a number of directions for future research.\n    ",
        "submission_date": "2009-09-04T00:00:00",
        "last_modified_date": "2010-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.0901",
        "title": "Assessing the Impact of Informedness on a Consultant's Profit",
        "authors": [
            "Eugen Staab",
            "Martin Caminada"
        ],
        "abstract": "  We study the notion of informedness in a client-consultant setting. Using a software simulator, we examine the extent to which it pays off for consultants to provide their clients with advice that is well-informed, or with advice that is merely meant to appear to be well-informed. The latter strategy is beneficial in that it costs less resources to keep up-to-date, but carries the risk of a decreased reputation if the clients discover the low level of informedness of the consultant. Our experimental results indicate that under different circumstances, different strategies yield the optimal results (net profit) for the consultants.\n    ",
        "submission_date": "2009-09-04T00:00:00",
        "last_modified_date": "2009-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.1021",
        "title": "A multiagent urban traffic simulation Part I: dealing with the ordinary",
        "authors": [
            "Pierrick Tranouez",
            "Patrice Langlois",
            "Eric Daud\u00e9"
        ],
        "abstract": "  We describe in this article a multiagent urban traffic simulation, as we believe individual-based modeling is necessary to encompass the complex influence the actions of an individual vehicle can have on the overall flow of vehicles. We first describe how we build a graph description of the network from purely geometric data, ESRI shapefiles. We then explain how we include traffic related data to this graph. We go on after that with the model of the vehicle agents: origin and destination, driving behavior, multiple lanes, crossroads, and interactions with the other vehicles in day-to-day, ?ordinary? traffic. We conclude with the presentation of the resulting simulation of this model on the Rouen agglomeration.\n    ",
        "submission_date": "2009-09-05T00:00:00",
        "last_modified_date": "2009-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.1151",
        "title": "n-Opposition theory to structure debates",
        "authors": [
            "Jean Sallantin",
            "Antoine Seilles"
        ],
        "abstract": "  2007 was the first international congress on the ?square of oppositions?. A first attempt to structure debate using n-opposition theory was presented along with the results of a first experiment on the web. Our proposal for this paper is to define relations between arguments through a structure of opposition (square of oppositions is one structure of opposition). We will be trying to answer the following questions: How to organize debates on the web 2.0? How to structure them in a logical way? What is the role of n-opposition theory, in this context? We present in this paper results of three experiments (Betapolitique 2007, ECAP 2008, Intermed 2008).\n    ",
        "submission_date": "2009-09-07T00:00:00",
        "last_modified_date": "2009-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.2091",
        "title": "Paired Comparisons-based Interactive Differential Evolution",
        "authors": [
            "Hideyuki Takagi",
            "Denis Pallez"
        ],
        "abstract": "  We propose Interactive Differential Evolution (IDE) based on paired comparisons for reducing user fatigue and evaluate its convergence speed in comparison with Interactive Genetic Algorithms (IGA) and tournament IGA. User interface and convergence performance are two big keys for reducing Interactive Evolutionary Computation (IEC) user fatigue. Unlike IGA and conventional IDE, users of the proposed IDE and tournament IGA do not need to compare whole individuals each other but compare pairs of individuals, which largely decreases user fatigue. In this paper, we design a pseudo-IEC user and evaluate another factor, IEC convergence performance, using IEC simulators and show that our proposed IDE converges significantly faster than IGA and tournament IGA, i.e. our proposed one is superior to others from both user interface and convergence performance points of view.\n    ",
        "submission_date": "2009-09-11T00:00:00",
        "last_modified_date": "2009-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.2309",
        "title": "Logic with Verbs",
        "authors": [
            "Jun Tanaka"
        ],
        "abstract": "  The aim of this paper is to introduce a logic in which nouns and verbs are handled together as a deductive reasoning, and also to observe the relationship between nouns and verbs as well as between logics and conversations.\n    ",
        "submission_date": "2009-09-12T00:00:00",
        "last_modified_date": "2009-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.2339",
        "title": "Back analysis based on SOM-RST system",
        "authors": [
            "H. Owladeghaffari",
            "H. Aghababaei"
        ],
        "abstract": "  This paper describes application of information granulation theory, on the back analysis of Jeffrey mine southeast wall Quebec. In this manner, using a combining of Self Organizing Map (SOM) and rough set theory (RST), crisp and rough granules are obtained. Balancing of crisp granules and sub rough granules is rendered in close-open iteration. Combining of hard and soft computing, namely finite difference method (FDM) and computational intelligence and taking in to account missing information are two main benefits of the proposed method. As a practical example, reverse analysis on the failure of the southeast wall Jeffrey mine is accomplished.\n    ",
        "submission_date": "2009-09-12T00:00:00",
        "last_modified_date": "2009-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.2375",
        "title": "Similarity Matching Techniques for Fault Diagnosis in Automotive Infotainment Electronics",
        "authors": [
            "Mashud Kabir"
        ],
        "abstract": "  Fault diagnosis has become a very important area of research during the last decade due to the advancement of mechanical and electrical systems in industries. The automobile is a crucial field where fault diagnosis is given a special attention. Due to the increasing complexity and newly added features in vehicles, a comprehensive study has to be performed in order to achieve an appropriate diagnosis model. A diagnosis system is capable of identifying the faults of a system by investigating the observable effects (or symptoms). The system categorizes the fault into a diagnosis class and identifies a probable cause based on the supplied fault symptoms. Fault categorization and identification are done using similarity matching techniques. The development of diagnosis classes is done by making use of previous experience, knowledge or information within an application area. The necessary information used may come from several sources of knowledge, such as from system analysis. In this paper similarity matching techniques for fault diagnosis in automotive infotainment applications are discussed.\n    ",
        "submission_date": "2009-09-12T00:00:00",
        "last_modified_date": "2009-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.2376",
        "title": "Performing Hybrid Recommendation in Intermodal Transportation-the FTMarket System's Recommendation Module",
        "authors": [
            "Alexis Lazanas"
        ],
        "abstract": "  Diverse recommendation techniques have been already proposed and encapsulated into several e-business applications, aiming to perform a more accurate evaluation of the existing information and accordingly augment the assistance provided to the users involved. This paper reports on the development and integration of a recommendation module in an agent-based transportation transactions management system. The module is built according to a novel hybrid recommendation technique, which combines the advantages of collaborative filtering and knowledge-based approaches. The proposed technique and supporting module assist customers in considering in detail alternative transportation transactions that satisfy their requests, as well as in evaluating completed transactions. The related services are invoked through a software agent that constructs the appropriate knowledge rules and performs a synthesis of the recommendation policy.\n    ",
        "submission_date": "2009-09-12T00:00:00",
        "last_modified_date": "2009-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.2542",
        "title": "Stochastic Optimization of Linear Dynamic Systems with Parametric Uncertainties",
        "authors": [
            "Vadim Yatsenko"
        ],
        "abstract": "  This paper describes a new approach to solving some stochastic optimization problems for linear dynamic system with various parametric uncertainties. Proposed approach is based on application of tensor formalism for creation the mathematical model of parametric uncertainties. Within proposed approach following problems are considered: prediction, data processing and optimal control. Outcomes of carried out simulation are used as illustration of properties and effectiveness of proposed methods.\n    ",
        "submission_date": "2009-09-14T00:00:00",
        "last_modified_date": "2009-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.3273",
        "title": "Decomposition of the NVALUE constraint",
        "authors": [
            "Christian Bessiere",
            "George Katsirelos",
            "Nina Narodytska",
            "Claude-Guy Quimper",
            "Toby Walsh"
        ],
        "abstract": "  We study decompositions of NVALUE, a global constraint that can be used to model a wide range of problems where values need to be counted. Whilst decomposition typically hinders propagation, we identify one decomposition that maintains a global view as enforcing bound consistency on the decomposition achieves bound consistency on the original global NVALUE constraint. Such decompositions offer the prospect for advanced solving techniques like nogood learning and impact based branching heuristics. They may also help SAT and IP solvers take advantage of the propagation of global constraints.\n    ",
        "submission_date": "2009-09-17T00:00:00",
        "last_modified_date": "2009-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.3276",
        "title": "Symmetries of Symmetry Breaking Constraints",
        "authors": [
            "George Katsirelos",
            "Toby Walsh"
        ],
        "abstract": "  Symmetry is an important feature of many constraint programs. We show that any symmetry acting on a set of symmetry breaking constraints can be used to break symmetry. Different symmetries pick out different solutions in each symmetry class. We use these observations in two methods for eliminating symmetry from a problem. These methods are designed to have many of the advantages of symmetry breaking methods that post static symmetry breaking constraint without some of the disadvantages. In particular, the two methods prune the search space using fast and efficient propagation of posted constraints, whilst reducing the conflict between symmetry breaking and branching heuristics. Experimental results show that the two methods perform well on some standard benchmarks.\n    ",
        "submission_date": "2009-09-17T00:00:00",
        "last_modified_date": "2009-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.3648",
        "title": "Random scattering of bits by prediction",
        "authors": [
            "Joel Ratsaby"
        ],
        "abstract": "We investigate a population of binary mistake sequences that result from learning with parametric models of different order. We obtain estimates of their error, algorithmic complexity and divergence from a purely random Bernoulli sequence. We study the relationship of these variables to the learner's information density parameter which is defined as the ratio between the lengths of the compressed to uncompressed files that contain the learner's decision rule. The results indicate that good learners have a low information density$\\rho$ while bad learners have a high $\\rho$. Bad learners generate mistake sequences that are atypically complex or diverge stochastically from a purely random Bernoulli sequence. Good learners generate typically complex sequences with low divergence from Bernoulli sequences and they include mistake sequences generated by the Bayes optimal predictor. Based on the static algorithmic interference model of \\cite{Ratsaby_entropy} the learner here acts as a static structure which \"scatters\" the bits of an input sequence (to be predicted) in proportion to its information density $\\rho$ thereby deforming its randomness characteristics.\n    ",
        "submission_date": "2009-09-20T00:00:00",
        "last_modified_date": "2010-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.4437",
        "title": "Manipulation and gender neutrality in stable marriage procedures",
        "authors": [
            "Maria Pini",
            "Francesca Rossi",
            "Brent Venable",
            "Toby Walsh"
        ],
        "abstract": "  The stable marriage problem is a well-known problem of matching men to women so that no man and woman who are not married to each other both prefer each other. Such a problem has a wide variety of practical applications ranging from matching resident doctors to hospitals to matching students to schools. A well-known algorithm to solve this problem is the Gale-Shapley algorithm, which runs in polynomial time.\n",
        "submission_date": "2009-09-24T00:00:00",
        "last_modified_date": "2009-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.4441",
        "title": "Dealing with incomplete agents' preferences and an uncertain agenda in group decision making via sequential majority voting",
        "authors": [
            "Maria Pini",
            "Francesca Rossi",
            "Brent Venable",
            "Toby Walsh"
        ],
        "abstract": "  We consider multi-agent systems where agents' preferences are aggregated via sequential majority voting: each decision is taken by performing a sequence of pairwise comparisons where each comparison is a weighted majority vote among the agents. Incompleteness in the agents' preferences is common in many real-life settings due to privacy issues or an ongoing elicitation process. In addition, there may be uncertainty about how the preferences are aggregated. For example, the agenda (a tree whose leaves are labelled with the decisions being compared) may not yet be known or fixed. We therefore study how to determine collectively optimal decisions (also called winners) when preferences may be incomplete, and when the agenda may be uncertain. We show that it is computationally easy to determine if a candidate decision always wins, or may win, whatever the agenda. On the other hand, it is computationally hard to know wheth er a candidate decision wins in at least one agenda for at least one completion of the agents' preferences. These results hold even if the agenda must be balanced so that each candidate decision faces the same number of majority votes. Such results are useful for reasoning about preference elicitation. They help understand the complexity of tasks such as determining if a decision can be taken collectively, as well as knowing if the winner can be manipulated by appropriately ordering the agenda.\n    ",
        "submission_date": "2009-09-24T00:00:00",
        "last_modified_date": "2009-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.4446",
        "title": "Elicitation strategies for fuzzy constraint problems with missing preferences: algorithms and experimental studies",
        "authors": [
            "Mirco Gelain",
            "Maria Pini",
            "Francesca Rossi",
            "Brent Venable",
            "Toby Walsh"
        ],
        "abstract": "  Fuzzy constraints are a popular approach to handle preferences and over-constrained problems in scenarios where one needs to be cautious, such as in medical or space applications. We consider here fuzzy constraint problems where some of the preferences may be missing. This models, for example, settings where agents are distributed and have privacy issues, or where there is an ongoing preference elicitation process. In this setting, we study how to find a solution which is optimal irrespective of the missing preferences. In the process of finding such a solution, we may elicit preferences from the user if necessary. However, our goal is to ask the user as little as possible. We define a combined solving and preference elicitation scheme with a large number of different instantiations, each corresponding to a concrete algorithm which we compare experimentally. We compute both the number of elicited preferences and the \"user effort\", which may be larger, as it contains all the preference values the user has to compute to be able to respond to the elicitation requests. While the number of elicited preferences is important when the concern is to communicate as little information as possible, the user effort measures also the hidden work the user has to do to be able to communicate the elicited preferences. Our experimental results show that some of our algorithms are very good at finding a necessarily optimal solution while asking the user for only a very small fraction of the missing preferences. The user effort is also very small for the best algorithms. Finally, we test these algorithms on hard constraint problems with possibly missing constraints, where the aim is to find feasible solutions irrespective of the missing constraints.\n    ",
        "submission_date": "2009-09-24T00:00:00",
        "last_modified_date": "2009-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.4452",
        "title": "Flow-Based Propagators for the SEQUENCE and Related Global Constraints",
        "authors": [
            "Michael J. Maher",
            "Nina Narodytska",
            "Claude-Guy Quimper",
            "Toby Walsh"
        ],
        "abstract": "  We propose new filtering algorithms for the SEQUENCE constraint and some extensions of the SEQUENCE constraint based on network flows. We enforce domain consistency on the SEQUENCE constraint in $O(n^2)$ time down a branch of the search tree. This improves upon the best existing domain consistency algorithm by a factor of $O(\\log n)$. The flows used in these algorithms are derived from a linear program. Some of them differ from the flows used to propagate global constraints like GCC since the domains of the variables are encoded as costs on the edges rather than capacities. Such flows are efficient for maintaining bounds consistency over large domains and may be useful for other global constraints.\n    ",
        "submission_date": "2009-09-24T00:00:00",
        "last_modified_date": "2009-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.4456",
        "title": "The Weighted CFG Constraint",
        "authors": [
            "George Katsirelos",
            "Nina Narodytska",
            "Toby Walsh"
        ],
        "abstract": "  We introduce the weighted CFG constraint and propose a propagation algorithm that enforces domain consistency in $O(n^3|G|)$ time. We show that this algorithm can be decomposed into a set of primitive arithmetic constraints without hindering propagation.\n    ",
        "submission_date": "2009-09-24T00:00:00",
        "last_modified_date": "2009-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.5099",
        "title": "Breaking Generator Symmetry",
        "authors": [
            "George Katsirelos",
            "Nina Narodytska",
            "Toby Walsh"
        ],
        "abstract": "  Dealing with large numbers of symmetries is often problematic. One solution is to focus on just symmetries that generate the symmetry group. Whilst there are special cases where breaking just the symmetries in a generating set is complete, there are also cases where no irredundant generating set eliminates all symmetry. However, focusing on just generators improves tractability. We prove that it is polynomial in the size of the generating set to eliminate all symmetric solutions, but NP-hard to prune all symmetric values. Our proof considers row and column symmetry, a common type of symmetry in matrix models where breaking just generator symmetries is very effective. We show that propagating a conjunction of lexicographical ordering constraints on the rows and columns of a matrix of decision variables is NP-hard.\n    ",
        "submission_date": "2009-09-28T00:00:00",
        "last_modified_date": "2009-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.0542",
        "title": "Pre-processing in AI based Prediction of QSARs",
        "authors": [
            "Om Prasad Patri",
            "Amit Kumar Mishra"
        ],
        "abstract": "  Machine learning, data mining and artificial intelligence (AI) based methods have been used to determine the relations between chemical structure and biological activity, called quantitative structure activity relationships (QSARs) for the compounds. Pre-processing of the dataset, which includes the mapping from a large number of molecular descriptors in the original high dimensional space to a small number of components in the lower dimensional space while retaining the features of the original data, is the first step in this process. A common practice is to use a mapping method for a dataset without prior analysis. This pre-analysis has been stressed in our work by applying it to two important classes of QSAR prediction problems: drug design (predicting anti-HIV-1 activity) and predictive toxicology (estimating hepatocarcinogenicity of chemicals). We apply one linear and two nonlinear mapping methods on each of the datasets. Based on this analysis, we conclude the nature of the inherent relationships between the elements of each dataset, and hence, the mapping method best suited for it. We also show that proper preprocessing can help us in choosing the right feature extraction tool as well as give an insight about the type of classifier pertinent for the given problem.\n    ",
        "submission_date": "2009-10-03T00:00:00",
        "last_modified_date": "2009-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.1014",
        "title": "Building upon Fast Multipole Methods to Detect and Model Organizations",
        "authors": [
            "Pierrick Tranouez",
            "Antoine Dutot"
        ],
        "abstract": "  Many models in natural and social sciences are comprised of sets of inter-acting entities whose intensity of interaction decreases with distance. This often leads to structures of interest in these models composed of dense packs of entities. Fast Multipole Methods are a family of methods developed to help with the calculation of a number of computable models such as described above. We propose a method that builds upon FMM to detect and model the dense structures of these systems.\n    ",
        "submission_date": "2009-10-06T00:00:00",
        "last_modified_date": "2009-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.1026",
        "title": "A multiagent urban traffic simulation. Part II: dealing with the extraordinary",
        "authors": [
            "Eric Daud\u00e9",
            "Pierrick Tranouez",
            "Patrice Langlois"
        ],
        "abstract": "  In Probabilistic Risk Management, risk is characterized by two quantities: the magnitude (or severity) of the adverse consequences that can potentially result from the given activity or action, and by the likelihood of occurrence of the given adverse consequences. But a risk seldom exists in isolation: chain of consequences must be examined, as the outcome of one risk can increase the likelihood of other risks. Systemic theory must complement classic PRM. Indeed these chains are composed of many different elements, all of which may have a critical importance at many different levels. Furthermore, when urban catastrophes are envisioned, space and time constraints are key determinants of the workings and dynamics of these chains of catastrophes: models must include a correct spatial topology of the studied risk. Finally, literature insists on the importance small events can have on the risk on a greater scale: urban risks management models belong to self-organized criticality theory. We chose multiagent systems to incorporate this property in our model: the behavior of an agent can transform the dynamics of important groups of them.\n    ",
        "submission_date": "2009-10-06T00:00:00",
        "last_modified_date": "2009-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.1238",
        "title": "A Local Search Modeling for Constrained Optimum Paths Problems (Extended Abstract)",
        "authors": [
            "Quang Dung Pham",
            "Yves Deville",
            "Pascal Van Hentenryck"
        ],
        "abstract": "  Constrained Optimum Path (COP) problems appear in many real-life applications, especially on communication networks. Some of these problems have been considered and solved by specific techniques which are usually difficult to extend. In this paper, we introduce a novel local search modeling for solving some COPs by local search. The modeling features the compositionality, modularity, reuse and strengthens the benefits of Constrained-Based Local Search. We also apply the modeling to the edge-disjoint paths problem (EDP). We show that side constraints can easily be added in the model. Computational results show the significance of the approach.\n    ",
        "submission_date": "2009-10-07T00:00:00",
        "last_modified_date": "2009-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.1239",
        "title": "Dynamic Demand-Capacity Balancing for Air Traffic Management Using Constraint-Based Local Search: First Results",
        "authors": [
            "Farshid Hassani Bijarbooneh",
            "Pierre Flener",
            "Justin Pearson"
        ],
        "abstract": "  Using constraint-based local search, we effectively model and efficiently solve the problem of balancing the traffic demands on portions of the European airspace while ensuring that their capacity constraints are satisfied. The traffic demand of a portion of airspace is the hourly number of flights planned to enter it, and its capacity is the upper bound on this number under which air-traffic controllers can work. Currently, the only form of demand-capacity balancing we allow is ground holding, that is the changing of the take-off times of not yet airborne flights. Experiments with projected European flight plans of the year 2030 show that already this first form of demand-capacity balancing is feasible without incurring too much total delay and that it can lead to a significantly better demand-capacity balance.\n    ",
        "submission_date": "2009-10-07T00:00:00",
        "last_modified_date": "2009-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.1244",
        "title": "On Improving Local Search for Unsatisfiability",
        "authors": [
            "David Pereira",
            "In\u00eas Lynce",
            "Steven Prestwich"
        ],
        "abstract": "  Stochastic local search (SLS) has been an active field of research in the last few years, with new techniques and procedures being developed at an astonishing rate. SLS has been traditionally associated with satisfiability solving, that is, finding a solution for a given problem instance, as its intrinsic nature does not address unsatisfiable problems. Unsatisfiable instances were therefore commonly solved using backtrack search solvers. For this reason, in the late 90s Selman, Kautz and McAllester proposed a challenge to use local search instead to prove unsatisfiability. More recently, two SLS solvers - Ranger and Gunsat - have been developed, which are able to prove unsatisfiability albeit being SLS solvers. In this paper, we first compare Ranger with Gunsat and then propose to improve Ranger performance using some of Gunsat's techniques, namely unit propagation look-ahead and extended resolution.\n    ",
        "submission_date": "2009-10-07T00:00:00",
        "last_modified_date": "2009-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.1247",
        "title": "Integrating Conflict Driven Clause Learning to Local Search",
        "authors": [
            "Gilles Audenard",
            "Jean-Marie Lagniez",
            "Bertrand Mazure",
            "Lakhdar Sa\u00efs"
        ],
        "abstract": "  This article introduces SatHyS (SAT HYbrid Solver), a novel hybrid approach for propositional satisfiability. It combines local search and conflict driven clause learning (CDCL) scheme. Each time the local search part reaches a local minimum, the CDCL is launched. For SAT problems it behaves like a tabu list, whereas for UNSAT ones, the CDCL part tries to focus on minimum unsatisfiable sub-formula (MUS). Experimental results show good performances on many classes of SAT instances from the last SAT competitions.\n    ",
        "submission_date": "2009-10-07T00:00:00",
        "last_modified_date": "2009-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.1253",
        "title": "A Constraint-directed Local Search Approach to Nurse Rostering Problems",
        "authors": [
            "Fang He",
            "Rong Qu"
        ],
        "abstract": "  In this paper, we investigate the hybridization of constraint programming and local search techniques within a large neighbourhood search scheme for solving highly constrained nurse rostering problems. As identified by the research, a crucial part of the large neighbourhood search is the selection of the fragment (neighbourhood, i.e. the set of variables), to be relaxed and re-optimized iteratively. The success of the large neighbourhood search depends on the adequacy of this identified neighbourhood with regard to the problematic part of the solution assignment and the choice of the neighbourhood size. We investigate three strategies to choose the fragment of different sizes within the large neighbourhood search scheme. The first two strategies are tailored concerning the problem properties. The third strategy is more general, using the information of the cost from the soft constraint violations and their propagation as the indicator to choose the variables added into the fragment. The three strategies are analyzed and compared upon a benchmark nurse rostering problem. Promising results demonstrate the possibility of future work in the hybrid approach.\n    ",
        "submission_date": "2009-10-07T00:00:00",
        "last_modified_date": "2009-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.1255",
        "title": "Sonet Network Design Problems",
        "authors": [
            "Marie Pelleau",
            "Pascal Van Hentenryck",
            "Charlotte Truchet"
        ],
        "abstract": "  This paper presents a new method and a constraint-based objective function to solve two problems related to the design of optical telecommunication networks, namely the Synchronous Optical Network Ring Assignment Problem (SRAP) and the Intra-ring Synchronous Optical Network Design Problem (IDP). These network topology problems can be represented as a graph partitioning with capacity constraints as shown in previous works. We present here a new objective function and a new local search algorithm to solve these problems. Experiments conducted in Comet allow us to compare our method to previous ones and show that we obtain better results.\n    ",
        "submission_date": "2009-10-07T00:00:00",
        "last_modified_date": "2009-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.1264",
        "title": "Parallel local search for solving Constraint Problems on the Cell Broadband Engine (Preliminary Results)",
        "authors": [
            "Salvator Abreu",
            "Daniel Diaz",
            "Philippe Codognet"
        ],
        "abstract": "  We explore the use of the Cell Broadband Engine (Cell/BE for short) for combinatorial optimization applications: we present a parallel version of a constraint-based local search algorithm that has been implemented on a multiprocessor BladeCenter machine with twin Cell/BE processors (total of 16 SPUs per blade). This algorithm was chosen because it fits very well the Cell/BE architecture and requires neither shared memory nor communication between processors, while retaining a compact memory footprint. We study the performance on several large optimization benchmarks and show that this achieves mostly linear time speedups, even sometimes super-linear. This is possible because the parallel implementation might explore simultaneously different parts of the search space and therefore converge faster towards the best sub-space and thus towards a solution. Besides getting speedups, the resulting times exhibit a much smaller variance, which benefits applications where a timely reply is critical.\n    ",
        "submission_date": "2009-10-07T00:00:00",
        "last_modified_date": "2009-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.1266",
        "title": "Toward an automaton Constraint for Local Search",
        "authors": [
            "Jun He",
            "Pierre Flener",
            "Justin Pearson"
        ],
        "abstract": "  We explore the idea of using finite automata to implement new constraints for local search (this is already a successful technique in constraint-based global search). We show how it is possible to maintain incrementally the violations of a constraint and its decision variables from an automaton that describes a ground checker for that constraint. We establish the practicality of our approach idea on real-life personnel rostering problems, and show that it is competitive with the approach of [Pralong, 2007].\n    ",
        "submission_date": "2009-10-07T00:00:00",
        "last_modified_date": "2009-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.1404",
        "title": "Proceedings 6th International Workshop on Local Search Techniques in Constraint Satisfaction",
        "authors": [
            "Yves Deville",
            "Christine Solnon"
        ],
        "abstract": "  LSCS is a satellite workshop of the international conference on principles and practice of Constraint Programming (CP), since 2004. It is devoted to local search techniques in constraint satisfaction, and focuses on all aspects of local search techniques, including: design and implementation of new algorithms, hybrid stochastic-systematic search, reactive search optimization, adaptive search, modeling for local-search, global constraints, flexibility and robustness, learning methods, and specific applications.\n    ",
        "submission_date": "2009-10-08T00:00:00",
        "last_modified_date": "2009-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.1433",
        "title": "Tracking object's type changes with fuzzy based fusion rule",
        "authors": [
            "Albena Tchamova",
            "Jean Dezert",
            "Florentin Smarandache"
        ],
        "abstract": "  In this paper the behavior of three combinational rules for temporal/sequential attribute data fusion for target type estimation are analyzed. The comparative analysis is based on: Dempster's fusion rule proposed in Dempster-Shafer Theory; Proportional Conflict Redistribution rule no. 5 (PCR5), proposed in Dezert-Smarandache Theory and one alternative class fusion rule, connecting the combination rules for information fusion with particular fuzzy operators, focusing on the t-norm based Conjunctive rule as an analog of the ordinary conjunctive rule and t-conorm based Disjunctive rule as an analog of the ordinary disjunctive rule. The way how different t-conorms and t-norms functions within TCN fusion rule influence over target type estimation performance is studied and estimated.\n    ",
        "submission_date": "2009-10-08T00:00:00",
        "last_modified_date": "2009-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.1800",
        "title": "Scaling Analysis of Affinity Propagation",
        "authors": [
            "Cyril Furtlehner",
            "Michele Sebag",
            "Xiangliang Zhang"
        ],
        "abstract": "  We analyze and exploit some scaling properties of the Affinity Propagation (AP) clustering algorithm proposed by Frey and Dueck (2007). First we observe that a divide and conquer strategy, used on a large data set hierarchically reduces the complexity ${\\cal O}(N^2)$ to ${\\cal O}(N^{(h+2)/(h+1)})$, for a data-set of size $N$ and a depth $h$ of the hierarchical strategy. For a data-set embedded in a $d$-dimensional space, we show that this is obtained without notably damaging the precision except in dimension $d=2$. In fact, for $d$ larger than 2 the relative loss in precision scales like $N^{(2-d)/(h+1)d}$. Finally, under some conditions we observe that there is a value $s^*$ of the penalty coefficient, a free parameter used to fix the number of clusters, which separates a fragmentation phase (for $s<s^*$) from a coalescent one (for $s>s^*$) of the underlying hidden cluster structure. At this precise point holds a self-similarity property which can be exploited by the hierarchical strategy to actually locate its position. From this observation, a strategy based on \\AP can be defined to find out how many clusters are present in a given dataset.\n    ",
        "submission_date": "2009-10-09T00:00:00",
        "last_modified_date": "2009-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.2039",
        "title": "Higher coordination with less control - A result of information maximization in the sensorimotor loop",
        "authors": [
            "Keyan Zahedi",
            "Nihat Ay",
            "Ralf Der"
        ],
        "abstract": "This work presents a novel learning method in the context of embodied artificial intelligence and self-organization, which has as few assumptions and restrictions as possible about the world and the underlying model. The learning rule is derived from the principle of maximizing the predictive information in the sensorimotor loop. It is evaluated on robot chains of varying length with individually controlled, non-communicating segments. The comparison of the results shows that maximizing the predictive information per wheel leads to a higher coordinated behavior of the physically connected robots compared to a maximization per robot. Another focus of this paper is the analysis of the effect of the robot chain length on the overall behavior of the robots. It will be shown that longer chains with less capable controllers outperform those of shorter length and more complex controllers. The reason is found and discussed in the information-geometric interpretation of the learning process.\n    ",
        "submission_date": "2009-10-11T00:00:00",
        "last_modified_date": "2010-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.2217",
        "title": "Finite element model selection using Particle Swarm Optimization",
        "authors": [
            "Linda Mthembu",
            "Tshilidzi Marwala",
            "Michael I. Friswell",
            "Sondipon Adhikari"
        ],
        "abstract": "  This paper proposes the application of particle swarm optimization (PSO) to the problem of finite element model (FEM) selection. This problem arises when a choice of the best model for a system has to be made from set of competing models, each developed a priori from engineering judgment. PSO is a population-based stochastic search algorithm inspired by the behaviour of biological entities in nature when they are foraging for resources. Each potentially correct model is represented as a particle that exhibits both individualistic and group behaviour. Each particle moves within the model search space looking for the best solution by updating the parameters values that define it. The most important step in the particle swarm algorithm is the method of representing models which should take into account the number, location and variables of parameters to be updated. One example structural system is used to show the applicability of PSO in finding an optimal FEM. An optimal model is defined as the model that has the least number of updated parameters and has the smallest parameter variable variation from the mean material properties. Two different objective functions are used to compare performance of the PSO algorithm.\n    ",
        "submission_date": "2009-10-12T00:00:00",
        "last_modified_date": "2009-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.2593",
        "title": "A Component Based Heuristic Search Method with Evolutionary Eliminations",
        "authors": [
            "Jingpeng Li",
            "Uwe Aickelin",
            "Edmund Burke"
        ],
        "abstract": "  Nurse rostering is a complex scheduling problem that affects hospital personnel on a daily basis all over the world. This paper presents a new component-based approach with evolutionary eliminations, for a nurse scheduling problem arising at a major UK hospital. The main idea behind this technique is to decompose a schedule into its components (i.e. the allocated shift pattern of each nurse), and then to implement two evolutionary elimination strategies mimicking natural selection and natural mutation process on these components respectively to iteratively deliver better schedules. The worthiness of all components in the schedule has to be continuously demonstrated in order for them to remain there. This demonstration employs an evaluation function which evaluates how well each component contributes towards the final objective. Two elimination steps are then applied: the first elimination eliminates a number of components that are deemed not worthy to stay in the current schedule; the second elimination may also throw out, with a low level of probability, some worthy components. The eliminated components are replenished with new ones using a set of constructive heuristics using local optimality criteria. Computational results using 52 data instances demonstrate the applicability of the proposed approach in solving real-world problems.\n    ",
        "submission_date": "2009-10-14T00:00:00",
        "last_modified_date": "2009-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.2874",
        "title": "An Agent Based Classification Model",
        "authors": [
            "Feng Gu",
            "Uwe Aickelin",
            "Julie Greensmith"
        ],
        "abstract": "  The major function of this model is to access the UCI Wisconsin Breast Can- cer data-set[1] and classify the data items into two categories, which are normal and anomalous. This kind of classifi cation can be referred as anomaly detection, which discriminates anomalous behaviour from normal behaviour in computer systems. One popular solution for anomaly detection is Artifi cial Immune Sys- tems (AIS). AIS are adaptive systems inspired by theoretical immunology and observed immune functions, principles and models which are applied to prob- lem solving. The Dendritic Cell Algorithm (DCA)[2] is an AIS algorithm that is developed specifi cally for anomaly detection. It has been successfully applied to intrusion detection in computer security. It is believed that agent-based mod- elling is an ideal approach for implementing AIS, as intelligent agents could be the perfect representations of immune entities in AIS. This model evaluates the feasibility of re-implementing the DCA in an agent-based simulation environ- ment called AnyLogic, where the immune entities in the DCA are represented by intelligent agents. If this model can be successfully implemented, it makes it possible to implement more complicated and adaptive AIS models in the agent-based simulation environment.\n    ",
        "submission_date": "2009-10-15T00:00:00",
        "last_modified_date": "2009-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.3068",
        "title": "An Evolutionary Squeaky Wheel Optimisation Approach to Personnel Scheduling",
        "authors": [
            "Uwe Aickelin",
            "Jingpeng Li",
            "Edmund Burke"
        ],
        "abstract": "  The quest for robust heuristics that are able to solve more than one problem is ongoing. In this paper, we present, discuss and analyse a technique called Evolutionary Squeaky Wheel Optimisation and apply it to two different personnel scheduling problems. Evolutionary Squeaky Wheel Optimisation improves the original Squeaky Wheel Optimisation's effectiveness and execution speed by incorporating two extra steps (Selection and Mutation) for added evolution. In the Evolutionary Squeaky Wheel Optimisation, a cycle of Analysis-Selection-Mutation-Prioritization-Construction continues until stopping conditions are reached. The aim of the Analysis step is to identify below average solution components by calculating a fitness value for all components. The Selection step then chooses amongst these underperformers and discards some probabilistically based on fitness. The Mutation step further discards a few components at random. Solutions can become incomplete and thus repairs may be required. The repairs are carried out by using the Prioritization to first produce priorities that determine an order by which the following Construction step then schedules the remaining components. Therefore, improvement in the Evolutionary Squeaky Wheel Optimisation is achieved by selective solution disruption mixed with interative improvement and constructive repair. Strong experimental results are reported on two different domains of personnel scheduling: bus and rail driver scheduling and hospital nurse scheduling.\n    ",
        "submission_date": "2009-10-16T00:00:00",
        "last_modified_date": "2009-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.3115",
        "title": "An Idiotypic Immune Network as a Short Term Learning Architecture for Mobile Robots",
        "authors": [
            "Amanda Whitbrook",
            "Uwe Aickelin",
            "Jonathan M Garibaldi"
        ],
        "abstract": "  A combined Short-Term Learning (STL) and Long-Term Learning (LTL) approach to solving mobile robot navigation problems is presented and tested in both real and simulated environments. The LTL consists of rapid simulations that use a Genetic Algorithm to derive diverse sets of behaviours. These sets are then transferred to an idiotypic Artificial Immune System (AIS), which forms the STL phase, and the system is said to be seeded. The combined LTL-STL approach is compared with using STL only, and with using a handdesigned controller. In addition, the STL phase is tested when the idiotypic mechanism is turned off. The results provide substantial evidence that the best option is the seeded idiotypic system, i.e. the architecture that merges LTL with an idiotypic AIS for the STL. They also show that structurally different environments can be used for the two phases without compromising transferability\n    ",
        "submission_date": "2009-10-16T00:00:00",
        "last_modified_date": "2009-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.3117",
        "title": "An Immune Inspired Approach to Anomaly Detection",
        "authors": [
            "Jamie Twycross",
            "Uwe Aickelin"
        ],
        "abstract": "  The immune system provides a rich metaphor for computer security: anomaly detection that works in nature should work for machines. However, early artificial immune system approaches for computer security had only limited success. Arguably, this was due to these artificial systems being based on too simplistic a view of the immune system. We present here a second generation artificial immune system for process anomaly detection. It improves on earlier systems by having different artificial cell types that process information. Following detailed information about how to build such second generation systems, we find that communication between cells types is key to performance. Through realistic testing and validation we show that second generation artificial immune systems are capable of anomaly detection beyond generic system policies. The paper concludes with a discussion and outline of the next steps in this exciting area of computer security.\n    ",
        "submission_date": "2009-10-16T00:00:00",
        "last_modified_date": "2009-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.3124",
        "title": "An Immune Inspired Network Intrusion Detection System Utilising Correlation Context",
        "authors": [
            "Gianni Tedesco",
            "Uwe Aickelin"
        ],
        "abstract": "  Network Intrusion Detection Systems (NIDS) are computer systems which monitor a network with the aim of discerning malicious from benign activity on that network. While a wide range of approaches have met varying levels of success, most IDSs rely on having access to a database of known attack signatures which are written by security experts. Nowadays, in order to solve problems with false positive alerts, correlation algorithms are used to add additional structure to sequences of IDS alerts. However, such techniques are of no help in discovering novel attacks or variations of known attacks, something the human immune system (HIS) is capable of doing in its own specialised domain. This paper presents a novel immune algorithm for application to the IDS problem. The goal is to discover packets containing novel variations of attacks covered by an existing signature base.\n    ",
        "submission_date": "2009-10-16T00:00:00",
        "last_modified_date": "2009-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.3301",
        "title": "Faster Algorithms for Max-Product Message-Passing",
        "authors": [
            "Julian J. McAuley",
            "Tiberio S. Caetano"
        ],
        "abstract": "Maximum A Posteriori inference in graphical models is often solved via message-passing algorithms, such as the junction-tree algorithm, or loopy belief-propagation. The exact solution to this problem is well known to be exponential in the size of the model's maximal cliques after it is triangulated, while approximate inference is typically exponential in the size of the model's factors. In this paper, we take advantage of the fact that many models have maximal cliques that are larger than their constituent factors, and also of the fact that many factors consist entirely of latent variables (i.e., they do not depend on an observation). This is a common case in a wide variety of applications, including grids, trees, and ring-structured models. In such cases, we are able to decrease the exponent of complexity for message-passing by 0.5 for both exact and approximate inference.\n    ",
        "submission_date": "2009-10-17T00:00:00",
        "last_modified_date": "2010-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.3485",
        "title": "A Fuzzy Petri Nets Model for Computing With Words",
        "authors": [
            "Yongzhi Cao",
            "Guoqing Chen"
        ],
        "abstract": "  Motivated by Zadeh's paradigm of computing with words rather than numbers, several formal models of computing with words have recently been proposed. These models are based on automata and thus are not well-suited for concurrent computing. In this paper, we incorporate the well-known model of concurrent computing, Petri nets, together with fuzzy set theory and thereby establish a concurrency model of computing with words--fuzzy Petri nets for computing with words (FPNCWs). The new feature of such fuzzy Petri nets is that the labels of transitions are some special words modeled by fuzzy sets. By employing the methodology of fuzzy reasoning, we give a faithful extension of an FPNCW which makes it possible for computing with more words. The language expressiveness of the two formal models of computing with words, fuzzy automata for computing with words and FPNCWs, is compared as well. A few small examples are provided to illustrate the theoretical development.\n    ",
        "submission_date": "2009-10-19T00:00:00",
        "last_modified_date": "2009-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.4899",
        "title": "Artificial Immune Systems",
        "authors": [
            "Uwe Aickelin",
            "Dipankar Dasgupta"
        ],
        "abstract": "  The biological immune system is a robust, complex, adaptive system that defends the body from foreign pathogens. It is able to categorize all cells (or molecules) within the body as self-cells or non-self cells. It does this with the help of a distributed task force that has the intelligence to take action from a local and also a global perspective using its network of chemical messengers for communication. There are two major branches of the immune system. The innate immune system is an unchanging mechanism that detects and destroys certain invading organisms, whilst the adaptive immune system responds to previously unknown foreign cells and builds a response to them that can remain in the body over a long period of time. This remarkable information processing biological system has caught the attention of computer science in recent years. A novel computational intelligence technique, inspired by immunology, has emerged, called Artificial Immune Systems. Several concepts from the immune have been extracted and applied for solution to real world science and engineering problems. In this tutorial, we briefly describe the immune system metaphors that are relevant to existing Artificial Immune Systems methods. We will then show illustrative real-world problems suitable for Artificial Immune Systems and give a step-by-step algorithm walkthrough for one such problem. A comparison of the Artificial Immune Systems to other well-known algorithms, areas for future work, tips & tricks and a list of resources will round this tutorial off. It should be noted that as Artificial Immune Systems is still a young and evolving field, there is not yet a fixed algorithm template and hence actual implementations might differ somewhat from time to time and from those examples given here.\n    ",
        "submission_date": "2009-10-26T00:00:00",
        "last_modified_date": "2009-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.4903",
        "title": "Articulation and Clarification of the Dendritic Cell Algorithm",
        "authors": [
            "Julie Greensmith",
            "Uwe Aickelin",
            "Jamie Twycross"
        ],
        "abstract": "  The Dendritic Cell algorithm (DCA) is inspired by recent work in innate immunity. In this paper a formal description of the DCA is given. The DCA is described in detail, and its use as an anomaly detector is illustrated within the context of computer security. A port scan detection task is performed to substantiate the influence of signal selection on the behaviour of the algorithm. Experimental results provide a comparison of differing input signal mappings.\n    ",
        "submission_date": "2009-10-26T00:00:00",
        "last_modified_date": "2009-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.5405",
        "title": "Artificial Immune Tissue using Self-Orgamizing Networks",
        "authors": [
            "Jan Feyereisl",
            "Uwe Aickelin"
        ],
        "abstract": "  As introduced by Bentley et al. (2005), artificial immune systems (AIS) are lacking tissue, which is present in one form or another in all living multi-cellular organisms. Some have argued that this concept in the context of AIS brings little novelty to the already saturated field of the immune inspired computational research. This article aims to show that such a component of an AIS has the potential to bring an advantage to a data processing algorithm in terms of data pre-processing, clustering and extraction of features desired by the immune inspired system. The proposed tissue algorithm is based on self-organizing networks, such as self-organizing maps (SOM) developed by Kohonen (1996) and an analogy of the so called Toll-Like Receptors (TLR) affecting the activation function of the clusters developed by the SOM.\n    ",
        "submission_date": "2009-10-28T00:00:00",
        "last_modified_date": "2009-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.1021",
        "title": "Examples as Interaction: On Humans Teaching a Computer to Play a Game",
        "authors": [
            "Dimitris Kalles",
            "Ilias Fykouras"
        ],
        "abstract": "  This paper reviews an experiment in human-computer interaction, where interaction takes place when humans attempt to teach a computer to play a strategy board game. We show that while individually learned models can be shown to improve the playing performance of the computer, their straightforward composition results in diluting what was earlier learned. This observation suggests that interaction cannot be easily distributed when one hopes to harness multiple human experts to develop a quality computer player. This is related to similar approaches in robot task learning and to classic approaches to human learning and reinforces the need to develop tools that facilitate the mix of human-based tuition and computer self-learning.\n    ",
        "submission_date": "2009-11-05T00:00:00",
        "last_modified_date": "2009-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.1386",
        "title": "Machine Learning: When and Where the Horses Went Astray?",
        "authors": [
            "Emanuel Diamant"
        ],
        "abstract": "  Machine Learning is usually defined as a subfield of AI, which is busy with information extraction from raw data sets. Despite of its common acceptance and widespread recognition, this definition is wrong and groundless. Meaningful information does not belong to the data that bear it. It belongs to the observers of the data and it is a shared agreement and a convention among them. Therefore, this private information cannot be extracted from the data by any means. Therefore, all further attempts of Machine Learning apologists to justify their funny business are inappropriate.\n    ",
        "submission_date": "2009-11-07T00:00:00",
        "last_modified_date": "2009-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.1582",
        "title": "Manipulating Tournaments in Cup and Round Robin Competitions",
        "authors": [
            "Tyrel Russell",
            "Toby Walsh"
        ],
        "abstract": "  In sports competitions, teams can manipulate the result by, for instance, throwing games. We show that we can decide how to manipulate round robin and cup competitions, two of the most popular types of sporting competitions in polynomial time. In addition, we show that finding the minimal number of games that need to be thrown to manipulate the result can also be determined in polynomial time. Finally, we show that there are several different variations of standard cup competitions where manipulation remains polynomial.\n    ",
        "submission_date": "2009-11-09T00:00:00",
        "last_modified_date": "2009-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.1707",
        "title": "A Dynamic Vulnerability Map to Assess the Risk of Road Network Traffic Utilization",
        "authors": [
            "Michel Nabaa",
            "Cyrille Bertelle",
            "Antoine Dutot",
            "Damien Olivier",
            "Pascal Mallet"
        ],
        "abstract": "  Le Havre agglomeration (CODAH) includes 16 establishments classified Seveso with high threshold. In the literature, we construct vulnerability maps to help decision makers assess the risk. Such approaches remain static and do take into account the population displacement in the estimation of the vulnerability. We propose a decision making tool based on a dynamic vulnerability map to evaluate the difficulty of evacuation in the different sectors of CODAH. We use a Geographic Information system (GIS) to visualize the map which evolves with the road traffic state through a detection of communities in large graphs algorithm.\n    ",
        "submission_date": "2009-11-09T00:00:00",
        "last_modified_date": "2009-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.1708",
        "title": "Different goals in multiscale simulations and how to reach them",
        "authors": [
            "Pierrick Tranouez",
            "Antoine Dutot"
        ],
        "abstract": "  In this paper we sum up our works on multiscale programs, mainly simulations. We first start with describing what multiscaling is about, how it helps perceiving signal from a background noise in a ?ow of data for example, for a direct perception by a user or for a further use by another program. We then give three examples of multiscale techniques we used in the past, maintaining a summary, using an environmental marker introducing an history in the data and finally using a knowledge on the behavior of the different scales to really handle them at the same time.\n    ",
        "submission_date": "2009-11-09T00:00:00",
        "last_modified_date": "2009-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.2390",
        "title": "How Creative Should Creators Be To Optimize the Evolution of Ideas? A Computational Model",
        "authors": [
            "Stefan Leijnen",
            "Liane Gabora"
        ],
        "abstract": "  There are both benefits and drawbacks to creativity. In a social group it is not necessary for all members to be creative to benefit from creativity; some merely imitate or enjoy the fruits of others' creative efforts. What proportion should be creative? This paper contains a very preliminary investigation of this question carried out using a computer model of cultural evolution referred to as EVOC (for EVOlution of Culture). EVOC is composed of neural network based agents that evolve fitter ideas for actions by (1) inventing new ideas through modification of existing ones, and (2) imitating neighbors' ideas. The ideal proportion with respect to fitness of ideas occurs when thirty to forty percent of the individuals is creative. When creators are inventing 50% of iterations or less, mean fitness of actions in the society is a positive function of the ratio of creators to imitators; otherwise mean fitness of actions starts to drop when the ratio of creators to imitators exceeds approximately 30%. For all levels or creativity, the diversity of ideas in a population is positively correlated with the ratio of creative agents.\n    ",
        "submission_date": "2009-11-12T00:00:00",
        "last_modified_date": "2009-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.2405",
        "title": "Emotion: Appraisal-coping model for the \"Cascades\" problem",
        "authors": [
            "Karim Mahboub",
            "Evelyne Cl\u00e9ment",
            "Cyrille Bertelle",
            "V\u00e9ronique Jay"
        ],
        "abstract": "  Modelling emotion has become a challenge nowadays. Therefore, several models have been produced in order to express human emotional activity. However, only a few of them are currently able to express the close relationship existing between emotion and cognition. An appraisal-coping model is presented here, with the aim to simulate the emotional impact caused by the evaluation of a particular situation (appraisal), along with the consequent cognitive reaction intended to face the situation (coping). This model is applied to the \"Cascades\" problem, a small arithmetical exercise designed for ten-year-old pupils. The goal is to create a model corresponding to a child's behaviour when solving the problem using his own strategies.\n    ",
        "submission_date": "2009-11-12T00:00:00",
        "last_modified_date": "2009-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.2501",
        "title": "Emotion : mod\u00e8le d'appraisal-coping pour le probl\u00e8me des Cascades",
        "authors": [
            "Karim Mahboub",
            "Cyrille Bertelle",
            "V\u00e9ronique Jay",
            "Evelyne Cl\u00e9ment"
        ],
        "abstract": "  Modeling emotion has become a challenge nowadays. Therefore, several models have been produced in order to express human emotional activity. However, only a few of them are currently able to express the close relationship existing between emotion and cognition. An appraisal-coping model is presented here, with the aim to simulate the emotional impact caused by the evaluation of a particular situation (appraisal), along with the consequent cognitive reaction intended to face the situation (coping). This model is applied to the ?Cascades? problem, a small arithmetical exercise designed for ten-year-old pupils. The goal is to create a model corresponding to a child's behavior when solving the problem using his own strategies.\n    ",
        "submission_date": "2009-11-12T00:00:00",
        "last_modified_date": "2009-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.3108",
        "title": "On game psychology: an experiment on the chess board/screen, should you always \"do your best\", and why the programs with prescribed weaknesses cannot be our good friends?",
        "authors": [
            "Emanuel Gluskin"
        ],
        "abstract": "It is noted that some unusual moves against a strong chess program greatly weaken its ability to see the serious targets of the game, and its whole level of play... It is suggested to create programs with different weaknesses in order to analyze similar human behavior. Finally, a new version of chess, \"Chess Corrida\" is suggested.\n    ",
        "submission_date": "2009-11-16T00:00:00",
        "last_modified_date": "2011-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.3209",
        "title": "Apply Ant Colony Algorithm to Search All Extreme Points of Function",
        "authors": [
            "Chao-Yang Pang",
            "Hui Liu",
            "Xia Li",
            "Yun-Fei Wang",
            "Ben-Qiong Hu"
        ],
        "abstract": "  To find all extreme points of multimodal functions is called extremum problem, which is a well known difficult issue in optimization fields. Applying ant colony optimization (ACO) to solve this problem is rarely reported. The method of applying ACO to solve extremum problem is explored in this paper. Experiment shows that the solution error of the method presented in this paper is less than 10^-8. keywords: Extremum Problem; Ant Colony Optimization (ACO)\n    ",
        "submission_date": "2009-11-17T00:00:00",
        "last_modified_date": "2009-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.3708",
        "title": "Manipulability of Single Transferable Vote",
        "authors": [
            "Toby Walsh"
        ],
        "abstract": "  For many voting rules, it is NP-hard to compute a successful manipulation. However, NP-hardness only bounds the worst-case complexity. Recent theoretical results suggest that manipulation may often be easy in practice. We study empirically the cost of manipulating the single transferable vote (STV) rule. This was one of the first rules shown to be NP-hard to manipulate. It also appears to be one of the harder rules to manipulate since it involves multiple rounds and since, unlike many other rules, it is NP-hard for a single agent to manipulate without weights on the votes or uncertainty about how the other agents have voted. In almost every election in our experiments, it was easy to compute how a single agent could manipulate the election or to prove that manipulation by a single agent was impossible. It remains an interesting open question if manipulation by a coalition of agents is hard to compute in practice.\n    ",
        "submission_date": "2009-11-19T00:00:00",
        "last_modified_date": "2009-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.5043",
        "title": "A Semantic Similarity Measure for Expressive Description Logics",
        "authors": [
            "Claudia d'Amato",
            "Nicola Fanizzi",
            "Floriana Esposito"
        ],
        "abstract": "  A totally semantic measure is presented which is able to calculate a similarity value between concept descriptions and also between concept description and individual or between individuals expressed in an expressive description logic. It is applicable on symbolic descriptions although it uses a numeric approach for the calculus. Considering that Description Logics stand as the theoretic framework for the ontological knowledge representation and reasoning, the proposed measure can be effectively used for agglomerative and divisional clustering task applied to the semantic web domain.\n    ",
        "submission_date": "2009-11-26T00:00:00",
        "last_modified_date": "2009-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.5104",
        "title": "A Bayesian Rule for Adaptive Control based on Causal Interventions",
        "authors": [
            "Pedro A. Ortega",
            "Daniel A. Braun"
        ],
        "abstract": "  Explaining adaptive behavior is a central problem in artificial intelligence research. Here we formalize adaptive agents as mixture distributions over sequences of inputs and outputs (I/O). Each distribution of the mixture constitutes a `possible world', but the agent does not know which of the possible worlds it is actually facing. The problem is to adapt the I/O stream in a way that is compatible with the true world. A natural measure of adaptation can be obtained by the Kullback-Leibler (KL) divergence between the I/O distribution of the true world and the I/O distribution expected by the agent that is uncertain about possible worlds. In the case of pure input streams, the Bayesian mixture provides a well-known solution for this problem. We show, however, that in the case of I/O streams this solution breaks down, because outputs are issued by the agent itself and require a different probabilistic syntax as provided by intervention calculus. Based on this calculus, we obtain a Bayesian control rule that allows modeling adaptive behavior with mixture distributions over I/O streams. This rule might allow for a novel approach to adaptive control based on a minimum KL-principle.\n    ",
        "submission_date": "2009-11-26T00:00:00",
        "last_modified_date": "2009-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.5106",
        "title": "A conversion between utility and information",
        "authors": [
            "Pedro A. Ortega",
            "Daniel A. Braun"
        ],
        "abstract": "  Rewards typically express desirabilities or preferences over a set of alternatives. Here we propose that rewards can be defined for any probability distribution based on three desiderata, namely that rewards should be real-valued, additive and order-preserving, where the latter implies that more probable events should also be more desirable. Our main result states that rewards are then uniquely determined by the negative information content. To analyze stochastic processes, we define the utility of a realization as its reward rate. Under this interpretation, we show that the expected utility of a stochastic process is its negative entropy rate. Furthermore, we apply our results to analyze agent-environment interactions. We show that the expected utility that will actually be achieved by the agent is given by the negative cross-entropy from the input-output (I/O) distribution of the coupled interaction system and the agent's I/O distribution. Thus, our results allow for an information-theoretic interpretation of the notion of utility and the characterization of agent-environment interactions in terms of entropy dynamics.\n    ",
        "submission_date": "2009-11-26T00:00:00",
        "last_modified_date": "2009-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.5394",
        "title": "Covering rough sets based on neighborhoods: An approach without using neighborhoods",
        "authors": [
            "Ping Zhu"
        ],
        "abstract": "Rough set theory, a mathematical tool to deal with inexact or uncertain knowledge in information systems, has originally described the indiscernibility of elements by equivalence relations. Covering rough sets are a natural extension of classical rough sets by relaxing the partitions arising from equivalence relations to coverings. Recently, some topological concepts such as neighborhood have been applied to covering rough sets. In this paper, we further investigate the covering rough sets based on neighborhoods by approximation operations. We show that the upper approximation based on neighborhoods can be defined equivalently without using neighborhoods. To analyze the coverings themselves, we introduce unary and composition operations on coverings. A notion of homomorphismis provided to relate two covering approximation spaces. We also examine the properties of approximations preserved by the operations and homomorphisms, respectively.\n    ",
        "submission_date": "2009-11-28T00:00:00",
        "last_modified_date": "2010-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.5395",
        "title": "An axiomatic approach to the roughness measure of rough sets",
        "authors": [
            "Ping Zhu"
        ],
        "abstract": "In Pawlak's rough set theory, a set is approximated by a pair of lower and upper approximations. To measure numerically the roughness of an approximation, Pawlak introduced a quantitative measure of roughness by using the ratio of the cardinalities of the lower and upper approximations. Although the roughness measure is effective, it has the drawback of not being strictly monotonic with respect to the standard ordering on partitions. Recently, some improvements have been made by taking into account the granularity of partitions. In this paper, we approach the roughness measure in an axiomatic way. After axiomatically defining roughness measure and partition measure, we provide a unified construction of roughness measure, called strong Pawlak roughness measure, and then explore the properties of this measure. We show that the improved roughness measures in the literature are special instances of our strong Pawlak roughness measure and introduce three more strong Pawlak roughness measures as well. The advantage of our axiomatic approach is that some properties of a roughness measure follow immediately as soon as the measure satisfies the relevant axiomatic definition.\n    ",
        "submission_date": "2009-11-28T00:00:00",
        "last_modified_date": "2010-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.0132",
        "title": "Opportunistic Adaptation Knowledge Discovery",
        "authors": [
            "Fadi Badra",
            "Am\u00e9lie Cordier",
            "Jean Lieber"
        ],
        "abstract": "  Adaptation has long been considered as the Achilles' heel of case-based reasoning since it requires some domain-specific knowledge that is difficult to acquire. In this paper, two strategies are combined in order to reduce the knowledge engineering cost induced by the adaptation knowledge (CA) acquisition task: CA is learned from the case base by the means of knowledge discovery techniques, and the CA acquisition sessions are opportunistically triggered, i.e., at problem-solving time.\n    ",
        "submission_date": "2009-12-01T00:00:00",
        "last_modified_date": "2009-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.0224",
        "title": "A Multi-stage Probabilistic Algorithm for Dynamic Path-Planning",
        "authors": [
            "Nicolas A. Barriga",
            "Mauricio Araya-L\u00f3pez"
        ],
        "abstract": "  Probabilistic sampling methods have become very popular to solve single-shot path planning problems. Rapidly-exploring Random Trees (RRTs) in particular have been shown to be efficient in solving high dimensional problems. Even though several RRT variants have been proposed for dynamic replanning, these methods only perform well in environments with infrequent changes. This paper addresses the dynamic path planning problem by combining simple techniques in a multi-stage probabilistic algorithm. This algorithm uses RRTs for initial planning and informed local search for navigation. We show that this combination of simple techniques provides better responses to highly dynamic environments than the RRT extensions.\n    ",
        "submission_date": "2009-12-01T00:00:00",
        "last_modified_date": "2009-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.0266",
        "title": "Combining a Probabilistic Sampling Technique and Simple Heuristics to solve the Dynamic Path Planning Problem",
        "authors": [
            "Nicolas A. Barriga",
            "Mauricio Araya-L\u00f3pez",
            "Mauricio Solar"
        ],
        "abstract": "  Probabilistic sampling methods have become very popular to solve single-shot path planning problems. Rapidly-exploring Random Trees (RRTs) in particular have been shown to be very efficient in solving high dimensional problems. Even though several RRT variants have been proposed to tackle the dynamic replanning problem, these methods only perform well in environments with infrequent changes. This paper addresses the dynamic path planning problem by combining simple techniques in a multi-stage probabilistic algorithm. This algorithm uses RRTs as an initial solution, informed local search to fix unfeasible paths and a simple greedy optimizer. The algorithm is capable of recognizing when the local search is stuck, and subsequently restart the RRT. We show that this combination of simple techniques provides better responses to a highly dynamic environment than the dynamic RRT variants.\n    ",
        "submission_date": "2009-12-01T00:00:00",
        "last_modified_date": "2009-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.0270",
        "title": "Single-Agent On-line Path Planning in Continuous, Unpredictable and Highly Dynamic Environments",
        "authors": [
            "Nicolas A. Barriga"
        ],
        "abstract": "  This document is a thesis on the subject of single-agent on-line path planning in continuous,unpredictable and highly dynamic environments. The problem is finding and traversing a collision-free path for a holonomic robot, without kinodynamic restrictions, moving in an environment with several unpredictably moving obstacles or adversaries. The availability of perfect information of the environment at all times is assumed.\n",
        "submission_date": "2009-12-01T00:00:00",
        "last_modified_date": "2009-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.2563",
        "title": "A Model-Based Approach to Predicting Predator-Prey & Friend-Foe Relationships in Ant Colonies",
        "authors": [
            "Karthik Narayanaswami"
        ],
        "abstract": "  Understanding predator-prey relationships among insects is a challenging task in the domain of insect-colony research. This is due to several factors involved, such as determining whether a particular behavior is the result of a predator-prey interaction, a friend-foe interaction or another kind of interaction. In this paper, we analyze a series of predator-prey and friend-foe interactions in two colonies of carpenter ants to better understand and predict such behavior. Using the data gathered, we have also come up with a preliminary model for predicting such behavior under the specific conditions the experiment was conducted in. In this paper, we present the results of our data analysis as well as an overview of the processes involved.\n    ",
        "submission_date": "2009-12-14T00:00:00",
        "last_modified_date": "2009-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.2846",
        "title": "Multi-valued Action Languages in CLP(FD)",
        "authors": [
            "Agostino Dovier",
            "Andrea Formisano",
            "Enrico Pontelli"
        ],
        "abstract": "  Action description languages, such as A and B, are expressive instruments introduced for formalizing planning domains and planning problem instances. The paper starts by proposing a methodology to encode an action language (with conditional effects and static causal laws), a slight variation of B, using Constraint Logic Programming over Finite Domains. The approach is then generalized to raise the use of constraints to the level of the action language itself. A prototype implementation has been developed, and the preliminary results are presented and discussed.\n",
        "submission_date": "2009-12-15T00:00:00",
        "last_modified_date": "2009-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.3228",
        "title": "On Backtracking in Real-time Heuristic Search",
        "authors": [
            "Valeriy K. Bulitko",
            "Vadim Bulitko"
        ],
        "abstract": "  Real-time heuristic search algorithms are suitable for situated agents that need to make their decisions in constant time. Since the original work by Korf nearly two decades ago, numerous extensions have been suggested. One of the most intriguing extensions is the idea of backtracking wherein the agent decides to return to a previously visited state as opposed to moving forward greedily. This idea has been empirically shown to have a significant impact on various performance measures. The studies have been carried out in particular empirical testbeds with specific real-time search algorithms that use backtracking. Consequently, the extent to which the trends observed are characteristic of backtracking in general is unclear. In this paper, we present the first entirely theoretical study of backtracking in real-time heuristic search. In particular, we present upper bounds on the solution cost exponential and linear in a parameter regulating the amount of backtracking. The results hold for a wide class of real-time heuristic search algorithms that includes many existing algorithms as a small subclass.\n    ",
        "submission_date": "2009-12-16T00:00:00",
        "last_modified_date": "2009-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.3309",
        "title": "New Generalization Bounds for Learning Kernels",
        "authors": [
            "Corinna Cortes",
            "Mehryar Mohri",
            "Afshin Rostamizadeh"
        ],
        "abstract": "  This paper presents several novel generalization bounds for the problem of learning kernels based on the analysis of the Rademacher complexity of the corresponding hypothesis sets. Our bound for learning kernels with a convex combination of p base kernels has only a log(p) dependency on the number of kernels, p, which is considerably more favorable than the previous best bound given for the same problem. We also give a novel bound for learning with a linear combination of p base kernels with an L_2 regularization whose dependency on p is only in p^{1/4}.\n    ",
        "submission_date": "2009-12-17T00:00:00",
        "last_modified_date": "2009-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.4584",
        "title": "A Necessary and Sufficient Condition for Graph Matching Being Equivalent to the Maximum Weight Clique Problem",
        "authors": [
            "Brijnesh Jain",
            "Klaus Obermayer"
        ],
        "abstract": "  This paper formulates a necessary and sufficient condition for a generic graph matching problem to be equivalent to the maximum vertex and edge weight clique problem in a derived association graph. The consequences of this results are threefold: first, the condition is general enough to cover a broad range of practical graph matching problems; second, a proof to establish equivalence between graph matching and clique search reduces to showing that a given graph matching problem satisfies the proposed condition; and third, the result sets the scene for generic continuous solutions for a broad range of graph matching problems. To illustrate the mathematical framework, we apply it to a number of graph matching problems, including the problem of determining the graph edit distance.\n    ",
        "submission_date": "2009-12-23T00:00:00",
        "last_modified_date": "2009-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.4598",
        "title": "Elkan's k-Means for Graphs",
        "authors": [
            "Brijnesh J. Jain",
            "Klaus Obermayer"
        ],
        "abstract": "  This paper extends k-means algorithms from the Euclidean domain to the domain of graphs. To recompute the centroids, we apply subgradient methods for solving the optimization-based formulation of the sample mean of graphs. To accelerate the k-means algorithm for graphs without trading computational time against solution quality, we avoid unnecessary graph distance calculations by exploiting the triangle inequality of the underlying distance metric following Elkan's k-means algorithm proposed in \\cite{Elkan03}. In experiments we show that the accelerated k-means algorithm are faster than the standard k-means algorithm for graphs provided there is a cluster structure in the data.\n    ",
        "submission_date": "2009-12-23T00:00:00",
        "last_modified_date": "2009-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.4879",
        "title": "Similarit\u00e9 en intension vs en extension : \u00e0 la crois\u00e9e de l'informatique et du th\u00e9\u00e2tre",
        "authors": [
            "Alain Bonardi",
            "Francis Rousseaux"
        ],
        "abstract": "  Traditional staging is based on a formal approach of similarity leaning on dramaturgical ontologies and instanciation variations. Inspired by interactive data mining, that suggests different approaches, we give an overview of computer science and theater researches using computers as partners of the actor to escape the a priori specification of roles.\n    ",
        "submission_date": "2009-12-24T00:00:00",
        "last_modified_date": "2009-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.5073",
        "title": "A Rational Decision Maker with Ordinal Utility under Uncertainty: Optimism and Pessimism",
        "authors": [
            "Ji Han"
        ],
        "abstract": "  In game theory and artificial intelligence, decision making models often involve maximizing expected utility, which does not respect ordinal invariance. In this paper, the author discusses the possibility of preserving ordinal invariance and still making a rational decision under uncertainty.\n    ",
        "submission_date": "2009-12-27T00:00:00",
        "last_modified_date": "2010-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.5511",
        "title": "A general approach to belief change in answer set programming",
        "authors": [
            "James Delgrande",
            "Torsten Schaub",
            "Hans Tompits",
            "Stefan Woltran"
        ],
        "abstract": "  We address the problem of belief change in (nonmonotonic) logic programming under answer set semantics. Unlike previous approaches to belief change in logic programming, our formal techniques are analogous to those of distance-based belief revision in propositional logic. In developing our results, we build upon the model theory of logic programs furnished by SE models. Since SE models provide a formal, monotonic characterisation of logic programs, we can adapt techniques from the area of belief revision to belief change in logic programs. We introduce methods for revising and merging logic programs, respectively. For the former, we study both subset-based revision as well as cardinality-based revision, and we show that they satisfy the majority of the AGM postulates for revision. For merging, we consider operators following arbitration merging and IC merging, respectively. We also present encodings for computing the revision as well as the merging of logic programs within the same logic programming framework, giving rise to a direct implementation of our approach in terms of off-the-shelf answer set solvers. These encodings reflect in turn the fact that our change operators do not increase the complexity of the base formalism.\n    ",
        "submission_date": "2009-12-30T00:00:00",
        "last_modified_date": "2009-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.5533",
        "title": "Oriented Straight Line Segment Algebra: Qualitative Spatial Reasoning about Oriented Objects",
        "authors": [
            "Reinhard Moratz",
            "Dominik L\u00fccke",
            "Till Mossakowski"
        ],
        "abstract": "  Nearly 15 years ago, a set of qualitative spatial relations between oriented straight line segments (dipoles) was suggested by Schlieder. This work received substantial interest amongst the qualitative spatial reasoning community. However, it turned out to be difficult to establish a sound constraint calculus based on these relations. In this paper, we present the results of a new investigation into dipole constraint calculi which uses algebraic methods to derive sound results on the composition of relations and other properties of dipole calculi. Our results are based on a condensed semantics of the dipole relations.\n",
        "submission_date": "2009-12-30T00:00:00",
        "last_modified_date": "2009-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0901.0317",
        "title": "Design of a P System based Artificial Graph Chemistry",
        "authors": [
            "Janardan Misra"
        ],
        "abstract": "  Artificial Chemistries (ACs) are symbolic chemical metaphors for the exploration of Artificial Life, with specific focus on the origin of life. In this work we define a P system based artificial graph chemistry to understand the principles leading to the evolution of life-like structures in an AC set up and to develop a unified framework to characterize and classify symbolic artificial chemistries by devising appropriate formalism to capture semantic and organizational information. An extension of P system is considered by associating probabilities with the rules providing the topological framework for the evolution of a labeled undirected graph based molecular reaction semantics.\n    ",
        "submission_date": "2009-01-03T00:00:00",
        "last_modified_date": "2009-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0901.0597",
        "title": "On the Optimal Convergence Probability of Univariate Estimation of Distribution Algorithms",
        "authors": [
            "Reza Rastegar"
        ],
        "abstract": "  In this paper, we obtain bounds on the probability of convergence to the optimal solution for the compact Genetic Algorithm (cGA) and the Population Based Incremental Learning (PBIL). We also give a sufficient condition for convergence of these algorithms to the optimal solution and compute a range of possible values of the parameters of these algorithms for which they converge to the optimal solution with a confidence level.\n    ",
        "submission_date": "2009-01-06T00:00:00",
        "last_modified_date": "2010-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0901.0598",
        "title": "A Step Forward in Studying the Compact Genetic Algorithm",
        "authors": [
            "Reza Rastegar",
            "Arash Hariri"
        ],
        "abstract": "  The compact Genetic Algorithm (cGA) is an Estimation of Distribution Algorithm that generates offspring population according to the estimated probabilistic model of the parent population instead of using traditional recombination and mutation operators. The cGA only needs a small amount of memory; therefore, it may be quite useful in memory-constrained applications. This paper introduces a theoretical framework for studying the cGA from the convergence point of view in which, we model the cGA by a Markov process and approximate its behavior using an Ordinary Differential Equation (ODE). Then, we prove that the corresponding ODE converges to local optima and stays there. Consequently, we conclude that the cGA will converge to the local optima of the function to be optimized.\n    ",
        "submission_date": "2009-01-06T00:00:00",
        "last_modified_date": "2009-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0901.0733",
        "title": "Contextual hypotheses and semantics of logic programs",
        "authors": [
            "\u00c9ric A. Martin"
        ],
        "abstract": "Logic programming has developed as a rich field, built over a logical substratum whose main constituent is a nonclassical form of negation, sometimes coexisting with classical negation. The field has seen the advent of a number of alternative semantics, with Kripke-Kleene semantics, the well-founded semantics, the stable model semantics, and the answer-set semantics standing out as the most successful. We show that all aforementioned semantics are particular cases of a generic semantics, in a framework where classical negation is the unique form of negation and where the literals in the bodies of the rules can be `marked' to indicate that they can be the targets of hypotheses. A particular semantics then amounts to choosing a particular marking scheme and choosing a particular set of hypotheses. When a literal belongs to the chosen set of hypotheses, all marked occurrences of that literal in the body of a rule are assumed to be true, whereas the occurrences of that literal that have not been marked in the body of the rule are to be derived in order to contribute to the firing of the rule. Hence the notion of hypothetical reasoning that is presented in this framework is not based on making global assumptions, but more subtly on making local, contextual assumptions, taking effect as indicated by the chosen marking scheme on the basis of the chosen set of hypotheses. Our approach offers a unified view on the various semantics proposed in logic programming, classical in that only classical negation is used, and links the semantics of logic programs to mechanisms that endow rule-based systems with the power to harness hypothetical reasoning.\n    ",
        "submission_date": "2009-01-06T00:00:00",
        "last_modified_date": "2011-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0901.1230",
        "title": "Logical Algorithms meets CHR: A meta-complexity result for Constraint Handling Rules with rule priorities",
        "authors": [
            "Leslie De Koninck"
        ],
        "abstract": "  This paper investigates the relationship between the Logical Algorithms language (LA) of Ganzinger and McAllester and Constraint Handling Rules (CHR). We present a translation schema from LA to CHR-rp: CHR with rule priorities, and show that the meta-complexity theorem for LA can be applied to a subset of CHR-rp via inverse translation. Inspired by the high-level implementation proposal for Logical Algorithm by Ganzinger and McAllester and based on a new scheduling algorithm, we propose an alternative implementation for CHR-rp that gives strong complexity guarantees and results in a new and accurate meta-complexity theorem for CHR-rp. It is furthermore shown that the translation from Logical Algorithms to CHR-rp combined with the new CHR-rp implementation, satisfies the required complexity for the Logical Algorithms meta-complexity result to hold.\n    ",
        "submission_date": "2009-01-09T00:00:00",
        "last_modified_date": "2009-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0901.2130",
        "title": "Hiding Quiet Solutions in Random Constraint Satisfaction Problems",
        "authors": [
            "Florent Krzakala",
            "Lenka Zdeborov\u00e1"
        ],
        "abstract": "  We study constraint satisfaction problems on the so-called 'planted' random ensemble. We show that for a certain class of problems, e.g. graph coloring, many of the properties of the usual random ensemble are quantitatively identical in the planted random ensemble. We study the structural phase transitions, and the easy/hard/easy pattern in the average computational complexity. We also discuss the finite temperature phase diagram, finding a close connection with the liquid/glass/solid phenomenology.\n    ",
        "submission_date": "2009-01-14T00:00:00",
        "last_modified_date": "2009-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0901.3574",
        "title": "Automating Access Control Logics in Simple Type Theory with LEO-II",
        "authors": [
            "Christoph Benzmueller"
        ],
        "abstract": "  Garg and Abadi recently proved that prominent access control logics can be translated in a sound and complete way into modal logic S4. We have previously outlined how normal multimodal logics, including monomodal logics K and S4, can be embedded in simple type theory (which is also known as higher-order logic) and we have demonstrated that the higher-order theorem prover LEO-II can automate reasoning in and about them. In this paper we combine these results and describe a sound and complete embedding of different access control logics in simple type theory. Employing this framework we show that the off the shelf theorem prover LEO-II can be applied to automate reasoning in prominent access control logics.\n    ",
        "submission_date": "2009-01-23T00:00:00",
        "last_modified_date": "2009-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0901.3585",
        "title": "Resource Adaptive Agents in Interactive Theorem Proving",
        "authors": [
            "Christoph Benzmueller",
            "Volker Sorge"
        ],
        "abstract": "  We introduce a resource adaptive agent mechanism which supports the user in interactive theorem proving. The mechanism uses a two layered architecture of agent societies to suggest appropriate commands together with possible command argument instantiations. Experiments with this approach show that its effectiveness can be further improved by introducing a resource concept. In this paper we provide an abstract view on the overall mechanism, motivate the necessity of an appropriate resource concept and discuss its realization within the agent architecture.\n    ",
        "submission_date": "2009-01-23T00:00:00",
        "last_modified_date": "2009-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.0043",
        "title": "Cut-Simulation and Impredicativity",
        "authors": [
            "Christoph Benzmueller",
            "Chad E. Brown",
            "Michael Kohlhase"
        ],
        "abstract": "  We investigate cut-elimination and cut-simulation in impredicative (higher-order) logics. We illustrate that adding simple axioms such as Leibniz equations to a calculus for an impredicative logic -- in our case a sequent calculus for classical type theory -- is like adding cut. The phenomenon equally applies to prominent axioms like Boolean- and functional extensionality, induction, choice, and description. This calls for the development of calculi where these principles are built-in instead of being treated axiomatically.\n    ",
        "submission_date": "2009-01-31T00:00:00",
        "last_modified_date": "2009-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.0514",
        "title": "Graphical Reasoning in Compact Closed Categories for Quantum Computation",
        "authors": [
            "Lucas Dixon",
            "Ross Duncan"
        ],
        "abstract": "  Compact closed categories provide a foundational formalism for a variety of important domains, including quantum computation. These categories have a natural visualisation as a form of graphs. We present a formalism for equational reasoning about such graphs and develop this into a generic proof system with a fixed logical kernel for equational reasoning about compact closed categories. Automating this reasoning process is motivated by the slow and error prone nature of manual graph manipulation. A salient feature of our system is that it provides a formal and declarative account of derived results that can include `ellipses'-style notation. We illustrate the framework by instantiating it for a graphical language of quantum computation and show how this can be used to perform symbolic computation.\n    ",
        "submission_date": "2009-02-03T00:00:00",
        "last_modified_date": "2009-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.1629",
        "title": "Improvements of real coded genetic algorithms based on differential operators preventing premature convergence",
        "authors": [
            "O. Hrstka",
            "A. Kucerova"
        ],
        "abstract": "  This paper presents several types of evolutionary algorithms (EAs) used for global optimization on real domains. The interest has been focused on multimodal problems, where the difficulties of a premature convergence usually occurs. First the standard genetic algorithm (SGA) using binary encoding of real values and its unsatisfactory behavior with multimodal problems is briefly reviewed together with some improvements of fighting premature convergence. Two types of real encoded methods based on differential operators are examined in detail: the differential evolution (DE), a very modern and effective method firstly published by R. Storn and K. Price, and the simplified real-coded differential genetic algorithm SADE proposed by the authors. In addition, an improvement of the SADE method, called CERAF technology, enabling the population of solutions to escape from local extremes, is examined. All methods are tested on an identical set of objective functions and a systematic comparison based on a reliable methodology is presented. It is confirmed that real coded methods generally exhibit better behavior on real domains than the binary algorithms, even when extended by several improvements. Furthermore, the positive influence of the differential operators due to their possibility of self-adaptation is demonstrated. From the reliability point of view, it seems that the real encoded differential algorithm, improved by the technology described in this paper, is a universal and reliable method capable of solving all proposed test problems.\n    ",
        "submission_date": "2009-02-10T00:00:00",
        "last_modified_date": "2009-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.1647",
        "title": "A competitive comparison of different types of evolutionary algorithms",
        "authors": [
            "O. Hrstka",
            "A. Kucerova",
            "M. Leps",
            "J. Zeman"
        ],
        "abstract": "  This paper presents comparison of several stochastic optimization algorithms developed by authors in their previous works for the solution of some problems arising in Civil Engineering. The introduced optimization methods are: the integer augmented simulated annealing (IASA), the real-coded augmented simulated annealing (RASA), the differential evolution (DE) in its original fashion developed by R. Storn and K. Price and simplified real-coded differential genetic algorithm (SADE). Each of these methods was developed for some specific optimization problem; namely the Chebychev trial polynomial problem, the so called type 0 function and two engineering problems - the reinforced concrete beam layout and the periodic unit cell problem respectively. Detailed and extensive numerical tests were performed to examine the stability and efficiency of proposed algorithms. The results of our experiments suggest that the performance and robustness of RASA, IASA and SADE methods are comparable, while the DE algorithm performs slightly worse. This fact together with a small number of internal parameters promotes the SADE method as the most robust for practical use.\n    ",
        "submission_date": "2009-02-10T00:00:00",
        "last_modified_date": "2009-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.1690",
        "title": "Back analysis of microplane model parameters using soft computing methods",
        "authors": [
            "A. Kucerova",
            "M. Leps",
            "J. Zeman"
        ],
        "abstract": "  A new procedure based on layered feed-forward neural networks for the microplane material model parameters identification is proposed in the present paper. Novelties are usage of the Latin Hypercube Sampling method for the generation of training sets, a systematic employment of stochastic sensitivity analysis and a genetic algorithm-based training of a neural network by an evolutionary algorithm. Advantages and disadvantages of this approach together with possible extensions are thoroughly discussed and analyzed.\n    ",
        "submission_date": "2009-02-10T00:00:00",
        "last_modified_date": "2009-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.1911",
        "title": "Topological Centrality and Its Applications",
        "authors": [
            "Hai Zhuge",
            "Junsheng Zhang"
        ],
        "abstract": "  Recent development of network structure analysis shows that it plays an important role in characterizing complex system of many branches of sciences. Different from previous network centrality measures, this paper proposes the notion of topological centrality (TC) reflecting the topological positions of nodes and edges in general networks, and proposes an approach to calculating the topological centrality. The proposed topological centrality is then used to discover communities and build the backbone network. Experiments and applications on research network show the significance of the proposed approach.\n    ",
        "submission_date": "2009-02-11T00:00:00",
        "last_modified_date": "2009-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.2969",
        "title": "Ptarithmetic",
        "authors": [
            "Giorgi Japaridze"
        ],
        "abstract": "  The present article introduces ptarithmetic (short for \"polynomial time arithmetic\") -- a formal number theory similar to the well known Peano arithmetic, but based on the recently born computability logic (see ",
        "submission_date": "2009-02-17T00:00:00",
        "last_modified_date": "2010-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.3196",
        "title": "Symbolic Computing with Incremental Mindmaps to Manage and Mine Data Streams - Some Applications",
        "authors": [
            "Claudine Brucks",
            "Michael Hilker",
            "Christoph Schommer",
            "Cynthia Wagner",
            "Ralph Weires"
        ],
        "abstract": "  In our understanding, a mind-map is an adaptive engine that basically works incrementally on the fundament of existing transactional streams. Generally, mind-maps consist of symbolic cells that are connected with each other and that become either stronger or weaker depending on the transactional stream. Based on the underlying biologic principle, these symbolic cells and their connections as well may adaptively survive or die, forming different cell agglomerates of arbitrary size. In this work, we intend to prove mind-maps' eligibility following diverse application scenarios, for example being an underlying management system to represent normal and abnormal traffic behaviour in computer networks, supporting the detection of the user behaviour within search engines, or being a hidden communication layer for natural language interaction.\n    ",
        "submission_date": "2009-02-18T00:00:00",
        "last_modified_date": "2009-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.3430",
        "title": "Domain Adaptation: Learning Bounds and Algorithms",
        "authors": [
            "Yishay Mansour",
            "Mehryar Mohri",
            "Afshin Rostamizadeh"
        ],
        "abstract": "This paper addresses the general problem of domain adaptation which arises in a variety of applications where the distribution of the labeled sample available somewhat differs from that of the test data. Building on previous work by Ben-David et al. (2007), we introduce a novel distance between distributions, discrepancy distance, that is tailored to adaptation problems with arbitrary loss functions. We give Rademacher complexity bounds for estimating the discrepancy distance from finite samples for different loss functions. Using this distance, we derive novel generalization bounds for domain adaptation for a wide family of loss functions. We also present a series of novel adaptation bounds for large classes of regularization-based algorithms, including support vector machines and kernel ridge regression based on the empirical discrepancy. This motivates our analysis of the problem of minimizing the empirical discrepancy for various loss functions for which we also give novel algorithms. We report the results of preliminary experiments that demonstrate the benefits of our discrepancy minimization algorithms for domain adaptation.\n    ",
        "submission_date": "2009-02-19T00:00:00",
        "last_modified_date": "2023-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.4521",
        "title": "Are Tensor Decomposition Solutions Unique? On the global convergence of HOSVD and ParaFac algorithms",
        "authors": [
            "Dijun Luo",
            "Heng Huang",
            "Chris Ding"
        ],
        "abstract": "  For tensor decompositions such as HOSVD and ParaFac, the objective functions are nonconvex. This implies, theoretically, there exists a large number of local optimas: starting from different starting point, the iteratively improved solution will converge to different local solutions. This non-uniqueness present a stability and reliability problem for image compression and retrieval. In this paper, we present the results of a comprehensive investigation of this problem. We found that although all tensor decomposition algorithms fail to reach a unique global solution on random data and severely scrambled data; surprisingly however, on all real life several data sets (even with substantial scramble and occlusions), HOSVD always produce the unique global solution in the parameter region suitable to practical applications, while ParaFac produce non-unique solutions. We provide an eigenvalue based rule for the assessing the solution uniqueness.\n    ",
        "submission_date": "2009-02-26T00:00:00",
        "last_modified_date": "2009-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0902.4682",
        "title": "Lectures on Jacques Herbrand as a Logician",
        "authors": [
            "Claus-Peter Wirth",
            "Joerg Siekmann",
            "Christoph Benzmueller",
            "Serge Autexier"
        ],
        "abstract": "We give some lectures on the work on formal logic of Jacques Herbrand, and sketch his life and his influence on automated theorem proving. The intended audience ranges from students interested in logic over historians to logicians. Besides the well-known correction of Herbrand's False Lemma by Goedel and Dreben, we also present the hardly known unpublished correction of Heijenoort and its consequences on Herbrand's Modus Ponens Elimination. Besides Herbrand's Fundamental Theorem and its relation to the Loewenheim-Skolem-Theorem, we carefully investigate Herbrand's notion of intuitionism in connection with his notion of falsehood in an infinite domain. We sketch Herbrand's two proofs of the consistency of arithmetic and his notion of a recursive function, and last but not least, present the correct original text of his unification algorithm with a new translation.\n    ",
        "submission_date": "2009-02-26T00:00:00",
        "last_modified_date": "2014-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0194",
        "title": "A Graph Analysis of the Linked Data Cloud",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "abstract": "  The Linked Data community is focused on integrating Resource Description Framework (RDF) data sets into a single unified representation known as the Web of Data. The Web of Data can be traversed by both man and machine and shows promise as the \\textit{de facto} standard for integrating data world wide much like the World Wide Web is the \\textit{de facto} standard for integrating documents. On February 27$^\\text{th}$ of 2009, an updated Linked Data cloud visualization was made publicly available. This visualization represents the various RDF data sets currently in the Linked Data cloud and their interlinking relationships. For the purposes of this article, this visual representation was manually transformed into a directed graph and analyzed.\n    ",
        "submission_date": "2009-03-02T00:00:00",
        "last_modified_date": "2009-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.0200",
        "title": "Faith in the Algorithm, Part 1: Beyond the Turing Test",
        "authors": [
            "Marko A. Rodriguez",
            "Alberto Pepe"
        ],
        "abstract": "  Since the Turing test was first proposed by Alan Turing in 1950, the primary goal of artificial intelligence has been predicated on the ability for computers to imitate human behavior. However, the majority of uses for the computer can be said to fall outside the domain of human abilities and it is exactly outside of this domain where computers have demonstrated their greatest contribution to intelligence. Another goal for artificial intelligence is one that is not predicated on human mimicry, but instead, on human amplification. This article surveys various systems that contribute to the advancement of human and social intelligence.\n    ",
        "submission_date": "2009-03-02T00:00:00",
        "last_modified_date": "2009-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.1095",
        "title": "Decomposition, Reformulation, and Diving in University Course Timetabling",
        "authors": [
            "Edmund K. Burke",
            "Jakub Marecek",
            "Andrew J. Parkes",
            "Hana Rudova"
        ],
        "abstract": "  In many real-life optimisation problems, there are multiple interacting components in a solution. For example, different components might specify assignments to different kinds of resource. Often, each component is associated with different sets of soft constraints, and so with different measures of soft constraint violation. The goal is then to minimise a linear combination of such measures. This paper studies an approach to such problems, which can be thought of as multiphase exploitation of multiple objective-/value-restricted submodels. In this approach, only one computationally difficult component of a problem and the associated subset of objectives is considered at first. This produces partial solutions, which define interesting neighbourhoods in the search space of the complete problem. Often, it is possible to pick the initial component so that variable aggregation can be performed at the first stage, and the neighbourhoods to be explored next are guaranteed to contain feasible solutions. Using integer programming, it is then easy to implement heuristics producing solutions with bounds on their quality.\n",
        "submission_date": "2009-03-05T00:00:00",
        "last_modified_date": "2009-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.1147",
        "title": "Tetravex is NP-complete",
        "authors": [
            "Yasuhiko Takenaga",
            "Toby Walsh"
        ],
        "abstract": "  Tetravex is a widely played one person computer game in which you are given $n^2$ unit tiles, each edge of which is labelled with a number. The objective is to place each tile within a $n$ by $n$ square such that all neighbouring edges are labelled with an identical number. Unfortunately, playing Tetravex is computationally hard. More precisely, we prove that deciding if there is a tiling of the Tetravex board is NP-complete. Deciding where to place the tiles is therefore NP-hard. This may help to explain why Tetravex is a good puzzle. This result compliments a number of similar results for one person games involving tiling. For example, NP-completeness results have been shown for: the offline version of Tetris, KPlumber (which involves rotating tiles containing drawings of pipes to make a connected network), and shortest sliding puzzle problems. It raises a number of open questions. For example, is the infinite version Turing-complete? How do we generate Tetravex problems which are truly puzzling as random NP-complete problems are often surprising easy to solve? Can we observe phase transition behaviour? What about the complexity of the problem when it is guaranteed to have an unique solution? How do we generate puzzles with unique solutions?\n    ",
        "submission_date": "2009-03-06T00:00:00",
        "last_modified_date": "2009-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.2851",
        "title": "A parameter-free hedging algorithm",
        "authors": [
            "Kamalika Chaudhuri",
            "Yoav Freund",
            "Daniel Hsu"
        ],
        "abstract": "  We study the problem of decision-theoretic online learning (DTOL). Motivated by practical applications, we focus on DTOL when the number of actions is very large. Previous algorithms for learning in this framework have a tunable learning rate parameter, and a barrier to using online-learning in practical applications is that it is not understood how to set this parameter optimally, particularly when the number of actions is large.\n",
        "submission_date": "2009-03-16T00:00:00",
        "last_modified_date": "2010-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.2862",
        "title": "Tracking using explanation-based modeling",
        "authors": [
            "Kamalika Chaudhuri",
            "Yoav Freund",
            "Daniel Hsu"
        ],
        "abstract": "  We study the tracking problem, namely, estimating the hidden state of an object over time, from unreliable and noisy measurements. The standard framework for the tracking problem is the generative framework, which is the basis of solutions such as the Bayesian algorithm and its approximation, the particle filters. However, the problem with these solutions is that they are very sensitive to model mismatches. In this paper, motivated by online learning, we introduce a new framework -- an {\\em explanatory} framework -- for tracking. We provide an efficient tracking algorithm for this framework. We provide experimental results comparing our algorithm to the Bayesian algorithm on simulated data. Our experiments show that when there are slight model mismatches, our algorithm vastly outperforms the Bayesian algorithm.\n    ",
        "submission_date": "2009-03-16T00:00:00",
        "last_modified_date": "2010-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.2972",
        "title": "Optimistic Simulated Exploration as an Incentive for Real Exploration",
        "authors": [
            "Ivo Danihelka"
        ],
        "abstract": "  Many reinforcement learning exploration techniques are overly optimistic and try to explore every state. Such exploration is impossible in environments with the unlimited number of states. I propose to use simulated exploration with an optimistic model to discover promising paths for real exploration. This reduces the needs for the real exploration.\n    ",
        "submission_date": "2009-03-17T00:00:00",
        "last_modified_date": "2009-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.3103",
        "title": "Efficiently Learning a Detection Cascade with Sparse Eigenvectors",
        "authors": [
            "Chunhua Shen",
            "Sakrapee Paisitkriangkrai",
            "Jian Zhang"
        ],
        "abstract": "  In this work, we first show that feature selection methods other than boosting can also be used for training an efficient object detector. In particular, we introduce Greedy Sparse Linear Discriminant Analysis (GSLDA) \\cite{Moghaddam2007Fast} for its conceptual simplicity and computational efficiency; and slightly better detection performance is achieved compared with \\cite{Viola2004Robust}. Moreover, we propose a new technique, termed Boosted Greedy Sparse Linear Discriminant Analysis (BGSLDA), to efficiently train a detection cascade. BGSLDA exploits the sample re-weighting property of boosting and the class-separability criterion of GSLDA.\n    ",
        "submission_date": "2009-03-18T00:00:00",
        "last_modified_date": "2009-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.4217",
        "title": "Conditional Probability Tree Estimation Analysis and Algorithms",
        "authors": [
            "Alina Beygelzimer",
            "John Langford",
            "Yuri Lifshits",
            "Gregory Sorkin",
            "Alex Strehl"
        ],
        "abstract": "  We consider the problem of estimating the conditional probability of a label in time $O(\\log n)$, where $n$ is the number of possible labels. We analyze a natural reduction of this problem to a set of binary regression problems organized in a tree structure, proving a regret bound that scales with the depth of the tree. Motivated by this analysis, we propose the first online algorithm which provably constructs a logarithmic depth tree on the set of labels to solve this problem. We test the algorithm empirically, showing that it works succesfully on a dataset with roughly $10^6$ labels.\n    ",
        "submission_date": "2009-03-25T00:00:00",
        "last_modified_date": "2009-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.4513",
        "title": "Building the information kernel and the problem of recognition",
        "authors": [
            "Elena S. Vishnevskaya"
        ],
        "abstract": "  At this point in time there is a need for a new representation of different information, to identify and organize descending its characteristics. Today, science is a powerful tool for the description of reality - the numbers. Why the most important property of numbers. Suppose we have a number 0.2351734, it is clear that the figures are there in order of importance. If necessary, we can round the number up to some value, eg 0.235. Arguably, the 0,235 - the most important information of 0.2351734. Thus, we can reduce the size of numbers is not losing much with the accuracy. Clearly, if learning to provide a graphical or audio information kernel, we can provide the most relevant information, discarding the rest. Introduction of various kinds of information in an information kernel, is an important task, to solve many problems in artificial intelligence and information theory.\n    ",
        "submission_date": "2009-03-26T00:00:00",
        "last_modified_date": "2011-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.4856",
        "title": "A Combinatorial Algorithm to Compute Regularization Paths",
        "authors": [
            "Bernd G\u00e4rtner",
            "Joachim Giesen",
            "Martin Jaggi",
            "Torsten Welsch"
        ],
        "abstract": "  For a wide variety of regularization methods, algorithms computing the entire solution path have been developed recently. Solution path algorithms do not only compute the solution for one particular value of the regularization parameter but the entire path of solutions, making the selection of an optimal parameter much easier. Most of the currently used algorithms are not robust in the sense that they cannot deal with general or degenerate input. Here we present a new robust, generic method for parametric quadratic programming. Our algorithm directly applies to nearly all machine learning applications, where so far every application required its own different algorithm.\n",
        "submission_date": "2009-03-27T00:00:00",
        "last_modified_date": "2009-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0903.5188",
        "title": "Quantum decision theory as quantum theory of measurement",
        "authors": [
            "V.I. Yukalov",
            "D. Sornette"
        ],
        "abstract": "  We present a general theory of quantum information processing devices, that can be applied to human decision makers, to atomic multimode registers, or to molecular high-spin registers. Our quantum decision theory is a generalization of the quantum theory of measurement, endowed with an action ring, a prospect lattice and a probability operator measure. The algebra of probability operators plays the role of the algebra of local observables. Because of the composite nature of prospects and of the entangling properties of the probability operators, quantum interference terms appear, which make actions noncommutative and the prospect probabilities non-additive. The theory provides the basis for explaining a variety of paradoxes typical of the application of classical utility theory to real human decision making. The principal advantage of our approach is that it is formulated as a self-consistent mathematical theory, which allows us to explain not just one effect but actually all known paradoxes in human decision making. Being general, the approach can serve as a tool for characterizing quantum information processing by means of atomic, molecular, and condensed-matter systems.\n    ",
        "submission_date": "2009-03-30T00:00:00",
        "last_modified_date": "2009-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.0019",
        "title": "On Solving Boolean Multilevel Optimization Problems",
        "authors": [
            "Josep Argelich",
            "Ines Lynce",
            "Joao Marques-Silva"
        ],
        "abstract": "  Many combinatorial optimization problems entail a number of hierarchically dependent optimization problems. An often used solution is to associate a suitably large cost with each individual optimization problem, such that the solution of the resulting aggregated optimization problem solves the original set of hierarchically dependent optimization problems. This paper starts by studying the package upgradeability problem in software distributions. Straightforward solutions based on Maximum Satisfiability (MaxSAT) and pseudo-Boolean (PB) optimization are shown to be ineffective, and unlikely to scale for large problem instances. Afterwards, the package upgradeability problem is related to multilevel optimization. The paper then develops new algorithms for Boolean Multilevel Optimization (BMO) and highlights a large number of potential applications. The experimental results indicate that the proposed algorithms for BMO allow solving optimization problems that existing MaxSAT and PB solvers would otherwise be unable to solve.\n    ",
        "submission_date": "2009-03-31T00:00:00",
        "last_modified_date": "2009-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.0027",
        "title": "Faith in the Algorithm, Part 2: Computational Eudaemonics",
        "authors": [
            "Marko A. Rodriguez",
            "Jennifer H. Watkins"
        ],
        "abstract": "  Eudaemonics is the study of the nature, causes, and conditions of human well-being. According to the ethical theory of eudaemonia, reaping satisfaction and fulfillment from life is not only a desirable end, but a moral responsibility. However, in modern society, many individuals struggle to meet this responsibility. Computational mechanisms could better enable individuals to achieve eudaemonia by yielding practical real-world systems that embody algorithms that promote human flourishing. This article presents eudaemonic systems as the evolutionary goal of the present day recommender system.\n    ",
        "submission_date": "2009-04-01T00:00:00",
        "last_modified_date": "2009-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.0570",
        "title": "The Derivational Complexity Induced by the Dependency Pair Method",
        "authors": [
            "Georg Moser",
            "Andreas Schnabl"
        ],
        "abstract": " We study the derivational complexity induced by the dependency pair method, enhanced with standard refinements. We obtain upper bounds on the derivational complexity induced by the dependency pair method in terms of the derivational complexity of the base techniques employed. In particular we show that the derivational complexity induced by the dependency pair method based on some direct technique, possibly refined by argument filtering, the usable rules criterion, or dependency graphs, is primitive recursive in the derivational complexity induced by the direct method. This implies that the derivational complexity induced by a standard application of the dependency pair method based on traditional termination orders like KBO, LPO, and MPO is exactly the same as if those orders were applied as the only termination technique. \n    ",
        "submission_date": "2009-04-03T00:00:00",
        "last_modified_date": "2011-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.0721",
        "title": "Optimal Tableau Decision Procedures for PDL",
        "authors": [
            "Linh Anh Nguyen",
            "Andrzej Sza\u0142as"
        ],
        "abstract": "  We reformulate Pratt's tableau decision procedure of checking satisfiability of a set of formulas in PDL. Our formulation is simpler and more direct for implementation. Extending the method we give the first EXPTIME (optimal) tableau decision procedure not based on transformation for checking consistency of an ABox w.r.t. a TBox in PDL (here, PDL is treated as a description logic). We also prove the new result that the data complexity of the instance checking problem in PDL is coNP-complete.\n    ",
        "submission_date": "2009-04-04T00:00:00",
        "last_modified_date": "2009-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.0981",
        "title": "Dependency Pairs and Polynomial Path Orders",
        "authors": [
            "Martin Avanzini",
            "Georg Moser"
        ],
        "abstract": "We show how polynomial path orders can be employed efficiently in conjunction with weak innermost dependency pairs to automatically certify polynomial runtime complexity of term rewrite systems and the polytime computability of the functions computed. The established techniques have been implemented and we provide ample experimental data to assess the new method.\n    ",
        "submission_date": "2009-04-06T00:00:00",
        "last_modified_date": "2011-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.1629",
        "title": "Fuzzy inference based mentality estimation for eye robot agent",
        "authors": [
            "Yoichi Yamazaki",
            "Fangyan Dong",
            "Yuta Masuda",
            "Yukiko Uehara",
            "Petar Kormushev",
            "Hai An Vu",
            "Phuc Quang Le",
            "Kaoru Hirota"
        ],
        "abstract": "  Household robots need to communicate with human beings in a friendly fashion. To achieve better understanding of displayed information, an importance and a certainty of the information should be communicated together with the main information. The proposed intent expression system aims to convey this additional information using an eye robot. The eye motions are represented as states in a pleasure-arousal space model. Change of the model state is calculated by fuzzy inference according to the importance and certainty of the displayed information. This change influences the arousal-sleep coordinate in the space which corresponds to activeness in communication. The eye robot provides a basic interface for the mascot robot system which is an easy to understand information terminal for home environments in a humatronics society.\n    ",
        "submission_date": "2009-04-10T00:00:00",
        "last_modified_date": "2009-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.1631",
        "title": "Intent expression using eye robot for mascot robot system",
        "authors": [
            "Yoichi Yamazaki",
            "Fangyan Dong",
            "Yuta Masuda",
            "Yukiko Uehara",
            "Petar Kormushev",
            "Hai An Vu",
            "Phuc Quang Le",
            "Kaoru Hirota"
        ],
        "abstract": "  An intent expression system using eye robots is proposed for a mascot robot system from a viewpoint of humatronics. The eye robot aims at providing a basic interface method for an information terminal robot system. To achieve better understanding of the displayed information, the importance and the degree of certainty of the information should be communicated along with the main content. The proposed intent expression system aims at conveying this additional information using the eye robot system. Eye motions are represented as the states in a pleasure-arousal space model. Changes in the model state are calculated by fuzzy inference according to the importance and degree of certainty of the displayed information. These changes influence the arousal-sleep coordinates in the space that corresponds to levels of liveliness during communication. The eye robot provides a basic interface for the mascot robot system that is easy to be understood as an information terminal for home environments in a humatronics society.\n    ",
        "submission_date": "2009-04-10T00:00:00",
        "last_modified_date": "2009-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.1931",
        "title": "KiWi: A Scalable Subspace Clustering Algorithm for Gene Expression Analysis",
        "authors": [
            "Obi L. Griffith",
            "Byron J. Gao",
            "Mikhail Bilenky",
            "Yuliya Prichyna",
            "Martin Ester",
            "Steven J.M. Jones"
        ],
        "abstract": "  Subspace clustering has gained increasing popularity in the analysis of gene expression data. Among subspace cluster models, the recently introduced order-preserving sub-matrix (OPSM) has demonstrated high promise. An OPSM, essentially a pattern-based subspace cluster, is a subset of rows and columns in a data matrix for which all the rows induce the same linear ordering of columns. Existing OPSM discovery methods do not scale well to increasingly large expression datasets. In particular, twig clusters having few genes and many experiments incur explosive computational costs and are completely pruned off by existing methods. However, it is of particular interest to determine small groups of genes that are tightly coregulated across many conditions. In this paper, we present KiWi, an OPSM subspace clustering algorithm that is scalable to massive datasets, capable of discovering twig clusters and identifying negative as well as positive correlations. We extensively validate KiWi using relevant biological datasets and show that KiWi correctly assigns redundant probes to the same cluster, groups experiments with common clinical annotations, differentiates real promoter sequences from negative control sequences, and shows good association with cis-regulatory motif predictions.\n    ",
        "submission_date": "2009-04-13T00:00:00",
        "last_modified_date": "2009-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.2623",
        "title": "Exponential Family Graph Matching and Ranking",
        "authors": [
            "James Petterson",
            "Tiberio Caetano",
            "Julian McAuley",
            "Jin Yu"
        ],
        "abstract": "  We present a method for learning max-weight matching predictors in bipartite graphs. The method consists of performing maximum a posteriori estimation in exponential families with sufficient statistics that encode permutations and data features. Although inference is in general hard, we show that for one very relevant application - web page ranking - exact inference is efficient. For general model instances, an appropriate sampler is readily available. Contrary to existing max-margin matching models, our approach is statistically consistent and, in addition, experiments with increasing sample sizes indicate superior improvement over such models. We apply the method to graph matching in computer vision as well as to a standard benchmark dataset for learning web page ranking, in which we obtain state-of-the-art results, in particular improving on max-margin variants. The drawback of this method with respect to max-margin alternatives is its runtime for large graphs, which is comparatively high.\n    ",
        "submission_date": "2009-04-17T00:00:00",
        "last_modified_date": "2009-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.3310",
        "title": "FastLMFI: An Efficient Approach for Local Maximal Patterns Propagation and Maximal Patterns Superset Checking",
        "authors": [
            "Shariq Bashir",
            "Abdul Rauf Baig"
        ],
        "abstract": "  Maximal frequent patterns superset checking plays an important role in the efficient mining of complete Maximal Frequent Itemsets (MFI) and maximal search space pruning. In this paper we present a new indexing approach, FastLMFI for local maximal frequent patterns (itemset) propagation and maximal patterns superset checking. Experimental results on different sparse and dense datasets show that our work is better than the previous well known progressive focusing technique. We have also integrated our superset checking approach with an existing state of the art maximal itemsets algorithm Mafia, and compare our results with current best maximal itemsets algorithms afopt-max and FP (zhu)-max. Our results outperform afopt-max and FP (zhu)-max on dense (chess and mushroom) datasets on almost all support thresholds, which shows the effectiveness of our approach.\n    ",
        "submission_date": "2009-04-21T00:00:00",
        "last_modified_date": "2009-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.3312",
        "title": "HybridMiner: Mining Maximal Frequent Itemsets Using Hybrid Database Representation Approach",
        "authors": [
            "Shariq Bashir",
            "Abdul Rauf Baig"
        ],
        "abstract": "  In this paper we present a novel hybrid (arraybased layout and vertical bitmap layout) database representation approach for mining complete Maximal Frequent Itemset (MFI) on sparse and large datasets. Our work is novel in terms of scalability, item search order and two horizontal and vertical projection techniques. We also present a maximal algorithm using this hybrid database representation approach. Different experimental results on real and sparse benchmark datasets show that our approach is better than previous state of art maximal algorithms.\n    ",
        "submission_date": "2009-04-21T00:00:00",
        "last_modified_date": "2009-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.3316",
        "title": "Ramp: Fast Frequent Itemset Mining with Efficient Bit-Vector Projection Technique",
        "authors": [
            "Shariq Bashir",
            "Abdul Rauf Baig"
        ],
        "abstract": "  Mining frequent itemset using bit-vector representation approach is very efficient for dense type datasets, but highly inefficient for sparse datasets due to lack of any efficient bit-vector projection technique. In this paper we present a novel efficient bit-vector projection technique, for sparse and dense datasets. To check the efficiency of our bit-vector projection technique, we present a new frequent itemset mining algorithm Ramp (Real Algorithm for Mining Patterns) build upon our bit-vector projection technique. The performance of the Ramp is compared with the current best (all, maximal and closed) frequent itemset mining algorithms on benchmark datasets. Different experimental results on sparse and dense datasets show that mining frequent itemset using Ramp is faster than the current best algorithms, which show the effectiveness of our bit-vector projection idea. We also present a new local maximal frequent itemsets propagation and maximal itemset superset checking approach FastLMFI, build upon our PBR bit-vector projection technique. Our different computational experiments suggest that itemset maximality checking using FastLMFI is fast and efficient than a previous will known progressive focusing approach.\n    ",
        "submission_date": "2009-04-21T00:00:00",
        "last_modified_date": "2009-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.3319",
        "title": "Fast Algorithms for Mining Interesting Frequent Itemsets without Minimum Support",
        "authors": [
            "Shariq Bashir",
            "Zahoor Jan",
            "Abdul Rauf Baig"
        ],
        "abstract": "  Real world datasets are sparse, dirty and contain hundreds of items. In such situations, discovering interesting rules (results) using traditional frequent itemset mining approach by specifying a user defined input support threshold is not appropriate. Since without any domain knowledge, setting support threshold small or large can output nothing or a large number of redundant uninteresting results. Recently a novel approach of mining only N-most/Top-K interesting frequent itemsets has been proposed, which discovers the top N interesting results without specifying any user defined support threshold. However, mining interesting frequent itemsets without minimum support threshold are more costly in terms of itemset search space exploration and processing cost. Thereby, the efficiency of their mining highly depends upon three main factors (1) Database representation approach used for itemset frequency counting, (2) Projection of relevant transactions to lower level nodes of search space and (3) Algorithm implementation technique. Therefore, to improve the efficiency of mining process, in this paper we present two novel algorithms called (N-MostMiner and Top-K-Miner) using the bit-vector representation approach which is very efficient in terms of itemset frequency counting and transactions projection. In addition to this, several efficient implementation techniques of N-MostMiner and Top-K-Miner are also present which we experienced in our implementation. Our experimental results on benchmark datasets suggest that the NMostMiner and Top-K-Miner are very efficient in terms of processing time as compared to current best algorithms BOMO and TFP.\n    ",
        "submission_date": "2009-04-21T00:00:00",
        "last_modified_date": "2009-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.3320",
        "title": "Using Association Rules for Better Treatment of Missing Values",
        "authors": [
            "Shariq Bashir",
            "Saad Razzaq",
            "Umer Maqbool",
            "Sonya Tahir",
            "Abdul Rauf Baig"
        ],
        "abstract": "  The quality of training data for knowledge discovery in databases (KDD) and data mining depends upon many factors, but handling missing values is considered to be a crucial factor in overall data quality. Today real world datasets contains missing values due to human, operational error, hardware malfunctioning and many other factors. The quality of knowledge extracted, learning and decision problems depend directly upon the quality of training data. By considering the importance of handling missing values in KDD and data mining tasks, in this paper we propose a novel Hybrid Missing values Imputation Technique (HMiT) using association rules mining and hybrid combination of k-nearest neighbor approach. To check the effectiveness of our HMiT missing values imputation technique, we also perform detail experimental results on real world datasets. Our results suggest that the HMiT technique is not only better in term of accuracy but it also take less processing time as compared to current best missing values imputation technique based on k-nearest neighbor approach, which shows the effectiveness of our missing values imputation technique.\n    ",
        "submission_date": "2009-04-21T00:00:00",
        "last_modified_date": "2009-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.3321",
        "title": "Introducing Partial Matching Approach in Association Rules for Better Treatment of Missing Values",
        "authors": [
            "Shariq Bashir",
            "Saad Razzaq",
            "Umer Maqbool",
            "Sonya Tahir",
            "Abdul Rauf Baig"
        ],
        "abstract": "  Handling missing values in training datasets for constructing learning models or extracting useful information is considered to be an important research task in data mining and knowledge discovery in databases. In recent years, lot of techniques are proposed for imputing missing values by considering attribute relationships with missing value observation and other observations of training dataset. The main deficiency of such techniques is that, they depend upon single approach and do not combine multiple approaches, that why they are less accurate. To improve the accuracy of missing values imputation, in this paper we introduce a novel partial matching concept in association rules mining, which shows better results as compared to full matching concept that we described in our previous work. Our imputation technique combines the partial matching concept in association rules with k-nearest neighbor approach. Since this is a hybrid technique, therefore its accuracy is much better than as compared to those techniques which depend upon single approach. To check the efficiency of our technique, we also provide detail experimental results on number of benchmark datasets which show better results as compared to previous approaches.\n    ",
        "submission_date": "2009-04-21T00:00:00",
        "last_modified_date": "2009-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.3356",
        "title": "A method for Hedging in continuous time",
        "authors": [
            "Yoav Freund"
        ],
        "abstract": "  We present a method for hedging in continuous time.\n    ",
        "submission_date": "2009-04-21T00:00:00",
        "last_modified_date": "2009-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.3469",
        "title": "Toggling operators in computability logic",
        "authors": [
            "Giorgi Japaridze"
        ],
        "abstract": "  Computability logic (CL) (see ",
        "submission_date": "2009-04-22T00:00:00",
        "last_modified_date": "2010-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.3667",
        "title": "Considerations upon the Machine Learning Technologies",
        "authors": [
            "Alin Munteanu",
            "Cristina Ofelia Sofran"
        ],
        "abstract": "  Artificial intelligence offers superior techniques and methods by which problems from diverse domains may find an optimal solution. The Machine Learning technologies refer to the domain of artificial intelligence aiming to develop the techniques allowing the computers to \"learn\". Some systems based on Machine Learning technologies tend to eliminate the necessity of the human intelligence while the others adopt a man-machine collaborative approach.\n    ",
        "submission_date": "2009-04-23T00:00:00",
        "last_modified_date": "2009-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.4708",
        "title": "Quality Classifiers for Open Source Software Repositories",
        "authors": [
            "George Tsatsaronis",
            "Maria Halkidi",
            "Emmanouel A. Giakoumakis"
        ],
        "abstract": "  Open Source Software (OSS) often relies on large repositories, like SourceForge, for initial incubation. The OSS repositories offer a large variety of meta-data providing interesting information about projects and their success. In this paper we propose a data mining approach for training classifiers on the OSS meta-data provided by such data repositories. The classifiers learn to predict the successful continuation of an OSS project. The `successfulness' of projects is defined in terms of the classifier confidence with which it predicts that they could be ported in popular OSS projects (such as FreeBSD, Gentoo Portage).\n    ",
        "submission_date": "2009-04-29T00:00:00",
        "last_modified_date": "2009-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.4717",
        "title": "Continuous Strategy Replicator Dynamics for Multi--Agent Learning",
        "authors": [
            "Aram Galstyan"
        ],
        "abstract": "The problem of multi-agent learning and adaptation has attracted a great deal of attention in recent years. It has been suggested that the dynamics of multi agent learning can be studied using replicator equations from population biology. Most existing studies so far have been limited to discrete strategy spaces with a small number of available actions. In many cases, however, the choices available to agents are better characterized by continuous spectra. This paper suggests a generalization of the replicator framework that allows to study the adaptive dynamics of Q-learning agents with continuous strategy spaces. Instead of probability vectors, agents strategies are now characterized by probability measures over continuous variables. As a result, the ordinary differential equations for the discrete case are replaced by a system of coupled integral--differential replicator equations that describe the mutual evolution of individual agent strategies. We derive a set of functional equations describing the steady state of the replicator dynamics, examine their solutions for several two-player games, and confirm our analytical results using simulations.\n    ",
        "submission_date": "2009-04-29T00:00:00",
        "last_modified_date": "2011-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0904.4836",
        "title": "FaceBots: Steps Towards Enhanced Long-Term Human-Robot Interaction by Utilizing and Publishing Online Social Information",
        "authors": [
            "Nikolaos Mavridis",
            "Shervin Emami",
            "Chandan Datta",
            "Wajahat Kamzi",
            "Chiraz BenAbdelkader",
            "Panos Toulis",
            "Andry Tanoto",
            "Tamer Rabie"
        ],
        "abstract": "  Our project aims at supporting the creation of sustainable and meaningful longer-term human-robot relationships through the creation of embodied robots with face recognition and natural language dialogue capabilities, which exploit and publish social information available on the web (Facebook). Our main underlying experimental hypothesis is that such relationships can be significantly enhanced if the human and the robot are gradually creating a pool of shared episodic memories that they can co-refer to (shared memories), and if they are both embedded in a social web of other humans and robots they both know and encounter (shared friends). In this paper, we are presenting such a robot, which as we will see achieves two significant novelties.\n    ",
        "submission_date": "2009-04-30T00:00:00",
        "last_modified_date": "2009-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.0677",
        "title": "Feasibility of random basis function approximators for modeling and control",
        "authors": [
            "Ivan Tyukin",
            "Danil Prokhorov"
        ],
        "abstract": "  We discuss the role of random basis function approximators in modeling and control. We analyze the published work on random basis function approximators and demonstrate that their favorable error rate of convergence O(1/n) is guaranteed only with very substantial computational resources. We also discuss implications of our analysis for applications of neural networks in modeling and control.\n    ",
        "submission_date": "2009-05-05T00:00:00",
        "last_modified_date": "2009-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.1424",
        "title": "Concept Stability for Constructing Taxonomies of Web-site Users",
        "authors": [
            "Sergei O. Kuznetsov",
            "Dmitry I. Ignatov"
        ],
        "abstract": "Owners of a web-site are often interested in analysis of groups of users of their site. Information on these groups can help optimizing the structure and contents of the site. In this paper we use an approach based on formal concepts for constructing taxonomies of user groups. For decreasing the huge amount of concepts that arise in applications, we employ stability index of a concept, which describes how a group given by a concept extent differs from other such groups. We analyze resulting taxonomies of user groups for three target websites.\n    ",
        "submission_date": "2009-05-09T00:00:00",
        "last_modified_date": "2016-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.1751",
        "title": "Experiment Study of Entropy Convergence of Ant Colony Optimization",
        "authors": [
            "Chao-Yang Pang",
            "Chong-Bao Wang",
            "Ben-Qiong Hu"
        ],
        "abstract": "  Ant colony optimization (ACO) has been applied to the field of combinatorial optimization widely. But the study of convergence theory of ACO is rare under general condition. In this paper, the authors try to find the evidence to prove that entropy is related to the convergence of ACO, especially to the estimation of the minimum iteration number of convergence. Entropy is a new view point possibly to studying the ACO convergence under general condition. Key Words: Ant Colony Optimization, Convergence of ACO, Entropy\n    ",
        "submission_date": "2009-05-12T00:00:00",
        "last_modified_date": "2009-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.2004",
        "title": "Termination Prediction for General Logic Programs",
        "authors": [
            "Yi-Dong Shen",
            "Danny De Schreye",
            "Dean Voets"
        ],
        "abstract": "  We present a heuristic framework for attacking the undecidable termination problem of logic programs, as an alternative to current termination/non-termination proof approaches. We introduce an idea of termination prediction, which predicts termination of a logic program in case that neither a termination nor a non-termination proof is applicable. We establish a necessary and sufficient characterization of infinite (generalized) SLDNF-derivations with arbitrary (concrete or moded) queries, and develop an algorithm that predicts termination of general logic programs with arbitrary non-floundering queries. We have implemented a termination prediction tool and obtained quite satisfactory experimental results. Except for five programs which break the experiment time limit, our prediction is 100% correct for all 296 benchmark programs of the Termination Competition 2007, of which eighteen programs cannot be proved by any of the existing state-of-the-art analyzers like AProVE07, NTI, Polytool and TALP.\n    ",
        "submission_date": "2009-05-13T00:00:00",
        "last_modified_date": "2009-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.2449",
        "title": "The Role of Self-Forensics in Vehicle Crash Investigations and Event Reconstruction",
        "authors": [
            "Serguei A. Mokhov"
        ],
        "abstract": "  This paper further introduces and formalizes a novel concept of self-forensics for automotive vehicles, specified in the Forensic Lucid language. We argue that self-forensics, with the forensics taken out of the cybercrime domain, is applicable to \"self-dissection\" of intelligent vehicles and hardware systems for automated incident and anomaly analysis and event reconstruction by the software with or without the aid of the engineering teams in a variety of forensic scenarios. We propose a formal design, requirements, and specification of the self-forensic enabled units (similar to blackboxes) in vehicles that will help investigation of incidents and also automated reasoning and verification of theories along with the events reconstruction in a formal model. We argue such an analysis is beneficial to improve the safety of the passengers and their vehicles, like the airline industry does for planes.\n    ",
        "submission_date": "2009-05-15T00:00:00",
        "last_modified_date": "2009-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.2473",
        "title": "On the Workings of Genetic Algorithms: The Genoclique Fixing Hypothesis",
        "authors": [
            "Keki M. Burjorjee"
        ],
        "abstract": "  We recently reported that the simple genetic algorithm (SGA) is capable of performing a remarkable form of sublinear computation which has a straightforward connection with the general problem of interacting attributes in data-mining. In this paper we explain how the SGA can leverage this computational proficiency to perform efficient adaptation on a broad class of fitness functions. Based on the relative ease with which a practical fitness function might belong to this broad class, we submit a new hypothesis about the workings of genetic algorithms. We explain why our hypothesis is superior to the building block hypothesis, and, by way of empirical validation, we present the results of an experiment in which the use of a simple mechanism called clamping dramatically improved the performance of an SGA with uniform crossover on large, randomly generated instances of the MAX 3-SAT problem.\n    ",
        "submission_date": "2009-05-15T00:00:00",
        "last_modified_date": "2009-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.2882",
        "title": "Do not Choose Representation just Change: An Experimental Study in States based EA",
        "authors": [
            "Maroun Bercachi",
            "Philippe Collard",
            "Manuel Clergue",
            "Sebastien Verel"
        ],
        "abstract": "  Our aim in this paper is to analyse the phenotypic effects (evolvability) of diverse coding conversion operators in an instance of the states based evolutionary algorithm (SEA). Since the representation of solutions or the selection of the best encoding during the optimization process has been proved to be very important for the efficiency of evolutionary algorithms (EAs), we will discuss a strategy of coupling more than one representation and different procedures of conversion from one coding to another during the search. Elsewhere, some EAs try to use multiple representations (SM-GA, SEA, etc.) in intention to benefit from the characteristics of each of them. In spite of those results, this paper shows that the change of the representation is also a crucial approach to take into consideration while attempting to increase the performances of such EAs. As a demonstrative example, we use a two states SEA (2-SEA) which has two identical search spaces but different coding conversion operators. The results show that the way of changing from one coding to another and not only the choice of the best representation nor the representation itself is very advantageous and must be taken into account in order to well-desing and improve EAs execution.\n    ",
        "submission_date": "2009-05-18T00:00:00",
        "last_modified_date": "2009-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.3108",
        "title": "A Note on the Complexity of the Satisfiability Problem for Graded Modal Logics",
        "authors": [
            "Yevgeny Kazakov",
            "Ian Pratt-Hartmann"
        ],
        "abstract": "  Graded modal logic is the formal language obtained from ordinary (propositional) modal logic by endowing its modal operators with cardinality constraints. Under the familiar possible-worlds semantics, these augmented modal operators receive interpretations such as \"It is true at no fewer than 15 accessible worlds that...\", or \"It is true at no more than 2 accessible worlds that...\". We investigate the complexity of satisfiability for this language over some familiar classes of frames. This problem is more challenging than its ordinary modal logic counterpart--especially in the case of transitive frames, where graded modal logic lacks the tree-model property. We obtain tight complexity bounds for the problem of determining the satisfiability of a given graded modal logic formula over the classes of frames characterized by any combination of reflexivity, seriality, symmetry, transitivity and the Euclidean property.\n    ",
        "submission_date": "2009-05-19T00:00:00",
        "last_modified_date": "2009-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.3885",
        "title": "Swap Bribery",
        "authors": [
            "E. Elkind",
            "P. Faliszewski",
            "A. Slinko"
        ],
        "abstract": "  In voting theory, bribery is a form of manipulative behavior in which an external actor (the briber) offers to pay the voters to change their votes in order to get her preferred candidate elected. We investigate a model of bribery where the price of each vote depends on the amount of change that the voter is asked to implement. Specifically, in our model the briber can change a voter's preference list by paying for a sequence of swaps of consecutive candidates. Each swap may have a different price; the price of a bribery is the sum of the prices of all swaps that it involves. We prove complexity results for this model, which we call swap bribery, for a broad class of election systems, including variants of approval and k-approval, Borda, Copeland, and maximin.\n    ",
        "submission_date": "2009-05-24T00:00:00",
        "last_modified_date": "2009-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0905.4918",
        "title": "Divide and Conquer: Partitioning Online Social Networks",
        "authors": [
            "Josep M. Pujol",
            "Vijay Erramilli",
            "Pablo Rodriguez"
        ],
        "abstract": "  Online Social Networks (OSNs) have exploded in terms of scale and scope over the last few years. The unprecedented growth of these networks present challenges in terms of system design and maintenance. One way to cope with this is by partitioning such large networks and assigning these partitions to different machines. However, social networks possess unique properties that make the partitioning problem non-trivial. The main contribution of this paper is to understand different properties of social networks and how these properties can guide the choice of a partitioning algorithm. Using large scale measurements representing real OSNs, we first characterize different properties of social networks, and then we evaluate qualitatively different partitioning methods that cover the design space. We expose different trade-offs involved and understand them in light of properties of social networks. We show that a judicious choice of a partitioning scheme can help improve performance.\n    ",
        "submission_date": "2009-05-29T00:00:00",
        "last_modified_date": "2009-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.0052",
        "title": "A Minimum Description Length Approach to Multitask Feature Selection",
        "authors": [
            "Brian Tomasik"
        ],
        "abstract": "  Many regression problems involve not one but several response variables (y's). Often the responses are suspected to share a common underlying structure, in which case it may be advantageous to share information across them; this is known as multitask learning. As a special case, we can use multiple responses to better identify shared predictive features -- a project we might call multitask feature selection.\n",
        "submission_date": "2009-05-30T00:00:00",
        "last_modified_date": "2009-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.0885",
        "title": "Mining Compressed Repetitive Gapped Sequential Patterns Efficiently",
        "authors": [
            "Yongxin Tong",
            "Li Zhao",
            "Dan Yu",
            "Shilong Ma",
            "Ke Xu"
        ],
        "abstract": "  Mining frequent sequential patterns from sequence databases has been a central research topic in data mining and various efficient mining sequential patterns algorithms have been proposed and studied. Recently, in many problem domains (e.g, program execution traces), a novel sequential pattern mining research, called mining repetitive gapped sequential patterns, has attracted the attention of many researchers, considering not only the repetition of sequential pattern in different sequences but also the repetition within a sequence is more meaningful than the general sequential pattern mining which only captures occurrences in different sequences. However, the number of repetitive gapped sequential patterns generated by even these closed mining algorithms may be too large to understand for users, especially when support threshold is low. In this paper, we propose and study the problem of compressing repetitive gapped sequential patterns. Inspired by the ideas of summarizing frequent itemsets, RPglobal, we develop an algorithm, CRGSgrow (Compressing Repetitive Gapped Sequential pattern grow), including an efficient pruning strategy, SyncScan, and an efficient representative pattern checking scheme, -dominate sequential pattern checking. The CRGSgrow is a two-step approach: in the first step, we obtain all closed repetitive sequential patterns as the candidate set of representative repetitive sequential patterns, and at the same time get the most of representative repetitive sequential patterns; in the second step, we only spend a little time in finding the remaining the representative patterns from the candidate set. An empirical study with both real and synthetic data sets clearly shows that the CRGSgrow has good performance.\n    ",
        "submission_date": "2009-06-04T00:00:00",
        "last_modified_date": "2009-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.1713",
        "title": "Feature Reinforcement Learning: Part I: Unstructured MDPs",
        "authors": [
            "Marcus Hutter"
        ],
        "abstract": "  General-purpose, intelligent, learning agents cycle through sequences of observations, actions, and rewards that are complex, uncertain, unknown, and non-Markovian. On the other hand, reinforcement learning is well-developed for small finite state Markov decision processes (MDPs). Up to now, extracting the right state representations out of bare observations, that is, reducing the general agent setup to the MDP framework, is an art that involves significant effort by designers. The primary goal of this work is to automate the reduction process and thereby significantly expand the scope of many existing reinforcement learning algorithms and the agents that employ them. Before we can think of mechanizing this search for suitable MDPs, we need a formal objective criterion. The main contribution of this article is to develop such a criterion. I also integrate the various parts into one learning algorithm. Extensions to more realistic dynamic Bayesian networks are developed in Part II. The role of POMDPs is also considered there.\n    ",
        "submission_date": "2009-06-09T00:00:00",
        "last_modified_date": "2009-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.1814",
        "title": "Large-Margin kNN Classification Using a Deep Encoder Network",
        "authors": [
            "Martin Renqiang Min",
            "David A. Stanley",
            "Zineng Yuan",
            "Anthony Bonner",
            "Zhaolei Zhang"
        ],
        "abstract": "  KNN is one of the most popular classification methods, but it often fails to work well with inappropriate choice of distance metric or due to the presence of numerous class-irrelevant features. Linear feature transformation methods have been widely applied to extract class-relevant information to improve kNN classification, which is very limited in many applications. Kernels have been used to learn powerful non-linear feature transformations, but these methods fail to scale to large datasets. In this paper, we present a scalable non-linear feature mapping method based on a deep neural network pretrained with restricted boltzmann machines for improving kNN classification in a large-margin framework, which we call DNet-kNN. DNet-kNN can be used for both classification and for supervised dimensionality reduction. The experimental results on two benchmark handwritten digit datasets show that DNet-kNN has much better performance than large-margin kNN using a linear mapping and kNN based on a deep autoencoder pretrained with retricted boltzmann machines.\n    ",
        "submission_date": "2009-06-09T00:00:00",
        "last_modified_date": "2009-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.1845",
        "title": "Towards Improving Validation, Verification, Crash Investigations, and Event Reconstruction of Flight-Critical Systems with Self-Forensics",
        "authors": [
            "Serguei A. Mokhov"
        ],
        "abstract": "  This paper introduces a novel concept of self-forensics to complement the standard autonomic self-CHOP properties of the self-managed systems, to be specified in the Forensic Lucid language. We argue that self-forensics, with the forensics taken out of the cybercrime domain, is applicable to \"self-dissection\" for the purpose of verification of autonomous software and hardware systems of flight-critical systems for automated incident and anomaly analysis and event reconstruction by the engineering teams in a variety of incident scenarios during design and testing as well as actual flight data.\n    ",
        "submission_date": "2009-06-10T00:00:00",
        "last_modified_date": "2009-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.2154",
        "title": "From formulas to cirquents in computability logic",
        "authors": [
            "Giorgi Japaridze"
        ],
        "abstract": " Computability logic (CoL) (see ",
        "submission_date": "2009-06-11T00:00:00",
        "last_modified_date": "2011-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.2228",
        "title": "Characterising equilibrium logic and nested logic programs: Reductions and complexity",
        "authors": [
            "David Pearce",
            "Hans Tompits",
            "Stefan Woltran"
        ],
        "abstract": "  Equilibrium logic is an approach to nonmonotonic reasoning that extends the stable-model and answer-set semantics for logic programs. In particular, it includes the general case of nested logic programs, where arbitrary Boolean combinations are permitted in heads and bodies of rules, as special kinds of theories. In this paper, we present polynomial reductions of the main reasoning tasks associated with equilibrium logic and nested logic programs into quantified propositional logic, an extension of classical propositional logic where quantifications over atomic formulas are permitted. We provide reductions not only for decision problems, but also for the central semantical concepts of equilibrium logic and nested logic programs. In particular, our encodings map a given decision problem into some formula such that the latter is valid precisely in case the former holds. The basic tasks we deal with here are the consistency problem, brave reasoning, and skeptical reasoning. Additionally, we also provide encodings for testing equivalence of theories or programs under different notions of equivalence, viz. ordinary, strong, and uniform equivalence. For all considered reasoning tasks, we analyse their computational complexity and give strict complexity bounds.\n    ",
        "submission_date": "2009-06-11T00:00:00",
        "last_modified_date": "2009-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.2274",
        "title": "A Neural Network Classifier of Volume Datasets",
        "authors": [
            "D\u017eenan Zuki\u0107",
            "Christof Rezk-Salama",
            "Andreas Kolb"
        ],
        "abstract": "  Many state-of-the art visualization techniques must be tailored to the specific type of dataset, its modality (CT, MRI, etc.), the recorded object or anatomical region (head, spine, abdomen, etc.) and other parameters related to the data acquisition process. While parts of the information (imaging modality and acquisition sequence) may be obtained from the meta-data stored with the volume scan, there is important information which is not stored explicitly (anatomical region, tracing compound). Also, meta-data might be incomplete, inappropriate or simply missing.\n",
        "submission_date": "2009-06-12T00:00:00",
        "last_modified_date": "2009-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.2459",
        "title": "Exact Indexing for Massive Time Series Databases under Time Warping Distance",
        "authors": [
            "Vit Niennattrakul",
            "Pongsakorn Ruengronghirunya",
            "Chotirat Ann Ratanamahatana"
        ],
        "abstract": "  Among many existing distance measures for time series data, Dynamic Time Warping (DTW) distance has been recognized as one of the most accurate and suitable distance measures due to its flexibility in sequence alignment. However, DTW distance calculation is computationally intensive. Especially in very large time series databases, sequential scan through the entire database is definitely impractical, even with random access that exploits some index structures since high dimensionality of time series data incurs extremely high I/O cost. More specifically, a sequential structure consumes high CPU but low I/O costs, while an index structure requires low CPU but high I/O costs. In this work, we therefore propose a novel indexed sequential structure called TWIST (Time Warping in Indexed Sequential sTructure) which benefits from both sequential access and index structure. When a query sequence is issued, TWIST calculates lower bounding distances between a group of candidate sequences and the query sequence, and then identifies the data access order in advance, hence reducing a great number of both sequential and random accesses. Impressively, our indexed sequential structure achieves significant speedup in a querying process by a few orders of magnitude. In addition, our method shows superiority over existing rival methods in terms of query processing time, number of page accesses, and storage requirement with no false dismissal guaranteed.\n    ",
        "submission_date": "2009-06-13T00:00:00",
        "last_modified_date": "2009-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.3461",
        "title": "AIS for Misbehavior Detection in Wireless Sensor Networks: Performance and Design Principles",
        "authors": [
            "Martin Drozda",
            "Sven Schaust",
            "Helena Szczerbicka"
        ],
        "abstract": "  A sensor network is a collection of wireless devices that are able to monitor physical or environmental conditions. These devices (nodes) are expected to operate autonomously, be battery powered and have very limited computational capabilities. This makes the task of protecting a sensor network against misbehavior or possible malfunction a challenging problem. In this document we discuss performance of Artificial immune systems (AIS) when used as the mechanism for detecting misbehavior.\n",
        "submission_date": "2009-06-18T00:00:00",
        "last_modified_date": "2009-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.3815",
        "title": "Hybrid Rules with Well-Founded Semantics",
        "authors": [
            "W. Drabent",
            "J. Maluszynski"
        ],
        "abstract": "  A general framework is proposed for integration of rules and external first order theories. It is based on the well-founded semantics of normal logic programs and inspired by ideas of Constraint Logic Programming (CLP) and constructive negation for logic programs. Hybrid rules are normal clauses extended with constraints in the bodies; constraints are certain formulae in the language of the external theory. A hybrid program is a pair of a set of hybrid rules and an external theory. Instances of the framework are obtained by specifying the class of external theories, and the class of constraints. An example instance is integration of (non-disjunctive) Datalog with ontologies formalized as description logics.\n",
        "submission_date": "2009-06-20T00:00:00",
        "last_modified_date": "2009-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.4044",
        "title": "Recommender Systems for the Conference Paper Assignment Problem",
        "authors": [
            "Don Conry",
            "Yehuda Koren",
            "Naren Ramakrishnan"
        ],
        "abstract": "  Conference paper assignment, i.e., the task of assigning paper submissions to reviewers, presents multi-faceted issues for recommender systems research. Besides the traditional goal of predicting `who likes what?', a conference management system must take into account aspects such as: reviewer capacity constraints, adequate numbers of reviews for papers, expertise modeling, conflicts of interest, and an overall distribution of assignments that balances reviewer preferences with conference objectives. Among these, issues of modeling preferences and tastes in reviewing have traditionally been studied separately from the optimization of paper-reviewer assignment. In this paper, we present an integrated study of both these aspects. First, due to the paucity of data per reviewer or per paper (relative to other recommender systems applications) we show how we can integrate multiple sources of information to learn paper-reviewer preference models. Second, our models are evaluated not just in terms of prediction accuracy but in terms of the end-assignment quality. Using a linear programming-based assignment optimization formulation, we show how our approach better explores the space of unsupplied assignments to maximize the overall affinities of papers assigned to reviewers. We demonstrate our results on real reviewer preference data from the IEEE ICDM 2007 conference.\n    ",
        "submission_date": "2009-06-22T00:00:00",
        "last_modified_date": "2009-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.4096",
        "title": "An Event Based Approach To Situational Representation",
        "authors": [
            "Naveen Ashish",
            "Dmitri Kalashnikov",
            "Sharad Mehrotra",
            "Nalini Venkatasubramanian"
        ],
        "abstract": "  Many application domains require representing interrelated real-world activities and/or evolving physical phenomena. In the crisis response domain, for instance, one may be interested in representing the state of the unfolding crisis (e.g., forest fire), the progress of the response activities such as evacuation and traffic control, and the state of the crisis site(s). Such a situation representation can then be used to support a multitude of applications including situation monitoring, analysis, and planning. In this paper, we make a case for an event based representation of situations where events are defined to be domain-specific significant occurrences in space and time. We argue that events offer a unifying and powerful abstraction to building situational awareness applications. We identify challenges in building an Event Management System (EMS) for which traditional data and knowledge management systems prove to be limited and suggest possible directions and technologies to address the challenges.\n    ",
        "submission_date": "2009-06-22T00:00:00",
        "last_modified_date": "2009-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.4228",
        "title": "On Chase Termination Beyond Stratification",
        "authors": [
            "Michael Meier",
            "Michael Schmidt",
            "Georg Lausen"
        ],
        "abstract": "  We study the termination problem of the chase algorithm, a central tool in various database problems such as the constraint implication problem, Conjunctive Query optimization, rewriting queries using views, data exchange, and data integration. The basic idea of the chase is, given a database instance and a set of constraints as input, to fix constraint violations in the database instance. It is well-known that, for an arbitrary set of constraints, the chase does not necessarily terminate (in general, it is even undecidable if it does or not). Addressing this issue, we review the limitations of existing sufficient termination conditions for the chase and develop new techniques that allow us to establish weaker sufficient conditions. In particular, we introduce two novel termination conditions called safety and inductive restriction, and use them to define the so-called T-hierarchy of termination conditions. We then study the interrelations of our termination conditions with previous conditions and the complexity of checking our conditions. This analysis leads to an algorithm that checks membership in a level of the T-hierarchy and accounts for the complexity of termination conditions. As another contribution, we study the problem of data-dependent chase termination and present sufficient termination conditions w.r.t. fixed instances. They might guarantee termination although the chase does not terminate in the general case. As an application of our techniques beyond those already mentioned, we transfer our results into the field of query answering over knowledge bases where the chase on the underlying database may not terminate, making existing algorithms applicable to broader classes of constraints.\n    ",
        "submission_date": "2009-06-23T00:00:00",
        "last_modified_date": "2009-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.4316",
        "title": "Constructive Decision Theory",
        "authors": [
            "Lawrence Blume",
            "David Easley",
            "Joseph Y. Halpern"
        ],
        "abstract": "In most contemporary approaches to decision making, a decision problem is described by a sets of states and set of outcomes, and a rich set of acts, which are functions from states to outcomes over which the decision maker (DM) has preferences. Most interesting decision problems, however, do not come with a state space and an outcome space. Indeed, in complex problems it is often far from clear what the state and outcome spaces would be. We present an alternative foundation for decision making, in which the primitive objects of choice are syntactic programs. A representation theorem is proved in the spirit of standard representation theorems, showing that if the DM's preference relation on objects of choice satisfies appropriate axioms, then there exist a set S of states, a set O of outcomes, a way of interpreting the objects of choice as functions from S to O, a probability on S, and a utility function on O, such that the DM prefers choice a to choice b if and only if the expected utility of a is higher than that of b. Thus, the state space and outcome space are subjective, just like the probability and utility; they are not part of the description of the problem. In principle, a modeler can test for SEU behavior without having access to states or outcomes. We illustrate the power of our approach by showing that it can capture decision makers who are subject to framing effects.\n    ",
        "submission_date": "2009-06-23T00:00:00",
        "last_modified_date": "2021-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.5040",
        "title": "Towards the Patterns of Hard CSPs with Association Rule Mining",
        "authors": [
            "Chendong Li"
        ],
        "abstract": "  The hardness of finite domain Constraint Satisfaction Problems (CSPs) is a very important research area in Constraint Programming (CP) community. However, this problem has not yet attracted much attention from the researchers in the association rule mining community. As a popular data mining technique, association rule mining has an extremely wide application area and it has already been successfully applied to many interdisciplines. In this paper, we study the association rule mining techniques and propose a cascaded approach to extract the interesting patterns of the hard CSPs. As far as we know, this problem is investigated with the data mining techniques for the first time. Specifically, we generate the random CSPs and collect their characteristics by solving all the CSP instances, and then apply the data mining techniques on the data set and further to discover the interesting patterns of the hardness of the randomly generated CSPs\n    ",
        "submission_date": "2009-06-27T00:00:00",
        "last_modified_date": "2009-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.5120",
        "title": "Comments on \"A new combination of evidence based on compromise\" by K. Yamada",
        "authors": [
            "Jean Dezert",
            "Arnaud Martin",
            "Florentin Smarandache"
        ],
        "abstract": "  Comments on ``A new combination of evidence based on compromise'' by K. Yamada\n    ",
        "submission_date": "2009-06-28T00:00:00",
        "last_modified_date": "2009-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0906.5485",
        "title": "Query Significance in Databases via Randomizations",
        "authors": [
            "Markus Ojala",
            "Gemma C. Garriga",
            "Aristides Gionis",
            "Heikki Mannila"
        ],
        "abstract": "  Many sorts of structured data are commonly stored in a multi-relational format of interrelated tables. Under this relational model, exploratory data analysis can be done by using relational queries. As an example, in the Internet Movie Database (IMDb) a query can be used to check whether the average rank of action movies is higher than the average rank of drama movies.\n",
        "submission_date": "2009-06-30T00:00:00",
        "last_modified_date": "2009-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.0328",
        "title": "Degenerate neutrality creates evolvable fitness landscapes",
        "authors": [
            "James M Whitacre",
            "Axel Bender"
        ],
        "abstract": "  Understanding how systems can be designed to be evolvable is fundamental to research in optimization, evolution, and complex systems science. Many researchers have thus recognized the importance of evolvability, i.e. the ability to find new variants of higher fitness, in the fields of biological evolution and evolutionary computation. Recent studies by Ciliberti et al (Proc. Nat. Acad. Sci., 2007) and Wagner (Proc. R. Soc. B., 2008) propose a potentially important link between the robustness and the evolvability of a system. In particular, it has been suggested that robustness may actually lead to the emergence of evolvability. Here we study two design principles, redundancy and degeneracy, for achieving robustness and we show that they have a dramatically different impact on the evolvability of the system. In particular, purely redundant systems are found to have very little evolvability while systems with degeneracy, i.e. distributed robustness, can be orders of magnitude more evolvable. These results offer insights into the general principles for achieving evolvability and may prove to be an important step forward in the pursuit of evolvable representations in evolutionary computation.\n    ",
        "submission_date": "2009-07-02T00:00:00",
        "last_modified_date": "2009-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.0329",
        "title": "Evidence of coevolution in multi-objective evolutionary algorithms",
        "authors": [
            "James M Whitacre"
        ],
        "abstract": "  This paper demonstrates that simple yet important characteristics of coevolution can occur in evolutionary algorithms when only a few conditions are met. We find that interaction-based fitness measurements such as fitness (linear) ranking allow for a form of coevolutionary dynamics that is observed when 1) changes are made in what solutions are able to interact during the ranking process and 2) evolution takes place in a multi-objective environment. This research contributes to the study of simulated evolution in a at least two ways. First, it establishes a broader relationship between coevolution and multi-objective optimization than has been previously considered in the literature. Second, it demonstrates that the preconditions for coevolutionary behavior are weaker than previously thought. In particular, our model indicates that direct cooperation or competition between species is not required for coevolution to take place. Moreover, our experiments provide evidence that environmental perturbations can drive coevolutionary processes; a conclusion that mirrors arguments put forth in dual phase evolution theory. In the discussion, we briefly consider how our results may shed light onto this and other recent theories of evolution.\n    ",
        "submission_date": "2009-07-02T00:00:00",
        "last_modified_date": "2009-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.0332",
        "title": "Survival of the flexible: explaining the recent dominance of nature-inspired optimization within a rapidly evolving world",
        "authors": [
            "James M Whitacre"
        ],
        "abstract": "Although researchers often comment on the rising popularity of nature-inspired meta-heuristics (NIM), there has been a paucity of data to directly support the claim that NIM are growing in prominence compared to other optimization techniques. This study presents evidence that the use of NIM is not only growing, but indeed appears to have surpassed mathematical optimization techniques (MOT) in several important metrics related to academic research activity (publication frequency) and commercial activity (patenting frequency). Motivated by these findings, this article discusses some of the possible origins of this growing popularity. I review different explanations for NIM popularity and discuss why some of these arguments remain unsatisfying. I argue that a compelling and comprehensive explanation should directly account for the manner in which most NIM success has actually been achieved, e.g. through hybridization and customization to different problem environments. By taking a problem lifecycle perspective, this paper offers a fresh look at the hypothesis that nature-inspired meta-heuristics derive much of their utility from being flexible. I discuss global trends within the business environments where optimization algorithms are applied and I speculate that highly flexible algorithm frameworks could become increasingly popular within our diverse and rapidly changing world.\n    ",
        "submission_date": "2009-07-02T00:00:00",
        "last_modified_date": "2011-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.0334",
        "title": "The Self-Organization of Interaction Networks for Nature-Inspired Optimization",
        "authors": [
            "James M. Whitacre",
            "Ruhul A. Sarker",
            "Q. Tuan Pham"
        ],
        "abstract": "  Over the last decade, significant progress has been made in understanding complex biological systems, however there have been few attempts at incorporating this knowledge into nature inspired optimization algorithms. In this paper, we present a first attempt at incorporating some of the basic structural properties of complex biological systems which are believed to be necessary preconditions for system qualities such as robustness. In particular, we focus on two important conditions missing in Evolutionary Algorithm populations; a self-organized definition of locality and interaction epistasis. We demonstrate that these two features, when combined, provide algorithm behaviors not observed in the canonical Evolutionary Algorithm or in Evolutionary Algorithms with structured populations such as the Cellular Genetic Algorithm. The most noticeable change in algorithm behavior is an unprecedented capacity for sustainable coexistence of genetically distinct individuals within a single population. This capacity for sustained genetic diversity is not imposed on the population but instead emerges as a natural consequence of the dynamics of the system.\n    ",
        "submission_date": "2009-07-02T00:00:00",
        "last_modified_date": "2009-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.0340",
        "title": "Strategic Positioning in Tactical Scenario Planning",
        "authors": [
            "James M. Whitacre",
            "Hussein A. Abbass",
            "Ruhul Sarker",
            "Axel Bender",
            "Stephen Baker"
        ],
        "abstract": "  Capability planning problems are pervasive throughout many areas of human interest with prominent examples found in defense and security. Planning provides a unique context for optimization that has not been explored in great detail and involves a number of interesting challenges which are distinct from traditional optimization research. Planning problems demand solutions that can satisfy a number of competing objectives on multiple scales related to robustness, adaptiveness, risk, etc. The scenario method is a key approach for planning. Scenarios can be defined for long-term as well as short-term plans. This paper introduces computational scenario-based planning problems and proposes ways to accommodate strategic positioning within the tactical planning domain. We demonstrate the methodology in a resource planning problem that is solved with a multi-objective evolutionary algorithm. Our discussion and results highlight the fact that scenario-based planning is naturally framed within a multi-objective setting. However, the conflicting objectives occur on different system levels rather than within a single system alone. This paper also contends that planning problems are of vital interest in many human endeavors and that Evolutionary Computation may be well positioned for this problem domain.\n    ",
        "submission_date": "2009-07-02T00:00:00",
        "last_modified_date": "2009-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.0507",
        "title": "Spontaneous organization leads to robustness in evolutionary algorithms",
        "authors": [
            "James M. Whitacre",
            "Ruhul A. Sarker",
            "Q. Tuan Pham"
        ],
        "abstract": "  The interaction networks of biological systems are known to take on several non-random structural properties, some of which are believed to positively influence system robustness. Researchers are only starting to understand how these structural properties emerge, however suggested roles for component fitness and community development (modularity) have attracted interest from the scientific community. In this study, we apply some of these concepts to an evolutionary algorithm and spontaneously organize its population using information that the population receives as it moves over a fitness landscape. More precisely, we employ fitness and clustering based driving forces for guiding network structural dynamics, which in turn are controlled by the population dynamics of an evolutionary algorithm. To evaluate the effect this has on evolution, experiments are conducted on six engineering design problems and six artificial test functions and compared against cellular genetic algorithms and 16 other evolutionary algorithm designs. Our results indicate that a self-organizing topology evolutionary algorithm exhibits surprisingly robust search behavior with promising performance observed over short and long time scales. After a careful analysis of these results, we conclude that the coevolution between a population and its topology represents a powerful new paradigm for designing robust search heuristics.\n    ",
        "submission_date": "2009-07-03T00:00:00",
        "last_modified_date": "2011-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.0520",
        "title": "Computational Scenario-based Capability Planning",
        "authors": [
            "Hussein Abbass",
            "Axel Bender",
            "Helen Dam",
            "Stephen Baker",
            "James M Whitacre",
            "Ruhul Sarker"
        ],
        "abstract": "  Scenarios are pen-pictures of plausible futures, used for strategic planning. The aim of this investigation is to expand the horizon of scenario-based planning through computational models that are able to aid the analyst in the planning process. The investigation builds upon the advances of Information and Communication Technology (ICT) to create a novel, flexible and customizable computational capability-based planning methodology that is practical and theoretically sound. We will show how evolutionary computation, in particular evolutionary multi-objective optimization, can play a central role - both as an optimizer and as a source for innovation.\n    ",
        "submission_date": "2009-07-03T00:00:00",
        "last_modified_date": "2009-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.0592",
        "title": "Credit Assignment in Adaptive Evolutionary Algorithms",
        "authors": [
            "James M. Whitacre",
            "Tuan Q. Pham",
            "Ruhul A. Sarker"
        ],
        "abstract": "  In this paper, a new method for assigning credit to search operators is presented. Starting with the principle of optimizing search bias, search operators are selected based on an ability to create solutions that are historically linked to future generations. Using a novel framework for defining performance measurements, distributing credit for performance, and the statistical interpretation of this credit, a new adaptive method is developed and shown to outperform a variety of adaptive and non-adaptive competitors.\n    ",
        "submission_date": "2009-07-03T00:00:00",
        "last_modified_date": "2009-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.0595",
        "title": "Use of statistical outlier detection method in adaptive evolutionary algorithms",
        "authors": [
            "James M. Whitacre",
            "Tuan Q. Pham",
            "Ruhul A. Sarker"
        ],
        "abstract": "  In this paper, the issue of adapting probabilities for Evolutionary Algorithm (EA) search operators is revisited. A framework is devised for distinguishing between measurements of performance and the interpretation of those measurements for purposes of adaptation. Several examples of measurements and statistical interpretations are provided. Probability value adaptation is tested using an EA with 10 search operators against 10 test problems with results indicating that both the type of measurement and its statistical interpretation play significant roles in EA performance. We also find that selecting operators based on the prevalence of outliers rather than on average performance is able to provide considerable improvements to adaptive methods and soundly outperforms the non-adaptive case.\n    ",
        "submission_date": "2009-07-03T00:00:00",
        "last_modified_date": "2009-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.0597",
        "title": "Network Topology and Time Criticality Effects in the Modularised Fleet Mix Problem",
        "authors": [
            "James M. Whitacre",
            "Axel Bender",
            "Stephen Baker",
            "Qi Fan",
            "Ruhul A. Sarker",
            "Hussein Abbass"
        ],
        "abstract": "  In this paper, we explore the interplay between network topology and time criticality in a military logistics system. A general goal of this work (and previous work) is to evaluate land transportation requirements or, more specifically, how to design appropriate fleets of military general service vehicles that are tasked with the supply and re-supply of military units dispersed in an area of operation. The particular focus of this paper is to gain a better understanding of how the logistics environment changes when current Army vehicles with fixed transport characteristics are replaced by a new generation of modularised vehicles that can be configured task-specifically. The experimental work is conducted within a well developed strategic planning simulation environment which includes a scenario generation engine for automatically sampling supply and re-supply missions and a multi-objective meta-heuristic search algorithm (i.e. Evolutionary Algorithm) for solving the particular scheduling and routing problems. The results presented in this paper allow for a better understanding of how (and under what conditions) a modularised vehicle fleet can provide advantages over the currently implemented system.\n    ",
        "submission_date": "2009-07-03T00:00:00",
        "last_modified_date": "2009-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.0598",
        "title": "Robustness and Adaptiveness Analysis of Future Fleets",
        "authors": [
            "Slawomir Wesolkowski",
            "Michael Mazurek",
            "James M. Whitacre",
            "Axel Bender",
            "Hussein Abbass"
        ],
        "abstract": "  Making decisions about the structure of a future military fleet is a challenging task. Several issues need to be considered such as the existence of multiple competing objectives and the complexity of the operating environment. A particular challenge is posed by the various types of uncertainty that the future might hold. It is uncertain what future events might be encountered; how fleet design decisions will influence and shape the future; and how present and future decision makers will act based on available information, their personal biases regarding the importance of different objectives, and their economic preferences. In order to assist strategic decision-making, an analysis of future fleet options needs to account for conditions in which these different classes of uncertainty are exposed. It is important to understand what assumptions a particular fleet is robust to, what the fleet can readily adapt to, and what conditions present clear risks to the fleet. We call this the analysis of a fleet's strategic positioning. This paper introduces how strategic positioning can be evaluated using computer simulations. Our main aim is to introduce a framework for capturing information that can be useful to a decision maker and for defining the concepts of robustness and adaptiveness in the context of future fleet design. We demonstrate our conceptual framework using simulation studies of an air transportation fleet. We capture uncertainty by employing an explorative scenario-based approach. Each scenario represents a sampling of different future conditions, different model assumptions, and different economic preferences. Proposed changes to a fleet are then analysed based on their influence on the fleet's robustness, adaptiveness, and risk to different scenarios.\n    ",
        "submission_date": "2009-07-03T00:00:00",
        "last_modified_date": "2009-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.1012",
        "title": "Apply Local Clustering Method to Improve the Running Speed of Ant Colony Optimization",
        "authors": [
            "Chao-Yang Pang",
            "Wei Hu",
            "Xia Li",
            "Be-Qiong Hu"
        ],
        "abstract": "  Ant Colony Optimization (ACO) has time complexity O(t*m*N*N), and its typical application is to solve Traveling Salesman Problem (TSP), where t, m, and N denotes the iteration number, number of ants, number of cities respectively. Cutting down running time is one of study focuses, and one way is to decrease parameter t and N, especially N. For this focus, the following method is presented in this paper. Firstly, design a novel clustering algorithm named Special Local Clustering algorithm (SLC), then apply it to classify all cities into compact classes, where compact class is the class that all cities in this class cluster tightly in a small region. Secondly, let ACO act on every class to get a local TSP route. Thirdly, all local TSP routes are jointed to form solution. Fourthly, the inaccuracy of solution caused by clustering is eliminated. Simulation shows that the presented method improves the running speed of ACO by 200 factors at least. And this high speed is benefit from two factors. One is that class has small size and parameter N is cut down. The route length at every iteration step is convergent when ACO acts on compact class. The other factor is that, using the convergence of route length as termination criterion of ACO and parameter t is cut down.\n    ",
        "submission_date": "2009-07-06T00:00:00",
        "last_modified_date": "2009-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.1065",
        "title": "Design of an Optimal Bayesian Incentive Compatible Broadcast Protocol for Ad hoc Networks with Rational Nodes",
        "authors": [
            "Ramasuri Narayanam",
            "Y. Narahari"
        ],
        "abstract": "  Nodes in an ad hoc wireless network incur certain costs for forwarding packets since packet forwarding consumes the resources of the nodes. If the nodes are rational, free packet forwarding by the nodes cannot be taken for granted and incentive based protocols are required to stimulate cooperation among the nodes. Existing incentive based approaches are based on the VCG (Vickrey-Clarke-Groves) mechanism which leads to high levels of incentive budgets and restricted applicability to only certain topologies of networks. Moreover, the existing approaches have only focused on unicast and multicast. Motivated by this, we propose an incentive based broadcast protocol that satisfies Bayesian incentive compatibility and minimizes the incentive budgets required by the individual nodes. The proposed protocol, which we call {\\em BIC-B} (Bayesian incentive compatible broadcast) protocol, also satisfies budget balance. We also derive a necessary and sufficient condition for the ex-post individual rationality of the BIC-B protocol. The {\\em BIC-B} protocol exhibits superior performance in comparison to a dominant strategy incentive compatible broadcast protocol.\n    ",
        "submission_date": "2009-07-06T00:00:00",
        "last_modified_date": "2009-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.1245",
        "title": "How Controlled English can Improve Semantic Wikis",
        "authors": [
            "Tobias Kuhn"
        ],
        "abstract": "  The motivation of semantic wikis is to make acquisition, maintenance, and mining of formal knowledge simpler, faster, and more flexible. However, most existing semantic wikis have a very technical interface and are restricted to a relatively low level of expressivity. In this paper, we explain how AceWiki uses controlled English - concretely Attempto Controlled English (ACE) - to provide a natural and intuitive interface while supporting a high degree of expressivity. We introduce recent improvements of the AceWiki system and user studies that indicate that AceWiki is usable and useful.\n    ",
        "submission_date": "2009-07-07T00:00:00",
        "last_modified_date": "2009-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.1925",
        "title": "Modeling self-organizing traffic lights with elementary cellular automata",
        "authors": [
            "Carlos Gershenson",
            "David A. Rosenblueth"
        ],
        "abstract": "  There have been several highway traffic models proposed based on cellular automata. The simplest one is elementary cellular automaton rule 184. We extend this model to city traffic with cellular automata coupled at intersections using only rules 184, 252, and 136. The simplicity of the model offers a clear understanding of the main properties of city traffic and its phase transitions.\n",
        "submission_date": "2009-07-10T00:00:00",
        "last_modified_date": "2009-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.3099",
        "title": "Graph Theory and Optimization Problems for Very Large Networks",
        "authors": [
            "Kamal Ahmat"
        ],
        "abstract": "  Graph theory provides a primary tool for analyzing and designing computer communication networks. In the past few decades, Graph theory has been used to study various types of networks, including the Internet, wide Area Networks, Local Area Networks, and networking protocols such as border Gateway Protocol, Open shortest Path Protocol, and Networking Networks. In this paper, we present some key graph theory concepts used to represent different types of networks. Then we describe how networks are modeled to investigate problems related to network protocols. Finally, we present some of the tools used to generate graph for representing practical networks.\n    ",
        "submission_date": "2009-07-17T00:00:00",
        "last_modified_date": "2009-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.3220",
        "title": "Inter Genre Similarity Modelling For Automatic Music Genre Classification",
        "authors": [
            "Ulas Bagci",
            "Engin Erzin"
        ],
        "abstract": "  Music genre classification is an essential tool for music information retrieval systems and it has been finding critical applications in various media platforms. Two important problems of the automatic music genre classification are feature extraction and classifier design. This paper investigates inter-genre similarity modelling (IGS) to improve the performance of automatic music genre classification. Inter-genre similarity information is extracted over the mis-classified feature population. Once the inter-genre similarity is modelled, elimination of the inter-genre similarity reduces the inter-genre confusion and improves the identification rates. Inter-genre similarity modelling is further improved with iterative IGS modelling(IIGS) and score modelling for IGS elimination(SMIGS). Experimental results with promising classification improvements are provided.\n    ",
        "submission_date": "2009-07-18T00:00:00",
        "last_modified_date": "2009-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.3819",
        "title": "Self-adaptive web intrusion detection system",
        "authors": [
            "Thomas Guyet",
            "Ren\u00e9 Quiniou",
            "Wei Wang",
            "Marie-Odile Cordier"
        ],
        "abstract": "  The evolution of the web server contents and the emergence of new kinds of intrusions make necessary the adaptation of the intrusion detection systems (IDS). Nowadays, the adaptation of the IDS requires manual -- tedious and unreactive -- actions from system administrators. In this paper, we present a self-adaptive intrusion detection system which relies on a set of local model-based diagnosers. The redundancy of diagnoses is exploited, online, by a meta-diagnoser to check the consistency of computed partial diagnoses, and to trigger the adaptation of defective diagnoser models (or signatures) in case of inconsistency. This system is applied to the intrusion detection from a stream of HTTP requests. Our results show that our system 1) detects intrusion occurrences sensitively and precisely, 2) accurately self-adapts diagnoser model, thus improving its detection accuracy.\n    ",
        "submission_date": "2009-07-22T00:00:00",
        "last_modified_date": "2009-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.4385",
        "title": "The Cost of Stability in Coalitional Games",
        "authors": [
            "Yoram Bachrach",
            "Edith Elkind",
            "Reshef Meir",
            "Dmitrii Pasechnik",
            "Michael Zuckerman",
            "Joerg Rothe",
            "Jeffrey S. Rosenschein"
        ],
        "abstract": "  A key question in cooperative game theory is that of coalitional stability, usually captured by the notion of the \\emph{core}--the set of outcomes such that no subgroup of players has an incentive to deviate. However, some coalitional games have empty cores, and any outcome in such a game is unstable.\n",
        "submission_date": "2009-07-24T00:00:00",
        "last_modified_date": "2009-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0907.4447",
        "title": "Graphical Probabilistic Routing Model for OBS Networks with Realistic Traffic Scenario",
        "authors": [
            "Martin Levesque",
            "Halima Elbiaze"
        ],
        "abstract": "  Burst contention is a well-known challenging problem in Optical Burst Switching (OBS) networks. Contention resolution approaches are always reactive and attempt to minimize the BLR based on local information available at the core node. On the other hand, a proactive approach that avoids burst losses before they occur is desirable. To reduce the probability of burst contention, a more robust routing algorithm than the shortest path is needed. This paper proposes a new routing mechanism for JET-based OBS networks, called Graphical Probabilistic Routing Model (GPRM) that selects less utilized links, on a hop-by-hop basis by using a bayesian network. We assume no wavelength conversion and no buffering to be available at the core nodes of the OBS network. We simulate the proposed approach under dynamic load to demonstrate that it reduces the Burst Loss Ratio (BLR) compared to static approaches by using Network Simulator 2 (ns-2) on NSFnet network topology and with realistic traffic matrix. Simulation results clearly show that the proposed approach outperforms static approaches in terms of BLR.\n    ",
        "submission_date": "2009-07-25T00:00:00",
        "last_modified_date": "2009-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0908.0319",
        "title": "Regret Bounds for Opportunistic Channel Access",
        "authors": [
            "Sarah Filippi",
            "Olivier Capp\u00e9",
            "Aur\u00e9lien Garivier"
        ],
        "abstract": "  We consider the task of opportunistic channel access in a primary system composed of independent Gilbert-Elliot channels where the secondary (or opportunistic) user does not dispose of a priori information regarding the statistical characteristics of the system. It is shown that this problem may be cast into the framework of model-based learning in a specific class of Partially Observed Markov Decision Processes (POMDPs) for which we introduce an algorithm aimed at striking an optimal tradeoff between the exploration (or estimation) and exploitation requirements. We provide finite horizon regret bounds for this algorithm as well as a numerical evaluation of its performance in the single channel model as well as in the case of stochastically identical channels.\n    ",
        "submission_date": "2009-08-03T00:00:00",
        "last_modified_date": "2009-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0908.3148",
        "title": "Another Look at Quantum Neural Computing",
        "authors": [
            "Subhash Kak"
        ],
        "abstract": "The term quantum neural computing indicates a unity in the functioning of the brain. It assumes that the neural structures perform classical processing and that the virtual particles associated with the dynamical states of the structures define the underlying quantum state. We revisit the concept and also summarize new arguments related to the learning modes of the brain in response to sensory input that may be aggregated in three types: associative, reorganizational, and quantum. The associative and reorganizational types are quite apparent based on experimental findings; it is much harder to establish that the brain as an entity exhibits quantum properties. We argue that the reorganizational behavior of the brain may be viewed as inner adjustment corresponding to its quantum behavior at the system level. Not only neural structures but their higher abstractions also may be seen as whole entities. We consider the dualities associated with the behavior of the brain and how these dualities are bridged.\n    ",
        "submission_date": "2009-08-21T00:00:00",
        "last_modified_date": "2013-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0908.3162",
        "title": "Practical approach to programmable analog circuits with memristors",
        "authors": [
            "Yuriy V. Pershin",
            "Massimiliano Di Ventra"
        ],
        "abstract": "  We suggest an approach to use memristors (resistors with memory) in programmable analog circuits. Our idea consists in a circuit design in which low voltages are applied to memristors during their operation as analog circuit elements and high voltages are used to program the memristor's states. This way, as it was demonstrated in recent experiments, the state of memristors does not essentially change during analog mode operation. As an example of our approach, we have built several programmable analog circuits demonstrating memristor-based programming of threshold, gain and frequency.\n    ",
        "submission_date": "2009-08-21T00:00:00",
        "last_modified_date": "2010-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0908.3212",
        "title": "Quantifying Rational Belief",
        "authors": [
            "Ariel Caticha"
        ],
        "abstract": "  Some criticisms that have been raised against the Cox approach to probability theory are addressed. Should we use a single real number to measure a degree of rational belief? Can beliefs be compared? Are the Cox axioms obvious? Are there counterexamples to Cox? Rather than justifying Cox's choice of axioms we follow a different path and derive the sum and product rules of probability theory as the unique (up to regraduations) consistent representations of the Boolean AND and OR operations.\n    ",
        "submission_date": "2009-08-21T00:00:00",
        "last_modified_date": "2009-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0908.3633",
        "title": "Maximizing profit using recommender systems",
        "authors": [
            "Aparna Das",
            "Claire Mathieu",
            "Daniel Ricketts"
        ],
        "abstract": "  Traditional recommendation systems make recommendations based solely on the customer's past purchases, product ratings and demographic data without considering the profitability the items being recommended. In this work we study the question of how a vendor can directly incorporate the profitability of items into its recommender so as to maximize its expected profit while still providing accurate recommendations. Our approach uses the output of any traditional recommender system and adjust them according to item profitabilities. Our approach is parameterized so the vendor can control how much the recommendation incorporating profits can deviate from the traditional recommendation. We study our approach under two settings and show that it achieves approximately 22% more profit than traditional recommendations.\n    ",
        "submission_date": "2009-08-25T00:00:00",
        "last_modified_date": "2009-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0908.4144",
        "title": "ABC-LogitBoost for Multi-class Classification",
        "authors": [
            "Ping Li"
        ],
        "abstract": "  We develop abc-logitboost, based on the prior work on abc-boost and robust logitboost. Our extensive experiments on a variety of datasets demonstrate the considerable improvement of abc-logitboost over logitboost and abc-mart.\n    ",
        "submission_date": "2009-08-28T00:00:00",
        "last_modified_date": "2009-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.1186",
        "title": "Scheme of thinking quantum systems",
        "authors": [
            "V.I. Yukalov",
            "D. Sornette"
        ],
        "abstract": "  A general approach describing quantum decision procedures is developed. The approach can be applied to quantum information processing, quantum computing, creation of artificial quantum intelligence, as well as to analyzing decision processes of human decision makers. Our basic point is to consider an active quantum system possessing its own strategic state. Processing information by such a system is analogous to the cognitive processes associated to decision making by humans. The algebra of probability operators, associated with the possible options available to the decision maker, plays the role of the algebra of observables in quantum theory of measurements. A scheme is advanced for a practical realization of decision procedures by thinking quantum systems. Such thinking quantum systems can be realized by using spin lattices, systems of magnetic molecules, cold atoms trapped in optical lattices, ensembles of quantum dots, or multilevel atomic systems interacting with electromagnetic field.\n    ",
        "submission_date": "2009-09-07T00:00:00",
        "last_modified_date": "2009-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.1334",
        "title": "Lower Bounds for BMRM and Faster Rates for Training SVMs",
        "authors": [
            "Ankan Saha",
            "Xinhua Zhang",
            "S.V.N. Vishwanathan"
        ],
        "abstract": "  Regularized risk minimization with the binary hinge loss and its variants lies at the heart of many machine learning problems. Bundle methods for regularized risk minimization (BMRM) and the closely related SVMStruct are considered the best general purpose solvers to tackle this problem. It was recently shown that BMRM requires $O(1/\\epsilon)$ iterations to converge to an $\\epsilon$ accurate solution. In the first part of the paper we use the Hadamard matrix to construct a regularized risk minimization problem and show that these rates cannot be improved. We then show how one can exploit the structure of the objective function to devise an algorithm for the binary hinge loss which converges to an $\\epsilon$ accurate solution in $O(1/\\sqrt{\\epsilon})$ iterations.\n    ",
        "submission_date": "2009-09-07T00:00:00",
        "last_modified_date": "2009-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.1397",
        "title": "Resource Matchmaking Algorithm using Dynamic Rough Set in Grid Environment",
        "authors": [
            "Iraj Ataollahi",
            "Mortza Analoui"
        ],
        "abstract": "  Grid environment is a service oriented infrastructure in which many heterogeneous resources participate to provide the high performance computation. One of the bug issues in the grid environment is the vagueness and uncertainty between advertised resources and requested resources. Furthermore, in an environment such as grid dynamicity is considered as a crucial issue which must be dealt with. Classical rough set have been used to deal with the uncertainty and vagueness. But it can just be used on the static systems and can not support dynamicity in a system. In this work we propose a solution, called Dynamic Rough Set Resource Discovery (DRSRD), for dealing with cases of vagueness and uncertainty problems based on Dynamic rough set theory which considers dynamic features in this environment. In this way, requested resource properties have a weight as priority according to which resource matchmaking and ranking process is done. We also report the result of the solution obtained from the simulation in GridSim simulator. The comparison has been made between DRSRD, classical rough set theory based algorithm, and UDDI and OWL S combined algorithm. DRSRD shows much better precision for the cases with vagueness and uncertainty in a dynamic system such as the grid rather than the classical rough set theory based algorithm, and UDDI and OWL S combined algorithm.\n    ",
        "submission_date": "2009-09-08T00:00:00",
        "last_modified_date": "2009-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.1769",
        "title": "Interactive Data Integration through Smart Copy & Paste",
        "authors": [
            "Zachary Ives",
            "Craig Knoblock",
            "Steve Minton",
            "Marie Jacob",
            "Partha Talukdar",
            "Rattapoom Tuchinda",
            "Jose Luis Ambite",
            "Maria Muslea",
            "Cenk Gazen"
        ],
        "abstract": "  In many scenarios, such as emergency response or ad hoc collaboration, it is critical to reduce the overhead in integrating data. Ideally, one could perform the entire process interactively under one unified interface: defining extractors and wrappers for sources, creating a mediated schema, and adding schema mappings ? while seeing how these impact the integrated view of the data, and refining the design accordingly.\n",
        "submission_date": "2009-09-09T00:00:00",
        "last_modified_date": "2009-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.1830",
        "title": "Greedy Gossip with Eavesdropping",
        "authors": [
            "Deniz Ustebay",
            "Boris Oreshkin",
            "Mark Coates",
            "Michael Rabbat"
        ],
        "abstract": "  This paper presents greedy gossip with eavesdropping (GGE), a novel randomized gossip algorithm for distributed computation of the average consensus problem. In gossip algorithms, nodes in the network randomly communicate with their neighbors and exchange information iteratively. The algorithms are simple and decentralized, making them attractive for wireless network applications. In general, gossip algorithms are robust to unreliable wireless conditions and time varying network topologies. In this paper we introduce GGE and demonstrate that greedy updates lead to rapid convergence. We do not require nodes to have any location information. Instead, greedy updates are made possible by exploiting the broadcast nature of wireless communications. During the operation of GGE, when a node decides to gossip, instead of choosing one of its neighbors at random, it makes a greedy selection, choosing the node which has the value most different from its own. In order to make this selection, nodes need to know their neighbors' values. Therefore, we assume that all transmissions are wireless broadcasts and nodes keep track of their neighbors' values by eavesdropping on their communications. We show that the convergence of GGE is guaranteed for connected network topologies. We also study the rates of convergence and illustrate, through theoretical bounds and numerical simulations, that GGE consistently outperforms randomized gossip and performs comparably to geographic gossip on moderate-sized random geometric graph topologies.\n    ",
        "submission_date": "2009-09-09T00:00:00",
        "last_modified_date": "2009-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.2934",
        "title": "A Convergent Online Single Time Scale Actor Critic Algorithm",
        "authors": [
            "D. Di Castro",
            "R. Meir"
        ],
        "abstract": "  Actor-Critic based approaches were among the first to address reinforcement learning in a general setting. Recently, these algorithms have gained renewed interest due to their generality, good convergence properties, and possible biological relevance. In this paper, we introduce an online temporal difference based actor-critic algorithm which is proved to converge to a neighborhood of a local maximum of the average reward. Linear function approximation is used by the critic in order estimate the value function, and the temporal difference signal, which is passed from the critic to the actor. The main distinguishing feature of the present convergence proof is that both the actor and the critic operate on a similar time scale, while in most current convergence proofs they are required to have very different time scales in order to converge. Moreover, the same temporal difference signal is used to update the parameters of both the actor and the critic. A limitation of the proposed approach, compared to results available for two time scale convergence, is that convergence is guaranteed only to a neighborhood of an optimal value, rather to an optimal value itself. The single time scale and identical temporal difference signal used by the actor and the critic, may provide a step towards constructing more biologically realistic models of reinforcement learning in the brain.\n    ",
        "submission_date": "2009-09-16T00:00:00",
        "last_modified_date": "2009-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.3593",
        "title": "Exploiting Unlabeled Data to Enhance Ensemble Diversity",
        "authors": [
            "Min-Ling Zhang",
            "Zhi-Hua Zhou"
        ],
        "abstract": "Ensemble learning aims to improve generalization ability by using multiple base learners. It is well-known that to construct a good ensemble, the base learners should be accurate as well as diverse. In this paper, unlabeled data is exploited to facilitate ensemble learning by helping augment the diversity among the base learners. Specifically, a semi-supervised ensemble method named UDEED is proposed. Unlike existing semi-supervised ensemble methods where error-prone pseudo-labels are estimated for unlabeled data to enlarge the labeled data to improve accuracy, UDEED works by maximizing accuracies of base learners on labeled data while maximizing diversity among them on unlabeled data. Experiments show that UDEED can effectively utilize unlabeled data for ensemble learning and is highly competitive to well-established semi-supervised ensemble methods.\n    ",
        "submission_date": "2009-09-19T00:00:00",
        "last_modified_date": "2010-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.4889",
        "title": "Hybrid Intrusion Detection and Prediction multiAgent System HIDPAS",
        "authors": [
            "Farah Jemili",
            "Montaceur Zaghdoud",
            "Mohamed Ben Ahmed"
        ],
        "abstract": "  This paper proposes an intrusion detection and prediction system based on uncertain and imprecise inference networks and its implementation. Giving a historic of sessions, it is about proposing a method of supervised learning doubled of a classifier permitting to extract the necessary knowledge in order to identify the presence or not of an intrusion in a session and in the positive case to recognize its type and to predict the possible intrusions that will follow it. The proposed system takes into account the uncertainty and imprecision that can affect the statistical data of the historic. The systematic utilization of an unique probability distribution to represent this type of knowledge supposes a too rich subjective information and risk to be in part arbitrary. One of the first objectives of this work was therefore to permit the consistency between the manner of which we represent information and information which we really dispose.\n    ",
        "submission_date": "2009-09-26T00:00:00",
        "last_modified_date": "2009-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0909.5097",
        "title": "On the Scope of the Universal-Algebraic Approach to Constraint Satisfaction",
        "authors": [
            "Barnaby Martin",
            "Manuel Bodirsky",
            "Martin Hils"
        ],
        "abstract": " The universal-algebraic approach has proved a powerful tool in the study of the complexity of CSPs. This approach has previously been applied to the study of CSPs with finite or (infinite) omega-categorical templates, and relies on two facts. The first is that in finite or omega-categorical structures A, a relation is primitive positive definable if and only if it is preserved by the polymorphisms of A. The second is that every finite or omega-categorical structure is homomorphically equivalent to a core structure. In this paper, we present generalizations of these facts to infinite structures that are not necessarily omega-categorical. (This abstract has been severely curtailed by the space constraints of arXiv -- please read the full abstract in the article.) Finally, we present applications of our general results to the description and analysis of the complexity of CSPs. In particular, we give general hardness criteria based on the absence of polymorphisms that depend on more than one argument, and we present a polymorphism-based description of those CSPs that are first-order definable (and therefore can be solved in polynomial time). \n    ",
        "submission_date": "2009-09-28T00:00:00",
        "last_modified_date": "2013-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.0013",
        "title": "Algorithms for finding dispensable variables",
        "authors": [
            "Mikolas Janota",
            "Joao Marques-Silva",
            "Radu Grigore"
        ],
        "abstract": "  This short note reviews briefly three algorithms for finding the set of dispensable variables of a boolean formula. The presentation is light on proofs and heavy on intuitions.\n    ",
        "submission_date": "2009-09-30T00:00:00",
        "last_modified_date": "2009-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.0902",
        "title": "Reduced-Rank Hidden Markov Models",
        "authors": [
            "Sajid M. Siddiqi",
            "Byron Boots",
            "Geoffrey J. Gordon"
        ],
        "abstract": "  We introduce the Reduced-Rank Hidden Markov Model (RR-HMM), a generalization of HMMs that can model smooth state evolution as in Linear Dynamical Systems (LDSs) as well as non-log-concave predictive distributions as in continuous-observation HMMs. RR-HMMs assume an m-dimensional latent state and n discrete observations, with a transition matrix of rank k <= m. This implies the dynamics evolve in a k-dimensional subspace, while the shape of the set of predictive distributions is determined by m. Latent state belief is represented with a k-dimensional state vector and inference is carried out entirely in R^k, making RR-HMMs as computationally efficient as k-state HMMs yet more expressive. To learn RR-HMMs, we relax the assumptions of a recently proposed spectral learning algorithm for HMMs (Hsu, Kakade and Zhang 2009) and apply it to learn k-dimensional observable representations of rank-k RR-HMMs. The algorithm is consistent and free of local optima, and we extend its performance guarantees to cover the RR-HMM case. We show how this algorithm can be used in conjunction with a kernel density estimator to efficiently model high-dimensional multivariate continuous data. We also relax the assumption that single observations are sufficient to disambiguate state, and extend the algorithm accordingly. Experiments on synthetic data and a toy video, as well as on a difficult robot vision modeling problem, yield accurate models that compare favorably with standard alternatives in simulation quality and prediction capability.\n    ",
        "submission_date": "2009-10-06T00:00:00",
        "last_modified_date": "2009-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.2276",
        "title": "State of the Art Review for Applying Computational Intelligence and Machine Learning Techniques to Portfolio Optimisation",
        "authors": [
            "Evan Hurwitz",
            "Tshilidzi Marwala"
        ],
        "abstract": "  Computational techniques have shown much promise in the field of Finance, owing to their ability to extract sense out of dauntingly complex systems. This paper reviews the most promising of these techniques, from traditional computational intelligence methods to their machine learning siblings, with particular view to their application in optimising the management of a portfolio of financial instruments. The current state of the art is assessed, and prospective further work is assessed and recommended\n    ",
        "submission_date": "2009-10-13T00:00:00",
        "last_modified_date": "2009-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.3348",
        "title": "Algorithms for Image Analysis and Combination of Pattern Classifiers with Application to Medical Diagnosis",
        "authors": [
            "Harris Georgiou"
        ],
        "abstract": "  Medical Informatics and the application of modern signal processing in the assistance of the diagnostic process in medical imaging is one of the more recent and active research areas today. This thesis addresses a variety of issues related to the general problem of medical image analysis, specifically in mammography, and presents a series of algorithms and design approaches for all the intermediate levels of a modern system for computer-aided diagnosis (CAD). The diagnostic problem is analyzed with a systematic approach, first defining the imaging characteristics and features that are relevant to probable pathology in mammo-grams. Next, these features are quantified and fused into new, integrated radio-logical systems that exhibit embedded digital signal processing, in order to improve the final result and minimize the radiological dose for the patient. In a higher level, special algorithms are designed for detecting and encoding these clinically interest-ing imaging features, in order to be used as input to advanced pattern classifiers and machine learning models. Finally, these approaches are extended in multi-classifier models under the scope of Game Theory and optimum collective deci-sion, in order to produce efficient solutions for combining classifiers with minimum computational costs for advanced diagnostic systems. The material covered in this thesis is related to a total of 18 published papers, 6 in scientific journals and 12 in international conferences.\n    ",
        "submission_date": "2009-10-18T00:00:00",
        "last_modified_date": "2009-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.3913",
        "title": "How to Complete an Interactive Configuration Process?",
        "authors": [
            "Mikolas Janota",
            "Goetz Botterweck",
            "Radu Grigore",
            "Joao Marques-Silva"
        ],
        "abstract": "  When configuring customizable software, it is useful to provide interactive tool-support that ensures that the configuration does not breach given constraints.\n",
        "submission_date": "2009-10-20T00:00:00",
        "last_modified_date": "2009-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.4116",
        "title": "Swarm Intelligence",
        "authors": [
            "Sabu M. Thampi"
        ],
        "abstract": "  Biologically inspired computing is an area of computer science which uses the advantageous properties of biological systems. It is the amalgamation of computational intelligence and collective intelligence. Biologically inspired mechanisms have already proved successful in achieving major advances in a wide range of problems in computing and communication systems. The consortium of bio-inspired computing are artificial neural networks, evolutionary algorithms, swarm intelligence, artificial immune systems, fractal geometry, DNA computing and quantum computing, etc. This article gives an introduction to swarm intelligence.\n    ",
        "submission_date": "2009-10-21T00:00:00",
        "last_modified_date": "2009-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.4699",
        "title": "Sum of Us: Strategyproof Selection from the Selectors",
        "authors": [
            "Noga Alon",
            "Felix Fischer",
            "Ariel D. Procaccia",
            "Moshe Tennenholtz"
        ],
        "abstract": "  We consider directed graphs over a set of n agents, where an edge (i,j) is taken to mean that agent i supports or trusts agent j. Given such a graph and an integer k\\leq n, we wish to select a subset of k agents that maximizes the sum of indegrees, i.e., a subset of k most popular or most trusted agents. At the same time we assume that each individual agent is only interested in being selected, and may misreport its outgoing edges to this end. This problem formulation captures realistic scenarios where agents choose among themselves, which can be found in the context of Internet search, social networks like Twitter, or reputation systems like Epinions.\n",
        "submission_date": "2009-10-25T00:00:00",
        "last_modified_date": "2009-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.5410",
        "title": "The Uned systems at Senseval-2",
        "authors": [
            "David Fernandez-Amoros",
            "Julio Gonzalo",
            "Felisa Verdejo"
        ],
        "abstract": "  We have participated in the SENSEVAL-2 English tasks (all words and lexical sample) with an unsupervised system based on mutual information measured over a large corpus (277 million words) and some additional heuristics. A supervised extension of the system was also presented to the lexical sample task.\n",
        "submission_date": "2009-10-28T00:00:00",
        "last_modified_date": "2009-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.5419",
        "title": "Word Sense Disambiguation Based on Mutual Information and Syntactic Patterns",
        "authors": [
            "David Fernandez-Amoros"
        ],
        "abstract": "  This paper describes a hybrid system for WSD, presented to the English all-words and lexical-sample tasks, that relies on two different unsupervised approaches. The first one selects the senses according to mutual information proximity between a context word a variant of the sense. The second heuristic analyzes the examples of use in the glosses of the senses so that simple syntactic patterns are inferred. This patterns are matched against the disambiguation contexts. We show that the first heuristic obtains a precision and recall of .58 and .35 respectively in the all words task while the second obtains .80 and .25. The high precision obtained recommends deeper research of the techniques. Results for the lexical sample task are also provided.\n    ",
        "submission_date": "2009-10-28T00:00:00",
        "last_modified_date": "2009-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.5542",
        "title": "Forced Evolution in Silico by Artificial Transposons and their Genetic Operators: The John Muir Ant Problem",
        "authors": [
            "Alexander V. Spirov",
            "Alexander B. Kazansky",
            "Leonid Zamdborg",
            "Juan J. Merelo",
            "Vladimir F. Levchenko"
        ],
        "abstract": "  Modern evolutionary computation utilizes heuristic optimizations based upon concepts borrowed from the Darwinian theory of natural selection. We believe that a vital direction in this field must be algorithms that model the activity of genomic parasites, such as transposons, in biological evolution. This publication is our first step in the direction of developing a minimal assortment of algorithms that simulate the role of genomic parasites. Specifically, we started in the domain of genetic algorithms (GA) and selected the Artificial Ant Problem as a test case. We define these artificial transposons as a fragment of an ant's code that possesses properties that cause it to stand apart from the rest. We concluded that artificial transposons, analogous to real transposons, are truly capable of acting as intelligent mutators that adapt in response to an evolutionary problem in the course of co-evolution with their hosts.\n    ",
        "submission_date": "2009-10-29T00:00:00",
        "last_modified_date": "2009-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0910.5682",
        "title": "Word Sense Disambiguation Using English-Spanish Aligned Phrases over Comparable Corpora",
        "authors": [
            "David Fernandez-Amoros"
        ],
        "abstract": "  In this paper we describe a WSD experiment based on bilingual English-Spanish comparable corpora in which individual noun phrases have been identified and aligned with their respective counterparts in the other language. The evaluation of the experiment has been carried out against SemCor.\n",
        "submission_date": "2009-10-29T00:00:00",
        "last_modified_date": "2009-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.0460",
        "title": "Feature-Weighted Linear Stacking",
        "authors": [
            "Joseph Sill",
            "Gabor Takacs",
            "Lester Mackey",
            "David Lin"
        ],
        "abstract": "  Ensemble methods, such as stacking, are designed to boost predictive accuracy by blending the predictions of multiple machine learning models. Recent work has shown that the use of meta-features, additional inputs describing each example in a dataset, can boost the performance of ensemble methods, but the greatest reported gains have come from nonlinear procedures requiring significant tuning and training time. Here, we present a linear technique, Feature-Weighted Linear Stacking (FWLS), that incorporates meta-features for improved accuracy while retaining the well-known virtues of linear regression regarding speed, stability, and interpretability. FWLS combines model predictions linearly using coefficients that are themselves linear functions of meta-features. This technique was a key facet of the solution of the second place team in the recently concluded Netflix Prize competition. Significant increases in accuracy over standard linear stacking are demonstrated on the Netflix Prize collaborative filtering dataset.\n    ",
        "submission_date": "2009-11-03T00:00:00",
        "last_modified_date": "2009-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.1678",
        "title": "Industrial-Strength Formally Certified SAT Solving",
        "authors": [
            "Ashish Darbari",
            "Bernd Fischer",
            "Joao Marques-Silva"
        ],
        "abstract": "  Boolean Satisfiability (SAT) solvers are now routinely used in the verification of large industrial problems. However, their application in safety-critical domains such as the railways, avionics, and automotive industries requires some form of assurance for the results, as the solvers can (and sometimes do) have bugs. Unfortunately, the complexity of modern, highly optimized SAT solvers renders impractical the development of direct formal proofs of their correctness. This paper presents an alternative approach where an untrusted, industrial-strength, SAT solver is plugged into a trusted, formally certified, SAT proof checker to provide industrial-strength certified SAT solving. The key novelties and characteristics of our approach are (i) that the checker is automatically extracted from the formal development, (ii), that the combined system can be used as a standalone executable program independent of any supporting theorem prover, and (iii) that the checker certifies any SAT solver respecting the agreed format for satisfiability and unsatisfiability claims. The core of the system is a certified checker for unsatisfiability claims that is formally designed and verified in Coq. We present its formal design and outline the correctness proofs. The actual standalone checker is automatically extracted from the the Coq development. An evaluation of the certified checker on a representative set of industrial benchmarks from the SAT Race Competition shows that, albeit it is slower than uncertified SAT checkers, it is significantly faster than certified checkers implemented on top of an interactive theorem prover.\n    ",
        "submission_date": "2009-11-09T00:00:00",
        "last_modified_date": "2009-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.1965",
        "title": "Active Learning for Mention Detection: A Comparison of Sentence Selection Strategies",
        "authors": [
            "Nitin Madnani",
            "Hongyan Jing",
            "Nanda Kambhatla",
            "Salim Roukos"
        ],
        "abstract": "  We propose and compare various sentence selection strategies for active learning for the task of detecting mentions of entities. The best strategy employs the sum of confidences of two statistical classifiers trained on different views of the data. Our experimental results show that, compared to the random selection strategy, this strategy reduces the amount of required labeled training data by over 50% while achieving the same performance. The effect is even more significant when only named mentions are considered: the system achieves the same performance by using only 42% of the training data required by the random selection strategy.\n    ",
        "submission_date": "2009-11-10T00:00:00",
        "last_modified_date": "2009-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.2829",
        "title": "Proceedings Fifth Workshop on Developments in Computational Models--Computational Models From Nature",
        "authors": [
            "S. Barry Cooper",
            "Vincent Danos"
        ],
        "abstract": "  The special theme of DCM 2009, co-located with ICALP 2009, concerned Computational Models From Nature, with a particular emphasis on computational models derived from physics and biology. The intention was to bring together different approaches - in a community with a strong foundational background as proffered by the ICALP attendees - to create inspirational cross-boundary exchanges, and to lead to innovative further research. Specifically DCM 2009 sought contributions in quantum computation and information, probabilistic models, chemical, biological and bio-inspired ones, including spatial models, growth models and models of self-assembly. Contributions putting to the test logical or algorithmic aspects of computing (e.g., continuous computing with dynamical systems, or solid state computing models) were also very much welcomed.\n    ",
        "submission_date": "2009-11-15T00:00:00",
        "last_modified_date": "2009-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.2865",
        "title": "Neural Networks for Dynamic Shortest Path Routing Problems - A Survey",
        "authors": [
            "R. Nallusamy",
            "K. Duraiswamy"
        ],
        "abstract": "  This paper reviews the overview of the dynamic shortest path routing problem and the various neural networks to solve it. Different shortest path optimization problems can be solved by using various neural networks algorithms. The routing in packet switched multi-hop networks can be described as a classical combinatorial optimization problem i.e. a shortest path routing problem in graphs. The survey shows that the neural networks are the best candidates for the optimization of dynamic shortest path routing problems due to their fastness in computation comparing to other softcomputing and metaheuristics algorithms\n    ",
        "submission_date": "2009-11-15T00:00:00",
        "last_modified_date": "2010-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.4727",
        "title": "Exchangeability and sets of desirable gambles",
        "authors": [
            "Gert de Cooman",
            "Erik Quaeghebeur"
        ],
        "abstract": "Sets of desirable gambles constitute a quite general type of uncertainty model with an interesting geometrical interpretation. We give a general discussion of such models and their rationality criteria. We study exchangeability assessments for them, and prove counterparts of de Finetti's finite and infinite representation theorems. We show that the finite representation in terms of count vectors has a very nice geometrical interpretation, and that the representation in terms of frequency vectors is tied up with multivariate Bernstein (basis) polynomials. We also lay bare the relationships between the representations of updated exchangeable models, and discuss conservative inference (natural extension) under exchangeability and the extension of exchangeable sequences.\n    ",
        "submission_date": "2009-11-24T00:00:00",
        "last_modified_date": "2010-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.5372",
        "title": "Maximin affinity learning of image segmentation",
        "authors": [
            "Srinivas C. Turaga",
            "Kevin L. Briggman",
            "Moritz Helmstaedter",
            "Winfried Denk",
            "H. Sebastian Seung"
        ],
        "abstract": "  Images can be segmented by first using a classifier to predict an affinity graph that reflects the degree to which image pixels must be grouped together and then partitioning the graph to yield a segmentation. Machine learning has been applied to the affinity classifier to produce affinity graphs that are good in the sense of minimizing edge misclassification rates. However, this error measure is only indirectly related to the quality of segmentations produced by ultimately partitioning the affinity graph. We present the first machine learning algorithm for training a classifier to produce affinity graphs that are good in the sense of producing segmentations that directly minimize the Rand index, a well known segmentation performance measure. The Rand index measures segmentation performance by quantifying the classification of the connectivity of image pixel pairs after segmentation. By using the simple graph partitioning algorithm of finding the connected components of the thresholded affinity graph, we are able to train an affinity classifier to directly minimize the Rand index of segmentations resulting from the graph partitioning. Our learning algorithm corresponds to the learning of maximin affinities between image pixel pairs, which are predictive of the pixel-pair connectivity.\n    ",
        "submission_date": "2009-11-28T00:00:00",
        "last_modified_date": "2009-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.5548",
        "title": "A Decision-Optimization Approach to Quantum Mechanics and Game Theory",
        "authors": [
            "Xiaofei Huang"
        ],
        "abstract": "  The fundamental laws of quantum world upsets the logical foundation of classic physics. They are completely counter-intuitive with many bizarre behaviors. However, this paper shows that they may make sense from the perspective of a general decision-optimization principle for cooperation. This principle also offers a generalization of Nash equilibrium, a key concept in game theory, for better payoffs and stability of game playing.\n    ",
        "submission_date": "2009-11-30T00:00:00",
        "last_modified_date": "2009-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0911.5568",
        "title": "Acquisition d'informations lexicales \u00e0 partir de corpus C\u00e9dric Messiant et Thierry Poibeau",
        "authors": [
            "C\u00e9dric Messiant",
            "Thierry Poibeau"
        ],
        "abstract": "  This paper is about automatic acquisition of lexical information from corpora, especially subcategorization acquisition.\n    ",
        "submission_date": "2009-11-30T00:00:00",
        "last_modified_date": "2009-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.0071",
        "title": "Differentially Private Empirical Risk Minimization",
        "authors": [
            "Kamalika Chaudhuri",
            "Claire Monteleoni",
            "Anand D. Sarwate"
        ],
        "abstract": "Privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data, such as medical or financial records, are analyzed. We provide general techniques to produce privacy-preserving approximations of classifiers learned via (regularized) empirical risk minimization (ERM). These algorithms are private under the $\\epsilon$-differential privacy definition due to Dwork et al. (2006). First we apply the output perturbation ideas of Dwork et al. (2006), to ERM classification. Then we propose a new method, objective perturbation, for privacy-preserving machine learning algorithm design. This method entails perturbing the objective function before optimizing over classifiers. If the loss and regularizer satisfy certain convexity and differentiability criteria, we prove theoretical results showing that our algorithms preserve privacy, and provide generalization bounds for linear and nonlinear kernels. We further present a privacy-preserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines. We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets. Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance.\n    ",
        "submission_date": "2009-12-01T00:00:00",
        "last_modified_date": "2011-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.2282",
        "title": "Design of Intelligent layer for flexible querying in databases",
        "authors": [
            "Mrs. Neelu Nihalani",
            "Dr. Sanjay Silakari",
            "Dr. Mahesh Motwani"
        ],
        "abstract": "  Computer-based information technologies have been extensively used to help many organizations, private companies, and academic and education institutions manage their processes and information systems hereby become their nervous centre. The explosion of massive data sets created by businesses, science and governments necessitates intelligent and more powerful computing paradigms so that users can benefit from this data. Therefore most new-generation database applications demand intelligent information management to enhance efficient interactions between database and the users. Database systems support only a Boolean query model. A selection query on SQL database returns all those tuples that satisfy the conditions in the query.\n    ",
        "submission_date": "2009-12-11T00:00:00",
        "last_modified_date": "2009-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.2385",
        "title": "Closing the Learning-Planning Loop with Predictive State Representations",
        "authors": [
            "Byron Boots",
            "Sajid M. Siddiqi",
            "Geoffrey J. Gordon"
        ],
        "abstract": "  A central problem in artificial intelligence is that of planning to maximize future reward under uncertainty in a partially observable environment. In this paper we propose and demonstrate a novel algorithm which accurately learns a model of such an environment directly from sequences of action-observation pairs. We then close the loop from observations to actions by planning in the learned model and recovering a policy which is near-optimal in the original environment. Specifically, we present an efficient and statistically consistent spectral algorithm for learning the parameters of a Predictive State Representation (PSR). We demonstrate the algorithm by learning a model of a simulated high-dimensional, vision-based mobile robot planning task, and then perform approximate point-based planning in the learned PSR. Analysis of our results shows that the algorithm learns a state space which efficiently captures the essential features of the environment. This representation allows accurate prediction with a small number of parameters, and enables successful and efficient planning.\n    ",
        "submission_date": "2009-12-12T00:00:00",
        "last_modified_date": "2009-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.2415",
        "title": "Adapting Heuristic Mastermind Strategies to Evolutionary Algorithms",
        "authors": [
            "Tomas Philip Runarsson",
            "Juan J. Merelo-Guervos"
        ],
        "abstract": "  The art of solving the Mastermind puzzle was initiated by Donald Knuth and is already more than 30 years old; despite that, it still receives much attention in operational research and computer games journals, not to mention the nature-inspired stochastic algorithm literature. In this paper we try to suggest a strategy that will allow nature-inspired algorithms to obtain results as good as those based on exhaustive search strategies; in order to do that, we first review, compare and improve current approaches to solving the puzzle; then we test one of these strategies with an estimation of distribution algorithm. Finally, we try to find a strategy that falls short of being exhaustive, and is then amenable for inclusion in nature inspired algorithms (such as evolutionary or particle swarm algorithms). This paper proves that by the incorporation of local entropy into the fitness function of the evolutionary algorithm it becomes a better player than a random one, and gives a rule of thumb on how to incorporate the best heuristic strategies to evolutionary algorithms without incurring in an excessive computational cost.\n    ",
        "submission_date": "2009-12-12T00:00:00",
        "last_modified_date": "2009-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.3134",
        "title": "Complexity of Propositional Abduction for Restricted Sets of Boolean Functions",
        "authors": [
            "Nadia Creignou",
            "Johannes Schmidt",
            "Michael Thomas"
        ],
        "abstract": "Abduction is a fundamental and important form of non-monotonic reasoning. Given a knowledge base explaining how the world behaves it aims at finding an explanation for some observed manifestation. In this paper we focus on propositional abduction, where the knowledge base and the manifestation are represented by propositional formulae. The problem of deciding whether there exists an explanation has been shown to be SigmaP2-complete in general. We consider variants obtained by restricting the allowed connectives in the formulae to certain sets of Boolean functions. We give a complete classification of the complexity for all considerable sets of Boolean functions. In this way, we identify easier cases, namely NP-complete and polynomial cases; and we highlight sources of intractability. Further, we address the problem of counting the explanations and draw a complete picture for the counting complexity.\n    ",
        "submission_date": "2009-12-16T00:00:00",
        "last_modified_date": "2010-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.3747",
        "title": "A Survey of Paraphrasing and Textual Entailment Methods",
        "authors": [
            "Ion Androutsopoulos",
            "Prodromos Malakasiotis"
        ],
        "abstract": "Paraphrasing methods recognize, generate, or extract phrases, sentences, or longer natural language expressions that convey almost the same information. Textual entailment methods, on the other hand, recognize, generate, or extract pairs of natural language expressions, such that a human who reads (and trusts) the first element of a pair would most likely infer that the other element is also true. Paraphrasing can be seen as bidirectional textual entailment and methods from the two areas are often similar. Both kinds of methods are useful, at least in principle, in a wide range of natural language processing applications, including question answering, summarization, text generation, and machine translation. We summarize key ideas from the two areas by considering in turn recognition, generation, and extraction methods, also pointing to prominent articles and resources.\n    ",
        "submission_date": "2009-12-18T00:00:00",
        "last_modified_date": "2010-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.4473",
        "title": "Learning to Predict Combinatorial Structures",
        "authors": [
            "Shankar Vembu"
        ],
        "abstract": "The major challenge in designing a discriminative learning algorithm for predicting structured data is to address the computational issues arising from the exponential size of the output space. Existing algorithms make different assumptions to ensure efficient, polynomial time estimation of model parameters. For several combinatorial structures, including cycles, partially ordered sets, permutations and other graph classes, these assumptions do not hold. In this thesis, we address the problem of designing learning algorithms for predicting combinatorial structures by introducing two new assumptions: (i) The first assumption is that a particular counting problem can be solved efficiently. The consequence is a generalisation of the classical ridge regression for structured prediction. (ii) The second assumption is that a particular sampling problem can be solved efficiently. The consequence is a new technique for designing and analysing probabilistic structured prediction models. These results can be applied to solve several complex learning problems including but not limited to multi-label classification, multi-category hierarchical classification, and label ranking.\n    ",
        "submission_date": "2009-12-22T00:00:00",
        "last_modified_date": "2010-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.4553",
        "title": "Consensus Dynamics in a non-deterministic Naming Game with Shared Memory",
        "authors": [
            "Reginaldo J. da Silva Filho",
            "Matthias R. Brust",
            "Carlos H.C. Ribeiro"
        ],
        "abstract": "  In the naming game, individuals or agents exchange pairwise local information in order to communicate about objects in their common environment. The goal of the game is to reach a consensus about naming these objects. Originally used to investigate language formation and self-organizing vocabularies, we extend the classical naming game with a globally shared memory accessible by all agents. This shared memory can be interpreted as an external source of knowledge like a book or an Internet site. The extended naming game models an environment similar to one that can be found in the context of social bookmarking and collaborative tagging sites where users tag sites using appropriate labels, but also mimics an important aspect in the field of human-based image labeling. Although the extended naming game is non-deterministic in its word selection, we show that consensus towards a common vocabulary is reached. More importantly, we show the qualitative and quantitative influence of the external source of information, i.e. the shared memory, on the consensus dynamics between the agents.\n    ",
        "submission_date": "2009-12-23T00:00:00",
        "last_modified_date": "2009-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.4649",
        "title": "The use of ideas of Information Theory for studying \"language\" and intelligence in ants",
        "authors": [
            "Boris Ryabko",
            "Zhanna Reznikova"
        ],
        "abstract": "  In this review we integrate results of long term experimental study on ant \"language\" and intelligence which were fully based on fundamental ideas of Information Theory, such as the Shannon entropy, the Kolmogorov complexity, and the Shannon's equation connecting the length of a message ($l$) and its frequency $(p)$, i.e. $l = - \\log p$ for rational communication systems. This approach, new for studying biological communication systems, enabled us to obtain the following important results on ants' communication and intelligence: i) to reveal \"distant homing\" in ants, that is, their ability to transfer information about remote events; ii) to estimate the rate of information transmission; iii) to reveal that ants are able to grasp regularities and to use them for \"compression\" of information; iv) to reveal that ants are able to transfer to each other the information about the number of objects; v) to discover that ants can add and subtract small numbers. The obtained results show that Information Theory is not only wonderful mathematical theory, but many its results may be considered as Nature laws.\n    ",
        "submission_date": "2009-12-23T00:00:00",
        "last_modified_date": "2009-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.4883",
        "title": "On Finding Predictors for Arbitrary Families of Processes",
        "authors": [
            "Daniil Ryabko"
        ],
        "abstract": "  The problem is sequence prediction in the following setting. A sequence $x_1,...,x_n,...$ of discrete-valued observations is generated according to some unknown probabilistic law (measure) $\\mu$. After observing each outcome, it is required to give the conditional probabilities of the next observation. The measure $\\mu$ belongs to an arbitrary but known class $C$ of stochastic process measures. We are interested in predictors $\\rho$ whose conditional probabilities converge (in some sense) to the \"true\" $\\mu$-conditional probabilities if any $\\mu\\in C$ is chosen to generate the sequence. The contribution of this work is in characterizing the families $C$ for which such predictors exist, and in providing a specific and simple form in which to look for a solution. We show that if any predictor works, then there exists a Bayesian predictor, whose prior is discrete, and which works too. We also find several sufficient and necessary conditions for the existence of a predictor, in terms of topological characterizations of the family $C$, as well as in terms of local behaviour of the measures in $C$, which in some cases lead to procedures for constructing such predictors. It should be emphasized that the framework is completely general: the stochastic processes considered are not required to be i.i.d., stationary, or to belong to any parametric or countable family.\n    ",
        "submission_date": "2009-12-24T00:00:00",
        "last_modified_date": "2009-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.5029",
        "title": "Complexity of stochastic branch and bound methods for belief tree search in Bayesian reinforcement learning",
        "authors": [
            "Christos Dimitrakakis"
        ],
        "abstract": "  There has been a lot of recent work on Bayesian methods for reinforcement learning exhibiting near-optimal online performance. The main obstacle facing such methods is that in most problems of interest, the optimal solution involves planning in an infinitely large tree. However, it is possible to obtain stochastic lower and upper bounds on the value of each tree node. This enables us to use stochastic branch and bound algorithms to search the tree efficiently. This paper proposes two such algorithms and examines their complexity in this setting.\n    ",
        "submission_date": "2009-12-26T00:00:00",
        "last_modified_date": "2009-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.5241",
        "title": "Believe It or Not: Adding Belief Annotations to Databases",
        "authors": [
            "Wolfgang Gatterbauer",
            "Magdalena Balazinska",
            "Nodira Khoussainova",
            "Dan Suciu"
        ],
        "abstract": "  We propose a database model that allows users to annotate data with belief statements. Our motivation comes from scientific database applications where a community of users is working together to assemble, revise, and curate a shared data repository. As the community accumulates knowledge and the database content evolves over time, it may contain conflicting information and members can disagree on the information it should store. For example, Alice may believe that a tuple should be in the database, whereas Bob disagrees. He may also insert the reason why he thinks Alice believes the tuple should be in the database, and explain what he thinks the correct tuple should be instead.\n",
        "submission_date": "2009-12-30T00:00:00",
        "last_modified_date": "2009-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0912.5340",
        "title": "Why so? or Why no? Functional Causality for Explaining Query Answers",
        "authors": [
            "Alexandra Meliou",
            "Wolfgang Gatterbauer",
            "Katherine F. Moore",
            "Dan Suciu"
        ],
        "abstract": "  In this paper, we propose causality as a unified framework to explain query answers and non-answers, thus generalizing and extending several previously proposed approaches of provenance and missing query result explanations.\n",
        "submission_date": "2009-12-29T00:00:00",
        "last_modified_date": "2009-12-29T00:00:00"
    }
]