[
    {
        "url": "https://arxiv.org/abs/1101.2279",
        "title": "Planning with Partial Preference Models",
        "authors": [
            "Tuan Nguyen",
            "Minh Do",
            "Alfonso Gerevini",
            "Ivan Serina",
            "Biplav Srivastava",
            "Subbarao Kambhampati"
        ],
        "abstract": "Current work in planning with preferences assume that the user's preference models are completely specified and aim to search for a single solution plan. In many real-world planning scenarios, however, the user probably cannot provide any information about her desired plans, or in some cases can only express partial preferences. In such situations, the planner has to present not only one but a set of plans to the user, with the hope that some of them are similar to the plan she prefers. We first propose the usage of different measures to capture quality of plan sets that are suitable for such scenarios: domain-independent distance measures defined based on plan elements (actions, states, causal links) if no knowledge of the user's preferences is given, and the Integrated Convex Preference measure in case the user's partial preference is provided. We then investigate various heuristic approaches to find set of plans according to these measures, and present empirical results demonstrating the promise of our approach.\n    ",
        "submission_date": "2011-01-12T00:00:00",
        "last_modified_date": "2011-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.2320",
        "title": "Review and Evaluation of Feature Selection Algorithms in Synthetic Problems",
        "authors": [
            "L.A. Belanche",
            "F.F. Gonz\u00e1lez"
        ],
        "abstract": "The main purpose of Feature Subset Selection is to find a reduced subset of attributes from a data set described by a feature set. The task of a feature selection algorithm (FSA) is to provide with a computational solution motivated by a certain definition of relevance or by a reliable evaluation measure. In this paper several fundamental algorithms are studied to assess their performance in a controlled experimental scenario. A measure to evaluate FSAs is devised that computes the degree of matching between the output given by a FSA and the known optimal solutions. An extensive experimental study on synthetic problems is carried out to assess the behaviour of the algorithms in terms of solution accuracy and size as a function of the relevance, irrelevance, redundancy and size of the data samples. The controlled experimental conditions facilitate the derivation of better-supported and meaningful conclusions.\n    ",
        "submission_date": "2011-01-12T00:00:00",
        "last_modified_date": "2011-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.2378",
        "title": "Extracting Features from Ratings: The Role of Factor Models",
        "authors": [
            "Joachim Selke",
            "Wolf-Tilo Balke"
        ],
        "abstract": "Performing effective preference-based data retrieval requires detailed and preferentially meaningful structurized information about the current user as well as the items under consideration. A common problem is that representations of items often only consist of mere technical attributes, which do not resemble human perception. This is particularly true for integral items such as movies or songs. It is often claimed that meaningful item features could be extracted from collaborative rating data, which is becoming available through social networking services. However, there is only anecdotal evidence supporting this claim; but if it is true, the extracted information could very valuable for preference-based data retrieval. In this paper, we propose a methodology to systematically check this common claim. We performed a preliminary investigation on a large collection of movie ratings and present initial evidence.\n    ",
        "submission_date": "2011-01-12T00:00:00",
        "last_modified_date": "2011-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.3381",
        "title": "Efficient Independence-Based MAP Approach for Robust Markov Networks Structure Discovery",
        "authors": [
            "Facundo Bromberg",
            "Federico Schl\u00fcter"
        ],
        "abstract": "This work introduces the IB-score, a family of independence-based score functions for robust learning of Markov networks independence structures. Markov networks are a widely used graphical representation of probability distributions, with many applications in several fields of science. The main advantage of the IB-score is the possibility of computing it without the need of estimation of the numerical parameters, an NP-hard problem, usually solved through an approximate, data-intensive, iterative optimization. We derive a formal expression for the IB-score from first principles, mainly maximum a posteriori and conditional independence properties, and exemplify several instantiations of it, resulting in two novel algorithms for structure learning: IBMAP-HC and IBMAP-TS. Experimental results over both artificial and real world data show these algorithms achieve important error reductions in the learnt structures when compared with the state-of-the-art independence-based structure learning algorithm GSMN, achieving increments of more than 50% in the amount of independencies they encode correctly, and in some cases, learning correctly over 90% of the edges that GSMN learnt incorrectly. Theoretical analysis shows IBMAP-HC proceeds efficiently, achieving these improvements in a time polynomial to the number of random variables in the domain.\n    ",
        "submission_date": "2011-01-18T00:00:00",
        "last_modified_date": "2011-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.3465",
        "title": "The \"psychological map of the brain\", as a personal information card (file), - a project for the student of the 21st century",
        "authors": [
            "Emanuel Gluskin"
        ],
        "abstract": "We suggest a procedure that is relevant both to electronic performance and human psychology, so that the creative logic and the respect for human nature appear in a good agreement. The idea is to create an electronic card containing basic information about a person's psychological behavior in order to make it possible to quickly decide about the suitability of one for another. This \"psychological electronics\" approach could be tested via student projects.\n    ",
        "submission_date": "2011-01-17T00:00:00",
        "last_modified_date": "2011-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.4003",
        "title": "Dyna-H: a heuristic planning reinforcement learning algorithm applied to role-playing-game strategy decision systems",
        "authors": [
            "Matilde Santos",
            "Jose Antonio Martin H.",
            "Victoria Lopez",
            "Guillermo Botella"
        ],
        "abstract": "In a Role-Playing Game, finding optimal trajectories is one of the most important tasks. In fact, the strategy decision system becomes a key component of a game engine. Determining the way in which decisions are taken (online, batch or simulated) and the consumed resources in decision making (e.g. execution time, memory) will influence, in mayor degree, the game performance. When classical search algorithms such as A* can be used, they are the very first option. Nevertheless, such methods rely on precise and complete models of the search space, and there are many interesting scenarios where their application is not possible. Then, model free methods for sequential decision making under uncertainty are the best choice. In this paper, we propose a heuristic planning strategy to incorporate the ability of heuristic-search in path-finding into a Dyna agent. The proposed Dyna-H algorithm, as A* does, selects branches more likely to produce outcomes than other branches. Besides, it has the advantages of being a model-free online reinforcement learning algorithm. The proposal was evaluated against the one-step Q-Learning and Dyna-Q algorithms obtaining excellent experimental results: Dyna-H significantly overcomes both methods in all experiments. We suggest also, a functional analogy between the proposed sampling from worst trajectories heuristic and the role of dreams (e.g. nightmares) in human behavior.\n    ",
        "submission_date": "2011-01-20T00:00:00",
        "last_modified_date": "2011-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.4351",
        "title": "Building a Chaotic Proved Neural Network",
        "authors": [
            "Jacques M. Bahi",
            "Christophe Guyeux",
            "Michel Salomon"
        ],
        "abstract": "Chaotic neural networks have received a great deal of attention these last years. In this paper we establish a precise correspondence between the so-called chaotic iterations and a particular class of artificial neural networks: global recurrent multi-layer perceptrons. We show formally that it is possible to make these iterations behave chaotically, as defined by Devaney, and thus we obtain the first neural networks proven chaotic. Several neural networks with different architectures are trained to exhibit a chaotical behavior.\n    ",
        "submission_date": "2011-01-23T00:00:00",
        "last_modified_date": "2011-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.4356",
        "title": "Meaning Negotiation as Inference",
        "authors": [
            "Elisa Burato",
            "Matteo Cristani",
            "Luca Vigan\u00f2"
        ],
        "abstract": "Meaning negotiation (MN) is the general process with which agents reach an agreement about the meaning of a set of terms. Artificial Intelligence scholars have dealt with the problem of MN by means of argumentations schemes, beliefs merging and information fusion operators, and ontology alignment but the proposed approaches depend upon the number of participants. In this paper, we give a general model of MN for an arbitrary number of agents, in which each participant discusses with the others her viewpoint by exhibiting it in an actual set of constraints on the meaning of the negotiated terms. We call this presentation of individual viewpoints an angle. The agents do not aim at forming a common viewpoint but, instead, at agreeing about an acceptable common angle. We analyze separately the process of MN by two agents (\\emph{bilateral} or \\emph{pairwise} MN) and by more than two agents (\\emph{multiparty} MN), and we use game theoretic models to understand how the process develops in both cases: the models are Bargaining Game for bilateral MN and English Auction for multiparty MN. We formalize the process of reaching such an agreement by giving a deduction system that comprises of rules that are consistent and adequate for representing MN.\n    ",
        "submission_date": "2011-01-23T00:00:00",
        "last_modified_date": "2011-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.5460",
        "title": "A Human-Centric Approach to Group-Based Context-Awareness",
        "authors": [
            "Nasser Ghadiri",
            "Ahmad Baraani-Dastjerdi",
            "Nasser Ghasem-Aghaee",
            "Mohammad A. Nematbakhsh"
        ],
        "abstract": "The emerging need for qualitative approaches in context-aware information processing calls for proper modeling of context information and efficient handling of its inherent uncertainty resulted from human interpretation and usage. Many of the current approaches to context-awareness either lack a solid theoretical basis for modeling or ignore important requirements such as modularity, high-order uncertainty management and group-based context-awareness. Therefore, their real-world application and extendability remains limited. In this paper, we present f-Context as a service-based context-awareness framework, based on language-action perspective (LAP) theory for modeling. Then we identify some of the complex, informational parts of context which contain high-order uncertainties due to differences between members of the group in defining them. An agent-based perceptual computer architecture is proposed for implementing f-Context that uses computing with words (CWW) for handling uncertainty. The feasibility of f-Context is analyzed using a realistic scenario involving a group of mobile users. We believe that the proposed approach can open the door to future research on context-awareness by offering a theoretical foundation based on human communication, and a service-based layered architecture which exploits CWW for context-aware, group-based and platform-independent access to information systems.\n    ",
        "submission_date": "2011-01-28T00:00:00",
        "last_modified_date": "2011-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.6001",
        "title": "Boolean network robotics: a proof of concept",
        "authors": [
            "Andrea Roli",
            "Mattia Manfroni",
            "Carlo Pinciroli",
            "Mauro Birattari"
        ],
        "abstract": "Dynamical systems theory and complexity science provide powerful tools for analysing artificial agents and robots. Furthermore, they have been recently proposed also as a source of design principles and guidelines. Boolean networks are a prominent example of complex dynamical systems and they have been shown to effectively capture important phenomena in gene regulation. From an engineering perspective, these models are very compelling, because they can exhibit rich and complex behaviours, in spite of the compactness of their description. In this paper, we propose the use of Boolean networks for controlling robots' behaviour. The network is designed by means of an automatic procedure based on stochastic local search techniques. We show that this approach makes it possible to design a network which enables the robot to accomplish a task that requires the capability of navigating the space using a light stimulus, as well as the formation and use of an internal memory.\n    ",
        "submission_date": "2011-01-31T00:00:00",
        "last_modified_date": "2011-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.6009",
        "title": "Solving the Satisfiability Problem Through Boolean Networks",
        "authors": [
            "Andrea Roli",
            "Michela Milano"
        ],
        "abstract": "In this paper we present a new approach to solve the satisfiability problem (SAT), based on boolean networks (BN). We define a mapping between a SAT instance and a BN, and we solve SAT problem by simulating the BN dynamics. We prove that BN fixed points correspond to the SAT solutions. The mapping presented allows to develop a new class of algorithms to solve SAT. Moreover, this new approach suggests new ways to combine symbolic and connectionist computation and provides a general framework for local search algorithms.\n    ",
        "submission_date": "2011-01-31T00:00:00",
        "last_modified_date": "2011-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.0079",
        "title": "Information-theoretic measures associated with rough set approximations",
        "authors": [
            "Ping Zhu",
            "Qiaoyan Wen"
        ],
        "abstract": "Although some information-theoretic measures of uncertainty or granularity have been proposed in rough set theory, these measures are only dependent on the underlying partition and the cardinality of the universe, independent of the lower and upper approximations. It seems somewhat unreasonable since the basic idea of rough set theory aims at describing vague concepts by the lower and upper approximations. In this paper, we thus define new information-theoretic entropy and co-entropy functions associated to the partition and the approximations to measure the uncertainty and granularity of an approximation space. After introducing the novel notions of entropy and co-entropy, we then examine their properties. In particular, we discuss the relationship of co-entropies between different universes. The theoretical development is accompanied by illustrative numerical examples.\n    ",
        "submission_date": "2011-02-01T00:00:00",
        "last_modified_date": "2011-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.0183",
        "title": "High-Performance Neural Networks for Visual Object Classification",
        "authors": [
            "Dan C. Cire\u015fan",
            "Ueli Meier",
            "Jonathan Masci",
            "Luca M. Gambardella",
            "J\u00fcrgen Schmidhuber"
        ],
        "abstract": "We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs, respectively.\n    ",
        "submission_date": "2011-02-01T00:00:00",
        "last_modified_date": "2011-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.0714",
        "title": "An architecture for the evaluation of intelligent systems",
        "authors": [
            "Javier Insa-Cabrera",
            "Jose Hernandez-Orallo"
        ],
        "abstract": "One of the main research areas in Artificial Intelligence is the coding of agents (programs) which are able to learn by themselves in any situation. This means that agents must be useful for purposes other than those they were created for, as, for example, playing chess. In this way we try to get closer to the pristine goal of Artificial Intelligence. One of the problems to decide whether an agent is really intelligent or not is the measurement of its intelligence, since there is currently no way to measure it in a reliable way. The purpose of this project is to create an interpreter that allows for the execution of several environments, including those which are generated randomly, so that an agent (a person or a program) can interact with them. Once the interaction between the agent and the environment is over, the interpreter will measure the intelligence of the agent according to the actions, states and rewards the agent has undergone inside the environment during the test. As a result we will be able to measure agents' intelligence in any possible environment, and to make comparisons between several agents, in order to determine which of them is the most intelligent. In order to perform the tests, the interpreter must be able to randomly generate environments that are really useful to measure agents' intelligence, since not any randomly generated environment will serve that purpose.\n    ",
        "submission_date": "2011-02-03T00:00:00",
        "last_modified_date": "2011-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.0831",
        "title": "Intelligent Semantic Web Search Engines: A Brief Survey",
        "authors": [
            "G.Madhu",
            "Dr.A.Govardhan",
            "Dr.T.V.Rajinikanth"
        ],
        "abstract": "The World Wide Web (WWW) allows the people to share the information (data) from the large database repositories globally. The amount of information grows billions of databases. We need to search the information will specialize tools known generically search engine. There are many of search engines available today, retrieving meaningful information is difficult. However to overcome this problem in search engines to retrieve meaningful information intelligently, semantic web technologies are playing a major role. In this paper we present survey on the search engine generations and the role of search engines in intelligent web and semantic search technologies.\n    ",
        "submission_date": "2011-02-04T00:00:00",
        "last_modified_date": "2011-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.0899",
        "title": "Evidence Feed Forward Hidden Markov Model: A New Type of Hidden Markov Model",
        "authors": [
            "Michael DelRose",
            "Christian Wagner",
            "Philip Frederick"
        ],
        "abstract": "The ability to predict the intentions of people based solely on their visual actions is a skill only performed by humans and animals. The intelligence of current computer algorithms has not reached this level of complexity, but there are several research efforts that are working towards it. With the number of classification algorithms available, it is hard to determine which algorithm works best for a particular situation. In classification of visual human intent data, Hidden Markov Models (HMM), and their variants, are leading candidates.\n",
        "submission_date": "2011-02-04T00:00:00",
        "last_modified_date": "2011-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.1536",
        "title": "Evolutionary multiobjective optimization of the multi-location transshipment problem",
        "authors": [
            "Nabil Belgasmi",
            "Lamjed Ben Said",
            "Khaled Gh\u00e9dira"
        ],
        "abstract": "We consider a multi-location inventory system where inventory choices at each location are centrally coordinated. Lateral transshipments are allowed as recourse actions within the same echelon in the inventory system to reduce costs and improve service level. However, this transshipment process usually causes undesirable lead times. In this paper, we propose a multiobjective model of the multi-location transshipment problem which addresses optimizing three conflicting objectives: (1) minimizing the aggregate expected cost, (2) maximizing the expected fill rate, and (3) minimizing the expected transshipment lead times. We apply an evolutionary multiobjective optimization approach using the strength Pareto evolutionary algorithm (SPEA2), to approximate the optimal Pareto front. Simulation with a wide choice of model parameters shows the different trades-off between the conflicting objectives.\n    ",
        "submission_date": "2011-02-08T00:00:00",
        "last_modified_date": "2011-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.1808",
        "title": "From Machine Learning to Machine Reasoning",
        "authors": [
            "Leon Bottou"
        ],
        "abstract": "A plausible definition of \"reasoning\" could be \"algebraically manipulating previously acquired knowledge in order to answer a new question\". This definition covers first-order logical inference or probabilistic inference. It also includes much simpler manipulations commonly used to build large learning systems. For instance, we can build an optical character recognition system by first training a character segmenter, an isolated character recognizer, and a language model, using appropriate labeled training sets. Adequately concatenating these modules and fine tuning the resulting system can be viewed as an algebraic operation in a space of models. The resulting model answers a new question, that is, converting the image of a text page into a computer readable text.\n",
        "submission_date": "2011-02-09T00:00:00",
        "last_modified_date": "2011-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.2125",
        "title": "Improving DPLL Solver Performance with Domain-Specific Heuristics: the ASP Case",
        "authors": [
            "Marcello Balduccini"
        ],
        "abstract": "In spite of the recent improvements in the performance of the solvers based on the DPLL procedure, it is still possible for the search algorithm to focus on the wrong areas of the search space, preventing the solver from returning a solution in an acceptable amount of time. This prospect is a real concern e.g. in an industrial setting, where users typically expect consistent performance. To overcome this problem, we propose a framework that allows learning and using domain-specific heuristics in solvers based on the DPLL procedure. The learning is done off-line, on representative instances from the target domain, and the learned heuristics are then used for choice-point selection. In this paper we focus on Answer Set Programming (ASP) solvers. In our experiments, the introduction of domain-specific heuristics improved performance on hard instances by up to 3 orders of magnitude (and 2 on average), nearly completely eliminating the cases in which the solver had to be terminated because the wait for an answer had become unacceptable.\n    ",
        "submission_date": "2011-02-10T00:00:00",
        "last_modified_date": "2011-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.2670",
        "title": "Online Least Squares Estimation with Self-Normalized Processes: An Application to Bandit Problems",
        "authors": [
            "Yasin Abbasi-Yadkori",
            "David Pal",
            "Csaba Szepesvari"
        ],
        "abstract": "The analysis of online least squares estimation is at the heart of many stochastic sequential decision making problems. We employ tools from the self-normalized processes to provide a simple and self-contained proof of a tail bound of a vector-valued martingale. We use the bound to construct a new tighter confidence sets for the least squares estimate.\n",
        "submission_date": "2011-02-14T00:00:00",
        "last_modified_date": "2011-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.2984",
        "title": "Hybrid Model for Solving Multi-Objective Problems Using Evolutionary Algorithm and Tabu Search",
        "authors": [
            "Rjab Hajlaoui",
            "Mariem Gzara",
            "Abdelaziz Dammak"
        ],
        "abstract": "This paper presents a new multi-objective hybrid model that makes cooperation between the strength of research of neighborhood methods presented by the tabu search (TS) and the important exploration capacity of evolutionary algorithm. This model was implemented and tested in benchmark functions (ZDT1, ZDT2, and ZDT3), using a network of computers.\n    ",
        "submission_date": "2011-02-15T00:00:00",
        "last_modified_date": "2011-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.3680",
        "title": "Foundations for Understanding and Building Conscious Systems using Stable Parallel Looped Dynamics",
        "authors": [
            "Muralidhar Ravuri"
        ],
        "abstract": "The problem of consciousness faced several challenges for a few reasons: (a) a lack of necessary and sufficient conditions, without which we would not know how close we are to the solution, (b) a lack of a synthesis framework to build conscious systems and (c) a lack of mechanisms explaining the transition between the lower-level chemical dynamics and the higher-level abstractions. In this paper, I address these issues using a new framework. The central result is that a person is 'minimally' conscious if and only if he knows at least one truth. This lets us move away from the vagueness surrounding consciousness and instead focus equivalently on: (i) what truths are and how our brain represents/relates them to each other and (ii) how we attain a feeling of knowing for a truth. For the former problem, since truths are things that do not change, I replace the abstract notion with a dynamical one called fixed sets. These sets are guaranteed to exist for our brain and other stable parallel looped systems. The relationships between everyday events are now built using relationships between fixed sets, until our brain creates a unique dynamical state called the self-sustaining threshold 'membrane' of fixed sets. For the latter problem, I present necessary and sufficient conditions for attaining a feeling of knowing using a definition of continuity applied to abstractions. Combining these results, I now say that a person is minimally conscious if and only if his brain has a self-sustaining dynamical membrane with abstract continuous paths. A synthetic system built to satisfy this equivalent self-sustaining membrane condition appears indistinguishable from human consciousness.\n    ",
        "submission_date": "2011-02-17T00:00:00",
        "last_modified_date": "2011-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.3868",
        "title": "Evolved preambles for MAX-SAT heuristics",
        "authors": [
            "Luis O. Rigo Jr",
            "Valmir C. Barbosa"
        ],
        "abstract": "MAX-SAT heuristics normally operate from random initial truth assignments to the variables. We consider the use of what we call preambles, which are sequences of variables with corresponding single-variable assignment actions intended to be used to determine a more suitable initial truth assignment for a given problem instance and a given heuristic. For a number of well established MAX-SAT heuristics and benchmark instances, we demonstrate that preambles can be evolved by a genetic algorithm such that the heuristics are outperformed in a significant fraction of the cases.\n    ",
        "submission_date": "2011-02-18T00:00:00",
        "last_modified_date": "2011-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.4922",
        "title": "Counting Solutions of Constraint Satisfiability Problems:Exact Phase Transitions and Approximate Algorithm",
        "authors": [
            "Minghao Yin",
            "Ping Huang"
        ],
        "abstract": "The study of phase transition phenomenon of NP complete problems plays an important role in understanding the nature of hard problems. In this paper, we follow this line of research by considering the problem of counting solutions of Constraint Satisfaction Problems (#CSP). We consider the random model, i.e. RB model. We prove that phase transition of #CSP does exist as the number of variables approaches infinity and the critical values where phase transitions occur are precisely located. Preliminary experimental results also show that the critical point coincides with the theoretical derivation. Moreover, we propose an approximate algorithm to estimate the expectation value of the solutions number of a given CSP instance of RB model.\n    ",
        "submission_date": "2011-02-24T00:00:00",
        "last_modified_date": "2011-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.4924",
        "title": "New Worst-Case Upper Bound for #XSAT",
        "authors": [
            "Junping Zhou",
            "Minghao Yin"
        ],
        "abstract": "An algorithm running in O(1.1995n) is presented for counting models for exact satisfiability formulae(#XSAT). This is faster than the previously best algorithm which runs in O(1.2190n). In order to improve the efficiency of the algorithm, a new principle, i.e. the common literals principle, is addressed to simplify formulae. This allows us to eliminate more common literals. In addition, we firstly inject the resolution principles into solving #XSAT problem, and therefore this further improves the efficiency of the algorithm.\n    ",
        "submission_date": "2011-02-24T00:00:00",
        "last_modified_date": "2011-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.4925",
        "title": "Worst-Case Upper Bound for (1, 2)-QSAT",
        "authors": [
            "Minghao Yin"
        ],
        "abstract": "The rigorous theoretical analysis of the algorithm for a subclass of QSAT, i.e. (1, 2)-QSAT, has been proposed in the literature. (1, 2)-QSAT, first introduced in SAT'08, can be seen as quantified extended 2-CNF formulas. Until now, within our knowledge, there exists no algorithm presenting the worst upper bound for (1, 2)-QSAT. Therefore in this paper, we present an exact algorithm to solve (1, 2)-QSAT. By analyzing the algorithms, we obtain a worst-case upper bound O(1.4142m), where m is the number of clauses.\n    ",
        "submission_date": "2011-02-24T00:00:00",
        "last_modified_date": "2011-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.4926",
        "title": "New Worst-Case Upper Bound for X3SAT",
        "authors": [
            "Junping Zhou",
            "Minghao Yin"
        ],
        "abstract": "The rigorous theoretical analyses of algorithms for exact 3-satisfiability (X3SAT) have been proposed in the literature. As we know, previous algorithms for solving X3SAT have been analyzed only regarding the number of variables as the parameter. However, the time complexity for solving X3SAT instances depends not only on the number of variables, but also on the number of clauses. Therefore, it is significant to exploit the time complexity from the other point of view, i.e. the number of clauses. In this paper, we present algorithms for solving X3SAT with rigorous complexity analyses using the number of clauses as the parameter. By analyzing the algorithms, we obtain the new worst-case upper bounds O(1.15855m), where m is the number of clauses.\n    ",
        "submission_date": "2011-02-24T00:00:00",
        "last_modified_date": "2011-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.5385",
        "title": "Back and Forth Between Rules and SE-Models (Extended Version)",
        "authors": [
            "Martin Slota",
            "Jo\u00e3o Leite"
        ],
        "abstract": "Rules in logic programming encode information about mutual interdependencies between literals that is not captured by any of the commonly used semantics. This information becomes essential as soon as a program needs to be modified or further manipulated.\n",
        "submission_date": "2011-02-26T00:00:00",
        "last_modified_date": "2011-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.5561",
        "title": "Decision Making Agent Searching for Markov Models in Near-Deterministic World",
        "authors": [
            "Gabor Matuz",
            "Andras Lorincz"
        ],
        "abstract": "Reinforcement learning has solid foundations, but becomes inefficient in partially observed (non-Markovian) environments. Thus, a learning agent -born with a representation and a policy- might wish to investigate to what extent the Markov property holds. We propose a learning architecture that utilizes combinatorial policy optimization to overcome non-Markovity and to develop efficient behaviors, which are easy to inherit, tests the Markov property of the behavioral states, and corrects against non-Markovity by running a deterministic factored Finite State Model, which can be learned. We illustrate the properties of architecture in the near deterministic Ms. Pac-Man game. We analyze the architecture from the point of view of evolutionary, individual, and social learning.\n    ",
        "submission_date": "2011-02-27T00:00:00",
        "last_modified_date": "2011-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.5635",
        "title": "Practical inventory routing: A problem definition and an optimization method",
        "authors": [
            "Martin Josef Geiger",
            "Marc Sevaux"
        ],
        "abstract": "The global objective of this work is to provide practical optimization methods to companies involved in inventory routing problems, taking into account this new type of data. Also, companies are sometimes not able to deal with changing plans every period and would like to adopt regular structures for serving customers.\n    ",
        "submission_date": "2011-02-28T00:00:00",
        "last_modified_date": "2011-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.0127",
        "title": "Fuzzy Approach to Critical Bus Ranking under Normal and Line Outage Contingencies",
        "authors": [
            "Shobha Shankar",
            "Dr. T. Ananthapadmanabha"
        ],
        "abstract": "Identification of critical or weak buses for a given operating condition is an important task in the load dispatch centre. It has become more vital in view of the threat of voltage instability leading to voltage collapse. This paper presents a fuzzy approach for ranking critical buses in a power system under normal and network contingencies based on Line Flow index and voltage profiles at load buses. The Line Flow index determines the maximum load that is possible to be connected to a bus in order to maintain stability before the system reaches its bifurcation point. Line Flow index (LF index) along with voltage profiles at the load buses are represented in Fuzzy Set notation. Further they are evaluated using fuzzy rules to compute Criticality Index. Based on this index, critical buses are ranked. The bus with highest rank is the weakest bus as it can withstand a small amount of load before causing voltage collapse. The proposed method is tested on Five Bus Test System.\n    ",
        "submission_date": "2011-03-01T00:00:00",
        "last_modified_date": "2011-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.0605",
        "title": "Loopy Belief Propagation, Bethe Free Energy and Graph Zeta Function",
        "authors": [
            "Yusuke Watanabe",
            "Kenji Fukumizu"
        ],
        "abstract": "We propose a new approach to the theoretical analysis of Loopy Belief Propagation (LBP) and the Bethe free energy (BFE) by establishing a formula to connect LBP and BFE with a graph zeta function. The proposed approach is applicable to a wide class of models including multinomial and Gaussian types. The connection derives a number of new theoretical results on LBP and BFE. This paper focuses two of such topics. One is the analysis of the region where the Hessian of the Bethe free energy is positive definite, which derives the non-convexity of BFE for graphs with multiple cycles, and a condition of convexity on a restricted set. This analysis also gives a new condition for the uniqueness of the LBP fixed point. The other result is to clarify the relation between the local stability of a fixed point of LBP and local minima of the BFE, which implies, for example, that a locally stable fixed point of the Gaussian LBP is a local minimum of the Gaussian Bethe free energy.\n    ",
        "submission_date": "2011-03-03T00:00:00",
        "last_modified_date": "2011-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.0632",
        "title": "An Agent Based Architecture (Using Planning) for Dynamic and Semantic Web Services Composition in an EBXML Context",
        "authors": [
            "Hioual Ouassila",
            "Boufaida Zizette"
        ],
        "abstract": "The process-based semantic composition of Web Services is gaining a considerable momentum as an approach for the effective integration of distributed, heterogeneous, and autonomous applications. To compose Web Services semantically, we need an ontology. There are several ways of inserting semantics in Web Services. One of them consists of using description languages like OWL-S. In this paper, we introduce our work which consists in the proposition of a new model and the use of semantic matching technology for semantic and dynamic composition of ebXML business processes.\n    ",
        "submission_date": "2011-03-03T00:00:00",
        "last_modified_date": "2011-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.0697",
        "title": "A Wiki for Business Rules in Open Vocabulary, Executable English",
        "authors": [
            "Adrian Walker"
        ],
        "abstract": "The problem of business-IT alignment is of widespread economic concern.\n",
        "submission_date": "2011-03-03T00:00:00",
        "last_modified_date": "2011-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.1003",
        "title": "Teraflop-scale Incremental Machine Learning",
        "authors": [
            "Eray \u00d6zkural"
        ],
        "abstract": "We propose a long-term memory design for artificial general intelligence based on Solomonoff's incremental machine learning methods. We use R5RS Scheme and its standard library with a few omissions as the reference machine. We introduce a Levin Search variant based on Stochastic Context Free Grammar together with four synergistic update algorithms that use the same grammar as a guiding probability distribution of programs. The update algorithms include adjusting production probabilities, re-using previous solutions, learning programming idioms and discovery of frequent subprograms. Experiments with two training sequences demonstrate that our approach to incremental learning is effective.\n    ",
        "submission_date": "2011-03-05T00:00:00",
        "last_modified_date": "2011-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.1156",
        "title": "Efficient neuro-fuzzy system and its Memristor Crossbar-based Hardware Implementation",
        "authors": [
            "Farnood Merrikh-Bayat",
            "Saeed Bagheri-Shouraki"
        ],
        "abstract": "In this paper a novel neuro-fuzzy system is proposed where its learning is based on the creation of fuzzy relations by using new implication method without utilizing any exact mathematical techniques. Then, a simple memristor crossbar-based analog circuit is designed to implement this neuro-fuzzy system which offers very interesting properties. In addition to high connectivity between neurons and being fault-tolerant, all synaptic weights in our proposed method are always non-negative and there is no need to precisely adjust them. Finally, this structure is hierarchically expandable and can compute operations in real time since it is implemented through analog circuits. Simulation results show the efficiency and applicability of our neuro-fuzzy computing system. They also indicate that this system can be a good candidate to be used for creating artificial brain.\n    ",
        "submission_date": "2011-03-06T00:00:00",
        "last_modified_date": "2011-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.1157",
        "title": "GRASP and path-relinking for Coalition Structure Generation",
        "authors": [
            "Nicola Di Mauro",
            "Teresa M.A. Basile",
            "Stefano Ferilli",
            "Floriana Esposito"
        ],
        "abstract": "In Artificial Intelligence with Coalition Structure Generation (CSG) one refers to those cooperative complex problems that require to find an optimal partition, maximising a social welfare, of a set of entities involved in a system into exhaustive and disjoint coalitions. The solution of the CSG problem finds applications in many fields such as Machine Learning (covering machines, clustering), Data Mining (decision tree, discretization), Graph Theory, Natural Language Processing (aggregation), Semantic Web (service composition), and Bioinformatics. The problem of finding the optimal coalition structure is NP-complete. In this paper we present a greedy adaptive search procedure (GRASP) with path-relinking to efficiently search the space of coalition structures. Experiments and comparisons to other algorithms prove the validity of the proposed method in solving this hard combinatorial problem.\n    ",
        "submission_date": "2011-03-06T00:00:00",
        "last_modified_date": "2011-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.1205",
        "title": "A Directional Feature with Energy based Offline Signature Verification Network",
        "authors": [
            "Minal Tomar",
            "Pratibha Singh"
        ],
        "abstract": "Signature used as a biometric is implemented in various systems as well as every signature signed by each person is distinct at the same time. So, it is very important to have a computerized signature verification system. In offline signature verification system dynamic features are not available obviously, but one can use a signature as an image and apply image processing techniques to make an effective offline signature verification system. Author proposes a intelligent network used directional feature and energy density both as inputs to the same network and classifies the signature. Neural network is used as a classifier for this system. The results are compared with both the very basic energy density method and a simple directional feature method of offline signature verification system and this proposed new network is found very effective as compared to the above two methods, specially for less number of training samples, which can be implemented practically.\n    ",
        "submission_date": "2011-03-07T00:00:00",
        "last_modified_date": "2011-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.1252",
        "title": "Automatic Wrapper Adaptation by Tree Edit Distance Matching",
        "authors": [
            "Emilio Ferrara",
            "Robert Baumgartner"
        ],
        "abstract": "Information distributed through the Web keeps growing faster day by day, and for this reason, several techniques for extracting Web data have been suggested during last years. Often, extraction tasks are performed through so called wrappers, procedures extracting information from Web pages, e.g. implementing logic-based techniques. Many fields of application today require a strong degree of robustness of wrappers, in order not to compromise assets of information or reliability of data extracted. Unfortunately, wrappers may fail in the task of extracting data from a Web page, if its structure changes, sometimes even slightly, thus requiring the exploiting of new techniques to be automatically held so as to adapt the wrapper to the new structure of the page, in case of failure. In this work we present a novel approach of automatic wrapper adaptation based on the measurement of similarity of trees through improved tree edit distance matching techniques.\n    ",
        "submission_date": "2011-03-07T00:00:00",
        "last_modified_date": "2011-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.1254",
        "title": "Design of Automatically Adaptable Web Wrappers",
        "authors": [
            "Emilio Ferrara",
            "Robert Baumgartner"
        ],
        "abstract": "Nowadays, the huge amount of information distributed through the Web motivates studying techniques to be adopted in order to extract relevant data in an efficient and reliable way. Both academia and enterprises developed several approaches of Web data extraction, for example using techniques of artificial intelligence or machine learning. Some commonly adopted procedures, namely wrappers, ensure a high degree of precision of information extracted from Web pages, and, at the same time, have to prove robustness in order not to compromise quality and reliability of data themselves. In this paper we focus on some experimental aspects related to the robustness of the data extraction process and the possibility of automatically adapting wrappers. We discuss the implementation of algorithms for finding similarities between two different version of a Web page, in order to handle modifications, avoiding the failure of data extraction tasks and ensuring reliability of information extracted. Our purpose is to evaluate performances, advantages and draw-backs of our novel system of automatic wrapper adaptation.\n    ",
        "submission_date": "2011-03-07T00:00:00",
        "last_modified_date": "2011-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.1542",
        "title": "The tractability of CSP classes defined by forbidden patterns",
        "authors": [
            "David A. Cohen",
            "Martin C. Cooper",
            "P\u00e1id\u00ed Creed",
            "Andr\u00e1s Z. Salamon"
        ],
        "abstract": "The constraint satisfaction problem (CSP) is a general problem central to computer science and artificial intelligence. Although the CSP is NP-hard in general, considerable effort has been spent on identifying tractable subclasses. The main two approaches consider structural properties (restrictions on the hypergraph of constraint scopes) and relational properties (restrictions on the language of constraint relations). Recently, some authors have considered hybrid properties that restrict the constraint hypergraph and the relations simultaneously.\n",
        "submission_date": "2011-03-08T00:00:00",
        "last_modified_date": "2014-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.1604",
        "title": "On Minimal Constraint Networks",
        "authors": [
            "Georg Gottlob"
        ],
        "abstract": "In a minimal binary constraint network, every tuple of a constraint relation can be extended to a solution. The tractability or intractability of computing a solution to such a minimal network was a long standing open question. Dechter conjectured this computation problem to be NP-hard. We prove this conjecture. We also prove a conjecture by Dechter and Pearl stating that for k\\geq2 it is NP-hard to decide whether a single constraint can be decomposed into an equivalent k-ary constraint network. We show that this holds even in case of bi-valued constraints where k\\geq3, which proves another conjecture of Dechter and Pearl. Finally, we establish the tractability frontier for this problem with respect to the domain cardinality and the parameter k.\n    ",
        "submission_date": "2011-03-08T00:00:00",
        "last_modified_date": "2012-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.1711",
        "title": "Planning Graph Heuristics for Belief Space Search",
        "authors": [
            "D. Bryce",
            "S. Kambhampati",
            "D. E. Smith"
        ],
        "abstract": "Some recent works in conditional planning have proposed reachability heuristics to improve planner scalability, but many lack a formal description of the properties of their distance estimates. To place previous work in context and extend work on heuristics for conditional planning, we provide a formal basis for distance estimates between belief states.  We give a definition for the distance between belief states that relies on aggregating underlying state distance measures.  We give several techniques to aggregate state distances and their associated properties.  Many existing heuristics exhibit a subset of the properties, but in order to provide a standardized comparison we present several generalizations of planning graph heuristics that are used in a single planner.  We compliment our belief state distance estimate framework by also investigating efficient planning graph data structures that incorporate BDDs to compute the most effective heuristics.\nWe developed two planners to serve as test-beds for our investigation.  The first, CAltAlt, is a conformant regression planner that uses A* search.  The second, POND, is a conditional progression planner that uses AO* search. We show the relative effectiveness of our heuristic techniques within these planners. We also compare the performance of these planners with several state of the art approaches in conditional planning. \n    ",
        "submission_date": "2011-03-09T00:00:00",
        "last_modified_date": "2011-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.2091",
        "title": "An Artificial Immune System Model for Multi-Agents Resource Sharing in Distributed Environments",
        "authors": [
            "Tejbanta Singh Chingtham",
            "G. Sahoo",
            "M.K. Ghose"
        ],
        "abstract": "Natural Immune system plays a vital role in the survival of the all living being. It provides a mechanism to defend itself from external predates making it consistent systems, capable of adapting itself for survival incase of changes. The human immune system has motivated scientists and engineers for finding powerful information processing algorithms that has solved complex engineering tasks. This paper explores one of the various possibilities for solving problem in a Multiagent scenario wherein multiple robots are deployed to achieve a goal collectively. The final goal is dependent on the performance of individual robot and its survival without having to lose its energy beyond a predetermined threshold value by deploying an evolutionary computational technique otherwise called the artificial immune system that imitates the biological immune system.\n    ",
        "submission_date": "2011-02-24T00:00:00",
        "last_modified_date": "2011-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.2342",
        "title": "SPPAM - Statistical PreProcessing AlgorithM",
        "authors": [
            "Tiago Silva",
            "In\u00eas Dutra"
        ],
        "abstract": "Most machine learning tools work with a single table where each row is an instance and each column is an attribute. Each cell of the table contains an attribute value for an instance. This representation prevents one important form of learning, which is, classification based on groups of correlated records, such as multiple exams of a single patient, internet customer preferences, weather forecast or prediction of sea conditions for a given day. To some extent, relational learning methods, such as inductive logic programming, can capture this correlation through the use of intensional predicates added to the background knowledge. In this work, we propose SPPAM, an algorithm that aggregates past observations in one single record. We show that applying SPPAM to the original correlated data, before the learning task, can produce classifiers that are better than the ones trained using all records.\n    ",
        "submission_date": "2011-03-11T00:00:00",
        "last_modified_date": "2011-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.2376",
        "title": "Language, Emotions, and Cultures: Emotional Sapir-Whorf Hypothesis",
        "authors": [
            "Leonid Perlovsky"
        ],
        "abstract": "An emotional version of Sapir-Whorf hypothesis suggests that differences in language emotionalities influence differences among cultures no less than conceptual differences. Conceptual contents of languages and cultures to significant extent are determined by words and their semantic differences; these could be borrowed among languages and exchanged among cultures. Emotional differences, as suggested in the paper, are related to grammar and mostly cannot be borrowed. Conceptual and emotional mechanisms of languages are considered here along with their functions in the mind and cultural evolution. A fundamental contradiction in human mind is considered: language evolution requires reduced emotionality, but \"too low\" emotionality makes language \"irrelevant to life,\" disconnected from sensory-motor experience. Neural mechanisms of these processes are suggested as well as their mathematical models: the knowledge instinct, the language instinct, the dual model connecting language and cognition, dynamic logic, neural modeling fields. Mathematical results are related to cognitive science, linguistics, and psychology. Experimental evidence and theoretical arguments are discussed. Approximate equations for evolution of human minds and cultures are obtained. Their solutions identify three types of cultures: \"conceptual\"-pragmatic cultures, in which emotionality of language is reduced and differentiation overtakes synthesis resulting in fast evolution at the price of uncertainty of values, self doubts, and internal crises; \"traditional-emotional\" cultures where differentiation lags behind synthesis, resulting in cultural stability at the price of stagnation; and \"multi-cultural\" societies combining fast cultural evolution and stability. Unsolved problems and future theoretical and experimental directions are discussed.\n    ",
        "submission_date": "2011-03-11T00:00:00",
        "last_modified_date": "2011-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.3123",
        "title": "Reduced Ordered Binary Decision Diagram with Implied Literals: A New knowledge Compilation Approach",
        "authors": [
            "Yong Lai",
            "Dayou Liu",
            "Shengsheng Wang"
        ],
        "abstract": "Knowledge compilation is an approach to tackle the computational intractability of general reasoning problems. According to this approach, knowledge bases are converted off-line into a target compilation language which is tractable for on-line querying. Reduced ordered binary decision diagram (ROBDD) is one of the most influential target languages. We generalize ROBDD by associating some implied literals in each node and the new language is called reduced ordered binary decision diagram with implied literals (ROBDD-L). Then we discuss a kind of subsets of ROBDD-L called ROBDD-i with precisely i implied literals (0 \\leq i \\leq \\infty). In particular, ROBDD-0 is isomorphic to ROBDD; ROBDD-\\infty requires that each node should be associated by the implied literals as many as possible. We show that ROBDD-i has uniqueness over some specific variables order, and ROBDD-\\infty is the most succinct subset in ROBDD-L and can meet most of the querying requirements involved in the knowledge compilation map. Finally, we propose an ROBDD-i compilation algorithm for any i and a ROBDD-\\infty compilation algorithm. Based on them, we implement a ROBDD-L package called BDDjLu and then get some conclusions from preliminary experimental results: ROBDD-\\infty is obviously smaller than ROBDD for all benchmarks; ROBDD-\\infty is smaller than the d-DNNF the benchmarks whose compilation results are relatively small; it seems that it is better to transform ROBDDs-\\infty into FBDDs and ROBDDs rather than straight compile the benchmarks.\n    ",
        "submission_date": "2011-03-16T00:00:00",
        "last_modified_date": "2011-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.3223",
        "title": "Using Soft Computer Techniques on Smart Devices for Monitoring Chronic Diseases: the CHRONIOUS case",
        "authors": [
            "Piero Giacomelli",
            "Giulia Munaro",
            "Roberto Rosso"
        ],
        "abstract": "CHRONIOUS is an Open, Ubiquitous and Adaptive Chronic Disease Management Platform for Chronic Obstructive Pulmonary Disease(COPD) Chronic Kidney Disease (CKD) and Renal Insufficiency. It consists of several modules: an ontology based literature search engine, a rule based decision support system, remote sensors interacting with lifestyle interfaces (PDA, monitor touchscreen) and a machine learning module. All these modules interact each other to allow the monitoring of two types of chronic diseases and to help clinician in taking decision for cure purpose. This paper illustrates how some machine learning algorithms and a rule based decision support system can be used in smart devices, to monitor chronic patient. We will analyse how a set of machine learning algorithms can be used in smart devices to alert the clinician in case of a patient health condition worsening trend.\n    ",
        "submission_date": "2011-03-16T00:00:00",
        "last_modified_date": "2011-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.3240",
        "title": "Decentralized Constraint Satisfaction",
        "authors": [
            "K. R. Duffy",
            "C. Bordenave",
            "D. J. Leith"
        ],
        "abstract": "We show that several important resource allocation problems in wireless networks fit within the common framework of Constraint Satisfaction Problems (CSPs). Inspired by the requirements of these applications, where variables are located at distinct network devices that may not be able to communicate but may interfere, we define natural criteria that a CSP solver must possess in order to be practical. We term these algorithms decentralized CSP solvers. The best known CSP solvers were designed for centralized problems and do not meet these criteria. We introduce a stochastic decentralized CSP solver and prove that it will find a solution in almost surely finite time, should one exist, also showing it has many practically desirable properties. We benchmark the algorithm's performance on a well-studied class of CSPs, random k-SAT, illustrating that the time the algorithm takes to find a satisfying assignment is competitive with stochastic centralized solvers on problems with order a thousand variables despite its decentralized nature. We demonstrate the solver's practical utility for the problems that motivated its introduction by using it to find a non-interfering channel allocation for a network formed from data from downtown Manhattan.\n    ",
        "submission_date": "2011-03-02T00:00:00",
        "last_modified_date": "2012-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.3417",
        "title": "Finding Shortest Path for Developed Cognitive Map Using Medial Axis",
        "authors": [
            "Hazim A. Farhan",
            "Hussein H. Owaied",
            "Suhaib I. Al-Ghazi"
        ],
        "abstract": "this paper presents an enhancement of the medial axis algorithm to be used for finding the optimal shortest path for developed cognitive map. The cognitive map has been developed, based on the architectural blueprint maps. The idea for using the medial-axis is to find main path central pixels; each center pixel represents the center distance between two side boarder pixels. The need for these pixels in the algorithm comes from the need of building a network of nodes for the path, where each node represents a turning in the real world (left, right, critical left, critical right...). The algorithm also ignores from finding the center pixels paths that are too small for intelligent robot navigation. The Idea of this algorithm is to find the possible shortest path between start and end points. The goal of this research is to extract a simple, robust representation of the shape of the cognitive map together with the optimal shortest path between start and end points. The intelligent robot will use this algorithm in order to decrease the time that is needed for sweeping the targeted building.\n    ",
        "submission_date": "2011-03-17T00:00:00",
        "last_modified_date": "2011-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.3420",
        "title": "Extraction of handwritten areas from colored image of bank checks by an hybrid method",
        "authors": [
            "Sofiene Haboubi",
            "Samia Maddouri"
        ],
        "abstract": "One of the first step in the realization of an automatic system of check recognition is the extraction of the handwritten area. We propose in this paper an hybrid method to extract these areas. This method is based on digit recognition by Fourier descriptors and different steps of colored image processing . It requires the bank recognition of its code which is located in the check marking band as well as the handwritten color recognition by the method of difference of histograms. The areas extraction is then carried out by the use of some mathematical morphology tools.\n    ",
        "submission_date": "2011-03-17T00:00:00",
        "last_modified_date": "2011-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.3430",
        "title": "Identification of arabic word from bilingual text using character features",
        "authors": [
            "Sofiene Haboubi",
            "Samia Maddouri",
            "Hamid Amiri"
        ],
        "abstract": "The identification of the language of the script is an important stage in the process of recognition of the writing. There are several works in this research area, which treat various languages. Most of the used methods are global or statistical. In this present paper, we study the possibility of using the features of scripts to identify the language. The identification of the language of the script by characteristics returns the identification in the case of multilingual documents less difficult. We present by this work, a study on the possibility of using the structural features to identify the Arabic language from an Arabic / Latin text.\n    ",
        "submission_date": "2011-03-17T00:00:00",
        "last_modified_date": "2011-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.3687",
        "title": "Cost Based Satisficing Search Considered Harmful",
        "authors": [
            "William Cushing",
            "J. Benton",
            "Subbarao Kambhampati"
        ],
        "abstract": "Recently, several researchers have found that cost-based satisficing search with A* often runs into problems. Although some \"work arounds\" have been proposed to ameliorate the problem, there has not been any concerted effort to pinpoint its origin. In this paper, we argue that the origins can be traced back to the wide variance in action costs that is observed in most planning domains. We show that such cost variance misleads A* search, and that this is no trifling detail or accidental phenomenon, but a systemic weakness of the very concept of \"cost-based evaluation functions + systematic search + combinatorial graphs\". We show that satisficing search with sized-based evaluation functions is largely immune to this problem.\n    ",
        "submission_date": "2011-03-18T00:00:00",
        "last_modified_date": "2011-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.3745",
        "title": "The AllDifferent Constraint with Precedences",
        "authors": [
            "Christian Bessiere",
            "Nina Narodytska",
            "Claude-Guy Quimper",
            "Toby Walsh"
        ],
        "abstract": "We propose AllDiffPrecedence, a new global constraint that combines together an AllDifferent constraint with precedence constraints that strictly order given pairs of variables. We identify a number of applications for this global constraint including instruction scheduling and symmetry breaking. We give an efficient propagation algorithm that enforces bounds consistency on this global constraint. We show how to implement this propagator using a decomposition that extends the bounds consistency enforcing decomposition proposed for the AllDifferent constraint. Finally, we prove that enforcing domain consistency on this global constraint is NP-hard in general.\n    ",
        "submission_date": "2011-03-19T00:00:00",
        "last_modified_date": "2011-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.3904",
        "title": "Informed Heuristics for Guiding Stem-and-Cycle Ejection Chains",
        "authors": [
            "Daniel Harabor",
            "Philip Kilby"
        ],
        "abstract": "The state of the art in local search for the Traveling Salesman Problem is dominated by ejection chain methods utilising the Stem-and-Cycle reference structure. Though effective such algorithms employ very little information in their successor selection strategy, typically seeking only to minimise the cost of a move. We propose an alternative approach inspired from the AI literature and show how an admissible heuristic can be used to guide successor selection. We undertake an empirical analysis and demonstrate that this technique often produces better results than less informed strategies albeit at the cost of running in higher polynomial time.\n    ",
        "submission_date": "2011-03-21T00:00:00",
        "last_modified_date": "2011-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.3949",
        "title": "A Goal-Directed Implementation of Query Answering for Hybrid MKNF Knowledge Bases",
        "authors": [
            "Ana Sofia Gomes",
            "Jose Julio Alferes",
            "Terrance Swift"
        ],
        "abstract": "Ontologies and rules are usually loosely coupled in knowledge representation formalisms. In fact, ontologies use open-world reasoning while the leading semantics for rules use non-monotonic, closed-world reasoning. One exception is the tightly-coupled framework of Minimal Knowledge and Negation as Failure (MKNF), which allows statements about individuals to be jointly derived via entailment from an ontology and inferences from rules. Nonetheless, the practical usefulness of MKNF has not always been clear, although recent work has formalized a general resolution-based method for querying MKNF when rules are taken to have the well-founded semantics, and the ontology is modeled by a general oracle. That work leaves open what algorithms should be used to relate the entailments of the ontology and the inferences of rules. In this paper we provide such algorithms, and describe the implementation of a query-driven system, CDF-Rules, for hybrid knowledge bases combining both (non-monotonic) rules under the well-founded semantics and a (monotonic) ontology, represented by a CDF Type-1 (ALQ) theory. To appear in Theory and Practice of Logic Programming (TPLP)\n    ",
        "submission_date": "2011-03-21T00:00:00",
        "last_modified_date": "2012-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.3954",
        "title": "BoolVar/PB v1.0, a java library for translating pseudo-Boolean constraints into CNF formulae",
        "authors": [
            "Olivier Bailleux"
        ],
        "abstract": "BoolVar/PB is an open source java library dedicated to the translation of pseudo-Boolean constraints into CNF formulae. Input constraints can be categorized with tags. Several encoding schemes are implemented in a way that each input constraint can be translated using one or several encoders, according to the related tags. The library can be easily extended by adding new encoders and / or new output formats.\n    ",
        "submission_date": "2011-03-21T00:00:00",
        "last_modified_date": "2011-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.4558",
        "title": "Representing First-Order Causal Theories by Logic Programs",
        "authors": [
            "Paolo Ferraris",
            "Joohyung Lee",
            "Yuliya Lierler",
            "Vladimir Lifschitz",
            "Fangkai Yang"
        ],
        "abstract": "Nonmonotonic causal logic, introduced by Norman McCain and Hudson Turner, became a basis for the semantics of several expressive action languages. McCain's embedding of definite propositional causal theories into logic programming paved the way to the use of answer set solvers for answering queries about actions described in such languages. In this paper we extend this embedding to nondefinite theories and to first-order causal logic.\n    ",
        "submission_date": "2011-03-23T00:00:00",
        "last_modified_date": "2011-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.5034",
        "title": "On Understanding and Machine Understanding",
        "authors": [
            "Tong Chern"
        ],
        "abstract": "In the present paper, we try to propose a self-similar network theory for the basic understanding. By extending the natural languages to a kind of so called idealy sufficient language, we can proceed a few steps to the investigation of the language searching and the language understanding of AI.\n",
        "submission_date": "2011-03-24T00:00:00",
        "last_modified_date": "2018-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.5708",
        "title": "Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments",
        "authors": [
            "Yi Sun",
            "Faustino Gomez",
            "Juergen Schmidhuber"
        ],
        "abstract": "To maximize its success, an AGI typically needs to explore its initially unknown world. Is there an optimal way of doing so? Here we derive an affirmative answer for a broad class of environments.\n    ",
        "submission_date": "2011-03-29T00:00:00",
        "last_modified_date": "2011-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.0843",
        "title": "Phase Transitions in Knowledge Compilation: an Experimental Study",
        "authors": [
            "Jian Gao",
            "Minghao Yin",
            "Ke Xu"
        ],
        "abstract": "Phase transitions in many complex combinational problems have been widely studied in the past decade. In this paper, we investigate phase transitions in the knowledge compilation empirically, where DFA, OBDD and d-DNNF are chosen as the target languages to compile random k-SAT instances. We perform intensive experiments to analyze the sizes of compilation results and draw the following conclusions: there exists an easy-hard-easy pattern in compilations; the peak point of sizes in the pattern is only related to the ratio of the number of clauses to that of variables when k is fixed, regardless of target languages; most sizes of compilation results increase exponentially with the number of variables growing, but there also exists a phase transition that separates a polynomial-increment region from the exponential-increment region; Moreover, we explain why the phase transition in compilations occurs by analyzing microstructures of DFAs, and conclude that a kind of solution interchangeability with more than 2 variables has a sharp transition near the peak point of the easy-hard-easy pattern, and thus it has a great impact on sizes of DFAs.\n    ",
        "submission_date": "2011-04-05T00:00:00",
        "last_modified_date": "2011-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.1045",
        "title": "Tractable Set Constraints",
        "authors": [
            "Manuel Bodirsky",
            "Martin Hils",
            "Alex Krimkevich"
        ],
        "abstract": "Many fundamental problems in artificial intelligence, knowledge representation, and verification involve reasoning about sets and relations between sets and can be modeled as set constraint satisfaction problems (set CSPs). Such problems are frequently intractable, but there are several important set CSPs that are known to be polynomial-time tractable. We introduce a large class of set CSPs that can be solved in quadratic time. Our class, which we call EI, contains all previously known tractable set CSPs, but also some new ones that are of crucial importance for example in description logics. The class of EI set constraints has an elegant universal-algebraic characterization, which we use to show that every set constraint language that properly contains all EI set constraints already has a finite sublanguage with an NP-hard constraint satisfaction problem.\n    ",
        "submission_date": "2011-04-06T00:00:00",
        "last_modified_date": "2012-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.1677",
        "title": "Automatic Vehicle Checking Agent (VCA)",
        "authors": [
            "Bashir Ahmad",
            "Shakeel Ahmad",
            "Shahid Hussain",
            "Muhammad Zaheer Aslam",
            "Zafar Abbas"
        ],
        "abstract": "A definition of intelligence is given in terms of performance that can be quantitatively measured. In this study, we have presented a conceptual model of Intelligent Agent System for Automatic Vehicle Checking Agent (VCA). To achieve this goal, we have introduced several kinds of agents that exhibit intelligent features. These are the Management agent, internal agent, External Agent, Watcher agent and Report agent. Metrics and measurements are suggested for evaluating the performance of Automatic Vehicle Checking Agent (VCA). Calibrate data and test facilities are suggested to facilitate the development of intelligent systems.\n    ",
        "submission_date": "2011-04-09T00:00:00",
        "last_modified_date": "2011-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.1678",
        "title": "A Proposed Decision Support System/Expert System for Guiding Fresh Students in Selecting a Faculty in Gomal University, Pakistan",
        "authors": [
            "Muhammad Zaheer Aslam",
            "Nasimullah",
            "Abdur Rashid Khan"
        ],
        "abstract": "This paper presents the design and development of a proposed rule based Decision Support System that will help students in selecting the best suitable faculty/major decision while taking admission in Gomal University, Dera Ismail Khan, Pakistan. The basic idea of our approach is to design a model for testing and measuring the student capabilities like intelligence, understanding, comprehension, mathematical concepts plus his/her past academic record plus his/her intelligence level, and applying the module results to a rule-based decision support system to determine the compatibility of those capabilities with the available faculties/majors in Gomal University. The result is shown as a list of suggested faculties/majors with the student capabilities and abilities.\n    ",
        "submission_date": "2011-04-09T00:00:00",
        "last_modified_date": "2012-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.1924",
        "title": "Rational Deployment of CSP Heuristics",
        "authors": [
            "David Tolpin",
            "Solomon Eyal Shimony"
        ],
        "abstract": "Heuristics are crucial tools in decreasing search effort in varied fields of AI. In order to be effective, a heuristic must be efficient to compute, as well as provide useful information to the search algorithm. However, some well-known heuristics which do well in reducing backtracking are so heavy that the gain of deploying them in a search algorithm might be outweighed by their overhead.\n",
        "submission_date": "2011-04-11T00:00:00",
        "last_modified_date": "2011-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.2018",
        "title": "Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression",
        "authors": [
            "Sham Kakade",
            "Adam Tauman Kalai",
            "Varun Kanade",
            "Ohad Shamir"
        ],
        "abstract": "Generalized Linear Models (GLMs) and Single Index Models (SIMs) provide powerful generalizations of linear regression, where the target variable is assumed to be a (possibly unknown) 1-dimensional function of a linear predictor. In general, these problems entail non-convex estimation procedures, and, in practice, iterative local search heuristics are often used. Kalai and Sastry (2009) recently provided the first provably efficient method for learning SIMs and GLMs, under the assumptions that the data are in fact generated under a GLM and under certain monotonicity and Lipschitz constraints. However, to obtain provable performance, the method requires a fresh sample every iteration. In this paper, we provide algorithms for learning GLMs and SIMs, which are both computationally and statistically efficient. We also provide an empirical study, demonstrating their feasibility in practice.\n    ",
        "submission_date": "2011-04-11T00:00:00",
        "last_modified_date": "2011-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.2444",
        "title": "A Simplified and Improved Free-Variable Framework for Hilbert's epsilon as an Operator of Indefinite Committed Choice",
        "authors": [
            "Claus-Peter Wirth"
        ],
        "abstract": "Free variables occur frequently in mathematics and computer science with ad hoc and altering semantics. We present the most recent version of our free-variable framework for two-valued logics with properly improved functionality, but only two kinds of free variables left (instead of three): implicitly universally and implicitly existentially quantified ones, now simply called \"free atoms\" and \"free variables\", respectively. The quantificational expressiveness and the problem-solving facilities of our framework exceed standard first-order and even higher-order modal logics, and directly support Fermat's descente infinie. With the improved version of our framework, we can now model also Henkin quantification, neither using quantifiers (binders) nor raising (Skolemization). We propose a new semantics for Hilbert's epsilon as a choice operator with the following features: We avoid overspecification (such as right-uniqueness), but admit indefinite choice, committed choice, and classical logics. Moreover, our semantics for the epsilon supports reductive proof search optimally.\n    ",
        "submission_date": "2011-04-13T00:00:00",
        "last_modified_date": "2024-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.2541",
        "title": "Kernels for Global Constraints",
        "authors": [
            "Serge Gaspers",
            "Stefan Szeider"
        ],
        "abstract": "Bessiere et al. (AAAI'08) showed that several intractable global constraints can be efficiently propagated when certain natural problem parameters are small. In particular, the complete propagation of a global constraint is fixed-parameter tractable in k - the number of holes in domains - whenever bound consistency can be enforced in polynomial time; this applies to the global constraints AtMost-NValue and Extended Global Cardinality (EGC).\n",
        "submission_date": "2011-04-13T00:00:00",
        "last_modified_date": "2011-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.2842",
        "title": "Augmenting Tractable Fragments of Abstract Argumentation",
        "authors": [
            "Sebastian Ordyniak",
            "Stefan Szeider"
        ],
        "abstract": "We present a new and compelling approach to the efficient solution of important computational problems that arise in the context of abstract argumentation. Our approach makes known algorithms defined for restricted fragments generally applicable, at a computational cost that scales with the distance from the fragment. Thus, in a certain sense, we gradually augment tractable fragments. Surprisingly, it turns out that some tractable fragments admit such an augmentation and that others do not.\n",
        "submission_date": "2011-04-14T00:00:00",
        "last_modified_date": "2011-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.3152",
        "title": "Polyethism in a colony of artificial ants",
        "authors": [
            "Chris Marriott",
            "Carlos Gershenson"
        ],
        "abstract": "We explore self-organizing strategies for role assignment in a foraging task carried out by a colony of artificial agents. Our strategies are inspired by various mechanisms of division of labor (polyethism) observed in eusocial insects like ants, termites, or bees. Specifically we instantiate models of caste polyethism and age or temporal polyethism to evaluated the benefits to foraging in a dynamic environment. Our experiment is directly related to the exploration/exploitation trade of in machine learning.\n    ",
        "submission_date": "2011-04-15T00:00:00",
        "last_modified_date": "2011-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.3250",
        "title": "Adding noise to the input of a model trained with a regularized objective",
        "authors": [
            "Salah Rifai",
            "Xavier Glorot",
            "Yoshua Bengio",
            "Pascal Vincent"
        ],
        "abstract": "Regularization is a well studied problem in the context of neural networks. It is usually used to improve the generalization performance when the number of input samples is relatively small or heavily contaminated with noise. The regularization of a parametric model can be achieved in different manners some of which are early stopping (Morgan and Bourlard, 1990), weight decay, output smoothing that are used to avoid overfitting during the training of the considered model. From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters (Krogh and Hertz, 1991). Using Bishop's approximation (Bishop, 1995) of the objective function when a restricted type of noise is added to the input of a parametric function, we derive the higher order terms of the Taylor expansion and analyze the coefficients of the regularization terms induced by the noisy input. In particular we study the effect of penalizing the Hessian of the mapping function with respect to the input in terms of generalization performance. We also show how we can control independently this coefficient by explicitly penalizing the Jacobian of the mapping function on corrupted inputs.\n    ",
        "submission_date": "2011-04-16T00:00:00",
        "last_modified_date": "2011-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.3344",
        "title": "Quantum Structure in Cognition: Fundamentals and Applications",
        "authors": [
            "Diederik Aerts",
            "Liane Gabora",
            "Sandro Sozzo",
            "Tomas Veloz"
        ],
        "abstract": "Experiments in cognitive science and decision theory show that the ways in which people combine concepts and make decisions cannot be described by classical logic and probability theory. This has serious implications for applied disciplines such as information retrieval, artificial intelligence and robotics. Inspired by a mathematical formalism that generalizes quantum mechanics the authors have constructed a contextual framework for both concept representation and decision making, together with quantum models that are in strong alignment with experimental data. The results can be interpreted by assuming the existence in human thought of a double-layered structure, a 'classical logical thought' and a 'quantum conceptual thought', the latter being responsible of the above paradoxes and nonclassical effects. The presence of a quantum structure in cognition is relevant, for it shows that quantum mechanics provides not only a useful modeling tool for experimental data but also supplies a structural model for human and artificial thought processes. This approach has strong connections with theories formalizing meaning, such as semantic analysis, and has also a deep impact on computer science, information retrieval and artificial intelligence. More specifically, the links with information retrieval are discussed in this paper.\n    ",
        "submission_date": "2011-04-17T00:00:00",
        "last_modified_date": "2011-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.3345",
        "title": "Quantum Interaction Approach in Cognition, Artificial Intelligence and Robotics",
        "authors": [
            "Diederik Aerts",
            "Marek Czachor",
            "Sandro Sozzo"
        ],
        "abstract": "The mathematical formalism of quantum mechanics has been successfully employed in the last years to model situations in which the use of classical structures gives rise to problematical situations, and where typically quantum effects, such as 'contextuality' and 'entanglement', have been recognized. This 'Quantum Interaction Approach' is briefly reviewed in this paper focusing, in particular, on the quantum models that have been elaborated to describe how concepts combine in cognitive science, and on the ensuing identification of a quantum structure in human thought. We point out that these results provide interesting insights toward the development of a unified theory for meaning and knowledge formalization and representation. Then, we analyze the technological aspects and implications of our approach, and a particular attention is devoted to the connections with symbolic artificial intelligence, quantum computation and robotics.\n    ",
        "submission_date": "2011-04-17T00:00:00",
        "last_modified_date": "2011-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.3904",
        "title": "An expert system for detecting automobile insurance fraud using social network analysis",
        "authors": [
            "Lovro \u0160ubelj",
            "\u0160tefan Furlan",
            "Marko Bajec"
        ],
        "abstract": "The article proposes an expert system for detection, and subsequent investigation, of groups of collaborating automobile insurance fraudsters. The system is described and examined in great detail, several technical difficulties in detecting fraud are also considered, for it to be applicable in practice. Opposed to many other approaches, the system uses networks for representation of data. Networks are the most natural representation of such a relational domain, allowing formulation and analysis of complex relations between entities. Fraudulent entities are found by employing a novel assessment algorithm, \\textit{Iterative Assessment Algorithm} (\\textit{IAA}), also presented in the article. Besides intrinsic attributes of entities, the algorithm explores also the relations between entities. The prototype was evaluated and rigorously analyzed on real world data. Results show that automobile insurance fraud can be efficiently detected with the proposed system and that appropriate data representation is vital.\n    ",
        "submission_date": "2011-04-19T00:00:00",
        "last_modified_date": "2011-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.3927",
        "title": "Translation-based Constraint Answer Set Solving",
        "authors": [
            "Christian Drescher",
            "Toby Walsh"
        ],
        "abstract": "We solve constraint satisfaction problems through translation to answer set programming (ASP). Our reformulations have the property that unit-propagation in the ASP solver achieves well defined local consistency properties like arc, bound and range consistency. Experiments demonstrate the computational value of this approach.\n    ",
        "submission_date": "2011-04-20T00:00:00",
        "last_modified_date": "2011-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.3929",
        "title": "Understanding Exhaustive Pattern Learning",
        "authors": [
            "Libin Shen"
        ],
        "abstract": "Pattern learning in an important problem in Natural Language Processing (NLP). Some exhaustive pattern learning (EPL) methods (Bod, 1992) were proved to be flawed (Johnson, 2002), while similar algorithms (Och and Ney, 2004) showed great advantages on other tasks, such as machine translation. In this article, we first formalize EPL, and then show that the probability given by an EPL model is constant-factor approximation of the probability given by an ensemble method that integrates exponential number of models obtained with various segmentations of the training data. This work for the first time provides theoretical justification for the widely used EPL algorithm in NLP, which was previously viewed as a flawed heuristic method. Better understanding of EPL may lead to improved pattern learning algorithms in future.\n    ",
        "submission_date": "2011-04-20T00:00:00",
        "last_modified_date": "2011-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.4053",
        "title": "On the evolution of the instance level of DL-lite knowledge bases",
        "authors": [
            "Maurizio Lenzerini",
            "Domenico Fabio Savo"
        ],
        "abstract": "Recent papers address the issue of updating the instance level of knowledge bases expressed in Description Logic following a model-based approach. One of the outcomes of these papers is that the result of updating a knowledge base K is generally not expressible in the Description Logic used to express K. In this paper we introduce a formula-based approach to this problem, by revisiting some research work on formula-based updates developed in the '80s, in particular the WIDTIO (When In Doubt, Throw It Out) approach. We show that our operator enjoys desirable properties, including that both insertions and deletions according to such operator can be expressed in the DL used for the original KB. Also, we present polynomial time algorithms for the evolution of the instance level knowledge bases expressed in the most expressive Description Logics of the DL-lite family.\n    ",
        "submission_date": "2011-04-20T00:00:00",
        "last_modified_date": "2011-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.4153",
        "title": "Learning invariant features through local space contraction",
        "authors": [
            "Salah Rifai",
            "Xavier Muller",
            "Xavier Glorot",
            "Gregoire Mesnil",
            "Yoshua Bengio",
            "Pascal Vincent"
        ],
        "abstract": "We present in this paper a novel approach for training deterministic auto-encoders. We show that by adding a well chosen penalty term to the classical reconstruction cost function, we can achieve results that equal or surpass those attained by other regularized auto-encoders as well as denoising auto-encoders on a range of datasets. This penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. We show that this penalty term results in a localized space contraction which in turn yields robust features on the activation layer. Furthermore, we show how this penalty term is related to both regularized auto-encoders and denoising encoders and how it can be seen as a link between deterministic and non-deterministic auto-encoders. We find empirically that this penalty helps to carve a representation that better captures the local directions of variation dictated by the data, corresponding to a lower-dimensional non-linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold. Finally, we show that by using the learned features to initialize a MLP, we achieve state of the art classification error on a range of datasets, surpassing other methods of pre-training.\n    ",
        "submission_date": "2011-04-21T00:00:00",
        "last_modified_date": "2011-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.4290",
        "title": "Algorithms and Complexity Results for Persuasive Argumentation",
        "authors": [
            "Eun Jung Kim",
            "Sebastian Ordyniak",
            "Stefan Szeider"
        ],
        "abstract": "The study of arguments as abstract entities and their interaction as introduced by Dung (Artificial Intelligence 177, 1995) has become one of the most active research branches within Artificial Intelligence and Reasoning. A main issue for abstract argumentation systems is the selection of acceptable sets of arguments. Value-based argumentation, as introduced by Bench-Capon (J. Logic Comput. 13, 2003), extends Dung's framework. It takes into account the relative strength of arguments with respect to some ranking representing an audience: an argument is subjectively accepted if it is accepted with respect to some audience, it is objectively accepted if it is accepted with respect to all audiences. Deciding whether an argument is subjectively or objectively accepted, respectively, are computationally intractable problems. In fact, the problems remain intractable under structural restrictions that render the main computational problems for non-value-based argumentation systems tractable. In this paper we identify nontrivial classes of value-based argumentation systems for which the acceptance problems are polynomial-time tractable. The classes are defined by means of structural restrictions in terms of the underlying graphical structure of the value-based system. Furthermore we show that the acceptance problems are intractable for two classes of value-based systems that where conjectured to be tractable by Dunne (Artificial Intelligence 171, 2007).\n    ",
        "submission_date": "2011-04-21T00:00:00",
        "last_modified_date": "2011-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.4617",
        "title": "Boolean Equi-propagation for Optimized SAT Encoding",
        "authors": [
            "Amit Metodi",
            "Michael Codish",
            "Vitaly Lagoon",
            "Peter J. Stuckey"
        ],
        "abstract": "We present an approach to propagation based solving, Boolean equi-propagation, where constraints are modelled as propagators of information about equalities between Boolean literals. Propagation based solving applies this information as a form of partial evaluation resulting in optimized SAT encodings. We demonstrate for a variety of benchmarks that our approach results in smaller CNF encodings and leads to speed-ups in solving times.\n    ",
        "submission_date": "2011-04-24T00:00:00",
        "last_modified_date": "2011-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.4910",
        "title": "Hybrid Tractable Classes of Binary Quantified Constraint Satisfaction Problems",
        "authors": [
            "Jian Gao",
            "Minghao Yin",
            "Junping Zhou"
        ],
        "abstract": "In this paper, we investigate the hybrid tractability of binary Quantified Constraint Satisfaction Problems (QCSPs). First, a basic tractable class of binary QCSPs is identified by using the broken-triangle property. In this class, the variable ordering for the broken-triangle property must be same as that in the prefix of the QCSP. Second, we break this restriction to allow that existentially quantified variables can be shifted within or out of their blocks, and thus identify some novel tractable classes by introducing the broken-angle property. Finally, we identify a more generalized tractable class, i.e., the min-of-max extendable class for QCSPs.\n    ",
        "submission_date": "2011-04-26T00:00:00",
        "last_modified_date": "2011-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.4950",
        "title": "A Machine Learning Based Analytical Framework for Semantic Annotation Requirements",
        "authors": [
            "Hamed Hassanzadeh",
            "MohammadReza Keyvanpour"
        ],
        "abstract": "The Semantic Web is an extension of the current web in which information is given well-defined meaning. The perspective of Semantic Web is to promote the quality and intelligence of the current web by changing its contents into machine understandable form. Therefore, semantic level information is one of the cornerstones of the Semantic Web. The process of adding semantic metadata to web resources is called Semantic Annotation. There are many obstacles against the Semantic Annotation, such as multilinguality, scalability, and issues which are related to diversity and inconsistency in content of different web pages. Due to the wide range of domains and the dynamic environments that the Semantic Annotation systems must be performed on, the problem of automating annotation process is one of the significant challenges in this domain. To overcome this problem, different machine learning approaches such as supervised learning, unsupervised learning and more recent ones like, semi-supervised learning and active learning have been utilized. In this paper we present an inclusive layered classification of Semantic Annotation challenges and discuss the most important issues in this field. Also, we review and analyze machine learning applications for solving semantic annotation problems. For this goal, the article tries to closely study and categorize related researches for better understanding and to reach a framework that can map machine learning techniques into the Semantic Annotation challenges and requirements.\n    ",
        "submission_date": "2011-04-26T00:00:00",
        "last_modified_date": "2011-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.4966",
        "title": "Combining Ontology Development Methodologies and Semantic Web Platforms for E-government Domain Ontology Development",
        "authors": [
            "Jean Vincent Fonou Dombeu",
            "Magda Huisman"
        ],
        "abstract": "One of the key challenges in electronic government (e-government) is the development of systems that can be easily integrated and interoperated to provide seamless services delivery to citizens. In recent years, Semantic Web technologies based on ontology have emerged as promising solutions to the above engineering problems. However, current research practicing semantic development in e-government does not focus on the application of available methodologies and platforms for developing government domain ontologies. Furthermore, only a few of these researches provide detailed guidelines for developing semantic ontology models from a government service domain. This research presents a case study combining an ontology building methodology and two state-of-the-art Semantic Web platforms namely Protege and Java Jena ontology API for semantic ontology development in e-government. Firstly, a framework adopted from the Uschold and King ontology building methodology is employed to build a domain ontology describing the semantic content of a government service domain. Thereafter, UML is used to semi-formally represent the domain ontology. Finally, Protege and Jena API are employed to create the Web Ontology Language (OWL) and Resource Description Framework (RDF) representations of the domain ontology respectively to enable its computer processing. The study aims at: (1) providing e-government developers, particularly those from the developing world with detailed guidelines for practicing semantic content development in their e-government projects and (2), strengthening the adoption of semantic technologies in e-government. The study would also be of interest to novice Semantic Web developers who might used it as a starting point for further investigations.\n    ",
        "submission_date": "2011-04-26T00:00:00",
        "last_modified_date": "2011-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.4993",
        "title": "Arc Consistency and Friends",
        "authors": [
            "Hubie Chen",
            "Victor Dalmau",
            "Berit Gru\u00dfien"
        ],
        "abstract": "A natural and established way to restrict the constraint satisfaction problem is to fix the relations that can be used to pose constraints; such a family of relations is called a constraint language. In this article, we study arc consistency, a heavily investigated inference method, and three extensions thereof from the perspective of constraint languages. We conduct a comparison of the studied methods on the basis of which constraint languages they solve, and we present new polynomial-time tractability results for singleton arc consistency, the most powerful method studied.\n    ",
        "submission_date": "2011-04-26T00:00:00",
        "last_modified_date": "2011-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.5069",
        "title": "Synthesizing Robust Plans under Incomplete Domain Models",
        "authors": [
            "Tuan Nguyen",
            "Subbarao Kambhampati",
            "Minh Do"
        ],
        "abstract": "Most current planners assume complete domain models and focus on generating correct plans. Unfortunately, domain modeling is a laborious and error-prone task. While domain experts cannot guarantee completeness, often they are able to circumscribe the incompleteness of the model by providing annotations as to which parts of the domain model may be incomplete. In such cases, the goal should be to generate plans that are robust with respect to any known incompleteness of the domain. In this paper, we first introduce annotations expressing the knowledge of the domain incompleteness, and formalize the notion of plan robustness with respect to an incomplete domain model. We then propose an approach to compiling the problem of finding robust plans to the conformant probabilistic planning problem. We present experimental results with Probabilistic-FF, a state-of-the-art planner, showing the promise of our approach.\n    ",
        "submission_date": "2011-04-27T00:00:00",
        "last_modified_date": "2011-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.5256",
        "title": "Learning Undirected Graphical Models with Structure Penalty",
        "authors": [
            "Shilin Ding"
        ],
        "abstract": "In undirected graphical models, learning the graph structure and learning the functions that relate the predictive variables (features) to the responses given the structure are two topics that have been widely investigated in machine learning and statistics. Learning graphical models in two stages will have problems because graph structure may change after considering the features. The main contribution of this paper is the proposed method that learns the graph structure and functions on the graph at the same time. General graphical models with binary outcomes conditioned on predictive variables are proved to be equivalent to multivariate Bernoulli model. The reparameterization of the potential functions in graphical model by conditional log odds ratios in multivariate Bernoulli model offers advantage in the representation of the conditional independence structure in the model. Additionally, we impose a structure penalty on groups of conditional log odds ratios to learn the graph structure. These groups of functions are designed with overlaps to enforce hierarchical function selection. In this way, we are able to shrink higher order interactions to obtain a sparse graph structure. Simulation studies show that the method is able to recover the graph structure. The analysis of county data from Census Bureau gives interesting relations between unemployment rate, crime and others discovered by the model.\n    ",
        "submission_date": "2011-04-27T00:00:00",
        "last_modified_date": "2011-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.5566",
        "title": "Limits of Preprocessing",
        "authors": [
            "Stefan Szeider"
        ],
        "abstract": "We present a first theoretical analysis of the power of polynomial-time preprocessing for important combinatorial problems from various areas in AI. We consider problems from Constraint Satisfaction, Global Constraints, Satisfiability, Nonmonotonic and Bayesian Reasoning. We show that, subject to a complexity theoretic assumption, none of the considered problems can be reduced by polynomial-time preprocessing to a problem kernel whose size is polynomial in a structural problem parameter of the input, such as induced width or backdoor size. Our results provide a firm theoretical boundary for the performance of polynomial-time preprocessing algorithms for the considered problems.\n    ",
        "submission_date": "2011-04-29T00:00:00",
        "last_modified_date": "2011-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.0288",
        "title": "Splitting and Updating Hybrid Knowledge Bases (Extended Version)",
        "authors": [
            "Martin Slota",
            "Jo\u00e3o Leite",
            "Terrance Swift"
        ],
        "abstract": "Over the years, nonmonotonic rules have proven to be a very expressive and useful knowledge representation paradigm. They have recently been used to complement the expressive power of Description Logics (DLs), leading to the study of integrative formal frameworks, generally referred to as hybrid knowledge bases, where both DL axioms and rules can be used to represent knowledge. The need to use these hybrid knowledge bases in dynamic domains has called for the development of update operators, which, given the substantially different way Description Logics and rules are usually updated, has turned out to be an extremely difficult task.\n",
        "submission_date": "2011-05-02T00:00:00",
        "last_modified_date": "2011-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.0650",
        "title": "Transition Systems for Model Generators - A Unifying Approach",
        "authors": [
            "Yuliya Lierler",
            "Miroslaw Truszczynski"
        ],
        "abstract": "A fundamental task for propositional logic is to compute models of propositional formulas. Programs developed for this task are called satisfiability solvers. We show that transition systems introduced by Nieuwenhuis, Oliveras, and Tinelli to model and analyze satisfiability solvers can be adapted for solvers developed for two other propositional formalisms: logic programming under the answer-set semantics, and the logic PC(ID). We show that in each case the task of computing models can be seen as \"satisfiability modulo answer-set programming,\" where the goal is to find a model of a theory that also is an answer set of a certain program. The unifying perspective we develop shows, in particular, that solvers CLASP and MINISATID are closely related despite being developed for different formalisms, one for answer-set programming and the latter for the logic PC(ID).\n    ",
        "submission_date": "2011-05-03T00:00:00",
        "last_modified_date": "2011-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.0707",
        "title": "Parameterized Complexity of Problems in Coalitional Resource Games",
        "authors": [
            "Rajesh Chitnis",
            "MohammadTaghi Hajiaghayi",
            "Vahid Liaghat"
        ],
        "abstract": "Coalition formation is a key topic in multi-agent systems. Coalitions enable agents to achieve goals that they may not have been able to achieve on their own. Previous work has shown problems in coalitional games to be computationally hard. Wooldridge and Dunne (Artificial Intelligence 2006) studied the classical computational complexity of several natural decision problems in Coalitional Resource Games (CRG) - games in which each agent is endowed with a set of resources and coalitions can bring about a set of goals if they are collectively endowed with the necessary amount of resources. The input of coalitional resource games bundles together several elements, e.g., the agent set Ag, the goal set G, the resource set R, etc. Shrot, Aumann and Kraus (AAMAS 2009) examine coalition formation problems in the CRG model using the theory of Parameterized Complexity. Their refined analysis shows that not all parts of input act equal - some instances of the problem are indeed tractable while others still remain intractable.\n",
        "submission_date": "2011-05-03T00:00:00",
        "last_modified_date": "2011-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.0974",
        "title": "GANC: Greedy Agglomerative Normalized Cut",
        "authors": [
            "Seyed Salim Tabatabaei",
            "Mark Coates",
            "Michael Rabbat"
        ],
        "abstract": "This paper describes a graph clustering algorithm that aims to minimize the normalized cut criterion and has a model order selection procedure. The performance of the proposed algorithm is comparable to spectral approaches in terms of minimizing normalized cut. However, unlike spectral approaches, the proposed algorithm scales to graphs with millions of nodes and edges. The algorithm consists of three components that are processed sequentially: a greedy agglomerative hierarchical clustering procedure, model order selection, and a local refinement.\n",
        "submission_date": "2011-05-05T00:00:00",
        "last_modified_date": "2011-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.1247",
        "title": "Machine-Part cell formation through visual decipherable clustering of Self Organizing Map",
        "authors": [
            "Manojit Chattopadhyay",
            "Surajit Chattopadhyay",
            "Pranab K. Dan"
        ],
        "abstract": "Machine-part cell formation is used in cellular manufacturing in order to process a large variety, quality, lower work in process levels, reducing manufacturing lead-time and customer response time while retaining flexibility for new products. This paper presents a new and novel approach for obtaining machine cells and part families. In the cellular manufacturing the fundamental problem is the formation of part families and machine cells. The present paper deals with the Self Organising Map (SOM) method an unsupervised learning algorithm in Artificial Intelligence, and has been used as a visually decipherable clustering tool of machine-part cell formation. The objective of the paper is to cluster the binary machine-part matrix through visually decipherable cluster of SOM color-coding and labelling via the SOM map nodes in such a way that the part families are processed in that machine cells. The Umatrix, component plane, principal component projection, scatter plot and histogram of SOM have been reported in the present work for the successful visualization of the machine-part cell formation. Computational result with the proposed algorithm on a set of group technology problems available in the literature is also presented. The proposed SOM approach produced solutions with a grouping efficacy that is at least as good as any results earlier reported in the literature and improved the grouping efficacy for 70% of the problems and found immensely useful to both industry practitioners and researchers.\n    ",
        "submission_date": "2011-05-06T00:00:00",
        "last_modified_date": "2011-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.1436",
        "title": "Solving Rubik's Cube Using SAT Solvers",
        "authors": [
            "Jingchao Chen"
        ],
        "abstract": "Rubik's Cube is an easily-understood puzzle, which is originally called the \"magic cube\". It is a well-known planning problem, which has been studied for a long time. Yet many simple properties remain unknown. This paper studies whether modern SAT solvers are applicable to this puzzle. To our best knowledge, we are the first to translate Rubik's Cube to a SAT problem. To reduce the number of variables and clauses needed for the encoding, we replace a naive approach of 6 Boolean variables to represent each color on each facelet with a new approach of 3 or 2 Boolean variables. In order to be able to solve quickly Rubik's Cube, we replace the direct encoding of 18 turns with the layer encoding of 18-subtype turns based on 6-type turns. To speed up the solving further, we encode some properties of two-phase algorithm as an additional constraint, and restrict some move sequences by adding some constraint clauses. Using only efficient encoding cannot solve this puzzle. For this reason, we improve the existing SAT solvers, and develop a new SAT solver based on PrecoSAT, though it is suited only for Rubik's Cube. The new SAT solver replaces the lookahead solving strategy with an ALO (\\emph{at-least-one}) solving strategy, and decomposes the original problem into sub-problems. Each sub-problem is solved by PrecoSAT. The empirical results demonstrate both our SAT translation and new solving technique are efficient. Without the efficient SAT encoding and the new solving technique, Rubik's Cube will not be able to be solved still by any SAT solver. Using the improved SAT solver, we can find always a solution of length 20 in a reasonable time. Although our solver is slower than Kociemba's algorithm using lookup tables, but does not require a huge lookup table.\n    ",
        "submission_date": "2011-05-07T00:00:00",
        "last_modified_date": "2011-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.1749",
        "title": "A Real-Time Model-Based Reinforcement Learning Architecture for Robot Control",
        "authors": [
            "Todd Hester",
            "Michael Quinlan",
            "Peter Stone"
        ],
        "abstract": "Reinforcement Learning (RL) is a method for learning decision-making tasks that could enable robots to learn and adapt to their situation on-line. For an RL algorithm to be practical for robotic control tasks, it must learn in very few actions, while continually taking those actions in real-time. Existing model-based RL methods learn in relatively few actions, but typically take too much time between each action for practical on-line learning. In this paper, we present a novel parallel architecture for model-based RL that runs in real-time by 1) taking advantage of sample-based approximate planning methods and 2) parallelizing the acting, model learning, and planning processes such that the acting process is sufficiently fast for typical robot control cycles. We demonstrate that algorithms using this architecture perform nearly as well as methods using the typical sequential architecture when both are given unlimited time, and greatly out-perform these methods on tasks that require real-time actions such as controlling an autonomous vehicle.\n    ",
        "submission_date": "2011-05-09T00:00:00",
        "last_modified_date": "2011-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.1929",
        "title": "The Hidden Web, XML and Semantic Web: A Scientific Data Management Perspective",
        "authors": [
            "Fabian Suchanek",
            "Aparna Varde",
            "Richi Nayak",
            "Pierre Senellart"
        ],
        "abstract": "The World Wide Web no longer consists just of HTML pages. Our work sheds light on a number of trends on the Internet that go beyond simple Web pages. The hidden Web provides a wealth of data in semi-structured form, accessible through Web forms and Web services. These services, as well as numerous other applications on the Web, commonly use XML, the eXtensible Markup Language. XML has become the lingua franca of the Internet that allows customized markups to be defined for specific domains. On top of XML, the Semantic Web grows as a common structured data source. In this work, we first explain each of these developments in detail. Using real-world examples from scientific domains of great interest today, we then demonstrate how these new developments can assist the managing, harvesting, and organization of data on the Web. On the way, we also illustrate the current research avenues in these domains. We believe that this effort would help bridge multiple database tracks, thereby attracting researchers with a view to extend database technology.\n    ",
        "submission_date": "2011-05-10T00:00:00",
        "last_modified_date": "2011-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.2813",
        "title": "Optimal Upper and Lower Bounds for Boolean Expressions by Dissociation",
        "authors": [
            "Wolfgang Gatterbauer",
            "Dan Suciu"
        ],
        "abstract": "This paper develops upper and lower bounds for the probability of Boolean expressions by treating multiple occurrences of variables as independent and assigning them new individual probabilities. Our technique generalizes and extends the underlying idea of a number of recent approaches which are varyingly called node splitting, variable renaming, variable splitting, or dissociation for probabilistic databases. We prove that the probabilities we assign to new variables are the best possible in some sense.\n    ",
        "submission_date": "2011-05-13T00:00:00",
        "last_modified_date": "2011-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.2902",
        "title": "A Multi-Purpose Scenario-based Simulator for Smart House Environments",
        "authors": [
            "Zahra Forootan Jahromi",
            "Amir Rajabzadeh",
            "Ali Reza Manashty"
        ],
        "abstract": "Developing smart house systems has been a great challenge for researchers and engineers in this area because of the high cost of implementation and evaluation process of these systems, while being very time consuming. Testing a designed smart house before actually building it is considered as an obstacle towards an efficient smart house project. This is because of the variety of sensors, home appliances and devices available for a real smart environment. In this paper, we present the design and implementation of a multi-purpose smart house simulation system for designing and simulating all aspects of a smart house environment. This simulator provides the ability to design the house plan and different virtual sensors and appliances in a two dimensional model of the virtual house environment. This simulator can connect to any external smart house remote controlling system, providing evaluation capabilities to their system much easier than before. It also supports detailed adding of new emerging sensors and devices to help maintain its compatibility with future simulation needs. Scenarios can also be defined for testing various possible combinations of device states; so different criteria and variables can be simply evaluated without the need of experimenting on a real environment.\n    ",
        "submission_date": "2011-05-14T00:00:00",
        "last_modified_date": "2011-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.3486",
        "title": "Xapagy: a cognitive architecture for narrative reasoning",
        "authors": [
            "Ladislau B\u00f6l\u00f6ni"
        ],
        "abstract": "We introduce the Xapagy cognitive architecture: a software system designed to perform narrative reasoning. The architecture has been designed from scratch to model and mimic the activities performed by humans when witnessing, reading, recalling, narrating and talking about stories.\n    ",
        "submission_date": "2011-05-17T00:00:00",
        "last_modified_date": "2011-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.3635",
        "title": "Probabilistic Inference from Arbitrary Uncertainty using Mixtures of Factorized Generalized Gaussians",
        "authors": [
            "M. C. Garrido",
            "P. E. Lopez-de-Teruel",
            "A. Ruiz"
        ],
        "abstract": "This paper presents a general and efficient framework for    probabilistic inference and learning from arbitrary uncertain    information. It exploits the calculation properties of finite mixture    models, conjugate families and factorization. Both the joint    probability density of the variables and the likelihood function of    the (objective or subjective) observation are approximated by a    special mixture model, in such a way that any desired conditional    distribution can be directly obtained without numerical    integration. We have developed an extended version of the expectation    maximization (EM) algorithm to estimate the parameters of mixture    models from uncertain training examples (indirect observations). As a    consequence, any piece of exact or uncertain information about both    input and output values is consistently handled in the inference and    learning stages. This ability, extremely useful in certain situations,    is not found in most alternative methods. The proposed framework is    formally justified from standard probabilistic principles and    illustrative examples are provided in the fields of nonparametric    pattern classification, nonlinear regression and pattern    completion. Finally, experiments on a real application and comparative    results over standard databases provide empirical evidence of the    utility of the method in a wide range of applications.\n    ",
        "submission_date": "2011-05-18T00:00:00",
        "last_modified_date": "2011-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.3821",
        "title": "Ontological Crises in Artificial Agents' Value Systems",
        "authors": [
            "Peter de Blanc"
        ],
        "abstract": "Decision-theoretic agents predict and evaluate the results of their actions using a model, or ontology, of their environment. An agent's goal, or utility function, may also be specified in terms of the states of, or entities within, its ontology. If the agent may upgrade or replace its ontology, it faces a crisis: the agent's original goal may not be well-defined with respect to its new ontology. This crisis must be resolved before the agent can make plans towards achieving its goals.\n",
        "submission_date": "2011-05-19T00:00:00",
        "last_modified_date": "2011-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.3833",
        "title": "Typical models: minimizing false beliefs",
        "authors": [
            "Eliezer L. Lozinskii"
        ],
        "abstract": "A knowledge system S describing a part of real world does in general not contain complete information. Reasoning with incomplete information is prone to errors since any belief derived from S may be false in the present state of the world. A false belief may suggest wrong decisions and lead to harmful actions. So an important goal is to make false beliefs as unlikely as possible. This work introduces the notions of \"typical atoms\" and \"typical models\", and shows that reasoning with typical models minimizes the expected number of false beliefs over all ways of using incomplete information. Various properties of typical models are studied, in particular, correctness and stability of beliefs suggested by typical models, and their connection to oblivious reasoning.\n    ",
        "submission_date": "2011-05-19T00:00:00",
        "last_modified_date": "2011-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.4224",
        "title": "On A Semi-Automatic Method for Generating Composition Tables",
        "authors": [
            "Weiming Liu",
            "Sanjiang Li"
        ],
        "abstract": "Originating from Allen's Interval Algebra, composition-based reasoning has been widely acknowledged as the most popular reasoning technique in qualitative spatial and temporal reasoning. Given a qualitative calculus (i.e. a relation model), the first thing we should do is to establish its composition table (CT). In the past three decades, such work is usually done manually. This is undesirable and error-prone, given that the calculus may contain tens or hundreds of basic relations. Computing the correct CT has been identified by Tony Cohn as a challenge for computer scientists in 1995. This paper addresses this problem and introduces a semi-automatic method to compute the CT by randomly generating triples of elements. For several important qualitative calculi, our method can establish the correct CT in a reasonable short time. This is illustrated by applications to the Interval Algebra, the Region Connection Calculus RCC-8, the INDU calculus, and the Oriented Point Relation Algebras. Our method can also be used to generate CTs for customised qualitative calculi defined on restricted domains.\n    ",
        "submission_date": "2011-05-21T00:00:00",
        "last_modified_date": "2011-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5440",
        "title": "The Ariadne's Clew Algorithm",
        "authors": [
            "J. M. Ahuactzin",
            "P. Bessiere",
            "E. Mazer"
        ],
        "abstract": "We present a new approach to path planning, called the  \"Ariadne's clew algorithm\".  It is designed to find paths in    high-dimensional continuous spaces and applies to robots with many    degrees of freedom in static, as well as dynamic environments - ones    where obstacles may move. The Ariadne's clew algorithm comprises two    sub-algorithms, called Search and Explore, applied in an interleaved    manner. Explore builds a representation of the accessible space while    Search looks for the target. Both are posed as optimization problems.    We describe a real implementation of the algorithm to plan paths for a    six degrees of freedom arm in a dynamic environment where another six    degrees of freedom arm is used as a moving obstacle. Experimental    results show that a path is found in about one second without any    pre-processing.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5441",
        "title": "Computational Aspects of Reordering Plans",
        "authors": [
            "C. Backstrom"
        ],
        "abstract": "This article studies the problem of modifying the action    ordering of a plan in order to optimise the plan according to various    criteria.  One of these criteria is to make a plan less constrained    and the other is to minimize its parallel execution time.  Three    candidate definitions are proposed for the first of these criteria,    constituting a sequence of increasing optimality guarantees.  Two of    these are based on deordering plans, which means that ordering    relations may only be removed, not added, while the third one uses    reordering, where arbitrary modifications to the ordering are allowed.    It is shown that only the weakest one of the three criteria is    tractable to achieve, the other two being NP-hard and even difficult    to approximate.  Similarly, optimising the parallel execution time of    a plan is studied both for deordering and reordering of plans.  In the    general case, both of these computations are NP-hard.  However, it is    shown that optimal deorderings can be computed in polynomial time for    a class of planning languages based on the notions of producers,    consumers and threats, which includes most of the commonly used    planning languages.  Computing optimal reorderings can potentially    lead to even faster parallel executions, but this problem remains    NP-hard and difficult to approximate even under quite severe restrictions.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5442",
        "title": "The Divide-and-Conquer Subgoal-Ordering Algorithm for Speeding up Logic Inference",
        "authors": [
            "O. Ledeniov",
            "S. Markovitch"
        ],
        "abstract": "It is common to view programs as a combination of logic and    control: the logic part defines what the program must do, the control    part -- how to do it.  The Logic Programming paradigm was developed    with the intention of separating the logic from the control.    Recently, extensive research has been conducted on automatic    generation of control for logic programs.  Only a few of these works    considered the issue of automatic generation of control for improving    the efficiency of logic programs.  In this paper we present a novel    algorithm for automatic finding of lowest-cost subgoal orderings.  The    algorithm works using the divide-and-conquer strategy.  The given set    of subgoals is partitioned into smaller sets, based on co-occurrence    of free variables. The subsets are ordered recursively and merged,    yielding a provably optimal order.  We experimentally demonstrate the    utility of the algorithm by testing it in several domains, and discuss    the possibilities of its cooperation with other existing methods.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5443",
        "title": "The Gn,m Phase Transition is Not Hard for the Hamiltonian Cycle Problem",
        "authors": [
            "J. Culberson",
            "B. Vandegriend"
        ],
        "abstract": "Using an improved backtrack algorithm with sophisticated    pruning techniques, we revise previous observations correlating a high    frequency of hard to solve Hamiltonian Cycle instances with the Gn,m    phase transition between Hamiltonicity and non-Hamiltonicity. Instead    all tested graphs of 100 to 1500 vertices are easily solved.       When we artificially restrict the degree sequence with a bounded    maximum degree, although there is some increase in difficulty, the    frequency of hard graphs is still low.  When we consider more regular    graphs based on a generalization of knight's tours, we observe    frequent instances of really hard graphs, but on these the average    degree is bounded by a constant.  We design a set of graphs with a    feature our algorithm is unable to detect and so are very hard for our    algorithm, but in these we can vary the average degree from O(1) to    O(n).  We have so far found no class of graphs correlated with the    Gn,m phase transition which asymptotically produces a high frequency    of hard instances.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5444",
        "title": "Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language",
        "authors": [
            "P. Resnik"
        ],
        "abstract": "This article presents a measure of semantic similarity in an    IS-A taxonomy based on the notion of shared information content.    Experimental evaluation against a benchmark set of human similarity    judgments demonstrates that the measure performs better than the    traditional edge-counting approach.  The article presents algorithms    that take advantage of taxonomic similarity in resolving syntactic and    semantic ambiguity, along with experimental results demonstrating    their effectiveness.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5446",
        "title": "A Temporal Description Logic for Reasoning about Actions and Plans",
        "authors": [
            "A. Artale",
            "E. Franconi"
        ],
        "abstract": "A class of interval-based temporal languages for uniformly    representing and reasoning about actions and plans is presented.    Actions are represented by describing what is true while the action    itself is occurring, and plans are constructed by temporally relating    actions and world states.  The temporal languages are members of the    family of Description Logics, which are characterized by high    expressivity combined with good computational properties.  The    subsumption problem for a class of temporal Description Logics is    investigated and sound and complete decision procedures are given. The    basic language TL-F is considered first: it is the composition of a    temporal logic TL -- able to express interval temporal networks --    together with the non-temporal logic F -- a Feature Description Logic.    It is proven that subsumption in this language is an NP-complete    problem. Then it is shown how to reason with the more expressive    languages TLU-FU and TL-ALCF. The former adds disjunction both at the    temporal and non-temporal sides of the language, the latter extends    the non-temporal side with set-valued features (i.e., roles) and a    propositionally complete language.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5447",
        "title": "Adaptive Parallel Iterative Deepening Search",
        "authors": [
            "D. J. Cook",
            "R. C. Varnell"
        ],
        "abstract": "Many of the artificial intelligence techniques developed to    date rely on heuristic search through large spaces.  Unfortunately,    the size of these spaces and the corresponding computational effort    reduce the applicability of otherwise novel and effective algorithms.    A number of parallel and distributed approaches to search have    considerably improved the performance of the search process.        Our goal is to develop an architecture that automatically selects    parallel search strategies for optimal performance on a variety of    search problems.  In this paper we describe one such architecture    realized in the Eureka system, which combines the benefits of    many different approaches to parallel heuristic search.  Through    empirical and theoretical analyses we observe that features of the    problem space directly affect the choice of optimal parallel search    strategy.  We then employ machine learning techniques to select the    optimal parallel search strategy for a given problem space.  When a    new search task is input to the system, Eureka uses features    describing the search space and the chosen architecture to    automatically select the appropriate search strategy.  Eureka    has been tested on a MIMD parallel processor, a distributed network of    workstations, and a single workstation using multithreading.  Results    generated from fifteen puzzle problems, robot arm motion problems,    artificial search spaces, and planning problems indicate that     Eureka outperforms any of the tested strategies used exclusively for    all problem instances and is able to greatly reduce the search time    for these applications.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5448",
        "title": "Order of Magnitude Comparisons of Distance",
        "authors": [
            "E. Davis"
        ],
        "abstract": "Order of magnitude reasoning - reasoning by rough    comparisons of the sizes of quantities - is often called 'back of    the envelope calculation', with the implication that the calculations    are quick though approximate.  This paper exhibits an interesting    class of constraint sets in which order of magnitude reasoning is    demonstrably fast.  Specifically, we present a polynomial-time    algorithm that can solve a set of constraints of the form 'Points a    and b are much closer together than points c and d.'  We prove that    this algorithm can be applied if `much closer together' is    interpreted either as referring to an infinite difference in scale or    as referring to a finite difference in scale, as long as the    difference in scale is greater than the number of variables in the    constraint set.  We also prove that the first-order theory over such    constraints is decidable.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5449",
        "title": "AntNet: Distributed Stigmergetic Control for Communications Networks",
        "authors": [
            "G. Di Caro",
            "M. Dorigo"
        ],
        "abstract": "This paper introduces AntNet, a novel approach to the    adaptive learning of routing tables in communications networks.    AntNet is a distributed, mobile agents based Monte Carlo system that    was inspired by recent work on the ant colony metaphor for solving    optimization problems. AntNet's agents concurrently explore the    network and exchange collected information.  The communication among    the agents is indirect and asynchronous, mediated by the network    itself. This form of communication is typical of social insects and is    called stigmergy.  We compare our algorithm with six state-of-the-art    routing algorithms coming from the telecommunications and machine    learning fields.  The algorithms' performance is evaluated over a set    of realistic testbeds.  We run many experiments over real and    artificial IP datagram networks with increasing number of nodes and    under several paradigmatic spatial and temporal traffic distributions.    Results are very encouraging.  AntNet showed superior performance    under all the experimental conditions with respect to its competitors.    We analyze the main characteristics of the algorithm and try to    explain the reasons for its superiority.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5450",
        "title": "A Counter Example to Theorems of Cox and Fine",
        "authors": [
            "J. Y. Halpern"
        ],
        "abstract": "Cox's well-known theorem justifying the use of probability is shown not to hold in finite domains. The counterexample also suggests that Cox's assumptions are insufficient to prove the result even in infinite domains. The same counterexample is used to disprove a result of Fine on comparative conditional probability.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5451",
        "title": "The Automatic Inference of State Invariants in TIM",
        "authors": [
            "M. Fox",
            "D. Long"
        ],
        "abstract": "As planning is applied to larger and richer domains the    effort involved in constructing domain descriptions increases and    becomes a significant burden on the human application designer. If    general planners are to be applied successfully to large and complex    domains it is necessary to provide the domain designer with some    assistance in building correctly encoded domains.  One way of doing    this is to provide domain-independent techniques for extracting, from    a domain description, knowledge that is implicit in that description    and that can assist domain designers in debugging domain    descriptions. This knowledge can also be exploited to improve the    performance of planners: several researchers have explored the    potential of state invariants in speeding up the performance of    domain-independent planners. In this paper we describe a process by    which state invariants can be extracted from the automatically    inferred type structure of a domain. These techniques are being    developed for exploitation by STAN, a Graphplan based planner that    employs state analysis techniques to enhance its performance.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5452",
        "title": "Unifying Class-Based Representation Formalisms",
        "authors": [
            "D. Calvanese",
            "M. Lenzerini",
            "D. Nardi"
        ],
        "abstract": "The notion of class is ubiquitous in computer science and is    central in many formalisms for the representation of structured    knowledge used both in knowledge representation and in databases.  In    this paper we study the basic issues underlying such representation    formalisms and single out both their common characteristics and their    distinguishing features.  Such investigation leads us to propose a    unifying framework in which we are able to capture the fundamental    aspects of several representation languages used in different    contexts.  The proposed formalism is expressed in the style of    description logics, which have been introduced in knowledge    representation as a means to provide a semantically well-founded basis    for the structural aspects of knowledge representation systems. The    description logic considered in this paper is a subset of first order    logic with nice computational characteristics.  It is quite expressive    and features a novel combination of constructs that has not been    studied before.  The distinguishing constructs are number    restrictions, which generalize existence and functional dependencies,    inverse roles, which allow one to refer to the inverse of a    relationship, and possibly cyclic assertions, which are necessary for    capturing real world domains.  We are able to show that it is    precisely such combination of constructs that makes our logic powerful    enough to model the essential set of features for defining class    structures that are common to frame systems, object-oriented database    languages, and semantic data models.  As a consequence of the    established correspondences, several significant extensions of each of    the above formalisms become available. The high expressiveness of the    logic we propose and the need for capturing the reasoning in different    contexts forces us to distinguish between unrestricted and finite    model reasoning.  A notable feature of our proposal is that reasoning    in both cases is decidable.  We argue that, by virtue of the high    expressive power and of the associated reasoning capabilities on both    unrestricted and finite models, our logic provides a common core for    class-based representation formalisms.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5453",
        "title": "Complexity of Prioritized Default Logics",
        "authors": [
            "J. Rintanen"
        ],
        "abstract": "In default reasoning, usually not all possible ways of    resolving conflicts between default rules are acceptable.  Criteria    expressing acceptable ways of resolving the conflicts may be hardwired    in the inference mechanism, for example specificity in inheritance    reasoning can be handled this way, or they may be given abstractly as    an ordering on the default rules.  In this article we investigate    formalizations of the latter approach in Reiter's default logic.  Our    goal is to analyze and compare the computational properties of three    such formalizations in terms of their computational complexity: the    prioritized default logics of Baader and Hollunder, and Brewka, and a    prioritized default logic that is based on lexicographic comparison.    The analysis locates the propositional variants of these logics on the    second and third levels of the polynomial hierarchy, and identifies    the boundary between tractable and intractable inference for    restricted classes of prioritized default theories.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5454",
        "title": "Squeaky Wheel Optimization",
        "authors": [
            "D. P. Clements",
            "D. E. Joslin"
        ],
        "abstract": "We describe a general approach to optimization which we term    `Squeaky Wheel' Optimization (SWO).  In SWO, a greedy algorithm is    used to construct a solution which is then analyzed to find the    trouble spots, i.e., those elements, that, if improved, are likely to    improve the objective function score.  The results of the analysis are    used to generate new priorities that determine the order in which the    greedy algorithm constructs the next solution.  This    Construct/Analyze/Prioritize cycle continues until some limit is    reached, or an acceptable solution is found.         SWO can be viewed as operating on two search spaces: solutions and    prioritizations.  Successive solutions are only indirectly related,    via the re-prioritization that results from analyzing the prior    solution.  Similarly, successive prioritizations are generated by    constructing and analyzing solutions.  This `coupled search' has some    interesting properties, which we discuss.         We report encouraging experimental results on two domains, scheduling    problems that arise in fiber-optic cable manufacturing, and graph    coloring problems.  The fact that these domains are very different    supports our claim that SWO is a general technique for optimization.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5455",
        "title": "Variational Cumulant Expansions for Intractable Distributions",
        "authors": [
            "D. Barber",
            "P. de van Laar"
        ],
        "abstract": "Intractable distributions present a common difficulty in    inference within the probabilistic knowledge representation framework    and variational methods have recently been popular in providing an    approximate solution. In this article, we describe a perturbational    approach in the form of a cumulant expansion which, to lowest order,    recovers the standard Kullback-Leibler variational bound.    Higher-order terms describe corrections on the variational approach    without incurring much further computational cost.  The relationship    to other perturbational approaches such as TAP is also elucidated.  We    demonstrate the method on a particular class of undirected graphical    models, Boltzmann machines, for which our simulation results confirm    improved accuracy and enhanced stability during learning.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5457",
        "title": "Efficient Implementation of the Plan Graph in STAN",
        "authors": [
            "M. Fox",
            "D. Long"
        ],
        "abstract": "STAN is a Graphplan-based planner, so-called because it uses    a variety of STate ANalysis techniques to enhance its performance.    STAN competed in the AIPS-98 planning competition where it compared    well with the other competitors in terms of speed, finding solutions    fastest to many of the problems posed. Although the domain analysis    techniques STAN exploits are an important factor in its overall    performance, we believe that the speed at which STAN solved the    competition problems is largely due to the implementation of its plan    graph. The implementation is based on two insights: that many of the    graph construction operations can be implemented as bit-level logical    operations on bit vectors, and that the graph should not be explicitly    constructed beyond the fix point. This paper describes the    implementation of STAN's plan graph and provides experimental results    which demonstrate the circumstances under which advantages can be    obtained from using this implementation.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5458",
        "title": "Cooperation between Top-Down and Bottom-Up Theorem Provers",
        "authors": [
            "M. Fuchs",
            "D. Fuchs"
        ],
        "abstract": "Top-down and bottom-up theorem proving approaches each    have specific advantages and disadvantages.  Bottom-up provers profit    from strong redundancy control but suffer from the lack of    goal-orientation, whereas top-down provers are goal-oriented but often    have weak calculi when their proof lengths are considered.  In order    to integrate both approaches, we try to achieve cooperation between a    top-down and a bottom-up prover in two different ways: The first    technique aims at supporting a bottom-up with a top-down prover. A    top-down prover generates subgoal clauses, they are then processed by    a bottom-up prover.  The second technique deals with the use of    bottom-up generated lemmas in a top-down prover. We apply our concept    to the areas of model elimination and superposition.  We discuss the    ability of our techniques to shorten proofs as well as to reorder the    search space in an appropriate manner. Furthermore, in order to    identify subgoal clauses and lemmas which are actually relevant for    the proof task, we develop methods for a relevancy-based filtering.    Experiments with the provers SETHEO and SPASS performed in the problem    library TPTP reveal the high potential of our cooperation approaches.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5459",
        "title": "Solving Highly Constrained Search Problems with Quantum Computers",
        "authors": [
            "T. Hogg"
        ],
        "abstract": "A previously developed quantum search algorithm for solving    1-SAT problems in a single step is generalized to apply to a range    of highly constrained k-SAT problems. We identify a bound on the    number of clauses in satisfiability problems for which the    generalized algorithm can find a solution in a constant number of    steps as the number of variables increases. This performance    contrasts with the linear growth in the number of steps required by    the best classical algorithms, and the exponential number required    by classical and quantum methods that ignore the problem    structure. In some cases, the algorithm can also guarantee that    insoluble problems in fact have no solutions, unlike previously    proposed quantum search algorithms.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5460",
        "title": "Decision-Theoretic Planning: Structural Assumptions and Computational Leverage",
        "authors": [
            "C. Boutilier",
            "T. Dean",
            "S. Hanks"
        ],
        "abstract": "Planning under uncertainty is a central problem in the study of    automated sequential decision making, and has been addressed by    researchers in many different fields, including AI planning, decision    analysis, operations research, control theory and economics.  While    the assumptions and perspectives adopted in these areas often differ    in substantial ways, many planning problems of interest to researchers    in these fields can be modeled as Markov decision processes (MDPs)    and analyzed using the techniques of decision theory.       This paper presents an overview and synthesis of MDP-related methods,    showing how they provide a unifying framework for modeling many    classes of planning problems studied in AI. It also describes    structural properties of MDPs that, when exhibited by particular    classes of problems, can be exploited in the construction of optimal    or approximately optimal policies or plans.  Planning problems    commonly possess structure in the reward and value functions used to    describe performance criteria, in the functions used to describe state    transitions and observations, and in the relationships among features    used to describe states, actions, rewards, and observations.       Specialized representations, and algorithms employing these    representations, can achieve computational leverage by exploiting    these various forms of structure.  Certain AI techniques -- in    particular those based on the use of structured, intensional    representations -- can be viewed in this way.  This paper surveys    several types of representations for both classical and    decision-theoretic planning problems, and planning algorithms that    exploit these representations in a number of different ways to ease    the computational burden of constructing policies or plans.  It focuses    primarily on abstraction, aggregation and decomposition techniques    based on AI-style representations.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5461",
        "title": "Probabilistic Deduction with Conditional Constraints over Basic Events",
        "authors": [
            "T. Lukasiewicz"
        ],
        "abstract": "We study the problem of probabilistic deduction with    conditional constraints over basic events. We show that globally    complete probabilistic deduction with conditional constraints over    basic events is NP-hard. We then concentrate on the special case of    probabilistic deduction in conditional constraint trees. We elaborate    very efficient techniques for globally complete probabilistic    deduction. In detail, for conditional constraint trees with point    probabilities, we present a local approach to globally complete    probabilistic deduction, which runs in linear time in the size of the    conditional constraint trees. For conditional constraint trees with    interval probabilities, we show that globally complete probabilistic    deduction can be done in a global approach by solving nonlinear    programs. We show how these nonlinear programs can be transformed into    equivalent linear programs, which are solvable in polynomial time in    the size of the conditional constraint trees.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5462",
        "title": "Variational Probabilistic Inference and the QMR-DT Network",
        "authors": [
            "T. S. Jaakkola",
            "M. I. Jordan"
        ],
        "abstract": "We describe a variational approximation method for efficient    inference in large-scale probabilistic models.  Variational methods    are deterministic procedures that provide approximations to marginal    and conditional probabilities of interest.  They provide alternatives    to approximate inference methods based on stochastic sampling or    search.  We describe a variational approach to the problem of    diagnostic inference in the `Quick Medical Reference' (QMR) network.    The QMR network is a large-scale probabilistic graphical model built    on statistical and expert knowledge.  Exact probabilistic inference is    infeasible in this model for all but a small set of cases.  We    evaluate our variational inference algorithm on a large set of    diagnostic test cases, comparing the algorithm to a state-of-the-art    stochastic sampling method.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5463",
        "title": "Extensible Knowledge Representation: the Case of Description Reasoners",
        "authors": [
            "A. Borgida"
        ],
        "abstract": "This paper offers an approach to extensible knowledge    representation and reasoning for a family of formalisms known as    Description Logics. The approach is based on the notion of adding new    concept constructors, and includes a heuristic methodology for    specifying the desired extensions, as well as a modularized software    architecture that supports implementing extensions.  The architecture    detailed here falls in the normalize-compared paradigm, and supports    both intentional reasoning (subsumption) involving concepts, and    extensional reasoning involving individuals after incremental updates    to the knowledge base.      The resulting approach can be used to extend the reasoner with    specialized notions that are motivated by specific problems or    application areas, such as reasoning about dates, plans, etc. In    addition, it provides an opportunity to implement constructors that    are not currently yet sufficiently well understood theoretically, but    are needed in practice. Also, for constructors that are provably hard    to reason with (e.g., ones whose presence would lead to    undecidability), it allows the implementation of incomplete reasoners    where the incompleteness is tailored to be acceptable for the    application at hand.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5465",
        "title": "Constructing Conditional Plans by a Theorem-Prover",
        "authors": [
            "J. Rintanen"
        ],
        "abstract": "The research on conditional planning rejects the assumptions    that there is no uncertainty or incompleteness of knowledge with    respect to the state and changes of the system the plans operate on.    Without these assumptions the sequences of operations that achieve the    goals depend on the initial state and the outcomes of nondeterministic    changes in the system.  This setting raises the questions of how to    represent the plans and how to perform plan search. The answers are    quite different from those in the simpler classical framework.  In    this paper, we approach conditional planning from a new viewpoint that    is motivated by the use of satisfiability algorithms in classical    planning.  Translating conditional planning to formulae in the    propositional logic is not feasible because of inherent computational    limitations.  Instead, we translate conditional planning to quantified    Boolean formulae.  We discuss three formalizations of conditional    planning as quantified Boolean formulae, and present experimental    results obtained with a theorem-prover.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5466",
        "title": "Issues in Stacked Generalization",
        "authors": [
            "K. M. Ting",
            "I. H. Witten"
        ],
        "abstract": "Stacked generalization is a general method of using a    high-level model to combine lower-level models to achieve greater    predictive accuracy.  In this paper we address two crucial issues    which have been considered to be a `black art' in classification tasks    ever since the introduction of stacked generalization in 1992 by    Wolpert: the type of generalizer that is suitable to derive the    higher-level model, and the kind of attributes that should be used as    its input.  We find that best results are obtained when the    higher-level model combines the confidence (and not just the    predictions) of the lower-level ones.   We demonstrate the effectiveness of stacked generalization for combining     three different types of learning algorithms for classification tasks.    We also compare the performance of stacked generalization with    majority vote and published results of arcing and bagging.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5516",
        "title": "Ontology Alignment at the Instance and Schema Level",
        "authors": [
            "Fabian Suchanek",
            "Serge Abiteboul",
            "Pierre Senellart"
        ],
        "abstract": "We present PARIS, an approach for the automatic alignment of ontologies. PARIS aligns not only instances, but also relations and classes. Alignments at the instance-level cross-fertilize with alignments at the schema-level. Thereby, our system provides a truly holistic solution to the problem of ontology alignment. The heart of the approach is probabilistic. This allows PARIS to run without any parameter tuning. We demonstrate the efficiency of the algorithm and its precision through extensive experiments. In particular, we obtain a precision of around 90% in experiments with two of the world's largest ontologies.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5667",
        "title": "Complexity of and Algorithms for Borda Manipulation",
        "authors": [
            "Jessica Davies",
            "George Katsirelos",
            "Nina Narodytska",
            "Toby Walsh"
        ],
        "abstract": "We prove that it is NP-hard for a coalition of two manipulators to compute how to manipulate the Borda voting rule. This resolves one of the last open problems in the computational complexity of manipulating common voting rules. Because of this NP-hardness, we treat computing a manipulation as an approximation problem where we try to minimize the number of manipulators. Based on ideas from bin packing and multiprocessor scheduling, we propose two new approximation methods to compute manipulations of the Borda rule. Experiments show that these methods significantly outperform the previous best known %existing approximation method. We are able to find optimal manipulations in almost all the randomly generated elections tested. Our results suggest that, whilst computing a manipulation of the Borda rule by a coalition is NP-hard, computational complexity may provide only a weak barrier against manipulation in practice.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.6124",
        "title": "Reasoning on Interval and Point-based Disjunctive Metric Constraints in Temporal Contexts",
        "authors": [
            "F. Barber"
        ],
        "abstract": "We introduce a temporal model for reasoning on disjunctive    metric constraints on intervals and time points in temporal    contexts. This temporal model is composed of a labeled temporal    algebra and its reasoning algorithms. The labeled temporal algebra    defines labeled disjunctive metric point-based constraints, where each    disjunct in each input disjunctive constraint is univocally associated    to a label. Reasoning algorithms manage labeled constraints,    associated label lists, and sets of mutually inconsistent    disjuncts. These algorithms guarantee consistency and obtain a minimal    network. Additionally, constraints can be organized in a hierarchy of    alternative temporal contexts. Therefore, we can reason on    context-dependent disjunctive metric constraints on intervals and    points. Moreover, the model is able to represent non-binary    constraints, such that logical dependencies on disjuncts in    constraints can be handled.  The computational cost of reasoning    algorithms is exponential in accordance with the underlying problem    complexity, although some improvements are proposed.\n    ",
        "submission_date": "2011-05-30T00:00:00",
        "last_modified_date": "2011-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.6148",
        "title": "Overcoming Misleads In Logic Programs by Redefining Negation",
        "authors": [
            "M. A. El-Dosuky",
            "T. T. Hamza",
            "M. Z. Rashad",
            "A. H. Naguib"
        ],
        "abstract": "Negation as failure and incomplete information in logic programs have been studied by many researchers In order to explains HOW a negated conclusion was reached, we introduce and proof a different way for negating facts to overcoming misleads in logic programs. Negating facts can be achieved by asking the user for constants that do not appear elsewhere in the knowledge base.\n    ",
        "submission_date": "2011-05-31T00:00:00",
        "last_modified_date": "2013-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.6314",
        "title": "Activity-Based Search for Black-Box Contraint-Programming Solvers",
        "authors": [
            "L. Michel",
            "P. Van Hentenryck"
        ],
        "abstract": "Robust search procedures are a central component in the design of black-box constraint-programming solvers. This paper proposes activity-based search, the idea of using the activity of variables during propagation to guide the search. Activity-based search was compared experimentally to impact-based search and the WDEG heuristics. Experimental results on a variety of benchmarks show that activity-based search is more robust than other heuristics and may produce significant improvements in performance.\n    ",
        "submission_date": "2011-05-31T00:00:00",
        "last_modified_date": "2011-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0171",
        "title": "Proposal of Pattern Recognition as a necessary and sufficient Principle to Cognitive Science",
        "authors": [
            "Gilberto de Paiva"
        ],
        "abstract": "Despite the prevalence of the Computational Theory of Mind and the Connectionist Model, the establishing of the key principles of the Cognitive Science are still controversy and inconclusive. This paper proposes the concept of Pattern Recognition as Necessary and Sufficient Principle for a general cognitive science modeling, in a very ambitious scientific proposal. A formal physical definition of the pattern recognition concept is also proposed to solve many key conceptual gaps on the field.\n    ",
        "submission_date": "2011-05-31T00:00:00",
        "last_modified_date": "2011-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0218",
        "title": "The Good Old Davis-Putnam Procedure Helps Counting Models",
        "authors": [
            "E. Birnbaum",
            "E. L. Lozinskii"
        ],
        "abstract": "As was shown recently, many important AI problems require    counting the number of models of propositional formulas. The problem    of counting models of such formulas is, according to present    knowledge, computationally intractable in a worst case. Based on the    Davis-Putnam procedure, we present an algorithm, CDP, that computes    the exact number of models of a propositional CNF or DNF formula    F. Let m and n be the number of clauses and variables of F,    respectively, and let p denote the probability that a literal l of F    occurs in a clause C of F, then the average running time of CDP is    shown to be O(nm^d), where d=-1/log(1-p).  The practical    performance of CDP has been estimated in a series of experiments on a    wide variety of CNF formulas.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0219",
        "title": "Identifying Mislabeled Training Data",
        "authors": [
            "C. E. Brodley",
            "M. A. Friedl"
        ],
        "abstract": "This paper presents a new approach to identifying and    eliminating mislabeled training instances for supervised learning. The    goal of this approach is to improve classification accuracies produced    by learning algorithms by improving the quality of the training data.    Our approach uses a set of learning algorithms to create classifiers    that serve as noise filters for the training data.  We evaluate single    algorithm, majority vote and consensus filters on five datasets that    are prone to labeling errors.  Our experiments illustrate that    filtering significantly improves classification accuracy for noise    levels up to 30 percent.  An analytical and empirical evaluation of    the precision of our approach shows that consensus filters are    conservative at throwing away good data at the expense of retaining    bad data and that majority filters are better at detecting bad data at    the expense of throwing away good data.  This suggests that for    situations in which there is a paucity of data, consensus filters are    preferable, whereas majority vote filters are preferable for    situations with an abundance of data.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0220",
        "title": "Committee-Based Sample Selection for Probabilistic Classifiers",
        "authors": [
            "S. Argamon-Engelson",
            "I. Dagan"
        ],
        "abstract": "In many real-world learning tasks, it is expensive to    acquire a sufficient number of labeled examples for training.  This    paper investigates methods for reducing annotation cost by `sample    selection'. In this approach, during training the learning program    examines many unlabeled examples and selects for labeling only those    that are most informative at each stage. This avoids redundantly    labeling examples that contribute little new information.       Our work follows on previous research on Query By Committee, extending    the committee-based paradigm to the context of probabilistic    classification.  We describe a family of empirical methods for    committee-based sample selection in probabilistic classification    models, which evaluate the informativeness of an example by measuring    the degree of disagreement between several model variants.  These    variants (the committee) are drawn randomly from a probability    distribution conditioned by the training set labeled so far.       The method was applied to the real-world natural language processing    task of stochastic part-of-speech tagging.  We find that all variants    of the method achieve a significant reduction in annotation cost,    although their computational efficiency differs.  In particular, the    simplest variant, a two member committee with no parameters to tune,    gives excellent results.  We also show that sample selection yields a    significant reduction in the size of the model used by the tagger.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0222",
        "title": "Markov Localization for Mobile Robots in Dynamic Environments",
        "authors": [
            "W. Burgard",
            "D. Fox",
            "S. Thrun"
        ],
        "abstract": "Localization, that is the estimation of a robot's location    from sensor data, is a fundamental problem in mobile robotics.  This    papers presents a version of Markov localization which provides    accurate position estimates and which is tailored towards dynamic    environments. The key idea of Markov localization is to maintain a    probability density over the space of all locations of a robot in its    environment. Our approach represents this space metrically, using a    fine-grained grid to approximate densities.  It is able to globally    localize the robot from scratch and to recover from localization    failures.  It is robust to approximate models of the environment (such    as occupancy grid maps) and noisy sensors (such as ultrasound    sensors).  Our approach also includes a filtering technique which    allows a mobile robot to reliably estimate its position even in    densely populated environments in which crowds of people block the    robot's sensors for extended periods of time.  The method described    here has been implemented and tested in several real-world    applications of mobile robots, including the deployments of two mobile    robots as interactive museum tour-guides.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0224",
        "title": "Reasoning about Minimal Belief and Negation as Failure",
        "authors": [
            "R. Rosati"
        ],
        "abstract": "We investigate the problem of reasoning in the propositional    fragment of MBNF, the logic of minimal belief and negation as failure    introduced by Lifschitz, which can be considered as a unifying    framework for several nonmonotonic formalisms, including default    logic, autoepistemic logic, circumscription, epistemic queries, and    logic programming.  We characterize the complexity and provide    algorithms for reasoning in propositional MBNF.  In particular, we    show that entailment in propositional MBNF lies at the third level of    the polynomial hierarchy, hence it is harder than reasoning in all the    above mentioned propositional formalisms for nonmonotonic reasoning.    We also prove the exact correspondence between negation as failure in    MBNF and negative introspection in Moore's autoepistemic logic.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0225",
        "title": "Randomized Algorithms for the Loop Cutset Problem",
        "authors": [
            "R. Bar-Yehuda",
            "A. Becker",
            "D. Geiger"
        ],
        "abstract": "We show how to find a minimum weight loop cutset in a    Bayesian network with high probability. Finding such a loop cutset is    the first step in the method of conditioning for inference.  Our    randomized algorithm for finding a loop cutset outputs a minimum loop    cutset after O(c 6^k kn) steps with probability at least     1 - (1 - 1/(6^k))^c6^k, where c > 1 is a constant specified by the    user, k is the minimal size of a minimum weight loop cutset, and n is    the number of vertices.  We also show empirically that a variant of    this algorithm often finds a loop cutset that is closer to the minimum    weight loop cutset than the ones found by the best deterministic    algorithms known.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0229",
        "title": "OBDD-based Universal Planning for Synchronized Agents in Non-Deterministic Domains",
        "authors": [
            "R. M. Jensen",
            "M. M. Veloso"
        ],
        "abstract": "Recently model checking representation and search techniques    were shown to be efficiently applicable to planning, in particular to    non-deterministic planning. Such planning approaches use Ordered    Binary Decision Diagrams (OBDDs) to encode a planning domain as a    non-deterministic finite automaton and then apply fast algorithms from    model checking to search for a solution. OBDDs can effectively scale    and can provide universal plans for complex planning domains. We are    particularly interested in addressing the complexities arising in    non-deterministic, multi-agent domains.  In this article, we present    UMOP, a new universal OBDD-based planning framework for    non-deterministic, multi-agent domains. We introduce a new planning    domain description language, NADL, to specify non-deterministic,    multi-agent domains.  The language contributes the explicit definition    of controllable agents and uncontrollable environment agents. We    describe the syntax and semantics of NADL and show how to build an    efficient OBDD-based representation of an NADL description.  The UMOP    planning system uses NADL and different OBDD-based universal planning    algorithms. It includes the previously developed strong and strong    cyclic planning algorithms. In addition, we introduce our new    optimistic planning algorithm that relaxes optimality guarantees and    generates plausible universal plans in some domains where no strong    nor strong cyclic solution exists. We present empirical results    applying UMOP to domains ranging from deterministic and single-agent    with no environment actions to non-deterministic and multi-agent with    complex environment actions. UMOP is shown to be a rich and efficient    planning system.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0230",
        "title": "Planning Graph as a (Dynamic) CSP: Exploiting EBL, DDB and other CSP Search Techniques in Graphplan",
        "authors": [
            "S. Kambhampati"
        ],
        "abstract": "This paper reviews the connections between Graphplan's    planning-graph and the dynamic constraint satisfaction problem and    motivates the need for adapting CSP search techniques to the Graphplan    algorithm.  It then describes how explanation based learning,    dependency directed backtracking, dynamic variable ordering, forward    checking, sticky values and random-restart search strategies can be    adapted to Graphplan. Empirical results are provided to demonstrate    that these augmentations improve Graphplan's performance significantly    (up to 1000x speedups) on several benchmark problems.  Special    attention is paid to the explanation-based learning and dependency    directed backtracking techniques as they are empirically found to be    most useful in improving the performance of Graphplan.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0233",
        "title": "Space Efficiency of Propositional Knowledge Representation Formalisms",
        "authors": [
            "M. Cadoli",
            "F. M. Donini",
            "P. Liberatore",
            "M. Schaerf"
        ],
        "abstract": "We investigate the space efficiency of a Propositional    Knowledge Representation (PKR) formalism. Intuitively, the space    efficiency of a formalism F in representing a certain piece of    knowledge A, is the size of the shortest formula of F that    represents A. In this paper we assume that knowledge is    either a set of propositional interpretations (models) or a set of    propositional formulae (theorems). We provide a formal way of    talking about the relative ability of PKR formalisms to compactly    represent a set of models or a set of theorems. We introduce two new    compactness measures, the corresponding classes, and show that the    relative space efficiency of a PKR formalism in representing    models/theorems is directly related to such classes. In particular,    we consider formalisms for nonmonotonic reasoning, such as    circumscription and default logic, as well as belief revision    operators and the stable model semantics for logic programs with    negation. One interesting result is that formalisms with the same    time complexity do not necessarily belong to the same space    efficiency class.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0234",
        "title": "Value-Function Approximations for Partially Observable Markov Decision Processes",
        "authors": [
            "M. Hauskrecht"
        ],
        "abstract": "Partially observable Markov decision processes (POMDPs)    provide an elegant mathematical framework for modeling complex    decision and planning problems in stochastic domains in which states    of the system are observable only indirectly, via a set of imperfect    or noisy observations. The modeling advantage of POMDPs, however,    comes at a price -- exact methods for solving them are computationally    very expensive and thus applicable in practice only to very simple    problems. We focus on efficient approximation (heuristic) methods that    attempt to alleviate the computational problem and trade off accuracy    for speed. We have two objectives here. First, we survey various    approximation methods, analyze their properties and relations and    provide some new insights into their differences. Second, we present a    number of new approximation methods and novel refinements of existing    techniques. The theoretical results are supported by experiments on a    problem from the agent navigation domain.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0237",
        "title": "On Deducing Conditional Independence from d-Separation in Causal Graphs with Feedback (Research Note)",
        "authors": [
            "R. M. Neal"
        ],
        "abstract": "Pearl and Dechter (1996) claimed that the d-separation    criterion for conditional independence in acyclic causal networks also    applies to networks of discrete variables that have feedback cycles,    provided that the variables of the system are uniquely determined by    the random disturbances.  I show by example that this is not true in    general.  Some condition stronger than uniqueness is needed, such as    the existence of a causal dynamics guaranteed to lead to the unique    solution.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0238",
        "title": "What's in an Attribute? Consequences for the Least Common Subsumer",
        "authors": [
            "A. Borgida",
            "R. Kusters"
        ],
        "abstract": "Functional relationships between objects, called    `attributes', are of considerable importance in knowledge    representation languages, including Description Logics (DLs). A study    of the literature indicates that papers have made, often implicitly,    different assumptions about the nature of attributes: whether they are    always required to have a value, or whether they can be partial    functions. The work presented here is the first explicit study of this    difference for subclasses of the CLASSIC DL, involving the same-as    concept constructor.  It is shown that although determining    subsumption between concept descriptions has the same complexity    (though requiring different algorithms), the story is different in the    case of determining the least common subsumer (lcs). For attributes    interpreted as partial functions, the lcs exists and can be computed    relatively easily; even in this case our results correct and extend    three previous papers about the lcs of DLs.  In the case where    attributes must have a value, the lcs may not exist, and even if it    exists it may be of exponential size.  Interestingly, it is possible    to decide in polynomial time if the lcs exists.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0239",
        "title": "The Complexity of Reasoning with Cardinality Restrictions and Nominals in Expressive Description Logics",
        "authors": [
            "S. Tobies"
        ],
        "abstract": "We study the complexity of the combination of the    Description Logics ALCQ and ALCQI with a terminological formalism    based on cardinality restrictions on concepts. These combinations can    naturally be embedded into C^2, the two variable fragment of predicate    logic with counting quantifiers, which yields decidability in    NExpTime. We show that this approach leads to an optimal solution for    ALCQI, as ALCQI with cardinality restrictions has the same complexity    as C^2 (NExpTime-complete). In contrast, we show that for ALCQ, the    problem can be solved in ExpTime. This result is obtained by a    reduction of reasoning with cardinality restrictions to reasoning with    the (in general weaker) terminological formalism of general axioms for    ALCQ extended with nominals. Using the same reduction, we show that,    for the extension of ALCQI with nominals, reasoning with general    axioms is a NExpTime-complete problem. Finally, we sharpen this result    and show that pure concept satisfiability for ALCQI with nominals is    NExpTime-complete. Without nominals, this problem is known to be    PSpace-complete.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0240",
        "title": "Backbone Fragility and the Local Search Cost Peak",
        "authors": [
            "I. P. Gent",
            "J. Singer",
            "A. Smaill"
        ],
        "abstract": "The local search algorithm WSat is one of the most    successful algorithms for solving the satisfiability (SAT) problem. It    is notably effective at solving hard Random 3-SAT instances near the    so-called `satisfiability threshold', but still shows a peak in search    cost near the threshold and large variations in cost over different    instances. We make a number of significant contributions to the    analysis of WSat on high-cost random instances, using the    recently-introduced concept of the backbone of a SAT instance. The    backbone is the set of literals which are entailed by an instance. We    find that the number of solutions predicts the cost well for    small-backbone instances but is much less relevant for the    large-backbone instances which appear near the threshold and dominate    in the overconstrained region. We show a very strong correlation    between search cost and the Hamming distance to the nearest solution    early in WSat's search. This pattern leads us to introduce a measure    of the backbone fragility of an instance, which indicates how    persistent the backbone is as clauses are removed. We propose that    high-cost random instances for local search are those with very large    backbones which are also backbone-fragile. We suggest that the decay    in cost beyond the satisfiability threshold is due to increasing    backbone robustness (the opposite of backbone fragility). Our    hypothesis makes three correct predictions. First, that the backbone    robustness of an instance is negatively correlated with the local    search cost when other factors are controlled for. Second, that    backbone-minimal instances (which are 3-SAT instances altered so as to    be more backbone-fragile) are unusually hard for WSat. Third, that the    clauses most often unsatisfied during search are those whose deletion    has the most effect on the backbone. In understanding the pathologies    of local search methods, we hope to contribute to the development of    new and better techniques.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0241",
        "title": "An Application of Reinforcement Learning to Dialogue Strategy Selection in a Spoken Dialogue System for Email",
        "authors": [
            "M. A. Walker"
        ],
        "abstract": "This paper describes a novel method by which a spoken    dialogue system can learn to choose an optimal dialogue strategy from    its experience interacting with human users.  The method is based on a    combination of reinforcement learning and performance modeling of    spoken dialogue systems.  The reinforcement learning component applies    Q-learning (Watkins, 1989), while the performance modeling component    applies the PARADISE evaluation framework (Walker et al., 1997) to    learn the performance function (reward) used in reinforcement    learning.  We illustrate the method with a spoken dialogue system    named ELVIS (EmaiL Voice Interactive System), that supports access to    email over the phone.  We conduct a set of experiments for training an    optimal dialogue strategy on a corpus of 219 dialogues in which human    users interact with ELVIS over the phone. We then test that strategy    on a corpus of 18 dialogues.  We show that ELVIS can learn to optimize    its strategy selection for agent initiative, for reading messages, and    for summarizing email folders.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0242",
        "title": "Nonapproximability Results for Partially Observable Markov Decision Processes",
        "authors": [
            "J. Goldsmith",
            "C. Lusena",
            "M. Mundhenk"
        ],
        "abstract": "We show that for several variations of partially observable    Markov decision processes, polynomial-time algorithms for finding    control policies are unlikely to or simply don't have guarantees of    finding policies within a constant factor or a constant summand of    optimal.  Here \"unlikely\" means \"unless some complexity classes    collapse,\" where the collapses considered are P=NP, P=PSPACE, or    P=EXP.  Until or unless these collapses are shown to hold, any    control-policy designer must choose between such performance    guarantees and efficient computation.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0243",
        "title": "On Reasonable and Forced Goal Orderings and their Use in an Agenda-Driven Planning Algorithm",
        "authors": [
            "J. Hoffmann",
            "J. Koehler"
        ],
        "abstract": "The paper addresses the problem of computing goal orderings,  which is one of the longstanding issues in AI planning.  It makes two new contributions.  First, it formally defines and discusses two    different goal orderings, which are called the reasonable and the forced ordering. Both orderings are defined for simple STRIPS operators as well as for more complex ADL operators supporting    negation and conditional effects. The complexity of these orderings is investigated and their practical relevance is discussed. Secondly, two different methods to compute reasonable goal orderings are developed.  One of them is based on planning graphs, while the other investigates the set of actions directly. Finally, it is shown how the ordering relations, which have been derived for a given set of goals G, can be used to compute a so-called goal agenda that divides G into an ordered set of subgoals. Any planner can then, in principle, use the goal agenda to plan for increasing sets of subgoals.  This can lead to an exponential complexity reduction, as the solution to a complex planning problem is found by solving easier subproblems. Since only a polynomial overhead is caused by the goal agenda computation, a potential exists to dramatically speed up planning algorithms as we demonstrate in the empirical evaluation, where we use this method in the IPP planner.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0244",
        "title": "Asimovian Adaptive Agents",
        "authors": [
            "D. F. Gordon"
        ],
        "abstract": "The goal of this research is to develop agents that     are adaptive and predictable and timely. At first blush,     these three requirements seem contradictory. For example,      adaptation risks introducing undesirable side effects,     thereby making agents' behavior less predictable. Furthermore,    although formal verification can assist in ensuring    behavioral predictability, it is known to be time-consuming.  Our solution to the challenge of satisfying all three    requirements is the following. Agents have finite-state    automaton plans, which are adapted online via evolutionary    learning (perturbation) operators. To ensure that critical    behavioral constraints are always satisfied, agents' plans    are first formally verified. They are then reverified after    every adaptation. If reverification concludes that constraints    are violated, the plans are repaired. The main objective of     this paper is to improve the efficiency of reverification     after learning, so that agents have a sufficiently rapid     response time. We present two solutions: positive results     that certain learning operators are a priori guaranteed to    preserve useful classes of behavioral assurance constraints    (which implies that no reverification is needed for these     operators), and efficient incremental reverification algorithms     for those learning operators that have negative a priori results.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0245",
        "title": "A Model of Inductive Bias Learning",
        "authors": [
            "J. Baxter"
        ],
        "abstract": "A major problem in machine learning is that of inductive    bias: how to choose a learner's hypothesis space so that it is large    enough to contain a solution to the problem being learnt, yet small    enough to ensure reliable generalization from reasonably-sized    training sets.  Typically such bias is supplied by hand through the    skill and insights of experts. In this paper a model for automatically    learning bias is investigated. The central assumption of the model is    that the learner is embedded within an environment of related learning    tasks. Within such an environment the learner can sample from multiple    tasks, and hence it can search for a hypothesis space that contains    good solutions to many of the problems in the environment. Under    certain restrictions on the set of all hypothesis spaces available to    the learner, we show that a hypothesis space that performs well on a    sufficiently large number of training tasks will also perform well    when learning novel tasks in the same environment.  Explicit bounds    are also derived demonstrating that learning multiple tasks within an    environment of related tasks can potentially give much better    generalization than learning a single task.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0246",
        "title": "Mean Field Methods for a Special Class of Belief Networks",
        "authors": [
            "C. Bhattacharyya",
            "S. S. Keerthi"
        ],
        "abstract": "The chief aim of this paper is to propose mean-field    approximations for a broad class of Belief networks, of which sigmoid    and noisy-or networks can be seen as special cases.  The     approximations are based on a powerful mean-field theory suggested by    Plefka.  We show that Saul, Jaakkola and Jordan' s approach is the    first order approximation in Plefka's approach, via a variational    derivation.  The application of Plefka's theory to belief networks is    not computationally tractable.  To tackle this problem we propose new    approximations based on Taylor series.  Small scale experiments show    that the proposed schemes are attractive.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0247",
        "title": "On the Compilability and Expressive Power of Propositional Planning Formalisms",
        "authors": [
            "B. Nebel"
        ],
        "abstract": "The recent approaches of extending the GRAPHPLAN algorithm    to handle more expressive planning formalisms raise the question of    what the formal meaning of \"expressive power\" is. We formalize the    intuition that expressive power is a measure of how concisely planning    domains and plans can be expressed in a particular formalism by    introducing the notion of \"compilation schemes\" between planning    formalisms.  Using this notion, we analyze the expressiveness of a    large family of propositional planning formalisms, ranging from basic    STRIPS to a formalism with conditional effects, partial state    specifications, and propositional formulae in the preconditions.  One    of the results is that conditional effects cannot be compiled away if    plan size should grow only linearly but can be compiled away if we    allow for polynomial growth of the resulting plans. This result    confirms that the recently proposed extensions to the GRAPHPLAN    algorithm concerning conditional effects are optimal with respect to    the \"compilability\" framework.  Another result is that general    propositional formulae cannot be compiled into conditional effects if    the plan size should be preserved linearly.  This implies that    allowing general propositional formulae in preconditions and effect    conditions adds another level of difficulty in generating a plan.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0249",
        "title": "Partial-Order Planning with Concurrent Interacting Actions",
        "authors": [
            "C. Boutilier",
            "R. I. Brafman"
        ],
        "abstract": "In order to generate plans for agents with multiple    actuators, agent teams, or distributed controllers, we must be able to    represent and plan using concurrent actions with interacting    effects. This has historically been considered a challenging task    requiring a temporal planner with the ability to reason explicitly    about time. We show that with simple modifications, the STRIPS action    representation language can be used to represent interacting actions.    Moreover, algorithms for partial-order planning require only small    modifications in order to be applied in such multiagent domains.  We    demonstrate this fact by developing a sound and complete partial-order    planner for planning with concurrent interacting actions, POMP, that    extends existing partial-order planners in a straightforward    way. These results open the way to the use of partial-order planners    for the centralized control of cooperative multiagent systems.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0250",
        "title": "Planning by Rewriting",
        "authors": [
            "J. L. Ambite",
            "C. A. Knoblock"
        ],
        "abstract": "Domain-independent planning is a hard combinatorial    problem. Taking into account plan quality makes the task even more    difficult. This article introduces Planning by Rewriting (PbR), a new    paradigm for efficient high-quality domain-independent planning. PbR    exploits declarative plan-rewriting rules and efficient local search    techniques to transform an easy-to-generate, but possibly suboptimal,    initial plan into a high-quality plan. In addition to addressing the    issues of planning efficiency and plan quality, this framework offers    a new anytime planning algorithm. We have implemented this planner and    applied it to several existing domains. The experimental results show    that the PbR approach provides significant savings in planning effort    while generating high-quality plans.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0251",
        "title": "Speeding Up the Convergence of Value Iteration in Partially Observable Markov Decision Processes",
        "authors": [
            "N. L. Zhang",
            "W. Zhang"
        ],
        "abstract": "Partially observable Markov decision processes (POMDPs) have    recently become popular among many AI researchers because they serve    as a natural model for planning under uncertainty.  Value iteration is    a well-known algorithm for finding optimal policies for POMDPs.  It    typically takes a large number of iterations to converge.  This paper    proposes a method for accelerating the convergence of value iteration.    The method has been evaluated on an array of benchmark problems and    was found to be very effective: It enabled value iteration to converge    after only a few iterations on all the test problems.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0252",
        "title": "Conformant Planning via Symbolic Model Checking",
        "authors": [
            "A. Cimatti",
            "M. Roveri"
        ],
        "abstract": "We tackle the problem of planning in nondeterministic    domains, by presenting a new approach to conformant planning.    Conformant planning is the problem of finding a sequence of actions    that is guaranteed to achieve the goal despite the nondeterminism of    the domain. Our approach is based on the representation of the    planning domain as a finite state automaton. We use Symbolic Model    Checking techniques, in particular Binary Decision Diagrams, to    compactly represent and efficiently search the automaton. In this    paper we make the following contributions. First, we present a general    planning algorithm for conformant planning, which applies to fully    nondeterministic domains, with uncertainty in the initial condition    and in action effects. The algorithm is based on a breadth-first,    backward search, and returns conformant plans of minimal length, if a    solution to the planning problem exists, otherwise it terminates    concluding that the problem admits no conformant solution. Second, we    provide a symbolic representation of the search space based on Binary    Decision Diagrams (BDDs), which is the basis for search techniques    derived from symbolic model checking. The symbolic representation    makes it possible to analyze potentially large sets of states and    transitions in a single computation step, thus providing for an    efficient implementation.  Third, we present CMBP (Conformant Model    Based Planner), an efficient implementation of the data structures and    algorithm described above, directly based on BDD manipulations, which    allows for a compact representation of the search layers and an    efficient implementation of the search steps. Finally, we present an    experimental comparison of our approach with the state-of-the-art    conformant planners CGP, QBFPLAN and GPT. Our analysis includes all    the planning problems from the distribution packages of these systems,    plus other problems defined to stress a number of specific factors.    Our approach appears to be the most effective: CMBP is strictly more    expressive than QBFPLAN and CGP and, in all the problems where a    comparison is possible, CMBP outperforms its competitors, sometimes by    orders of magnitude.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0253",
        "title": "AIS-BN: An Adaptive Importance Sampling Algorithm for Evidential Reasoning in Large Bayesian Networks",
        "authors": [
            "J. Cheng",
            "M. J. Druzdzel"
        ],
        "abstract": "Stochastic sampling algorithms, while an attractive alternative to    exact algorithms in very large Bayesian network models, have been    observed to perform poorly in evidential reasoning with extremely    unlikely evidence. To address this problem, we propose an adaptive    importance sampling algorithm, AIS-BN, that shows promising    convergence rates even under extreme conditions and seems to    outperform the existing sampling algorithms consistently. Three    sources of this performance improvement are (1) two heuristics for    initialization of the importance function that are based on the    theoretical properties of importance sampling in finite-dimensional    integrals and the structural advantages of Bayesian networks, (2) a    smooth learning method for the importance function, and (3) a dynamic    weighting function for combining samples from different stages of the    algorithm.       We tested the performance of the AIS-BN algorithm along with two state    of the art general purpose sampling algorithms, likelihood weighting    (Fung and Chang, 1989; Shachter and Peot, 1989) and self-importance    sampling (Shachter and Peot, 1989). We used in our tests three large    real Bayesian network models available to the scientific community:    the CPCS network (Pradhan et al., 1994), the PathFinder network    (Heckerman, Horvitz, and Nathwani, 1990), and the ANDES network (Conati,    Gertner, VanLehn, and Druzdzel, 1997), with evidence as unlikely as    10^-41. While the AIS-BN algorithm always performed better than the    other two algorithms, in the majority of the test cases it achieved    orders of magnitude improvement in precision of the results.    Improvement in speed given a desired precision is even more dramatic,    although we are unable to report numerical results here, as the other    algorithms almost never achieved the precision reached even by the    first few iterations of the AIS-BN algorithm.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0254",
        "title": "Conflict-Directed Backjumping Revisited",
        "authors": [
            "X. Chen",
            "P. van Beek"
        ],
        "abstract": "In recent years, many improvements to backtracking algorithms for    solving constraint satisfaction problems have been proposed.    The techniques for improving backtracking algorithms can    be conveniently classified as look-ahead schemes and    look-back schemes.  Unfortunately, look-ahead and look-back    schemes are not entirely orthogonal as it has been observed    empirically that the enhancement of look-ahead techniques    is sometimes counterproductive to the effects of look-back    techniques. In this paper, we focus on the relationship between    the two most important look-ahead techniques---using a variable    ordering heuristic and maintaining a level of local consistency    during the backtracking search---and the look-back technique of    conflict-directed backjumping (CBJ). We show that there exists    a \"perfect\" dynamic variable ordering such that CBJ becomes    redundant. We also show theoretically that as the level of local    consistency that is maintained in the backtracking search is    increased, the less that backjumping will be an improvement.    Our theoretical results partially explain why a backtracking    algorithm doing more in the look-ahead phase cannot benefit    more from the backjumping look-back scheme. Finally, we show    empirically that adding CBJ to a backtracking algorithm that    maintains generalized arc consistency (GAC), an algorithm that    we refer to as GAC-CBJ, can still provide orders of magnitude    speedups. Our empirical results contrast with Bessiere and    Regin's conclusion (1996) that CBJ is useless to an algorithm    that maintains arc consistency.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0256",
        "title": "Grounding the Lexical Semantics of Verbs in Visual Perception using Force Dynamics and Event Logic",
        "authors": [
            "J. M. Siskind"
        ],
        "abstract": "This paper presents an implemented system for recognizing    the occurrence of events described by simple spatial-motion verbs in    short image sequences. The semantics of these verbs is specified with    event-logic expressions that describe changes in the state of    force-dynamic relations between the participants of the event.  An    efficient finite representation is introduced for the infinite sets of    intervals that occur when describing liquid and semi-liquid events.    Additionally, an efficient procedure using this representation is    presented for inferring occurrences of compound events, described with    event-logic expressions, from occurrences of primitive events.  Using    force dynamics and event logic to specify the lexical semantics of    events allows the system to be more robust than prior systems based on    motion profile.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0257",
        "title": "Popular Ensemble Methods: An Empirical Study",
        "authors": [
            "R. Maclin",
            "D. Opitz"
        ],
        "abstract": "An ensemble consists of a set of individually trained    classifiers (such as neural networks or decision trees) whose    predictions are combined when classifying novel instances.  Previous    research has shown that an ensemble is often more accurate than any of    the single classifiers in the ensemble.  Bagging (Breiman, 1996c) and    Boosting (Freund and Shapire, 1996; Shapire, 1990) are two relatively    new but popular methods for producing ensembles.  In this paper we    evaluate these methods on 23 data sets using both neural networks and    decision trees as our classification algorithm.  Our results clearly    indicate a number of conclusions.  First, while Bagging is almost    always more accurate than a single classifier, it is sometimes much    less accurate than Boosting.  On the other hand, Boosting can create    ensembles that are less accurate than a single classifier --    especially when using neural networks.  Analysis indicates that the    performance of the Boosting methods is dependent on the    characteristics of the data set being examined.  In fact, further    results show that Boosting ensembles may overfit noisy data sets, thus    decreasing its performance.  Finally, consistent with previous    studies, our work suggests that most of the gain in an ensemble's    performance comes in the first few classifiers combined; however,    relatively large gains can be seen up to 25 classifiers when Boosting    decision trees.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0284",
        "title": "An Evolutionary Algorithm with Advanced Goal and Priority Specification for Multi-objective Optimization",
        "authors": [
            "E. F. Khor",
            "T. H. Lee",
            "R. Sathikannan",
            "K. C. Tan"
        ],
        "abstract": "This paper presents an evolutionary algorithm with a new    goal-sequence domination scheme for better decision support in    multi-objective optimization. The approach allows the inclusion of    advanced hard/soft priority and constraint information on each    objective component, and is capable of incorporating multiple    specifications with overlapping or non-overlapping objective functions    via logical 'OR' and 'AND' connectives to drive the search    towards multiple regions of trade-off. In addition, we propose a    dynamic sharing scheme that is simple and adaptively estimated    according to the on-line population distribution without needing any a    priori parameter setting. Each feature in the proposed algorithm is    examined to show its respective contribution, and the performance of    the algorithm is compared with other evolutionary optimization    methods. It is shown that the proposed algorithm has performed well in    the diversity of evolutionary search and uniform distribution of    non-dominated individuals along the final trade-offs, without    significant computational effort. The algorithm is also applied to the    design optimization of a practical servo control system for hard disk    drives with a single voice-coil-motor actuator. Results of the    evolutionary designed servo control system show a superior closed-loop    performance compared to classical PID or RPT approaches.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0285",
        "title": "The GRT Planning System: Backward Heuristic Construction in Forward State-Space Planning",
        "authors": [
            "I. Refanidis",
            "I. Vlahavas"
        ],
        "abstract": "This paper presents GRT, a domain-independent heuristic    planning system for STRIPS worlds. GRT solves problems in two    phases. In the pre-processing phase, it estimates the distance between    each fact and the goals of the problem, in a backward direction. Then,    in the search phase, these estimates are used in order to further    estimate the distance between each intermediate state and the goals,    guiding so the search process in a forward direction and on a    best-first basis. The paper presents the benefits from the adoption of    opposite directions between the preprocessing and the search phases,    discusses some difficulties that arise in the pre-processing phase and    introduces techniques to cope with them. Moreover, it presents several    methods of improving the efficiency of the heuristic, by enriching the    representation and by reducing the size of the problem. Finally, a    method of overcoming local optimal states, based on domain axioms, is    proposed. According to it, difficult problems are decomposed into    easier sub-problems that have to be solved sequentially. The    performance results from various domains, including those of the    recent planning competitions, show that GRT is among the fastest    planners.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0483",
        "title": "Learning unbelievable marginal probabilities",
        "authors": [
            "Xaq Pitkow",
            "Yashar Ahmadian",
            "Ken D. Miller"
        ],
        "abstract": "Loopy belief propagation performs approximate inference on graphical models with loops. One might hope to compensate for the approximation by adjusting model parameters. Learning algorithms for this purpose have been explored previously, and the claim has been made that every set of locally consistent marginals can arise from belief propagation run on a graphical model. On the contrary, here we show that many probability distributions have marginals that cannot be reached by belief propagation using any set of model parameters or any learning algorithm. We call such marginals `unbelievable.' This problem occurs whenever the Hessian of the Bethe free energy is not positive-definite at the target marginals. All learning algorithms for belief propagation necessarily fail in these cases, producing beliefs or sets of beliefs that may even be worse than the pre-learning approximation. We then show that averaging inaccurate beliefs, each obtained from belief propagation using model parameters perturbed about some learned mean values, can achieve the unbelievable marginals.\n    ",
        "submission_date": "2011-06-02T00:00:00",
        "last_modified_date": "2011-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0566",
        "title": "The Impact of Mutation Rate on the Computation Time of Evolutionary Dynamic Optimization",
        "authors": [
            "Tianshi Chen",
            "Yunji Chen",
            "Ke Tang",
            "Guoliang Chen",
            "Xin Yao"
        ],
        "abstract": "Mutation has traditionally been regarded as an important operator in evolutionary algorithms. In particular, there have been many experimental studies which showed the effectiveness of adapting mutation rates for various static optimization problems. Given the perceived effectiveness of adaptive and self-adaptive mutation for static optimization problems, there have been speculations that adaptive and self-adaptive mutation can benefit dynamic optimization problems even more since adaptation and self-adaptation are capable of following a dynamic environment. However, few theoretical results are available in analyzing rigorously evolutionary algorithms for dynamic optimization problems. It is unclear when adaptive and self-adaptive mutation rates are likely to be useful for evolutionary algorithms in solving dynamic optimization problems. This paper provides the first rigorous analysis of adaptive mutation and its impact on the computation times of evolutionary algorithms in solving certain dynamic optimization problems. More specifically, for both individual-based and population-based EAs, we have shown that any time-variable mutation rate scheme will not significantly outperform a fixed mutation rate on some dynamic optimization problem instances. The proofs also offer some insights into conditions under which any time-variable mutation scheme is unlikely to be useful and into the relationships between the problem characteristics and algorithmic features (e.g., different mutation schemes).\n    ",
        "submission_date": "2011-06-03T00:00:00",
        "last_modified_date": "2011-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0664",
        "title": "The Complexity of Reasoning about Spatial Congruence",
        "authors": [
            "M. Cristani"
        ],
        "abstract": "In the recent literature of Artificial Intelligence, an    intensive research effort has been spent, for various algebras of    qualitative relations used in the representation of temporal and    spatial knowledge, on the problem of classifying the computational    complexity of reasoning problems for subsets of algebras.  The main    purpose of these researches is to describe a restricted set of maximal    tractable subalgebras, ideally in an exhaustive fashion with respect    to the hosting algebras.      In this paper we introduce a novel algebra for reasoning about Spatial    Congruence, show that the satisfiability problem in the spatial    algebra MC-4 is NP-complete, and present a complete classification of    tractability in the algebra, based on the individuation of three    maximal tractable subclasses, one containing the basic relations.  The    three algebras are formed by 14, 10 and 9 relations out of 16 which    form the full algebra.\n    ",
        "submission_date": "2011-06-03T00:00:00",
        "last_modified_date": "2011-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0665",
        "title": "Infinite-Horizon Policy-Gradient Estimation",
        "authors": [
            "Jonathan Baxter",
            "Peter L. Bartlett"
        ],
        "abstract": "Gradient-based approaches to direct policy search in reinforcement learning have received much recent attention as a means to solve problems of partial observability and to avoid some of the problems associated with policy degradation in value-function methods. In this paper we introduce GPOMDP, a simulation-based algorithm for generating a {\\em biased} estimate of the gradient of the {\\em average reward} in Partially Observable Markov Decision Processes (POMDPs) controlled by parameterized stochastic policies. A similar algorithm was proposed by Kimura, Yamamura, and Kobayashi (1995). The algorithm's chief advantages are that it requires storage of only twice the number of policy parameters, uses one free parameter $\\beta\\in [0,1)$ (which has a natural interpretation in terms of bias-variance trade-off), and requires no knowledge of the underlying state. We prove convergence of GPOMDP, and show how the correct choice of the parameter $\\beta$ is related to the {\\em mixing time} of the controlled POMDP. We briefly describe extensions of GPOMDP to controlled Markov chains, continuous state, observation and control spaces, multiple-agents, higher-order derivatives, and a version for training stochastic policies with internal states. In a companion paper (Baxter, Bartlett, & Weaver, 2001) we show how the gradient estimates generated by GPOMDP can be used in both a traditional stochastic gradient algorithm and a conjugate-gradient procedure to find local optima of the average reward\n    ",
        "submission_date": "2011-06-03T00:00:00",
        "last_modified_date": "2019-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0666",
        "title": "Experiments with Infinite-Horizon, Policy-Gradient Estimation",
        "authors": [
            "J. Baxter",
            "P. L. Bartlett",
            "L. Weaver"
        ],
        "abstract": "In this paper, we present algorithms that perform gradient ascent of the average reward in a partially observable Markov decision process (POMDP). These algorithms are based on GPOMDP, an algorithm introduced in a companion paper (Baxter and Bartlett, this volume), which computes biased estimates of the performance gradient in POMDPs. The algorithm's chief advantages are that it uses only one free parameter beta, which has a natural interpretation in terms of bias-variance trade-off, it requires no knowledge of the underlying state, and it can be applied to infinite state, control and observation spaces. We show how the gradient estimates produced by GPOMDP can be used to perform gradient ascent, both with a traditional stochastic-gradient algorithm, and with an algorithm based on conjugate-gradients that utilizes gradient information to bracket maxima in line searches. Experimental results are presented illustrating both the theoretical results of (Baxter and Bartlett, this volume) on a toy problem, and practical aspects of the algorithms on a number of more realistic problems.\n    ",
        "submission_date": "2011-06-03T00:00:00",
        "last_modified_date": "2019-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0667",
        "title": "Reasoning within Fuzzy Description Logics",
        "authors": [
            "U. Straccia"
        ],
        "abstract": "Description Logics (DLs) are suitable, well-known, logics    for managing structured knowledge.  They allow reasoning about    individuals and well defined concepts, i.e., set of individuals with    common properties.  The experience in using DLs in applications has    shown that in many cases we would like to extend their capabilities.    In particular, their use in the context of Multimedia Information    Retrieval (MIR) leads to the convincement that such DLs should allow    the treatment of the inherent imprecision in multimedia object content    representation and retrieval.           In this paper we will present a fuzzy extension of ALC, combining    Zadeh's fuzzy logic with a classical DL. In particular, concepts    becomes fuzzy and, thus, reasoning about imprecise concepts is    supported.  We will define its syntax, its semantics, describe its    properties and present a constraint propagation calculus for reasoning    in it.\n    ",
        "submission_date": "2011-06-03T00:00:00",
        "last_modified_date": "2011-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0668",
        "title": "An Analysis of Reduced Error Pruning",
        "authors": [
            "T. Elomaa",
            "M. Kaariainen"
        ],
        "abstract": "Top-down induction of decision trees has been observed to    suffer from the inadequate functioning of the pruning phase.  In    particular, it is known that the size of the resulting tree grows    linearly with the sample size, even though the accuracy of the tree    does not improve.  Reduced Error Pruning is an algorithm that has been    used as a representative technique in attempts to explain the problems    of decision tree learning.      In this paper we present analyses of Reduced Error Pruning in three    different settings.  First we study the basic algorithmic properties    of the method, properties that hold independent of the input decision    tree and pruning examples.  Then we examine a situation that    intuitively should lead to the subtree under consideration to be    replaced by a leaf node, one in which the class label and attribute    values of the pruning examples are independent of each other.  This    analysis is conducted under two different assumptions.  The general    analysis shows that the pruning probability of a node fitting pure    noise is bounded by a function that decreases exponentially as the    size of the tree grows.  In a specific analysis we assume that the    examples are distributed uniformly to the tree.  This assumption lets    us approximate the number of subtrees that are pruned because they do    not receive any pruning examples.      This paper clarifies the different variants of the Reduced Error    Pruning algorithm, brings new insight to its algorithmic properties,    analyses the algorithm with less imposed assumptions than before, and    includes the previously overlooked empty subtrees to the analysis.\n    ",
        "submission_date": "2011-06-03T00:00:00",
        "last_modified_date": "2011-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0669",
        "title": "GIB: Imperfect Information in a Computationally Challenging Game",
        "authors": [
            "M. L. Ginsberg"
        ],
        "abstract": "This paper investigates the problems arising in the    construction of a program to play the game of contract bridge.  These    problems include both the difficulty of solving the game's perfect    information variant, and techniques needed to address the fact that    bridge is not, in fact, a perfect information game.  GIB, the program    being described, involves five separate technical advances: partition    search, the practical application of Monte Carlo techniques to    realistic problems, a focus on achievable sets to solve problems    inherent in the Monte Carlo approach, an extension of alpha-beta    pruning from total orders to arbitrary distributive lattices, and the    use of squeaky wheel optimization to find approximately optimal    solutions to cardplay problems.     GIB is currently believed to be of approximately expert caliber, and    is currently the strongest computer bridge program in the world.\n    ",
        "submission_date": "2011-06-03T00:00:00",
        "last_modified_date": "2011-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0671",
        "title": "Domain Filtering Consistencies",
        "authors": [
            "C. Bessiere",
            "R. Debruyne"
        ],
        "abstract": "Enforcing local consistencies is one of the main features    of constraint reasoning. Which level of local consistency should be    used when searching for solutions in a constraint network is a basic    question. Arc consistency and partial forms of arc consistency have    been widely studied, and have been known for sometime through the    forward checking or the MAC search algorithms. Until recently,    stronger forms of local consistency remained limited to those that    change the structure of the constraint graph, and thus, could not be    used in practice, especially on large networks. This paper focuses on    the local consistencies that are stronger than arc consistency,    without changing the structure of the network, i.e., only removing    inconsistent values from the domains. In the last five years, several    such local consistencies have been proposed by us or by others. We    make an overview of all of them, and highlight some relations between    them. We compare them both theoretically and experimentally,    considering their pruning efficiency and the time required to enforce    them.\n    ",
        "submission_date": "2011-06-03T00:00:00",
        "last_modified_date": "2011-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0672",
        "title": "Policy Recognition in the Abstract Hidden Markov Model",
        "authors": [
            "H. H. Bui",
            "S. Venkatesh",
            "G. West"
        ],
        "abstract": "In this paper, we present a method for recognising an    agent's behaviour in dynamic, noisy, uncertain domains, and across    multiple levels of abstraction.  We term this problem on-line plan    recognition under uncertainty and view it generally as probabilistic    inference on the stochastic process representing the execution of the    agent's plan. Our contributions in this paper are twofold.  In terms    of probabilistic inference, we introduce the Abstract Hidden Markov    Model (AHMM), a novel type of stochastic processes, provide its    dynamic Bayesian network (DBN) structure and analyse the properties of    this network.  We then describe an application of the    Rao-Blackwellised Particle Filter to the AHMM which allows us to    construct an efficient, hybrid inference method for this model.  In    terms of plan recognition, we propose a novel plan recognition    framework based on the AHMM as the plan execution model.  The    Rao-Blackwellised hybrid inference for AHMM can take advantage of the    independence properties inherent in a model of plan execution, leading    to an algorithm for online probabilistic plan recognition that scales    well with the number of levels in the plan hierarchy.  This    illustrates that while stochastic models for plan execution can be    complex, they exhibit special structures which, if exploited, can lead    to efficient plan recognition algorithms.  We demonstrate the    usefulness of the AHMM framework via a behaviour recognition system in    a complex spatial environment using distributed video surveillance    data.\n    ",
        "submission_date": "2011-06-03T00:00:00",
        "last_modified_date": "2011-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0675",
        "title": "The FF Planning System: Fast Plan Generation Through Heuristic Search",
        "authors": [
            "J. Hoffmann",
            "B. Nebel"
        ],
        "abstract": "We describe and evaluate the algorithmic techniques that are    used in the FF planning system. Like the HSP system, FF relies on    forward state space search, using a heuristic that estimates goal    distances by ignoring delete lists. Unlike HSP's heuristic, our method    does not assume facts to be independent. We introduce a novel search    strategy that combines hill-climbing with systematic search, and we    show how other powerful heuristic information can be extracted and    used to prune the search space. FF was the most successful automatic    planner at the recent AIPS-2000 planning competition. We review the    results of the competition, give data for other benchmark domains, and    investigate the reasons for the runtime performance of FF compared to    HSP.\n    ",
        "submission_date": "2011-06-03T00:00:00",
        "last_modified_date": "2011-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0678",
        "title": "ATTac-2000: An Adaptive Autonomous Bidding Agent",
        "authors": [
            "M. Kearns",
            "M. L. Littman",
            "S. Singh",
            "P. Stone"
        ],
        "abstract": "The First Trading Agent Competition (TAC) was held from June    22nd to July 8th, 2000.  TAC was designed to create a benchmark    problem in the complex domain of e-marketplaces and to motivate    researchers to apply unique approaches to a common task.  This article    describes ATTac-2000, the first-place finisher in TAC.  ATTac-2000    uses a principled bidding strategy that includes several elements of    adaptivity.  In addition to the success at the competition, isolated    empirical results are presented indicating the robustness and    effectiveness of ATTac-2000's adaptive strategy.\n    ",
        "submission_date": "2011-06-03T00:00:00",
        "last_modified_date": "2011-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0679",
        "title": "Efficient Methods for Qualitative Spatial Reasoning",
        "authors": [
            "B. Nebel",
            "J. Renz"
        ],
        "abstract": "The theoretical properties of qualitative spatial reasoning    in the RCC8 framework have been analyzed extensively. However, no    empirical investigation has been made yet. Our experiments show that    the adaption of the algorithms used for qualitative temporal reasoning    can solve large RCC8 instances, even if they are in the phase    transition region -- provided that one uses the maximal tractable    subsets of RCC8 that have been identified by us. In particular, we    demonstrate that the orthogonal combination of heuristic methods is    successful in solving almost all apparently hard instances in the    phase transition region up to a certain size in reasonable time.\n    ",
        "submission_date": "2011-06-03T00:00:00",
        "last_modified_date": "2011-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0680",
        "title": "Learning Geometrically-Constrained Hidden Markov Models for Robot Navigation: Bridging the Topological-Geometrical Gap",
        "authors": [
            "L. P. Kaelbling",
            "H. Shatkay"
        ],
        "abstract": "Hidden Markov models (HMMs) and partially observable Markov    decision processes (POMDPs) provide useful tools for modeling    dynamical systems.  They are particularly useful for representing the    topology of environments such as road networks and office buildings,    which are typical for robot navigation and planning.  The work    presented here describes a formal framework for incorporating readily    available odometric information and geometrical constraints into both    the models and the algorithm that learns them.  By taking advantage of    such information, learning HMMs/POMDPs can be made to generate better    solutions and require fewer iterations, while being robust in the face    of data reduction.  Experimental results, obtained from both simulated    and real robot data, demonstrate the effectiveness of the approach.\n    ",
        "submission_date": "2011-06-03T00:00:00",
        "last_modified_date": "2011-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0776",
        "title": "Semantics for Possibilistic Disjunctive Programs",
        "authors": [
            "Juan Carlos Nieves",
            "Mauricio Osorio",
            "Ulises Cort\u00e9s"
        ],
        "abstract": "In this paper, a possibilistic disjunctive logic programming approach for modeling uncertain, incomplete and inconsistent information is defined. This approach introduces the use of possibilistic disjunctive clauses which are able to capture incomplete information and incomplete states of a knowledge base at the same time.\n",
        "submission_date": "2011-06-03T00:00:00",
        "last_modified_date": "2011-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1510",
        "title": "Towards OWL-based Knowledge Representation in Petrology",
        "authors": [
            "Alex Shkotin",
            "Vladimir Ryakhovsky",
            "Dmitry Kudryavtsev"
        ],
        "abstract": "This paper presents our work on development of OWL-driven systems for formal representation and reasoning about terminological knowledge and facts in petrology. The long-term aim of our project is to provide solid foundations for a large-scale integration of various kinds of knowledge, including basic terms, rock classification algorithms, findings and reports. We describe three steps we have taken towards that goal here. First, we develop a semi-automated procedure for transforming a database of igneous rock samples to texts in a controlled natural language (CNL), and then a collection of OWL ontologies. Second, we create an OWL ontology of important petrology terms currently described in natural language thesauri. We describe a prototype of a tool for collecting definitions from domain experts. Third, we present an approach to formalization of current industrial standards for classification of rock samples, which requires linear equations in OWL 2. In conclusion, we discuss a range of opportunities arising from the use of semantic technologies in petrology and outline the future work in this area.\n    ",
        "submission_date": "2011-06-08T00:00:00",
        "last_modified_date": "2011-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1636",
        "title": "A Sequence of Relaxations Constraining Hidden Variable Models",
        "authors": [
            "Greg Ver Steeg",
            "Aram Galstyan"
        ],
        "abstract": "Many widely studied graphical models with latent variables lead to nontrivial constraints on the distribution of the observed variables. Inspired by the Bell inequalities in quantum mechanics, we refer to any linear inequality whose violation rules out some latent variable model as a \"hidden variable test\" for that model. Our main contribution is to introduce a sequence of relaxations which provides progressively tighter hidden variable tests. We demonstrate applicability to mixtures of sequences of i.i.d. variables, Bell inequalities, and homophily models in social networks. For the last, we demonstrate that our method provides a test that is able to rule out latent homophily as the sole explanation for correlations on a real social network that are known to be due to influence.\n    ",
        "submission_date": "2011-06-08T00:00:00",
        "last_modified_date": "2011-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1716",
        "title": "Predicting growth fluctuation in network economy",
        "authors": [
            "Yoshiharu Maeno"
        ],
        "abstract": "This study presents a method to predict the growth fluctuation of firms interdependent in a network economy. The risk of downward growth fluctuation of firms is calculated from the statistics on Japanese industry.\n    ",
        "submission_date": "2011-06-09T00:00:00",
        "last_modified_date": "2011-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1796",
        "title": "Accelerating Reinforcement Learning by Composing Solutions of Automatically Identified Subtasks",
        "authors": [
            "C. Drummond"
        ],
        "abstract": "This paper discusses a system that accelerates reinforcement    learning by using transfer from related tasks.  Without such    transfer, even if two tasks are very similar at some abstract level,    an extensive re-learning effort is required.  The system achieves    much of its power by transferring parts of previously learned    solutions rather than a single complete solution. The system    exploits strong features in the multi-dimensional function produced    by reinforcement learning in solving a particular task. These    features are stable and easy to recognize early in the learning    process. They generate a partitioning of the state space and thus    the function.  The partition is represented as a graph.  This is    used to index and compose functions stored in a case base to form a    close approximation to the solution of the new task.  Experiments    demonstrate that function composition often produces more than an    order of magnitude increase in learning rate compared to a basic    reinforcement learning algorithm.\n    ",
        "submission_date": "2011-06-09T00:00:00",
        "last_modified_date": "2011-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1797",
        "title": "Parameter Learning of Logic Programs for Symbolic-Statistical Modeling",
        "authors": [
            "T. Sato",
            "Y. Kameya"
        ],
        "abstract": "We propose a logical/mathematical framework for statistical    parameter learning of parameterized logic programs, i.e.  definite    clause programs containing probabilistic facts with a parameterized    distribution.  It extends the traditional least Herbrand model    semantics in logic programming to distribution semantics, possible    world semantics with a probability distribution which is    unconditionally applicable to arbitrary logic programs including ones    for HMMs, PCFGs and Bayesian networks.        We also propose a new EM algorithm, the graphical EM algorithm, that    runs for a class of parameterized logic programs representing    sequential decision processes where each decision is exclusive and    independent.  It runs on a new data structure called support graphs    describing the logical relationship between observations and their    explanations, and learns parameters by computing inside and outside    probability generalized for logic programs.           The complexity analysis shows that when combined with OLDT search for    all explanations for observations, the graphical EM algorithm, despite    its generality, has the same time complexity as existing EM    algorithms, i.e. the Baum-Welch algorithm for HMMs, the Inside-Outside    algorithm for PCFGs, and the one for singly connected Bayesian    networks that have been developed independently in each research    field.  Learning experiments with PCFGs using two corpora of moderate    size indicate that the graphical EM algorithm can significantly    outperform the Inside-Outside algorithm.\n    ",
        "submission_date": "2011-06-09T00:00:00",
        "last_modified_date": "2011-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1799",
        "title": "Finding a Path is Harder than Finding a Tree",
        "authors": [
            "C. Meek"
        ],
        "abstract": "I consider the problem of learning an optimal path graphical    model from data and show the problem to be NP-hard for the maximum    likelihood and minimum description length approaches and a Bayesian    approach. This hardness result holds despite the fact that the problem    is a restriction of the polynomially solvable problem of finding the    optimal tree graphical model.\n    ",
        "submission_date": "2011-06-09T00:00:00",
        "last_modified_date": "2011-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1800",
        "title": "Extensions of Simple Conceptual Graphs: the Complexity of Rules and Constraints",
        "authors": [
            "J. F. Baget",
            "M. L. Mugnier"
        ],
        "abstract": "Simple conceptual graphs are considered as the kernel of    most knowledge representation formalisms built upon Sowa's    model. Reasoning in this model can be expressed by a graph    homomorphism called projection, whose semantics is usually given in    terms of positive, conjunctive, existential FOL.  We present here a    family of extensions of this model, based on rules and constraints,    keeping graph homomorphism as the basic operation. We focus on the    formal definitions of the different models obtained, including their    operational semantics and relationships with FOL, and we analyze the    decidability and complexity of the associated problems (consistency    and deduction). As soon as rules are involved in reasonings, these    problems are not decidable, but we exhibit a condition under which    they fall in the polynomial hierarchy.  These results extend and    complete the ones already published by the authors.  Moreover we    systematically study the complexity of some particular cases obtained    by restricting the form of constraints and/or rules.\n    ",
        "submission_date": "2011-06-09T00:00:00",
        "last_modified_date": "2011-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1802",
        "title": "Fusions of Description Logics and Abstract Description Systems",
        "authors": [
            "F. Baader",
            "C. Lutz",
            "H. Sturm",
            "F. Wolter"
        ],
        "abstract": "Fusions are a simple way of combining logics. For normal    modal logics, fusions have been investigated in detail. In particular,    it is known that, under certain conditions, decidability transfers    from the component logics to their fusion.  Though description logics    are closely related to modal logics, they are not necessarily    normal. In addition, ABox reasoning in description logics is not    covered by the results from modal logics.           In this paper, we extend the decidability transfer results from normal    modal logics to a large class of description logics. To cover    different description logics in a uniform way, we introduce abstract    description systems, which can be seen as a common generalization of    description and modal logics, and show the transfer results in this    general setting.\n    ",
        "submission_date": "2011-06-09T00:00:00",
        "last_modified_date": "2011-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1803",
        "title": "Improving the Efficiency of Inductive Logic Programming Through the Use of Query Packs",
        "authors": [
            "H. Blockeel",
            "L. Dehaspe",
            "B. Demoen",
            "G. Janssens",
            "J. Ramon",
            "H. Vandecasteele"
        ],
        "abstract": "Inductive logic programming, or relational learning, is a    powerful paradigm for machine learning or data mining.  However, in    order for ILP to become practically useful, the efficiency of ILP    systems must improve substantially. To this end, the notion of a query    pack is introduced: it structures sets of similar    queries. Furthermore, a mechanism is described for executing such    query packs.  A complexity analysis shows that considerable efficiency    improvements can be achieved through the use of this query pack    execution mechanism. This claim is supported by empirical results    obtained by incorporating support for query pack execution in two    existing learning systems.\n    ",
        "submission_date": "2011-06-09T00:00:00",
        "last_modified_date": "2011-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1804",
        "title": "A Critical Assessment of Benchmark Comparison in Planning",
        "authors": [
            "E. Dahlman",
            "A. E. Howe"
        ],
        "abstract": "Recent trends in planning research have led to empirical     comparison becoming commonplace. The field has started to settle into    a methodology for such comparisons, which for obvious practical    reasons requires running a subset of planners on a subset of    problems.  In this paper, we characterize the methodology and    examine eight implicit assumptions about the problems, planners and    metrics used in many of these comparisons. The problem assumptions    are: PR1) the performance of a general purpose planner should not be    penalized/biased if executed on a sampling of problems and domains,    PR2) minor syntactic differences in representation do not affect    performance, and PR3) problems should be solvable by STRIPS capable    planners unless they require ADL. The planner assumptions are: PL1)    the latest version of a planner is the best one to use, PL2) default    parameter settings approximate good performance, and PL3) time    cut-offs do not unduly bias outcome. The metrics assumptions are:    M1) performance degrades similarly for each planner when run on    degraded runtime environments (e.g., machine platform) and M2) the    number of plan steps distinguishes performance. We find that most of    these assumptions are not supported empirically; in particular, that    planners are affected differently by these assumptions. We conclude    with a call to the community to devote research resources to    improving the state of the practice and especially to enhancing the    available benchmark problems.\n    ",
        "submission_date": "2011-06-09T00:00:00",
        "last_modified_date": "2011-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1813",
        "title": "SMOTE: Synthetic Minority Over-sampling Technique",
        "authors": [
            "N. V. Chawla",
            "K. W. Bowyer",
            "L. O. Hall",
            "W. P. Kegelmeyer"
        ],
        "abstract": "An approach to the construction of classifiers from    imbalanced datasets is described. A dataset is imbalanced if the    classification categories are not approximately equally    represented. Often real-world data sets are predominately composed of    \"normal\" examples with only a small percentage of \"abnormal\" or    \"interesting\" examples. It is also the case that the cost of    misclassifying an abnormal (interesting) example as a normal example    is often much higher than the cost of the reverse    error. Under-sampling of the majority (normal) class has been proposed    as a good means of increasing the sensitivity of a classifier to the    minority class. This paper shows that a combination of our method of    over-sampling the minority (abnormal) class and under-sampling the    majority (normal) class can achieve better classifier performance (in    ROC space) than only under-sampling the majority class.  This paper    also shows that a combination of our method of over-sampling the    minority class and under-sampling the majority class can achieve    better classifier performance (in ROC space) than varying the loss    ratios in Ripper or class priors in Naive Bayes. Our method of    over-sampling the minority class involves creating synthetic minority    class examples.  Experiments are performed using C4.5, Ripper and a    Naive Bayes classifier. The method is evaluated using the area under    the Receiver Operating Characteristic curve (AUC) and the ROC convex    hull strategy.\n    ",
        "submission_date": "2011-06-09T00:00:00",
        "last_modified_date": "2011-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1814",
        "title": "When do Numbers Really Matter?",
        "authors": [
            "H. Chan",
            "A. Darwiche"
        ],
        "abstract": "Common wisdom has it that small distinctions in the    probabilities (parameters) quantifying a belief network do not matter    much for the results of probabilistic queries. Yet, one can develop    realistic scenarios under which small variations in network parameters    can lead to significant changes in computed queries. A pending    theoretical question is then to analytically characterize parameter    changes that do or do not matter. In this paper, we study the    sensitivity of probabilistic queries to changes in network parameters    and prove some tight bounds on the impact that such parameters can    have on queries. Our analytic results pinpoint some interesting    situations under which parameter changes do or do not matter. These    results are important for knowledge engineers as they help them    identify influential network parameters. They also help explain some    of the previous experimental results and observations with regards to    network robustness against parameter changes.\n    ",
        "submission_date": "2011-06-09T00:00:00",
        "last_modified_date": "2011-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1816",
        "title": "Monitoring Teams by Overhearing: A Multi-Agent Plan-Recognition Approach",
        "authors": [
            "G. A. Kaminka",
            "D. V. Pynadath",
            "M. Tambe"
        ],
        "abstract": "Recent years are seeing an increasing need for on-line    monitoring of teams of cooperating agents, e.g., for visualization, or    performance tracking. However, in monitoring deployed teams, we often    cannot rely on the agents to always communicate their state to the    monitoring system. This paper presents a non-intrusive approach to    monitoring by 'overhearing', where the monitored team's state is    inferred (via plan-recognition) from team-members' routine    communications, exchanged as part of their coordinated task execution,    and observed (overheard) by the monitoring system. Key challenges in    this approach include the demanding run-time requirements of    monitoring, the scarceness of observations (increasing monitoring    uncertainty), and the need to scale-up monitoring to address    potentially large teams. To address these, we present a set of    complementary novel techniques, exploiting knowledge of the social    structures and procedures in the monitored team: (i) an efficient    probabilistic plan-recognition algorithm, well-suited for processing    communications as observations; (ii) an approach to exploiting    knowledge of the team's social behavior to predict future observations    during execution (reducing monitoring uncertainty); and (iii)    monitoring algorithms that trade expressivity for scalability,    representing only certain useful monitoring hypotheses, but allowing    for any number of agents and their different activities to be    represented in a single coherent entity. We present an empirical    evaluation of these techniques, in combination and apart, in    monitoring a deployed team of agents, running on machines physically    distributed across the country, and engaged in complex, dynamic task    execution. We also compare the performance of these techniques to    human expert and novice monitors, and show that the techniques    presented are capable of monitoring at human-expert levels, despite    the difficulty of the task.\n    ",
        "submission_date": "2011-06-09T00:00:00",
        "last_modified_date": "2011-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1817",
        "title": "Automatically Training a Problematic Dialogue Predictor for a Spoken Dialogue System",
        "authors": [
            "A. Gorin",
            "I. Langkilde-Geary",
            "M. A. Walker",
            "J. Wright",
            "H. Wright Hastie"
        ],
        "abstract": "Spoken dialogue systems promise efficient and natural access    to a large variety of information sources and services from any phone.    However, current spoken dialogue systems are deficient in their    strategies for preventing, identifying and repairing problems that    arise in the conversation. This paper reports results on automatically    training a Problematic Dialogue Predictor to predict problematic    human-computer dialogues using a corpus of 4692 dialogues collected    with the 'How May I Help You' (SM) spoken dialogue system.  The    Problematic Dialogue Predictor can be immediately applied to the    system's decision of whether to transfer the call to a human customer    care agent, or be used as a cue to the system's dialogue manager to    modify its behavior to repair problems, and even perhaps, to prevent    them. We show that a Problematic Dialogue Predictor using    automatically-obtainable features from the first two exchanges in the    dialogue can predict problematic dialogues 13.2% more accurately than    the baseline.\n    ",
        "submission_date": "2011-06-09T00:00:00",
        "last_modified_date": "2011-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1818",
        "title": "Inducing Interpretable Voting Classifiers without Trading Accuracy for Simplicity: Theoretical Results, Approximation Algorithms",
        "authors": [
            "R. Nock"
        ],
        "abstract": "Recent advances in the study of voting classification    algorithms have brought empirical and theoretical results clearly    showing the discrimination power of ensemble classifiers. It has been    previously argued that the search of this classification power in the    design of the algorithms has marginalized the need to obtain    interpretable classifiers. Therefore, the question of whether one    might have to dispense with interpretability in order to keep    classification strength is being raised in a growing number of machine    learning or data mining papers. The purpose of this paper is to study    both theoretically and empirically the problem. First, we provide    numerous results giving insight into the hardness of the    simplicity-accuracy tradeoff for voting classifiers. Then we provide    an efficient \"top-down and prune\" induction heuristic, WIDC, mainly    derived from recent results on the weak learning and boosting    frameworks.  It is to our knowledge the first attempt to build a    voting classifier as a base formula using the weak learning framework    (the one which was previously highly successful for decision tree    induction), and not the strong learning framework (as usual for such    classifiers with boosting-like approaches). While it uses a well-known    induction scheme previously successful in other classes of concept    representations, thus making it easy to implement and compare, WIDC    also relies on recent or new results we give about particular cases of    boosting known as partition boosting and ranking loss    boosting. Experimental results on thirty-one domains, most of which    readily available, tend to display the ability of WIDC to produce    small, accurate, and interpretable decision committees.\n    ",
        "submission_date": "2011-06-09T00:00:00",
        "last_modified_date": "2011-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1819",
        "title": "A Knowledge Compilation Map",
        "authors": [
            "A. Darwiche",
            "P. Marquis"
        ],
        "abstract": "We propose a perspective on knowledge compilation which    calls for analyzing different compilation approaches according to two    key dimensions: the succinctness of the target compilation language,    and the class of queries and transformations that the language    supports in polytime. We then provide a knowledge compilation map,    which analyzes a large number of existing target compilation languages    according to their succinctness and their polytime transformations and    queries. We argue that such analysis is necessary for placing new    compilation approaches within the context of existing ones. We also go    beyond classical, flat target compilation languages based on CNF and    DNF, and consider a richer, nested class based on directed acyclic    graphs (such as OBDDs), which we show to include a relatively large    number of target compilation languages.\n    ",
        "submission_date": "2011-06-09T00:00:00",
        "last_modified_date": "2011-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1820",
        "title": "Inferring Strategies for Sentence Ordering in Multidocument News Summarization",
        "authors": [
            "R. Barzilay",
            "N. Elhadad"
        ],
        "abstract": "The problem of organizing information for multidocument    summarization so that the generated summary is coherent has received    relatively little attention.  While sentence ordering for single    document summarization can be determined from the ordering of    sentences in the input article, this is not the case for multidocument    summarization where summary sentences may be drawn from different    input articles. In this paper, we propose a methodology for studying    the properties of ordering information in the news genre and describe    experiments done on a corpus of multiple acceptable orderings we    developed for the task. Based on these experiments, we implemented a    strategy for ordering information that combines constraints from    chronological order of events and topical relatedness.  Evaluation of    our augmented algorithm shows a significant improvement of the    ordering over two baseline strategies.\n    ",
        "submission_date": "2011-06-09T00:00:00",
        "last_modified_date": "2011-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1821",
        "title": "Collective Intelligence, Data Routing and Braess' Paradox",
        "authors": [
            "K. Tumer",
            "D. H. Wolpert"
        ],
        "abstract": "We consider the problem of designing the the utility    functions of the utility-maximizing agents in a multi-agent system so    that they work synergistically to maximize a global utility. The    particular problem domain we explore is the control of network routing    by placing agents on all the routers in the network.  Conventional    approaches to this task have the agents all use the Ideal Shortest    Path routing Algorithm (ISPA).  We demonstrate that in many cases, due    to the side-effects of one agent's actions on another agent's    performance, having agents use ISPA's is suboptimal as far as global    aggregate cost is concerned, even when they are only used to route    infinitesimally small amounts of traffic.  The utility functions of    the individual agents are not \"aligned\" with the global utility,    intuitively speaking.  As a particular example of this we present an    instance of Braess' paradox in which adding new links to a network    whose agents all use the ISPA results in a decrease in overall    throughput. We also demonstrate that load-balancing, in which the    agents' decisions are collectively made to optimize the global cost    incurred by all traffic currently being routed, is suboptimal as far    as global cost averaged across time is concerned. This is also due to    'side-effects', in this case of current routing decision on future    traffic. The mathematics of Collective Intelligence (COIN) is    concerned precisely with the issue of avoiding such deleterious    side-effects in multi-agent systems, both over time and space. We    present key concepts from that mathematics and use them to derive an    algorithm whose ideal version should have better performance than that    of having all agents use the ISPA, even in the infinitesimal limit. We    present experiments verifying this, and also showing that a    machine-learning-based version of this COIN algorithm in which costs    are only imprecisely estimated via empirical means (a version    potentially applicable in the real world) also outperforms the ISPA,    despite having access to less information than does the ISPA. In    particular, this COIN algorithm almost always avoids Braess' paradox.\n    ",
        "submission_date": "2011-06-09T00:00:00",
        "last_modified_date": "2011-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1822",
        "title": "Efficient Solution Algorithms for Factored MDPs",
        "authors": [
            "C. Guestrin",
            "D. Koller",
            "R. Parr",
            "S. Venkataraman"
        ],
        "abstract": "This paper addresses the problem of planning under uncertainty in large Markov Decision Processes (MDPs). Factored MDPs represent a complex state space using state variables and the transition model using a dynamic Bayesian network. This representation often allows an exponential reduction in the representation size of structured MDPs, but the complexity of exact solution algorithms for such MDPs can grow exponentially in the representation size.  In this paper, we present two approximate solution algorithms that exploit structure in factored MDPs.  Both use an approximate value function represented as a linear combination of basis functions, where each basis function involves only a small subset of the domain variables.  A key contribution of this paper is that it shows how the basic operations of both algorithms can be performed efficiently in closed form, by exploiting both additive and context-specific structure in a factored MDP.  A central element of our algorithms is a novel linear program decomposition technique, analogous to variable elimination in Bayesian networks, which reduces an exponentially large LP to a provably equivalent, polynomial-sized one.  One algorithm uses approximate linear programming, and the second approximate dynamic programming. Our dynamic programming algorithm is novel in that it uses an approximation based on max-norm, a technique that more directly minimizes the terms that appear in error bounds for approximate MDP algorithms.  We provide experimental results on problems with over 10^40 states, demonstrating a promising indication of the scalability of our approach, and compare our algorithm to an existing state-of-the-art approach, showing, in some problems, exponential gains in computation time.\n    ",
        "submission_date": "2011-06-09T00:00:00",
        "last_modified_date": "2011-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1853",
        "title": "Intelligent decision: towards interpreting the Pe Algorithm",
        "authors": [
            "Ching-an Hsiao",
            "Xinchun Tian"
        ],
        "abstract": "The human intelligence lies in the algorithm, the nature of algorithm lies in the classification, and the classification is equal to outlier detection. A lot of algorithms have been proposed to detect outliers, meanwhile a lot of definitions. Unsatisfying point is that definitions seem vague, which makes the solution an ad hoc one. We analyzed the nature of outliers, and give two clear definitions. We then develop an efficient RDD algorithm, which converts outlier problem to pattern and degree problem. Furthermore, a collapse mechanism was introduced by IIR algorithm, which can be united seamlessly with the RDD algorithm and serve for the final decision. Both algorithms are originated from the study on general AI. The combined edition is named as Pe algorithm, which is the basis of the intelligent decision. Here we introduce longest k-turn subsequence problem and corresponding solution as an example to interpret the function of Pe algorithm in detecting curve-type outliers. We also give a comparison between IIR algorithm and Pe algorithm, where we can get a better understanding at both algorithms. A short discussion about intelligence is added to demonstrate the function of the Pe algorithm. Related experimental results indicate its robustness.\n    ",
        "submission_date": "2011-06-09T00:00:00",
        "last_modified_date": "2011-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1957",
        "title": "Interdefinability of defeasible logic and logic programming under the well-founded semantics",
        "authors": [
            "Frederick Maier"
        ],
        "abstract": "We provide a method of translating theories of Nute's defeasible logic into logic programs, and a corresponding translation in the opposite direction. Under certain natural restrictions, the conclusions of defeasible theories under the ambiguity propagating defeasible logic ADL correspond to those of the well-founded semantics for normal logic programs, and so it turns out that the two formalisms are closely related. Using the same translation of logic programs into defeasible theories, the semantics for the ambiguity blocking defeasible logic NDL can be seen as indirectly providing an ambiguity blocking semantics for logic programs. We also provide antimonotone operators for both ADL and NDL, each based on the Gelfond-Lifschitz (GL) operator for logic programs. For defeasible theories without defeaters or priorities on rules, the operator for ADL corresponds to the GL operator and so can be seen as partially capturing the consequences according to ADL. Similarly, the operator for NDL captures the consequences according to NDL, though in this case no restrictions on theories apply. Both operators can be used to define stable model semantics for defeasible theories.\n    ",
        "submission_date": "2011-06-10T00:00:00",
        "last_modified_date": "2011-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1998",
        "title": "A Linear Time Natural Evolution Strategy for Non-Separable Functions",
        "authors": [
            "Yi Sun",
            "Faustino Gomez",
            "Tom Schaul",
            "Juergen Schmidhuber"
        ],
        "abstract": "We present a novel Natural Evolution Strategy (NES) variant, the Rank-One NES (R1-NES), which uses a low rank approximation of the search distribution covariance matrix. The algorithm allows computation of the natural gradient with cost linear in the dimensionality of the parameter space, and excels in solving high-dimensional non-separable problems, including the best result to date on the Rosenbrock function (512 dimensions).\n    ",
        "submission_date": "2011-06-10T00:00:00",
        "last_modified_date": "2011-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.2647",
        "title": "From Causal Models To Counterfactual Structures",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "abstract": "Galles and Pearl claimed that \"for recursive models, the causal model framework does not add any restrictions to counterfactuals, beyond those imposed by Lewis's [possible-worlds] framework.\" This claim is examined carefully, with the goal of clarifying the exact relationship between causal models and Lewis's framework. Recursive models are shown to correspond precisely to a subclass of (possible-world) counterfactual structures. On the other hand, a slight generalization of recursive models, models where all equations have unique solutions, is shown to be incomparable in expressive power to counterfactual structures, despite the fact that the Galles and Pearl arguments should apply to them as well. The problem with the Galles and Pearl argument is identified: an axiom that they viewed as irrelevant, because it involved disjunction (which was not in their language), is not irrelevant at all.\n    ",
        "submission_date": "2011-06-14T00:00:00",
        "last_modified_date": "2013-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.2652",
        "title": "Actual causation and the art of modeling",
        "authors": [
            "Joseph Y. Halpern",
            "Christopher Hitchcock"
        ],
        "abstract": "We look more carefully at the modeling of causality using structural equations. It is clear that the structural equations can have a major impact on the conclusions we draw about causality. In particular, the choice of variables and their values can also have a significant impact on causality. These choices are, to some extent, subjective. We consider what counts as an appropriate choice. More generally, we consider what makes a model an appropriate model, especially if we want to take defaults into account, as was argued is necessary in recent work.\n    ",
        "submission_date": "2011-06-14T00:00:00",
        "last_modified_date": "2011-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.2692",
        "title": "Generating Schemata of Resolution Proofs",
        "authors": [
            "Vincent Aravantinos",
            "Nicolas Peltier"
        ],
        "abstract": "Two distinct algorithms are presented to extract (schemata of) resolution proofs from closed tableaux for propositional schemata. The first one handles the most efficient version of the tableau calculus but generates very complex derivations (denoted by rather elaborate rewrite systems). The second one has the advantage that much simpler systems can be obtained, however the considered proof procedure is less efficient.\n    ",
        "submission_date": "2011-06-14T00:00:00",
        "last_modified_date": "2011-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.3361",
        "title": "Random forest models of the retention constants in the thin layer chromatography",
        "authors": [
            "Miron B. Kursa",
            "\u0141ukasz Komsta",
            "Witold R. Rudnicki"
        ],
        "abstract": "In the current study we examine an application of the machine learning methods to model the retention constants in the thin layer chromatography (TLC). This problem can be described with hundreds or even thousands of descriptors relevant to various molecular properties, most of them redundant and not relevant for the retention constant prediction. Hence we employed feature selection to significantly reduce the number of attributes. Additionally we have tested application of the bagging procedure to the feature selection. The random forest regression models were built using selected variables. The resulting models have better correlation with the experimental data than the reference models obtained with linear regression. The cross-validation confirms robustness of the models.\n    ",
        "submission_date": "2011-06-16T00:00:00",
        "last_modified_date": "2011-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.3498",
        "title": "On the expressive power of unit resolution",
        "authors": [
            "Olivier Bailleux"
        ],
        "abstract": "This preliminary report addresses the expressive power of unit resolution regarding input data encoded with partial truth assignments of propositional variables. A characterization of the functions that are computable in this way, which we propose to call propagatable functions, is given. By establishing that propagatable functions can also be computed using monotone circuits, we show that there exist polynomial time complexity propagable functions requiring an exponential amount of clauses to be computed using unit resolution. These results shed new light on studying CNF encodings of NP-complete problems in order to solve them using propositional satisfiability algorithms.\n    ",
        "submission_date": "2011-06-17T00:00:00",
        "last_modified_date": "2011-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.3685",
        "title": "Embedding and Automating Conditional Logics in Classical Higher-Order Logic",
        "authors": [
            "Christoph Benzmueller",
            "Dov Gabbay",
            "Valerio Genovese",
            "Daniele Rispoli"
        ],
        "abstract": "A sound and complete embedding of conditional logics into classical higher-order logic is presented. This embedding enables the application of off-the-shelf higher-order automated theorem provers and model finders for reasoning within and about conditional logics.\n    ",
        "submission_date": "2011-06-18T00:00:00",
        "last_modified_date": "2011-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.3767",
        "title": "Rewriting Ontological Queries into Small Nonrecursive Datalog Programs",
        "authors": [
            "Georg Gottlob",
            "Thomas Schwentick"
        ],
        "abstract": "We consider the setting of ontological database access, where an Abox is given in form of a relational database D and where a Boolean conjunctive query q has to be evaluated against D modulo a Tbox T formulated in DL-Lite or Linear Datalog+/-. It is well-known that (T,q) can be rewritten into an equivalent nonrecursive Datalog program P that can be directly evaluated over D. However, for Linear Datalog? or for DL-Lite versions that allow for role inclusion, the rewriting methods described so far result in a nonrecursive Datalog program P of size exponential in the joint size of T and q. This gives rise to the interesting question of whether such a rewriting necessarily needs to be of exponential size. In this paper we show that it is actually possible to translate (T,q) into a polynomially sized equivalent nonrecursive Datalog program P.\n    ",
        "submission_date": "2011-06-19T00:00:00",
        "last_modified_date": "2011-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.3876",
        "title": "Uncertainty in Ontologies: Dempster-Shafer Theory for Data Fusion Applications",
        "authors": [
            "Amandine Bellenger",
            "Sylvain Gatepaille"
        ],
        "abstract": "Nowadays ontologies present a growing interest in Data Fusion applications. As a matter of fact, the ontologies are seen as a semantic tool for describing and reasoning about sensor data, objects, relations and general domain theories. In addition, uncertainty is perhaps one of the most important characteristics of the data and information handled by Data Fusion. However, the fundamental nature of ontologies implies that ontologies describe only asserted and veracious facts of the world. Different probabilistic, fuzzy and evidential approaches already exist to fill this gap; this paper recaps the most popular tools. However none of the tools meets exactly our purposes. Therefore, we constructed a Dempster-Shafer ontology that can be imported into any specific domain ontology and that enables us to instantiate it in an uncertain manner. We also developed a Java application that enables reasoning about these uncertain ontological instances.\n    ",
        "submission_date": "2011-06-20T00:00:00",
        "last_modified_date": "2011-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.3932",
        "title": "Coincidences and the encounter problem: A formal account",
        "authors": [
            "Jean-Louis J.-L. Dessalles"
        ],
        "abstract": "Individuals have an intuitive perception of what makes a good coincidence. Though the sensitivity to coincidences has often been presented as resulting from an erroneous assessment of probability, it appears to be a genuine competence, based on non-trivial computations. The model presented here suggests that coincidences occur when subjects perceive complexity drops. Co-occurring events are, together, simpler than if considered separately. This model leads to a possible redefinition of subjective probability.\n    ",
        "submission_date": "2011-06-20T00:00:00",
        "last_modified_date": "2011-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.3967",
        "title": "Intelligent Self-Repairable Web Wrappers",
        "authors": [
            "Emilio Ferrara",
            "Robert Baumgartner"
        ],
        "abstract": "The amount of information available on the Web grows at an incredible high rate. Systems and procedures devised to extract these data from Web sources already exist, and different approaches and techniques have been investigated during the last years. On the one hand, reliable solutions should provide robust algorithms of Web data mining which could automatically face possible malfunctioning or failures. On the other, in literature there is a lack of solutions about the maintenance of these systems. Procedures that extract Web data may be strictly interconnected with the structure of the data source itself; thus, malfunctioning or acquisition of corrupted data could be caused, for example, by structural modifications of data sources brought by their owners. Nowadays, verification of data integrity and maintenance are mostly manually managed, in order to ensure that these systems work correctly and reliably. In this paper we propose a novel approach to create procedures able to extract data from Web sources -- the so called Web wrappers -- which can face possible malfunctioning caused by modifications of the structure of the data source, and can automatically repair themselves.\n    ",
        "submission_date": "2011-06-20T00:00:00",
        "last_modified_date": "2011-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4083",
        "title": "Symmetry-Based Search Space Reduction For Grid Maps",
        "authors": [
            "Daniel Harabor",
            "Adi Botea",
            "Philip Kilby"
        ],
        "abstract": "In this paper we explore a symmetry-based search space reduction technique which can speed up optimal pathfinding on undirected uniform-cost grid maps by up to 38 times. Our technique decomposes grid maps into a set of empty rectangles, removing from each rectangle all interior nodes and possibly some from along the perimeter. We then add a series of macro-edges between selected pairs of remaining perimeter nodes to facilitate provably optimal traversal through each rectangle. We also develop a novel online pruning technique to further speed up search. Our algorithm is fast, memory efficient and retains the same optimality and completeness guarantees as searching on an unmodified grid map.\n    ",
        "submission_date": "2011-06-21T00:00:00",
        "last_modified_date": "2011-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4218",
        "title": "Rooting opinions in the minds: a cognitive model and a formal account of opinions and their dynamics",
        "authors": [
            "Francesca Giardini",
            "Walter Quattrociocchi",
            "Rosaria Conte"
        ],
        "abstract": "The study of opinions, their formation and change, is one of the defining topics addressed by social psychology, but in recent years other disciplines, like computer science and complexity, have tried to deal with this issue. Despite the flourishing of different models and theories in both fields, several key questions still remain unanswered. The understanding of how opinions change and the way they are affected by social influence are challenging issues requiring a thorough analysis of opinion per se but also of the way in which they travel between agents' minds and are modulated by these exchanges. To account for the two-faceted nature of opinions, which are mental entities undergoing complex social processes, we outline a preliminary model in which a cognitive theory of opinions is put forward and it is paired with a formal description of them and of their spreading among minds. Furthermore, investigating social influence also implies the necessity to account for the way in which people change their minds, as a consequence of interacting with other people, and the need to explain the higher or lower persistence of such changes.\n    ",
        "submission_date": "2011-06-21T00:00:00",
        "last_modified_date": "2011-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4221",
        "title": "Understanding opinions. A cognitive and formal account",
        "authors": [
            "Francesca Giardini",
            "Walter Quattrociocchi",
            "Rosaria Conte"
        ],
        "abstract": "The study of opinions, their formation and change, is one of the defining topics addressed by social psychology, but in recent years other disciplines, as computer science and complexity, have addressed this challenge. Despite the flourishing of different models and theories in both fields, several key questions still remain unanswered. The aim of this paper is to challenge the current theories on opinion by putting forward a cognitively grounded model where opinions are described as specific mental representations whose main properties are put forward. A comparison with reputation will be also presented.\n    ",
        "submission_date": "2011-06-21T00:00:00",
        "last_modified_date": "2011-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4509",
        "title": "Machine Learning Markets",
        "authors": [
            "Amos Storkey"
        ],
        "abstract": "Prediction markets show considerable promise for developing flexible mechanisms for machine learning. Here, machine learning markets for multivariate systems are defined, and a utility-based framework is established for their analysis. This differs from the usual approach of defining static betting functions. It is shown that such markets can implement model combination methods used in machine learning, such as product of expert and mixture of expert approaches as equilibrium pricing models, by varying agent utility functions. They can also implement models composed of local potentials, and message passing methods. Prediction markets also allow for more flexible combinations, by combining multiple different utility functions. Conversely, the market mechanisms implement inference in the relevant probabilistic models. This means that market mechanism can be utilized for implementing parallelized model building and inference for probabilistic modelling.\n    ",
        "submission_date": "2011-06-22T00:00:00",
        "last_modified_date": "2011-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4557",
        "title": "Learning When Training Data are Costly: The Effect of Class Distribution on Tree Induction",
        "authors": [
            "F. Provost",
            "G. M. Weiss"
        ],
        "abstract": "For large, real-world inductive learning problems, the number of training examples often must be limited due to the costs associated with procuring, preparing, and storing the training examples and/or the computational costs associated with learning from them. In such circumstances, one question of practical importance is: if only n training examples can be selected, in what proportion should the classes be represented?  In this article we help to answer this question by analyzing, for a fixed training-set size, the relationship between the class distribution of the training data and the performance of classification trees induced from these data. We study twenty-six data sets and, for each, determine the best class distribution for learning.  The naturally occurring class distribution is shown to generally perform well when classifier performance is evaluated using undifferentiated error rate (0/1 loss).  However, when the area under the ROC curve is used to evaluate classifier performance, a balanced distribution is shown to perform well.  Since neither of these choices for class distribution always generates the best-performing classifier, we introduce a budget-sensitive progressive sampling algorithm for selecting training examples based on the class associated with each example.  An empirical analysis of this algorithm shows that the class distribution of the resulting training set yields classifiers with good (nearly-optimal) classification performance.\n    ",
        "submission_date": "2011-06-22T00:00:00",
        "last_modified_date": "2011-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4561",
        "title": "PDDL2.1: An Extension to PDDL for Expressing Temporal Planning Domains",
        "authors": [
            "M. Fox",
            "D. Long"
        ],
        "abstract": "In recent years research in the planning community has moved increasingly toward s application of planners to realistic problems involving both time and many typ es of resources. For example, interest in planning demonstrated by the space res earch community has inspired work in observation scheduling, planetary rover ex ploration and spacecraft control domains. Other temporal and resource-intensive domains including logistics planning, plant control and manufacturing have also helped to focus the community on the modelling and reasoning issues that must be confronted to make planning technology meet the challenges of application.   The International Planning Competitions have acted as an important motivating fo rce behind the progress that has been made in planning since 1998. The third com petition (held in 2002) set the planning community the challenge of handling tim e and numeric resources. This necessitated the development of a modelling langua ge capable of expressing temporal and numeric properties of planning domains. In this paper we describe the language, PDDL2.1, that was used in the competition.  We describe the syntax of the language, its formal semantics and the validation of concurrent plans. We observe that PDDL2.1 has considerable modelling power --- exceeding the capabilities of current planning technology --- and presents a number of important challenges to the research community.\n    ",
        "submission_date": "2011-06-22T00:00:00",
        "last_modified_date": "2011-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4569",
        "title": "The Communicative Multiagent Team Decision Problem: Analyzing Teamwork Theories and Models",
        "authors": [
            "D. V. Pynadath",
            "M. Tambe"
        ],
        "abstract": "Despite the significant progress in multiagent teamwork,    existing research does not address the optimality of its prescriptions    nor the complexity of the teamwork problem.  Without a    characterization of the optimality-complexity tradeoffs, it is    impossible to determine whether the assumptions and approximations    made by a particular theory gain enough efficiency to justify the    losses in overall performance.  To provide a tool for use by    multiagent researchers in evaluating this tradeoff, we present a    unified framework, the COMmunicative Multiagent Team Decision Problem    (COM-MTDP).  The COM-MTDP model combines and extends existing    multiagent theories, such as decentralized partially observable Markov    decision processes and economic team theory.  In addition to their    generality of representation, COM-MTDPs also support the analysis of    both the optimality of team performance and the computational    complexity of the agents' decision problem.  In analyzing complexity,    we present a breakdown of the computational complexity of constructing    optimal teams under various classes of problem domains, along the    dimensions of observability and communication cost.  In analyzing    optimality, we exploit the COM-MTDP's ability to encode existing    teamwork theories and models to encode two instantiations of joint    intentions theory taken from the literature.  Furthermore, the    COM-MTDP model provides a basis for the development of novel team    coordination algorithms.  We derive a domain-independent criterion for    optimal communication and provide a comparative analysis of the two    joint intentions instantiations with respect to this optimal policy.    We have implemented a reusable, domain-independent software package    based on COM-MTDPs to analyze teamwork coordination strategies, and we    demonstrate its use by encoding and evaluating the two joint    intentions strategies within an example domain.\n    ",
        "submission_date": "2011-06-22T00:00:00",
        "last_modified_date": "2011-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4572",
        "title": "Specific-to-General Learning for Temporal Events with Application to Learning Event Definitions from Video",
        "authors": [
            "A. Fern",
            "R. Givan",
            "J. M. Siskind"
        ],
        "abstract": "We develop, analyze, and evaluate a novel, supervised,    specific-to-general learner for a simple temporal logic and use the    resulting algorithm to learn visual event definitions from video    sequences.  First, we introduce a simple, propositional, temporal,    event-description language called AMA that is sufficiently expressive    to represent many events yet sufficiently restrictive to support    learning.  We then give algorithms, along with lower and upper    complexity bounds, for the subsumption and generalization problems for    AMA formulas. We present a positive-examples--only specific-to-general    learning method based on these algorithms. We also present a    polynomial-time--computable ``syntactic'' subsumption test that    implies semantic subsumption without being equivalent to it. A    generalization algorithm based on syntactic subsumption can be used in    place of semantic generalization to improve the asymptotic complexity    of the resulting learning algorithm. Finally, we apply this algorithm    to the task of learning relational event definitions from video and    show that it yields definitions that are competitive with hand-coded    ones.\n    ",
        "submission_date": "2011-06-22T00:00:00",
        "last_modified_date": "2011-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4573",
        "title": "Towards Adjustable Autonomy for the Real World",
        "authors": [
            "D. V. Pynadath",
            "P. Scerri",
            "M. Tambe"
        ],
        "abstract": "Adjustable autonomy refers to entities dynamically varying    their own autonomy, transferring decision-making control to other    entities (typically agents transferring control to human users) in key    situations.  Determining whether and when such transfers-of-control    should occur is arguably the fundamental research problem in    adjustable autonomy. Previous work has investigated various approaches    to addressing this problem but has often focused on individual    agent-human interactions.  Unfortunately, domains requiring    collaboration between teams of agents and humans reveal two key    shortcomings of these previous approaches. First, these approaches use    rigid one-shot transfers of control that can result in unacceptable    coordination failures in multiagent settings.  Second, they ignore    costs (e.g., in terms of time delays or effects on actions) to an    agent's team due to such transfers-of-control.     To remedy these problems, this article presents a novel approach to    adjustable autonomy, based on the notion of a transfer-of-control    strategy.  A transfer-of-control strategy consists of a conditional    sequence of two types of actions: (i) actions to transfer    decision-making control (e.g., from an agent to a user or vice versa)    and (ii) actions to change an agent's pre-specified coordination    constraints with team members, aimed at minimizing miscoordination    costs. The goal is for high-quality individual decisions to be made    with minimal disruption to the coordination of the team.  We present a    mathematical model of transfer-of-control strategies. The model guides    and informs the operationalization of the strategies using Markov    Decision Processes, which select an optimal strategy, given an    uncertain environment and costs to the individuals and teams. The    approach has been carefully evaluated, including via its use in a    real-world, deployed multi-agent system that assists a research group    in its daily activities.\n    ",
        "submission_date": "2011-06-22T00:00:00",
        "last_modified_date": "2011-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4575",
        "title": "An Analysis of Phase Transition in NK Landscapes",
        "authors": [
            "J. Culberson",
            "Y. Gao"
        ],
        "abstract": "In this paper, we analyze the decision version of the NK    landscape model from the perspective of threshold phenomena and phase    transitions under two random distributions, the uniform probability    model and the fixed ratio model. For the uniform probability model, we    prove that the phase transition is easy in the sense that there is a    polynomial algorithm that can solve a random instance of the problem    with the probability asymptotic to 1 as the problem size tends to    infinity. For the fixed ratio model, we establish several upper bounds    for the solubility threshold, and prove that random instances with    parameters above these upper bounds can be solved polynomially. This,    together with our empirical study for random instances generated below    and in the phase transition region, suggests that the phase transition    of the fixed ratio model is also easy.\n    ",
        "submission_date": "2011-06-22T00:00:00",
        "last_modified_date": "2011-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4576",
        "title": "Expert-Guided Subgroup Discovery: Methodology and Application",
        "authors": [
            "D. Gamberger",
            "N. Lavrac"
        ],
        "abstract": "This paper presents an approach to expert-guided subgroup    discovery.  The main step of the subgroup discovery process, the    induction of subgroup descriptions, is performed by a heuristic beam    search algorithm, using a novel parametrized definition of rule    quality which is analyzed in detail.  The other important steps of the    proposed subgroup discovery process are the detection of statistically    significant properties of selected subgroups and subgroup    visualization: statistically significant properties are used to enrich    the descriptions of induced subgroups, while the visualization shows    subgroup properties in the form of distributions of the numbers of    examples in the subgroups. The approach is illustrated by the results    obtained for a medical problem of early detection of patient risk    groups.\n    ",
        "submission_date": "2011-06-22T00:00:00",
        "last_modified_date": "2011-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4578",
        "title": "Propositional Independence - Formula-Variable Independence and Forgetting",
        "authors": [
            "J. Lang",
            "P. Liberatore",
            "P. Marquis"
        ],
        "abstract": "Independence -- the study of what is relevant to a given problem of reasoning -- has received an increasing attention from the AI community. In this paper, we consider two basic forms of independence, namely, a syntactic one and a semantic one. We show features and drawbacks of them. In particular, while the syntactic form of independence is computationally easy to check, there are cases in which things that intuitively are not relevant are not recognized as such. We also consider the problem of forgetting, i.e., distilling from a knowledge base only the part that is relevant to the set of queries constructed from a subset of the alphabet. While such process is computationally hard, it allows for a simplification of subsequent reasoning, and can thus be viewed as a form of compilation: once the relevant part of a knowledge base has been extracted, all reasoning tasks to be performed can be simplified.\n    ",
        "submission_date": "2011-06-22T00:00:00",
        "last_modified_date": "2011-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4863",
        "title": "Monte Carlo Methods for Tempo Tracking and Rhythm Quantization",
        "authors": [
            "A. T. Cemgil",
            "B. Kappen"
        ],
        "abstract": "We present a probabilistic generative model for timing deviations in expressive music performance. The structure of the proposed model is equivalent to a switching state space model. The switch variables correspond to discrete note locations as in a musical score. The continuous hidden variables denote the tempo. We formulate two well known music recognition problems, namely tempo tracking and automatic transcription (rhythm quantization) as filtering and maximum a posteriori (MAP) state estimation tasks. Exact computation of posterior features such as the MAP state is intractable in this model class, so we introduce Monte Carlo methods for integration and optimization. We compare Markov Chain Monte Carlo (MCMC) methods (such as Gibbs sampling, simulated annealing and iterative improvement) and sequential Monte Carlo methods (particle filters). Our simulation results suggest better results with sequential methods. The methods can be applied in both online and batch scenarios such as tempo tracking and transcription and are thus potentially useful in a number of music applications such as adaptive automatic accompaniment, score typesetting and music information retrieval.\n    ",
        "submission_date": "2011-06-24T00:00:00",
        "last_modified_date": "2011-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4864",
        "title": "Exploiting Contextual Independence In Probabilistic Inference",
        "authors": [
            "D. Poole",
            "N. L. Zhang"
        ],
        "abstract": "Bayesian belief networks have grown to prominence because they provide compact representations for many problems for which probabilistic inference is appropriate, and there are algorithms to exploit this compactness. The next step is to allow compact representations of the conditional probabilities of a variable given its parents. In this paper we present such a representation that exploits contextual independence in terms of parent contexts; which variables act as parents may depend on the value of other variables. The internal representation is in terms of contextual factors (confactors) that is simply a pair of a context and a table. The algorithm, contextual variable elimination, is based on the standard variable elimination algorithm that eliminates the non-query variables in turn, but when eliminating a variable, the tables that need to be multiplied can depend on the context. This algorithm reduces to standard variable elimination when there is no contextual independence structure to exploit. We show how this can be much more efficient than variable elimination when there is structure to exploit. We explain why this new method can exploit more structure than previous methods for structured belief network inference and an analogous algorithm that uses trees.\n    ",
        "submission_date": "2011-06-24T00:00:00",
        "last_modified_date": "2011-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4865",
        "title": "Bound Propagation",
        "authors": [
            "B. Kappen",
            "M. Leisink"
        ],
        "abstract": "In this article we present an algorithm to compute bounds on the marginals of a graphical model.  For several small clusters of nodes upper and lower bounds on the marginal values are computed independently of the rest of the network.  The range of allowed probability distributions over the surrounding nodes is restricted using earlier computed bounds. As we will show, this can be considered as a set of constraints in a linear programming problem of which the objective function is the marginal probability of the center nodes.  In this way knowledge about the maginals of neighbouring clusters is passed to other clusters thereby tightening the bounds on their marginals.  We show that sharp bounds can be obtained for undirected and directed graphs that are used for practical applications, but for which exact computations are infeasible.\n    ",
        "submission_date": "2011-06-24T00:00:00",
        "last_modified_date": "2011-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4866",
        "title": "On Polynomial Sized MDP Succinct Policies",
        "authors": [
            "P. Liberatore"
        ],
        "abstract": "Policies of Markov Decision Processes (MDPs) determine the next action to execute from the current state and, possibly, the history (the past states). When the number of states is large, succinct representations are often used to compactly represent both the MDPs and the policies in a reduced amount of space. In this paper, some problems related to the size of succinctly represented policies are analyzed. Namely, it is shown that some MDPs have policies that can only be represented in space super-polynomial in the size of the MDP, unless the polynomial hierarchy collapses. This fact motivates the study of the problem of deciding whether a given MDP has a policy of a given size and reward. Since some algorithms for MDPs work by finding a succinct representation of the value function, the problem of deciding the existence of a succinct representation of a value function of a given size and reward is also considered.\n    ",
        "submission_date": "2011-06-24T00:00:00",
        "last_modified_date": "2011-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4867",
        "title": "Compiling Causal Theories to Successor State Axioms and STRIPS-Like Systems",
        "authors": [
            "F. Lin"
        ],
        "abstract": "We describe a system for specifying the effects of actions. Unlike those commonly used in AI planning, our system uses an action description language that allows one to specify the effects of actions using domain rules, which are state constraints that can entail new action effects from old ones. Declaratively, an action domain in our language corresponds to a nonmonotonic causal theory in the situation calculus. Procedurally, such an action domain is compiled into a set of logical theories, one for each action in the domain, from which fully instantiated successor state-like axioms and STRIPS-like systems are then generated. We expect the system to be a useful tool for knowledge engineers writing action specifications for classical AI planning systems, GOLOG systems, and other systems where formal specifications of actions are needed.\n    ",
        "submission_date": "2011-06-24T00:00:00",
        "last_modified_date": "2011-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4868",
        "title": "VHPOP: Versatile Heuristic Partial Order Planner",
        "authors": [
            "R. G. Simmons",
            "H. L.S. Younes"
        ],
        "abstract": "VHPOP is a partial order causal link (POCL) planner loosely based on UCPOP. It draws from the experience gained in the early to mid 1990's on flaw selection strategies for POCL planning, and combines this with more recent developments in the field of domain independent planning such as distance based heuristics and reachability analysis. We present an adaptation of the additive heuristic for plan space planning, and modify it to account for possible reuse of existing actions in a plan. We also propose a large set of novel flaw selection strategies, and show how these can help us solve more problems than previously possible by POCL planners. VHPOP also supports planning with durative actions by incorporating standard techniques for temporal constraint reasoning. We demonstrate that the same heuristic techniques used to boost the performance of classical POCL planning can be effective in domains with durative actions as well. The result is a versatile heuristic POCL planner competitive with established CSP-based and heuristic state space planners.\n    ",
        "submission_date": "2011-06-24T00:00:00",
        "last_modified_date": "2011-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4869",
        "title": "SHOP2: An HTN Planning System",
        "authors": [
            "T. C. Au",
            "O. Ilghami",
            "U. Kuter",
            "J. W. Murdock",
            "D. S. Nau",
            "D. Wu",
            "F. Yaman"
        ],
        "abstract": "The SHOP2 planning system received one of the awards for distinguished performance in the 2002 International Planning Competition. This paper describes the features of SHOP2 which enabled it to excel in the competition, especially those aspects of SHOP2 that deal with temporal and metric planning domains.\n    ",
        "submission_date": "2011-06-24T00:00:00",
        "last_modified_date": "2011-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4871",
        "title": "An Architectural Approach to Ensuring Consistency in Hierarchical Execution",
        "authors": [
            "J. E. Laird",
            "R. E. Wray"
        ],
        "abstract": "Hierarchical task decomposition is a method used in many agent systems to organize agent knowledge. This work shows how the combination of a hierarchy and persistent assertions of knowledge can lead to difficulty in maintaining logical consistency in asserted knowledge. We explore the problematic consequences of persistent assumptions in the reasoning process and introduce novel potential solutions. Having implemented one of the possible solutions, Dynamic Hierarchical Justification, its effectiveness is demonstrated with an empirical analysis.\n    ",
        "submission_date": "2011-06-24T00:00:00",
        "last_modified_date": "2011-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4872",
        "title": "Wrapper Maintenance: A Machine Learning Approach",
        "authors": [
            "C. A. Knoblock",
            "K. Lerman",
            "S. N. Minton"
        ],
        "abstract": "The proliferation of online information sources has led to an increased use of wrappers for extracting data from Web sources. While most of the previous research has focused on quick and efficient generation of wrappers, the development of tools for wrapper maintenance has received less attention. This is an important research problem because Web sources often change in ways that prevent the wrappers from extracting data correctly. We present an efficient algorithm that learns structural information about data from positive examples alone. We describe how this information can be used for two wrapper maintenance applications: wrapper verification and reinduction. The wrapper verification system detects when a wrapper is not extracting correct data, usually because the Web source has changed its format. The reinduction algorithm automatically recovers from changes in the Web source by identifying data on Web pages so that a new wrapper may be generated for this source. To validate our approach, we monitored 27 wrappers over a period of a year. The verification algorithm correctly discovered 35 of the 37 wrapper changes, and made 16 mistakes, resulting in precision of 0.73 and recall of 0.95. We validated the reinduction algorithm on ten Web sources. We were able to successfully reinduce the wrappers, obtaining precision and recall values of 0.90 and 0.80 on the data extraction task.\n    ",
        "submission_date": "2011-06-24T00:00:00",
        "last_modified_date": "2011-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5111",
        "title": "Exploiting Reputation in Distributed Virtual Environments",
        "authors": [
            "Walter Quattrociocchi",
            "Rosaria Conte"
        ],
        "abstract": "The cognitive research on reputation has shown several interesting properties that can improve both the quality of services and the security in distributed electronic environments. In this paper, the impact of reputation on decision-making under scarcity of information will be shown. First, a cognitive theory of reputation will be presented, then a selection of simulation experimental results from different studies will be discussed. Such results concern the benefits of reputation when agents need to find out good sellers in a virtual market-place under uncertainty and informational cheating.\n    ",
        "submission_date": "2011-06-25T00:00:00",
        "last_modified_date": "2011-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5112",
        "title": "The All Relevant Feature Selection using Random Forest",
        "authors": [
            "Miron B. Kursa",
            "Witold R. Rudnicki"
        ],
        "abstract": "In this paper we examine the application of the random forest classifier for the all relevant feature selection problem. To this end we first examine two recently proposed all relevant feature selection algorithms, both being a random forest wrappers, on a series of synthetic data sets with varying size. We show that reasonable accuracy of predictions can be achieved and that heuristic algorithms that were designed to handle the all relevant problem, have performance that is close to that of the reference ideal algorithm. Then, we apply one of the algorithms to four families of semi-synthetic data sets to assess how the properties of particular data set influence results of feature selection. Finally we test the procedure using a well-known gene expression data set. The relevance of nearly all previously established important genes was confirmed, moreover the relevance of several new ones is discovered.\n    ",
        "submission_date": "2011-06-25T00:00:00",
        "last_modified_date": "2011-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5256",
        "title": "Structure and Complexity in Planning with Unary Operators",
        "authors": [
            "R. I. Brafman",
            "C. Domshlak"
        ],
        "abstract": "Unary operator domains -- i.e., domains in which operators have a single effect -- arise naturally in many control problems. In its most general form, the problem of STRIPS planning in unary operator domains is known to be as hard as the general STRIPS planning problem -- both are PSPACE-complete. However, unary operator domains induce a natural structure, called the domain's causal graph. This graph relates between the preconditions and effect of each domain operator. Causal graphs were exploited by Williams and Nayak in order to analyze plan generation for one of the controllers in NASA's Deep-Space One spacecraft. There, they utilized the fact that when this graph is acyclic, a serialization ordering over any subgoal can be obtained quickly. In this paper we conduct a comprehensive study of the relationship between the structure of a domain's causal graph and the complexity of planning in this domain. On the positive side, we show that a non-trivial polynomial time plan generation algorithm exists for domains whose causal graph induces a polytree with a constant bound on its node indegree. On the negative side, we show that even plan existence is hard when the graph is a directed-path singly connected DAG. More generally, we show that the number of paths in the causal graph is closely related to the complexity of planning in the associated domain. Finally we relate our results to the question of complexity of planning with serializable subgoals.\n    ",
        "submission_date": "2011-06-26T00:00:00",
        "last_modified_date": "2011-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5257",
        "title": "Answer Set Planning Under Action Costs",
        "authors": [
            "T. Eiter",
            "W. Faber",
            "N. Leone",
            "G. Pfeifer",
            "A. Polleres"
        ],
        "abstract": "Recently, planning based on answer set programming has been proposed as an approach towards realizing declarative planning systems. In this paper, we present the language Kc, which extends the declarative planning language K by action costs. Kc provides the notion of admissible and optimal plans, which are plans whose overall action costs are within a given limit resp. minimum over all plans (i.e., cheapest plans). As we demonstrate, this novel language allows for expressing some nontrivial planning tasks in a declarative way. Furthermore, it can be utilized for representing planning problems under other optimality criteria, such as computing ``shortest'' plans (with the least number of steps), and refinement combinations of cheapest and fastest plans. We study complexity aspects of the language Kc and provide a transformation to logic programs, such that planning problems are solved via answer set programming. Furthermore, we report experimental results on selected problems. Our experience is encouraging that answer set planning may be a valuable approach to expressive planning systems in which intricate planning problems can be naturally specified and solved.\n    ",
        "submission_date": "2011-06-26T00:00:00",
        "last_modified_date": "2011-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5258",
        "title": "Learning to Coordinate Efficiently: A Model-based Approach",
        "authors": [
            "R. I. Brafman",
            "M. Tennenholtz"
        ],
        "abstract": "In common-interest stochastic games all players receive an identical payoff. Players participating in such games must learn to coordinate with each other in order to receive the highest-possible value. A number of reinforcement learning algorithms have been proposed for this problem, and some have been shown to converge to good solutions in the limit. In this paper we show that using very simple model-based algorithms, much better (i.e., polynomial) convergence rates can be attained. Moreover, our model-based algorithms are guaranteed to converge to the optimal value, unlike many of the existing algorithms.\n    ",
        "submission_date": "2011-06-26T00:00:00",
        "last_modified_date": "2011-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5260",
        "title": "SAPA: A Multi-objective Metric Temporal Planner",
        "authors": [
            "M. Do",
            "S. Kambhampati"
        ],
        "abstract": "SAPA is a domain-independent heuristic forward chaining planner that can handle durative actions, metric resource constraints, and deadline goals. It is designed to be capable of handling the multi-objective nature of metric temporal planning. Our technical contributions include (i) planning-graph based methods for deriving heuristics that are sensitive to both cost and makespan (ii) techniques for adjusting the heuristic estimates to take action interactions and metric resource limitations into account and (iii) a linear time greedy post-processing technique to improve execution flexibility of the solution plans. An implementation of SAPA using many of the techniques presented in this paper was one of the best domain independent planners for domains with metric and temporal constraints in the third International Planning Competition, held at AIPS-02. We describe the technical details of extracting the heuristics and present an empirical evaluation of the current implementation of SAPA.\n    ",
        "submission_date": "2011-06-26T00:00:00",
        "last_modified_date": "2011-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5261",
        "title": "A New General Method to Generate Random Modal Formulae for Testing Decision Procedures",
        "authors": [
            "P. F. Patel-Schneider",
            "R. Sebastiani"
        ],
        "abstract": "The recent emergence of heavily-optimized modal decision procedures has highlighted the key role of empirical testing in this domain. Unfortunately, the introduction of extensive empirical tests for modal logics is recent, and so far none of the proposed test generators is very satisfactory. To cope with this fact, we present a new random generation method that provides benefits over previous methods for generating empirical tests. It fixes and much generalizes one of the best-known methods, the random CNF_[]m test, allowing for generating a much wider variety of problems, covering in principle the whole input space. Our new method produces much more suitable test sets for the current generation of modal decision procedures. We analyze the features of the new method by means of an extensive collection of empirical tests.\n    ",
        "submission_date": "2011-06-26T00:00:00",
        "last_modified_date": "2011-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5262",
        "title": "AltAltp: Online Parallelization of Plans with Heuristic State Search",
        "authors": [
            "S. Kambhampati",
            "R. Sanchez"
        ],
        "abstract": "Despite their near dominance, heuristic state search planners still lag behind disjunctive planners in the generation of parallel plans in classical planning. The reason is that directly searching for parallel solutions in state space planners would require the planners to branch on all possible subsets of parallel actions, thus increasing the branching factor exponentially. We present a variant of our heuristic state search planner AltAlt, called AltAltp which generates parallel plans by using greedy online parallelization of partial plans. The greedy approach is significantly informed by the use of novel distance heuristics that AltAltp derives from a graphplan-style planning graph for the problem. While this approach is not guaranteed to provide optimal parallel plans, empirical results show that AltAltp is capable of generating good quality parallel plans at a fraction of the cost incurred by the disjunctive planners.\n    ",
        "submission_date": "2011-06-26T00:00:00",
        "last_modified_date": "2011-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5263",
        "title": "New Polynomial Classes for Logic-Based Abduction",
        "authors": [
            "B. Zanuttini"
        ],
        "abstract": "We address the problem of propositional logic-based abduction, i.e., the problem of searching for a best explanation for a given propositional observation according to a given propositional knowledge base. We give a general algorithm, based on the notion of projection; then we study restrictions over the representations of the knowledge base and of the query, and find new polynomial classes of abduction problems.\n    ",
        "submission_date": "2011-06-26T00:00:00",
        "last_modified_date": "2011-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5265",
        "title": "Planning Through Stochastic Local Search and Temporal Action Graphs in LPG",
        "authors": [
            "A. Gerevini",
            "A. Saetti",
            "I. Serina"
        ],
        "abstract": "We present some techniques for planning in domains specified with the recent standard language PDDL2.1, supporting 'durative actions' and numerical quantities. These techniques are implemented in LPG, a domain-independent planner that took part in the 3rd International Planning Competition (IPC). LPG is an incremental, any time system producing multi-criteria quality plans. The core of the system is based on a stochastic local search method and on a graph-based representation called 'Temporal Action Graphs' (TA-graphs). This paper focuses on temporal planning, introducing TA-graphs and proposing some techniques to guide the search in LPG using this representation. The experimental results of the 3rd IPC, as well as further results presented in this paper, show that our techniques can be very effective. Often LPG outperforms all other fully-automated planners of the 3rd IPC in terms of speed to derive a solution, or quality of the solutions that can be produced.\n    ",
        "submission_date": "2011-06-26T00:00:00",
        "last_modified_date": "2011-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5266",
        "title": "TALplanner in IPC-2002: Extensions and Control Rules",
        "authors": [
            "J. Kvarnstr\u00f6m",
            "M. Magnusson"
        ],
        "abstract": "TALplanner is a forward-chaining planner that relies on domain knowledge in the shape of temporal logic formulas in order to prune irrelevant parts of the search space. TALplanner recently participated in the third International Planning Competition, which had a clear emphasis on increasing the complexity of the problem domains being used as benchmark tests and the expressivity required to represent these domains in a planning system. Like many other planners, TALplanner had support for some but not all aspects of this increase in expressivity, and a number of changes to the planner were required. After a short introduction to TALplanner, this article describes some of the changes that were made before and during the competition. We also describe the process of introducing suitable domain knowledge for several of the competition domains.\n    ",
        "submission_date": "2011-06-26T00:00:00",
        "last_modified_date": "2011-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5268",
        "title": "Temporal Decision Trees: Model-based Diagnosis of Dynamic Systems On-Board",
        "authors": [
            "L. Console",
            "C. Picardi",
            "D. Theseider Dupr\u00e8"
        ],
        "abstract": "The automatic generation of decision trees based on off-line reasoning on models of a domain is a reasonable compromise between the advantages of using a model-based approach in technical domains and the constraints imposed by embedded applications. In this paper we extend the approach to deal with temporal information. We introduce a notion of temporal decision tree, which is designed to make use of relevant information as long as it is acquired, and we present an algorithm for compiling such trees from a model-based reasoning system.\n    ",
        "submission_date": "2011-06-26T00:00:00",
        "last_modified_date": "2011-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5269",
        "title": "Optimal Schedules for Parallelizing Anytime Algorithms: The Case of Shared Resources",
        "authors": [
            "L. Finkelstein",
            "S. Markovitch",
            "E. Rivlin"
        ],
        "abstract": "The performance of anytime algorithms can be improved by simultaneously solving several instances of algorithm-problem pairs. These pairs may include different instances of a problem (such as starting from a different initial state), different algorithms (if several alternatives exist), or several runs of the same algorithm (for non-deterministic algorithms). In this paper we present a methodology for designing an optimal scheduling policy based on the statistical characteristics of the algorithms involved. We formally analyze the case where the processes share resources (a single-processor model), and provide an algorithm for optimal scheduling. We analyze, theoretically and empirically, the behavior of our scheduling algorithm for various distribution types. Finally, we present empirical results of applying our scheduling algorithm to the Latin Square problem.\n    ",
        "submission_date": "2011-06-26T00:00:00",
        "last_modified_date": "2011-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5270",
        "title": "Decision-Theoretic Bidding Based on Learned Density Models in Simultaneous, Interacting Auctions",
        "authors": [
            "J. A. Csirik",
            "M. L. Littman",
            "D. McAllester",
            "R. E. Schapire",
            "P. Stone"
        ],
        "abstract": "Auctions are becoming an increasingly popular method for transacting business, especially over the Internet. This article presents a general approach to building autonomous bidding agents to bid in multiple simultaneous auctions for interacting goods. A core component of our approach learns a model of the empirical price dynamics based on past data and uses the model to analytically calculate, to the greatest extent possible, optimal bids. We introduce a new and general boosting-based algorithm for conditional density estimation problems of this kind, i.e., supervised learning problems in which the goal is to estimate the entire conditional distribution of the real-valued label. This approach is fully implemented as ATTac-2001, a top-scoring agent in the second Trading Agent Competition (TAC-01). We present experiments demonstrating the effectiveness of our boosting-based price predictor relative to several reasonable alternatives.\n    ",
        "submission_date": "2011-06-26T00:00:00",
        "last_modified_date": "2011-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5271",
        "title": "The Metric-FF Planning System: Translating \"Ignoring Delete Lists\" to Numeric State Variables",
        "authors": [
            "J. Hoffmann"
        ],
        "abstract": "Planning with numeric state variables has been a challenge for many years, and was a part of the 3rd International Planning Competition (IPC-3). Currently one of the most popular and successful algorithmic techniques in STRIPS planning is to guide search by a heuristic function, where the heuristic is based on relaxing the planning task by ignoring the delete lists of the available actions. We present a natural extension of ``ignoring delete lists'' to numeric state variables, preserving the relevant theoretical properties of the STRIPS relaxation under the condition that the numeric task at hand is ``monotonic''. We then identify a subset of the numeric IPC-3 competition language, ``linear tasks'', where monotonicity can be achieved by pre-processing. Based on that, we extend the algorithms used in the heuristic planning system FF to linear tasks. The resulting system Metric-FF is, according to the IPC-3 results which we discuss, one of the two currently most efficient numeric planners.\n    ",
        "submission_date": "2011-06-26T00:00:00",
        "last_modified_date": "2011-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5312",
        "title": "Manipulation of Nanson's and Baldwin's Rules",
        "authors": [
            "Nina Narodytska",
            "Toby Walsh",
            "Lirong Xia"
        ],
        "abstract": "Nanson's and Baldwin's voting rules select a winner by successively eliminating candidates with low Borda scores. We show that these rules have a number of desirable computational properties. In particular, with unweighted votes, it is NP-hard to manipulate either rule with one manipulator, whilst with weighted votes, it is NP-hard to manipulate either rule with a small number of candidates and a coalition of manipulators. As only a couple of other voting rules are known to be NP-hard to manipulate with a single manipulator, Nanson's and Baldwin's rules appear to be particularly resistant to manipulation from a theoretical perspective. We also propose a number of approximation methods for manipulating these two rules. Experiments demonstrate that both rules are often difficult to manipulate in practice. These results suggest that elimination style voting rules deserve further study.\n    ",
        "submission_date": "2011-06-27T00:00:00",
        "last_modified_date": "2011-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5316",
        "title": "Online Cake Cutting (published version)",
        "authors": [
            "Toby Walsh"
        ],
        "abstract": "We propose an online form of the cake cutting problem. This models situations where agents arrive and depart during the process of dividing a resource. We show that well known fair division procedures like cut-and-choose and the Dubins-Spanier moving knife procedure can be adapted to apply to such online problems. We propose some fairness properties that online cake cutting procedures can possess like online forms of proportionality and envy-freeness. We also consider the impact of collusion between agents. Finally, we study theoretically and empirically the competitive ratio of these online cake cutting procedures. Based on its resistance to collusion, and its good performance in practice, our results favour the online version of the cut-and-choose procedure over the online version of the moving knife procedure.\n    ",
        "submission_date": "2011-06-27T00:00:00",
        "last_modified_date": "2011-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5427",
        "title": "Theory and Algorithms for Partial Order Based Reduction in Planning",
        "authors": [
            "You Xu",
            "Yixin Chen",
            "Qiang Lu",
            "Ruoyun Huang"
        ],
        "abstract": "Search is a major technique for planning. It amounts to exploring a state space of planning domains typically modeled as a directed graph. However, prohibitively large sizes of the search space make search expensive. Developing better heuristic functions has been the main technique for improving search efficiency. Nevertheless, recent studies have shown that improving heuristics alone has certain fundamental limits on improving search efficiency. Recently, a new direction of research called partial order based reduction (POR) has been proposed as an alternative to improving heuristics. POR has shown promise in speeding up searches.\n",
        "submission_date": "2011-06-27T00:00:00",
        "last_modified_date": "2011-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5448",
        "title": "Dominating Manipulations in Voting with Partial Information",
        "authors": [
            "Vincent Conitzer",
            "Toby Walsh",
            "Lirong Xia"
        ],
        "abstract": "We consider manipulation problems when the manipulator only has partial information about the votes of the nonmanipulators. Such partial information is described by an information set, which is the set of profiles of the nonmanipulators that are indistinguishable to the manipulator. Given such an information set, a dominating manipulation is a non-truthful vote that the manipulator can cast which makes the winner at least as preferable (and sometimes more preferable) as the winner when the manipulator votes truthfully. When the manipulator has full information, computing whether or not there exists a dominating manipulation is in P for many common voting rules (by known results). We show that when the manipulator has no information, there is no dominating manipulation for many common voting rules. When the manipulator's information is represented by partial orders and only a small portion of the preferences are unknown, computing a dominating manipulation is NP-hard for many common voting rules. Our results thus throw light on whether we can prevent strategic behavior by limiting information about the votes of other voters.\n    ",
        "submission_date": "2011-06-27T00:00:00",
        "last_modified_date": "2011-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5890",
        "title": "A Comparison of Lex Bounds for Multiset Variables in Constraint Programming",
        "authors": [
            "Yat-Chiu Law",
            "Jimmy Ho-Man Lee",
            "May Hiu-Chun Woo",
            "Toby Walsh"
        ],
        "abstract": "Set and multiset variables in constraint programming have typically been represented using subset bounds. However, this is a weak representation that neglects potentially useful information about a set such as its cardinality. For set variables, the length-lex (LL) representation successfully provides information about the length (cardinality) and position in the lexicographic ordering. For multiset variables, where elements can be repeated, we consider richer representations that take into account additional information. We study eight different representations in which we maintain bounds according to one of the eight different orderings: length-(co)lex (LL/LC), variety-(co)lex (VL/VC), length-variety-(co)lex (LVL/LVC), and variety-length-(co)lex (VLL/VLC) orderings. These representations integrate together information about the cardinality, variety (number of distinct elements in the multiset), and position in some total ordering. Theoretical and empirical comparisons of expressiveness and compactness of the eight representations suggest that length-variety-(co)lex (LVL/LVC) and variety-length-(co)lex (VLL/VLC) usually give tighter bounds after constraint propagation. We implement the eight representations and evaluate them against the subset bounds representation with cardinality and variety reasoning. Results demonstrate that they offer significantly better pruning and runtime.\n    ",
        "submission_date": "2011-06-29T00:00:00",
        "last_modified_date": "2011-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5917",
        "title": "Implementing Human-like Intuition Mechanism in Artificial Intelligence",
        "authors": [
            "Jitesh Dundas",
            "David Chik"
        ],
        "abstract": "Human intuition has been simulated by several research projects using artificial intelligence techniques. Most of these algorithms or models lack the ability to handle complications or diversions. Moreover, they also do not explain the factors influencing intuition and the accuracy of the results from this process. In this paper, we present a simple series based model for implementation of human-like intuition using the principles of connectivity and unknown entities. By using Poker hand datasets and Car evaluation datasets, we compare the performance of some well-known models with our intuition model. The aim of the experiment was to predict the maximum accurate answers using intuition based models. We found that the presence of unknown entities, diversion from the current problem scenario, and identifying weakness without the normal logic based execution, greatly affects the reliability of the answers. Generally, the intuition based models cannot be a substitute for the logic based mechanisms in handling such problems. The intuition can only act as a support for an ongoing logic based model that processes all the steps in a sequential manner. However, when time and computational cost are very strict constraints, this intuition based model becomes extremely important and useful, because it can give a reasonably good performance. Factors affecting intuition are analyzed and interpreted through our model.\n    ",
        "submission_date": "2011-06-29T00:00:00",
        "last_modified_date": "2011-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5995",
        "title": "From Cognitive Binary Logic to Cognitive Intelligent Agents",
        "authors": [
            "Nicolaie Popescu-Bodorin",
            "Valentina E. Balas"
        ],
        "abstract": "The relation between self awareness and intelligence is an open problem these days. Despite the fact that self awarness is usually related to Emotional Intelligence, this is not the case here. The problem described in this paper is how to model an agent which knows (Cognitive) Binary Logic and which is also able to pass (without any mistake) a certain family of Turing Tests designed to verify its knowledge and its discourse about the modal states of truth corresponding to well-formed formulae within the language of Propositional Binary Logic.\n    ",
        "submission_date": "2011-06-18T00:00:00",
        "last_modified_date": "2011-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5998",
        "title": "The 3rd International Planning Competition: Results and Analysis",
        "authors": [
            "M. Fox",
            "D. Long"
        ],
        "abstract": "This paper reports the outcome of the third in the series of biennial international planning competitions, held in association with the International Conference on AI Planning and Scheduling (AIPS) in 2002. In addition to describing the domains, the planners and the objectives of the competition, the paper includes analysis of the results. The results are analysed from several perspectives, in order to address the questions of comparative performance between planners, comparative difficulty of domains, the degree of agreement between planners about the relative difficulty of individual problem instances and the question of how well planners scale relative to one another over increasingly difficult problems. The paper addresses these questions through statistical analysis of the raw results of the competition, in order to determine which results can be considered to be adequately supported by the data. The paper concludes with a discussion of some challenges for the future of the competition series.\n    ",
        "submission_date": "2011-06-29T00:00:00",
        "last_modified_date": "2011-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.6022",
        "title": "Use of Markov Chains to Design an Agent Bidding Strategy for Continuous Double Auctions",
        "authors": [
            "W. P. Birmingham",
            "E. H. Durfee",
            "S. Park"
        ],
        "abstract": "As computational agents are developed for increasingly complicated e-commerce applications, the complexity of the decisions they face demands advances in artificial intelligence techniques. For example, an agent representing a seller in an auction should try to maximize the seller's profit by reasoning about a variety of possibly uncertain pieces of information, such as the maximum prices various buyers might be willing to pay, the possible prices being offered by competing sellers, the rules by which the auction operates, the dynamic arrival and matching of offers to buy and sell, and so on. A naive application of multiagent reasoning techniques would require the seller's agent to explicitly model all of the other agents through an extended time horizon, rendering the problem intractable for many realistically-sized problems. We have instead devised a new strategy that an agent can use to determine its bid price based on a more tractable Markov chain model of the auction process.  We have experimentally identified the conditions under which our new strategy works well, as well as how well it works in comparison to the optimal performance the agent could have achieved had it known the future. Our results show that our new strategy in general performs well, outperforming other tractable heuristic strategies in a majority of experiments, and is particularly effective in a 'seller?s market', where many buy offers are available.\n    ",
        "submission_date": "2011-06-29T00:00:00",
        "last_modified_date": "2011-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0018",
        "title": "A New Technique for Combining Multiple Classifiers using The Dempster-Shafer Theory of Evidence",
        "authors": [
            "A. Al-Ani",
            "M. Deriche"
        ],
        "abstract": "This paper presents a new classifier combination    technique based on the Dempster-Shafer theory of evidence. The    Dempster-Shafer theory of evidence is a powerful method for    combining measures of evidence from different classifiers. However,    since each of the available methods that estimates the evidence of    classifiers has its own limitations, we propose here a new    implementation which adapts to training data so that the overall    mean square error is minimized. The proposed technique is shown to    outperform most available classifier combination methods when    tested on three different classification problems.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0019",
        "title": "Searching for Bayesian Network Structures in the Space of Restricted Acyclic Partially Directed Graphs",
        "authors": [
            "S. Acid",
            "L. M. de Campos"
        ],
        "abstract": "Although many algorithms have been designed to construct    Bayesian network structures using different approaches and principles,    they all employ only two methods: those based on independence    criteria, and those based on a scoring function and a search procedure    (although some methods combine the two). Within the score+search    paradigm, the dominant approach uses local search methods in the space    of directed acyclic graphs (DAGs), where the usual choices for    defining the elementary modifications (local changes) that can be    applied are arc addition, arc deletion, and arc reversal. In this    paper, we propose a new local search method that uses a different    search space, and which takes account of the concept of equivalence    between network structures: restricted acyclic partially directed    graphs (RPDAGs). In this way, the number of different configurations    of the search space is reduced, thus improving efficiency. Moreover,    although the final result must necessarily be a local optimum given    the nature of the search method, the topology of the new search space,    which avoids making early decisions about the directions of the arcs,    may help to find better local optima than those obtained by searching    in the DAG space. Detailed results of the evaluation of the proposed    search method on several test problems, including the well-known Alarm    Monitoring System, are also presented.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0020",
        "title": "Learning to Order BDD Variables in Verification",
        "authors": [
            "O. Grumberg",
            "S. Livne",
            "S. Markovitch"
        ],
        "abstract": "The size and complexity of software and hardware systems    have significantly increased in the past years.  As a result, it is    harder to guarantee their correct behavior. One of the most successful    methods for automated verification of finite-state systems is model    checking. Most of the current model-checking systems use binary    decision diagrams (BDDs) for the representation of the tested model    and in the verification process of its properties. Generally, BDDs    allow a canonical compact representation of a boolean function (given    an order of its variables). The more compact the BDD is, the better    performance one gets from the verifier. However, finding an optimal    order for a BDD is an NP-complete problem. Therefore, several    heuristic methods based on expert knowledge have been developed for    variable ordering.       We propose an alternative approach in which the variable ordering     algorithm gains 'ordering experience' from training models and     uses the learned knowledge for finding good orders. Our     methodology is based on offline learning of pair precedence     classifiers from training models, that is, learning which variable     pair permutation is more likely to lead to a good order. For each     training model, a number of training sequences are evaluated. Every     training model variable pair permutation is then tagged based on     its performance on the evaluated orders. The tagged permutations     are then passed through a feature extractor and are given as     examples to a classifier creation algorithm. Given a model for     which an order is requested, the ordering algorithm consults each     precedence classifier and constructs a pair precedence table     which is used to create the order.      Our algorithm was integrated with SMV, which is one of the most     widely used verification systems. Preliminary empirical evaluation of our     methodology, using real benchmark models, shows performance that     is better than random ordering and is competitive with existing     algorithms that use expert knowledge. We believe that in     sub-domains of models (alu, caches, etc.) our system will prove     even more valuable. This is because it features the ability to     learn sub-domain knowledge, something that no other ordering     algorithm does.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0021",
        "title": "Decentralized Supply Chain Formation: A Market Protocol and Competitive Equilibrium Analysis",
        "authors": [
            "W. E. Walsh",
            "M. P. Wellman"
        ],
        "abstract": "Supply chain formation is the process of determining the structure and terms of exchange relationships to enable a multilevel, multiagent production activity.  We present a simple model of supply chains, highlighting two characteristic features: hierarchical subtask decomposition, and resource contention.  To decentralize the formation process, we introduce a market price system over the resources produced along the chain.  In a competitive equilibrium for this system, agents choose locally optimal allocations with respect to prices, and outcomes are optimal overall.  To determine prices, we define a market protocol based on distributed, progressive auctions, and myopic, non-strategic agent bidding policies.  In the presence of resource contention, this protocol produces better solutions than the greedy protocols common in the artificial intelligence and multiagent systems literature.  The protocol often converges to high-value supply chains, and when competitive equilibria exist, typically to approximate competitive equilibria.  However, complementarities in agent production technologies can cause the protocol to wastefully allocate inputs to agents that do not produce their outputs.  A subsequent decommitment phase recovers a significant fraction of the lost surplus.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0023",
        "title": "CP-nets: A Tool for Representing and Reasoning withConditional Ceteris Paribus Preference Statements",
        "authors": [
            "C. Boutilier",
            "R. I. Brafman",
            "C. Domshlak",
            "H. H. Hoos",
            "D. Poole"
        ],
        "abstract": "Information about user preferences plays a key role in automated decision making. In many domains it is desirable to assess such preferences in a qualitative rather than quantitative way. In this paper, we propose a qualitative graphical representation of preferences that reflects conditional dependence and independence of preference statements under a ceteris paribus (all else being equal) interpretation. Such a representation is often compact and arguably quite natural in many circumstances. We provide a formal semantics for this model, and describe how the structure of the network can be exploited in several inference tasks, such as determining whether one outcome dominates (is preferred to) another, ordering a set outcomes according to the preference relation, and constructing the best outcome subject to available evidence.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0024",
        "title": "Complexity Results and Approximation Strategies for MAP Explanations",
        "authors": [
            "A. Darwiche",
            "J. D. Park"
        ],
        "abstract": "MAP is the problem of finding a most probable instantiation of a set of variables given evidence. MAP has always been perceived to be significantly harder than the related problems of computing the probability of a variable instantiation Pr, or the problem of computing the most probable explanation (MPE). This paper investigates the complexity of MAP in Bayesian networks. Specifically, we show that MAP is complete for NP^PP and provide further negative complexity results for algorithms based on variable elimination. We also show that MAP remains hard even when MPE and Pr become easy. For example, we show that MAP is NP-complete when the networks are restricted to polytrees, and even then can not be effectively approximated.  Given the difficulty of computing MAP exactly, and the difficulty of approximating MAP while providing useful guarantees on the resulting approximation, we investigate best effort approximations. We introduce a generic MAP approximation framework. We provide two instantiations of the framework; one for networks which are amenable to exact inference Pr, and one for networks for which even exact inference is too hard. This allows MAP approximation on networks that are too complex to even exactly solve the easier problems, Pr and MPE. Experimental results indicate that using these approximation algorithms provides much better solutions than standard techniques, and provide accurate MAP estimates in many cases. \n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0025",
        "title": "Taming Numbers and Durations in the Model Checking Integrated Planning System",
        "authors": [
            "S. Edelkamp"
        ],
        "abstract": "The Model Checking Integrated Planning System (MIPS) is a temporal least commitment heuristic search planner based on a flexible object-oriented workbench architecture. Its design clearly separates explicit and symbolic directed exploration algorithms from the set of on-line and off-line computed estimates and associated data structures.   MIPS has shown distinguished performance in the last two international planning competitions. In the last event the description language was extended from pure propositional planning to include numerical state variables, action durations, and plan quality objective functions. Plans were no longer sequences of actions but time-stamped schedules.   As a participant of the fully automated track of the competition, MIPS has proven to be a general system; in each track and every benchmark domain it efficiently computed plans of remarkable quality. This article introduces and analyzes the most important algorithmic novelties that were necessary to tackle the new layers of expressiveness in the benchmark problems and to achieve a high level of performance.   The extensions include critical path analysis of sequentially generated plans to generate corresponding optimal parallel plans.  The linear time algorithm to compute the parallel plan bypasses known NP hardness results for partial ordering by scheduling plans with respect to the set of actions and the imposed precedence relations. The efficiency of this algorithm also allows us to improve the exploration guidance: for each encountered planning state the corresponding approximate sequential plan is scheduled.   One major strength of MIPS is its static analysis phase that grounds and simplifies parameterized predicates, functions and operators, that infers knowledge to minimize the state description length, and that detects domain object symmetries.  The latter aspect is analyzed in detail.   MIPS has been developed to serve as a complete and optimal state space planner, with admissible estimates, exploration engines and branching cuts. In the competition version, however, certain performance compromises had to be made, including floating point arithmetic, weighted heuristic search exploration according to an inadmissible estimate and parameterized optimization.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0026",
        "title": "IDL-Expressions: A Formalism for Representing and Parsing Finite Languages in Natural Language Processing",
        "authors": [
            "M. J. Nederhof",
            "G. Satta"
        ],
        "abstract": "We  propose  a  formalism  for  representation  of  finite  languages, referred to  as the class of IDL-expressions,  which combines concepts that were  only considered in  isolation in existing  formalisms.  The suggested  applications  are  in  natural  language  processing,  more specifically  in surface  natural language  generation and  in machine translation, where a sentence is  obtained by first generating a large set of candidate sentences, represented  in a compact way, and then by filtering  such a  set  through  a parser.   We  study several  formal properties of IDL-expressions and compare this new formalism with more standard  ones.   We  also  present  a  novel  parsing  algorithm  for IDL-expressions  and  prove a  non-trivial  upper  bound  on its  time complexity.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0027",
        "title": "Effective Dimensions of Hierarchical Latent Class Models",
        "authors": [
            "T. Kocka",
            "N. L. Zhang"
        ],
        "abstract": "Hierarchical latent class (HLC) models are tree-structured Bayesian networks where leaf nodes are observed while internal nodes are latent.  There are no theoretically well justified model selection criteria for HLC models in particular and Bayesian networks with latent nodes in general. Nonetheless, empirical studies suggest that the BIC score is a reasonable criterion to use in practice for learning HLC models.  Empirical studies also suggest that sometimes model selection can be improved if standard model dimension is replaced  with effective model dimension in the penalty term of the BIC score.  Effective dimensions are difficult to compute. In this paper, we prove a theorem that relates the effective dimension of an HLC model to the effective dimensions of a number of latent class models.  The theorem makes it computationally feasible to compute the effective dimensions of large HLC models.  The theorem can also be used to compute the effective dimensions of general tree models.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0030",
        "title": "Coherent Integration of Databases by Abductive Logic Programming",
        "authors": [
            "O. Arieli",
            "M. Bruynooghe",
            "M. Denecker",
            "B. Van Nuffelen"
        ],
        "abstract": "  We introduce an abductive method for a coherent integration of independent data-sources. The idea is to compute a list of data-facts that should be inserted to the amalgamated database or retracted from it in order to restore its consistency. This method is implemented by an abductive solver, called Asystem, that applies SLDNFA-resolution on a meta-theory that relates different, possibly contradicting, input databases. We also give a pure model-theoretic analysis of the possible ways to `recover' consistent data from an inconsistent database in terms of those models of the database that exhibit as minimal inconsistent information as reasonably possible. This allows us to characterize the `recovered databases' in terms of the `preferred' (i.e., most consistent) models of the theory. The outcome is an abductive-based application that is sound and complete with respect to a corresponding model-based, preferential semantics, and -- to the best of our knowledge -- is more expressive (thus more general) than any other implementation of coherent integration of databases.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0031",
        "title": "Grounded Semantic Composition for Visual Scenes",
        "authors": [
            "P. Gorniak",
            "D. Roy"
        ],
        "abstract": "We present a visually-grounded language understanding model based on a study of how people verbally describe objects in scenes. The emphasis of the model is on the combination of individual word meanings to produce meanings for complex referring expressions. The model has been implemented, and it is able to understand a broad range of spatial referring expressions. We describe our implementation of word level visually-grounded semantics and their embedding in a compositional parsing framework. The implemented system selects the correct referents in response to natural language expressions for a large percentage of test cases. In an analysis of the system's successes and failures we reveal how visual context influences the semantics of utterances and propose future extensions to the model that take such context into account.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0034",
        "title": "Price Prediction in a Trading Agent Competition",
        "authors": [
            "K. M. Lochner",
            "D. M. Reeves",
            "Y. Vorobeychik",
            "M. P. Wellman"
        ],
        "abstract": "The 2002 Trading Agent Competition (TAC) presented a challenging market game in the domain of travel shopping.  One of the pivotal issues in this domain is uncertainty about hotel prices, which have a significant influence on the relative cost of alternative trip schedules.  Thus, virtually all participants employ some method for predicting hotel prices.  We survey approaches employed in the tournament, finding that agents apply an interesting diversity of techniques, taking into account differing sources of evidence bearing on prices.  Based on data provided by entrants on their agents' actual predictions in the TAC-02 finals and semifinals, we analyze the relative efficacy of these approaches.  The results show that taking into account game-specific information about flight prices is a major distinguishing factor.  Machine learning methods effectively induce the relationship between flight and hotel prices from game data, and a purely analytical approach based on competitive equilibrium analysis achieves equal accuracy with no historical data.  Employing a new measure of prediction quality, we relate absolute accuracy to bottom-line performance in the game.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0035",
        "title": "Compositional Model Repositories via Dynamic Constraint Satisfaction with Order-of-Magnitude Preferences",
        "authors": [
            "J. Keppens",
            "Q. Shen"
        ],
        "abstract": "The predominant knowledge-based approach to automated model construction, compositional modelling, employs a set of models of particular functional components.  Its inference mechanism takes a scenario describing the constituent interacting components of a system and translates it into a useful mathematical model.  This paper presents a novel compositional modelling approach aimed at building model repositories.  It furthers the field in two respects.  Firstly, it expands the application domain of compositional modelling to systems that can not be easily described in terms of interacting functional components, such as ecological systems.  Secondly, it enables the incorporation of user preferences into the model selection process.  These features are achieved by casting the compositional modelling problem as an activity-based dynamic preference constraint satisfaction problem, where the dynamic constraints describe the restrictions imposed over the composition of partial models and the preferences correspond to those of the user of the automated modeller. In addition, the preference levels are represented through the use of symbolic values that differ in orders of magnitude.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0036",
        "title": "Can We Learn to Beat the Best Stock",
        "authors": [
            "A. Borodin",
            "R. El-Yaniv",
            "V. Gogan"
        ],
        "abstract": "A novel algorithm for actively trading stocks is presented. While traditional expert advice and \"universal\" algorithms (as well as standard technical trading heuristics) attempt to predict winners or trends, our approach relies on predictable statistical relations between all pairs of stocks in the market. Our empirical results on historical markets provide strong evidence that this type of technical trading can \"beat the market\" and moreover, can beat the best stock in the market. In doing so we utilize a new idea for smoothing critical parameters in the context of expert learning.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0037",
        "title": "Competitive Coevolution through Evolutionary Complexification",
        "authors": [
            "R. Miikkulainen",
            "K. O. Stanley"
        ],
        "abstract": "Two major goals in machine learning are the discovery and improvement of solutions to complex problems.  In this paper, we argue that complexification, i.e. the incremental elaboration of solutions through adding new structure, achieves both these goals.  We demonstrate the power of complexification through the NeuroEvolution of Augmenting Topologies (NEAT) method, which evolves increasingly complex neural network architectures.  NEAT is applied to an open-ended coevolutionary robot duel domain where robot controllers compete head to head.  Because the robot duel domain supports a wide range of strategies, and because coevolution benefits from an escalating arms race, it serves as a suitable testbed for studying complexification.  When compared to the evolution of networks with fixed structure, complexifying evolution discovers significantly more sophisticated strategies.  The results suggest that in order to discover and improve complex solutions, evolution, and search in general, should be allowed to complexify as well as optimize.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0038",
        "title": "Dual Modelling of Permutation and Injection Problems",
        "authors": [
            "B. Hnich",
            "B. M. Smith",
            "T. Walsh"
        ],
        "abstract": "When writing a constraint program, we have to choose which variables  should be the decision variables, and how to represent the constraints  on these variables. In many cases, there is considerable choice for  the decision variables.  Consider, for example, permutation problems in which we have as many values as variables, and each variable takes  an unique value. In such problems, we can choose between a primal and a dual viewpoint. In the dual viewpoint, each dual variable  represents one of the primal values, whilst each dual value represents one of the primal variables. Alternatively, by means of channelling  constraints to link the primal and dual variables, we can have a  combined model with both sets of variables. In this paper, we perform  an extensive theoretical and empirical study of such primal, dual and  combined models for two classes of problems: permutation problems and  injection problems. Our results show that it often be advantageous to  use multiple viewpoints, and to have constraints which channel between them to maintain consistency. They also illustrate a general  methodology for comparing different constraint models.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0040",
        "title": "Generalizing Boolean Satisfiability I: Background and Survey of Existing Work",
        "authors": [
            "H. E. Dixon",
            "M. L. Ginsberg",
            "A. J. Parkes"
        ],
        "abstract": "This is the first of three planned papers describing ZAP, a satisfiability engine that substantially generalizes existing tools while retaining the performance characteristics of modern high-performance solvers.  The fundamental idea underlying ZAP is that many problems passed to such engines contain rich internal structure that is obscured by the Boolean representation used; our goal is to define a representation in which this structure is apparent and can easily be exploited to improve computational performance.  This paper is a survey of the work underlying ZAP, and discusses previous attempts to improve the performance of the Davis-Putnam-Logemann-Loveland algorithm by exploiting the structure of the problem being solved.  We examine existing ideas including extensions of the Boolean language to allow cardinality constraints, pseudo-Boolean representations, symmetry, and a limited form of quantification.  While this paper is intended as a survey, our research results are contained in the two subsequent articles, with the theoretical structure of ZAP described in the second paper in this series, and ZAP's implementation described in the third.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0041",
        "title": "PHA*: Finding the Shortest Path with A* in An Unknown Physical Environment",
        "authors": [
            "A. Ben-Yair",
            "A. Felner",
            "S. Kraus",
            "N. Netanyahu",
            "R. Stern"
        ],
        "abstract": "We address the problem of finding the shortest path between two points in an unknown real physical environment, where a traveling agent must move around in the environment to explore unknown territory.  We introduce the Physical-A* algorithm (PHA*) for solving this problem. PHA* expands all the mandatory nodes that A* would expand and returns the shortest path between the two points.  However, due to the physical nature of the problem, the complexity of the algorithm is measured by the traveling effort of the moving agent and not by the number of generated nodes, as in standard A*.  PHA* is presented as a two-level algorithm, such that its high level, A*, chooses the next node to be expanded and its low level directs the agent to that node in order to explore it.  We present a number of variations for both the high-level and low-level procedures and evaluate their performance theoretically and experimentally.  We show that the travel cost of our best variation is fairly close to the optimal travel cost, assuming that the mandatory nodes of A* are known in advance.  We then generalize our algorithm to the multi-agent case, where a number of cooperative agents are designed to solve the problem.  Specifically, we provide an experimental implementation for such a system.  It should be noted that the problem addressed here is not a navigation problem, but rather a problem of finding the shortest path between two points for future usage.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0042",
        "title": "Restricted Value Iteration: Theory and Algorithms",
        "authors": [
            "N. L. Zhang",
            "W. Zhang"
        ],
        "abstract": "Value iteration is a popular algorithm for finding near optimal policies for POMDPs.  It is inefficient due to the need to account for the entire belief space, which necessitates the solution of large numbers of linear programs.  In this paper, we study value iteration restricted to belief subsets. We show that, together with properly chosen belief subsets, restricted value iteration yields near-optimal policies and we give a condition for determining whether a given belief subset would bring about savings in space and time. We also apply restricted value iteration to two interesting classes of POMDPs, namely informative POMDPs and near-discernible POMDPs.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0043",
        "title": "A Maximal Tractable Class of Soft Constraints",
        "authors": [
            "D. Cohen",
            "M. Cooper",
            "P. Jeavons",
            "A. Krokhin"
        ],
        "abstract": "Many researchers in artificial intelligence are beginning to explore the use of soft constraints to express a set of (possibly conflicting) problem  requirements. A soft constraint is a function defined on a collection of  variables which associates some measure of desirability with each possible  combination of values for those variables. However, the crucial question of  the computational complexity of finding the optimal solution to a collection of soft constraints has so far received very little attention. In this paper we identify a class of soft binary constraints for which the problem of  finding the optimal solution is tractable. In other words, we show that for  any given set of such constraints, there exists a polynomial time algorithm  to determine the assignment having the best overall combined measure of  desirability. This tractable class includes many commonly-occurring soft constraints, such as 'as near as possible' or 'as soon as possible after', as well as crisp constraints such as 'greater than'. Finally, we show that  this tractable class is maximal, in the sense that adding any other form of  soft binary constraint which is not in the class gives rise to a class of  problems which is NP-hard.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0044",
        "title": "Towards Understanding and Harnessing the Potential of Clause Learning",
        "authors": [
            "P. Beame",
            "H. Kautz",
            "A. Sabharwal"
        ],
        "abstract": "Efficient implementations of DPLL with the addition of clause learning are the fastest complete Boolean satisfiability solvers and can handle many significant real-world problems, such as verification, planning and design. Despite its importance, little is known of the ultimate strengths and limitations of the technique. This paper presents the first precise characterization of clause learning as a proof system (CL), and begins the task of understanding its power by relating it to the well-studied resolution proof system. In particular, we show that with a new learning scheme, CL can provide exponentially shorter proofs than many proper refinements of general resolution (RES) satisfying a natural property. These include regular and Davis-Putnam resolution, which are already known to be much stronger than ordinary DPLL. We also show that a slight variant of CL with unlimited restarts is as powerful as RES itself. Translating these analytical results to practice, however, presents a challenge because of the nondeterministic nature of clause learning algorithms. We propose a novel way of exploiting the underlying problem structure, in the form of a high level problem description such as a graph or PDDL specification, to guide clause learning algorithms toward faster solutions. We show that this leads to exponential speed-ups on grid and randomized pebbling problems, as well as substantial improvements on certain ordering formulas.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0045",
        "title": "Graduality in Argumentation",
        "authors": [
            "C. Cayrol",
            "M. C. Lagasquie-Schiex"
        ],
        "abstract": "Argumentation is based on the exchange and valuation of interacting arguments, followed by the selection of the most acceptable of them (for example, in order to take a decision, to make a choice). Starting from the framework proposed by Dung in 1995, our purpose is to introduce 'graduality' in the selection of the best arguments, i.e., to be able to partition the set of the arguments in more than the two usual subsets of 'selected' and 'non-selected' arguments in order to represent different levels of selection.  Our basic idea is that an argument is all the more acceptable if it can be preferred to its attackers.  First, we discuss general principles underlying a 'gradual' valuation of arguments based on their interactions. Following these principles, we define several valuation models for an abstract argumentation system.  Then, we introduce 'graduality' in the concept of acceptability of arguments. We propose new acceptability classes and a refinement of existing classes taking advantage of an available 'gradual' valuation.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0046",
        "title": "Explicit Learning Curves for Transduction and Application to Clustering and Compression Algorithms",
        "authors": [
            "P. Derbeko",
            "R. El-Yaniv",
            "R. Meir"
        ],
        "abstract": "Inductive learning is based on inferring a general rule from a finite data set and using it to label new data. In transduction one attempts to solve the problem of using a labeled training set to label a set of unlabeled points, which are given to the learner prior to learning. Although transduction seems at the outset to be an easier task than induction, there have not been many provably useful algorithms for transduction. Moreover, the precise relation between induction and transduction has not yet been determined. The main theoretical developments related to transduction were presented by Vapnik more than twenty years ago. One of Vapnik's basic results is a rather tight error bound for transductive classification based on an exact computation of the hypergeometric tail. While tight, this bound is given implicitly via a computational routine. Our first contribution is a somewhat looser but explicit characterization of a slightly extended PAC-Bayesian version of Vapnik's transductive bound. This characterization is obtained using concentration inequalities for the tail of sums of random variables obtained by sampling without replacement. We then derive error bounds for compression schemes such as (transductive) support vector machines and for transduction algorithms based on clustering. The main observation used for deriving these new error bounds and algorithms is that the unlabeled test points, which in the transductive setting are known in advance, can be used in order to construct useful data dependent prior distributions over the hypothesis space.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0047",
        "title": "Decentralized Control of Cooperative Systems: Categorization and Complexity Analysis",
        "authors": [
            "C. V. Goldman",
            "S. Zilberstein"
        ],
        "abstract": "Decentralized control of cooperative systems captures the operation of a group of decision makers that share a single global objective.  The difficulty in solving optimally such problems arises when the agents lack full observability of the global state of the system when they operate. The general problem has been shown to be NEXP-complete. In this paper, we identify classes of decentralized control problems whose complexity ranges between NEXP and P. In particular, we study problems characterized by independent transitions, independent observations, and goal-oriented objective functions.  Two algorithms are shown to solve optimally useful classes of goal-oriented decentralized processes in polynomial time.  This paper also studies information sharing among the decision-makers, which can improve their performance. We distinguish between three ways in which agents can exchange information: indirect communication, direct communication and sharing state features that are not controlled by the agents.  Our analysis shows that for every class of problems we consider, introducing direct or indirect communication does not change the worst-case complexity.  The results provide a better understanding of the complexity of decentralized control problems that arise in practice and facilitate the development of planning algorithms for these problems.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0048",
        "title": "Reinforcement Learning for Agents with Many Sensors and Actuators Acting in Categorizable Environments",
        "authors": [
            "E. Celaya",
            "J. M. Porta"
        ],
        "abstract": "In this paper, we confront the problem of applying reinforcement learning to agents that perceive the environment through many sensors and that can perform  parallel actions using many actuators as is the case in complex autonomous robots. We argue that reinforcement learning can only be successfully applied to this case  if strong assumptions are made on the characteristics of the environment in which  the learning is performed, so that the relevant sensor readings and motor commands can be  readily identified. The introduction of such assumptions leads to strongly-biased  learning systems that can eventually lose the generality of traditional  reinforcement-learning algorithms.  In this line, we observe that, in realistic situations, the reward received by the robot  depends only on a reduced subset of all the executed actions and that only a reduced  subset of the sensor inputs (possibly different in each situation and for each action)  are relevant to predict the reward. We formalize this property in the so called  'categorizability assumption' and we present an algorithm that takes advantage of  the categorizability of the environment, allowing a decrease in the learning time with  respect to existing reinforcement-learning algorithms. Results of the application of the  algorithm to a couple of simulated realistic-robotic problems (landmark-based navigation  and the six-legged robot gait generation) are reported to validate our approach and to  compare it to existing flat and generalization-based reinforcement-learning approaches.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0050",
        "title": "Additive Pattern Database Heuristics",
        "authors": [
            "A. Felner",
            "S. Hanan",
            "R. E. Korf"
        ],
        "abstract": "We explore a method for computing admissible heuristic evaluation functions for search problems. It utilizes pattern databases, which are precomputed tables of the exact cost of solving various subproblems of an existing problem. Unlike standard pattern database heuristics, however, we partition our problems into disjoint subproblems, so that the costs of solving the different subproblems can be added together without overestimating the cost of solving the original problem. Previously, we showed how to statically partition the sliding-tile puzzles into disjoint groups of tiles to compute an admissible heuristic, using the same partition for each state and problem instance. Here we extend the method and show that it applies to other domains as well. We also present another method for additive heuristics which we call dynamically partitioned pattern databases. Here we partition the problem into disjoint subproblems for each state of the search dynamically. We discuss the pros and cons of each of these methods and apply both methods to three different problem domains: the sliding-tile puzzles, the 4-peg Towers of Hanoi problem, and finding an optimal vertex cover of a graph. We find that in some problem domains, static partitioning is most effective, while in others dynamic partitioning is a better choice. In each of these problem domains, either statically partitioned or dynamically partitioned pattern database heuristics are the best known heuristics for the problem.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0051",
        "title": "On Prediction Using Variable Order Markov Models",
        "authors": [
            "R. Begleiter",
            "R. El-Yaniv",
            "G. Yona"
        ],
        "abstract": "This paper is concerned with algorithms for prediction of discrete sequences over a finite alphabet, using variable order Markov models. The class of such algorithms is large and in principle includes any lossless compression algorithm. We focus on six prominent prediction algorithms, including Context Tree Weighting (CTW), Prediction by Partial Match (PPM) and Probabilistic Suffix Trees (PSTs). We discuss the properties of these algorithms and compare their performance using real life sequences from three domains: proteins, English text and music pieces. The comparison is made with respect to prediction quality as measured by the average log-loss. We also compare classification algorithms based on these predictors with respect to a number of large protein classification tasks. Our results indicate that a \"decomposed\" CTW (a variant of the CTW algorithm) and PPM outperform all other algorithms in sequence prediction tasks. Somewhat surprisingly, a different algorithm, which is a modification of the Lempel-Ziv compression algorithm, significantly outperforms all algorithms on the protein classification problems.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0052",
        "title": "Ordered Landmarks in Planning",
        "authors": [
            "J. Hoffmann",
            "J. Porteous",
            "L. Sebastia"
        ],
        "abstract": "Many known planning tasks have inherent constraints concerning the best order in which to achieve the goals. A number of research efforts have been made to detect such constraints and to use them for guiding search, in the hope of speeding up the planning process.     We go beyond the previous approaches by considering ordering constraints not only over the (top-level) goals, but also over the sub-goals that will necessarily arise during planning. Landmarks are facts that must be true at some point in every valid solution plan.  We extend Koehler and Hoffmann's definition of reasonable orders between top level goals to the more general case of landmarks. We show how landmarks can be found, how their reasonable orders can be approximated, and how this information can be used to decompose a given planning task into several smaller sub-tasks. Our methodology is completely domain- and planner-independent. The implementation demonstrates that the approach can yield significant runtime performance improvements when used as a control loop around state-of-the-art sub-optimal planning systems, as exemplified by FF and LPG.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0053",
        "title": "Finding Approximate POMDP solutions Through Belief Compression",
        "authors": [
            "N. Roy",
            "G. Gordon",
            "S. Thrun"
        ],
        "abstract": "Standard value function approaches to finding policies for Partially Observable Markov Decision Processes (POMDPs) are generally considered to be intractable for large models. The intractability of these algorithms is to a large extent a consequence of computing an exact, optimal policy over the entire belief space. However, in real-world POMDP problems, computing the optimal policy for the full belief space is often unnecessary for good control even for problems with complicated policy classes. The beliefs experienced by the controller often lie near a structured, low-dimensional subspace embedded in the high-dimensional belief space. Finding a good approximation to the optimal value function for only this subspace can be much easier than computing the full value function. We introduce a new method for solving large-scale POMDPs by reducing the dimensionality of the belief space. We use Exponential family Principal Components Analysis (Collins, Dasgupta and Schapire, 2002) to represent sparse, high-dimensional belief spaces using small sets of learned features of the belief state. We then plan only in terms of the low-dimensional belief features. By planning in this low-dimensional space, we can find policies for POMDP models that are orders of magnitude larger than models that can be handled by conventional techniques. We demonstrate the use of this algorithm on a synthetic problem and on mobile robot navigation tasks.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0054",
        "title": "A Comprehensive Trainable Error Model for Sung Music Queries",
        "authors": [
            "W. P. Birmingham",
            "C. J. Meek"
        ],
        "abstract": "We propose a model for errors in sung queries, a variant of the hidden Markov model (HMM). This is a solution to the problem of identifying the degree of similarity between a (typically error-laden) sung query and a potential target in a database of musical works, an important problem in the field of music information retrieval. Similarity metrics are a critical component of query-by-humming (QBH) applications which search audio and multimedia databases for strong matches to oral queries. Our model comprehensively expresses the types of error or variation between target and query: cumulative and non-cumulative local errors, transposition, tempo and tempo changes, insertions, deletions and modulation. The model is not only expressive, but automatically trainable, or able to learn and generalize from query examples. We present results of simulations, designed to assess the discriminatory potential of the model, and tests with real sung queries, to demonstrate relevance to real-world applications.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0055",
        "title": "Phase Transitions and Backbones of the Asymmetric Traveling Salesman Problem",
        "authors": [
            "W. Zhang"
        ],
        "abstract": "In recent years, there has been much interest in phase transitions of combinatorial problems.  Phase transitions have been successfully used to analyze combinatorial optimization problems, characterize their typical-case features and locate the hardest problem instances.  In this paper, we study phase transitions of the asymmetric Traveling Salesman Problem (ATSP), an NP-hard combinatorial optimization problem that has many real-world applications.  Using random instances of up to 1,500 cities in which intercity distances are uniformly distributed, we empirically show that many properties of the problem, including the optimal tour cost and backbone size, experience sharp transitions as the precision of intercity distances increases across a critical value.  Our experimental results on the costs of the ATSP tours and assignment problem agree with the theoretical result that the asymptotic cost of assignment problem is pi ^2 /6 the number of cities goes to infinity.  In addition, we show that the average computational cost of the well-known branch-and-bound subtour elimination algorithm for the problem also exhibits a thrashing behavior, transitioning from easy to difficult as the distance precision increases.  These results answer positively an open question regarding the existence of phase transitions in the ATSP, and provide guidance on how difficult ATSP problem instances should be generated.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0134",
        "title": "The Influence of Global Constraints on Similarity Measures for Time-Series Databases",
        "authors": [
            "Vladimir Kurbalija",
            "Milo\u0161 Radovanovi\u0107",
            "Zoltan Geler",
            "Mirjana Ivanovi\u0107"
        ],
        "abstract": "A time series consists of a series of values or events obtained over repeated measurements in time. Analysis of time series represents and important tool in many application areas, such as stock market analysis, process and quality control, observation of natural phenomena, medical treatments, etc. A vital component in many types of time-series analysis is the choice of an appropriate distance/similarity measure. Numerous measures have been proposed to date, with the most successful ones based on dynamic programming. Being of quadratic time complexity, however, global constraints are often employed to limit the search space in the matrix during the dynamic programming procedure, in order to speed up computation. Furthermore, it has been reported that such constrained measures can also achieve better accuracy. In this paper, we investigate two representative time-series distance/similarity measures based on dynamic programming, Dynamic Time Warping (DTW) and Longest Common Subsequence (LCS), and the effects of global constraints on them. Through extensive experiments on a large number of time-series data sets, we demonstrate how global constrains can significantly reduce the computation time of DTW and LCS. We also show that, if the constraint parameter is tight enough (less than 10-15% of time-series length), the constrained measure becomes significantly different from its unconstrained counterpart, in the sense of producing qualitatively different 1-nearest neighbor graphs. This observation explains the potential for accuracy gains when using constrained measures, highlighting the need for careful tuning of constraint parameters in order to achieve a good trade-off between speed and accuracy.\n    ",
        "submission_date": "2011-07-01T00:00:00",
        "last_modified_date": "2013-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0194",
        "title": "Law of Connectivity in Machine Learning",
        "authors": [
            "Jitesh Dundas"
        ],
        "abstract": "We present in this paper our law that there is always a connection present between two entities, with a selfconnection being present at least in each node. An entity is an object, physical or imaginary, that is connected by a path (or connection) and which is important for achieving the desired result of the scenario. In machine learning, we state that for any scenario, a subject entity is always, directly or indirectly, connected and affected by single or multiple independent / dependent entities, and their impact on the subject entity is dependent on various factors falling into the categories such as the existenc\n    ",
        "submission_date": "2011-07-01T00:00:00",
        "last_modified_date": "2011-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0268",
        "title": "Simple Algorithm Portfolio for SAT",
        "authors": [
            "Mladen Nikolic",
            "Filip Maric",
            "Predrag Janicic"
        ],
        "abstract": "The importance of algorithm portfolio techniques for SAT has long been noted, and a number of very successful systems have been devised, including the most successful one --- SATzilla. However, all these systems are quite complex (to understand, reimplement, or modify). In this paper we propose a new algorithm portfolio for SAT that is extremely simple, but in the same time so efficient that it outperforms SATzilla. For a new SAT instance to be solved, our portfolio finds its k-nearest neighbors from the training set and invokes a solver that performs the best at those instances. The main distinguishing feature of our algorithm portfolio is the locality of the selection procedure --- the selection of a SAT solver is based only on few instances similar to the input one.\n    ",
        "submission_date": "2011-07-01T00:00:00",
        "last_modified_date": "2011-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0434",
        "title": "Abstraction Super-structuring Normal Forms: Towards a Theory of Structural Induction",
        "authors": [
            "Adrian Silvescu",
            "Vasant Honavar"
        ],
        "abstract": "Induction is the process by which we obtain predictive laws or theories or models of the world. We consider the structural aspect of induction. We answer the question as to whether we can find a finite and minmalistic set of operations on structural elements in terms of which any theory can be expressed. We identify abstraction (grouping similar entities) and super-structuring (combining topologically e.g., spatio-temporally close entities) as the essential structural operations in the induction process. We show that only two more structural operations, namely, reverse abstraction and reverse super-structuring (the duals of abstraction and super-structuring respectively) suffice in order to exploit the full power of Turing-equivalent generative grammars in induction. We explore the implications of this theorem with respect to the nature of hidden variables, radical positivism and the 2-century old claim of David Hume about the principles of connexion among ideas.\n    ",
        "submission_date": "2011-07-03T00:00:00",
        "last_modified_date": "2011-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.1020",
        "title": "A Novel Multicriteria Group Decision Making Approach With Intuitionistic Fuzzy SIR Method",
        "authors": [
            "Junyi Chai",
            "James N.K. Liu"
        ],
        "abstract": "The superiority and inferiority ranking (SIR) method is a generation of the well-known PROMETHEE method, which can be more efficient to deal with multi-criterion decision making (MCDM) problem. Intuitionistic fuzzy sets (IFSs), as an important extension of fuzzy sets (IFs), include both membership functions and non-membership functions and can be used to, more precisely describe uncertain information. In real world, decision situations are usually under uncertain environment and involve multiple individuals who have their own points of view on handing of decision problems. In order to solve uncertainty group MCDM problem, we propose a novel intuitionistic fuzzy SIR method in this paper. This approach uses intuitionistic fuzzy aggregation operators and SIR ranking methods to handle uncertain information; integrate individual opinions into group opinions; make decisions on multiple-criterion; and finally structure a specific decision map. The proposed approach is illustrated in a simulation of group decision making problem related to supply chain management.\n    ",
        "submission_date": "2011-07-06T00:00:00",
        "last_modified_date": "2011-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.1322",
        "title": "Text Classification: A Sequential Reading Approach",
        "authors": [
            "Gabriel Dulac-Arnold",
            "Ludovic Denoyer",
            "Patrick Gallinari"
        ],
        "abstract": "We propose to model the text classification process as a sequential decision process. In this process, an agent learns to classify documents into topics while reading the document sentences sequentially and learns to stop as soon as enough information was read for deciding. The proposed algorithm is based on a modelisation of Text Classification as a Markov Decision Process and learns by using Reinforcement Learning. Experiments on four different classical mono-label corpora show that the proposed approach performs comparably to classical SVM approaches for large training sets, and better for small training sets. In addition, the model automatically adapts its reading process to the quantity of training information provided.\n    ",
        "submission_date": "2011-07-07T00:00:00",
        "last_modified_date": "2011-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.1686",
        "title": "Proceedings of the Doctoral Consortium and Poster Session of the 5th International Symposium on Rules (RuleML 2011@IJCAI)",
        "authors": [
            "Carlos Viegas Dam\u00e1sio",
            "Alun Preece",
            "Umberto Straccia"
        ],
        "abstract": "This volume contains the papers presented at the first edition of the Doctoral Consortium of the 5th International Symposium on Rules (RuleML 2011@IJCAI) held on July 19th, 2011 in Barcelona, as well as the poster session papers of the RuleML 2011@IJCAI main conference.\n    ",
        "submission_date": "2011-07-08T00:00:00",
        "last_modified_date": "2011-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.1950",
        "title": "Knowledge Embedding and Retrieval Strategies in an Informledge System",
        "authors": [
            "Dr T.R. Gopalakrishnan Nair",
            "Meenakshi Malhotra"
        ],
        "abstract": "Informledge System (ILS) is a knowledge network with autonomous nodes and intelligent links that integrate and structure the pieces of knowledge. In this paper, we put forward the strategies for knowledge embedding and retrieval in an ILS. ILS is a powerful knowledge network system dealing with logical storage and connectivity of information units to form knowledge using autonomous nodes and multi-lateral links. In ILS, the autonomous nodes known as Knowledge Network Nodes (KNN)s play vital roles which are not only used in storage, parsing and in forming the multi-lateral linkages between knowledge points but also in helping the realization of intelligent retrieval of linked information units in the form of knowledge. Knowledge built in to the ILS forms the shape of sphere. The intelligence incorporated into the links of a KNN helps in retrieving various knowledge threads from a specific set of KNNs. A developed entity of information realized through KNN forms in to the shape of a knowledge cone\n    ",
        "submission_date": "2011-07-11T00:00:00",
        "last_modified_date": "2011-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2086",
        "title": "Extend Commitment Protocols with Temporal Regulations: Why and How",
        "authors": [
            "Elisa Marengo",
            "Matteo Baldoni",
            "Cristina Baroglio"
        ],
        "abstract": "The proposal of Elisa Marengo's thesis is to extend commitment protocols to explicitly account for temporal regulations. This extension will satisfy two needs: (1) it will allow representing, in a flexible and modular way, temporal regulations with a normative force, posed on the interaction, so as to represent conventions, laws and suchlike; (2) it will allow committing to complex conditions, which describe not only what will be achieved but to some extent also how. These two aspects will be deeply investigated in the proposal of a unified framework, which is part of the ongoing work and will be included in the thesis.\n    ",
        "submission_date": "2011-07-11T00:00:00",
        "last_modified_date": "2011-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2087",
        "title": "Rule-Based Semantic Sensing",
        "authors": [
            "Przemyslaw Woznowski",
            "Alun Preece"
        ],
        "abstract": "Rule-Based Systems have been in use for decades to solve a variety of problems but not in the sensor informatics domain. Rules aid the aggregation of low-level sensor readings to form a more complete picture of the real world and help to address 10 identified challenges for sensor network middleware. This paper presents the reader with an overview of a system architecture and a pilot application to demonstrate the usefulness of a system integrating rules with sensor middleware.\n    ",
        "submission_date": "2011-07-11T00:00:00",
        "last_modified_date": "2011-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2088",
        "title": "Advancing Multi-Context Systems by Inconsistency Management",
        "authors": [
            "Antonius Weinzierl"
        ],
        "abstract": "Multi-Context Systems are an expressive formalism to model (possibly) non-monotonic information exchange between heterogeneous knowledge bases. Such information exchange, however, often comes with unforseen side-effects leading to violation of constraints, making the system inconsistent, and thus unusable. Although there are many approaches to assess and repair a single inconsistent knowledge base, the heterogeneous nature of Multi-Context Systems poses problems which have not yet been addressed in a satisfying way: How to identify and explain a inconsistency that spreads over multiple knowledge bases with different logical formalisms (e.g., logic programs and ontologies)? What are the causes of inconsistency if inference/information exchange is non-monotonic (e.g., absent information as cause)? How to deal with inconsistency if access to knowledge bases is restricted (e.g., companies exchange information, but do not allow arbitrary modifications to their knowledge bases)? Many traditional approaches solely aim for a consistent system, but automatic removal of inconsistency is not always desireable. Therefore a human operator has to be supported in finding the erroneous parts contributing to the inconsistency. In my thesis those issues will be adressed mainly from a foundational perspective, while our research project also provides algorithms and prototype implementations.\n    ",
        "submission_date": "2011-07-11T00:00:00",
        "last_modified_date": "2011-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2089",
        "title": "Rule-based query answering method for a knowledge base of economic crimes",
        "authors": [
            "Jaroslaw Bak"
        ],
        "abstract": "We present a description of the PhD thesis which aims to propose a rule-based query answering method for relational data. In this approach we use an additional knowledge which is represented as a set of rules and describes the source data at concept (ontological) level. Queries are posed in the terms of abstract level. We present two methods. The first one uses hybrid reasoning and the second one exploits only forward chaining. These two methods are demonstrated by the prototypical implementation of the system coupled with the Jess engine. Tests are performed on the knowledge base of the selected economic crimes: fraudulent disbursement and money laundering.\n    ",
        "submission_date": "2011-07-11T00:00:00",
        "last_modified_date": "2011-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2090",
        "title": "Semantic-ontological combination of Business Rules and Business Processes in IT Service Management",
        "authors": [
            "Alexander Sellner",
            "Christopher Schwarz",
            "Erwin Zinser"
        ],
        "abstract": "IT Service Management deals with managing a broad range of items related to complex system environments. As there is both, a close connection to business interests and IT infrastructure, the application of semantic expressions which are seamlessly integrated within applications for managing ITSM environments, can help to improve transparency and profitability. This paper focuses on the challenges regarding the integration of semantics and ontologies within ITSM environments. It will describe the paradigm of relationships and inheritance within complex service trees and will present an approach of ontologically expressing them. Furthermore, the application of SBVR-based rules as executable SQL triggers will be discussed. Finally, the broad range of topics for further research, derived from the findings, will be presented.\n    ",
        "submission_date": "2011-07-11T00:00:00",
        "last_modified_date": "2011-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2997",
        "title": "An Ontology-driven Framework for Supporting Complex Decision Process",
        "authors": [
            "Junyi Chai",
            "James N.K. Liu"
        ],
        "abstract": "The study proposes a framework of ONTOlogy-based Group Decision Support System (ONTOGDSS) for decision process which exhibits the complex structure of decision-problem and decision-group. It is capable of reducing the complexity of problem structure and group relations. The system allows decision makers to participate in group decision-making through the web environment, via the ontology relation. It facilitates the management of decision process as a whole, from criteria generation, alternative evaluation, and opinion interaction to decision aggregation. The embedded ontology structure in ONTOGDSS provides the important formal description features to facilitate decision analysis and verification. It examines the software architecture, the selection methods, the decision path, etc. Finally, the ontology application of this system is illustrated with specific real case to demonstrate its potentials towards decision-making development.\n    ",
        "submission_date": "2011-07-15T00:00:00",
        "last_modified_date": "2011-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.3298",
        "title": "From decision to action : intentionality, a guide for the specification of intelligent agents' behaviour",
        "authors": [
            "Pierre De Loor",
            "Favier Pierre-Alexandre"
        ],
        "abstract": "This article introduces a reflexion about behavioural specification for interactive and participative agent-based simulation in virtual reality. Within this context, it is neces sary to reach a high level of expressivness in order to enforce interactions between the designer and the behavioural model during the in-line prototyping. This requires to consider the need of semantic very early in the design process. The Intentional agent model is here exposed as a possible answer. It relies on a mixed imperative and declarative approach which focuses on the link between decision and action. The design of a tool able to simulate virtual environment implying agents based on this model is discuss\n    ",
        "submission_date": "2011-07-17T00:00:00",
        "last_modified_date": "2011-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.3302",
        "title": "A Temporal Neuro-Fuzzy Monitoring System to Manufacturing Systems",
        "authors": [
            "Rafik Mahdaoui",
            "Leila Hayet Mouss",
            "Mohamed Djamel Mouss",
            "Ouahiba Chouhal"
        ],
        "abstract": "Fault diagnosis and failure prognosis are essential techniques in improving the safety of many manufacturing systems. Therefore, on-line fault detection and isolation is one of the most important tasks in safety-critical and intelligent control systems. Computational intelligence techniques are being investigated as extension of the traditional fault diagnosis methods. This paper discusses the Temporal Neuro-Fuzzy Systems (TNFS) fault diagnosis within an application study of a manufacturing system. The key issues of finding a suitable structure for detecting and isolating ten realistic actuator faults are described. Within this framework, data-processing interactive software of simulation baptized NEFDIAG (NEuro Fuzzy DIAGnosis) version 1.0 is developed.\n",
        "submission_date": "2011-07-17T00:00:00",
        "last_modified_date": "2011-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.3326",
        "title": "Real-time retrieval for case-based reasoning in interactive multiagent-based simulations",
        "authors": [
            "Pierre De Loor",
            "Romain B\u00e9nard",
            "Chevaillier Pierre"
        ],
        "abstract": "The aim of this paper is to present the principles and results about case-based reasoning adapted to real- time interactive simulations, more precisely concerning retrieval mechanisms. The article begins by introducing the constraints involved in interactive multiagent-based simulations. The second section pre- sents a framework stemming from case-based reasoning by autonomous agents. Each agent uses a case base of local situations and, from this base, it can choose an action in order to interact with other auton- omous agents or users' avatars. We illustrate this framework with an example dedicated to the study of dynamic situations in football. We then go on to address the difficulties of conducting such simulations in real-time and propose a model for case and for case base. Using generic agents and adequate case base structure associated with a dedicated recall algorithm, we improve retrieval performance under time pressure compared to classic CBR techniques. We present some results relating to the performance of this solution. The article concludes by outlining future development of our project.\n    ",
        "submission_date": "2011-07-17T00:00:00",
        "last_modified_date": "2011-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.3663",
        "title": "Towards Open-Text Semantic Parsing via Multi-Task Learning of Structured Embeddings",
        "authors": [
            "Antoine Bordes",
            "Xavier Glorot",
            "Jason Weston",
            "Yoshua Bengio"
        ],
        "abstract": "Open-text (or open-domain) semantic parsers are designed to interpret any statement in natural language by inferring a corresponding meaning representation (MR). Unfortunately, large scale systems cannot be easily machine-learned due to lack of directly supervised data. We propose here a method that learns to assign MRs to a wide range of text (using a dictionary of more than 70,000 words, which are mapped to more than 40,000 entities) thanks to a training scheme that combines learning from WordNet and ConceptNet with learning from raw text. The model learns structured embeddings of words, entities and MRs via a multi-task training process operating on these diverse sources of data that integrates all the learnt knowledge into a single system. This work ends up combining methods for knowledge acquisition, semantic parsing, and word-sense disambiguation. Experiments on various tasks indicate that our approach is indeed successful and can form a basis for future more sophisticated systems.\n    ",
        "submission_date": "2011-07-19T00:00:00",
        "last_modified_date": "2011-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.3765",
        "title": "Using Variational Inference and MapReduce to Scale Topic Modeling",
        "authors": [
            "Ke Zhai",
            "Jordan Boyd-Graber",
            "Nima Asadi"
        ],
        "abstract": "Latent Dirichlet Allocation (LDA) is a popular topic modeling technique for exploring document collections. Because of the increasing prevalence of large datasets, there is a need to improve the scalability of inference of LDA. In this paper, we propose a technique called ~\\emph{MapReduce LDA} (Mr. LDA) to accommodate very large corpus collections in the MapReduce framework. In contrast to other techniques to scale inference for LDA, which use Gibbs sampling, we use variational inference. Our solution efficiently distributes computation and is relatively simple to implement. More importantly, this variational implementation, unlike highly tuned and specialized implementations, is easily extensible. We demonstrate two extensions of the model possible with this scalable framework: informed priors to guide topic discovery and modeling topics from a multilingual corpus.\n    ",
        "submission_date": "2011-07-19T00:00:00",
        "last_modified_date": "2011-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.3894",
        "title": "Online Anomaly Detection Systems Using Incremental Commute Time",
        "authors": [
            "Nguyen Lu Dang Khoa",
            "Sanjay Chawla"
        ],
        "abstract": "Commute Time Distance (CTD) is a random walk based metric on graphs. CTD has found widespread applications in many domains including personalized search, collaborative filtering and making search engines robust against manipulation. Our interest is inspired by the use of CTD as a metric for anomaly detection. It has been shown that CTD can be used to simultaneously identify both global and local anomalies. Here we propose an accurate and efficient approximation for computing the CTD in an incremental fashion in order to facilitate real-time applications. An online anomaly detection algorithm is designed where the CTD of each new arriving data point to any point in the current graph can be estimated in constant time ensuring a real-time response. Moreover, the proposed approach can also be applied in many other applications that utilize commute time distance.\n    ",
        "submission_date": "2011-07-20T00:00:00",
        "last_modified_date": "2011-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4035",
        "title": "Towards Completely Lifted Search-based Probabilistic Inference",
        "authors": [
            "David Poole",
            "Fahiem Bacchus",
            "Jacek Kisynski"
        ],
        "abstract": "The promise of lifted probabilistic inference is to carry out probabilistic inference in a relational probabilistic model without needing to reason about each individual separately (grounding out the representation) by treating the undistinguished individuals as a block. Current exact methods still need to ground out in some cases, typically because the representation of the intermediate results is not closed under the lifted operations. We set out to answer the question as to whether there is some fundamental reason why lifted algorithms would need to ground out undifferentiated individuals. We have two main results: (1) We completely characterize the cases where grounding is polynomial in a population size, and show how we can do lifted inference in time polynomial in the logarithm of the population size for these cases. (2) For the case of no-argument and single-argument parametrized random variables where the grounding is not polynomial in a population size, we present lifted inference which is polynomial in the population size whereas grounding is exponential. Neither of these cases requires reasoning separately about the individuals that are not explicitly mentioned.\n    ",
        "submission_date": "2011-07-20T00:00:00",
        "last_modified_date": "2011-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4057",
        "title": "The Harmonic Theory; A mathematical framework to build intelligent contextual and adaptive computing, cognition and sensory system",
        "authors": [
            "Nick Mehrdad Loghmani"
        ],
        "abstract": "Harmonic theory provides a mathematical framework to describe the structure, behavior, evolution and emergence of harmonic systems. A harmonic system is context aware, contains elements that manifest characteristics either collaboratively or independently according to system's expression and can interact with its environment. This theory provides a fresh way to analyze emergence and collaboration of \"ad-hoc\" and complex systems.\n    ",
        "submission_date": "2011-07-20T00:00:00",
        "last_modified_date": "2011-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4161",
        "title": "Local Optima Networks of the Quadratic Assignment Problem",
        "authors": [
            "Fabio Daolio",
            "S\u00e9bastien Verel",
            "Gabriela Ochoa",
            "Marco Tomassini"
        ],
        "abstract": "Using a recently proposed model for combinatorial landscapes, Local Optima Networks (LON), we conduct a thorough analysis of two types of instances of the Quadratic Assignment Problem (QAP). This network model is a reduction of the landscape in which the nodes correspond to the local optima, and the edges account for the notion of adjacency between their basins of attraction. The model was inspired by the notion of 'inherent network' of potential energy surfaces proposed in physical-chemistry. The local optima networks extracted from the so called uniform and real-like QAP instances, show features clearly distinguishing these two types of instances. Apart from a clear confirmation that the search difficulty increases with the problem dimension, the analysis provides new confirming evidence explaining why the real-like instances are easier to solve exactly using heuristic search, while the uniform instances are easier to solve approximately. Although the local optima network model is still under development, we argue that it provides a novel view of combinatorial landscapes, opening up the possibilities for new analytical tools and understanding of problem difficulty in combinatorial optimization.\n    ",
        "submission_date": "2011-07-21T00:00:00",
        "last_modified_date": "2011-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4162",
        "title": "Local Optima Networks of NK Landscapes with Neutrality",
        "authors": [
            "S\u00e9bastien Verel",
            "Gabriela Ochoa",
            "Marco Tomassini"
        ],
        "abstract": "In previous work we have introduced a network-based model that abstracts many details of the underlying landscape and compresses the landscape information into a weighted, oriented graph which we call the local optima network. The vertices of this graph are the local optima of the given fitness landscape, while the arcs are transition probabilities between local optima basins. Here we extend this formalism to neutral fitness landscapes, which are common in difficult combinatorial search spaces. By using two known neutral variants of the NK family (i.e. NKp and NKq) in which the amount of neutrality can be tuned by a parameter, we show that our new definitions of the optima networks and the associated basins are consistent with the previous definitions for the non-neutral case. Moreover, our empirical study and statistical analysis show that the features of neutral landscapes interpolate smoothly between landscapes with maximum neutrality and non-neutral ones. We found some unknown structural differences between the two studied families of neutral landscapes. But overall, the network features studied confirmed that neutrality, in landscapes with percolating neutral networks, may enhance heuristic search. Our current methodology requires the exhaustive enumeration of the underlying search space. Therefore, sampling techniques should be developed before this analysis can have practical implications. We argue, however, that the proposed model offers a new perspective into the problem difficulty of combinatorial optimization problems and may inspire the design of more effective search heuristics.\n    ",
        "submission_date": "2011-07-21T00:00:00",
        "last_modified_date": "2011-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4163",
        "title": "Centric selection: a way to tune the exploration/exploitation trade-off",
        "authors": [
            "David Simoncini",
            "S\u00e9bastien Verel",
            "Philippe Collard",
            "Manuel Clergue"
        ],
        "abstract": "In this paper, we study the exploration / exploitation trade-off in cellular genetic algorithms. We define a new selection scheme, the centric selection, which is tunable and allows controlling the selective pressure with a single parameter. The equilibrium model is used to study the influence of the centric selection on the selective pressure and a new model which takes into account problem dependent statistics and selective pressure in order to deal with the exploration / exploitation trade-off is proposed: the punctuated equilibria model. Performances on the quadratic assignment problem and NK-Landscapes put in evidence an optimal exploration / exploitation trade-off on both of the classes of problems. The punctuated equilibria model is used to explain these results.\n    ",
        "submission_date": "2011-07-21T00:00:00",
        "last_modified_date": "2011-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4164",
        "title": "NK landscapes difficulty and Negative Slope Coefficient: How Sampling Influences the Results",
        "authors": [
            "Leonardo Vanneschi",
            "S\u00e9bastien Verel",
            "Philippe Collard",
            "Marco Tomassini"
        ],
        "abstract": "Negative Slope Coefficient is an indicator of problem hardness that has been introduced in 2004 and that has returned promising results on a large set of problems. It is based on the concept of fitness cloud and works by partitioning the cloud into a number of bins representing as many different regions of the fitness landscape. The measure is calculated by joining the bins centroids by segments and summing all their negative slopes. In this paper, for the first time, we point out a potential problem of the Negative Slope Coefficient: we study its value for different instances of the well known NK-landscapes and we show how this indicator is dramatically influenced by the minimum number of points contained into a bin. Successively, we formally justify this behavior of the Negative Slope Coefficient and we discuss pros and cons of this measure.\n    ",
        "submission_date": "2011-07-21T00:00:00",
        "last_modified_date": "2011-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4303",
        "title": "Interactive ontology debugging: two query strategies for efficient fault localization",
        "authors": [
            "Kostyantyn Shchekotykhin",
            "Gerhard Friedrich",
            "Philipp Fleiss",
            "Patrick Rodler"
        ],
        "abstract": "Effective debugging of ontologies is an important prerequisite for their broad application, especially in areas that rely on everyday users to create and maintain knowledge bases, such as the Semantic Web. In such systems ontologies capture formalized vocabularies of terms shared by its users. However in many cases users have different local views of the domain, i.e. of the context in which a given term is used. Inappropriate usage of terms together with natural complications when formulating and understanding logical descriptions may result in faulty ontologies. Recent ontology debugging approaches use diagnosis methods to identify causes of the faults. In most debugging scenarios these methods return many alternative diagnoses, thus placing the burden of fault localization on the user. This paper demonstrates how the target diagnosis can be identified by performing a sequence of observations, that is, by querying an oracle about entailments of the target ontology. To identify the best query we propose two query selection strategies: a simple \"split-in-half\" strategy and an entropy-based strategy. The latter allows knowledge about typical user errors to be exploited to minimize the number of queries. Our evaluation showed that the entropy-based method significantly reduces the number of required queries compared to the \"split-in-half\" approach. We experimented with different probability distributions of user errors and different qualities of the a-priori probabilities. Our measurements demonstrated the superiority of entropy-based query selection even in cases where all fault probabilities are equal, i.e. where no information about typical user errors is available.\n    ",
        "submission_date": "2011-07-20T00:00:00",
        "last_modified_date": "2014-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4502",
        "title": "MeLinDa: an interlinking framework for the web of data",
        "authors": [
            "Fran\u00e7ois Scharffe",
            "J\u00e9r\u00f4me Euzenat"
        ],
        "abstract": "The web of data consists of data published on the web in such a way that they can be interpreted and connected together. It is thus critical to establish links between these data, both for the web of data and for the semantic web that it contributes to feed. We consider here the various techniques developed for that purpose and analyze their commonalities and differences. We propose a general framework and show how the diverse techniques fit in the framework. From this framework we consider the relation between data interlinking and ontology matching. Although, they can be considered similar at a certain level (they both relate formal entities), they serve different purposes, but would find a mutual benefit at collaborating. We thus present a scheme under which it is possible for data linking tools to take advantage of ontology alignments.\n    ",
        "submission_date": "2011-07-22T00:00:00",
        "last_modified_date": "2011-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4553",
        "title": "Solving Linear Constraints in Elementary Abelian p-Groups of Symmetries",
        "authors": [
            "Thierry Boy de la Tour",
            "Mnacho Echenim"
        ],
        "abstract": "Symmetries occur naturally in CSP or SAT problems and are not very difficult to discover, but using them to prune the search space tends to be very challenging. Indeed, this usually requires finding specific elements in a group of symmetries that can be huge, and the problem of their very existence is NP-hard. We formulate such an existence problem as a constraint problem on one variable (the symmetry to be used) ranging over a group, and try to find restrictions that may be solved in polynomial time. By considering a simple form of constraints (restricted by a cardinality k) and the class of groups that have the structure of Fp-vector spaces, we propose a partial algorithm based on linear algebra. This polynomial algorithm always applies when k=p=2, but may fail otherwise as we prove the problem to be NP-hard for all other values of k and p. Experiments show that this approach though restricted should allow for an efficient use of at least some groups of symmetries. We conclude with a few directions to be explored to efficiently solve this problem on the general case.\n    ",
        "submission_date": "2011-07-22T00:00:00",
        "last_modified_date": "2011-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4573",
        "title": "Analogy perception applied to seven tests of word comprehension",
        "authors": [
            "Peter D. Turney"
        ],
        "abstract": "It has been argued that analogy is the core of cognition. In AI research, algorithms for analogy are often limited by the need for hand-coded high-level representations as input. An alternative approach is to use high-level perception, in which high-level representations are automatically generated from raw data. Analogy perception is the process of recognizing analogies using high-level perception. We present PairClass, an algorithm for analogy perception that recognizes lexical proportional analogies using representations that are automatically generated from a large corpus of raw textual data. A proportional analogy is an analogy of the form A:B::C:D, meaning \"A is to B as C is to D\". A lexical proportional analogy is a proportional analogy with words, such as carpenter:wood::mason:stone. PairClass represents the semantic relations between two words using a high-dimensional feature vector, in which the elements are based on frequencies of patterns in the corpus. PairClass recognizes analogies by applying standard supervised machine learning techniques to the feature vectors. We show how seven different tests of word comprehension can be framed as problems of analogy perception and we then apply PairClass to the seven resulting sets of analogy perception problems. We achieve competitive results on all seven tests. This is the first time a uniform approach has handled such a range of tests of word comprehension.\n    ",
        "submission_date": "2011-07-22T00:00:00",
        "last_modified_date": "2011-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4747",
        "title": "The PITA System: Tabling and Answer Subsumption for Reasoning under Uncertainty",
        "authors": [
            "Fabrizio Riguzzi",
            "Terrance Swift"
        ],
        "abstract": "Many real world domains require the representation of a measure of uncertainty. The most common such representation is probability, and the combination of probability with logic programs has given rise to the field of Probabilistic Logic Programming (PLP), leading to languages such as the Independent Choice Logic, Logic Programs with Annotated Disjunctions (LPADs), Problog, PRISM and others. These languages share a similar distribution semantics, and methods have been devised to translate programs between these languages. The complexity of computing the probability of queries to these general PLP programs is very high due to the need to combine the probabilities of explanations that may not be exclusive. As one alternative, the PRISM system reduces the complexity of query answering by restricting the form of programs it can evaluate. As an entirely different alternative, Possibilistic Logic Programs adopt a simpler metric of uncertainty than probability. Each of these approaches -- general PLP, restricted PLP, and Possibilistic Logic Programming -- can be useful in different domains depending on the form of uncertainty to be represented, on the form of programs needed to model problems, and on the scale of the problems to be solved. In this paper, we show how the PITA system, which originally supported the general PLP language of LPADs, can also efficiently support restricted PLP and Possibilistic Logic Programs. PITA relies on tabling with answer subsumption and consists of a transformation along with an API for library functions that interface with answer subsumption.\n    ",
        "submission_date": "2011-07-24T00:00:00",
        "last_modified_date": "2011-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4865",
        "title": "Actual Causation in CP-logic",
        "authors": [
            "Joost Vennekens"
        ],
        "abstract": "Given a causal model of some domain and a particular story that has taken place in this domain, the problem of actual causation is deciding which of the possible causes for some effect actually caused it. One of the most influential approaches to this problem has been developed by Halpern and Pearl in the context of structural models. In this paper, I argue that this is actually not the best setting for studying this problem. As an alternative, I offer the probabilistic logic programming language of CP-logic. Unlike structural models, CP-logic incorporates the deviant/default distinction that is generally considered an important aspect of actual causation, and it has an explicitly dynamic semantics, which helps to formalize the stories that serve as input to an actual causation problem.\n    ",
        "submission_date": "2011-07-25T00:00:00",
        "last_modified_date": "2011-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4937",
        "title": "Instantiation Schemes for Nested Theories",
        "authors": [
            "Mnacho Echenim",
            "Nicolas Peltier"
        ],
        "abstract": "This paper investigates under which conditions instantiation-based proof procedures can be combined in a nested way, in order to mechanically construct new instantiation procedures for richer theories. Interesting applications in the field of verification are emphasized, particularly for handling extensions of the theory of arrays.\n    ",
        "submission_date": "2011-07-25T00:00:00",
        "last_modified_date": "2011-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4966",
        "title": "Lifted Graphical Models: A Survey",
        "authors": [
            "Lilyana Mihalkova",
            "Lise Getoor"
        ],
        "abstract": "This article presents a survey of work on lifted graphical models. We review a general form for a lifted graphical model, a par-factor graph, and show how a number of existing statistical relational representations map to this formalism. We discuss inference algorithms, including lifted inference algorithms, that efficiently compute the answers to probabilistic queries. We also review work in learning lifted graphical models from data. It is our belief that the need for statistical relational models (whether it goes by that name or another) will grow in the coming decades, as we are inundated with data which is a mix of structured and unstructured, with entities and relations extracted in a noisy manner from text, and with the need to reason effectively with this data. We hope that this synthesis of ideas from many different research groups will provide an accessible starting point for new researchers in this expanding field.\n    ",
        "submission_date": "2011-07-25T00:00:00",
        "last_modified_date": "2011-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.5462",
        "title": "HyFlex: A Benchmark Framework for Cross-domain Heuristic Search",
        "authors": [
            "Edmund Burke",
            "Tim Curtois",
            "Matthew Hyde",
            "Gabriela Ochoa",
            "Jose A. Vazquez-Rodriguez"
        ],
        "abstract": "Automating the design of heuristic search methods is an active research field within computer science, artificial intelligence and operational research. In order to make these methods more generally applicable, it is important to eliminate or reduce the role of the human expert in the process of designing an effective methodology to solve a given computational search problem. Researchers developing such methodologies are often constrained on the number of problem domains on which to test their adaptive, self-configuring algorithms; which can be explained by the inherent difficulty of implementing their corresponding domain specific software components.\n",
        "submission_date": "2011-07-27T00:00:00",
        "last_modified_date": "2011-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.5474",
        "title": "Selecting Attributes for Sport Forecasting using Formal Concept Analysis",
        "authors": [
            "Gonzalo A. Aranda-Corral",
            "Joaqu\u00edn Borrego-D\u00edaz",
            "Juan Gal\u00e1n-P\u00e1ez"
        ],
        "abstract": "In order to address complex systems, apply pattern recongnition on their evolution could play an key role to understand their dynamics. Global patterns are required to detect emergent concepts and trends, some of them with qualitative nature. Formal Concept Analysis (FCA) is a theory whose goal is to discover and to extract Knowledge from qualitative data. It provides tools for reasoning with implication basis (and association rules). Implications and association rules are usefull to reasoning on previously selected attributes, providing a formal foundation for logical reasoning. In this paper we analyse how to apply FCA reasoning to increase confidence in sports betting, by means of detecting temporal regularities from data. It is applied to build a Knowledge-Based system for confidence reasoning.\n    ",
        "submission_date": "2011-07-27T00:00:00",
        "last_modified_date": "2011-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.5528",
        "title": "Time Consistent Discounting",
        "authors": [
            "Tor Lattimore",
            "Marcus Hutter"
        ],
        "abstract": "A possibly immortal agent tries to maximise its summed discounted rewards over time, where discounting is used to avoid infinite utilities and encourage the agent to value current rewards more than future ones. Some commonly used discount functions lead to time-inconsistent behavior where the agent changes its plan over time. These inconsistencies can lead to very poor behavior. We generalise the usual discounted utility model to one where the discount function changes with the age of the agent. We then give a simple characterisation of time-(in)consistent discount functions and show the existence of a rational policy for an agent that knows its discount function is time-inconsistent.\n    ",
        "submission_date": "2011-07-27T00:00:00",
        "last_modified_date": "2011-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.5537",
        "title": "Asymptotically Optimal Agents",
        "authors": [
            "Tor Lattimore",
            "Marcus Hutter"
        ],
        "abstract": "Artificial general intelligence aims to create agents capable of learning to solve arbitrary interesting problems. We define two versions of asymptotic optimality and prove that no agent can satisfy the strong version while in some cases, depending on discounting, there does exist a non-computable weak asymptotically optimal agent.\n    ",
        "submission_date": "2011-07-27T00:00:00",
        "last_modified_date": "2011-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.5766",
        "title": "Information, Utility & Bounded Rationality",
        "authors": [
            "Pedro A. Ortega",
            "Daniel A. Braun"
        ],
        "abstract": "Perfectly rational decision-makers maximize expected utility, but crucially ignore the resource costs incurred when determining optimal actions. Here we employ an axiomatic framework for bounded rational decision-making based on a thermodynamic interpretation of resource costs as information costs. This leads to a variational \"free utility\" principle akin to thermodynamical free energy that trades off utility and information costs. We show that bounded optimal control solutions can be derived from this variational principle, which leads in general to stochastic policies. Furthermore, we show that risk-sensitive and robust (minimax) control schemes fall out naturally from this framework if the environment is considered as a bounded rational and perfectly rational opponent, respectively. When resource costs are ignored, the maximum expected utility principle is recovered.\n    ",
        "submission_date": "2011-07-28T00:00:00",
        "last_modified_date": "2011-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.5930",
        "title": "Technical Note: Towards ROC Curves in Cost Space",
        "authors": [
            "Jos\u00e9 Hern\u00e1ndez-Orallo",
            "Peter Flach",
            "C\u00e8sar Ferri"
        ],
        "abstract": "ROC curves and cost curves are two popular ways of visualising classifier performance, finding appropriate thresholds according to the operating condition, and deriving useful aggregated measures such as the area under the ROC curve (AUC) or the area under the optimal cost curve. In this note we present some new findings and connections between ROC space and cost space, by using the expected loss over a range of operating conditions. In particular, we show that ROC curves can be transferred to cost space by means of a very natural way of understanding how thresholds should be chosen, by selecting the threshold such that the proportion of positive predictions equals the operating condition (either in the form of cost proportion or skew). We call these new curves {ROC Cost Curves}, and we demonstrate that the expected loss as measured by the area under these curves is linearly related to AUC. This opens up a series of new possibilities and clarifies the notion of cost curve and its relation to ROC analysis. In addition, we show that for a classifier that assigns the scores in an evenly-spaced way, these curves are equal to the Brier Curves. As a result, this establishes the first clear connection between AUC and the Brier score.\n    ",
        "submission_date": "2011-07-29T00:00:00",
        "last_modified_date": "2011-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.0039",
        "title": "CBR with Commonsense Reasoning and Structure Mapping: An Application to Mediation",
        "authors": [
            "Atilim Gunes Baydin",
            "Ramon Lopez de Mantaras",
            "Simeon Simoff",
            "Carles Sierra"
        ],
        "abstract": "Mediation is an important method in dispute resolution. We implement a case based reasoning approach to mediation integrating analogical and commonsense reasoning components that allow an artificial mediation agent to satisfy requirements expected from a human mediator, in particular: utilizing experience with cases in different domains; and structurally transforming the set of issues for a better solution. We utilize a case structure based on ontologies reflecting the perceptions of the parties in dispute. The analogical reasoning component, employing the Structure Mapping Theory from psychology, provides a flexibility to respond innovatively in unusual circumstances, in contrast with conventional approaches confined into specialized problem domains. We aim to build a mediation case base incorporating real world instances ranging from interpersonal or intergroup disputes to international conflicts.\n    ",
        "submission_date": "2011-07-30T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.0155",
        "title": "Reasoning in the OWL 2 Full Ontology Language using First-Order Automated Theorem Proving",
        "authors": [
            "Michael Schneider",
            "Geoff Sutcliffe"
        ],
        "abstract": "OWL 2 has been standardized by the World Wide Web Consortium (W3C) as a family of ontology languages for the Semantic Web. The most expressive of these languages is OWL 2 Full, but to date no reasoner has been implemented for this language. Consistency and entailment checking are known to be undecidable for OWL 2 Full. We have translated a large fragment of the OWL 2 Full semantics into first-order logic, and used automated theorem proving systems to do reasoning based on this theory. The results are promising, and indicate that this approach can be applied in practice for effective OWL reasoning, beyond the capabilities of current Semantic Web reasoners.\n",
        "submission_date": "2011-07-31T00:00:00",
        "last_modified_date": "2011-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.0294",
        "title": "Scaling Inference for Markov Logic with a Task-Decomposition Approach",
        "authors": [
            "Feng Niu",
            "Ce Zhang",
            "Christopher R\u00e9",
            "Jude Shavlik"
        ],
        "abstract": "Motivated by applications in large-scale knowledge base construction, we study the problem of scaling up a sophisticated statistical inference framework called Markov Logic Networks (MLNs). Our approach, Felix, uses the idea of Lagrangian relaxation from mathematical programming to decompose a program into smaller tasks while preserving the joint-inference property of the original MLN. The advantage is that we can use highly scalable specialized algorithms for common tasks such as classification and coreference. We propose an architecture to support Lagrangian relaxation in an RDBMS which we show enables scalable joint inference for MLNs. We empirically validate that Felix is significantly more scalable and efficient than prior approaches to MLN inference by constructing a knowledge base from 1.8M documents as part of the TAC challenge. We show that Felix scales and achieves state-of-the-art quality numbers. In contrast, prior approaches do not scale even to a subset of the corpus that is three orders of magnitude smaller.\n    ",
        "submission_date": "2011-08-01T00:00:00",
        "last_modified_date": "2012-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.0404",
        "title": "Exploiting Agent and Type Independence in Collaborative Graphical Bayesian Games",
        "authors": [
            "Frans A. Oliehoek",
            "Shimon Whiteson",
            "Matthijs T.J. Spaan"
        ],
        "abstract": "Efficient collaborative decision making is an important challenge for multiagent systems. Finding optimal joint actions is especially challenging when each agent has only imperfect information about the state of its environment. Such problems can be modeled as collaborative Bayesian games in which each agent receives private information in the form of its type. However, representing and solving such games requires space and computation time exponential in the number of agents. This article introduces collaborative graphical Bayesian games (CGBGs), which facilitate more efficient collaborative decision making by decomposing the global payoff function as the sum of local payoff functions that depend on only a few agents. We propose a framework for the efficient solution of CGBGs based on the insight that they posses two different types of independence, which we call agent independence and type independence. In particular, we present a factor graph representation that captures both forms of independence and thus enables efficient solutions. In addition, we show how this representation can provide leverage in sequential tasks by using it to construct a novel method for decentralized partially observable Markov decision processes. Experimental results in both random and benchmark tasks demonstrate the improved scalability of our methods compared to several existing alternatives.\n    ",
        "submission_date": "2011-08-01T00:00:00",
        "last_modified_date": "2014-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.1045",
        "title": "A Data Mining Approach to the Diagnosis of Tuberculosis by Cascading Clustering and Classification",
        "authors": [
            "Asha.T",
            "S. Natarajan",
            "K.N.B. Murthy"
        ],
        "abstract": "In this paper, a methodology for the automated detection and classification of Tuberculosis(TB) is presented. Tuberculosis is a disease caused by mycobacterium which spreads through the air and attacks low immune bodies easily. Our methodology is based on clustering and classification that classifies TB into two categories, Pulmonary Tuberculosis(PTB) and retroviral PTB(RPTB) that is those with Human Immunodeficiency Virus (HIV) infection. Initially K-means clustering is used to group the TB data into two clusters and assigns classes to clusters. Subsequently multiple different classification algorithms are trained on the result set to build the final classifier model based on K-fold cross validation method. This methodology is evaluated using 700 raw TB data obtained from a city hospital. The best obtained accuracy was 98.7% from support vector machine (SVM) compared to other classifiers. The proposed approach helps doctors in their diagnosis decisions and also in their treatment planning procedures for different categories.\n    ",
        "submission_date": "2011-08-04T00:00:00",
        "last_modified_date": "2011-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.1488",
        "title": "'Just Enough' Ontology Engineering",
        "authors": [
            "P. Di Maio"
        ],
        "abstract": "This paper introduces 'just enough' principles and 'systems engineering' approach to the practice of ontology development to provide a minimal yet complete, lightweight, agile and integrated development process, supportive of stakeholder management and implementation independence.\n    ",
        "submission_date": "2011-08-06T00:00:00",
        "last_modified_date": "2011-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.1500",
        "title": "Gender Recognition Based on Sift Features",
        "authors": [
            "Sahar Yousefi",
            "Morteza Zahedi"
        ],
        "abstract": "This paper proposes a robust approach for face detection and gender classification in color images. Previous researches about gender recognition suppose an expensive computational and time-consuming pre-processing step in order to alignment in which face images are aligned so that facial landmarks like eyes, nose, lips, chin are placed in uniform locations in image. In this paper, a novel technique based on mathematical analysis is represented in three stages that eliminates alignment step. First, a new color based face detection method is represented with a better result and more robustness in complex backgrounds. Next, the features which are invariant to affine transformations are extracted from each face using scale invariant feature transform (SIFT) method. To evaluate the performance of the proposed algorithm, experiments have been conducted by employing a SVM classifier on a database of face images which contains 500 images from distinct people with equal ratio of male and female.\n    ",
        "submission_date": "2011-08-06T00:00:00",
        "last_modified_date": "2011-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.1986",
        "title": "A Knowledge Mining Model for Ranking Institutions using Rough Computing with Ordering Rules and Formal Concept analysis",
        "authors": [
            "D. P. Acharjya",
            "L. Ezhilarasi"
        ],
        "abstract": "Emergences of computers and information technological revolution made tremendous changes in the real world and provides a different dimension for the intelligent data analysis. Well formed fact, the information at right time and at right place deploy a better ",
        "submission_date": "2011-08-09T00:00:00",
        "last_modified_date": "2011-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.2096",
        "title": "Reputation-based Incentive Protocols in Crowdsourcing Applications",
        "authors": [
            "Yu Zhang",
            "Mihaela van der Schaar"
        ],
        "abstract": "Crowdsourcing websites (e.g. Yahoo! Answers, Amazon Mechanical Turk, and etc.) emerged in recent years that allow requesters from all around the world to post tasks and seek help from an equally global pool of workers. However, intrinsic incentive problems reside in crowdsourcing applications as workers and requester are selfish and aim to strategically maximize their own benefit. In this paper, we propose to provide incentives for workers to exert effort using a novel game-theoretic model based on repeated games. As there is always a gap in the social welfare between the non-cooperative equilibria emerging when workers pursue their self-interests and the desirable Pareto efficient outcome, we propose a novel class of incentive protocols based on social norms which integrates reputation mechanisms into the existing pricing schemes currently implemented on crowdsourcing websites, in order to improve the performance of the non-cooperative equilibria emerging in such applications. We first formulate the exchanges on a crowdsourcing website as a two-sided market where requesters and workers are matched and play gift-giving games repeatedly. Subsequently, we study the protocol designer's problem of finding an optimal and sustainable (equilibrium) protocol which achieves the highest social welfare for that website. We prove that the proposed incentives protocol can make the website operate close to Pareto efficiency. Moreover, we also examine an alternative scenario, where the protocol designer aims at maximizing the revenue of the website and evaluate the performance of the optimal protocol.\n    ",
        "submission_date": "2011-08-10T00:00:00",
        "last_modified_date": "2011-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.2115",
        "title": "The Ditmarsch Tale of Wonders - The Dynamics of Lying",
        "authors": [
            "Hans van Ditmarsch"
        ],
        "abstract": "We propose a dynamic logic of lying, wherein a 'lie that phi' (where phi is a formula in the logic) is an action in the sense of dynamic modal logic, that is interpreted as a state transformer relative to the formula phi. The states that are being transformed are pointed Kripke models encoding the uncertainty of agents about their beliefs. Lies can be about factual propositions but also about modal formulas, such as the beliefs of other agents or the belief consequences of the lies of other agents. We distinguish (i) an outside observer who is lying to an agent that is modelled in the system, from (ii) one agent who is lying to another agent, and where both are modelled in the system. For either case, we further distinguish (iii) the agent who believes everything that it is told (even at the price of inconsistency), from (iv) the agent who only believes what it is told if that is consistent with its current beliefs, and from (v) the agent who believes everything that it is told by consistently revising its current beliefs. The logics have complete axiomatizations, which can most elegantly be shown by way of their embedding in what is known as action model logic or the extension of that logic to belief revision.\n    ",
        "submission_date": "2011-08-10T00:00:00",
        "last_modified_date": "2012-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.2283",
        "title": "A survey on independence-based Markov networks learning",
        "authors": [
            "Federico Schl\u00fcter"
        ],
        "abstract": "This work reports the most relevant technical aspects in the problem of learning the \\emph{Markov network structure} from data. Such problem has become increasingly important in machine learning, and many other application fields of machine learning. Markov networks, together with Bayesian networks, are probabilistic graphical models, a widely used formalism for handling probability distributions in intelligent systems. Learning graphical models from data have been extensively applied for the case of Bayesian networks, but for Markov networks learning it is not tractable in practice. However, this situation is changing with time, given the exponential growth of computers capacity, the plethora of available digital data, and the researching on new learning technologies. This work stresses on a technology called independence-based learning, which allows the learning of the independence structure of those networks from data in an efficient and sound manner, whenever the dataset is sufficiently large, and data is a representative sampling of the target distribution. In the analysis of such technology, this work surveys the current state-of-the-art algorithms for learning Markov networks structure, discussing its current limitations, and proposing a series of open problems where future works may produce some advances in the area in terms of quality and efficiency. The paper concludes by opening a discussion about how to develop a general formalism for improving the quality of the structures learned, when data is scarce.\n    ",
        "submission_date": "2011-08-10T00:00:00",
        "last_modified_date": "2013-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.2865",
        "title": "Conscious Machines and Consciousness Oriented Programming",
        "authors": [
            "Norbert B\u00e1tfai"
        ],
        "abstract": "In this paper, we investigate the following question: how could you write such computer programs that can work like conscious beings? The motivation behind this question is that we want to create such applications that can see the future. The aim of this paper is to provide an overall conceptual framework for this new approach to machine consciousness. So we introduce a new programming paradigm called Consciousness Oriented Programming (COP).\n    ",
        "submission_date": "2011-08-14T00:00:00",
        "last_modified_date": "2011-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.3019",
        "title": "A First Approach on Modelling Staff Proactiveness in Retail Simulation Models",
        "authors": [
            "Peer-Olaf Siebers",
            "Uwe Aickelin"
        ],
        "abstract": "There has been a noticeable shift in the relative composition of the industry in the developed countries in recent years; manufacturing is decreasing while the service sector is becoming more important. However, currently most simulation models for investigating service systems are still built in the same way as manufacturing simulation models, using a process-oriented world view, i.e. they model the flow of passive entities through a system. These kinds of models allow studying aspects of operational management but are not well suited for studying the dynamics that appear in service systems due to human behaviour. For these kinds of studies we require tools that allow modelling the system and entities using an object-oriented world view, where intelligent objects serve as abstract \"actors\" that are goal directed and can behave proactively. In our work we combine process-oriented discrete event simulation modelling and object-oriented agent based simulation modelling to investigate the impact of people management practices on retail productivity. In this paper, we reveal in a series of experiments what impact considering proactivity can have on the output accuracy of simulation models of human centric systems. The model and data we use for this investigation are based on a case study in a UK department store. We show that considering proactivity positively influences the validity of these kinds of models and therefore allows analysts to make better recommendations regarding strategies to apply people management practises.\n    ",
        "submission_date": "2011-08-15T00:00:00",
        "last_modified_date": "2011-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.3074",
        "title": "Selectivity in Probabilistic Causality: Drawing Arrows from Inputs to Stochastic Outputs",
        "authors": [
            "Ehtibar N. Dzhafarov",
            "Janne V. Kujala"
        ],
        "abstract": "Given a set of several inputs into a system (e.g., independent variables characterizing stimuli) and a set of several stochastically non-independent outputs (e.g., random variables describing different aspects of responses), how can one determine, for each of the outputs, which of the inputs it is influenced by? The problem has applications ranging from modeling pairwise comparisons to reconstructing mental processing architectures to conjoint testing. A necessary and sufficient condition for a given pattern of selective influences is provided by the Joint Distribution Criterion, according to which the problem of \"what influences what\" is equivalent to that of the existence of a joint distribution for a certain set of random variables. For inputs and outputs with finite sets of values this criterion translates into a test of consistency of a certain system of linear equations and inequalities (Linear Feasibility Test) which can be performed by means of linear programming. The Joint Distribution Criterion also leads to a metatheoretical principle for generating a broad class of necessary conditions (tests) for diagrams of selective influences. Among them is the class of distance-type tests based on the observation that certain functionals on jointly distributed random variables satisfy triangle inequality.\n    ",
        "submission_date": "2011-08-15T00:00:00",
        "last_modified_date": "2011-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.3260",
        "title": "Finding Similar/Diverse Solutions in Answer Set Programming",
        "authors": [
            "Thomas Eiter",
            "Esra Erdem",
            "Halit Erdogan",
            "Michael Fink"
        ],
        "abstract": "For some computational problems (e.g., product configuration, planning, diagnosis, query answering, phylogeny reconstruction) computing a set of similar/diverse solutions may be desirable for better decision-making. With this motivation, we studied several decision/optimization versions of this problem in the context of Answer Set Programming (ASP), analyzed their computational complexity, and introduced offline/online methods to compute similar/diverse solutions of such computational problems with respect to a given distance function. All these methods rely on the idea of computing solutions to a problem by means of finding the answer sets for an ASP program that describes the problem. The offline methods compute all solutions in advance using the ASP formulation of the problem with an ASP solver, like Clasp, and then identify similar/diverse solutions using clustering methods. The online methods compute similar/diverse solutions following one of the three approaches: by reformulating the ASP representation of the problem to compute similar/diverse solutions at once using an ASP solver; by computing similar/diverse solutions iteratively (one after other) using an ASP solver; by modifying the search algorithm of an ASP solver to compute similar/diverse solutions incrementally. We modified Clasp to implement the last online method and called it Clasp-NK. In the first two online methods, the given distance function is represented in ASP; in the last one it is implemented in C++. We showed the applicability and the effectiveness of these methods on reconstruction of similar/diverse phylogenies for Indo-European languages, and on several planning problems in Blocks World. We observed that in terms of computational efficiency the last online method outperforms the others; also it allows us to compute similar/diverse solutions when the distance function cannot be represented in ASP.\n    ",
        "submission_date": "2011-08-16T00:00:00",
        "last_modified_date": "2011-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.3278",
        "title": "Reiter's Default Logic Is a Logic of Autoepistemic Reasoning And a Good One, Too",
        "authors": [
            "Marc Denecker",
            "Victor W. Marek",
            "Miroslaw Truszczynski"
        ],
        "abstract": "A fact apparently not observed earlier in the literature of nonmonotonic reasoning is that Reiter, in his default logic paper, did not directly formalize informal defaults. Instead, he translated a default into a certain natural language proposition and provided a formalization of the latter. A few years later, Moore noted that propositions like the one used by Reiter are fundamentally different than defaults and exhibit a certain autoepistemic nature. Thus, Reiter had developed his default logic as a formalization of autoepistemic propositions rather than of defaults.\n",
        "submission_date": "2011-08-16T00:00:00",
        "last_modified_date": "2011-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.3279",
        "title": "Revisiting Epistemic Specifications",
        "authors": [
            "Miroslaw Truszczynski"
        ],
        "abstract": "In 1991, Michael Gelfond introduced the language of epistemic specifications. The goal was to develop tools for modeling problems that require some form of meta-reasoning, that is, reasoning over multiple possible worlds. Despite their relevance to knowledge representation, epistemic specifications have received relatively little attention so far. In this paper, we revisit the formalism of epistemic specification. We offer a new definition of the formalism, propose several semantics (one of which, under syntactic restrictions we assume, turns out to be equivalent to the original semantics by Gelfond), derive some complexity results and, finally, show the effectiveness of the formalism for modeling problems requiring meta-reasoning considered recently by Faber and Woltran. All these results show that epistemic specifications deserve much more attention that has been afforded to them so far.\n    ",
        "submission_date": "2011-08-16T00:00:00",
        "last_modified_date": "2011-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.3281",
        "title": "Origins of Answer-Set Programming - Some Background And Two Personal Accounts",
        "authors": [
            "Victor W. Marek",
            "Ilkka Niemela",
            "Miroslaw Truszczynski"
        ],
        "abstract": "We discuss the evolution of aspects of nonmonotonic reasoning towards the computational paradigm of answer-set programming (ASP). We give a general overview of the roots of ASP and follow up with the personal perspective on research developments that helped verbalize the main principles of ASP and differentiated it from the classical logic programming.\n    ",
        "submission_date": "2011-08-16T00:00:00",
        "last_modified_date": "2011-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.3614",
        "title": "Feature Reinforcement Learning In Practice",
        "authors": [
            "Phuong Nguyen",
            "Peter Sunehag",
            "Marcus Hutter"
        ],
        "abstract": "Following a recent surge in using history-based methods for resolving perceptual aliasing in reinforcement learning, we introduce an algorithm based on the feature reinforcement learning framework called PhiMDP. To create a practical algorithm we devise a stochastic search procedure for a class of context trees based on parallel tempering and a specialized proposal distribution. We provide the first empirical evaluation for PhiMDP. Our proposed algorithm achieves superior performance to the classical U-tree algorithm and the recent active-LZ algorithm, and is competitive with MC-AIXI-CTW that maintains a bayesian mixture over all context trees up to a chosen ",
        "submission_date": "2011-08-18T00:00:00",
        "last_modified_date": "2011-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.3711",
        "title": "Doing Better Than UCT: Rational Monte Carlo Sampling in Trees",
        "authors": [
            "David Tolpin",
            "Solomon Eyal Shimony"
        ],
        "abstract": "UCT, a state-of-the art algorithm for Monte Carlo tree sampling (MCTS), is based on UCB, a sampling policy for the Multi-armed Bandit Problem (MAB) that minimizes the accumulated regret. However, MCTS differs from MAB in that only the final choice, rather than all arm pulls, brings a reward, that is, the simple regret, as opposite to the cumulative regret, must be minimized. This ongoing work aims at applying meta-reasoning techniques to MCTS, which is non-trivial. We begin by introducing policies for multi-armed bandits with lower simple regret than UCB, and an algorithm for MCTS which combines cumulative and simple regret minimization and outperforms UCT. We also develop a sampling scheme loosely based on a myopic version of perfect value of information. Finite-time and asymptotic analysis of the policies is provided, and the algorithms are compared empirically.\n    ",
        "submission_date": "2011-08-18T00:00:00",
        "last_modified_date": "2012-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.3757",
        "title": "Self-Organizing Mixture Networks for Representation of Grayscale Digital Images",
        "authors": [
            "Patryk Filipiak"
        ],
        "abstract": "Self-Organizing Maps are commonly used for unsupervised learning purposes. This paper is dedicated to the certain modification of SOM called SOMN (Self-Organizing Mixture Networks) used as a mechanism for representing grayscale digital images. Any grayscale digital image regarded as a distribution function can be approximated by the corresponding Gaussian mixture. In this paper, the use of SOMN is proposed in order to obtain such approximations for input grayscale images in unsupervised manner.\n    ",
        "submission_date": "2011-08-18T00:00:00",
        "last_modified_date": "2011-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.4199",
        "title": "Biomimetic use of genetic algorithms",
        "authors": [
            "Jean-Louis Dessalles"
        ],
        "abstract": "Genetic algorithms are considered as an original way to solve problems, probably because of their generality and of their \"blind\" nature. But GAs are also unusual since the features of many implementations (among all that could be thought of) are principally led by the biological metaphor, while efficiency measurements intervene only afterwards. We propose here to examine the relevance of these biomimetic aspects, by pointing out some fundamental similarities and divergences between GAs and the genome of living beings shaped by natural selection. One of the main differences comes from the fact that GAs rely principally on the so-called implicit parallelism, while giving to the mutation/selection mechanism the second role. Such differences could suggest new ways of employing GAs on complex problems, using complex codings and starting from nearly homogeneous populations.\n    ",
        "submission_date": "2011-08-21T00:00:00",
        "last_modified_date": "2011-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.4220",
        "title": "A Dynamical Systems Approach for Static Evaluation in Go",
        "authors": [
            "Thomas Wolf"
        ],
        "abstract": "In the paper arguments are given why the concept of static evaluation has the potential to be a useful extension to Monte Carlo tree search. A new concept of modeling static evaluation through a dynamical system is introduced and strengths and weaknesses are discussed. The general suitability of this approach is demonstrated.\n    ",
        "submission_date": "2011-08-21T00:00:00",
        "last_modified_date": "2011-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.4279",
        "title": "Detection and emergence",
        "authors": [
            "Eric Bonabeau",
            "Jean-Louis Dessalles"
        ],
        "abstract": "Two different conceptions of emergence are reconciled as two instances of the phenomenon of detection. In the process of comparing these two conceptions, we find that the notions of complexity and detection allow us to form a unified definition of emergence that clearly delineates the role of the observer.\n    ",
        "submission_date": "2011-08-22T00:00:00",
        "last_modified_date": "2011-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.4804",
        "title": "dynPARTIX - A Dynamic Programming Reasoner for Abstract Argumentation",
        "authors": [
            "Wolfgang Dvo\u0159\u00e1k",
            "Michael Morak",
            "Clemens Nopp",
            "Stefan Woltran"
        ],
        "abstract": "The aim of this paper is to announce the release of a novel system for abstract argumentation which is based on decomposition and dynamic programming. We provide first experimental evaluations to show the feasibility of this approach.\n    ",
        "submission_date": "2011-08-24T00:00:00",
        "last_modified_date": "2011-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.4891",
        "title": "Computing with Logic as Operator Elimination: The ToyElim System",
        "authors": [
            "Christoph Wernhard"
        ],
        "abstract": "A prototype system is described whose core functionality is, based on propositional logic, the elimination of second-order operators, such as Boolean quantifiers and operators for projection, forgetting and circumscription. This approach allows to express many representational and computational tasks in knowledge representation - for example computation of abductive explanations and models with respect to logic programming semantics - in a uniform operational system, backed by a uniform classical semantic framework.\n    ",
        "submission_date": "2011-08-24T00:00:00",
        "last_modified_date": "2011-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.4942",
        "title": "Making Use of Advances in Answer-Set Programming for Abstract Argumentation Systems",
        "authors": [
            "Wolfgang Dvo\u0159\u00e1k",
            "Sarah Alice Gaggl",
            "Johannes Wallner",
            "Stefan Woltran"
        ],
        "abstract": "Dung's famous abstract argumentation frameworks represent the core formalism for many problems and applications in the field of argumentation which significantly evolved within the last decade. Recent work in the field has thus focused on implementations for these frameworks, whereby one of the main approaches is to use Answer-Set Programming (ASP). While some of the argumentation semantics can be nicely expressed within the ASP language, others required rather cumbersome encoding techniques. Recent advances in ASP systems, in particular, the metasp optimization frontend for the ASP-package gringo/claspD provides direct commands to filter answer sets satisfying certain subset-minimality (or -maximality) constraints. This allows for much simpler encodings compared to the ones in standard ASP language. In this paper, we experimentally compare the original encodings (for the argumentation semantics based on preferred, semi-stable, and respectively, stage extensions) with new metasp encodings. Moreover, we provide novel encodings for the recently introduced resolution-based grounded semantics. Our experimental results indicate that the metasp approach works well in those cases where the complexity of the encoded problem is adequately mirrored within the metasp approach.\n    ",
        "submission_date": "2011-08-24T00:00:00",
        "last_modified_date": "2011-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.5002",
        "title": "Verbal Characterization of Probabilistic Clusters using Minimal Discriminative Propositions",
        "authors": [
            "Yoshitaka Kameya",
            "Satoru Nakamura",
            "Tatsuya Iwasaki",
            "Taisuke Sato"
        ],
        "abstract": "In a knowledge discovery process, interpretation and evaluation of the mined results are indispensable in practice. In the case of data clustering, however, it is often difficult to see in what aspect each cluster has been formed. This paper proposes a method for automatic and objective characterization or \"verbalization\" of the clusters obtained by mixture models, in which we collect conjunctions of propositions (attribute-value pairs) that help us interpret or evaluate the clusters. The proposed method provides us with a new, in-depth and consistent tool for cluster interpretation/evaluation, and works for various types of datasets including continuous attributes and missing values. Experimental results with a couple of standard datasets exhibit the utility of the proposed method, and the importance of the feedbacks from the interpretation/evaluation step.\n    ",
        "submission_date": "2011-08-25T00:00:00",
        "last_modified_date": "2011-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.5250",
        "title": "Single-trial EEG Discrimination between Wrist and Finger Movement Imagery and Execution in a Sensorimotor BCI",
        "authors": [
            "A.K. Mohamed",
            "T. Marwala",
            "L.R. John"
        ],
        "abstract": "A brain-computer interface (BCI) may be used to control a prosthetic or orthotic hand using neural activity from the brain. The core of this sensorimotor BCI lies in the interpretation of the neural information extracted from electroencephalogram (EEG). It is desired to improve on the interpretation of EEG to allow people with neuromuscular disorders to perform daily activities. This paper investigates the possibility of discriminating between the EEG associated with wrist and finger movements. The EEG was recorded from test subjects as they executed and imagined five essential hand movements using both hands. Independent component analysis (ICA) and time-frequency techniques were used to extract spectral features based on event-related (de)synchronisation (ERD/ERS), while the Bhattacharyya distance (BD) was used for feature reduction. Mahalanobis distance (MD) clustering and artificial neural networks (ANN) were used as classifiers and obtained average accuracies of 65 % and 71 % respectively. This shows that EEG discrimination between wrist and finger movements is possible. The research introduces a new combination of motor tasks to BCI research.\n    ",
        "submission_date": "2011-08-26T00:00:00",
        "last_modified_date": "2011-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.5567",
        "title": "Parsing Combinatory Categorial Grammar with Answer Set Programming: Preliminary Report",
        "authors": [
            "Yuliya Lierler",
            "Peter Sch\u00fcller"
        ],
        "abstract": "Combinatory categorial grammar (CCG) is a grammar formalism used for natural language parsing. CCG assigns structured lexical categories to words and uses a small set of combinatory rules to combine these categories to parse a sentence. In this work we propose and implement a new approach to CCG parsing that relies on a prominent knowledge representation formalism, answer set programming (ASP) - a declarative programming paradigm. We formulate the task of CCG parsing as a planning problem and use an ASP computational tool to compute solutions that correspond to valid parses. Compared to other approaches, there is no need to implement a specific parsing algorithm using such a declarative method. Our approach aims at producing all semantically distinct parse trees for a given sentence. From this goal, normalization and efficiency issues arise, and we deal with them by combining and extending existing strategies. We have implemented a CCG parsing tool kit - AspCcgTk - that uses ASP as its main computational means. The C&C supertagger can be used as a preprocessor within AspCcgTk, which allows us to achieve wide-coverage natural language parsing.\n    ",
        "submission_date": "2011-08-29T00:00:00",
        "last_modified_date": "2011-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.5586",
        "title": "FdConfig: A Constraint-Based Interactive Product Configurator",
        "authors": [
            "Denny Schneeweiss",
            "Petra Hofstedt"
        ],
        "abstract": "We present a constraint-based approach to interactive product configuration. Our configurator tool FdConfig is based on feature models for the representation of the product domain. Such models can be directly mapped into constraint satisfaction problems and dealt with by appropriate constraint solvers. During the interactive configuration process the user generates new constraints as a result of his configuration decisions and even may retract constraints posted earlier. We discuss the configuration process, explain the underlying techniques and show optimizations.\n    ",
        "submission_date": "2011-08-29T00:00:00",
        "last_modified_date": "2011-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.5626",
        "title": "Nested HEX-Programs",
        "authors": [
            "Thomas Eiter",
            "Thomas Krennwallner",
            "Christoph Redl"
        ],
        "abstract": "Answer-Set Programming (ASP) is an established declarative programming paradigm. However, classical ASP lacks subprogram calls as in procedural programming, and access to external computations (like remote procedure calls) in general. The feature is desired for increasing modularity and---assuming proper access in place---(meta-)reasoning over subprogram results. While HEX-programs extend classical ASP with external source access, they do not support calls of (sub-)programs upfront. We present nested HEX-programs, which extend HEX-programs to serve the desired feature, in a user-friendly manner. Notably, the answer sets of called sub-programs can be individually accessed. This is particularly useful for applications that need to reason over answer sets like belief set merging, user-defined aggregate functions, or preferences of answer sets.\n    ",
        "submission_date": "2011-08-29T00:00:00",
        "last_modified_date": "2011-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.5667",
        "title": "A prototype of a knowledge-based programming environment",
        "authors": [
            "Stef De Pooter",
            "Johan Wittocx",
            "Marc Denecker"
        ],
        "abstract": "In this paper we present a proposal for a knowledge-based programming environment. In such an environment, declarative background knowledge, procedures, and concrete data are represented in suitable languages and combined in a flexible manner. This leads to a highly declarative programming style. We illustrate our approach on an example and report about our prototype implementation.\n    ",
        "submission_date": "2011-08-29T00:00:00",
        "last_modified_date": "2011-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.5668",
        "title": "Datum-Wise Classification: A Sequential Approach to Sparsity",
        "authors": [
            "Gabriel Dulac-Arnold",
            "Ludovic Denoyer",
            "Philippe Preux",
            "Patrick Gallinari"
        ],
        "abstract": "We propose a novel classification technique whose aim is to select an appropriate representation for each datapoint, in contrast to the usual approach of selecting a representation encompassing the whole dataset. This datum-wise representation is found by using a sparsity inducing empirical risk, which is a relaxation of the standard L 0 regularized risk. The classification problem is modeled as a sequential decision process that sequentially chooses, for each datapoint, which features to use before classifying. Datum-Wise Classification extends naturally to multi-class tasks, and we describe a specific case where our inference has equivalent complexity to a traditional linear classifier, while still using a variable number of features. We compare our classifier to classical L 1 regularized linear models (L 1-SVM and LARS) on a set of common binary and multi-class datasets and show that for an equal average number of features used we can get improved performance using our method.\n    ",
        "submission_date": "2011-08-29T00:00:00",
        "last_modified_date": "2011-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.5717",
        "title": "Structure Selection from Streaming Relational Data",
        "authors": [
            "Lilyana Mihalkova",
            "Walaa Eldin Moustafa"
        ],
        "abstract": "Statistical relational learning techniques have been successfully applied in a wide range of relational domains. In most of these applications, the human designers capitalized on their background knowledge by following a trial-and-error trajectory, where relational features are manually defined by a human engineer, parameters are learned for those features on the training data, the resulting model is validated, and the cycle repeats as the engineer adjusts the set of features. This paper seeks to streamline application development in large relational domains by introducing a light-weight approach that efficiently evaluates relational features on pieces of the relational graph that are streamed to it one at a time. We evaluate our approach on two social media tasks and demonstrate that it leads to more accurate models that are learned faster.\n    ",
        "submission_date": "2011-08-29T00:00:00",
        "last_modified_date": "2011-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.5794",
        "title": "A Constraint Logic Programming Approach for Computing Ordinal Conditional Functions",
        "authors": [
            "Christoph Beierle",
            "Gabriele Kern-Isberner",
            "Karl S\u00f6dler"
        ],
        "abstract": "In order to give appropriate semantics to qualitative conditionals of the form \"if A then normally B\", ordinal conditional functions (OCFs) ranking the possible worlds according to their degree of plausibility can be used. An OCF accepting all conditionals of a knowledge base R can be characterized as the solution of a constraint satisfaction problem. We present a high-level, declarative approach using constraint logic programming techniques for solving this constraint satisfaction problem. In particular, the approach developed here supports the generation of all minimal solutions; these minimal solutions are of special interest as they provide a basis for model-based inference from R.\n    ",
        "submission_date": "2011-08-30T00:00:00",
        "last_modified_date": "2011-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.5825",
        "title": "Confidentiality-Preserving Data Publishing for Credulous Users by Extended Abduction",
        "authors": [
            "Katsumi Inoue",
            "Chiaki Sakama",
            "Lena Wiese"
        ],
        "abstract": "Publishing private data on external servers incurs the problem of how to avoid unwanted disclosure of confidential data. We study a problem of confidentiality in extended disjunctive logic programs and show how it can be solved by extended abduction. In particular, we analyze how credulous non-monotonic reasoning affects confidentiality.\n    ",
        "submission_date": "2011-08-30T00:00:00",
        "last_modified_date": "2011-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.5837",
        "title": "Translating Answer-Set Programs into Bit-Vector Logic",
        "authors": [
            "Mai Nguyen",
            "Tomi Janhunen",
            "Ilkka Niemel\u00e4"
        ],
        "abstract": "Answer set programming (ASP) is a paradigm for declarative problem solving where problems are first formalized as rule sets, i.e., answer-set programs, in a uniform way and then solved by computing answer sets for programs. The satisfiability modulo theories (SMT) framework follows a similar modelling philosophy but the syntax is based on extensions of propositional logic rather than rules. Quite recently, a translation from answer-set programs into difference logic was provided---enabling the use of particular SMT solvers for the computation of answer sets. In this paper, the translation is revised for another SMT fragment, namely that based on fixed-width bit-vector theories. Thus, even further SMT solvers can be harnessed for the task of computing answer sets. The results of a preliminary experimental comparison are also reported. They suggest a level of performance which is similar to that achieved via difference logic.\n    ",
        "submission_date": "2011-08-30T00:00:00",
        "last_modified_date": "2011-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.5943",
        "title": "Proof System for Plan Verification under 0-Approximation Semantics",
        "authors": [
            "Xishun Zhao",
            "Yuping Shen"
        ],
        "abstract": "In this paper a proof system is developed for plan verification problems $\\{X\\}c\\{Y\\}$ and $\\{X\\}c\\{KW p\\}$ under 0-approximation semantics for ${\\mathcal A}_K$. Here, for a plan $c$, two sets $X,Y$ of fluent literals, and a literal $p$, $\\{X\\}c\\{Y\\}$ (resp. $\\{X\\}c\\{KW p\\}$) means that all literals of $Y$ become true (resp. $p$ becomes known) after executing $c$ in any initial state in which all literals in $X$ are ",
        "submission_date": "2011-08-30T00:00:00",
        "last_modified_date": "2011-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.6007",
        "title": "Domain-specific Languages in a Finite Domain Constraint Programming System",
        "authors": [
            "Markus Triska"
        ],
        "abstract": "In this paper, we present domain-specific languages (DSLs) that we devised for their use in the implementation of a finite domain constraint programming system, available as library(clpfd) in SWI-Prolog and YAP-Prolog. These DSLs are used in propagator selection and constraint reification. In these areas, they lead to concise specifications that are easy to read and reason about. At compilation time, these specifications are translated to Prolog code, reducing interpretative run-time overheads. The devised languages can be used in the implementation of other finite domain constraint solvers as well and may contribute to their correctness, conciseness and efficiency.\n    ",
        "submission_date": "2011-08-30T00:00:00",
        "last_modified_date": "2011-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.6208",
        "title": "Coprocessor - a Standalone SAT Preprocessor",
        "authors": [
            "Norbert Manthey"
        ],
        "abstract": "In this work a stand-alone preprocessor for SAT is presented that is able to perform most of the known preprocessing techniques. Preprocessing a formula in SAT is important for performance since redundancy can be removed. The preprocessor is part of the SAT solver riss and is called Coprocessor. Not only riss, but also MiniSat 2.2 benefit from it, because the SatELite preprocessor of MiniSat does not implement recent techniques. By using more advanced techniques, Coprocessor is able to reduce the redundancy in a formula further and improves the overall solving performance.\n    ",
        "submission_date": "2011-08-31T00:00:00",
        "last_modified_date": "2011-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.6211",
        "title": "Transfer from Multiple MDPs",
        "authors": [
            "Alessandro Lazaric",
            "Marcello Restelli"
        ],
        "abstract": "Transfer reinforcement learning (RL) methods leverage on the experience collected on a set of source tasks to speed-up RL algorithms. A simple and effective approach is to transfer samples from source tasks and include them into the training set used to solve a given target task. In this paper, we investigate the theoretical properties of this transfer method and we introduce novel algorithms adapting the transfer process on the basis of the similarity between source and target tasks. Finally, we report illustrative experimental results in a continuous chain problem.\n    ",
        "submission_date": "2011-08-31T00:00:00",
        "last_modified_date": "2011-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.0113",
        "title": "aspcud: A Linux Package Configuration Tool Based on Answer Set Programming",
        "authors": [
            "Martin Gebser",
            "Roland Kaminski",
            "Torsten Schaub"
        ],
        "abstract": "We present the Linux package configuration tool aspcud based on Answer Set Programming. In particular, we detail aspcud's preprocessor turning a CUDF specification into a set of logical facts.\n    ",
        "submission_date": "2011-09-01T00:00:00",
        "last_modified_date": "2011-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.0114",
        "title": "(Re)configuration based on model generation",
        "authors": [
            "Gerhard Friedrich",
            "Anna Ryabokon",
            "Andreas A. Falkner",
            "Alois Haselb\u00f6ck",
            "Gottfried Schenner",
            "Herwig Schreiner"
        ],
        "abstract": "Reconfiguration is an important activity for companies selling configurable products or services which have a long life time. However, identification of a set of required changes in a legacy configuration is a hard problem, since even small changes in the requirements might imply significant modifications. In this paper we show a solution based on answer set programming, which is a logic-based knowledge representation formalism well suited for a compact description of (re)configuration problems. Its applicability is demonstrated on simple abstractions of several real-world scenarios.  The evaluation of our solution on a set of benchmark instances derived from commercial (re)configuration problems shows its practical applicability.\n    ",
        "submission_date": "2011-09-01T00:00:00",
        "last_modified_date": "2011-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.0621",
        "title": "Visual Inference Specification Methods for Modularized Rulebases. Overview and Integration Proposal",
        "authors": [
            "Krzysztof Kluza",
            "Grzegorz J. Nalepa",
            "\u0141ukasz \u0141ysik"
        ],
        "abstract": "The paper concerns selected rule modularization techniques. Three visual methods for inference specification for modularized rule- bases are described: Drools Flow, BPMN and XTT2. Drools Flow is a popular technology for workflow or process modeling, BPMN is an OMG standard for modeling business processes, and XTT2 is a hierarchical tab- ular system specification method. Because of some limitations of these solutions, several proposals of their integration are given.\n    ",
        "submission_date": "2011-09-03T00:00:00",
        "last_modified_date": "2011-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1032",
        "title": "Tech Report A Variational HEM Algorithm for Clustering Hidden Markov Models",
        "authors": [
            "Emanuele Coviello",
            "Antoni B. Chan",
            "Gert R.G. Lanckriet"
        ],
        "abstract": "The hidden Markov model (HMM) is a generative model that treats sequential data under the assumption that each observation is conditioned on the state of a discrete hidden variable that evolves in time as a Markov chain. In this paper, we derive a novel algorithm to cluster HMMs through their probability distributions. We propose a hierarchical EM algorithm that i) clusters a given collection of HMMs into groups of HMMs that are similar, in terms of the distributions they represent, and ii) characterizes each group by a \"cluster center\", i.e., a novel HMM that is representative for the group. We present several empirical studies that illustrate the benefits of the proposed algorithm.\n    ",
        "submission_date": "2011-09-06T00:00:00",
        "last_modified_date": "2011-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1231",
        "title": "A Combinatorial Optimisation Approach to Designing Dual-Parented Long-Reach Passive Optical Networks",
        "authors": [
            "Hadrien Cambazard",
            "Deepak Mehta",
            "Barry O'Sullivan",
            "Luis Quesada",
            "Marco Ruffini",
            "David Payne",
            "Linda Doyle"
        ],
        "abstract": "We present an application focused on the design of resilient long-reach passive optical networks. We specifically consider dual-parented networks whereby each customer must be connected to two metro sites via local exchange sites. An important property of such a placement is resilience to single metro node failure. The objective of the application is to determine the optimal position of a set of metro nodes such that the total optical fibre length is minimized. We prove that this problem is NP-Complete. We present two alternative combinatorial optimisation approaches to finding an optimal metro node placement using: a mixed integer linear programming (MIP) formulation of the problem; and, a hybrid approach that uses clustering as a preprocessing step. We consider a detailed case-study based on a network for Ireland. The hybrid approach scales well and finds solutions that are close to optimal, with a runtime that is two orders-of-magnitude better than the MIP model.\n    ",
        "submission_date": "2011-09-06T00:00:00",
        "last_modified_date": "2011-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1276",
        "title": "Application of the Modified 2-opt and Jumping Gene Operators in Multi-Objective Genetic Algorithm to solve MOTSP",
        "authors": [
            "Rohan Agrawal"
        ],
        "abstract": "Evolutionary Multi-Objective Optimization is becoming a hot research area and quite a few papers regarding these algorithms have been published. However the role of local search techniques has not been expanded adequately. This paper studies the role of a local search technique called 2-opt for the Multi-Objective Travelling Salesman Problem (MOTSP). A new mutation operator called Jumping Gene (JG) is also used. Since 2-opt operator was intended for the single objective TSP, its domain has been expanded to MOTSP in this paper. This new technique is applied to the list of KroAB100 cities.\n    ",
        "submission_date": "2011-09-06T00:00:00",
        "last_modified_date": "2011-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1314",
        "title": "Measuring Intelligence through Games",
        "authors": [
            "Tom Schaul",
            "Julian Togelius",
            "J\u00fcrgen Schmidhuber"
        ],
        "abstract": "Artificial general intelligence (AGI) refers to research aimed at tackling the full problem of artificial intelligence, that is, create truly intelligent agents. This sets it apart from most AI research which aims at solving relatively narrow domains, such as character recognition, motion planning, or increasing player satisfaction in games. But how do we know when an agent is truly intelligent? A common point of reference in the AGI community is Legg and Hutter's formal definition of universal intelligence, which has the appeal of simplicity and generality but is unfortunately incomputable. Games of various kinds are commonly used as benchmarks for \"narrow\" AI research, as they are considered to have many important properties. We argue that many of these properties carry over to the testing of general intelligence as well. We then sketch how such testing could practically be carried out. The central part of this sketch is an extension of universal intelligence to deal with finite time, and the use of sampling of the space of games expressed in a suitably biased game description language.\n    ",
        "submission_date": "2011-09-06T00:00:00",
        "last_modified_date": "2011-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1498",
        "title": "Structured Knowledge Representation for Image Retrieval",
        "authors": [
            "E. Di Sciascio",
            "F. M. Donini",
            "M. Mongiello"
        ],
        "abstract": "We propose a structured approach to the problem of retrieval    of images by content and present a description logic that has been    devised for the semantic indexing and retrieval of images containing    complex objects.        As other approaches do, we start from low-level features extracted    with image analysis to detect and characterize regions in an    image. However, in contrast with feature-based approaches, we provide    a syntax to describe segmented regions as basic objects and complex    objects as compositions of basic ones. Then we introduce a companion    extensional semantics for defining reasoning services, such as    retrieval, classification, and subsumption.  These services can be    used for both exact and approximate matching, using similarity    measures.           Using our logical approach as a formal specification, we implemented a    complete client-server image retrieval system, which allows a user to    pose both queries by sketch and queries by example. A set of    experiments has been carried out on a testbed of images to assess the    retrieval capabilities of the system in comparison with expert users    ranking. Results are presented adopting a well-established measure of    quality borrowed from textual information retrieval.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1724",
        "title": "The Complexity of Approximating a Bethe Equilibrium",
        "authors": [
            "Jinwoo Shin"
        ],
        "abstract": "This paper resolves a common complexity issue in the Bethe approximation of statistical physics and the Belief Propagation (BP) algorithm of artificial intelligence. The Bethe approximation and the BP algorithm are heuristic methods for estimating the partition function and marginal probabilities in graphical models, respectively. The computational complexity of the Bethe approximation is decided by the number of operations required to solve a set of non-linear equations, the so-called Bethe equation. Although the BP algorithm was inspired and developed independently, Yedidia, Freeman and Weiss (2004) showed that the BP algorithm solves the Bethe equation if it converges (however, it often does not). This naturally motivates the following question to understand limitations and empirical successes of the Bethe and BP methods: is the Bethe equation computationally easy to solve? We present a message-passing algorithm solving the Bethe equation in a polynomial number of operations for general binary graphical models of n variables where the maximum degree in the underlying graph is O(log n). Our algorithm can be used as an alternative to BP fixing its convergence issue and is the first fully polynomial-time approximation scheme for the BP fixed-point computation in such a large class of graphical models, while the approximate fixed-point computation is known to be (PPAD-)hard in general. We believe that our technique is of broader interest to understand the computational complexity of the cavity method in statistical physics.\n    ",
        "submission_date": "2011-09-08T00:00:00",
        "last_modified_date": "2013-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1754",
        "title": "Solving Limited Memory Influence Diagrams",
        "authors": [
            "Denis Deratani Mau\u00e1",
            "Cassio Polpo de Campos",
            "Marco Zaffalon"
        ],
        "abstract": "We present a new algorithm for exactly solving decision making problems represented as influence diagrams. We do not require the usual assumptions of no forgetting and regularity; this allows us to solve problems with simultaneous decisions and limited information. The algorithm is empirically shown to outperform a state-of-the-art algorithm on randomly generated problems of up to 150 variables and $10^{64}$ solutions. We show that the problem is NP-hard even if the underlying graph structure of the problem has small treewidth and the variables take on a bounded number of states, but that a fully polynomial time approximation scheme exists for these cases. Moreover, we show that the bound on the number of states is a necessary condition for any efficient approximation scheme.\n    ",
        "submission_date": "2011-09-08T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1774",
        "title": "Conjure Revisited: Towards Automated Constraint Modelling",
        "authors": [
            "Ozgur Akgun",
            "Alan M. Frisch",
            "Brahim Hnich",
            "Chris Jefferson",
            "Ian Miguel"
        ],
        "abstract": "Automating the constraint modelling process is one of the key challenges facing the constraints field, and one of the principal obstacles preventing widespread adoption of constraint solving. This paper focuses on the refinement-based approach to automated modelling, where a user specifies a problem in an abstract constraint specification language and it is then automatically refined into a constraint model. In particular, we revisit the Conjure system that first appeared in prototype form in 2005 and present a new implementation with a much greater coverage of the specification language Essence.\n    ",
        "submission_date": "2011-09-08T00:00:00",
        "last_modified_date": "2011-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1922",
        "title": "Predicting the Energy Output of Wind Farms Based on Weather Data: Important Variables and their Correlation",
        "authors": [
            "Katya Vladislavleva",
            "Tobias Friedrich",
            "Frank Neumann",
            "Markus Wagner"
        ],
        "abstract": "Wind energy plays an increasing role in the supply of energy world-wide. The energy output of a wind farm is highly dependent on the weather condition present at the wind farm. If the output can be predicted more accurately, energy suppliers can coordinate the collaborative production of different energy sources more efficiently to avoid costly overproductions.\n",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1966",
        "title": "The path inference filter: model-based low-latency map matching of probe vehicle data",
        "authors": [
            "Timothy Hunter",
            "Pieter Abbeel",
            "Alexandre Bayen"
        ],
        "abstract": "We consider the problem of reconstructing vehicle trajectories from sparse sequences of GPS points, for which the sampling interval is between 10 seconds and 2 minutes. We introduce a new class of algorithms, called altogether path inference filter (PIF), that maps GPS data in real time, for a variety of trade-offs and scenarios, and with a high throughput. Numerous prior approaches in map-matching can be shown to be special cases of the path inference filter presented in this article. We present an efficient procedure for automatically training the filter on new data, with or without ground truth observations. The framework is evaluated on a large San Francisco taxi dataset and is shown to improve upon the current state of the art. This filter also provides insights about driving patterns of drivers. The path inference filter has been deployed at an industrial scale inside the Mobile Millennium traffic information system, and is used to map fleets of data in San Francisco, Sacramento, Stockholm and Porto.\n    ",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2048",
        "title": "An Expressive Language and Efficient Execution System for Software Agents",
        "authors": [
            "G. Barish",
            "C. A. Knoblock"
        ],
        "abstract": "Software agents can be used to automate many of the tedious, time-consuming information processing tasks that humans currently have to complete manually.  However, to do so, agent plans must be capable of representing the myriad of actions and control flows required to perform those tasks.  In addition, since these tasks can require integrating multiple sources of remote information ? typically, a slow, I/O-bound process ? it is desirable to make execution as efficient as possible.  To address both of these needs, we present a flexible software agent plan language and a highly parallel execution system that enable the efficient execution of expressive agent plans. The plan language allows complex tasks to be more easily expressed by providing a variety of operators for flexibly processing the data as well as supporting subplans (for modularity) and recursion (for indeterminate looping).  The executor is based on a streaming dataflow model of execution to maximize the amount of operator and data parallelism possible at runtime.  We have implemented both the language and executor in a system called THESEUS.  Our results from testing THESEUS show that streaming dataflow execution can yield significant speedups over both traditional serial (von Neumann) as well as non-streaming dataflow-style execution that existing software and robot agent execution systems currently support.  In addition, we show how plans written in the language we present can represent certain types of subtasks that cannot be accomplished using the languages supported by network query engines.  Finally, we demonstrate that the increased expressivity of our plan language does not hamper performance; specifically, we show how data can be integrated from multiple remote sources just as efficiently using our architecture as is possible with a state-of-the-art streaming-dataflow network query engine.\n    ",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2049",
        "title": "Structure-Based Local Search Heuristics for Circuit-Level Boolean Satisfiability",
        "authors": [
            "Anton Belov",
            "Matti J\u00e4rvisalo"
        ],
        "abstract": "This work focuses on improving state-of-the-art in stochastic local search (SLS) for solving Boolean satisfiability (SAT) instances arising from real-world industrial SAT application domains. The recently introduced SLS method CRSat has been shown to noticeably improve on previously suggested SLS techniques in solving such real-world instances by combining justification-based local search with limited Boolean constraint propagation on the non-clausal formula representation form of Boolean circuits. In this work, we study possibilities of further improving the performance of CRSat by exploiting circuit-level structural knowledge for developing new search heuristics for CRSat. To this end, we introduce and experimentally evaluate a variety of search heuristics, many of which are motivated by circuit-level heuristics originally developed in completely different contexts, e.g., for electronic design automation applications. To the best of our knowledge, most of the heuristics are novel in the context of SLS for SAT and, more generally, SLS for constraint satisfaction problems.\n    ",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2127",
        "title": "Integrating Learning from Examples into the Search for Diagnostic Policies",
        "authors": [
            "V. Bayer-Zubek",
            "T. G. Dietterich"
        ],
        "abstract": "This paper studies the problem of learning diagnostic policies from training examples. A diagnostic policy is a complete description of the decision-making actions of a diagnostician (i.e., tests followed by a diagnostic decision) for all possible combinations of test results.  An optimal diagnostic policy is one that minimizes the expected total cost, which is the sum of measurement costs and misdiagnosis costs.  In most diagnostic settings, there is a tradeoff between these two kinds of costs.  This paper formalizes diagnostic decision making as a Markov Decision Process (MDP). The paper introduces a new family of systematic search algorithms based on the AO* algorithm to solve this MDP.  To make AO* efficient, the paper describes an admissible heuristic that enables AO* to prune large parts of the search space.  The paper also introduces several greedy algorithms including some improvements over previously-published methods. The paper then addresses the question of learning diagnostic policies from examples.  When the probabilities of diseases and test results are computed from training data, there is a great danger of overfitting. To reduce overfitting, regularizers are integrated into the search algorithms.  Finally, the paper compares the proposed methods on five benchmark diagnostic data sets.  The studies show that in most cases the systematic search methods produce better diagnostic policies than the greedy methods. In addition, the studies show that for training sets of realistic size, the systematic search algorithms are practical on todays desktop computers.\n    ",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2131",
        "title": "On the Practical use of Variable Elimination in Constraint Optimization Problems: 'Still-life' as a Case Study",
        "authors": [
            "J. Larrosa",
            "E. Morancho",
            "D. Niso"
        ],
        "abstract": "Variable elimination is a general technique for constraint processing. It is often discarded because of its high space complexity. However, it can be extremely useful when combined with other techniques. In this paper we study the applicability of variable elimination to the challenging problem of finding still-lifes.  We illustrate several alternatives: variable elimination as a stand-alone algorithm, interleaved with search, and as a source of good quality lower bounds.  We show that these techniques are the best known option both theoretically and empirically. In our experiments we have been able to solve the n=20 instance, which is far beyond reach with alternative approaches.\n    ",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2134",
        "title": "Generalizing Boolean Satisfiability II: Theory",
        "authors": [
            "H. E. Dixon",
            "M. L. Ginsberg",
            "E. M. Luks",
            "A. J. Parkes"
        ],
        "abstract": "This is the second of three planned papers describing ZAP, a satisfiability engine that substantially generalizes existing tools while retaining the performance characteristics of modern high performance solvers.  The fundamental idea underlying ZAP is that many problems passed to such engines contain rich internal structure that is obscured by the Boolean representation used; our goal is to define a representation in which this structure is apparent and can easily be exploited to improve computational performance.  This paper presents the theoretical basis for the ideas underlying ZAP, arguing that existing ideas in this area exploit a single, recurring structure in that multiple database axioms can be obtained by operating on a single axiom using a subgroup of the group of permutations on the literals in the problem.  We argue that the group structure precisely captures the general structure at which earlier approaches hinted, and give numerous examples of its use.  We go on to extend the Davis-Putnam-Logemann-Loveland inference procedure to this broader setting, and show that earlier computational improvements are either subsumed or left intact by the new method.  The third paper in this series discusses ZAPs implementation and presents experimental performance results.\n    ",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2135",
        "title": "A Framework for Sequential Planning in Multi-Agent Settings",
        "authors": [
            "P. Doshi",
            "P. J. Gmytrasiewicz"
        ],
        "abstract": "This paper extends the framework of partially observable Markov decision processes (POMDPs) to multi-agent settings by incorporating the notion of agent models into the state space.  Agents maintain beliefs over physical states of the environment and over models of other agents, and they use Bayesian updates to maintain their beliefs over time. The solutions map belief states to actions. Models of other agents may include their belief states and are related to agent types considered in games of incomplete information.  We express the agents autonomy by postulating that their models are not directly manipulable or observable by other agents.  We show that important properties of POMDPs, such as convergence of value iteration, the rate of convergence, and piece-wise linearity and convexity of the value functions carry over to our framework.  Our approach complements a more traditional approach to interactive settings which uses Nash equilibria as a solution paradigm.  We seek to avoid some of the drawbacks of equilibria which may be non-unique and do not capture off-equilibrium behaviors.  We do so at the cost of having to represent, process and continuously revise models of other agents. Since the agents beliefs may be arbitrarily nested, the optimal solutions to decision making problems are only asymptotically computable.  However, approximate belief updates and approximately optimal plans are computable. We illustrate our framework using a simple application domain, and we show examples of belief updates and value functions.\n    ",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2137",
        "title": "Relational Dynamic Bayesian Networks",
        "authors": [
            "P. Domingos",
            "S. Sanghai",
            "D. Weld"
        ],
        "abstract": "Stochastic processes that involve the creation of objects and relations over time are widespread, but relatively poorly studied. For example, accurate fault diagnosis in factory assembly processes requires inferring the probabilities of erroneous assembly operations, but doing this efficiently and accurately is difficult. Modeled as dynamic Bayesian networks, these processes have discrete variables with very large domains and extremely high dimensionality. In this paper, we introduce relational dynamic Bayesian networks (RDBNs), which are an extension of dynamic Bayesian networks (DBNs) to first-order logic. RDBNs are a generalization of dynamic probabilistic relational models (DPRMs), which we had proposed in our previous work to model dynamic uncertain domains. We first extend the Rao-Blackwellised particle filtering described in our earlier work to RDBNs. Next, we lift the assumptions associated with Rao-Blackwellization in RDBNs and propose two new forms of particle filtering. The first one uses abstraction hierarchies over the predicates to smooth the particle filters estimates. The second employs kernel density estimation with a kernel function specifically designed for relational domains. Experiments show these two methods greatly outperform standard particle filtering on the task of assembly plan execution monitoring. \n\n    ",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2138",
        "title": "Reasoning about Action: An Argumentation - Theoretic Approach",
        "authors": [
            "N. Y. Foo",
            "Q. B. Vo"
        ],
        "abstract": "We present a uniform non-monotonic solution to the problems of reasoning about action on the basis of an argumentation-theoretic approach. Our theory is provably correct relative to a sensible minimisation policy introduced on top of a temporal propositional logic. Sophisticated problem domains can be formalised in our framework.  As much attention of researchers in the field has been paid to the traditional and basic problems in reasoning about actions such as the frame, the qualification and the ramification problems, approaches to these problems within our formalisation lie at heart of the expositions presented in this paper.\n    ",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2139",
        "title": "Solving Set Constraint Satisfaction Problems using ROBDDs",
        "authors": [
            "P. J. Hawkins",
            "V. Lagoon",
            "P. J. Stuckey"
        ],
        "abstract": "In this paper we present a new approach to modeling finite set domain constraint problems using Reduced Ordered Binary Decision Diagrams (ROBDDs). We show that it is possible to construct an efficient set domain propagator which compactly represents many set domains and set constraints using ROBDDs.  We demonstrate that the ROBDD-based approach provides unprecedented flexibility in modeling constraint satisfaction problems, leading to performance improvements. We also show that the ROBDD-based modeling approach can be extended to the modeling of integer and multiset constraint problems in a straightforward manner. Since domain propagation is not always practical, we also show how to incorporate less strict consistency notions into the ROBDD framework, such as set bounds, cardinality bounds and lexicographic bounds consistency. Finally, we present experimental results that demonstrate the ROBDD-based solver performs better than various more conventional constraint solvers on several standard set constraint problems.\n    ",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2140",
        "title": "Learning Concept Hierarchies from Text Corpora using Formal Concept Analysis",
        "authors": [
            "P. Cimiano",
            "A. Hotho",
            "S. Staab"
        ],
        "abstract": "We present a novel approach to the automatic acquisition of taxonomies or concept hierarchies from a text corpus. The approach is based on Formal Concept Analysis (FCA), a method mainly used for the analysis of data, i.e. for investigating and processing explicitly given information.  We follow Harris distributional hypothesis and model the context of a certain term as a vector representing syntactic dependencies which are automatically acquired from the text corpus with a linguistic parser.  On the basis of this context information, FCA produces a lattice that we convert into a special kind of partial order constituting a concept hierarchy.  The approach is evaluated by comparing the resulting concept hierarchies with hand-crafted taxonomies for two domains: tourism and finance.  We also directly compare our approach with hierarchical agglomerative clustering as well as with Bi-Section-KMeans as an instance of a divisive clustering algorithm. Furthermore, we investigate the impact of using different measures weighting the contribution of each attribute as well as of applying a particular smoothing technique to cope with data sparseness.\n    ",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2142",
        "title": "Generalizing Boolean Satisfiability III: Implementation",
        "authors": [
            "H. E. Dixon",
            "M. L. Ginsberg",
            "D. Hofer",
            "E. M. Luks",
            "A. J. Parkes"
        ],
        "abstract": "This is the third of three papers describing ZAP, a satisfiability engine that substantially generalizes existing tools while retaining the performance characteristics of modern high-performance solvers. The fundamental idea underlying ZAP is that many problems passed to such engines contain rich internal structure that is obscured by the Boolean representation used; our goal has been to define a representation in which this structure is apparent and can be exploited to improve computational performance.  The first paper surveyed existing work that (knowingly or not) exploited problem structure to improve the performance of satisfiability engines, and the second paper showed that this structure could be understood in terms of groups of permutations acting on individual clauses in any particular Boolean theory.  We conclude the series by discussing the techniques needed to implement our ideas, and by reporting on their performance on a variety of problem instances.\n    ",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2143",
        "title": "Ignorability in Statistical and Probabilistic Inference",
        "authors": [
            "M. Jaeger"
        ],
        "abstract": "When dealing with incomplete data in statistical learning, or incomplete observations in probabilistic inference, one needs to distinguish the fact that a certain event is observed from the fact that the observed event has happened. Since the modeling and computational complexities entailed by maintaining this proper distinction are often prohibitive, one asks for conditions under which it can be safely ignored. Such conditions are given by the missing at random (mar) and coarsened at random (car) assumptions. In this paper we provide an in-depth analysis of several questions relating to mar/car assumptions. Main purpose of our study is to provide criteria by which one may evaluate whether a car assumption is reasonable for a particular data collecting or observational process. This question is complicated by the fact that several distinct versions of mar/car assumptions exist. We therefore first provide an overview over these different versions, in which we highlight the distinction between distributional and coarsening variable induced versions. We show that distributional versions are less restrictive and sufficient for most applications. We then address from two different perspectives the question of when the mar/car assumption is warranted. First we provide a static analysis that characterizes the admissibility of the car assumption in terms of the support structure of the joint probability distribution of complete data and incomplete observations. Here we obtain an equivalence characterization that improves and extends a recent result by Grunwald and Halpern. We then turn to a procedural analysis that characterizes the admissibility of the car assumption in terms of procedural models for the actual data (or observation) generating process. The main result of this analysis is that the stronger coarsened completely at random (ccar) condition is arguably the most reasonable assumption, as it alone corresponds to data coarsening procedures that satisfy a natural robustness property. \n\n    ",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2145",
        "title": "Perseus: Randomized Point-based Value Iteration for POMDPs",
        "authors": [
            "M. T.J. Spaan",
            "N. Vlassis"
        ],
        "abstract": "Partially observable Markov decision processes (POMDPs) form an attractive and principled framework for agent planning under uncertainty.  Point-based approximate techniques for POMDPs compute a policy based on a finite set of points collected in advance from the agents belief space.  We present a randomized point-based value iteration algorithm called Perseus.  The algorithm performs approximate value backup stages, ensuring that in each backup stage the value of each point in the belief set is improved; the key observation is that a single backup may improve the value of many belief points.  Contrary to other point-based methods, Perseus backs up only a (randomly selected) subset of points in the belief set, sufficient for improving the value of each belief point in the set. We show how the same idea can be extended to dealing with continuous action spaces.  Experimental results show the potential of Perseus in large scale POMDP problems.\n    ",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2148",
        "title": "Logical Hidden Markov Models",
        "authors": [
            "L. De Raedt",
            "K. Kersting",
            "T. Raiko"
        ],
        "abstract": "Logical hidden Markov models (LOHMMs) upgrade traditional hidden Markov models to deal with sequences of structured symbols in the form of logical atoms, rather than flat characters.\nThis note formally introduces LOHMMs and presents solutions to the three central inference problems for LOHMMs: evaluation, most likely hidden state sequence and parameter estimation. The resulting representation and algorithms are experimentally evaluated on problems from the domain of bioinformatics.\n    ",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2153",
        "title": "mGPT: A Probabilistic Planner Based on Heuristic Search",
        "authors": [
            "B. Bonet",
            "H. Geffner"
        ],
        "abstract": "We describe the version of the GPT planner used in the probabilistic track of the 4th International Planning Competition (IPC-4). This version, called mGPT, solves Markov Decision Processes specified in the PPDDL language by extracting and using different classes of lower bounds along with various heuristic-search algorithms. The lower bounds are extracted from deterministic relaxations where the alternative probabilistic effects of an action are mapped into different, independent, deterministic actions. The heuristic-search algorithms use these lower bounds for focusing the updates and delivering a consistent value function over all states reachable from the initial state and the greedy policy. \n\n    ",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2154",
        "title": "Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators",
        "authors": [
            "A. Botea",
            "M. Enzenberger",
            "M. Mueller",
            "J. Schaeffer"
        ],
        "abstract": "Despite recent progress in AI planning, many benchmarks remain challenging for current planners. In many domains, the performance of a planner can greatly be improved by discovering and exploiting information about the domain structure that is not explicitly encoded in the initial PDDL formulation. In this paper we present and compare two automated methods that learn relevant information from previous experience in a domain and use it to solve new problem instances. Our methods share a common four-step strategy. First, a domain is analyzed and structural information is extracted, then macro-operators are generated based on the previously discovered structure. A filtering and ranking procedure selects the most useful macro-operators. Finally, the selected macros are used to speed up future searches.  We have successfully used such an approach in the fourth international planning competition IPC-4. Our system, Macro-FF, extends Hoffmanns state-of-the-art planner FF 2.3 with support for two kinds of macro-operators, and with engineering enhancements. We demonstrate the effectiveness of our ideas on benchmarks from international planning competitions. Our results indicate a large reduction in search effort in those complex domains where structural information can be inferred.   \n    ",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2155",
        "title": "Optiplan: Unifying IP-based and Graph-based Planning",
        "authors": [
            "S. Kambhampati",
            "M.H.L. van den Briel"
        ],
        "abstract": "The Optiplan planning system is the first integer programming-based planner that successfully participated in the international planning competition. This engineering note describes the architecture of Optiplan and provides the integer programming formulation that enabled it to perform reasonably well in the competition. We also touch upon some recent developments that make integer programming encodings significantly more competitive.\n    ",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2156",
        "title": "Approximate Policy Iteration with a Policy Language Bias: Solving Relational Markov Decision Processes",
        "authors": [
            "A. Fern",
            "R. Givan",
            "S. Yoon"
        ],
        "abstract": "We study an approach to policy selection for large relational Markov Decision Processes (MDPs). We consider a variant of approximate policy iteration (API) that replaces the usual value-function learning step with a learning step in policy space. This is advantageous in domains where good policies are easier to represent and learn than the corresponding value functions, which is often the case for the relational MDPs we are interested in. In order to apply API to such problems, we introduce a relational policy language and corresponding learner. In addition, we introduce a new bootstrapping routine for goal-based planning domains, based on random walks. Such bootstrapping is necessary for many large relational MDPs, where reward is extremely sparse, as API is ineffective in such domains when initialized with an uninformed policy. Our experiments show that the resulting system is able to find good policies for a number of classical planning domains and their stochastic variants by solving them as extremely large relational MDPs. The experiments also point to some limitations of our approach, suggesting future work.\n    ",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2271",
        "title": "Feature-Based Matrix Factorization",
        "authors": [
            "Tianqi Chen",
            "Zhao Zheng",
            "Qiuxia Lu",
            "Weinan Zhang",
            "Yong Yu"
        ],
        "abstract": "Recommender system has been more and more popular and widely used in many applications recently. The increasing information available, not only in quantities but also in types, leads to a big challenge for recommender system that how to leverage these rich information to get a better performance. Most traditional approaches try to design a specific model for each scenario, which demands great efforts in developing and modifying models. In this technical report, we describe our implementation of feature-based matrix factorization. This model is an abstract of many variants of matrix factorization models, and new types of information can be utilized by simply defining new features, without modifying any lines of code. Using the toolkit, we built the best single model reported on track 1 of KDDCup'11.\n    ",
        "submission_date": "2011-09-11T00:00:00",
        "last_modified_date": "2011-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2346",
        "title": "Linking Search Space Structure, Run-Time Dynamics, and Problem Difficulty: A Step Toward Demystifying Tabu Search",
        "authors": [
            "A. E. Howe",
            "J. P. Watson",
            "L. D. Whitley"
        ],
        "abstract": "Tabu search is one of the most effective heuristics for locating high-quality solutions to a diverse array of NP-hard combinatorial optimization problems. Despite the widespread success of tabu search, researchers have a poor understanding of many key theoretical aspects of this algorithm, including models of the high-level run-time dynamics and identification of those search space features that influence problem  difficulty. We consider these questions in the context of the job-shop  scheduling problem (JSP), a domain where tabu search algorithms have  been shown to be remarkably effective. Previously, we demonstrated  that the mean distance between random local optima and the nearest  optimal solution is highly correlated with problem difficulty for a  well-known tabu search algorithm for the JSP introduced by Taillard. In this paper, we discuss various shortcomings of this measure and  develop a new model of problem difficulty that corrects these deficiencies. We show that Taillards algorithm can be modeled  with high fidelity as a simple variant of a straightforward random  walk. The random walk model accounts for nearly all of the variability in the cost required to locate both optimal and sub-optimal solutions to random JSPs, and provides an explanation for differences in the difficulty of random versus structured JSPs. Finally, we discuss and  empirically substantiate two novel predictions regarding tabu search  algorithm behavior. First, the method for constructing the initial solution is  highly unlikely to impact the performance of tabu search. Second, tabu  tenure should be selected to be as small as possible while simultaneously  avoiding search stagnation; values larger than necessary lead to  significant degradations in performance.\n    ",
        "submission_date": "2011-09-11T00:00:00",
        "last_modified_date": "2011-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2347",
        "title": "Breaking Instance-Independent Symmetries In Exact Graph Coloring",
        "authors": [
            "F. A. Aloul",
            "I. L. Markov",
            "A. Ramani",
            "K. A. Sakallah"
        ],
        "abstract": "Code optimization and high level synthesis can be posed as constraint satisfaction and optimization problems,  such as graph coloring used in register allocation.  Graph coloring is also used to model more traditional CSPs relevant to AI, such as planning, time-tabling and scheduling.  Provably optimal solutions may be desirable for commercial and defense applications. Additionally, for applications such as register allocation and code optimization, naturally-occurring instances    of graph coloring are often small and can be solved optimally. A recent  wave of improvements in algorithms for Boolean satisfiability (SAT) and 0-1 Integer Linear Programming (ILP) suggests generic problem-reduction  methods, rather than problem-specific heuristics, because (1) heuristics may be upset by new constraints, (2) heuristics tend to ignore structure, and (3) many relevant problems are provably inapproximable. \nProblem reductions often lead to highly symmetric SAT instances, and symmetries are known to slow down SAT solvers. In this work, we compare several avenues for symmetry breaking, in particular when certain kinds of symmetry are present in all generated instances.  Our focus on reducing CSPs to SAT allows us to leverage recent dramatic improvement in SAT solvers and automatically benefit from future progress. We can use a variety of black-box SAT solvers  without modifying their source code because our symmetry-breaking techniques are static, i.e., we detect symmetries and add symmetry breaking predicates (SBPs) during pre-processing.\nAn important result of our work is that among the types  of instance-independent SBPs we studied and their combinations, the simplest and least complete constructions are the most effective. Our experiments also clearly indicate that instance-independent symmetries should mostly be processed together with instance-specific symmetries rather than at the specification level, contrary to what has been suggested in the literature.\n    ",
        "submission_date": "2011-09-11T00:00:00",
        "last_modified_date": "2011-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2355",
        "title": "Decision-Theoretic Planning with non-Markovian Rewards",
        "authors": [
            "C. Gretton",
            "F. Kabanza",
            "D. Price",
            "J. Slaney",
            "S. Thiebaux"
        ],
        "abstract": "A decision process in which rewards depend on history rather than merely on the current state is called a decision process with non-Markovian rewards (NMRDP). In decision-theoretic planning, where many desirable behaviours are more naturally expressed as properties of execution sequences rather than as properties of states, NMRDPs form a more natural model than the commonly adopted fully Markovian decision process (MDP) model. While the more tractable solution methods developed for MDPs do not directly apply in the presence of non-Markovian rewards, a number of solution methods for NMRDPs have been proposed in the literature. These all exploit a compact specification of the non-Markovian reward function in temporal logic, to automatically translate the NMRDP into an equivalent MDP which is solved using efficient MDP solution methods. This paper presents NMRDPP (Non-Markovian Reward Decision Process Planner), a software platform for the development and experimentation of methods for decision-theoretic planning with non-Markovian rewards. The current version of NMRDPP implements, under a single interface, a family of methods based on existing as well as new approaches which we describe in detail. These include dynamic programming, heuristic search, and structured methods. Using NMRDPP, we compare the methods and identify certain problem features that affect their performance. NMRDPPs treatment of non-Markovian rewards is inspired by the treatment of domain-specific search control knowledge in the TLPlan planner, which it incorporates as a special case. In the First International Probabilistic Planning Competition, NMRDPP was able to compete and perform well in both the domain-independent and hand-coded tracks, using search control knowledge in the latter. \n\n    ",
        "submission_date": "2011-09-11T00:00:00",
        "last_modified_date": "2011-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2752",
        "title": "On Validating Boolean Optimizers",
        "authors": [
            "Antonio Morgado",
            "Joao Marques-Silva"
        ],
        "abstract": "Boolean optimization finds a wide range of application domains, that motivated a number of different organizations of Boolean optimizers since the mid 90s. Some of the most successful approaches are based on iterative calls to an NP oracle, using either linear search, binary search or the identification of unsatisfiable sub-formulas. The increasing use of Boolean optimizers in practical settings raises the question of confidence in computed results. For example, the issue of confidence is paramount in safety critical settings. One way of increasing the confidence of the results computed by Boolean optimizers is to develop techniques for validating the results. Recent work studied the validation of Boolean optimizers based on branch-and-bound search. This paper complements existing work, and develops methods for validating Boolean optimizers that are based on iterative calls to an NP oracle. This entails implementing solutions for validating both satisfiable and unsatisfiable answers from the NP oracle. The work described in this paper can be applied to a wide range of Boolean optimizers, that find application in Pseudo-Boolean Optimization and in Maximum Satisfiability. Preliminary experimental results indicate that the impact of the proposed method in overall performance is negligible.\n    ",
        "submission_date": "2011-09-13T00:00:00",
        "last_modified_date": "2011-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.3094",
        "title": "On the use of reference points for the biobjective Inventory Routing Problem",
        "authors": [
            "Martin Josef Geiger",
            "Marc Sevaux"
        ],
        "abstract": "The article presents a study on the biobjective inventory routing problem. Contrary to most previous research, the problem is treated as a true multi-objective optimization problem, with the goal of identifying Pareto-optimal solutions. Due to the hardness of the problem at hand, a reference point based optimization approach is presented and implemented into an optimization and decision support system, which allows for the computation of a true subset of the optimal outcomes. Experimental investigation involving local search metaheuristics are conducted on benchmark data, and numerical results are reported and analyzed.\n    ",
        "submission_date": "2011-09-14T00:00:00",
        "last_modified_date": "2011-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.3313",
        "title": "Neigborhood Selection in Variable Neighborhood Search",
        "authors": [
            "Martin Josef Geiger",
            "Marc Sevaux",
            "Stefan Voss"
        ],
        "abstract": "Variable neighborhood search (VNS) is a metaheuristic for solving optimization problems based on a simple principle: systematic changes of neighborhoods within the search, both in the descent to local minima and in the escape from the valleys which contain them. Designing these neighborhoods and applying them in a meaningful fashion is not an easy task. Moreover, an appropriate order in which they are applied must be determined. In this paper we attempt to investigate this issue. Assume that we are given an optimization problem that is intended to be solved by applying the VNS scheme, how many and which types of neighborhoods should be investigated and what could be appropriate selection criteria to apply these neighborhoods. More specifically, does it pay to \"look ahead\" (see, e.g., in the context of VNS and GRASP) when attempting to switch from one neighborhood to another?\n    ",
        "submission_date": "2011-09-15T00:00:00",
        "last_modified_date": "2011-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.3532",
        "title": "A Characterization of the Combined Effects of Overlap and Imbalance on the SVM Classifier",
        "authors": [
            "Misha Denil",
            "Thomas Trappenberg"
        ],
        "abstract": "In this paper we demonstrate that two common problems in Machine Learning---imbalanced and overlapping data distributions---do not have independent effects on the performance of SVM classifiers. This result is notable since it shows that a model of either of these factors must account for the presence of the other. Our study of the relationship between these problems has lead to the discovery of a previously unreported form of \"covert\" overfitting which is resilient to commonly used empirical regularization techniques. We demonstrate the existance of this covert phenomenon through several methods based around the parametric regularization of trained SVMs. Our findings in this area suggest a possible approach to quantifying overlap in real world data sets.\n    ",
        "submission_date": "2011-09-16T00:00:00",
        "last_modified_date": "2011-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.3700",
        "title": "Contradiction measures and specificity degrees of basic belief assignments",
        "authors": [
            "Florentin Smarandache",
            "Arnaud Martin",
            "Christophe Osswald"
        ],
        "abstract": "In the theory of belief functions, many measures of uncertainty have been introduced. However, it is not always easy to understand what these measures really try to represent. In this paper, we re-interpret some measures of uncertainty in the theory of belief functions. We present some interests and drawbacks of the existing measures. On these observations, we introduce a measure of contradiction. Therefore, we present some degrees of non-specificity and Bayesianity of a mass. We propose a degree of specificity based on the distance between a mass and its most specific associated mass. We also show how to use the degree of specificity to measure the specificity of a fusion rule. Illustrations on simple examples are given.\n    ",
        "submission_date": "2011-09-16T00:00:00",
        "last_modified_date": "2011-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.3737",
        "title": "Learning where to Attend with Deep Architectures for Image Tracking",
        "authors": [
            "Misha Denil",
            "Loris Bazzani",
            "Hugo Larochelle",
            "Nando de Freitas"
        ],
        "abstract": "We discuss an attentional model for simultaneous object tracking and recognition that is driven by gaze data. Motivated by theories of perception, the model consists of two interacting pathways: identity and control, intended to mirror the what and where pathways in neuroscience models. The identity pathway models object appearance and performs classification using deep (factored)-Restricted Boltzmann Machines. At each point in time the observations consist of foveated images, with decaying resolution toward the periphery of the gaze. The control pathway models the location, orientation, scale and speed of the attended object. The posterior distribution of these states is estimated with particle filtering. Deeper in the control pathway, we encounter an attentional mechanism that learns to select gazes so as to minimize tracking uncertainty. Unlike in our previous work, we introduce gaze selection strategies which operate in the presence of partial information and on a continuous action space. We show that a straightforward extension of the existing approach to the partial information setting results in poor performance, and we propose an alternative method based on modeling the reward surface as a Gaussian Process. This approach gives good performance in the presence of partial information and allows us to expand the action space from a small, discrete set of fixation points to a continuous domain.\n    ",
        "submission_date": "2011-09-16T00:00:00",
        "last_modified_date": "2011-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.4335",
        "title": "Social choice rules driven by propositional logic",
        "authors": [
            "Rosa Camps",
            "Xavier Mora",
            "Laia Saumell"
        ],
        "abstract": "Several rules for social choice are examined from a unifying point of view that looks at them as procedures for revising a system of degrees of belief in accordance with certain specified logical constraints. Belief is here a social attribute, its degrees being measured by the fraction of people who share a given opinion. Different known rules and some new ones are obtained depending on which particular constraints are assumed. These constraints allow to model different notions of choiceness. In particular, we give a new method to deal with approval-disapproval-preferential voting.\n    ",
        "submission_date": "2011-07-28T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.4603",
        "title": "Explicit Approximations of the Gaussian Kernel",
        "authors": [
            "Andrew Cotter",
            "Joseph Keshet",
            "Nathan Srebro"
        ],
        "abstract": "We investigate training and using Gaussian kernel SVMs by approximating the kernel with an explicit finite- dimensional polynomial feature representation based on the Taylor expansion of the exponential. Although not as efficient as the recently-proposed random Fourier features [Rahimi and Recht, 2007] in terms of the number of features, we show how this polynomial representation can provide a better approximation in terms of the computational cost involved. This makes our \"Taylor features\" especially attractive for use on very large data sets, in conjunction with online or stochastic training.\n    ",
        "submission_date": "2011-09-21T00:00:00",
        "last_modified_date": "2011-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.4623",
        "title": "Outlier detection in default logics: the tractability/intractability frontier",
        "authors": [
            "F. Angiulli",
            "R. Ben-Eliyahu-Zohary",
            "L. Palopoli"
        ],
        "abstract": "In default theories, outliers denote sets of literals featuring unexpected properties. In previous papers, we have defined outliers in default logics and investigated their formal properties. Specifically, we have looked into the computational complexity of outlier detection problems and proved that while they are generally intractable, interesting tractable cases can be singled out. Following those results, we study here the tractability frontier in outlier detection problems, by analyzing it with respect to (i) the considered outlier detection problem, (ii) the reference default logic fragment, and (iii) the adopted notion of outlier. As for point (i), we shall consider three problems of increasing complexity, called Outlier-Witness Recognition, Outlier Recognition and Outlier Existence, respectively. As for point (ii), as we look for conditions under which outlier detection can be done efficiently, attention will be limited to subsets of Disjunction-free propositional default theories. As for point (iii), we shall refer to both the notion of outlier of [ABP08] and a new and more restrictive one, called strong outlier. After complexity results, we present a polynomial time algorithm for enumerating all strong outliers of bounded size in an quasi-acyclic normal unary default theory. Some of our tractability results rely on the Incremental Lemma that provides conditions for a deafult logic fragment to have a monotonic behavior. Finally, in order to show that the simple fragments of DL we deal with are still rich enough to solve interesting problems and, therefore, the tractability results that we prove are interesting not only on the mere theoretical side, insights into the expressive capabilities of these fragments are provided, by showing that normal unary theories express all NL queries, hereby indirectly answering a question raised by Kautz and Selman.\n    ",
        "submission_date": "2011-09-21T00:00:00",
        "last_modified_date": "2013-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.4684",
        "title": "Exhaustive and Efficient Constraint Propagation: A Semi-Supervised Learning Perspective and Its Applications",
        "authors": [
            "Zhiwu Lu",
            "Horace H.S. Ip",
            "Yuxin Peng"
        ],
        "abstract": "This paper presents a novel pairwise constraint propagation approach by decomposing the challenging constraint propagation problem into a set of independent semi-supervised learning subproblems which can be solved in quadratic time using label propagation based on k-nearest neighbor graphs. Considering that this time cost is proportional to the number of all possible pairwise constraints, our approach actually provides an efficient solution for exhaustively propagating pairwise constraints throughout the entire dataset. The resulting exhaustive set of propagated pairwise constraints are further used to adjust the similarity matrix for constrained spectral clustering. Other than the traditional constraint propagation on single-source data, our approach is also extended to more challenging constraint propagation on multi-source data where each pairwise constraint is defined over a pair of data points from different sources. This multi-source constraint propagation has an important application to cross-modal multimedia retrieval. Extensive results have shown the superior performance of our approach.\n    ",
        "submission_date": "2011-09-22T00:00:00",
        "last_modified_date": "2011-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.5072",
        "title": "Analysis of first prototype universal intelligence tests: evaluating and comparing AI algorithms and humans",
        "authors": [
            "Javier Insa-Cabrera",
            "Jose Hernandez-Orallo"
        ],
        "abstract": "Today, available methods that assess AI systems are focused on using empirical techniques to measure the performance of algorithms in some specific tasks (e.g., playing chess, solving mazes or land a helicopter). However, these methods are not appropriate if we want to evaluate the general intelligence of AI and, even less, if we compare it with human intelligence. The ANYNT project has designed a new method of evaluation that tries to assess AI systems using well known computational notions and problems which are as general as possible. This new method serves to assess general intelligence (which allows us to learn how to solve any new kind of problem we face) and not only to evaluate performance on a set of specific tasks. This method not only focuses on measuring the intelligence of algorithms, but also to assess any intelligent system (human beings, animals, AI, aliens?,...), and letting us to place their results on the same scale and, therefore, to be able to compare them. This new approach will allow us (in the future) to evaluate and compare any kind of intelligent system known or even to build/find, be it artificial or biological. This master thesis aims at ensuring that this new method provides consistent results when evaluating AI algorithms, this is done through the design and implementation of prototypes of universal intelligence tests and their application to different intelligent systems (AI algorithms and humans beings). From the study we analyze whether the results obtained by two different intelligent systems are properly located on the same scale and we propose changes and refinements to these prototypes in order to, in the future, being able to achieve a truly universal intelligence test.\n    ",
        "submission_date": "2011-09-23T00:00:00",
        "last_modified_date": "2011-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.5663",
        "title": "The Deterministic Part of IPC-4: An Overview",
        "authors": [
            "S. Edelkamp",
            "J. Hoffmann"
        ],
        "abstract": "We provide an overview of the organization and results of the deterministic part of the 4th International Planning Competition, i.e., of the part concerned with evaluating systems doing deterministic planning. IPC-4 attracted even more competing systems than its already large predecessors, and the competition event was revised in several important respects. After giving an introduction to the IPC, we briefly explain the main differences between the deterministic part of IPC-4 and its predecessors. We then introduce formally the language used, called PDDL2.2 that extends PDDL2.1 by derived predicates and timed initial literals. We list the competing systems and overview the results of the competition. The entire set of data is far too large to be presented in full. We provide a detailed summary; the complete data is available in an online appendix. We explain how we awarded the competition prizes.  \n    ",
        "submission_date": "2011-09-26T00:00:00",
        "last_modified_date": "2011-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.5665",
        "title": "PDDL2.1 - The Art of the Possible? Commentary on Fox and Long",
        "authors": [
            "D. McDermott"
        ],
        "abstract": "PDDL2.1 was designed to push the envelope of what planning algorithms can do, and it has succeeded. It adds two important features: durative actions,which take time (and may have continuous effects); and objective functions for measuring the quality of plans. The concept of durative actions is flawed; and the treatment of their semantics reveals too strong an attachment to the way many contemporary planners work. Future PDDL innovators should focus on producing a clean semantics for additions to the language, and let planner implementers worry about coupling their algorithms to problems expressed in the latest version of the language.\n    ",
        "submission_date": "2011-09-26T00:00:00",
        "last_modified_date": "2011-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.5666",
        "title": "The Case for Durative Actions: A Commentary on PDDL2.1",
        "authors": [
            "D. E. Smith"
        ],
        "abstract": "The addition of durative actions to PDDL2.1 sparked some controversy. Fox and Long argued that actions should be considered as instantaneous, but can start and stop processes.  Ultimately, a limited notion of durative actions was incorporated into the language. I argue that this notion is still impoverished, and that the underlying philosophical position of regarding durative actions as being a shorthand for a start action, process, and stop action ignores the realities of modelling and execution for complex systems.\n    ",
        "submission_date": "2011-09-26T00:00:00",
        "last_modified_date": "2011-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.5711",
        "title": "Engineering a Conformant Probabilistic Planner",
        "authors": [
            "L. Li",
            "N. Onder",
            "G. C. Whelan"
        ],
        "abstract": "We present a partial-order, conformant, probabilistic planner, Probapop which competed in the blind track of the Probabilistic Planning Competition in IPC-4. We explain how we adapt distance based heuristics for use with probabilistic domains. Probapop also incorporates heuristics based on probability of success. We explain the successes and difficulties encountered during the design and implementation of Probapop. \n    ",
        "submission_date": "2011-09-26T00:00:00",
        "last_modified_date": "2011-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.5713",
        "title": "Where 'Ignoring Delete Lists' Works: Local Search Topology in Planning Benchmarks",
        "authors": [
            "J. Hoffmann"
        ],
        "abstract": "Between 1998 and 2004, the planning community has seen vast progress in terms of the sizes of benchmark examples that domain-independent planners can tackle successfully. The key technique behind this progress is the use of heuristic functions based on relaxing the planning task at hand, where the relaxation is to assume that all delete lists are empty. The unprecedented success of such methods, in many commonly used benchmark examples, calls for an understanding of what classes of domains these methods are well suited for.   In the investigation at hand, we derive a formal background to such an understanding. We perform a case study covering a range of 30 commonly used STRIPS and ADL benchmark domains, including all examples used in the first four international planning competitions. We *prove* connections between domain structure and local search topology -- heuristic cost surface properties -- under an idealized version of the heuristic functions used in modern planners. The idealized heuristic function is called h^+, and differs from the practically used functions in that it returns the length of an *optimal* relaxed plan, which is NP-hard to compute. We identify several key characteristics of the topology under h^+, concerning the existence/non-existence of unrecognized dead ends, as well as the existence/non-existence of constant upper bounds on the difficulty of escaping local minima and benches. These distinctions divide the (set of all) planning domains into a taxonomy of classes of varying h^+ topology. As it turns out, many of the 30 investigated domains lie in classes with a relatively easy topology. Most particularly, 12 of the domains lie in classes where FFs search algorithm, provided with h^+, is a polynomial solving mechanism.   We also present results relating h^+ to its approximation as implemented in FF. The behavior regarding dead ends is provably the same. We summarize the results of an empirical investigation showing that, in many domains, the topological qualities of h^+ are largely inherited by the approximation. The overall investigation gives a rare example of a successful analysis of the connections between typical-case problem structure, and search performance. The theoretical investigation also gives hints on how the topological phenomena might be automatically recognizable by domain analysis techniques. We outline some preliminary steps we made into that direction. \n    ",
        "submission_date": "2011-09-26T00:00:00",
        "last_modified_date": "2011-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.5714",
        "title": "Binary Encodings of Non-binary Constraint Satisfaction Problems: Algorithms and Experimental Results",
        "authors": [
            "N. Samaras",
            "K. Stergiou"
        ],
        "abstract": "A non-binary Constraint Satisfaction Problem (CSP) can be solved directly using extended versions of binary techniques. Alternatively, the non-binary problem can be translated into an equivalent binary one. In this case, it is generally accepted that the translated problem can be solved by applying well-established techniques for binary CSPs. In this paper we evaluate the applicability of the latter approach. We demonstrate that the use of standard techniques for binary CSPs in the encodings of non-binary problems is problematic and results in models that are very rarely competitive with the non-binary representation. To overcome this, we propose specialized arc consistency and search algorithms for binary encodings, and we evaluate them theoretically and empirically. We consider three binary representations; the hidden variable encoding, the dual encoding, and the double encoding. Theoretical and empirical results show that, for certain classes of non-binary constraints, binary encodings are a competitive option, and in many cases, a better one than the non-binary representation. \n    ",
        "submission_date": "2011-09-26T00:00:00",
        "last_modified_date": "2011-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.5716",
        "title": "Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web",
        "authors": [
            "P. Adjiman",
            "P. Chatalic",
            "F. Goasdoue",
            "M. C. Rousset",
            "L. Simon"
        ],
        "abstract": "In a peer-to-peer inference system, each peer can reason locally but can also solicit some of its acquaintances, which are peers sharing part of its vocabulary. In this paper, we consider peer-to-peer inference systems in which the local theory of each peer is a set of propositional clauses defined upon a local vocabulary. An important characteristic of peer-to-peer inference systems is that the global theory (the union of all peer theories) is not known (as opposed to partition-based reasoning systems). The main contribution of this paper is to provide the first consequence finding algorithm in a peer-to-peer setting: DeCA. It is anytime and computes consequences gradually from the solicited peer to peers that are more and more distant. We exhibit a sufficient condition on the acquaintance graph of the peer-to-peer inference system for guaranteeing the completeness of this algorithm. Another important contribution is to apply this general distributed reasoning setting to the setting of the Semantic Web through the Somewhere semantic peer-to-peer data management system. The last contribution of this paper is to provide an experimental analysis of the scalability of the peer-to-peer infrastructure that we propose, on large networks of 1000 peers. \n    ",
        "submission_date": "2011-09-26T00:00:00",
        "last_modified_date": "2011-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.5717",
        "title": "Dynamic Local Search for the Maximum Clique Problem",
        "authors": [
            "H. H. Hoos",
            "W. Pullan"
        ],
        "abstract": "In this paper, we introduce DLS-MC, a new stochastic local search algorithm for the maximum clique problem. DLS-MC alternates between phases of iterative improvement, during which suitable vertices are added to the current clique, and plateau search, during which vertices of the current clique are swapped with vertices not contained in the current clique. The selection of vertices is solely based on vertex penalties that are dynamically adjusted during the search, and a perturbation mechanism is used to overcome search stagnation. The behaviour of DLS-MC is controlled by a single parameter, penalty delay, which controls the frequency at which vertex penalties are reduced. We show empirically that DLS-MC achieves substantial performance improvements over state-of-the-art algorithms for the maximum clique problem over a large range of the commonly used DIMACS benchmark instances.\n    ",
        "submission_date": "2011-09-26T00:00:00",
        "last_modified_date": "2011-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.5732",
        "title": "Representing Conversations for Scalable Overhearing",
        "authors": [
            "G. Gutnik",
            "G. A. Kaminka"
        ],
        "abstract": "Open distributed multi-agent systems are gaining interest in the academic community and in industry. In such open settings, agents are often coordinated using standardized agent conversation protocols. The representation of such protocols (for analysis, validation, monitoring, etc) is an important aspect of multi-agent applications. Recently, Petri nets have been shown to be an interesting approach to such representation, and radically different approaches using Petri nets have been proposed. However, their relative strengths and weaknesses have not been examined. Moreover, their scalability and suitability for different tasks have not been addressed. This paper addresses both these challenges. First, we analyze existing Petri net representations in terms of their scalability and appropriateness for overhearing, an important task in monitoring open multi-agent systems. Then, building on the insights gained, we introduce a novel representation using Colored Petri nets that explicitly represent legal joint conversation states and messages. This representation approach offers significant improvements in scalability and is particularly suitable for overhearing. Furthermore, we show that this new representation offers a comprehensive coverage of all conversation features of FIPA conversation standards. We also present a procedure for transforming AUML conversation protocol diagrams (a standard human-readable representation), to our Colored Petri net representation. \n\n    ",
        "submission_date": "2011-09-26T00:00:00",
        "last_modified_date": "2011-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.5750",
        "title": "Improving Heuristics Through Relaxed Search - An Analysis of TP4 and HSP*a in the 2004 Planning Competition",
        "authors": [
            "P. Haslum"
        ],
        "abstract": "The hm admissible heuristics for (sequential and temporal) regression planning are defined by a parameterized relaxation of the optimal cost function in the regression search space, where the parameter m offers a trade-off between the accuracy and computational cost of theheuristic. Existing methods for computing the hm heuristic require time exponential in m, limiting them to small values (m andlt= 2). The hm heuristic can also be viewed as the optimal cost function in a relaxation of the search space: this paper presents relaxed search, a method for computing this function partially by searching in the relaxed space. The relaxed search method, because it computes hm only partially, is computationally cheaper and therefore usable for higher values of m. The (complete) hm heuristic is combined with partial hm heuristics, for m = 3,..., computed by relaxed search, resulting in a more accurate heuristic.\nThis use of the relaxed search method to improve on the hm heuristic is evaluated by comparing two optimal temporal planners: TP4, which does not use it, and HSP*a, which uses it but is otherwise identical to TP4. The comparison is made on the domains used in the 2004 International Planning Competition, in which both planners participated. Relaxed search is found to be cost effective in some of these domains, but not all. Analysis reveals a characterization of the domains in which relaxed search can be expected to be cost effective, in terms of two measures on the original and relaxed search spaces. In the domains where relaxed search is cost effective, expanding small states is computationally cheaper than expanding large states and small states tend to have small successor states.\n\n    ",
        "submission_date": "2011-09-27T00:00:00",
        "last_modified_date": "2011-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.5920",
        "title": "Models and Strategies for Variants of the Job Shop Scheduling Problem",
        "authors": [
            "Diarmuid Grimes",
            "Emmanuel Hebrard"
        ],
        "abstract": "Recently, a variety of constraint programming and Boolean satisfiability approaches to scheduling problems have been introduced. They have in common the use of relatively simple propagation mechanisms and an adaptive way to focus on the most constrained part of the problem. In some cases, these methods compare favorably to more classical constraint programming methods relying on propagation algorithms for global unary or cumulative resource constraints and dedicated search heuristics. In particular, we described an approach that combines restarting, with a generic adaptive heuristic and solution guided branching on a simple model based on a decomposition of disjunctive constraints. In this paper, we introduce an adaptation of this technique for an important subclass of job shop scheduling problems (JSPs), where the objective function involves minimization of earliness/tardiness costs. We further show that our technique can be improved by adding domain specific information for one variant of the JSP (involving time lag constraints). In particular we introduce a dedicated greedy heuristic, and an improved model for the case where the maximal time lag is 0 (also referred to as no-wait JSPs).\n    ",
        "submission_date": "2011-09-27T00:00:00",
        "last_modified_date": "2011-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.5951",
        "title": "An Approximation of the Universal Intelligence Measure",
        "authors": [
            "Shane Legg",
            "Joel Veness"
        ],
        "abstract": "The Universal Intelligence Measure is a recently proposed formal definition of intelligence. It is mathematically specified, extremely general, and captures the essence of many informal definitions of intelligence. It is based on Hutter's Universal Artificial Intelligence theory, an extension of Ray Solomonoff's pioneering work on universal induction. Since the Universal Intelligence Measure is only asymptotically computable, building a practical intelligence test from it is not straightforward. This paper studies the practical issues involved in developing a real-world UIM-based performance metric. Based on our investigation, we develop a prototype implementation which we use to evaluate a number of different artificial agents.\n    ",
        "submission_date": "2011-09-27T00:00:00",
        "last_modified_date": "2011-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6029",
        "title": "An Improved Search Algorithm for Optimal Multiple-Sequence Alignment",
        "authors": [
            "S. Schroedl"
        ],
        "abstract": "Multiple sequence alignment (MSA) is a ubiquitous problem in computational biology. Although it is NP-hard to find an optimal solution for an arbitrary number of sequences, due to the importance of this problem researchers are trying to push the limits of exact algorithms further. Since MSA can be cast as a classical path finding problem, it is attracting a growing number of AI researchers interested in heuristic search algorithms as a challenge with actual practical relevance.  In this paper, we first review two previous, complementary lines of research. Based on Hirschbergs algorithm, Dynamic Programming needs O(kN^(k-1)) space to store both the search frontier and the nodes needed to reconstruct the solution path, for k sequences of length N. Best first search, on the other hand, has the advantage of bounding the search space that has to be explored using a heuristic.  However, it is necessary to maintain all explored nodes up to the final solution in order to prevent the search from re-expanding them at higher cost.  Earlier approaches to reduce the Closed list are either incompatible with pruning methods for the Open list, or must retain at least the boundary of the Closed list.  In this article, we present an algorithm that attempts at combining the respective advantages; like A* it uses a heuristic for pruning the search space, but reduces both the maximum Open and Closed size to O(kN^(k-1)), as in Dynamic Programming.  The underlying idea is to conduct a series of searches with successively increasing upper bounds, but using the DP ordering as the key for the Open priority queue.  With a suitable choice of thresholds, in practice, a running time below four times that of A* can be expected.  In our experiments we show that our algorithm outperforms one of the currently most successful algorithms for optimal multiple sequence alignments, Partial Expansion A*, both in time and memory. Moreover, we apply a refined heuristic based on optimal alignments not only of pairs of sequences, but of larger subsets.  This idea is not new; however, to make it practically relevant we show that it is equally important to bound the heuristic computation appropriately, or the overhead can obliterate any possible gain.  Furthermore, we discuss a number of improvements in time and space efficiency with regard to practical implementations.  Our algorithm, used in conjunction with higher-dimensional heuristics, is able to calculate for the first time the optimal alignment for almost all of the problems in Reference 1 of the benchmark database BAliBASE.\n    ",
        "submission_date": "2011-09-27T00:00:00",
        "last_modified_date": "2011-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6030",
        "title": "Probabilistic Hybrid Action Models for Predicting Concurrent Percept-driven Robot Behavior",
        "authors": [
            "M. Beetz",
            "H. Grosskreutz"
        ],
        "abstract": "This article develops Probabilistic Hybrid Action Models (PHAMs), a realistic causal model for predicting the behavior generated by modern percept-driven robot plans. PHAMs represent aspects of robot behavior that cannot be represented by most action models used in AI planning: the temporal structure of continuous control processes, their non-deterministic effects, several modes of their interferences, and the achievement of triggering conditions in closed-loop robot plans. \nThe main contributions of this article are: (1) PHAMs, a model of concurrent percept-driven behavior, its formalization, and proofs that the model generates probably, qualitatively accurate predictions; and (2) a resource-efficient inference method for PHAMs based on sampling projections from probabilistic action models and state descriptions. We show how PHAMs can be applied to planning the course of action of an autonomous robot office courier based on analytical and experimental results. \n\n    ",
        "submission_date": "2011-09-27T00:00:00",
        "last_modified_date": "2011-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6033",
        "title": "Generative Prior Knowledge for Discriminative Classification",
        "authors": [
            "G. DeJong",
            "A. Epshteyn"
        ],
        "abstract": "We present a novel framework for integrating prior knowledge into discriminative classifiers.  Our framework allows discriminative classifiers such as Support Vector Machines (SVMs) to utilize prior knowledge specified in the generative setting. The dual objective of fitting the data and respecting prior knowledge is formulated as a bilevel program, which is solved (approximately) via iterative application of second-order cone programming.  To test our approach, we consider the problem of using WordNet (a semantic database of English language) to improve low-sample classification accuracy of newsgroup categorization.  WordNet is viewed as an approximate, but readily available source of background knowledge, and our framework is capable of utilizing it in a flexible way.\n\n    ",
        "submission_date": "2011-09-27T00:00:00",
        "last_modified_date": "2011-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6051",
        "title": "The Fast Downward Planning System",
        "authors": [
            "M. Helmert"
        ],
        "abstract": "Fast Downward is a classical planning system based on heuristic search. It can deal with general deterministic planning problems encoded in the propositional fragment of PDDL2.2, including advanced features like ADL conditions and effects and derived predicates (axioms). Like other well-known planners such as HSP and FF, Fast Downward is a progression planner, searching the space of world states of a planning task in the forward direction. However, unlike other PDDL planning systems, Fast Downward does not use the propositional PDDL representation of a planning task directly. Instead, the input is first translated into an alternative representation called multi-valued planning tasks, which makes many of the implicit constraints of a propositional planning task explicit. Exploiting this alternative representation, Fast Downward uses hierarchical decompositions of planning tasks for computing its heuristic function, called the causal graph heuristic, which is very different from traditional HSP-like heuristics based on ignoring negative interactions of operators.\nIn this article, we give a full account of Fast Downwards approach to solving multi-valued planning tasks. We extend our earlier discussion of the causal graph heuristic to tasks involving axioms and conditional effects and present some novel techniques for search control that are used within Fast Downwards best-first search algorithm: preferred operators transfer the idea of helpful actions from local search to global best-first search, deferred evaluation of heuristic functions mitigates the negative effect of large branching factors on search performance, and multi-heuristic best-first search combines several heuristic evaluation functions within a single search algorithm in an orthogonal way. We also describe efficient data structures for fast state expansion (successor generators and axiom evaluators) and present a new non-heuristic search algorithm called focused iterative-broadening search, which utilizes the information encoded in causal graphs in a novel way.\nFast Downward has proven remarkably successful: It won the \"classical (i.e., propositional, non-optimising) track of the 4th International Planning Competition at ICAPS 2004, following in the footsteps of planners such as FF and LPG. Our experiments show that it also performs very well on the benchmarks of the earlier planning competitions and provide some insights about the usefulness of the new search enhancements.\n\n    ",
        "submission_date": "2011-09-27T00:00:00",
        "last_modified_date": "2011-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6052",
        "title": "Asynchronous Partial Overlay: A New Algorithm for Solving Distributed Constraint Satisfaction Problems",
        "authors": [
            "V. R. Lesser",
            "R. Mailler"
        ],
        "abstract": "Distributed Constraint Satisfaction (DCSP) has long been considered an important problem in multi-agent systems research.  This is because many real-world problems can be represented as constraint satisfaction and these problems often present themselves in a distributed form.  In this article, we present a new complete, distributed algorithm called Asynchronous Partial Overlay (APO) for solving DCSPs that is based on a cooperative mediation process.  The primary ideas behind this algorithm are that agents, when acting as a mediator, centralize small, relevant portions of the DCSP, that these centralized subproblems overlap, and that agents increase the size of their subproblems along critical paths within the DCSP as the problem solving unfolds.  We present empirical evidence that shows that APO outperforms other known, complete DCSP techniques.\n    ",
        "submission_date": "2011-09-27T00:00:00",
        "last_modified_date": "2011-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6112",
        "title": "A Visual Entity-Relationship Model for Constraint-Based University Timetabling",
        "authors": [
            "Islam Abdelraouf",
            "Slim Abdennadher",
            "Carmen Gervet"
        ],
        "abstract": "University timetabling (UTT) is a complex problem due to its combinatorial nature but also the type of constraints involved. The holy grail of (constraint) programming: \"the user states the problem the program solves it\" remains a challenge since solution quality is tightly coupled with deriving \"effective models\", best handled by technology experts. In this paper, focusing on the field of university timetabling, we introduce a visual graphic communication tool that lets the user specify her problem in an abstract manner, using a visual entity-relationship model. The entities are nodes of mainly two types: resource nodes (lecturers, assistants, student groups) and events nodes (lectures, lab sessions, tutorials). The links between the nodes signify a desired relationship between them. The visual modeling abstraction focuses on the nature of the entities and their relationships and abstracts from an actual constraint model.\n    ",
        "submission_date": "2011-09-28T00:00:00",
        "last_modified_date": "2011-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6344",
        "title": "Admissible and Restrained Revision",
        "authors": [
            "R. Booth",
            "T. Meyer"
        ],
        "abstract": "As partial justification of their framework for iterated belief revision Darwiche and Pearl convincingly argued against Boutiliers natural revision and provided a prototypical revision operator that fits into their scheme.  We show that the Darwiche-Pearl arguments lead naturally to the acceptance of a smaller class of operators which we refer to as admissible. Admissible revision ensures that the penultimate input is not ignored completely, thereby eliminating natural revision, but includes the Darwiche-Pearl operator, Nayaks lexicographic revision operator, and a newly introduced operator called restrained revision. We demonstrate that restrained revision is the most conservative of admissible revision operators,  effecting as few changes as possible, while lexicographic revision is the least conservative, and point out that restrained revision can also be viewed as a composite operator, consisting of natural revision preceded by an application of a \"backwards revision\" operator previously studied by Papini. Finally, we propose the establishment of a principled approach for choosing an appropriate revision operator in different contexts and discuss future work. \n    ",
        "submission_date": "2011-09-28T00:00:00",
        "last_modified_date": "2011-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6345",
        "title": "On Graphical Modeling of Preference and Importance",
        "authors": [
            "R. I. Brafman",
            "C. Domshlak",
            "S. E. Shimony"
        ],
        "abstract": "In recent years, CP-nets have emerged as a useful tool for supporting preference elicitation, reasoning, and representation. CP-nets capture and support reasoning with qualitative conditional preference statements, statements that are relatively natural for users to express. In this paper, we extend the CP-nets formalism to handle another class of very natural qualitative statements one often uses in expressing preferences in daily life - statements of relative importance of attributes. The resulting formalism, TCP-nets, maintains the spirit of CP-nets, in that it remains focused on using only simple and natural preference statements, uses the ceteris paribus semantics, and utilizes a graphical representation of this information to reason about its consistency and to perform, possibly constrained, optimization using it. The extra expressiveness it provides allows us to better model tradeoffs users would like to make, more faithfully representing their preferences.\n    ",
        "submission_date": "2011-09-28T00:00:00",
        "last_modified_date": "2011-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6346",
        "title": "The Planning Spectrum - One, Two, Three, Infinity",
        "authors": [
            "M. Pistore",
            "M. Y. Vardi"
        ],
        "abstract": "Linear Temporal Logic (LTL) is widely used for defining conditions on the execution paths of dynamic systems.  In the case of dynamic systems that allow for nondeterministic evolutions, one has to specify, along with an LTL formula f, which are the paths that are required to satisfy the formula.  Two extreme cases are the universal interpretation A.f, which requires that the formula be satisfied for all execution paths, and the existential interpretation E.f, which requires that the formula be satisfied for some execution path.\n",
        "submission_date": "2011-09-28T00:00:00",
        "last_modified_date": "2011-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6348",
        "title": "Fault Tolerant Boolean Satisfiability",
        "authors": [
            "A. Roy"
        ],
        "abstract": "A delta-model is a satisfying assignment of a Boolean formula for which any small alteration, such as a single bit flip, can be repaired by flips to some small number of other bits, yielding a new satisfying assignment.  These satisfying assignments represent robust solutions to optimization problems (e.g., scheduling) where it is possible to recover from unforeseen events (e.g., a resource becoming unavailable).  The concept of delta-models was introduced by Ginsberg, Parkes and Roy (AAAI 1998), where it was proved that finding delta-models for general Boolean formulas is NP-complete.  In this paper, we extend that result by studying the complexity of finding delta-models for classes of Boolean formulas which are known to have polynomial time satisfiability solvers.  In particular, we examine 2-SAT, Horn-SAT, Affine-SAT, dual-Horn-SAT, 0-valid and 1-valid SAT.  We see a wide variation in the complexity of finding delta-models, e.g., while 2-SAT and Affine-SAT have polynomial time tests for delta-models, testing whether a Horn-SAT formula has one is NP-complete.\n    ",
        "submission_date": "2011-09-28T00:00:00",
        "last_modified_date": "2011-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6361",
        "title": "Cognitive Principles in Robust Multimodal Interpretation",
        "authors": [
            "J. Y. Chai",
            "Z. Prasov",
            "S. Qu"
        ],
        "abstract": "Multimodal conversational interfaces provide a natural means for users to communicate with computer systems through multiple modalities such as speech and gesture. To build effective multimodal interfaces, automated interpretation of user multimodal inputs is important. Inspired by the previous investigation on cognitive status in multimodal human machine interaction, we have developed a greedy algorithm for interpreting user referring expressions (i.e., multimodal reference resolution). This algorithm incorporates the cognitive principles of Conversational Implicature and Givenness Hierarchy and applies constraints from various sources (e.g., temporal, semantic, and contextual) to resolve references. Our empirical results have shown the advantage of this algorithm in efficiently resolving a variety of user references. Because of its simplicity and generality, this approach has the potential to improve the robustness of multimodal input interpretation.\n    ",
        "submission_date": "2011-09-28T00:00:00",
        "last_modified_date": "2011-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6618",
        "title": "Multiple-Goal Heuristic Search",
        "authors": [
            "D. Davidov",
            "S. Markovitch"
        ],
        "abstract": "This paper presents a new framework for anytime heuristic search where the task is to achieve as many goals as possible within the allocated resources. We show the inadequacy of traditional distance-estimation heuristics for tasks of this type and present alternative heuristics that are more appropriate for multiple-goal search.  In particular, we introduce the marginal-utility heuristic, which estimates the cost and the benefit of exploring a subtree below a search node. We developed two methods for online learning of the marginal-utility heuristic. One is based on local similarity of the partial marginal utility of sibling nodes, and the other generalizes marginal-utility over the state feature space. We apply our adaptive and non-adaptive multiple-goal search algorithms to several problems, including focused crawling, and show their superiority over existing methods.\n    ",
        "submission_date": "2011-09-29T00:00:00",
        "last_modified_date": "2011-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6621",
        "title": "FluCaP: A Heuristic Search Planner for First-Order MDPs",
        "authors": [
            "S. Hoelldobler",
            "E. Karabaev",
            "O. Skvortsova"
        ],
        "abstract": "We present a heuristic search algorithm for solving first-order Markov Decision Processes (FOMDPs). Our approach combines first-order state abstraction that avoids evaluating states individually, and heuristic search that avoids evaluating all states. Firstly, in contrast to existing systems, which start with propositionalizing the FOMDP and then perform state abstraction on its propositionalized version we apply state abstraction directly on the FOMDP avoiding propositionalization. This kind of abstraction is referred to as first-order state abstraction. Secondly, guided by an admissible heuristic, the search is restricted to those states that are reachable from the initial state. We demonstrate the usefulness of the above techniques for solving FOMDPs with a system, referred to as FluCaP (formerly, FCPlanner), that entered the probabilistic track of the 2004 International Planning Competition (IPC2004) and demonstrated an advantage over other planners on the problems represented in first-order terms.\n\n    ",
        "submission_date": "2011-09-29T00:00:00",
        "last_modified_date": "2011-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6841",
        "title": "Learning Dependency-Based Compositional Semantics",
        "authors": [
            "Percy Liang",
            "Michael I. Jordan",
            "Dan Klein"
        ],
        "abstract": "Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Semantic parsers are typically trained from examples of questions annotated with their target logical forms, but this type of annotation is expensive.\n",
        "submission_date": "2011-09-30T00:00:00",
        "last_modified_date": "2011-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0020",
        "title": "Causes of Ineradicable Spurious Predictions in Qualitative Simulation",
        "authors": [
            "\u00d6. Y\u0131lmaz",
            "A. C. C. Say"
        ],
        "abstract": "It was recently proved that a sound and complete qualitative simulator does not exist, that is, as long as the input-output vocabulary of the state-of-the-art QSIM algorithm is used, there will always be input models which cause any simulator with a coverage guarantee to make spurious predictions in its output. In this paper, we examine whether a meaningfully expressive restriction of this vocabulary is possible so that one can build a simulator with both the soundness and completeness properties. We prove several negative results: All sound qualitative simulators, employing subsets of the QSIM representation which retain the operating region transition feature, and support at least the addition and constancy constraints, are shown to be inherently incomplete. Even when the simulations are restricted to run in a single operating region, a constraint vocabulary containing just the addition, constancy, derivative, and multiplication relations makes the construction of sound and complete qualitative simulators impossible.\n    ",
        "submission_date": "2011-09-30T00:00:00",
        "last_modified_date": "2011-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0023",
        "title": "Properties and Applications of Programs with Monotone and Convex Constraints",
        "authors": [
            "L. Liu",
            "M. Truszczynski"
        ],
        "abstract": "We study properties of programs with monotone and convex constraints. We extend to these formalisms concepts and results from normal logic programming. They include the notions of strong and uniform equivalence with their characterizations, tight programs and Fages Lemma, program completion and loop formulas. Our results provide an abstract account of properties of some recent extensions of logic programming with aggregates, especially the formalism of lparse programs. They imply a method to compute stable models of lparse programs by means of off-the-shelf solvers of pseudo-boolean constraints, which is often much faster than the smodels system.\n    ",
        "submission_date": "2011-09-30T00:00:00",
        "last_modified_date": "2011-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0024",
        "title": "How the Landscape of Random Job Shop Scheduling Instances Depends on the Ratio of Jobs to Machines",
        "authors": [
            "S. F. Smith",
            "M. J. Streeter"
        ],
        "abstract": "We characterize the search landscape of random instances of the job shop scheduling problem (JSP).  Specifically, we investigate how the expected values of (1) backbone size, (2) distance between near-optimal schedules, and (3) makespan of random schedules vary as a function of the job to machine ratio (N/M).  For the limiting cases N/M approaches 0 and N/M approaches infinity we provide analytical results, while for intermediate values of N/M we perform experiments.  We prove that as N/M approaches 0, backbone size approaches 100%, while as N/M approaches infinity the backbone vanishes.  In the process we show that as N/M approaches 0 (resp. N/M approaches infinity), simple priority rules almost surely generate an optimal schedule, providing theoretical evidence of an \"easy-hard-easy\" pattern of typical-case instance difficulty in job shop scheduling.  We also draw connections between our theoretical results and the \"big valley\" picture of JSP landscapes.\n    ",
        "submission_date": "2011-09-30T00:00:00",
        "last_modified_date": "2011-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0026",
        "title": "Preference-based Search using Example-Critiquing with Suggestions",
        "authors": [
            "B. Faltings",
            "P. Pu",
            "P. Viappiani"
        ],
        "abstract": "We consider interactive tools that help users search for their most preferred item in a large collection of options. In particular, we examine example-critiquing, a technique for enabling users to incrementally construct preference models by critiquing example options that are presented to them. We present novel techniques for improving the example-critiquing technology by adding suggestions to its displayed options. Such suggestions are calculated based on an analysis of users current preference model and their potential hidden preferences. We evaluate the performance of our model-based suggestion techniques with both synthetic and real users.  Results show that such suggestions are highly attractive to users and can stimulate them to express more preferences to improve the chance of identifying their most preferred item by up to 78%.\n    ",
        "submission_date": "2011-09-30T00:00:00",
        "last_modified_date": "2011-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0027",
        "title": "Anytime Point-Based Approximations for Large POMDPs",
        "authors": [
            "J. Pineau",
            "G. Gordon",
            "S. Thrun"
        ],
        "abstract": "The Partially Observable Markov Decision Process has long been recognized as a rich framework for real-world planning and control problems, especially in robotics. However exact solutions in this framework are typically computationally intractable for all but the smallest problems. A well-known technique for speeding up POMDP solving involves performing value backups at specific belief points, rather than over the entire belief simplex. The efficiency of this approach, however, depends greatly on the selection of points. This paper presents a set of novel techniques for selecting informative belief points which work well in practice. The point selection procedure is combined with point-based value backups to form an effective anytime POMDP algorithm called Point-Based Value Iteration (PBVI). The first aim of this paper is to introduce this algorithm and present a theoretical analysis justifying the choice of belief selection technique. The second aim of this paper is to provide a thorough empirical comparison between PBVI and other state-of-the-art POMDP methods, in particular the Perseus algorithm, in an effort to highlight their similarities and differences. Evaluation is performed using both standard POMDP domains and realistic robotic tasks.\n    ",
        "submission_date": "2011-09-30T00:00:00",
        "last_modified_date": "2011-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0028",
        "title": "Solving Factored MDPs with Hybrid State and Action Variables",
        "authors": [
            "C. Guestrin",
            "M. Hauskrecht",
            "B. Kveton"
        ],
        "abstract": "Efficient representations and solutions for large decision problems with continuous and discrete variables are among the most important challenges faced by the designers of automated decision support systems. In this paper, we describe a novel hybrid factored Markov decision process (MDP) model that allows for a compact representation of these problems, and a new hybrid approximate linear programming (HALP) framework that permits their efficient solutions. The central idea of HALP is to approximate the optimal value function by a linear combination of basis functions and optimize its weights by linear programming. We analyze both theoretical and computational aspects of this approach, and demonstrate its scale-up potential on several hybrid optimization problems.\n\n    ",
        "submission_date": "2011-09-30T00:00:00",
        "last_modified_date": "2011-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0029",
        "title": "Combination Strategies for Semantic Role Labeling",
        "authors": [
            "M. Surdeanu",
            "L. Marquez",
            "X. Carreras",
            "P. R. Comas"
        ],
        "abstract": "This paper introduces and analyzes a battery of inference models for the problem of semantic role labeling: one based on constraint satisfaction, and several strategies that model the inference as a meta-learning problem using discriminative classifiers. These classifiers are developed with a rich set of novel features that encode proposition and sentence-level information. To our knowledge, this is the first work that: (a) performs a thorough analysis of learning-based inference models for semantic role labeling, and (b) compares several inference strategies in this context. We evaluate the proposed inference strategies in the framework of the CoNLL-2005 shared task using only automatically-generated syntactic information. The extensive experimental evaluation and analysis indicates that all the proposed inference strategies are successful -they all outperform the current best results reported in the CoNLL-2005 evaluation exercise- but each of the proposed approaches has its advantages and disadvantages. Several important traits of a state-of-the-art SRL combination strategy emerge from this analysis: (i) individual models should be combined at the granularity of candidate arguments rather than at the granularity of complete solutions; (ii) the best combination strategy uses an inference model based in learning; and (iii) the learning-based inference benefits from max-margin classifiers and global feedback.\n    ",
        "submission_date": "2011-09-30T00:00:00",
        "last_modified_date": "2011-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0248",
        "title": "A Behavioral Distance for Fuzzy-Transition Systems",
        "authors": [
            "Yongzhi Cao",
            "Huaiqing Wang",
            "Sherry X. Sun",
            "Guoqing Chen"
        ],
        "abstract": "In contrast to the existing approaches to bisimulation for fuzzy systems, we introduce a behavioral distance to measure the behavioral similarity of states in a nondeterministic fuzzy-transition system. This behavioral distance is defined as the greatest fixed point of a suitable monotonic function and provides a quantitative analogue of bisimilarity. The behavioral distance has the important property that two states are at zero distance if and only if they are bisimilar. Moreover, for any given threshold, we find that states with behavioral distances bounded by the threshold are equivalent. In addition, we show that two system combinators---parallel composition and product---are non-expansive with respect to our behavioral distance, which makes compositional verification possible.\n    ",
        "submission_date": "2011-10-03T00:00:00",
        "last_modified_date": "2011-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0532",
        "title": "Strange Beta: An Assistance System for Indoor Rock Climbing Route Setting Using Chaotic Variations and Machine Learning",
        "authors": [
            "Caleb Phillips",
            "Lee Becker",
            "Elizabeth Bradley"
        ],
        "abstract": "This paper applies machine learning and the mathematics of chaos to the task of designing indoor rock-climbing routes. Chaotic variation has been used to great advantage on music and dance, but the challenges here are quite different, beginning with the representation. We present a formalized system for transcribing rock climbing problems, then describe a variation generator that is designed to support human route-setters in designing new and interesting climbing problems. This variation generator, termed Strange Beta, combines chaos and machine learning, using the former to introduce novelty and the latter to smooth transitions in a manner that is consistent with the style of the climbs This entails parsing the domain-specific natural language that rock climbers use to describe routes and movement and then learning the patterns in the results. We validated this approach with a pilot study in a small university rock climbing gym, followed by a large blinded study in a commercial climbing gym, in cooperation with experienced climbers and expert route setters. The results show that {\\sc Strange Beta} can help a human setter produce routes that are at least as good as, and in some cases better than, those produced in the traditional manner.\n    ",
        "submission_date": "2011-10-03T00:00:00",
        "last_modified_date": "2011-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0631",
        "title": "Well-Definedness and Efficient Inference for Probabilistic Logic Programming under the Distribution Semantics",
        "authors": [
            "Fabrizio Riguzzi",
            "Terrance Swift"
        ],
        "abstract": "The distribution semantics is one of the most prominent approaches for the combination of logic programming and probability theory. Many languages follow this semantics, such as Independent Choice Logic, PRISM, pD, Logic Programs with Annotated Disjunctions (LPADs) and ProbLog. When a program contains functions symbols, the distribution semantics is well-defined only if the set of explanations for a query is finite and so is each explanation. Well-definedness is usually either explicitly imposed or is achieved by severely limiting the class of allowed programs. In this paper we identify a larger class of programs for which the semantics is well-defined together with an efficient procedure for computing the probability of queries. Since LPADs offer the most general syntax, we present our results for them, but our results are applicable to all languages under the distribution semantics. We present the algorithm \"Probabilistic Inference with Tabling and Answer subsumption\" (PITA) that computes the probability of queries by transforming a probabilistic program into a normal program and then applying SLG resolution with answer subsumption. PITA has been implemented in XSB and tested on six domains: two with function symbols and four without. The execution times are compared with those of ProbLog, cplint and CVE, PITA was almost always able to solve larger problems in a shorter time, on domains with and without function symbols.\n    ",
        "submission_date": "2011-10-04T00:00:00",
        "last_modified_date": "2011-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.1016",
        "title": "Engineering Benchmarks for Planning: the Domains Used in the Deterministic Part of IPC-4",
        "authors": [
            "S. Edelkamp",
            "R. Englert",
            "J. Hoffmann",
            "F. Liporace",
            "S. Thiebaux",
            "S. Trueg"
        ],
        "abstract": "In a field of research about general reasoning mechanisms, it is essential to have appropriate benchmarks. Ideally, the benchmarks should reflect possible applications of the developed technology. In AI Planning, researchers more and more tend to draw their testing examples from the benchmark collections used in the International Planning Competition (IPC). In the organization of (the deterministic part of) the fourth IPC,  IPC-4, the authors therefore invested significant effort to create a useful set of benchmarks. They come from five different (potential) real-world applications of planning: airport ground traffic control, oil derivative transportation in pipeline networks, model-checking safety properties, power supply restoration, and UMTS call setup. Adapting and preparing such an application for use as a benchmark in the IPC involves, at the time, inevitable (often drastic) simplifications, as well as careful choice between, and engineering of, domain encodings. For the first time in the IPC, we used compilations to formulate complex domain features in simple languages such as STRIPS, rather than just dropping the more interesting problem constraints in the simpler language subsets. The article explains and discusses the five application domains and their adaptation to form the PDDL test suites used in IPC-4. We summarize known theoretical results on structural properties of the domains, regarding their computational complexity and provable properties of their topology under the h+ function (an idealized version of the relaxed plan heuristic). We present new (empirical) results illuminating properties such as the quality of the most wide-spread heuristic functions (planning graph, serial planning graph, and relaxed plan), the growth of propositional representations over instance size, and the number of actions available to achieve each fact; we discuss these data in conjunction with the best results achieved by the different kinds of planners participating in IPC-4.\n    ",
        "submission_date": "2011-09-29T00:00:00",
        "last_modified_date": "2011-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2162",
        "title": "Large-Margin Learning of Submodular Summarization Methods",
        "authors": [
            "Ruben Sipos",
            "Pannaga Shivaswamy",
            "Thorsten Joachims"
        ],
        "abstract": "In this paper, we present a supervised learning approach to training submodular scoring functions for extractive multi-document summarization. By taking a structured predicition approach, we provide a large-margin method that directly optimizes a convex relaxation of the desired performance measure. The learning method applies to all submodular summarization methods, and we demonstrate its effectiveness for both pairwise as well as coverage-based scoring functions on multiple datasets. Compared to state-of-the-art functions that were tuned manually, our method significantly improves performance and enables high-fidelity models with numbers of parameters well beyond what could reasonbly be tuned by hand.\n    ",
        "submission_date": "2011-10-10T00:00:00",
        "last_modified_date": "2011-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2200",
        "title": "Modelling Mixed Discrete-Continuous Domains for Planning",
        "authors": [
            "M. Fox",
            "D. Long"
        ],
        "abstract": "In this paper we present pddl+, a planning domain description language for modelling mixed discrete-continuous planning domains. We describe the syntax and modelling style of pddl+, showing that the language makes convenient the modelling of complex time-dependent effects. We provide a formal semantics for pddl+ by mapping planning instances into constructs of hybrid automata. Using the syntax of HAs as our semantic model we construct a semantic mapping to labelled transition systems to complete the formal interpretation of pddl+ planning instances. An advantage of building a mapping from pddl+ to HA theory is that it forms a bridge between the Planning and Real Time Systems research communities. One consequence is that we can expect to make use of some of the theoretical properties of HAs. For example, for a restricted class of HAs the Reachability problem (which is equivalent to Plan Existence) is decidable. pddl+ provides an alternative to the continuous durative action model of pddl2.1, adding a more flexible and robust model of time-dependent behaviour.\n    ",
        "submission_date": "2011-10-10T00:00:00",
        "last_modified_date": "2011-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2203",
        "title": "Set Intersection and Consistency in Constraint Networks",
        "authors": [
            "R. H. C. Yap",
            "Y. Zhang"
        ],
        "abstract": "In this paper, we show that there is a close relation between consistency in a constraint network and set intersection. A proof schema is provided as a generic way to obtain consistency properties from properties on set intersection. This approach not only simplifies the understanding of and unifies many existing consistency results, but also directs the study of consistency to that of set intersection properties in many situations, as demonstrated by the results on the convexity and tightness of constraints in this paper. Specifically, we identify a new class of tree convex constraints where local consistency ensures global consistency. This generalizes row convex constraints. Various consistency results are also obtained on constraint networks where only some, in contrast to all in the existing work,constraints are tight. \n\n    ",
        "submission_date": "2011-10-10T00:00:00",
        "last_modified_date": "2011-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2204",
        "title": "Consistency and Random Constraint Satisfaction Models",
        "authors": [
            "J. Culberson",
            "Y. Gao"
        ],
        "abstract": "In this paper, we study the possibility of designing non-trivial random CSP models by exploiting the intrinsic connection between structures and typical-case hardness. We show that constraint consistency, a notion that has been developed to improve  the efficiency of CSP algorithms, is in fact the key to the design of random CSP models that have interesting phase transition behavior and guaranteed exponential resolution complexity without putting much restriction on the parameter of constraint tightness or the domain size of the problem.  We propose a very flexible framework for constructing problem instances withinteresting behavior and develop a variety of concrete methods to construct specific  random CSP models that enforce different levels of constraint consistency.\nA series of experimental studies with interesting observations are carried out to illustrate the effectiveness of introducing structural elements in random instances, to verify the robustness of our proposal, and to investigate features of some specific models based on our framework that are highly related to the behavior of backtracking search algorithms.\n    ",
        "submission_date": "2011-10-10T00:00:00",
        "last_modified_date": "2011-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2205",
        "title": "Answer Sets for Logic Programs with Arbitrary Abstract Constraint Atoms",
        "authors": [
            "E. Pontelli",
            "T. C. Son",
            "P. H. Tu"
        ],
        "abstract": "In this paper, we present two alternative approaches to defining answer sets for logic programs with arbitrary types of abstract constraint atoms (c-atoms). These approaches generalize the fixpoint-based and the level mapping based answer set semantics of normal logic programs to the case of logic programs with arbitrary types of c-atoms. The results are four different answer set definitions which are equivalent when applied to normal logic programs.  \nThe standard fixpoint-based semantics of logic programs is generalized in two directions, called answer set by reduct and answer set by complement. These definitions, which differ from each other in the treatment of negation-as-failure (naf) atoms, make use of an immediate consequence operator to perform answer set checking, whose definition relies on the notion of conditional satisfaction of c-atoms w.r.t. a pair of interpretations. \nThe other two definitions, called strongly and weakly well-supported models, are  generalizations of the notion of well-supported models of normal logic programs to the case of programs with c-atoms. As for the case of  fixpoint-based semantics, the difference between these two definitions is rooted in the treatment of naf atoms. \nWe prove that answer sets by reduct (resp. by complement) are equivalent to weakly (resp. strongly) well-supported models of a program, thus generalizing the theorem on the correspondence between stable models and well-supported models of a normal logic program to the class of programs with c-atoms.  \nWe show that the newly defined semantics coincide with previously introduced semantics for logic programs with monotone c-atoms, and they extend the original answer set semantics of normal logic programs. We also study some properties of answer sets of programs with c-atoms, and relate our definitions to several semantics for logic programs with aggregates presented in the literature.\n    ",
        "submission_date": "2011-10-10T00:00:00",
        "last_modified_date": "2011-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2209",
        "title": "Bin Completion Algorithms for Multicontainer Packing, Knapsack, and Covering Problems",
        "authors": [
            "A. S. Fukunaga",
            "R. E. Korf"
        ],
        "abstract": "Many combinatorial optimization problems such as the bin packing and multiple knapsack problems involve assigning a set of discrete objects to multiple containers. These problems can be used to model task and resource allocation problems in multi-agent systems and distributed systms, and can also be found as subproblems of scheduling problems. We propose bin completion, a branch-and-bound strategy for one-dimensional, multicontainer packing problems.  Bin completion combines a bin-oriented search space with a powerful dominance criterion that enables us to prune much of the space. The performance of the basic bin completion framework can be enhanced by using a number of extensions, including nogood-based pruning techniques that allow further exploitation of the dominance criterion.  Bin completion is applied to four problems: multiple knapsack, bin covering, min-cost covering, and bin packing.  We show that our bin completion algorithms yield new, state-of-the-art results for the multiple knapsack, bin covering, and min-cost covering problems, outperforming previous algorithms by several orders of magnitude with respect to runtime on some classes of hard, random problem instances.  For the bin packing problem, we demonstrate significant improvements compared to most previous results, but show that bin completion is not competitive with current state-of-the-art cutting-stock based approaches.\n    ",
        "submission_date": "2011-10-10T00:00:00",
        "last_modified_date": "2011-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2212",
        "title": "Uncertainty in Soft Temporal Constraint Problems:A General Framework and Controllability Algorithms for the Fuzzy Case",
        "authors": [
            "F. Rossi",
            "K. B. Venable",
            "N. Yorke-Smith"
        ],
        "abstract": "In real-life temporal scenarios, uncertainty and preferences are often  essential and coexisting aspects.  We present a formalism where quantitative temporal constraints with both preferences and uncertainty can be defined.  We show how three classical notions of controllability (that is, strong, weak, and dynamic), which have been developed for uncertain temporal problems, can be generalized to handle preferences as well. After defining this general framework, we focus on problems where preferences follow the fuzzy approach, and with properties that assure tractability. For such problems, we propose algorithms to check the presence of the controllability properties. In particular, we show that in such a setting dealing simultaneously with preferences and uncertainty does not increase the complexity of controllability testing.  We also develop a dynamic execution algorithm, of polynomial complexity, that produces temporal plans under uncertainty that are optimal with respect to fuzzy preferences.\n    ",
        "submission_date": "2011-10-10T00:00:00",
        "last_modified_date": "2011-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2213",
        "title": "Supporting Temporal Reasoning by Mapping Calendar Expressions to Minimal Periodic Sets",
        "authors": [
            "C. Bettini",
            "S. Mascetti",
            "X. S. Wang"
        ],
        "abstract": "In the recent years several research efforts have focused on the concept of time granularity and its applications. A first stream of research investigated the mathematical models behind the notion of granularity and the algorithms to manage temporal data based on those models. A second stream of research investigated symbolic formalisms providing a set of algebraic operators to define granularities in a compact and compositional way. However, only very limited manipulation algorithms have been proposed to operate directly on the algebraic representation making it unsuitable to use the symbolic formalisms in applications that need manipulation of granularities.\nThis paper aims at filling the gap between the results from these two streams of research, by providing an efficient conversion from the algebraic representation to the equivalent low-level representation based on the mathematical models. In addition, the conversion returns a minimal representation in terms of period length. Our results have a major\npractical impact: users can more easily define arbitrary granularities in terms of algebraic operators, and then access granularity reasoning and other services operating efficiently on the equivalent, minimal low-level representation. As an example, we illustrate the application to temporal constraint reasoning with multiple granularities.\nFrom a technical point of view, we propose an hybrid algorithm that interleaves the conversion of calendar subexpressions into periodical sets with the minimization of the period length. The algorithm returns set-based granularity representations having minimal period length, which is the most relevant parameter for the performance of the considered reasoning services. Extensive experimental work supports the techniques used in the algorithm, and shows the efficiency and effectiveness of the algorithm.\n    ",
        "submission_date": "2011-10-10T00:00:00",
        "last_modified_date": "2011-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2216",
        "title": "The Generalized A* Architecture",
        "authors": [
            "P. F. Felzenszwalb",
            "D. McAllester"
        ],
        "abstract": "We consider the problem of computing a lightest derivation of a global structure using a set of weighted rules.  A large variety of inference problems in AI can be formulated in this framework.  We generalize A* search and heuristics derived from abstractions to a broad class of lightest derivation problems.  We also describe a new algorithm that searches for lightest derivations using a hierarchy of abstractions.  Our generalization of A* gives a new algorithm for searching AND/OR graphs in a bottom-up fashion. \nWe discuss how the algorithms described here provide a general architecture for addressing the pipeline problem --- the problem of passing information back and forth between various stages of processing in a perceptual system.  We consider examples in computer vision and natural language processing.  We apply the hierarchical search algorithm to the problem of estimating the boundaries of convex objects in grayscale images and compare it to other search methods.  A second set of experiments demonstrate the use of a new compositional model for finding salient curves in images.\n    ",
        "submission_date": "2011-10-10T00:00:00",
        "last_modified_date": "2011-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2726",
        "title": "Combining Spatial and Temporal Logics: Expressiveness vs. Complexity",
        "authors": [
            "D. Gabelaia",
            "R. Kontchakov",
            "A. Kurucz",
            "F. Wolter",
            "M. Zakharyaschev"
        ],
        "abstract": "In this paper, we construct and investigate a hierarchy of spatio-temporal formalisms that result from various combinations of propositional spatial and temporal logics such as the propositional temporal logic PTL, the spatial logics RCC-8, BRCC-8, S4u and their fragments. The obtained results give a clear picture of the trade-off between expressiveness and computational realisability within the hierarchy. We demonstrate how different combining principles as well as spatial and temporal primitives can produce NP-, PSPACE-, EXPSPACE-, 2EXPSPACE-complete, and even undecidable spatio-temporal logics out of components that are at most NP- or PSPACE-complete.\n    ",
        "submission_date": "2011-10-12T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2728",
        "title": "An Approach to Temporal Planning and Scheduling in Domains with Predictable Exogenous Events",
        "authors": [
            "A. Gerevini",
            "A. Saetti",
            "I. Serina"
        ],
        "abstract": "The treatment of exogenous events in planning is practically important in many real-world domains where the preconditions of certain plan actions are affected by such events. In this paper we focus on planning in temporal domains with exogenous events that happen at known times, imposing the constraint that certain actions in the plan must be executed during some predefined time windows. When actions have durations, handling such temporal constraints adds an extra difficulty to planning. We propose an approach to planning in these domains which integrates constraint-based temporal reasoning into a graph-based planning framework using local search. Our techniques are implemented in a planner that took part in the 4th International Planning Competition (IPC-4). A statistical analysis of the results of IPC-4 demonstrates the effectiveness of our approach in terms of both CPU-time and plan quality. Additional experiments show the good performance of the temporal reasoning techniques integrated into our planner. \n    ",
        "submission_date": "2011-10-12T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2729",
        "title": "The Power of Modeling - a Response to PDDL2.1",
        "authors": [
            "F. Bacchus"
        ],
        "abstract": "In this commentary I argue that although PDDL is a very useful standard for the planning competition, its design does not properly consider the issue of domain modeling. Hence, I would not advocate its use in specifying planning domains outside of the context of the planning competition. Rather, the field needs to explore different approaches and grapple more directly with the problem of effectively modeling and utilizing all of the diverse pieces of knowledge we typically have about planning domains.\n    ",
        "submission_date": "2011-10-12T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2730",
        "title": "Imperfect Match: PDDL 2.1 and Real Applications",
        "authors": [
            "M. S. Boddy"
        ],
        "abstract": "PDDL was originally conceived and constructed as a lingua franca for the International Planning Competition.  PDDL2.1 embodies a set of extensions intended to support the expression of something closer to real planning problems.  This objective has only been partially achieved, due in large part to a deliberate focus on not moving too far from classical planning models and solution methods.\n    ",
        "submission_date": "2011-10-12T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2731",
        "title": "PDDL 2.1: Representation vs. Computation",
        "authors": [
            "H. A. Geffner"
        ],
        "abstract": "I comment on the PDDL 2.1 language and its use in the planning competition, focusing on the choices made for accommodating time and concurrency.  I also discuss some methodological issues that have to do with the move toward more expressive planning languages and the balance needed in planning research between semantics and computation.\n    ",
        "submission_date": "2011-10-12T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2732",
        "title": "Proactive Algorithms for Job Shop Scheduling with Probabilistic Durations",
        "authors": [
            "J. C. Beck",
            "N. Wilson"
        ],
        "abstract": "Most classical scheduling formulations assume a fixed and known duration for each activity.  In this paper, we weaken this assumption, requiring instead that each duration can be represented by an independent random variable with a known mean and variance. The best solutions are ones which have a high probability of achieving a good makespan. We first create a theoretical framework, formally showing how Monte Carlo simulation can be combined with deterministic scheduling algorithms to solve this problem.  We propose an associated deterministic scheduling problem whose solution is proved, under certain conditions, to be a lower bound for the probabilistic problem. We then propose and investigate a number of techniques for solving such problems based on combinations of Monte Carlo simulation, solutions to the associated deterministic problem, and either constraint programming or tabu search. Our empirical results demonstrate that a combination of the use of the associated deterministic problem and Monte Carlo simulation results in algorithms that scale best both in terms of problem size and uncertainty. Further experiments point to the correlation between the quality of the deterministic solution and the quality of the probabilistic solution as a major factor responsible for this success.\n    ",
        "submission_date": "2011-10-12T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2734",
        "title": "The Language of Search",
        "authors": [
            "A. Darwiche",
            "J. Huang"
        ],
        "abstract": "This paper is concerned with a class of algorithms that perform exhaustive search on propositional knowledge bases. We show that each of these algorithms defines and generates a propositional language. Specifically, we show that the trace of a search can be interpreted as a combinational circuit, and a search algorithm then defines a propositional language consisting of circuits that are generated across all possible executions of the algorithm. In particular, we show that several versions of exhaustive DPLL search correspond to such well-known languages as FBDD, OBDD, and a precisely-defined subset of d-DNNF. By thus mapping search algorithms to propositional languages, we provide a uniform and practical framework in which successful search techniques can be harnessed for compilation of knowledge into various languages of interest, and a new methodology whereby the power and limitations of search algorithms can be understood by looking up the tractability and succinctness of the corresponding propositional languages.\n    ",
        "submission_date": "2011-10-12T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2735",
        "title": "Understanding Algorithm Performance on an Oversubscribed Scheduling Application",
        "authors": [
            "L. Barbulescu",
            "A. E. Howe",
            "M. Roberts",
            "L. D. Whitley"
        ],
        "abstract": "The best performing algorithms for a particular oversubscribed scheduling application, Air Force Satellite Control Network (AFSCN) scheduling, appear to have little in common. Yet, through careful experimentation and modeling of performance in real problem instances, we can relate characteristics of the best algorithms to characteristics of the application. In particular, we find that plateaus dominate the search spaces (thus favoring algorithms that make larger changes to solutions) and that some randomization in exploration is critical to good performance (due to the lack of gradient information on the plateaus). Based on our explanations of algorithm performance, we develop a new algorithm that combines characteristics of the best performers; the new algorithms performance is better than the previous best. We show how hypothesis driven experimentation and search modeling can both explain algorithm performance and motivate the design of a new algorithm.\n    ",
        "submission_date": "2011-10-12T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2736",
        "title": "Marvin: A Heuristic Search Planner with Online Macro-Action Learning",
        "authors": [
            "A. I. Coles",
            "A. J. Smith"
        ],
        "abstract": "This paper describes Marvin, a planner that competed in the Fourth International Planning Competition (IPC 4). Marvin uses action-sequence-memoisation techniques to generate macro-actions, which are then used during search for a solution plan. We provide an overview of its architecture and search behaviour, detailing the algorithms used. We also empirically demonstrate the effectiveness of its features in various planning domains; in particular, the effects on performance due to the use of macro-actions, the novel features of its search behaviour, and the native support of ADL and Derived Predicates.\n    ",
        "submission_date": "2011-10-12T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2737",
        "title": "Anytime Heuristic Search",
        "authors": [
            "E. A. Hansen",
            "R. Zhou"
        ],
        "abstract": "We describe how to convert the heuristic search algorithm A* into an anytime algorithm that finds a sequence of improved solutions and eventually converges to an optimal solution. The approach we adopt uses weighted heuristic search to find an approximate solution quickly, and then continues the weighted search to find improved solutions as well as to improve a bound on the suboptimality of the current solution. When the time available to solve a search problem is limited or uncertain, this creates an anytime heuristic search algorithm that allows a flexible tradeoff between search time and solution quality. We analyze the properties of the resulting Anytime A* algorithm, and consider its performance in three domains; sliding-tile puzzles, STRIPS planning, and multiple sequence alignment. To illustrate the generality of this approach, we also describe how to transform the memory-efficient search algorithm Recursive Best-First Search (RBFS) into an anytime algorithm.\n    ",
        "submission_date": "2011-10-12T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2738",
        "title": "Discovering Classes of Strongly Equivalent Logic Programs",
        "authors": [
            "Y. Chen",
            "F. Lin"
        ],
        "abstract": "In this paper we apply computer-aided theorem discovery technique to discover theorems about strongly equivalent logic programs under the answer set semantics. Our discovered theorems capture new classes of strongly equivalent logic programs that can lead to new program simplification rules that preserve strong equivalence. Specifically, with the help of computers, we discovered exact conditions that capture the strong equivalence between a rule and the empty set, between two rules, between two rules and one of the two rules, between two rules and another rule, and between three rules and two of the three rules.\n    ",
        "submission_date": "2011-10-12T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2739",
        "title": "Phase Transition for Random Quantified XOR-Formulas",
        "authors": [
            "N. Creignou",
            "H. Daude",
            "U. Egly"
        ],
        "abstract": "The QXORSAT problem is the quantified version of the satisfiability problem XORSAT in which the connective exclusive-or is used instead of the usual or. We study the phase transition associated with random QXORSAT instances. We give a description of this phase transition in the case of one alternation of quantifiers, thus performing an advanced practical and theoretical study on the phase transition of a quantified roblem.\n\n    ",
        "submission_date": "2011-10-12T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2740",
        "title": "Cutset Sampling for Bayesian Networks",
        "authors": [
            "B. Bidyuk",
            "R. Dechter"
        ],
        "abstract": "The paper presents a new sampling methodology for Bayesian networks that samples only a subset of variables and applies exact inference to the rest.  Cutset sampling is a network structure-exploiting application of the Rao-Blackwellisation principle to sampling in Bayesian networks.  It improves convergence by exploiting memory-based inference algorithms.  It can also be viewed as an anytime approximation of the exact cutset-conditioning algorithm developed by Pearl.  Cutset sampling can be implemented efficiently when the sampled variables constitute a loop-cutset of the Bayesian network and, more generally, when the induced width of the networks graph conditioned on the observed sampled variables is bounded by a constant w.  We demonstrate empirically the benefit of this scheme on a range of benchmarks.\n    ",
        "submission_date": "2011-10-12T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2741",
        "title": "An Algebraic Graphical Model for Decision with Uncertainties, Feasibilities, and Utilities",
        "authors": [
            "C. Pralet",
            "T. Schiex",
            "G. Verfaillie"
        ],
        "abstract": "Numerous formalisms and dedicated algorithms have been designed in the last decades to model and solve decision making problems. Some formalisms, such as constraint networks, can express \"simple\" decision problems, while others are designed to take into account uncertainties, unfeasible decisions, and utilities. Even in a single formalism, several variants are often proposed to model different types of uncertainty (probability, possibility...) or utility (additive or not).  In this article, we introduce an algebraic graphical model that encompasses a large number of such formalisms: (1) we first adapt previous structures from Friedman, Chu and Halpern for representing uncertainty, utility, and expected utility in order to deal with generic forms of sequential decision making; (2) on these structures, we then introduce composite graphical models that express information via variables linked by \"local\" functions, thanks to conditional independence; (3) on these graphical models, we finally define a simple class of queries which can represent various scenarios in terms of observabilities and controllabilities. A natural decision-tree semantics for such queries is completed by an equivalent operational semantics, which induces generic algorithms.  The proposed framework, called the Plausibility-Feasibility-Utility (PFU) framework,  not only provides a better understanding of the links between existing formalisms, but it also covers yet unpublished frameworks (such as possibilistic influence diagrams) and unifies formalisms such as quantified boolean formulas and influence diagrams. Our backtrack and variable elimination generic algorithms are a first step towards unified algorithms.\n    ",
        "submission_date": "2011-10-12T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2742",
        "title": "Semantic Matchmaking as Non-Monotonic Reasoning: A Description Logic Approach",
        "authors": [
            "T. Di Noia",
            "E. Di Sciascio",
            "F. M. Donini"
        ],
        "abstract": "Matchmaking arises when supply and demand meet in an electronic marketplace, or when agents search for a web service to perform some task, or even when recruiting agencies match curricula and job profiles.  In such open environments, the objective of a matchmaking process is to discover best available offers to a given request.\nWe address the problem of matchmaking from a knowledge representation perspective, with a formalization based on Description Logics. We devise Concept Abduction and Concept Contraction as non-monotonic inferences in Description Logics suitable for modeling matchmaking in a logical framework, and prove some related complexity results.  We also present reasonable algorithms for semantic matchmaking based on the devised inferences, and prove that they obey to some commonsense properties.\nFinally, we report on the implementation of the proposed matchmaking framework, which has been used both as a mediator in e-marketplaces and for semantic web services discovery.\n    ",
        "submission_date": "2011-10-12T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2743",
        "title": "Solution-Guided Multi-Point Constructive Search for Job Shop Scheduling",
        "authors": [
            "J. C. Beck"
        ],
        "abstract": "Solution-Guided Multi-Point Constructive Search (SGMPCS) is a novel constructive search technique that performs a series of resource-limited tree searches where each search begins either from an empty solution (as in randomized restart) or from a solution that has been encountered during the search. A small number of these \"elite solutions is maintained during the search. We introduce the technique and perform three sets of experiments on the job shop scheduling problem. First, a systematic, fully crossed study of SGMPCS is carried out to evaluate the performance impact of various parameter settings. Second, we inquire into the diversity of the elite solution set, showing, contrary to expectations, that a less diverse set leads to stronger performance. Finally, we compare the best parameter setting of SGMPCS from the first two experiments to chronological backtracking, limited discrepancy search, randomized restart, and a sophisticated tabu search algorithm on a set of well-known benchmark problems. Results demonstrate that SGMPCS is significantly better than the other constructive techniques tested, though lags behind the tabu search.\n\n    ",
        "submission_date": "2011-10-12T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.3002",
        "title": "Are Minds Computable?",
        "authors": [
            "Carlos Gershenson"
        ],
        "abstract": "This essay explores the limits of Turing machines concerning the modeling of minds and suggests alternatives to go beyond those limits.\n    ",
        "submission_date": "2011-10-13T00:00:00",
        "last_modified_date": "2011-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.3385",
        "title": "Fuzzy Inference Systems Optimization",
        "authors": [
            "Pretesh Patel",
            "Tshilidzi Marwala"
        ],
        "abstract": "This paper compares various optimization methods for fuzzy inference system optimization. The optimization methods compared are genetic algorithm, particle swarm optimization and simulated annealing. When these techniques were implemented it was observed that the performance of each technique within the fuzzy inference system classification was context dependent.\n    ",
        "submission_date": "2011-10-15T00:00:00",
        "last_modified_date": "2011-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.3672",
        "title": "Reasoning about Actions with Temporal Answer Sets",
        "authors": [
            "Laura Giordano",
            "Alberto Martelli",
            "Daniele Theseider Dupr\u00e9"
        ],
        "abstract": "In this paper we combine Answer Set Programming (ASP) with Dynamic Linear Time Temporal Logic (DLTL) to define a temporal logic programming language for reasoning about complex actions and infinite computations. DLTL extends propositional temporal logic of linear time with regular programs of propositional dynamic logic, which are used for indexing temporal modalities. The action language allows general DLTL formulas to be included in domain descriptions to constrain the space of possible extensions. We introduce a notion of Temporal Answer Set for domain descriptions, based on the usual notion of Answer Set. Also, we provide a translation of domain descriptions into standard ASP and we use Bounded Model Checking techniques for the verification of DLTL constraints.\n    ",
        "submission_date": "2011-10-17T00:00:00",
        "last_modified_date": "2011-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.3888",
        "title": "Handling controversial arguments by matrix",
        "authors": [
            "Xu Yuming"
        ],
        "abstract": "We introduce matrix and its block to the Dung's theory of argumentation framework. It is showed that each argumentation framework has a matrix representation, and the indirect attack relation and indirect defence relation can be characterized by computing the matrix. This provide a powerful mathematics way to determine the \"controversial arguments\" in an argumentation framework. Also, we introduce several kinds of blocks based on the matrix, and various prudent semantics of argumentation frameworks can all be determined by computing and comparing the matrices and their blocks which we have defined. In contrast with traditional method of directed graph, the matrix method has an excellent advantage: computability(even can be realized on computer easily). So, there is an intensive perspective to import the theory of matrix to the research of argumentation frameworks and its related areas.\n    ",
        "submission_date": "2011-10-18T00:00:00",
        "last_modified_date": "2011-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.4076",
        "title": "Learning in Real-Time Search: A Unifying Framework",
        "authors": [
            "V. Bulitko",
            "G. Lee"
        ],
        "abstract": "Real-time search methods are suited for tasks in which the agent is interacting with an initially unknown environment in real time. In such simultaneous planning and learning problems, the agent has to select its actions in a limited amount of time, while sensing only a local part of the environment centered at the agents current location. Real-time heuristic search agents select actions using a limited lookahead search and evaluating the frontier states with a heuristic function. Over repeated experiences, they refine heuristic values of states to avoid infinite loops and to converge to better solutions. The wide spread of such settings in autonomous software and hardware agents has led to an explosion of real-time search algorithms over the last two decades. Not only is a potential user confronted with a hodgepodge of algorithms, but he also faces the choice of control parameters they use. In this paper we address both problems. The first contribution is an introduction of a simple three-parameter framework (named LRTS) which extracts the core ideas behind many existing algorithms. We then prove that LRTA*, epsilon-LRTA*, SLA*, and gamma-Trap algorithms are special cases of our framework. Thus, they are unified and extended with additional features. Second, we prove completeness and convergence of any algorithm covered by the LRTS framework. Third, we prove several upper-bounds relating the control parameters and solution quality. Finally, we analyze the influence of the three control parameters empirically in the realistic scalable domains of real-time navigation on initially unknown maps from a commercial role-playing game as well as routing in ad hoc sensor networks.\n    ",
        "submission_date": "2011-09-26T00:00:00",
        "last_modified_date": "2011-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.4657",
        "title": "A Version of Geiringer-like Theorem for Decision Making in the Environments with Randomness and Incomplete Information",
        "authors": [
            "Boris Mitavskiy",
            "Jonathan Rowe",
            "Chris Cannings"
        ],
        "abstract": "Purpose: In recent years Monte-Carlo sampling methods, such as Monte Carlo tree search, have achieved tremendous success in model free reinforcement learning. A combination of the so called upper confidence bounds policy to preserve the \"exploration vs. exploitation\" balance to select actions for sample evaluations together with massive computing power to store and to update dynamically a rather large pre-evaluated game tree lead to the development of software that has beaten the top human player in the game of Go on a 9 by 9 board. Much effort in the current research is devoted to widening the range of applicability of the Monte-Carlo sampling methodology to partially observable Markov decision processes with non-immediate payoffs. The main challenge introduced by randomness and incomplete information is to deal with the action evaluation at the chance nodes due to drastic differences in the possible payoffs the same action could lead to. The aim of this article is to establish a version of a theorem that originated from population genetics and has been later adopted in evolutionary computation theory that will lead to novel Monte-Carlo sampling algorithms that provably increase the AI potential. Due to space limitations the actual algorithms themselves will be presented in the sequel papers, however, the current paper provides a solid mathematical foundation for the development of such algorithms and explains why they are so promising.\n    ",
        "submission_date": "2011-10-20T00:00:00",
        "last_modified_date": "2011-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.4719",
        "title": "A Generalized Arc-Consistency Algorithm for a Class of Counting Constraints: Revised Edition that Incorporates One Correction",
        "authors": [
            "Thierry Petit",
            "Nicolas Beldiceanu",
            "Xavier Lorca"
        ],
        "abstract": "This paper introduces the SEQ BIN meta-constraint with a polytime algorithm achieving general- ized arc-consistency according to some properties. SEQ BIN can be used for encoding counting con- straints such as CHANGE, SMOOTH or INCREAS- ING NVALUE. For some of these constraints and some of their variants GAC can be enforced with a time and space complexity linear in the sum of domain sizes, which improves or equals the best known results of the literature.\n    ",
        "submission_date": "2011-10-21T00:00:00",
        "last_modified_date": "2011-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.5172",
        "title": "Quels formalismes temporels pour repr\u00e9senter des connaissances extraites de textes de recettes de cuisine ?",
        "authors": [
            "Valmi Dufour-Lussier",
            "Florence Le Ber",
            "Jean Lieber"
        ],
        "abstract": "The Taaable projet goal is to create a case-based reasoning system for retrieval and adaptation of cooking recipes. Within this framework, we are discussing the temporal aspects of recipes and the means of representing those in order to adapt their text.\n    ",
        "submission_date": "2011-10-24T00:00:00",
        "last_modified_date": "2011-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.5667",
        "title": "Inducing Probabilistic Programs by Bayesian Program Merging",
        "authors": [
            "Irvin Hwang",
            "Andreas Stuhlm\u00fcller",
            "Noah D. Goodman"
        ],
        "abstract": "This report outlines an approach to learning generative models from data. We express models as probabilistic programs, which allows us to capture abstract patterns within the examples. By choosing our language for programs to be an extension of the algebraic data type of the examples, we can begin with a program that generates all and only the examples. We then introduce greater abstraction, and hence generalization, incrementally to the extent that it improves the posterior probability of the examples given the program. Motivated by previous approaches to model merging and program induction, we search for such explanatory abstractions using program transformations. We consider two types of transformation: Abstraction merges common subexpressions within a program into new functions (a form of anti-unification). Deargumentation simplifies functions by reducing the number of arguments. We demonstrate that this approach finds key patterns in the domain of nested lists, including parameterized sub-functions and stochastic recursion.\n    ",
        "submission_date": "2011-10-25T00:00:00",
        "last_modified_date": "2011-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.6290",
        "title": "Modelling Constraint Solver Architecture Design as a Constraint Problem",
        "authors": [
            "Ian P. Gent",
            "Chris Jefferson",
            "Lars Kotthoff",
            "Ian Miguel"
        ],
        "abstract": "Designing component-based constraint solvers is a complex problem. Some components are required, some are optional and there are interdependencies between the components. Because of this, previous approaches to solver design and modification have been ad-hoc and limited. We present a system that transforms a description of the components and the characteristics of the target constraint solver into a constraint problem. Solving this problem yields the description of a valid solver. Our approach represents a significant step towards the automated design and synthesis of constraint solvers that are specialised for individual constraint problem classes or instances.\n    ",
        "submission_date": "2011-10-28T00:00:00",
        "last_modified_date": "2011-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.6589",
        "title": "A cognitive diversity framework for radar target classification",
        "authors": [
            "Amit K. Mishra",
            "Chris Baker"
        ],
        "abstract": "Classification of targets by radar has proved to be notoriously difficult with the best systems still yet to attain sufficiently high levels of performance and reliability. In the current contribution we explore a new design of radar based target recognition, where angular diversity is used in a cognitive manner to attain better performance. Performance is bench- marked against conventional classification schemes. The proposed scheme can easily be extended to cognitive target recognition based on multiple diversity strategies.\n    ",
        "submission_date": "2011-10-30T00:00:00",
        "last_modified_date": "2011-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0039",
        "title": "Reasoning with Very Expressive Fuzzy Description Logics",
        "authors": [
            "I. Horrocks",
            "J. Z. Pan",
            "G. Stamou",
            "G. Stoilos",
            "V. Tzouvaras"
        ],
        "abstract": "It is widely recognized today that the management of imprecision and vagueness will yield more intelligent and realistic knowledge-based applications. Description Logics (DLs) are a family of knowledge representation languages that have gained considerable attention the last decade, mainly due to their decidability and the existence of empirically high performance of reasoning algorithms. In this paper, we extend the well known fuzzy ALC DL to the fuzzy SHIN DL, which extends the fuzzy ALC DL with transitive role axioms (S), inverse roles (I), role hierarchies (H) and number restrictions (N). We illustrate why transitive role axioms are difficult to handle in the presence of fuzzy interpretations and how to handle them properly. Then we extend these results by adding role hierarchies and finally number restrictions. The main contributions of the paper are the decidability proof of the fuzzy DL languages fuzzy-SI and fuzzy-SHIN, as well as decision procedures for the knowledge base satisfiability problem of the fuzzy-SI and fuzzy-SHIN.\n    ",
        "submission_date": "2011-10-31T00:00:00",
        "last_modified_date": "2011-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0040",
        "title": "New Inference Rules for Max-SAT",
        "authors": [
            "C. M. Li",
            "F. Manya",
            "J. Planes"
        ],
        "abstract": "Exact Max-SAT solvers, compared with SAT solvers, apply little inference at each node of the proof tree. Commonly used SAT inference rules like unit propagation produce a simplified formula that preserves satisfiability but, unfortunately, solving the Max-SAT problem for the simplified formula is not equivalent to solving it for the original formula. In this paper, we define a number of original inference rules that, besides being applied efficiently, transform Max-SAT instances into equivalent Max-SAT instances which are easier to solve. The soundness of the rules, that can be seen as refinements of unit resolution adapted to Max-SAT, are proved in a novel and simple way via an integer programming transformation. With the aim of finding out how powerful the inference rules are in practice, we have developed a new Max-SAT solver, called MaxSatz, which incorporates those rules, and performed  an experimental investigation. The results provide empirical evidence that MaxSatz is very competitive, at least, on random Max-2SAT, random Max-3SAT, Max-Cut, and Graph 3-coloring instances, as well as on the benchmarks from the Max-SAT Evaluation 2006.\n    ",
        "submission_date": "2011-10-31T00:00:00",
        "last_modified_date": "2011-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0041",
        "title": "On the Formal Semantics of Speech-Act Based Communication in an Agent-Oriented Programming Language",
        "authors": [
            "R. H. Bordini",
            "A. F. Moreira",
            "R. Vieira",
            "M. Wooldridge"
        ],
        "abstract": "Research on agent communication languages has typically taken the speech acts paradigm as its starting point. Despite their manifest attractions, speech-act models of communication have several serious disadvantages as a foundation for communication in artificial agent systems. In particular, it has proved to be extremely difficult to give a satisfactory semantics to speech-act based agent communication languages. In part, the problem is that speech-act semantics typically make reference to the \"mental states\" of agents (their beliefs, desires, and intentions), and there is in general no way to attribute such attitudes to arbitrary computational agents. In addition, agent programming languages have only had their semantics formalised for abstract, stand-alone versions, neglecting aspects such as communication primitives. With respect to communication, implemented agent programming languages have tended to be rather ad hoc. This paper addresses both of these problems, by giving semantics to speech-act based messages received by an AgentSpeak agent. AgentSpeak is a logic-based agent programming language which incorporates the main features of the PRS model of reactive planning systems. The paper builds upon a structural operational semantics to AgentSpeak that we developed in previous work. The main contributions of this paper are as follows: an extension of our earlier work on the theoretical foundations of AgentSpeak interpreters; a computationally grounded semantics for (the core) performatives used in speech-act based agent communication languages; and a well-defined extension of AgentSpeak that supports agent communication.\n    ",
        "submission_date": "2011-10-31T00:00:00",
        "last_modified_date": "2011-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0043",
        "title": "Obtaining Reliable Feedback for Sanctioning Reputation Mechanisms",
        "authors": [
            "B. Faltings",
            "R. Jurca"
        ],
        "abstract": "Reputation mechanisms offer an effective alternative to verification authorities for building trust in electronic markets with moral hazard. Future clients guide their business decisions by considering the feedback from past transactions; if truthfully exposed, cheating behavior is sanctioned and thus becomes irrational.\nIt therefore becomes important to ensure that rational clients have the right incentives to report honestly. As an alternative to side-payment schemes that explicitly reward truthful reports, we show that honesty can emerge as a rational behavior when clients have a repeated presence in the market. To this end we describe a mechanism that supports an equilibrium where truthful feedback is obtained. Then we characterize the set of pareto-optimal equilibria of the mechanism, and derive an upper bound on the percentage of false reports that can be recorded by the mechanism. An important role in the existence of this bound is played by the fact that rational clients can establish a reputation for reporting honestly.\n    ",
        "submission_date": "2011-10-31T00:00:00",
        "last_modified_date": "2011-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0044",
        "title": "Probabilistic Planning via Heuristic Forward Search and Weighted Model Counting",
        "authors": [
            "C. Domshlak",
            "J. Hoffmann"
        ],
        "abstract": "We present a new algorithm for probabilistic planning with no observability.  Our algorithm, called  Probabilistic-FF, extends the heuristic forward-search machinery of Conformant-FF to problems with probabilistic uncertainty about both the initial state and action effects. Specifically,  Probabilistic-FF combines Conformant-FFs techniques with a powerful machinery for weighted model counting in (weighted) CNFs, serving to elegantly define both the search space and the heuristic function. Our evaluation of  Probabilistic-FF shows its fine scalability in a range of probabilistic domains, constituting a several orders of magnitude improvement over previous results in this area. We use a problematic case to point out the main open issue to be addressed by further research.\n    ",
        "submission_date": "2011-10-31T00:00:00",
        "last_modified_date": "2011-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0049",
        "title": "Conjunctive Query Answering for the Description Logic SHIQ",
        "authors": [
            "Birte Glimm",
            "Ian Horrocks",
            "Carsten Lutz",
            "Ulrike Sattler"
        ],
        "abstract": "Conjunctive queries play an important role as an expressive query language for Description Logics (DLs). Although modern DLs usually provide for transitive roles, conjunctive query answering over DL knowledge bases is only poorly understood if transitive roles are admitted in the query. In this paper, we consider unions of conjunctive queries over knowledge bases formulated in the prominent DL SHIQ and allow transitive roles in both the query and the knowledge base. We show decidability of query answering in this setting and establish two tight complexity bounds: regarding combined complexity, we prove that there is a deterministic algorithm for query answering that needs time single exponential in the size of the KB and double exponential in the size of the query, which is optimal. Regarding data complexity, we prove containment in co-NP.\n    ",
        "submission_date": "2011-10-31T00:00:00",
        "last_modified_date": "2011-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0051",
        "title": "Qualitative System Identification from Imperfect Data",
        "authors": [
            "George M. Coghill",
            "Ross D. King",
            "Ashwin Srinivasan"
        ],
        "abstract": "Experience in the physical sciences suggests that the only realistic means of understanding complex systems is through the use of mathematical models. Typically, this has come to mean the identification of quantitative models expressed as differential equations. Quantitative modelling works best when the structure of the model (i.e., the form of the equations) is known; and the primary concern is one of estimating the values of the parameters in the model. For complex biological systems, the model-structure is rarely known and the modeler has to deal with both model-identification and parameter-estimation. In this paper we are concerned with providing automated assistance to the first of these problems. Specifically, we examine the identification by machine of the structural relationships between experimentally observed variables. These relationship will be expressed in the form of qualitative abstractions of a quantitative model. Such qualitative models may not only provide clues to the precise quantitative model, but also assist in understanding the essence of that model. Our position in this paper is that background knowledge incorporating system modelling principles can be used to constrain effectively the set of good qualitative models. Utilising the model-identification framework provided by Inductive Logic Programming (ILP) we present empirical support for this position using a series of increasingly complex artificial datasets. The results are obtained with qualitative and quantitative data subject to varying amounts of noise and different degrees of sparsity. The results also point to the presence of a set of qualitative states, which we term kernel subsets, that may be necessary for a qualitative model-learner to learn correct models. We demonstrate scalability of the method to biological system modelling by identification of the glycolysis metabolic pathway from data.\n    ",
        "submission_date": "2011-10-31T00:00:00",
        "last_modified_date": "2011-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0053",
        "title": "Exploiting Subgraph Structure in Multi-Robot Path Planning",
        "authors": [
            "Malcolm Ross Kinsella Ryan"
        ],
        "abstract": "Multi-robot path planning is difficult due to the combinatorial explosion of the search space with every new robot added. Complete search of the combined state-space soon becomes intractable. In this paper we present a novel form of abstraction that allows us to plan much more efficiently. The key to this abstraction is the partitioning of the map into subgraphs of known structure with entry and exit restrictions which we can represent compactly. Planning then becomes a search in the much smaller space of subgraph configurations. Once an abstract plan is found, it can be quickly resolved into a correct (but possibly sub-optimal) concrete plan without the need for further search. We prove that this technique is sound and complete and demonstrate its practical effectiveness on a real map.\n",
        "submission_date": "2011-10-31T00:00:00",
        "last_modified_date": "2011-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0054",
        "title": "CTL Model Update for System Modifications",
        "authors": [
            "Yulin Ding",
            "Y. Ding",
            "Yan Zhang",
            "Y. Zhang"
        ],
        "abstract": "Model checking is a promising technology, which has been applied for verification of many hardware and software systems. In this paper, we introduce the concept of model update towards the development of an automatic system modification tool that extends model checking functions. We define primitive update operations on the models of Computation Tree Logic (CTL) and formalize the principle of minimal change for CTL model update. These primitive update operations, together with the underlying minimal change principle, serve as the foundation for CTL model update. Essential semantic and computational characterizations are provided for our CTL model update approach. We then describe a formal algorithm that implements this approach. We also illustrate two case studies of CTL model updates for the well-known microwave oven example and the Andrew File System 1, from which we further propose a method to optimize the update results in complex system modifications.\n    ",
        "submission_date": "2011-10-31T00:00:00",
        "last_modified_date": "2011-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0055",
        "title": "Extended RDF as a Semantic Foundation of Rule Markup Languages",
        "authors": [
            "Anastasia Analyti",
            "Grigoris Antoniou",
            "Carlos Viegas Dam\u00e1sio",
            "Gerd Wagner"
        ],
        "abstract": "Ontologies and automated reasoning are the building blocks of the Semantic Web initiative. Derivation rules can be included in an ontology to define derived concepts, based on base concepts. For example, rules allow to define the extension of a class or property, based on a complex relation between the extensions of the same or other classes and properties. On the other hand, the inclusion of negative information both in the form of negation-as-failure and explicit negative information is also needed to enable various forms of reasoning. In this paper, we extend RDF graphs with weak and strong negation, as well as derivation rules. The ERDF stable model semantics of the extended framework (Extended RDF) is defined, extending RDF(S) semantics. A distinctive feature of our theory, which is based on Partial Logic, is that both truth and falsity extensions of properties and classes are considered, allowing for truth value gaps. Our framework supports both closed-world and open-world reasoning through the explicit representation of the particular closed-world assumptions and the ERDF ontological categories of total properties and total classes.\n    ",
        "submission_date": "2011-10-31T00:00:00",
        "last_modified_date": "2011-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0056",
        "title": "The Complexity of Planning Problems With Simple Causal Graphs",
        "authors": [
            "Omer Gim\u00e9nez",
            "Anders Jonsson"
        ],
        "abstract": "We present three new complexity results for classes of planning problems with simple causal graphs. First, we describe a polynomial-time algorithm that uses macros to generate plans for the class 3S of planning problems with binary state variables and acyclic causal graphs. This implies that plan generation may be tractable even when a planning problem has an exponentially long minimal solution. We also prove that the problem of plan existence for planning problems with multi-valued variables and chain causal graphs is NP-hard. Finally, we show that plan existence for planning problems with binary state variables and polytree causal graphs is NP-complete.\n    ",
        "submission_date": "2011-10-31T00:00:00",
        "last_modified_date": "2011-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0059",
        "title": "Loosely Coupled Formulations for Automated Planning: An Integer Programming Perspective",
        "authors": [
            "Menkes Hector Louis van den Briel",
            "Thomas Vossen",
            "Subbarao Kambhampati"
        ],
        "abstract": "We represent planning as a set of loosely coupled network flow problems, where each network corresponds to one of the state variables in the planning domain. The network nodes correspond to the state variable values and the network arcs correspond to the value transitions. The planning problem is to find a path (a sequence of actions) in each network such that, when merged, they constitute a feasible plan. In this paper we present a number of integer programming formulations that model these loosely coupled networks with varying degrees of flexibility. Since merging may introduce exponentially many ordering constraints we implement a so-called branch-and-cut algorithm, in which these constraints are dynamically generated and added to the formulation when needed. Our results are very promising, they improve upon previous planning as integer programming approaches and lay the foundation for integer programming approaches for cost optimal planning.\n    ",
        "submission_date": "2011-10-31T00:00:00",
        "last_modified_date": "2011-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0060",
        "title": "A Constraint Programming Approach for Solving a Queueing Control Problem",
        "authors": [
            "Daria Terekhov",
            "J. Christopher Beck"
        ],
        "abstract": "In a facility with front room and back room operations, it is useful to switch workers between the rooms in order to cope with changing customer demand. Assuming stochastic customer arrival and service times, we seek a policy for switching workers such that the expected customer waiting time is minimized while the expected back room staffing is sufficient to perform all work. Three novel constraint programming models and several shaving procedures for these models are presented. Experimental results show that a model based on closed-form expressions together with a combination of shaving procedures is the most efficient. This model is able to find and prove optimal solutions for many problem instances within a reasonable run-time. Previously, the only available approach was a heuristic algorithm. Furthermore, a hybrid method combining the heuristic and the best constraint programming method is shown to perform as well as the heuristic in terms of solution quality over time, while achieving the same performance in terms of proving optimality as the pure constraint programming model. This is the first work of which we are aware that solves such queueing-based problems with constraint programming.\n    ",
        "submission_date": "2011-10-31T00:00:00",
        "last_modified_date": "2011-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0062",
        "title": "Optimal and Approximate Q-value Functions for Decentralized POMDPs",
        "authors": [
            "Frans A. Oliehoek",
            "Matthijs T. J. Spaan",
            "Nikos Vlassis"
        ],
        "abstract": "Decision-theoretic planning is a popular approach to sequential decision making problems, because it treats uncertainty in sensing and acting in a principled way. In single-agent frameworks like MDPs and POMDPs, planning can be carried out by resorting to Q-value functions: an optimal Q-value function Q* is computed in a recursive manner by dynamic programming, and then an optimal policy is extracted from Q*. In this paper we study whether similar Q-value functions can be defined for decentralized POMDP models (Dec-POMDPs), and how policies can be extracted from such value functions. We define two forms of the optimal Q-value function for Dec-POMDPs: one that gives a normative description as the Q-value function of an optimal pure joint policy and another one that is sequentially rational and thus gives a recipe for computation. This computation, however, is infeasible for all but the smallest problems. Therefore, we analyze various approximate Q-value functions that allow for efficient computation. We describe how they relate, and we prove that they all provide an upper bound to the optimal Q-value function Q*. Finally, unifying some previous approaches for solving Dec-POMDPs, we describe a family of algorithms for extracting policies from such Q-value functions, and perform an experimental evaluation on existing test problems, including a new firefighting benchmark problem.\n    ",
        "submission_date": "2011-10-31T00:00:00",
        "last_modified_date": "2011-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0065",
        "title": "Communication-Based Decomposition Mechanisms for Decentralized MDPs",
        "authors": [
            "Claudia V. Goldman",
            "Shlomo Zilberstein"
        ],
        "abstract": "Multi-agent planning in stochastic environments can be framed formally as a decentralized Markov decision problem. Many real-life distributed problems that arise in manufacturing, multi-robot coordination and information gathering scenarios can be formalized using this framework. However, finding the optimal solution in the general case is hard, limiting the applicability of recently developed algorithms. This paper provides a practical approach for solving decentralized control problems when communication among the decision makers is possible, but costly. We develop the notion of communication-based mechanism that allows us to decompose a decentralized MDP into multiple single-agent problems. In this framework, referred to as decentralized semi-Markov decision process with direct communication (Dec-SMDP-Com), agents operate separately between communications. We show that finding an optimal mechanism is equivalent to solving optimally a Dec-SMDP-Com. We also provide a heuristic search algorithm that converges on the optimal decomposition. Restricting the decomposition to some specific types of local behaviors reduces significantly the complexity of planning. In particular, we present a polynomial-time algorithm for the case in which individual agents perform goal-oriented behaviors between communications. The paper concludes with an additional tractable algorithm that enables the introduction of human knowledge, thereby reducing the overall problem to finding the best time to communicate. Empirical results show that these approaches provide good approximate solutions.\n    ",
        "submission_date": "2011-10-31T00:00:00",
        "last_modified_date": "2011-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0067",
        "title": "A General Theory of Additive State Space Abstractions",
        "authors": [
            "Fan Yang",
            "Joseph Culberson",
            "Robert Holte",
            "Uzi Zahavi",
            "Ariel Felner"
        ],
        "abstract": "Informally, a set of abstractions of a state space S is additive if the distance between any two states in S is always greater than or equal to the sum of the corresponding distances in the abstract spaces. The first known additive abstractions, called disjoint pattern databases, were experimentally demonstrated to produce state of the art performance on certain state spaces. However, previous applications were restricted to state spaces with special properties, which precludes disjoint pattern databases from being defined for several commonly used testbeds, such as Rubiks Cube, TopSpin and the Pancake puzzle. In this paper we give a general definition of additive abstractions that can be applied to any state space and prove that heuristics based on additive abstractions are consistent as well as admissible. We use this new definition to create additive abstractions for these testbeds and show experimentally that well chosen additive abstractions can reduce search time substantially for the (18,4)-TopSpin puzzle and by three orders of magnitude over state of the art methods for the 17-Pancake puzzle. We also derive a way of testing if the heuristic value returned by additive abstractions is provably too low and show that the use of this test can reduce search time for the 15-puzzle and TopSpin by roughly a factor of two.\n    ",
        "submission_date": "2011-10-31T00:00:00",
        "last_modified_date": "2011-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0068",
        "title": "First Order Decision Diagrams for Relational MDPs",
        "authors": [
            "Chenggang Wang",
            "Saket Joshi",
            "Roni Khardon"
        ],
        "abstract": "Markov decision processes capture sequential decision making under uncertainty, where an agent must choose actions so as to optimize long term reward. The paper studies efficient reasoning mechanisms for Relational Markov Decision Processes (RMDP) where world states have an internal relational structure that can be naturally described in terms of objects and relations among them. Two contributions are presented. First, the paper develops First Order Decision Diagrams (FODD), a new compact representation for functions over relational structures, together with a set of operators to combine FODDs, and novel reduction techniques to keep the representation small. Second, the paper shows how FODDs can be used to develop solutions for RMDPs, where reasoning is performed at the abstract level and the resulting optimal policy is independent of domain size (number of objects) or instantiation. In particular, a variant of the value iteration algorithm is developed by using special operations over FODDs, and the algorithm is shown to converge to the optimal policy.\n    ",
        "submission_date": "2011-10-31T00:00:00",
        "last_modified_date": "2011-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0860",
        "title": "Clause/Term Resolution and Learning in the Evaluation of Quantified Boolean Formulas",
        "authors": [
            "E. Giunchiglia",
            "M. Narizzano",
            "A. Tacchella"
        ],
        "abstract": " Resolution is the rule of inference at the basis of most procedures for automated reasoning. In these procedures, the input formula is first translated into an equisatisfiable formula in conjunctive normal form (CNF) and then represented as a set of clauses. Deduction starts by inferring new clauses by resolution, and goes on until the empty clause is generated or satisfiability of the set of clauses is proven, e.g., because no new clauses can be generated.\n",
        "submission_date": "2011-09-26T00:00:00",
        "last_modified_date": "2011-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.1321",
        "title": "MIVAR: Transition from Productions to Bipartite Graphs MIVAR Nets and Practical Realization of Automated Constructor of Algorithms Handling More than Three Million Production Rules",
        "authors": [
            "Oleg O. Varlamov"
        ],
        "abstract": "The theoretical transition from the graphs of production systems to the bipartite graphs of the MIVAR nets is shown. Examples of the implementation of the MIVAR nets in the formalisms of matrixes and graphs are given. The linear computational complexity of algorithms for automated building of objects and rules of the MIVAR nets is theoretically proved. On the basis of the MIVAR nets the UDAV software complex is developed, handling more than 1.17 million objects and more than 3.5 million rules on ordinary computers. The results of experiments that confirm a linear computational complexity of the MIVAR method of information processing are given.\n",
        "submission_date": "2011-11-05T00:00:00",
        "last_modified_date": "2011-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.1486",
        "title": "Embedding Description Logic Programs into Default Logic",
        "authors": [
            "Yisong Wang",
            "Jia-Huai You",
            "Li Yan Yuan",
            "Yi-Dong Shen",
            "Thomas Eiter"
        ],
        "abstract": "Description logic programs (dl-programs) under the answer set semantics formulated by Eiter {\\em et al.} have been considered as a prominent formalism for integrating rules and ontology knowledge bases. A question of interest has been whether dl-programs can be captured in a general formalism of nonmonotonic logic. In this paper, we study the possibility of embedding dl-programs into default logic. We show that dl-programs under the strong and weak answer set semantics can be embedded in default logic by combining two translations, one of which eliminates the constraint operator from nonmonotonic dl-atoms and the other translates a dl-program into a default theory. For dl-programs without nonmonotonic dl-atoms but with the negation-as-failure operator, our embedding is polynomial, faithful, and modular. In addition, our default logic encoding can be extended in a simple way to capture recently proposed weakly well-supported answer set semantics, for arbitrary dl-programs. These results reinforce the argument that default logic can serve as a fruitful foundation for query-based approaches to integrating ontology and rules. With its simple syntax and intuitive semantics, plus available computational results, default logic can be considered an attractive approach to integration of ontology and rules.\n    ",
        "submission_date": "2011-11-07T00:00:00",
        "last_modified_date": "2011-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.1941",
        "title": "Semantic-Driven e-Government: Application of Uschold and King Ontology Building Methodology for Semantic Ontology Models Development",
        "authors": [
            "Jean Vincent Fonou-Dombeu",
            "Magda Huisman"
        ],
        "abstract": "Electronic government (e-government) has been one of the most active areas of ontology development during the past six years. In e-government, ontologies are being used to describe and specify e-government services (e-services) because they enable easy composition, matching, mapping and merging of various e-government services. More importantly, they also facilitate the semantic integration and interoperability of e-government services. However, it is still unclear in the current literature how an existing ontology building methodology can be applied to develop semantic ontology models in a government service domain. In this paper the Uschold and King ontology building methodology is applied to develop semantic ontology models in a government service domain. Firstly, the Uschold and King methodology is presented, discussed and applied to build a government domain ontology. Secondly, the domain ontology is evaluated for semantic consistency using its semi-formal representation in Description Logic. Thirdly, an alignment of the domain ontology with the Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE) upper level ontology is drawn to allow its wider visibility and facilitate its integration with existing metadata standard. Finally, the domain ontology is formally written in Web Ontology Language (OWL) to enable its automatic processing by computers. The study aims to provide direction for the application of existing ontology building methodologies in the Semantic Web development processes of e-government domain specific ontology models; which would enable their repeatability in other e-government projects and strengthen the adoption of semantic technologies in e-government.\n    ",
        "submission_date": "2011-11-08T00:00:00",
        "last_modified_date": "2011-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.2249",
        "title": "SATzilla: Portfolio-based Algorithm Selection for SAT",
        "authors": [
            "Lin Xu",
            "Frank Hutter",
            "Holger H. Hoos",
            "Kevin Leyton-Brown"
        ],
        "abstract": "It has been widely observed that there is no single \"dominant\" SAT solver; instead, different solvers perform best on different instances. Rather than following the traditional approach of choosing the best solver for a given class of instances, we advocate making this decision online on a per-instance basis. Building on previous work, we describe SATzilla, an automated approach for constructing per-instance algorithm portfolios for SAT that use so-called empirical hardness models to choose among their constituent solvers. This approach takes as input a distribution of problem instances and a set of component solvers, and constructs a portfolio optimizing a given objective function (such as mean runtime, percent of instances solved, or score in a competition). The excellent performance of SATzilla was independently verified in the 2007 SAT Competition, where our SATzilla07 solvers won three gold, one silver and one bronze medal. In this article, we go well beyond SATzilla07 by making the portfolio construction scalable and completely automated, and improving it by integrating local search solvers as candidate solvers, by predicting performance score instead of runtime, and by using hierarchical hardness models that take into account different types of SAT instances. We demonstrate the effectiveness of these new techniques in extensive experimental results on data sets including instances from the most recent SAT competition.\n    ",
        "submission_date": "2011-10-31T00:00:00",
        "last_modified_date": "2011-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.2763",
        "title": "8-Valent Fuzzy Logic for Iris Recognition and Biometry",
        "authors": [
            "N. Popescu-Bodorin",
            "V.E. Balas",
            "I.M. Motoc"
        ],
        "abstract": "This paper shows that maintaining logical consistency of an iris recognition system is a matter of finding a suitable partitioning of the input space in enrollable and unenrollable pairs by negotiating the user comfort and the safety of the biometric system. In other words, consistent enrollment is mandatory in order to preserve system consistency. A fuzzy 3-valued disambiguated model of iris recognition is proposed and analyzed in terms of completeness, consistency, user comfort and biometric safety. It is also shown here that the fuzzy 3-valued model of iris recognition is hosted by an 8-valued Boolean algebra of modulo 8 integers that represents the computational formalization in which a biometric system (a software agent) can achieve the artificial understanding of iris recognition in a logically consistent manner.\n    ",
        "submission_date": "2011-11-08T00:00:00",
        "last_modified_date": "2011-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.3690",
        "title": "New Candidates Welcome! Possible Winners with respect to the Addition of New Candidates",
        "authors": [
            "Yann Chevaleyre",
            "J\u00e9r\u00f4me Lang",
            "Nicolas Maudet",
            "J\u00e9r\u00f4me Monnot",
            "Lirong Xia"
        ],
        "abstract": "In voting contexts, some new candidates may show up in the course of the process. In this case, we may want to determine which of the initial candidates are possible winners, given that a fixed number $k$ of new candidates will be added. We give a computational study of this problem, focusing on scoring rules, and we provide a formal comparison with related problems such as control via adding candidates or cloning.\n    ",
        "submission_date": "2011-11-15T00:00:00",
        "last_modified_date": "2011-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.3934",
        "title": "Model-based Utility Functions",
        "authors": [
            "Bill Hibbard"
        ],
        "abstract": "Orseau and Ring, as well as Dewey, have recently described problems, including self-delusion, with the behavior of agents using various definitions of utility functions. An agent's utility function is defined in terms of the agent's history of interactions with its environment. This paper argues, via two examples, that the behavior problems can be avoided by formulating the utility function in two steps: 1) inferring a model of the environment from interactions, and 2) computing utility as a function of the environment model. Basing a utility function on a model that the agent must learn implies that the utility function must initially be expressed in terms of specifications to be matched to structures in the learned model. These specifications constitute prior assumptions about the environment so this approach will not work with arbitrary environments. But the approach should work for agents designed by humans to act in the physical world. The paper also addresses the issue of self-modifying agents and shows that if provided with the possibility to modify their utility functions agents will not choose to do so, under some usual assumptions.\n    ",
        "submission_date": "2011-11-16T00:00:00",
        "last_modified_date": "2012-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.4083",
        "title": "Unbiased Statistics of a CSP - A Controlled-Bias Generator",
        "authors": [
            "Denis Berthier"
        ],
        "abstract": "We show that estimating the complexity (mean and distribution) of the instances of a fixed size Constraint Satisfaction Problem (CSP) can be very hard. We deal with the main two aspects of the problem: defining a measure of complexity and generating random unbiased instances. For the first problem, we rely on a general framework and a measure of complexity we presented at CISSE08. For the generation problem, we restrict our analysis to the Sudoku example and we provide a solution that also explains why it is so difficult.\n    ",
        "submission_date": "2011-11-17T00:00:00",
        "last_modified_date": "2011-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.4232",
        "title": "A Model of Spatial Thinking for Computational Intelligence",
        "authors": [
            "Kirill A. Sorudeykin"
        ],
        "abstract": "Trying to be effective (no matter who exactly and in what field) a person face the problem which inevitably destroys all our attempts to easily get to a desired goal. The problem is the existence of some insuperable barriers for our mind, anotherwords barriers for principles of thinking. They are our clue and main reason for research. Here we investigate these barriers and their features exposing the nature of mental process. We start from special structures which reflect the ways to define relations between objects. Then we came to realizing about what is the material our mind uses to build thoughts, to make conclusions, to understand, to form reasoning, etc. This can be called a mental dynamics. After this the nature of mental barriers on the required level of abstraction as well as the ways to pass through them became clear. We begin to understand why thinking flows in such a way, with such specifics and with such limitations we can observe in reality. This can help us to be more optimal. At the final step we start to understand, what ma-thematical models can be applied to such a picture. We start to express our thoughts in a language of mathematics, developing an apparatus for our Spatial Theory of Mind, suitable to represent processes and infrastructure of thinking. We use abstract algebra and stay invariant in relation to the nature of objects.\n    ",
        "submission_date": "2011-11-17T00:00:00",
        "last_modified_date": "2011-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.4267",
        "title": "Control Neuronal por Modelo Inverso de un Servosistema Usando Algoritmos de Aprendizaje Levenberg-Marquardt y Bayesiano",
        "authors": [
            "Victor A. Rodriguez-Toro",
            "Jaime E. Garzon",
            "Jesus A. Lopez"
        ],
        "abstract": "In this paper we present the experimental results of the neural network control of a servo-system in order to control its speed. The control strategy is implemented by using an inverse-model control based on Artificial Neural Networks (ANNs). The network training was performed using two learning algorithms: Levenberg-Marquardt and Bayesian regularization. We evaluate the generalization capability for each method according to both the correct operation of the controller to follow the reference signal, and the control efforts developed by the ANN-based controller.\n    ",
        "submission_date": "2011-11-18T00:00:00",
        "last_modified_date": "2011-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.5312",
        "title": "Representations and Ensemble Methods for Dynamic Relational Classification",
        "authors": [
            "Ryan A. Rossi",
            "Jennifer Neville"
        ],
        "abstract": "Temporal networks are ubiquitous and evolve over time by the addition, deletion, and changing of links, nodes, and attributes. Although many relational datasets contain temporal information, the majority of existing techniques in relational learning focus on static snapshots and ignore the temporal dynamics. We propose a framework for discovering temporal representations of relational data to increase the accuracy of statistical relational learning algorithms. The temporal relational representations serve as a basis for classification, ensembles, and pattern mining in evolving domains. The framework includes (1) selecting the time-varying relational components (links, attributes, nodes), (2) selecting the temporal granularity, (3) predicting the temporal influence of each time-varying relational component, and (4) choosing the weighted relational classifier. Additionally, we propose temporal ensemble methods that exploit the temporal-dimension of relational data. These ensembles outperform traditional and more sophisticated relational ensembles while avoiding the issue of learning the most optimal representation. Finally, the space of temporal-relational models are evaluated using a sample of classifiers. In all cases, the proposed temporal-relational classifiers outperform competing models that ignore the temporal information. The results demonstrate the capability and necessity of the temporal-relational representations for classification, ensembles, and for mining temporal datasets.\n    ",
        "submission_date": "2011-11-22T00:00:00",
        "last_modified_date": "2011-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.5689",
        "title": "Revisiting Numerical Pattern Mining with Formal Concept Analysis",
        "authors": [
            "Mehdi Kaytoue",
            "Sergei O. Kuznetsov",
            "Amedeo Napoli"
        ],
        "abstract": "In this paper, we investigate the problem of mining numerical data in the framework of Formal Concept Analysis. The usual way is to use a scaling procedure --transforming numerical attributes into binary ones-- leading either to a loss of information or of efficiency, in particular w.r.t. the volume of extracted patterns. By contrast, we propose to directly work on numerical data in a more precise and efficient way, and we prove it. For that, the notions of closed patterns, generators and equivalent classes are revisited in the numerical context. Moreover, two original algorithms are proposed and used in an evaluation involving real-world data, showing the predominance of the present approach.\n    ",
        "submission_date": "2011-11-24T00:00:00",
        "last_modified_date": "2011-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.5720",
        "title": "A GP-MOEA/D Approach for Modelling Total Electron Content over Cyprus",
        "authors": [
            "Andreas Konstantinidis",
            "Haris Haralambous",
            "Alexandros Agapitos",
            "Harris Papadopoulos"
        ],
        "abstract": "Vertical Total Electron Content (vTEC) is an ionospheric characteristic used to derive the signal delay imposed by the ionosphere on near-vertical trans-ionospheric links. The major aim of this paper is to design a prediction model based on the main factors that influence the variability of this parameter on a diurnal, seasonal and long-term time-scale. The model should be accurate and general (comprehensive) enough for efficiently approximating the high variations of vTEC. However, good approximation and generalization are conflicting objectives. For this reason a Genetic Programming (GP) with Multi-objective Evolutionary Algorithm based on Decomposition characteristics (GP-MOEA/D) is designed and proposed for modeling vTEC over Cyprus. Experimental results show that the Multi-Objective GP-model, considering real vTEC measurements obtained over a period of 11 years, has produced a good approximation of the modeled parameter and can be implemented as a local model to account for the ionospheric imposed error in positioning. Particulary, the GP-MOEA/D approach performs better than a Single Objective Optimization GP, a GP with Non-dominated Sorting Genetic Algorithm-II (NSGA-II) characteristics and the previously proposed Neural Network-based approach in most cases.\n    ",
        "submission_date": "2011-11-24T00:00:00",
        "last_modified_date": "2011-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.6117",
        "title": "Principles of Solomonoff Induction and AIXI",
        "authors": [
            "Peter Sunehag",
            "Marcus Hutter"
        ],
        "abstract": "We identify principles characterizing Solomonoff Induction by demands on an agent's external behaviour. Key concepts are rationality, computability, indifference and time consistency. Furthermore, we discuss extensions to the full AI case to derive AIXI.\n    ",
        "submission_date": "2011-11-25T00:00:00",
        "last_modified_date": "2011-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.6191",
        "title": "Pattern-Based Classification: A Unifying Perspective",
        "authors": [
            "Bj\u00f6rn Bringmann",
            "Siegfried Nijssen",
            "Albrecht Zimmermann"
        ],
        "abstract": "The use of patterns in predictive models is a topic that has received a lot of attention in recent years. Pattern mining can help to obtain models for structured domains, such as graphs and sequences, and has been proposed as a means to obtain more accurate and more interpretable models. Despite the large amount of publications devoted to this topic, we believe however that an overview of what has been accomplished in this area is missing. This paper presents our perspective on this evolving area. We identify the principles of pattern mining that are important when mining patterns for models and provide an overview of pattern-based classification methods. We categorize these methods along the following dimensions: (1) whether they post-process a pre-computed set of patterns or iteratively execute pattern mining algorithms; (2) whether they select patterns model-independently or whether the pattern selection is guided by a model. We summarize the results that have been obtained for each of these methods.\n    ",
        "submission_date": "2011-11-26T00:00:00",
        "last_modified_date": "2011-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.6401",
        "title": "Graph based E-Government web service composition",
        "authors": [
            "Hajar Elmaghraoui",
            "Imane Zaoui",
            "Dalila Chiadmi",
            "Laila Benhlima"
        ],
        "abstract": "Nowadays, e-government has emerged as a government policy to improve the quality and efficiency of public administrations. By exploiting the potential of new information and communication technologies, government agencies are providing a wide spectrum of online services. These services are composed of several web services that comply with well defined processes. One of the big challenges is the need to optimize the composition of the elementary web services. In this paper, we present a solution for optimizing the computation effort in web service composition. Our method is based on Graph Theory. We model the semantic relationship between the involved web services through a directed graph. Then, we compute all shortest paths using for the first time, an extended version of the Floyd-Warshall algorithm.\n    ",
        "submission_date": "2011-11-28T00:00:00",
        "last_modified_date": "2011-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.6713",
        "title": "An Enhanced Indexing And Ranking Technique On The Semantic Web",
        "authors": [
            "Ahmed Tolba",
            "Nabila Eladawi",
            "Mohammed Elmogy"
        ],
        "abstract": "With the fast growth of the Internet, more and more information is available on the Web. The Semantic Web has many features which cannot be handled by using the traditional search engines. It extracts metadata for each discovered Web documents in RDF or OWL formats, and computes relations between documents. We proposed a hybrid indexing and ranking technique for the Semantic Web which finds relevant documents and computes the similarity among a set of documents. First, it returns with the most related document from the repository of Semantic Web Documents (SWDs) by using a modified version of the ObjectRank technique. Then, it creates a sub-graph for the most related SWDs. Finally, It returns the hubs and authorities of these document by using the HITS algorithm. Our technique increases the quality of the results and decreases the execution time of processing the user's query.\n    ",
        "submission_date": "2011-11-29T00:00:00",
        "last_modified_date": "2011-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.6790",
        "title": "Constraining the Size Growth of the Task Space with Socially Guided Intrinsic Motivation using Demonstrations",
        "authors": [
            "Sao Mai Nguyen",
            "Adrien Baranes",
            "Pierre-Yves Oudeyer"
        ],
        "abstract": "This paper presents an algorithm for learning a highly redundant inverse model in continuous and non-preset environments. Our Socially Guided Intrinsic Motivation by Demonstrations (SGIM-D) algorithm combines the advantages of both social learning and intrinsic motivation, to specialise in a wide range of skills, while lessening its dependence on the teacher. SGIM-D is evaluated on a fishing skill learning experiment.\n    ",
        "submission_date": "2011-11-29T00:00:00",
        "last_modified_date": "2011-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.6825",
        "title": "A Fuzzy Realistic Mobility Model For Ad hoc Networks",
        "authors": [
            "Alireza Amirshahi",
            "Mahmood Fathi",
            "Morteza Romoozi",
            "Mohammad Assarian"
        ],
        "abstract": "Realistic mobility models can demonstrate more precise evaluation results because their parameters are closer to the reality. In this paper a realistic Fuzzy Mobility Model has been proposed. This model has rules which is changeable depending on nodes and environment conditions. This model is more complete and precise than the other mobility models and this is the advantage of this model. After simulation, it was found out that not only considering nodes movement as being imprecise (fuzzy) has a positive effects on most of ad hoc network parameters, but also, more importantly as they are closer to the real world condition, they can have a more positive effect on the implementation of ad hoc network protocols.\n    ",
        "submission_date": "2011-11-29T00:00:00",
        "last_modified_date": "2011-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.6843",
        "title": "Understanding the Social Cascading of Geekspeak and the Upshots for Social Cognitive Systems",
        "authors": [
            "Micha\u0142 B. Paradowski",
            "\u0141ukasz Jonak"
        ],
        "abstract": "Barring swarm robotics, a substantial share of current machine-human and machine-machine learning and interaction mechanisms are being developed and fed by results of agent-based computer simulations, game-theoretic models, or robotic experiments based on a dyadic communication pattern. Yet, in real life, humans no less frequently communicate in groups, and gain knowledge and take decisions basing on information cumulatively gleaned from more than one single source. These properties should be taken into consideration in the design of autonomous artificial cognitive systems construed to interact with learn from more than one contact or 'neighbour'. To this end, significant practical import can be gleaned from research applying strict science methodology to human and social phenomena, e.g. to discovery of realistic creativity potential spans, or the 'exposure thresholds' after which new information could be accepted by a cognitive agent. The results will be presented of a project analysing the social propagation of neologisms in a microblogging service. From local, low-level interactions and information flows between agents inventing and imitating discrete lexemes we aim to describe the processes of the emergence of more global systemic order and dynamics, using the latest methods of complexity science. Whether in order to mimic them, or to 'enhance' them, parameters gleaned from complexity science approaches to humans' social and humanistic behaviour should subsequently be incorporated as points of reference in the field of robotics and human-machine interaction.\n    ",
        "submission_date": "2011-11-29T00:00:00",
        "last_modified_date": "2012-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.6883",
        "title": "Dynamics of Knowledge in DeLP through Argument Theory Change",
        "authors": [
            "Mart\u00edn O. Moguillansky",
            "Nicol\u00e1s D. Rotstein",
            "Marcelo A. Falappa",
            "Alejandro J. Garc\u00eda",
            "Guillermo R. Simari"
        ],
        "abstract": "This article is devoted to the study of methods to change defeasible logic programs (de.l.p.s) which are the knowledge bases used by the Defeasible Logic Programming (DeLP) interpreter. DeLP is an argumentation formalism that allows to reason over potentially inconsistent de.l.p.s. Argument Theory Change (ATC) studies certain aspects of belief revision in order to make them suitable for abstract argumentation systems. In this article, abstract arguments are rendered concrete by using the particular rule-based defeasible logic adopted by DeLP. The objective of our proposal is to define prioritized argument revision operators \u00e0 la ATC for de.l.p.s, in such a way that the newly inserted argument ends up undefeated after the revision, thus warranting its conclusion. In order to ensure this warrant, the de.l.p. has to be changed in concordance with a minimal change principle. To this end, we discuss different minimal change criteria that could be adopted. Finally, an algorithm is presented, implementing the argument revision operations.\n    ",
        "submission_date": "2011-11-29T00:00:00",
        "last_modified_date": "2011-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.7025",
        "title": "Task Interaction in an HTN Planner",
        "authors": [
            "Il\u010de Georgievski",
            "Alexander Lazovik",
            "Marco Aiello"
        ],
        "abstract": "Hierarchical Task Network (HTN) planning uses task decomposition to plan for an executable sequence of actions as a solution to a problem. In order to reason effectively, an HTN planner needs expressive domain knowledge. For instance, a simplified HTN planning system such as JSHOP2 uses such expressivity and avoids some task interactions due to the increased complexity of the planning process. We address the possibility of simplifying the domain representation needed for an HTN planner to find good solutions, especially in real-world domains describing home and building automation environments. We extend the JSHOP2 planner to reason about task interaction that happens when task's effects are already achieved by other tasks. The planner then prunes some of the redundant searches that can occur due to the planning process's interleaving nature. We evaluate the original and our improved planner on two benchmark domains. We show that our planner behaves better by using simplified domain knowledge and outperforms JSHOP2 in a number of relevant cases.\n    ",
        "submission_date": "2011-11-30T00:00:00",
        "last_modified_date": "2011-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.7190",
        "title": "Developing Embodied Multisensory Dialogue Agents",
        "authors": [
            "Micha\u0142 B. Paradowski"
        ],
        "abstract": "A few decades of work in the AI field have focused efforts on developing a new generation of systems which can acquire knowledge via interaction with the world. Yet, until very recently, most such attempts were underpinned by research which predominantly regarded linguistic phenomena as separated from the brain and body. This could lead one into believing that to emulate linguistic behaviour, it suffices to develop 'software' operating on abstract representations that will work on any computational machine. This picture is inaccurate for several reasons, which are elucidated in this paper and extend beyond sensorimotor and semantic resonance. Beginning with a review of research, I list several heterogeneous arguments against disembodied language, in an attempt to draw conclusions for developing embodied multisensory agents which communicate verbally and non-verbally with their environment. Without taking into account both the architecture of the human brain, and embodiment, it is unrealistic to replicate accurately the processes which take place during language acquisition, comprehension, production, or during non-linguistic actions. While robots are far from isomorphic with humans, they could benefit from strengthened associative connections in the optimization of their processes and their reactivity and sensitivity to environmental stimuli, and in situated human-machine interaction. The concept of multisensory integration should be extended to cover linguistic input and the complementary information combined from temporally coincident sensory impressions.\n    ",
        "submission_date": "2011-11-29T00:00:00",
        "last_modified_date": "2012-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.0508",
        "title": "Label Ranking with Abstention: Predicting Partial Orders by Thresholding Probability Distributions (Extended Abstract)",
        "authors": [
            "Weiwei Cheng",
            "Eyke H\u00fcllermeier"
        ],
        "abstract": "We consider an extension of the setting of label ranking, in which the learner is allowed to make predictions in the form of partial instead of total orders. Predictions of that kind are interpreted as a partial abstention: If the learner is not sufficiently certain regarding the relative order of two alternatives, it may abstain from this decision and instead declare these alternatives as being incomparable. We propose a new method for learning to predict partial orders that improves on an existing approach, both theoretically and empirically. Our method is based on the idea of thresholding the probabilities of pairwise preferences between labels as induced by a predicted (parameterized) probability distribution on the set of all rankings.\n    ",
        "submission_date": "2011-12-02T00:00:00",
        "last_modified_date": "2011-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.1489",
        "title": "Multi-granular Perspectives on Covering",
        "authors": [
            "Wan-Li Chen"
        ],
        "abstract": "Covering model provides a general framework for granular computing in that overlapping among granules are almost indispensable. For any given covering, both intersection and union of covering blocks containing an element are exploited as granules to form granular worlds at different abstraction levels, respectively, and transformations among these different granular worlds are also discussed. As an application of the presented multi-granular perspective on covering, relational interpretation and axiomization of four types of covering based rough upper approximation operators are investigated, which can be dually applied to lower ones.\n    ",
        "submission_date": "2011-12-07T00:00:00",
        "last_modified_date": "2011-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.1670",
        "title": "Data Mining Session-Based Patient Reported Outcomes (PROs) in a Mental Health Setting: Toward Data-Driven Clinical Decision Support and Personalized Treatment",
        "authors": [
            "Casey Bennett",
            "Thomas Doub",
            "April Bragg",
            "Jason Luellen",
            "Christina Van Regenmorter",
            "Jennifer Lockman",
            "Randall Reiserer"
        ],
        "abstract": "The CDOI outcome measure - a patient-reported outcome (PRO) instrument utilizing direct client feedback - was implemented in a large, real-world behavioral healthcare setting in order to evaluate previous findings from smaller controlled studies. PROs provide an alternative window into treatment effectiveness based on client perception and facilitate detection of problems/symptoms for which there is no discernible measure (e.g. pain). The principal focus of the study was to evaluate the utility of the CDOI for predictive modeling of outcomes in a live clinical setting. Implementation factors were also addressed within the framework of the Theory of Planned Behavior by linking adoption rates to implementation practices and clinician perceptions. The results showed that the CDOI does contain significant capacity to predict outcome delta over time based on baseline and early change scores in a large, real-world clinical setting, as suggested in previous research. The implementation analysis revealed a number of critical factors affecting successful implementation and adoption of the CDOI outcome measure, though there was a notable disconnect between clinician intentions and actual behavior. Most importantly, the predictive capacity of the CDOI underscores the utility of direct client feedback measures such as PROs and their potential use as the basis for next generation clinical decision support tools and personalized treatment approaches.\n    ",
        "submission_date": "2011-12-07T00:00:00",
        "last_modified_date": "2011-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.2095",
        "title": "Real-time face swapping as a tool for understanding infant self-recognition",
        "authors": [
            "Sao Mai Nguyen",
            "Masaki Ogino",
            "Minoru Asada"
        ],
        "abstract": "To study the preference of infants for contingency of movements and familiarity of faces during self-recognition task, we built, as an accurate and instantaneous imitator, a real-time face- swapper for videos. We present a non-constraint face-swapper based on 3D visual tracking that achieves real-time performance through parallel computing. Our imitator system is par- ticularly suited for experiments involving children with Autistic Spectrum Disorder who are often strongly disturbed by the constraints of other methods.\n    ",
        "submission_date": "2011-12-09T00:00:00",
        "last_modified_date": "2011-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.2113",
        "title": "Incremental Slow Feature Analysis: Adaptive and Episodic Learning from High-Dimensional Input Streams",
        "authors": [
            "Varun Raj Kompella",
            "Matthew Luciw",
            "Juergen Schmidhuber"
        ],
        "abstract": "Slow Feature Analysis (SFA) extracts features representing the underlying causes of changes within a temporally coherent high-dimensional raw sensory input signal. Our novel incremental version of SFA (IncSFA) combines incremental Principal Components Analysis and Minor Components Analysis. Unlike standard batch-based SFA, IncSFA adapts along with non-stationary environments, is amenable to episodic training, is not corrupted by outliers, and is covariance-free. These properties make IncSFA a generally useful unsupervised preprocessor for autonomous learning agents and robots. In IncSFA, the CCIPCA and MCA updates take the form of Hebbian and anti-Hebbian updating, extending the biological plausibility of SFA. In both single node and deep network versions, IncSFA learns to encode its input streams (such as high-dimensional video) by informative slow features representing meaningful abstract environmental properties. It can handle cases where batch SFA fails.\n    ",
        "submission_date": "2011-12-09T00:00:00",
        "last_modified_date": "2011-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.2144",
        "title": "An Information Theoretic Analysis of Decision in Computer Chess",
        "authors": [
            "Alexandru Godescu"
        ],
        "abstract": "The basis of the method proposed in this article is the idea that information is one of the most important factors in strategic decisions, including decisions in computer chess and other strategy games. The model proposed in this article and the algorithm described are based on the idea of a information theoretic basis of decision in strategy games . The model generalizes and provides a mathematical justification for one of the most popular search algorithms used in leading computer chess programs, the fractional ply scheme. However, despite its success in leading computer chess applications, until now few has been published about this method. The article creates a fundamental basis for this method in the axioms of information theory, then derives the principles used in programming the search and describes mathematically the form of the coefficients. One of the most important parameters of the fractional ply search is derived from fundamental principles. Until now this coefficient has been usually handcrafted or determined from intuitive elements or data mining. There is a deep, information theoretical justification for such a parameter. In one way the method proposed is a generalization of previous methods. More important, it shows why the fractional depth ply scheme is so powerful. It is because the algorithm navigates along the lines where the highest information gain is possible. A working and original implementation has been written and tested for this algorithm and is provided in the appendix. The article is essentially self-contained and gives proper background knowledge and references. The assumptions are intuitive and in the direction expected and described intuitively by great champions of chess.\n    ",
        "submission_date": "2011-12-09T00:00:00",
        "last_modified_date": "2011-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.2149",
        "title": "Information and Search in Computer Chess",
        "authors": [
            "Alexandru Godescu"
        ],
        "abstract": "The article describes a model of chess based on information theory. A mathematical model of the partial depth scheme is outlined and a formula for the partial depth added for each ply is calculated from the principles of the model. An implementation of alpha-beta with partial depth is given. The method is tested using an experimental strategy having as objective to show the effect of allocation of a higher amount of search resources on areas of the search tree with higher information. The search proceeds in the direction of lines with higher information gain. The effects on search performance of allocating higher search resources on lines with higher information gain are tested experimentaly and conclusive results are obtained. In order to isolate the effects of the partial depth scheme no other heuristic is used.\n    ",
        "submission_date": "2011-12-09T00:00:00",
        "last_modified_date": "2011-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.2640",
        "title": "Threshold Choice Methods: the Missing Link",
        "authors": [
            "Jos\u00e9 Hern\u00e1ndez-Orallo",
            "Peter Flach",
            "C\u00e8sar Ferri"
        ],
        "abstract": "Many performance metrics have been introduced for the evaluation of classification performance, with different origins and niches of application: accuracy, macro-accuracy, area under the ROC curve, the ROC convex hull, the absolute error, and the Brier score (with its decomposition into refinement and calibration). One way of understanding the relation among some of these metrics is the use of variable operating conditions (either in the form of misclassification costs or class proportions). Thus, a metric may correspond to some expected loss over a range of operating conditions. One dimension for the analysis has been precisely the distribution we take for this range of operating conditions, leading to some important connections in the area of proper scoring rules. However, we show that there is another dimension which has not received attention in the analysis of performance metrics. This new dimension is given by the decision rule, which is typically implemented as a threshold choice method when using scoring models. In this paper, we explore many old and new threshold choice methods: fixed, score-uniform, score-driven, rate-driven and optimal, among others. By calculating the loss of these methods for a uniform range of operating conditions we get the 0-1 loss, the absolute error, the Brier score (mean squared error), the AUC and the refinement loss respectively. This provides a comprehensive view of performance metrics as well as a systematic approach to loss minimisation, namely: take a model, apply several threshold choice methods consistent with the information which is (and will be) available about the operating condition, and compare their expected losses. In order to assist in this procedure we also derive several connections between the aforementioned performance metrics, and we highlight the role of calibration in choosing the threshold choice method.\n    ",
        "submission_date": "2011-12-12T00:00:00",
        "last_modified_date": "2012-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.2681",
        "title": "Inference in Probabilistic Logic Programs with Continuous Random Variables",
        "authors": [
            "Muhammad Asiful Islam",
            "C. R. Ramakrishnan",
            "I. V. Ramakrishnan"
        ],
        "abstract": "Probabilistic Logic Programming (PLP), exemplified by Sato and Kameya's PRISM, Poole's ICL, Raedt et al's ProbLog and Vennekens et al's LPAD, is aimed at combining statistical and logical knowledge representation and inference. A key characteristic of PLP frameworks is that they are conservative extensions to non-probabilistic logic programs which have been widely used for knowledge representation. PLP frameworks extend traditional logic programming semantics to a distribution semantics, where the semantics of a probabilistic logic program is given in terms of a distribution over possible models of the program. However, the inference techniques used in these works rely on enumerating sets of explanations for a query answer. Consequently, these languages permit very limited use of random variables with continuous distributions. In this paper, we present a symbolic inference procedure that uses constraints and represents sets of explanations without enumeration. This permits us to reason over PLPs with Gaussian or Gamma-distributed random variables (in addition to discrete-valued random variables) and linear equality constraints over reals. We develop the inference procedure in the context of PRISM; however the procedure's core ideas can be easily applied to other PLP languages as well. An interesting aspect of our inference procedure is that PRISM's query evaluation process becomes a special case in the absence of any continuous random variables in the program. The symbolic inference procedure enables us to reason over complex probabilistic models such as Kalman filters and a large subclass of Hybrid Bayesian networks that were hitherto not possible in PLP frameworks. (To appear in Theory and Practice of Logic Programming).\n    ",
        "submission_date": "2011-12-12T00:00:00",
        "last_modified_date": "2012-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.4057",
        "title": "Performance Evaluation of Road Traffic Control Using a Fuzzy Cellular Model",
        "authors": [
            "Bart\u0142omiej P\u0142aczek"
        ],
        "abstract": "In this paper a method is proposed for performance evaluation of road traffic control systems. The method is designed to be implemented in an on-line simulation environment, which enables optimisation of adaptive traffic control strategies. Performance measures are computed using a fuzzy cellular traffic model, formulated as a hybrid system combining cellular automata and fuzzy calculus. Experimental results show that the introduced method allows the performance to be evaluated using imprecise traffic measurements. Moreover, the fuzzy definitions of performance measures are convenient for uncertainty determination in traffic control decisions.\n    ",
        "submission_date": "2011-12-17T00:00:00",
        "last_modified_date": "2011-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.5309",
        "title": "POWERPLAY: Training an Increasingly General Problem Solver by Continually Searching for the Simplest Still Unsolvable Problem",
        "authors": [
            "J\u00fcrgen Schmidhuber"
        ],
        "abstract": "Most of computer science focuses on automatically solving given computational problems. I focus on automatically inventing or discovering problems in a way inspired by the playful behavior of animals and humans, to train a more and more general problem solver from scratch in an unsupervised fashion. Consider the infinite set of all computable descriptions of tasks with possibly computable solutions. The novel algorithmic framework POWERPLAY (2011) continually searches the space of possible pairs of new tasks and modifications of the current problem solver, until it finds a more powerful problem solver that provably solves all previously learned tasks plus the new one, while the unmodified predecessor does not. Wow-effects are achieved by continually making previously learned skills more efficient such that they require less time and space. New skills may (partially) re-use previously learned skills. POWERPLAY's search orders candidate pairs of tasks and solver modifications by their conditional computational (time & space) complexity, given the stored experience so far. The new task and its corresponding task-solving skill are those first found and validated. The computational costs of validating new tasks need not grow with task repertoire size. POWERPLAY's ongoing search for novelty keeps breaking the generalization abilities of its present solver. This is related to Goedel's sequence of increasingly powerful formal theories based on adding formerly unprovable statements to the axioms without affecting previously provable theorems. The continually increasing repertoire of problem solving procedures can be exploited by a parallel search for solutions to additional externally posed tasks. POWERPLAY may be viewed as a greedy but practical implementation of basic principles of creativity. A first experimental analysis can be found in separate papers [53,54].\n    ",
        "submission_date": "2011-12-22T00:00:00",
        "last_modified_date": "2012-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.5370",
        "title": "Enhancing Support for Knowledge Works: A relatively unexplored vista of computing research",
        "authors": [
            "Arijit Laha"
        ],
        "abstract": "Let us envision a new class of IT systems, the \"Support Systems for Knowledge Works\" or SSKW. An SSKW can be defined as a system built for providing comprehensive support to human knowledge-workers while performing instances of complex knowledge-works of a particular type within a particular domain of professional activities To get an idea what an SSKW-enabled work environment can be like, let us look into a hypothetical scenario that depicts the interaction between a physician and a patient-care SSKW during the activity of diagnosing a patient.\n    ",
        "submission_date": "2011-12-22T00:00:00",
        "last_modified_date": "2011-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.5381",
        "title": "Improving the Efficiency of Approximate Inference for Probabilistic Logical Models by means of Program Specialization",
        "authors": [
            "Daan Fierens"
        ],
        "abstract": "We consider the task of performing probabilistic inference with probabilistic logical models. Many algorithms for approximate inference with such models are based on sampling. From a logic programming perspective, sampling boils down to repeatedly calling the same queries on a knowledge base composed of a static part and a dynamic part. The larger the static part, the more redundancy there is in these repeated calls. This is problematic since inefficient sampling yields poor approximations.\n",
        "submission_date": "2011-12-22T00:00:00",
        "last_modified_date": "2011-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.0245",
        "title": "Use of Python and Phoenix-M Interface in Robotics",
        "authors": [
            "Shubham Chakraborty"
        ],
        "abstract": "In this paper I will show how to use Python programming with a computer interface such as Phoenix-M 1 to drive simple robots. In my quest towards Artificial Intelligence(AI) I am experimenting with a lot of different possibilities in Robotics. This one will try to mimic the working of a simple insect's nervous system using hard wiring and some minimal software usage. This is the precursor to my advanced robotics and AI integration where I plan to use a new paradigm of AI based on Machine Learning and Self Consciousness via Knowledge Feedback and Update Process.\n    ",
        "submission_date": "2010-12-31T00:00:00",
        "last_modified_date": "2010-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.0309",
        "title": "Concrete Sentence Spaces for Compositional Distributional Models of Meaning",
        "authors": [
            "Edward Grefenstette",
            "Mehrnoosh Sadrzadeh",
            "Stephen Clark",
            "Bob Coecke",
            "Stephen Pulman"
        ],
        "abstract": "Coecke, Sadrzadeh, and Clark (",
        "submission_date": "2010-12-31T00:00:00",
        "last_modified_date": "2010-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.0428",
        "title": "The Local Optimality of Reinforcement Learning by Value Gradients, and its Relationship to Policy Gradient Learning",
        "authors": [
            "Michael Fairbank",
            "Eduardo Alonso"
        ],
        "abstract": "In this theoretical paper we are concerned with the problem of learning a value function by a smooth general function approximator, to solve a deterministic episodic control problem in a large continuous state space. It is shown that learning the gradient of the value-function at every point along a trajectory generated by a greedy policy is a sufficient condition for the trajectory to be locally extremal, and often locally optimal, and we argue that this brings greater efficiency to value-function learning. This contrasts to traditional value-function learning in which the value-function must be learnt over the whole of state space.\n",
        "submission_date": "2011-01-02T00:00:00",
        "last_modified_date": "2011-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.1715",
        "title": "Finding Consensus Bayesian Network Structures",
        "authors": [
            "Jose M. Pe\u00f1a"
        ],
        "abstract": "Suppose that multiple experts (or learning algorithms) provide us with alternative Bayesian network (BN) structures over a domain, and that we are interested in combining them into a single consensus BN structure. Specifically, we are interested in that the consensus BN structure only represents independences all the given BN structures agree upon and that it has as few parameters associated as possible. In this paper, we prove that there may exist several non-equivalent consensus BN structures and that finding one of them is NP-hard. Thus, we decide to resort to heuristics to find an approximated consensus BN structure. In this paper, we consider the heuristic proposed in \\citep{MatzkevichandAbramson1992,MatzkevichandAbramson1993a,MatzkevichandAbramson1993b}. This heuristic builds upon two algorithms, called Methods A and B, for efficiently deriving the minimal directed independence map of a BN structure relative to a given node ordering. Methods A and B are claimed to be correct although no proof is provided (a proof is just sketched). In this paper, we show that Methods A and B are not correct and propose a correction of them.\n    ",
        "submission_date": "2011-01-10T00:00:00",
        "last_modified_date": "2011-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.2301",
        "title": "A Factorial Experiment on Scalability of Search Based Software Testing",
        "authors": [
            "Arash Mehrmand",
            "Robert Feldt"
        ],
        "abstract": "Software testing is an expensive process, which is vital in the industry. Construction of the test-data in software testing requires the major cost and to decide which method to use in order to generate the test data is important. This paper discusses the efficiency of search-based algorithms (preferably genetic algorithm) versus random testing, in soft- ware test-data generation. This study differs from all previous studies due to sample programs (SUTs) which are used. Since we want to in- crease the complexity of SUTs gradually, and the program generation is automatic as well, Grammatical Evolution is used to guide the program generation. SUTs are generated according to the grammar we provide, with different levels of complexity. SUTs will first undergo genetic al- gorithm and then random testing. Based on the test results, this paper recommends one method to use for automation of software testing.\n    ",
        "submission_date": "2011-01-12T00:00:00",
        "last_modified_date": "2011-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.3755",
        "title": "Transductive-Inductive Cluster Approximation Via Multivariate Chebyshev Inequality",
        "authors": [
            "Shriprakash Sinha"
        ],
        "abstract": "Approximating adequate number of clusters in multidimensional data is an open area of research, given a level of compromise made on the quality of acceptable results. The manuscript addresses the issue by formulating a transductive inductive learning algorithm which uses multivariate Chebyshev inequality. Considering clustering problem in imaging, theoretical proofs for a particular level of compromise are derived to show the convergence of the reconstruction error to a finite value with increasing (a) number of unseen examples and (b) the number of clusters, respectively. Upper bounds for these error rates are also proved. Non-parametric estimates of these error from a random sample of sequences empirically point to a stable number of clusters. Lastly, the generalization of algorithm can be applied to multidimensional data sets from different fields.\n    ",
        "submission_date": "2011-01-19T00:00:00",
        "last_modified_date": "2012-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.4101",
        "title": "Context Capture in Software Development",
        "authors": [
            "Bruno Antunes",
            "Francisco Correia",
            "Paulo Gomes"
        ],
        "abstract": "The context of a software developer is something hard to define and capture, as it represents a complex network of elements across different dimensions that are not limited to the work developed on an IDE. We propose the definition of a software developer context model that takes into account all the dimensions that characterize the work environment of the developer. We are especially focused on what the software developer context encompasses at the project level and how it can be captured. The experimental work done so far show that useful context information can be extracted from project management tools. The extraction, analysis and availability of this context information can be used to enrich the work environment of the developer with additional knowledge to support her/his work.\n    ",
        "submission_date": "2011-01-21T00:00:00",
        "last_modified_date": "2011-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.4103",
        "title": "Evolutionary Mechanics: new engineering principles for the emergence of flexibility in a dynamic and uncertain world",
        "authors": [
            "James M Whitacre",
            "Philipp Rohlfshagen",
            "Axel Bender",
            "Xin Yao"
        ],
        "abstract": "Engineered systems are designed to deftly operate under predetermined conditions yet are notoriously fragile when unexpected perturbations arise. In contrast, biological systems operate in a highly flexible manner; learn quickly adequate responses to novel conditions, and evolve new routines/traits to remain competitive under persistent environmental change. A recent theory on the origins of biological flexibility has proposed that degeneracy - the existence of multi-functional components with partially overlapping functions - is a primary determinant of the robustness and adaptability found in evolved systems. While degeneracy's contribution to biological flexibility is well documented, there has been little investigation of degeneracy design principles for achieving flexibility in systems engineering. Actually, the conditions that can lead to degeneracy are routinely eliminated in engineering design.\n",
        "submission_date": "2011-01-21T00:00:00",
        "last_modified_date": "2011-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.4450",
        "title": "Adaptive Submodular Optimization under Matroid Constraints",
        "authors": [
            "Daniel Golovin",
            "Andreas Krause"
        ],
        "abstract": "Many important problems in discrete optimization require maximization of a monotonic submodular function subject to matroid constraints. For these problems, a simple greedy algorithm is guaranteed to obtain near-optimal solutions. In this article, we extend this classic result to a general class of adaptive optimization problems under partial observability, where each choice can depend on observations resulting from past choices. Specifically, we prove that a natural adaptive greedy algorithm provides a $1/(p+1)$ approximation for the problem of maximizing an adaptive monotone submodular function subject to $p$ matroid constraints, and more generally over arbitrary $p$-independence systems. We illustrate the usefulness of our result on a complex adaptive match-making application.\n    ",
        "submission_date": "2011-01-24T00:00:00",
        "last_modified_date": "2011-01-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.4479",
        "title": "A Context-theoretic Framework for Compositionality in Distributional Semantics",
        "authors": [
            "Daoud Clarke"
        ],
        "abstract": "Techniques in which words are represented as vectors have proved useful in many applications in computational linguistics, however there is currently no general semantic formalism for representing meaning in terms of vectors. We present a framework for natural language semantics in which words, phrases and sentences are all represented as vectors, based on a theoretical analysis which assumes that meaning is determined by context.\n",
        "submission_date": "2011-01-24T00:00:00",
        "last_modified_date": "2011-01-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.4573",
        "title": "Finding undetected protein associations in cell signaling by belief propagation",
        "authors": [
            "M. Bailly-Bechet",
            "C. Borgs",
            "A. Braunstein",
            "J. Chayes",
            "A. Dagkessamanskaia",
            "J.-M. Fran\u00e7ois",
            "R. Zecchina"
        ],
        "abstract": "External information propagates in the cell mainly through signaling cascades and transcriptional activation, allowing it to react to a wide spectrum of environmental changes. High throughput experiments identify numerous molecular components of such cascades that may, however, interact through unknown partners. Some of them may be detected using data coming from the integration of a protein-protein interaction network and mRNA expression profiles. This inference problem can be mapped onto the problem of finding appropriate optimal connected subgraphs of a network defined by these datasets. The optimization procedure turns out to be computationally intractable in general. Here we present a new distributed algorithm for this task, inspired from statistical physics, and apply this scheme to alpha factor and drug perturbations data in yeast. We identify the role of the COS8 protein, a member of a gene family of previously unknown function, and validate the results by genetic experiments. The algorithm we present is specially suited for very large datasets, can run in parallel, and can be adapted to other problems in systems biology. On renowned benchmarks it outperforms other algorithms in the field.\n    ",
        "submission_date": "2011-01-24T00:00:00",
        "last_modified_date": "2011-01-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.4918",
        "title": "Using Feature Weights to Improve Performance of Neural Networks",
        "authors": [
            "Ridwan Al Iqbal"
        ],
        "abstract": "Different features have different relevance to a particular learning problem. Some features are less relevant; while some very important. Instead of selecting the most relevant features using feature selection, an algorithm can be given this knowledge of feature importance based on expert opinion or prior learning. Learning can be faster and more accurate if learners take feature importance into account. Correlation aided Neural Networks (CANN) is presented which is such an algorithm. CANN treats feature importance as the correlation coefficient between the target attribute and the features. CANN modifies normal feed-forward Neural Network to fit both correlation values and training data. Empirical evaluation shows that CANN is faster and more accurate than applying the two step approach of feature selection and then using normal learning algorithms.\n    ",
        "submission_date": "2011-01-25T00:00:00",
        "last_modified_date": "2011-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.4924",
        "title": "A Generalized Method for Integrating Rule-based Knowledge into Inductive Methods Through Virtual Sample Creation",
        "authors": [
            "Ridwan Al Iqbal"
        ],
        "abstract": "Hybrid learning methods use theoretical knowledge of a domain and a set of classified examples to develop a method for classification. Methods that use domain knowledge have been shown to perform better than inductive learners. However, there is no general method to include domain knowledge into all inductive learning algorithms as all hybrid methods are highly specialized for a particular algorithm. We present an algorithm that will take domain knowledge in the form of propositional rules, generate artificial examples from the rules and also remove instances likely to be flawed. This enriched dataset then can be used by any learning algorithm. Experimental results of different scenarios are shown that demonstrate this method to be more effective than simple inductive learning.\n    ",
        "submission_date": "2011-01-25T00:00:00",
        "last_modified_date": "2011-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.5632",
        "title": "Active Markov Information-Theoretic Path Planning for Robotic Environmental Sensing",
        "authors": [
            "Kian Hsiang Low",
            "John M. Dolan",
            "Pradeep Khosla"
        ],
        "abstract": "Recent research in multi-robot exploration and mapping has focused on sampling environmental fields, which are typically modeled using the Gaussian process (GP). Existing information-theoretic exploration strategies for learning GP-based environmental field maps adopt the non-Markovian problem structure and consequently scale poorly with the length of history of observations. Hence, it becomes computationally impractical to use these strategies for in situ, real-time active sampling. To ease this computational burden, this paper presents a Markov-based approach to efficient information-theoretic path planning for active sampling of GP-based fields. We analyze the time complexity of solving the Markov-based path planning problem, and demonstrate analytically that it scales better than that of deriving the non-Markovian strategies with increasing length of planning horizon. For a class of exploration tasks called the transect sampling task, we provide theoretical guarantees on the active sampling performance of our Markov-based policy, from which ideal environmental field conditions and sampling task settings can be established to limit its performance degradation due to violation of the Markov assumption. Empirical evaluation on real-world temperature and plankton density field data shows that our Markov-based policy can generally achieve active sampling performance comparable to that of the widely-used non-Markovian greedy policies under less favorable realistic field conditions and task settings while enjoying significant computational gain over them.\n    ",
        "submission_date": "2011-01-28T00:00:00",
        "last_modified_date": "2011-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.0230",
        "title": "Speeding up SAT solver by exploring CNF symmetries : Revisited",
        "authors": [
            "Arup Kumar Ghosh"
        ],
        "abstract": "Boolean Satisfiability solvers have gone through dramatic improvements in their performances and scalability over the last few years by considering symmetries. It has been shown that by using graph symmetries and generating symmetry breaking predicates (SBPs) it is possible to break symmetries in Conjunctive Normal Form (CNF). The SBPs cut down the search space to the nonsymmetric regions of the space without affecting the satisfiability of the CNF formula. The symmetry breaking predicates are created by representing the formula as a graph, finding the graph symmetries and using some symmetry extraction mechanism (Crawford et al.). Here in this paper we take one non-trivial CNF and explore its symmetries. Finally, we generate the SBPs and adding it to CNF we show how it helps to prune the search tree, so that SAT solver would take short time. Here we present the pruning procedure of the search tree from scratch, starting from the CNF and its graph representation. As we explore the whole mechanism by a non-trivial example, it would be easily comprehendible. Also we have given a new idea of generating symmetry breaking predicates for breaking symmetry in CNF, not derived from Crawford's conditions. At last we propose a backtrack SAT solver with inbuilt SBP generator.\n    ",
        "submission_date": "2011-02-01T00:00:00",
        "last_modified_date": "2011-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.0257",
        "title": "Emergence through Selection: The Evolution of a Scientific Challenge",
        "authors": [
            "Walter Quattrociocchi",
            "Frederic Amblard"
        ],
        "abstract": "One of the most interesting scientific challenges nowadays deals with the analysis and the understanding of complex networks' dynamics and how their processes lead to emergence according to the interactions among their components. In this paper we approach the definition of new methodologies for the visualization and the exploration of the dynamics at play in real dynamic social networks. We present a recently introduced formalism called TVG (for time-varying graphs), which was initially developed to model and analyze highly-dynamic and infrastructure-less communication networks such as mobile ad-hoc networks, wireless sensor networks, or vehicular networks. We discuss its applicability to complex networks in general, and social networks in particular, by showing how it enables the specification and analysis of complex dynamic phenomena in terms of temporal interactions, and allows to easily switch the perspective between local and global dynamics. As an example, we chose the case of scientific communities by analyzing portion of the ArXiv repository (ten years of publications in physics) focusing on the social determinants (e.g. goals and potential interactions among individuals) behind the emergence and the resilience of scientific communities. We consider that scientific communities are at the same time communities of practice (through co-authorship) and that they exist also as representations in the scientists' mind, since references to other scientists' works is not merely an objective link to a relevant work, but it reveals social objects that one manipulates, select and refers to. In the paper we show the emergence/selection of a community as a goal-driven preferential attachment toward a set of authors among which there are some key scientists (Nobel prizes).\n    ",
        "submission_date": "2011-02-01T00:00:00",
        "last_modified_date": "2011-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.0629",
        "title": "Time-Varying Graphs and Social Network Analysis: Temporal Indicators and Metrics",
        "authors": [
            "Nicola Santoro",
            "Walter Quattrociocchi",
            "Paola Flocchini",
            "Arnaud Casteigts",
            "Frederic Amblard"
        ],
        "abstract": "Most instruments - formalisms, concepts, and metrics - for social networks analysis fail to capture their dynamics. Typical systems exhibit different scales of dynamics, ranging from the fine-grain dynamics of interactions (which recently led researchers to consider temporal versions of distance, connectivity, and related indicators), to the evolution of network properties over longer periods of time. This paper proposes a general approach to study that evolution for both atemporal and temporal indicators, based respectively on sequences of static graphs and sequences of time-varying graphs that cover successive time-windows. All the concepts and indicators, some of which are new, are expressed using a time-varying graph formalism.\n    ",
        "submission_date": "2011-02-03T00:00:00",
        "last_modified_date": "2011-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.1027",
        "title": "Collective Classification of Textual Documents by Guided Self-Organization in T-Cell Cross-Regulation Dynamics",
        "authors": [
            "Alaa Abi-Haidar",
            "Luis M. Rocha"
        ],
        "abstract": "We present and study an agent-based model of T-Cell cross-regulation in the adaptive immune system, which we apply to binary classification. Our method expands an existing analytical model of T-cell cross-regulation (Carneiro et al. in Immunol Rev 216(1):48-68, 2007) that was used to study the self-organizing dynamics of a single population of T-Cells in interaction with an idealized antigen presenting cell capable of presenting a single antigen. With agent-based modeling we are able to study the self-organizing dynamics of multiple populations of distinct T-cells which interact via antigen presenting cells that present hundreds of distinct antigens. Moreover, we show that such self-organizing dynamics can be guided to produce an effective binary classification of antigens, which is competitive with existing machine learning methods when applied to biomedical text classification. More specifically, here we test our model on a dataset of publicly available full-text biomedical articles provided by the BioCreative challenge (Krallinger in The biocreative ii. 5 challenge overview, p 19, 2009). We study the robustness of our model's parameter configurations, and show that it leads to encouraging results comparable to state-of-the-art classifiers. Our results help us understand both T-cell cross-regulation as a general principle of guided self-organization, as well as its applicability to document classification. Therefore, we show that our bio-inspired algorithm is a promising novel method for biomedical article classification and for binary document classification in general.\n    ",
        "submission_date": "2011-02-04T00:00:00",
        "last_modified_date": "2011-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.1691",
        "title": "Schema Redescription in Cellular Automata: Revisiting Emergence in Complex Systems",
        "authors": [
            "Manuel Marques-Pita",
            "Luis M. Rocha"
        ],
        "abstract": "We present a method to eliminate redundancy in the transition tables of Boolean automata: schema redescription with two symbols. One symbol is used to capture redundancy of individual input variables, and another to capture permutability in sets of input variables: fully characterizing the canalization present in Boolean functions. Two-symbol schemata explain aspects of the behaviour of automata networks that the characterization of their emergent patterns does not capture. We use our method to compare two well-known cellular automata for the density classification task: the human engineered CA GKL, and another obtained via genetic programming (GP). We show that despite having very different collective behaviour, these rules are very similar. Indeed, GKL is a special case of GP. Therefore, we demonstrate that it is more feasible to compare cellular automata via schema redescriptions of their rules, than by looking at their emergent behaviour, leading us to question the tendency in complexity research to pay much more attention to emergent patterns than to local interactions.\n    ",
        "submission_date": "2011-02-08T00:00:00",
        "last_modified_date": "2011-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.1745",
        "title": "Restructuring in Combinatorial Optimization",
        "authors": [
            "Mark Sh. Levin"
        ],
        "abstract": "The paper addresses a new class of combinatorial problems which consist in restructuring of solutions (as structures) in combinatorial optimization. Two main features of the restructuring process are examined: (i) a cost of the restructuring, (ii) a closeness to a goal solution. This problem corresponds to redesign (improvement, upgrade) of modular systems or solutions. The restructuring approach is described and illustrated for the following combinatorial optimization problems: knapsack problem, multiple choice problem, assignment problem, spanning tree problems. Examples illustrate the restructuring processes.\n    ",
        "submission_date": "2011-02-08T00:00:00",
        "last_modified_date": "2011-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.1747",
        "title": "Graph Coalition Structure Generation",
        "authors": [
            "Thomas D. Voice",
            "Maria Polukarov",
            "Nicholas R. Jennings"
        ],
        "abstract": "We give the first analysis of the computational complexity of {\\it coalition structure generation over graphs}. Given an undirected graph $G=(N,E)$ and a valuation function $v:2^N\\rightarrow\\RR$ over the subsets of nodes, the problem is to find a partition of $N$ into connected subsets, that maximises the sum of the components' values. This problem is generally NP--complete; in particular, it is hard for a defined class of valuation functions which are {\\it independent of disconnected members}---that is, two nodes have no effect on each other's marginal contribution to their vertex separator. Nonetheless, for all such functions we provide bounds on the complexity of coalition structure generation over general and minor free graphs. Our proof is constructive and yields algorithms for solving corresponding instances of the problem. Furthermore, we derive polynomial time bounds for acyclic, $K_{2,3}$ and $K_4$ minor free graphs. However, as we show, the problem remains NP--complete for planar graphs, and hence, for any $K_k$ minor free graphs where $k\\geq 5$. Moreover, our hardness result holds for a particular subclass of valuation functions, termed {\\it edge sum}, where the value of each subset of nodes is simply determined by the sum of given weights of the edges in the induced subgraph.\n    ",
        "submission_date": "2011-02-08T00:00:00",
        "last_modified_date": "2011-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.1803",
        "title": "Proposing LT based Search in PDM Systems for Better Information Retrieval",
        "authors": [
            "Zeeshan Ahmed"
        ],
        "abstract": "PDM Systems contain and manage heavy amount of data but the search mechanism of most of the systems is not intelligent which can process user\"s natural language based queries to extract desired information. Currently available search mechanisms in almost all of the PDM systems are not very efficient and based on old ways of searching information by entering the relevant information to the respective fields of search forms to find out some specific information from attached repositories. Targeting this issue, a thorough research was conducted in fields of PDM Systems and Language Technology. Concerning the PDM System, conducted research provides the information about PDM and PDM Systems in detail. Concerning the field of Language Technology, helps in implementing a search mechanism for PDM Systems to search user\"s needed information by analyzing user\"s natural language based requests. The accomplished goal of this research was to support the field of PDM with a new proposition of a conceptual model for the implementation of natural language based search. The proposed conceptual model is successfully designed and partially implementation in the form of a prototype. Describing the proposition in detail the main concept, implementation designs and developed prototype of proposed approach is discussed in this paper. Implemented prototype is compared with respective functions of existing PDM systems .i.e., Windchill and CIM to evaluate its effectiveness against targeted challenges.\n    ",
        "submission_date": "2011-02-09T00:00:00",
        "last_modified_date": "2011-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.1889",
        "title": "Ologs: a categorical framework for knowledge representation",
        "authors": [
            "David I. Spivak",
            "Robert E. Kent"
        ],
        "abstract": "In this paper we introduce the olog, or ontology log, a category-theoretic model for knowledge representation (KR). Grounded in formal mathematics, ologs can be rigorously formulated and cross-compared in ways that other KR models (such as semantic networks) cannot. An olog is similar to a relational database schema; in fact an olog can serve as a data repository if desired. Unlike database schemas, which are generally difficult to create or modify, ologs are designed to be user-friendly enough that authoring or reconfiguring an olog is a matter of course rather than a difficult chore. It is hoped that learning to author ologs is much simpler than learning a database definition language, despite their similarity. We describe ologs carefully and illustrate with many examples. As an application we show that any primitive recursive function can be described by an olog. We also show that ologs can be aligned or connected together into a larger network using functors. The various methods of information flow and institutions can then be used to integrate local and global world-views. We finish by providing several different avenues for future research.\n    ",
        "submission_date": "2011-02-09T00:00:00",
        "last_modified_date": "2011-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.2174",
        "title": "Linear Temporal Logic and Propositional Schemata, Back and Forth (extended version)",
        "authors": [
            "Vincent Aravantinos",
            "Ricardo Caferra",
            "Nicolas Peltier"
        ],
        "abstract": "This paper relates the well-known Linear Temporal Logic with the logic of propositional schemata introduced by the authors. We prove that LTL is equivalent to a class of schemata in the sense that polynomial-time reductions exist from one logic to the other. Some consequences about complexity are given. We report about first experiments and the consequences about possible improvements in existing implementations are analyzed.\n    ",
        "submission_date": "2011-02-10T00:00:00",
        "last_modified_date": "2011-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.2336",
        "title": "Opinions within Media, Power and Gossip",
        "authors": [
            "Walter Quattrociocchi",
            "Rosaria Conte",
            "Elena Lodi"
        ],
        "abstract": "Despite the increasing diffusion of the Internet technology, TV remains the principal medium of communication. People's perceptions, knowledge, beliefs and opinions about matter of facts get (in)formed through the information reported on by the mass-media. However, a single source of information (and consensus) could be a potential cause of anomalies in the structure and evolution of a society. Hence, as the information available (and the way it is reported) is fundamental for our perceptions and opinions, the definition of conditions allowing for a good information to be disseminated is a pressing challenge. In this paper starting from a report on the last Italian political campaign in 2008, we derive a socio-cognitive computational model of opinion dynamics where agents get informed by different sources of information. Then, a what-if analysis, performed trough simulations on the model's parameters space, is shown. In particular, the scenario implemented includes three main streams of information acquisition, differing in both the contents and the perceived reliability of the messages spread. Agents' internal opinion is updated either by accessing one of the information sources, namely media and experts, or by exchanging information with one another. They are also endowed with cognitive mechanisms to accept, reject or partially consider the acquired information.\n    ",
        "submission_date": "2011-02-11T00:00:00",
        "last_modified_date": "2011-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.2468",
        "title": "Algorithmic Randomness as Foundation of Inductive Reasoning and Artificial Intelligence",
        "authors": [
            "Marcus Hutter"
        ],
        "abstract": "This article is a brief personal account of the past, present, and future of algorithmic randomness, emphasizing its role in inductive inference and artificial intelligence. It is written for a general audience interested in science and philosophy. Intuitively, randomness is a lack of order or predictability. If randomness is the opposite of determinism, then algorithmic randomness is the opposite of computability. Besides many other things, these concepts have been used to quantify Ockham's razor, solve the induction problem, and define intelligence.\n    ",
        "submission_date": "2011-02-12T00:00:00",
        "last_modified_date": "2011-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.2524",
        "title": "Multicriteria Steiner Tree Problem for Communication Network",
        "authors": [
            "Mark Sh. Levin",
            "Rustem I. Nuriakhmetov"
        ],
        "abstract": "This paper addresses combinatorial optimization scheme for solving the multicriteria Steiner tree problem for communication network topology design (e.g., wireless mesh network). The solving scheme is based on several models: multicriteria ranking, clustering, minimum spanning tree, and minimum Steiner tree problem. An illustrative numerical example corresponds to designing a covering long-distance Wi-Fi network (static Ad-Hoc network). The set of criteria (i.e., objective functions) involves the following: total cost, total edge length, overall throughput (capacity), and estimate of QoS. Obtained computing results show the suggested solving scheme provides good network topologies which can be compared with minimum spanning trees.\n    ",
        "submission_date": "2011-02-12T00:00:00",
        "last_modified_date": "2011-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.2738",
        "title": "Decision Theory with Prospect Interference and Entanglement",
        "authors": [
            "V.I. Yukalov",
            "D. Sornette"
        ],
        "abstract": "We present a novel variant of decision making based on the mathematical theory of separable Hilbert spaces. This mathematical structure captures the effect of superposition of composite prospects, including many incorporated intentions, which allows us to describe a variety of interesting fallacies and anomalies that have been reported to particularize the decision making of real human beings. The theory characterizes entangled decision making, non-commutativity of subsequent decisions, and intention interference. We demonstrate how the violation of the Savage's sure-thing principle, known as the disjunction effect, can be explained quantitatively as a result of the interference of intentions, when making decisions under uncertainty. The disjunction effects, observed in experiments, are accurately predicted using a theorem on interference alternation that we derive, which connects aversion-to-uncertainty to the appearance of negative interference terms suppressing the probability of actions. The conjunction fallacy is also explained by the presence of the interference terms. A series of experiments are analysed and shown to be in excellent agreement with a priori evaluation of interference effects. The conjunction fallacy is also shown to be a sufficient condition for the disjunction effect and novel experiments testing the combined interplay between the two effects are suggested.\n    ",
        "submission_date": "2011-02-14T00:00:00",
        "last_modified_date": "2011-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.2739",
        "title": "A General Framework for Development of the Cortex-like Visual Object Recognition System: Waves of Spikes, Predictive Coding and Universal Dictionary of Features",
        "authors": [
            "Sergey S. Tarasenko"
        ],
        "abstract": "This study is focused on the development of the cortex-like visual object recognition system. We propose a general framework, which consists of three hierarchical levels (modules). These modules functionally correspond to the V1, V4 and IT areas. Both bottom-up and top-down connections between the hierarchical levels V4 and IT are employed. The higher the degree of matching between the input and the preferred stimulus, the shorter the response time of the neuron. Therefore information about a single stimulus is distributed in time and is transmitted by the waves of spikes. The reciprocal connections and waves of spikes implement predictive coding: an initial hypothesis is generated on the basis of information delivered by the first wave of spikes and is tested with the information carried by the consecutive waves. The development is considered as extraction and accumulation of features in V4 and objects in IT. Once stored a feature can be disposed, if rarely activated. This cause update of feature repository. Consequently, objects in IT are also updated. This illustrates the growing process and dynamical change of topological structures of V4, IT and connections between these areas.\n    ",
        "submission_date": "2011-02-14T00:00:00",
        "last_modified_date": "2011-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.2748",
        "title": "Feature Selection via Sparse Approximation for Face Recognition",
        "authors": [
            "Yixiong Liang",
            "Lei Wang",
            "Yao Xiang",
            "Beiji Zou"
        ],
        "abstract": "Inspired by biological vision systems, the over-complete local features with huge cardinality are increasingly used for face recognition during the last decades. Accordingly, feature selection has become more and more important and plays a critical role for face data description and recognition. In this paper, we propose a trainable feature selection algorithm based on the regularized frame for face recognition. By enforcing a sparsity penalty term on the minimum squared error (MSE) criterion, we cast the feature selection problem into a combinatorial sparse approximation problem, which can be solved by greedy methods or convex relaxation methods. Moreover, based on the same frame, we propose a sparse Ho-Kashyap (HK) procedure to obtain simultaneously the optimal sparse solution and the corresponding margin vector of the MSE criterion. The proposed methods are used for selecting the most informative Gabor features of face images for recognition and the experimental results on benchmark face databases demonstrate the effectiveness of the proposed methods.\n    ",
        "submission_date": "2011-02-14T00:00:00",
        "last_modified_date": "2011-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.2749",
        "title": "Multi-task GLOH feature selection for human age estimation",
        "authors": [
            "Yixiong Liang",
            "Lingbo Liu",
            "Ying Xu",
            "Yao Xiang",
            "Beiji Zou"
        ],
        "abstract": "In this paper, we propose a novel age estimation method based on GLOH feature descriptor and multi-task learning (MTL). The GLOH feature descriptor, one of the state-of-the-art feature descriptor, is used to capture the age-related local and spatial information of face image. As the exacted GLOH features are often redundant, MTL is designed to select the most informative feature bins for age estimation problem, while the corresponding weights are determined by ridge regression. This approach largely reduces the dimensions of feature, which can not only improve performance but also decrease the computational burden. Experiments on the public available FG-NET database show that the proposed method can achieve comparable performance over previous approaches while using much fewer features.\n    ",
        "submission_date": "2011-02-14T00:00:00",
        "last_modified_date": "2011-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.3129",
        "title": "Automated Complexity Analysis Based on the Dependency Pair Method",
        "authors": [
            "Nao Hirokawa",
            "Georg Moser"
        ],
        "abstract": "This article is concerned with automated complexity analysis of term rewrite systems. Since these systems underlie much of declarative programming, time complexity of functions defined by rewrite systems is of particular interest. Among other results, we present a variant of the dependency pair method for analysing runtime complexities of term rewrite systems automatically. The established results significantly extent previously known techniques: we give examples of rewrite systems subject to our methods that could previously not been analysed automatically. Furthermore, the techniques have been implemented in the Tyrolean Complexity Tool. We provide ample numerical data for assessing the viability of the method.\n    ",
        "submission_date": "2011-02-15T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.3830",
        "title": "A linear framework for region-based image segmentation and inpainting involving curvature penalization",
        "authors": [
            "Thomas Schoenemann",
            "Fredrik Kahl",
            "Simon Masnou",
            "Daniel Cremers"
        ],
        "abstract": "We present the first method to handle curvature regularity in region-based image segmentation and inpainting that is independent of initialization.\n",
        "submission_date": "2011-02-18T00:00:00",
        "last_modified_date": "2011-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.3919",
        "title": "Inferring Disease and Gene Set Associations with Rank Coherence in Networks",
        "authors": [
            "TaeHyun Hwang",
            "Wei Zhang",
            "Maoqiang Xie",
            "Rui Kuang"
        ],
        "abstract": "A computational challenge to validate the candidate disease genes identified in a high-throughput genomic study is to elucidate the associations between the set of candidate genes and disease phenotypes. The conventional gene set enrichment analysis often fails to reveal associations between disease phenotypes and the gene sets with a short list of poorly annotated genes, because the existing annotations of disease causative genes are incomplete. We propose a network-based computational approach called rcNet to discover the associations between gene sets and disease phenotypes. Assuming coherent associations between the genes ranked by their relevance to the query gene set, and the disease phenotypes ranked by their relevance to the hidden target disease phenotypes of the query gene set, we formulate a learning framework maximizing the rank coherence with respect to the known disease phenotype-gene associations. An efficient algorithm coupling ridge regression with label propagation, and two variants are introduced to find the optimal solution of the framework. We evaluated the rcNet algorithms and existing baseline methods with both leave-one-out cross-validation and a task of predicting recently discovered disease-gene associations in OMIM. The experiments demonstrated that the rcNet algorithms achieved the best overall rankings compared to the baselines. To further validate the reproducibility of the performance, we applied the algorithms to identify the target diseases of novel candidate disease genes obtained from recent studies of GWAS, DNA copy number variation analysis, and gene expression profiling. The algorithms ranked the target disease of the candidate genes at the top of the rank list in many cases across all the three case studies. The rcNet algorithms are available as a webtool for disease and gene set association analysis at ",
        "submission_date": "2011-02-18T00:00:00",
        "last_modified_date": "2011-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.4498",
        "title": "Digraph description of k-interchange technique for optimization over permutations and adaptive algorithm system",
        "authors": [
            "Mark Sh. Levin"
        ],
        "abstract": "The paper describes a general glance to the use of element exchange techniques for optimization over permutations. A multi-level description of problems is proposed which is a fundamental to understand nature and complexity of optimization problems over permutations (e.g., ordering, scheduling, traveling salesman problem). The description is based on permutation neighborhoods of several kinds (e.g., by improvement of an objective function). Our proposed operational digraph and its kinds can be considered as a way to understand convexity and polynomial solvability for combinatorial optimization problems over permutations. Issues of an analysis of problems and a design of hierarchical heuristics are discussed. The discussion leads to a multi-level adaptive algorithm system which analyzes an individual problem and selects/designs a solving strategy (trajectory).\n    ",
        "submission_date": "2011-02-22T00:00:00",
        "last_modified_date": "2011-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.5185",
        "title": "Universal Higher Order Grammar",
        "authors": [
            "Victor Gluzberg"
        ],
        "abstract": "We examine the class of languages that can be defined entirely in terms of provability in an extension of the sorted type theory (Ty_n) by embedding the logic of phonologies, without introduction of special types for syntactic entities. This class is proven to precisely coincide with the class of logically closed languages that may be thought of as functions from expressions to sets of logically equivalent Ty_n terms. For a specific sub-class of logically closed languages that are described by finite sets of rules or rule schemata, we find effective procedures for building a compact Ty_n representation, involving a finite number of axioms or axiom schemata. The proposed formalism is characterized by some useful features unavailable in a two-component architecture of a language model. A further specialization and extension of the formalism with a context type enable effective account of intensional and dynamic semantics.\n    ",
        "submission_date": "2011-02-25T00:00:00",
        "last_modified_date": "2011-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.5451",
        "title": "Reduction of fuzzy automata by means of fuzzy quasi-orders",
        "authors": [
            "Aleksandar Stamenkovi\u0107",
            "Miroslav \u0106iri\u0107",
            "Jelena Ignjatovi\u0107"
        ],
        "abstract": "In our recent paper we have established close relationships between state reduction of a fuzzy recognizer and resolution of a particular system of fuzzy relation equations. In that paper we have also studied reductions by means of those solutions which are fuzzy equivalences. In this paper we will see that in some cases better reductions can be obtained using the solutions of this system that are fuzzy quasi-orders. Generally, fuzzy quasi-orders and fuzzy equivalences are equally good in the state reduction, but we show that right and left invariant fuzzy quasi-orders give better reductions than right and left invariant fuzzy equivalences. We also show that alternate reductions by means of fuzzy quasi-orders give better results than alternate reductions by means of fuzzy equivalences. Furthermore we study a more general type of fuzzy quasi-orders, weakly right and left invariant ones, and we show that they are closely related to determinization of fuzzy recognizers. We also demonstrate some applications of weakly left invariant fuzzy quasi-orders in conflict analysis of fuzzy discrete event systems.\n    ",
        "submission_date": "2011-02-26T00:00:00",
        "last_modified_date": "2011-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.5452",
        "title": "Bisimulations for fuzzy automata",
        "authors": [
            "Miroslav \u0106iri\u0107",
            "Jelena Ignjatovi\u0107",
            "Nada Damljanovi\u0107",
            "Milan Ba\u0161i\u0107"
        ],
        "abstract": "Bisimulations have been widely used in many areas of computer science to model equivalence between various systems, and to reduce the number of states of these systems, whereas uniform fuzzy relations have recently been introduced as a means to model the fuzzy equivalence between elements of two possible different sets. Here we use the conjunction of these two concepts as a powerful tool in the study of equivalence between fuzzy automata. We prove that a uniform fuzzy relation between fuzzy automata $\\cal A$ and $\\cal B$ is a forward bisimulation if and only if its kernel and co-kernel are forward bisimulation fuzzy equivalences on $\\cal A$ and $\\cal B$ and there is a special isomorphism between factor fuzzy automata with respect to these fuzzy equivalences. As a consequence we get that fuzzy automata $\\cal A$ and $\\cal B$ are UFB-equivalent, i.e., there is a uniform forward bisimulation between them, if and only if there is a special isomorphism between the factor fuzzy automata of $\\cal A$ and $\\cal B$ with respect to their greatest forward bisimulation fuzzy equivalences. This result reduces the problem of testing UFB-equivalence to the problem of testing isomorphism of fuzzy automata, which is closely related to the well-known graph isomorphism problem. We prove some similar results for backward-forward bisimulations, and we point to fundamental differences. Because of the duality with the studied concepts, backward and forward-backward bisimulations are not considered separately. Finally, we give a comprehensive overview of various concepts on deterministic, nondeterministic, fuzzy, and weighted automata, which are related to bisimulations.\n    ",
        "submission_date": "2011-02-26T00:00:00",
        "last_modified_date": "2011-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.5549",
        "title": "Instant Replay: Investigating statistical Analysis in Sports",
        "authors": [
            "Gagan Sidhu"
        ],
        "abstract": "Technology has had an unquestionable impact on the way people watch sports. Along with this technological evolution has come a higher standard to ensure a good viewing experience for the casual sports fan. It can be argued that the pervasion of statistical analysis in sports serves to satiate the fan's desire for detailed sports statistics. The goal of statistical analysis in sports is a simple one: to eliminate subjective analysis. In this paper, we review previous work that attempts to analyze various aspects in sports by using ideas from Markov Chains, Bayesian Inference and Markov Chain Monte Carlo (MCMC) methods. The unifying goal of these works is to achieve an accurate representation of the player's ability, the sport, or the environmental effects on the player's performance. With the prevalence of cheap computation, it is possible that using techniques in Artificial Intelligence could improve the result of statistical analysis in sport. This is best illustrated when evaluating football using Neuro Dynamic Programming, a Control Theory paradigm heavily based on theory in Stochastic processes. The results from this method suggest that statistical analysis in sports may benefit from using ideas from the area of Control Theory or Machine Learning\n    ",
        "submission_date": "2011-02-27T00:00:00",
        "last_modified_date": "2011-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.1516",
        "title": "Climbing depth-bounded adjacent discrepancy search for solving hybrid flow shop scheduling problems with multiprocessor tasks",
        "authors": [
            "Asma Lahimer",
            "Pierre Lopez",
            "Mohamed Haouari"
        ],
        "abstract": "This paper considers multiprocessor task scheduling in a multistage hybrid flow-shop environment. The problem even in its simplest form is NP-hard in the strong sense. The great deal of interest for this problem, besides its theoretical complexity, is animated by needs of various manufacturing and computing systems. We propose a new approach based on limited discrepancy search to solve the problem. Our method is tested with reference to a proposed lower bound as well as the best-known solutions in literature. Computational results show that the developed approach is efficient in particular for large-size problems.\n    ",
        "submission_date": "2011-03-08T00:00:00",
        "last_modified_date": "2011-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.1530",
        "title": "A Discrete Evolutionary Model for Chess Players' Ratings",
        "authors": [
            "Trevor Fenner",
            "Mark Levene",
            "George Loizou"
        ],
        "abstract": "The Elo system for rating chess players, also used in other games and sports, was adopted by the World Chess Federation over four decades ago. Although not without controversy, it is accepted as generally reliable and provides a method for assessing players' strengths and ranking them in official tournaments.\n",
        "submission_date": "2011-03-08T00:00:00",
        "last_modified_date": "2011-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.1791",
        "title": "Integrated information increases with fitness in the evolution of animats",
        "authors": [
            "Jeffrey Edlund",
            "Nicolas Chaumont",
            "Arend Hintze",
            "Christof Koch",
            "Giulio Tononi",
            "Christoph Adami"
        ],
        "abstract": "One of the hallmarks of biological organisms is their ability to integrate disparate information sources to optimize their behavior in complex environments. How this capability can be quantified and related to the functional complexity of an organism remains a challenging problem, in particular since organismal functional complexity is not well-defined. We present here several candidate measures that quantify information and integration, and study their dependence on fitness as an artificial agent (\"animat\") evolves over thousands of generations to solve a navigation task in a simple, simulated environment. We compare the ability of these measures to predict high fitness with more conventional information-theoretic processing measures. As the animat adapts by increasing its \"fit\" to the world, information integration and processing increase commensurately along the evolutionary line of descent. We suggest that the correlation of fitness with information integration and with processing measures implies that high fitness requires both information processing as well as integration, but that information integration may be a better measure when the task requires memory. A correlation of measures of information integration (but also information processing) and fitness strongly suggests that these measures reflect the functional complexity of the animat, and that such measures can be used to quantify functional complexity even in the absence of fitness data.\n    ",
        "submission_date": "2011-03-09T00:00:00",
        "last_modified_date": "2011-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.2110",
        "title": "A hybrid model for bankruptcy prediction using genetic algorithm, fuzzy c-means and mars",
        "authors": [
            "A.Martin",
            "V.Gayathri",
            "G.Saranya",
            "P.Gayathri",
            "Prasanna Venkatesan"
        ],
        "abstract": "Bankruptcy prediction is very important for all the organization since it affects the economy and rise many social problems with high costs. There are large number of techniques have been developed to predict the bankruptcy, which helps the decision makers such as investors and financial analysts. One of the bankruptcy prediction models is the hybrid model using Fuzzy C-means clustering and MARS, which uses static ratios taken from the bank financial statements for prediction, which has its own theoretical advantages. The performance of existing bankruptcy model can be improved by selecting the best features dynamically depend on the nature of the firm. This dynamic selection can be accomplished by Genetic Algorithm and it improves the performance of prediction model.\n    ",
        "submission_date": "2011-03-01T00:00:00",
        "last_modified_date": "2011-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.2325",
        "title": "Self reference in word definitions",
        "authors": [
            "David Levary",
            "Jean-Pierre Eckmann",
            "Elisha Moses",
            "Tsvi Tlusty"
        ],
        "abstract": "Dictionaries are inherently circular in nature. A given word is linked to a set of alternative words (the definition) which in turn point to further descendants. Iterating through definitions in this way, one typically finds that definitions loop back upon themselves. The graph formed by such definitional relations is our object of study. By eliminating those links which are not in loops, we arrive at a core subgraph of highly connected nodes.\n",
        "submission_date": "2011-03-11T00:00:00",
        "last_modified_date": "2011-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.3735",
        "title": "Refining Recency Search Results with User Click Feedback",
        "authors": [
            "Taesup Moon",
            "Wei Chu",
            "Lihong Li",
            "Zhaohui Zheng",
            "Yi Chang"
        ],
        "abstract": "Traditional machine-learned ranking systems for web search are often trained to capture stationary relevance of documents to queries, which has limited ability to track non-stationary user intention in a timely manner. In recency search, for instance, the relevance of documents to a query on breaking news often changes significantly over time, requiring effective adaptation to user intention. In this paper, we focus on recency search and study a number of algorithms to improve ranking results by leveraging user click feedback. Our contributions are three-fold. First, we use real search sessions collected in a random exploration bucket for \\emph{reliable} offline evaluation of these algorithms, which provides an unbiased comparison across algorithms without online bucket tests. Second, we propose a re-ranking approach to improve search results for recency queries using user clicks. Third, our empirical comparison of a dozen algorithms on real-life search data suggests importance of a few algorithmic choices in these applications, including generalization across different query-document pairs, specialization to popular queries, and real-time adaptation of user clicks.\n    ",
        "submission_date": "2011-03-19T00:00:00",
        "last_modified_date": "2011-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.4487",
        "title": "Handwritten Digit Recognition with a Committee of Deep Neural Nets on GPUs",
        "authors": [
            "Dan C. Cire\u015fan",
            "Ueli Meier",
            "Luca M. Gambardella",
            "J\u00fcrgen Schmidhuber"
        ],
        "abstract": "The competitive MNIST handwritten digit recognition benchmark has a long history of broken records since 1998. The most recent substantial improvement by others dates back 7 years (error rate 0.4%) . Recently we were able to significantly improve this result, using graphics cards to greatly speed up training of simple but deep MLPs, which achieved 0.35%, outperforming all the previous more complex methods. Here we report another substantial improvement: 0.31% obtained using a committee of MLPs.\n    ",
        "submission_date": "2011-03-23T00:00:00",
        "last_modified_date": "2011-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.4601",
        "title": "Doubly Robust Policy Evaluation and Learning",
        "authors": [
            "Miroslav Dudik",
            "John Langford",
            "Lihong Li"
        ],
        "abstract": "We study decision making in environments where the reward is only partially observed, but can be modeled as a function of an action and an observed context. This setting, known as contextual bandits, encompasses a wide variety of applications including health-care policy and Internet advertising. A central task is evaluation of a new policy given historic data consisting of contexts, actions and received rewards. The key challenge is that the past data typically does not faithfully represent proportions of actions taken by a new policy. Previous approaches rely either on models of rewards or models of the past policy. The former are plagued by a large bias whereas the latter have a large variance.\n",
        "submission_date": "2011-03-23T00:00:00",
        "last_modified_date": "2011-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.4778",
        "title": "Formal and Computational Properties of the Confidence Boost of Association Rules",
        "authors": [
            "Jos\u00e9 L. Balc\u00e1zar"
        ],
        "abstract": "Some existing notions of redundancy among association rules allow for a logical-style characterization and lead to irredundant bases of absolutely minimum size. One can push the intuition of redundancy further and find an intuitive notion of interest of an association rule, in terms of its \"novelty\" with respect to other rules. Namely: an irredundant rule is so because its confidence is higher than what the rest of the rules would suggest; then, one can ask: how much higher? We propose to measure such a sort of \"novelty\" through the confidence boost of a rule, which encompasses two previous similar notions (confidence width and rule blocking, of which the latter is closely related to the earlier measure \"improvement\"). Acting as a complement to confidence and support, the confidence boost helps to obtain small and crisp sets of mined association rules, and solves the well-known problem that, in certain cases, rules of negative correlation may pass the confidence bound. We analyze the properties of two versions of the notion of confidence boost, one of them a natural generalization of the other. We develop efficient algorithmics to filter rules according to their confidence boost, compare the concept to some similar notions in the bibliography, and describe the results of some experimentation employing the new notions on standard benchmark datasets. We describe an open-source association mining tool that embodies one of our variants of confidence boost in such a way that the data mining process does not require the user to select any value for any parameter.\n    ",
        "submission_date": "2011-03-24T00:00:00",
        "last_modified_date": "2011-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.4854",
        "title": "When is social computation better than the sum of its parts?",
        "authors": [
            "Vadas Gintautas",
            "Aric Hagberg",
            "Luis M. A. Bettencourt"
        ],
        "abstract": "Social computation, whether in the form of searches performed by swarms of agents or collective predictions of markets, often supplies remarkably good solutions to complex problems. In many examples, individuals trying to solve a problem locally can aggregate their information and work together to arrive at a superior global solution. This suggests that there may be general principles of information aggregation and coordination that can transcend particular applications. Here we show that the general structure of this problem can be cast in terms of information theory and derive mathematical conditions that lead to optimal multi-agent searches. Specifically, we illustrate the problem in terms of local search algorithms for autonomous agents looking for the spatial location of a stochastic source. We explore the types of search problems, defined in terms of the statistical properties of the source and the nature of measurements at each agent, for which coordination among multiple searchers yields an advantage beyond that gained by having the same number of independent searchers. We show that effective coordination corresponds to synergy and that ineffective coordination corresponds to independence as defined using information theory. We classify explicit types of sources in terms of their potential for synergy. We show that sources that emit uncorrelated signals provide no opportunity for synergetic coordination while sources that emit signals that are correlated in some way, do allow for strong synergy between searchers. These general considerations are crucial for designing optimal algorithms for particular search problems in real world settings.\n    ",
        "submission_date": "2011-03-24T00:00:00",
        "last_modified_date": "2011-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.4888",
        "title": "Cooperative searching for stochastic targets",
        "authors": [
            "Vadas Gintautas",
            "Aric Hagberg",
            "Luis M. A. Bettencourt"
        ],
        "abstract": "Spatial search problems abound in the real world, from locating hidden nuclear or chemical sources to finding skiers after an avalanche. We exemplify the formalism and solution for spatial searches involving two agents that may or may not choose to share information during a search. For certain classes of tasks, sharing information between multiple searchers makes cooperative searching advantageous. In some examples, agents are able to realize synergy by aggregating information and moving based on local judgments about maximal information gathering expectations. We also explore one- and two-dimensional simplified situations analytically and numerically to provide a framework for analyzing more complex problems. These general considerations provide a guide for designing optimal algorithms for real-world search problems.\n    ",
        "submission_date": "2011-03-25T00:00:00",
        "last_modified_date": "2011-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.5002",
        "title": "User Modeling Combining Access Logs, Page Content and Semantics",
        "authors": [
            "Blaz Fortuna",
            "Dunja Mladenic",
            "Marko Grobelnik"
        ],
        "abstract": "The paper proposes an approach to modeling users of large Web sites based on combining different data sources: access logs and content of the accessed pages are combined with semantic information about the Web pages, the users and the accesses of the users to the Web site. The assumption is that we are dealing with a large Web site providing content to a large number of users accessing the site. The proposed approach represents each user by a set of features derived from the different data sources, where some feature values may be missing for some users. It further enables user modeling based on the provided characteristics of the targeted user subset. The approach is evaluated on real-world data where we compare performance of the automatic assignment of a user to a predefined user segment when different data sources are used to represent the users.\n    ",
        "submission_date": "2011-03-25T00:00:00",
        "last_modified_date": "2011-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.5043",
        "title": "An Empirical Study of Real-World SPARQL Queries",
        "authors": [
            "Mario Arias",
            "Javier D. Fern\u00e1ndez",
            "Miguel A. Mart\u00ednez-Prieto",
            "Pablo de la Fuente"
        ],
        "abstract": "Understanding how users tailor their SPARQL queries is crucial when designing query evaluation engines or fine-tuning RDF stores with performance in mind. In this paper we analyze 3 million real-world SPARQL queries extracted from logs of the DBPedia and SWDF public endpoints. We aim at finding which are the most used language elements both from syntactical and structural perspectives, paying special attention to triple patterns and joins, since they are indeed some of the most expensive SPARQL operations at evaluation phase. We have determined that most of the queries are simple and include few triple patterns and joins, being Subject-Subject, Subject-Object and Object-Object the most common join types. The graph patterns are usually star-shaped and despite triple pattern chains exist, they are generally short.\n    ",
        "submission_date": "2011-03-25T00:00:00",
        "last_modified_date": "2011-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.5044",
        "title": "Mining User Comment Activity for Detecting Forum Spammers in YouTube",
        "authors": [
            "Ashish Sureka"
        ],
        "abstract": "Research shows that comment spamming (comments which are unsolicited, unrelated, abusive, hateful, commercial advertisements etc) in online discussion forums has become a common phenomenon in Web 2.0 applications and there is a strong need to counter or combat comment spamming. We present a method to automatically detect comment spammer in YouTube (largest and a popular video sharing website) forums. The proposed technique is based on mining comment activity log of a user and extracting patterns (such as time interval between subsequent comments, presence of exactly same comment across multiple unrelated videos) indicating spam behavior. We perform empirical analysis on data crawled from YouTube and demonstrate that the proposed method is effective for the task of comment spammer detection.\n    ",
        "submission_date": "2011-03-25T00:00:00",
        "last_modified_date": "2011-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.5046",
        "title": "From Linked Data to Relevant Data -- Time is the Essence",
        "authors": [
            "Markus Kirchberg",
            "Ryan K L Ko",
            "Bu Sung Lee"
        ],
        "abstract": "The Semantic Web initiative puts emphasis not primarily on putting data on the Web, but rather on creating links in a way that both humans and machines can explore the Web of data. When such users access the Web, they leave a trail as Web servers maintain a history of requests. Web usage mining approaches have been studied since the beginning of the Web given the log's huge potential for purposes such as resource annotation, personalization, forecasting etc. However, the impact of any such efforts has not really gone beyond generating statistics detailing who, when, and how Web pages maintained by a Web server were visited.\n    ",
        "submission_date": "2011-03-25T00:00:00",
        "last_modified_date": "2011-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.5078",
        "title": "Algorithms for computing the greatest simulations and bisimulations between fuzzy automata",
        "authors": [
            "Miroslav \u0106iri\u0107",
            "Jelena Ignjatovi\u0107",
            "Ivana Jan\u010di\u0107",
            "Nada Damljanovi\u0107"
        ],
        "abstract": "Recently, two types of simulations (forward and backward simulations) and four types of bisimulations (forward, backward, forward-backward, and backward-forward bisimulations) between fuzzy automata have been introduced. If there is at least one simulation/bisimulation of some of these types between the given fuzzy automata, it has been proved that there is the greatest simulation/bisimulation of this kind. In the present paper, for any of the above-mentioned types of simulations/bisimulations we provide an effective algorithm for deciding whether there is a simulation/bisimulation of this type between the given fuzzy automata, and for computing the greatest one, whenever it exists. The algorithms are based on the method developed in [J. Ignjatovi\u0107, M. \u0106iri\u0107, S. Bogdanovi\u0107, On the greatest solutions to certain systems of fuzzy relation inequalities and equations, Fuzzy Sets and Systems 161 (2010) 3081-3113], which comes down to the computing of the greatest post-fixed point, contained in a given fuzzy relation, of an isotone function on the lattice of fuzzy relations.\n    ",
        "submission_date": "2011-03-25T00:00:00",
        "last_modified_date": "2011-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.0126",
        "title": "U-Sem: Semantic Enrichment, User Modeling and Mining of Usage Data on the Social Web",
        "authors": [
            "Fabian Abel",
            "Ilknur Celik",
            "Claudia Hauff",
            "Laura Hollink",
            "Geert-Jan Houben"
        ],
        "abstract": "With the growing popularity of Social Web applications, more and more user data is published on the Web everyday. Our research focuses on investigating ways of mining data from such platforms that can be used for modeling users and for semantically augmenting user profiles. This process can enhance adaptation and personalization in various adaptive Web-based systems. In this paper, we present the U-Sem people modeling service, a framework for the semantic enrichment and mining of people's profiles from usage data on the Social Web. We explain the architecture of our people modeling service and describe its application in an adult e-learning context as an example. Versions: Mar 21, 10:10, Mar 25, 09:37\n    ",
        "submission_date": "2011-04-01T00:00:00",
        "last_modified_date": "2011-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.0128",
        "title": "Towards an automated query modification assistant",
        "authors": [
            "Vera Hollink",
            "Arjen de Vries"
        ],
        "abstract": "Users who need several queries before finding what they need can benefit from an automatic search assistant that provides feedback on their query modification strategies. We present a method to learn from a search log which types of query modifications have and have not been effective in the past. The method analyses query modifications along two dimensions: a traditional term-based dimension and a semantic dimension, for which queries are enriches with linked data entities. Applying the method to the search logs of two search engines, we identify six opportunities for a query modification assistant to improve search: modification strategies that are commonly used, but that often do not lead to satisfactory results.\n    ",
        "submission_date": "2011-04-01T00:00:00",
        "last_modified_date": "2011-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.1477",
        "title": "An Agent-based Architecture for a Knowledge-work Support System",
        "authors": [
            "Arijit Laha"
        ],
        "abstract": "Enhancement of technology-based system support for knowledge workers is an issue of great importance. The \"Knowledge work Support System (KwSS)\" framework analyzes this issue from a holistic perspective. KwSS proposes a set of design principles for building a comprehensive IT-based support system, which enhances the capability of a human agent for performing a set of complex and interrelated knowledge-works relevant to one or more target task-types within a domain of professional activities. In this paper, we propose a high-level, software-agent based architecture for realizing a KwSS system that incorporates these design principles. Here we focus on developing a number of crucial enabling components of the architecture, including (1) an Activity Theory-based novel modeling technique for knowledgeintensive activities; (2) a graph theoretic formalism for representing these models in a knowledge base in conjunction with relevant entity taxonomies/ontologies; and (3) an algorithm for reasoning, using the knowledge base, about various aspects of possible supports for activities at performance-time.\n    ",
        "submission_date": "2011-04-08T00:00:00",
        "last_modified_date": "2011-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.2788",
        "title": "Backdoors to Tractable Answer-Set Programming",
        "authors": [
            "Johannes Klaus Fichte",
            "Stefan Szeider"
        ],
        "abstract": "Answer Set Programming (ASP) is an increasingly popular framework for declarative programming that admits the description of problems by means of rules and constraints that form a disjunctive logic program. In particular, many AI problems such as reasoning in a nonmonotonic setting can be directly formulated in ASP. Although the main problems of ASP are of high computational complexity, located at the second level of the Polynomial Hierarchy, several restrictions of ASP have been identified in the literature, under which ASP problems become tractable.\n",
        "submission_date": "2011-04-14T00:00:00",
        "last_modified_date": "2014-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.2825",
        "title": "Foundations for Uniform Interpolation and Forgetting in Expressive Description Logics",
        "authors": [
            "Carsten Lutz",
            "Frank Wolter"
        ],
        "abstract": "We study uniform interpolation and forgetting in the description logic ALC. Our main results are model-theoretic characterizations of uniform inter- polants and their existence in terms of bisimula- tions, tight complexity bounds for deciding the existence of uniform interpolants, an approach to computing interpolants when they exist, and tight bounds on their size. We use a mix of model- theoretic and automata-theoretic methods that, as a by-product, also provides characterizations of and decision procedures for conservative extensions.\n    ",
        "submission_date": "2011-04-14T00:00:00",
        "last_modified_date": "2011-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.2829",
        "title": "Self-organizing traffic lights at multiple-street intersections",
        "authors": [
            "Carlos Gershenson",
            "David A. Rosenblueth"
        ],
        "abstract": "Summary: Traffic light coordination is a complex problem. In this paper, we extend previous work on an abstract model of city traffic to allow for multiple street intersections. We test a self-organizing method in our model, showing that it is close to theoretical optima and superior to a traditional method of traffic light coordination.\n",
        "submission_date": "2011-04-14T00:00:00",
        "last_modified_date": "2011-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.4024",
        "title": "Palette-colouring: a belief-propagation approach",
        "authors": [
            "Alessandro Pelizzola",
            "Marco Pretti",
            "Jort van Mourik"
        ],
        "abstract": "We consider a variation of the prototype combinatorial-optimisation problem known as graph-colouring. Our optimisation goal is to colour the vertices of a graph with a fixed number of colours, in a way to maximise the number of different colours present in the set of nearest neighbours of each given vertex. This problem, which we pictorially call \"palette-colouring\", has been recently addressed as a basic example of problem arising in the context of distributed data storage. Even though it has not been proved to be NP complete, random search algorithms find the problem hard to solve. Heuristics based on a naive belief propagation algorithm are observed to work quite well in certain conditions. In this paper, we build upon the mentioned result, working out the correct belief propagation algorithm, which needs to take into account the many-body nature of the constraints present in this problem. This method improves the naive belief propagation approach, at the cost of increased computational effort. We also investigate the emergence of a satisfiable to unsatisfiable \"phase transition\" as a function of the vertex mean degree, for different ensembles of sparse random graphs in the large size (\"thermodynamic\") limit.\n    ",
        "submission_date": "2011-04-20T00:00:00",
        "last_modified_date": "2011-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.5601",
        "title": "Mean-Variance Optimization in Markov Decision Processes",
        "authors": [
            "Shie Mannor",
            "John Tsitsiklis"
        ],
        "abstract": "We consider finite horizon Markov decision processes under performance measures that involve both the mean and the variance of the cumulative reward. We show that either randomized or history-based policies can improve performance. We prove that the complexity of computing a policy that maximizes the mean reward under a variance constraint is NP-hard for some cases, and strongly NP-hard for others. We finally offer pseudopolynomial exact and approximation algorithms.\n    ",
        "submission_date": "2011-04-29T00:00:00",
        "last_modified_date": "2011-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.0167",
        "title": "SERAPH: Semi-supervised Metric Learning Paradigm with Hyper Sparsity",
        "authors": [
            "Gang Niu",
            "Bo Dai",
            "Makoto Yamada",
            "Masashi Sugiyama"
        ],
        "abstract": "We propose a general information-theoretic approach called Seraph (SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) for metric learning that does not rely upon the manifold assumption. Given the probability parameterized by a Mahalanobis distance, we maximize the entropy of that probability on labeled data and minimize it on unlabeled data following entropy regularization, which allows the supervised and unsupervised parts to be integrated in a natural and meaningful way. Furthermore, Seraph is regularized by encouraging a low-rank projection induced from the metric. The optimization of Seraph is solved efficiently and stably by an EM-like scheme with the analytical E-Step and convex M-Step. Experiments demonstrate that Seraph compares favorably with many well-known global and local metric learning methods.\n    ",
        "submission_date": "2011-05-01T00:00:00",
        "last_modified_date": "2012-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.0728",
        "title": "Structured Sparsity via Alternating Direction Methods",
        "authors": [
            "Zhiwei Qin",
            "Donald Goldfarb"
        ],
        "abstract": "We consider a class of sparse learning problems in high dimensional feature space regularized by a structured sparsity-inducing norm which incorporates prior knowledge of the group structure of the features. Such problems often pose a considerable challenge to optimization algorithms due to the non-smoothness and non-separability of the regularization term. In this paper, we focus on two commonly adopted sparsity-inducing regularization terms, the overlapping Group Lasso penalty $l_1/l_2$-norm and the $l_1/l_\\infty$-norm. We propose a unified framework based on the augmented Lagrangian method, under which problems with both types of regularization and their variants can be efficiently solved. As the core building-block of this framework, we develop new algorithms using an alternating partial-linearization/splitting technique, and we prove that the accelerated versions of these algorithms require $O(\\frac{1}{\\sqrt{\\epsilon}})$ iterations to obtain an $\\epsilon$-optimal solution. To demonstrate the efficiency and relevance of our algorithms, we test them on a collection of data sets and apply them to two real-world problems to compare the relative merits of the two norms.\n    ",
        "submission_date": "2011-05-04T00:00:00",
        "last_modified_date": "2011-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.0972",
        "title": "Rapid Feature Learning with Stacked Linear Denoisers",
        "authors": [
            "Zhixiang Eddie Xu",
            "Kilian Q. Weinberger",
            "Fei Sha"
        ],
        "abstract": "We investigate unsupervised pre-training of deep architectures as feature generators for \"shallow\" classifiers. Stacked Denoising Autoencoders (SdA), when used as feature pre-processing tools for SVM classification, can lead to significant improvements in accuracy - however, at the price of a substantial increase in computational cost. In this paper we create a simple algorithm which mimics the layer by layer training of SdAs. However, in contrast to SdAs, our algorithm requires no training through gradient descent as the parameters can be computed in closed-form. It can be implemented in less than 20 lines of MATLABTMand reduces the computation time from several hours to mere seconds. We show that our feature transformation reliably improves the results of SVM classification significantly on all our data sets - often outperforming SdAs and even deep neural networks in three out of four deep learning benchmarks.\n    ",
        "submission_date": "2011-05-05T00:00:00",
        "last_modified_date": "2011-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.1386",
        "title": "Self-organized adaptation of a simple neural circuit enables complex robot behaviour",
        "authors": [
            "Silke Steingrube",
            "Marc Timme",
            "Florentin Woergoetter",
            "Poramate Manoonpong"
        ],
        "abstract": "Controlling sensori-motor systems in higher animals or complex robots is a challenging combinatorial problem, because many sensory signals need to be simultaneously coordinated into a broad behavioural spectrum. To rapidly interact with the environment, this control needs to be fast and adaptive. Current robotic solutions operate with limited autonomy and are mostly restricted to few behavioural patterns. Here we introduce chaos control as a new strategy to generate complex behaviour of an autonomous robot. In the presented system, 18 sensors drive 18 motors via a simple neural control circuit, thereby generating 11 basic behavioural patterns (e.g., orienting, taxis, self-protection, various gaits) and their combinations. The control signal quickly and reversibly adapts to new situations and additionally enables learning and synaptic long-term storage of behaviourally useful motor responses. Thus, such neural control provides a powerful yet simple way to self-organize versatile behaviours in autonomous agents with many degrees of freedom.\n    ",
        "submission_date": "2011-05-06T00:00:00",
        "last_modified_date": "2011-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.1853",
        "title": "Feedback Message Passing for Inference in Gaussian Graphical Models",
        "authors": [
            "Ying Liu",
            "Venkat Chandrasekaran",
            "Animashree Anandkumar",
            "Alan S. Willsky"
        ],
        "abstract": "While loopy belief propagation (LBP) performs reasonably well for inference in some Gaussian graphical models with cycles, its performance is unsatisfactory for many others. In particular for some models LBP does not converge, and in general when it does converge, the computed variances are incorrect (except for cycle-free graphs for which belief propagation (BP) is non-iterative and exact). In this paper we propose {\\em feedback message passing} (FMP), a message-passing algorithm that makes use of a special set of vertices (called a {\\em feedback vertex set} or {\\em FVS}) whose removal results in a cycle-free graph. In FMP, standard BP is employed several times on the cycle-free subgraph excluding the FVS while a special message-passing scheme is used for the nodes in the FVS. The computational complexity of exact inference is $O(k^2n)$, where $k$ is the number of feedback nodes, and $n$ is the total number of nodes. When the size of the FVS is very large, FMP is intractable. Hence we propose {\\em approximate FMP}, where a pseudo-FVS is used instead of an FVS, and where inference in the non-cycle-free graph obtained by removing the pseudo-FVS is carried out approximately using LBP. We show that, when approximate FMP converges, it yields exact means and variances on the pseudo-FVS and exact means throughout the remainder of the graph. We also provide theoretical results on the convergence and accuracy of approximate FMP. In particular, we prove error bounds on variance computation. Based on these theoretical results, we design efficient algorithms to select a pseudo-FVS of bounded size. The choice of the pseudo-FVS allows us to explicitly trade off between efficiency and accuracy. Experimental results show that using a pseudo-FVS of size no larger than $\\log(n)$, this procedure converges much more often, more quickly, and provides more accurate results than LBP on the entire graph.\n    ",
        "submission_date": "2011-05-10T00:00:00",
        "last_modified_date": "2011-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.2790",
        "title": "On the equivalence of Hopfield Networks and Boltzmann Machines",
        "authors": [
            "Adriano Barra",
            "Alberto Bernacchia",
            "Enrica Santucci",
            "Pierluigi Contucci"
        ],
        "abstract": "A specific type of neural network, the Restricted Boltzmann Machine (RBM), is implemented for classification and feature detection in machine learning. RBM is characterized by separate layers of visible and hidden units, which are able to learn efficiently a generative model of the observed data. We study a \"hybrid\" version of RBM's, in which hidden units are analog and visible units are binary, and we show that thermodynamics of visible units are equivalent to those of a Hopfield network, in which the N visible units are the neurons and the P hidden units are the learned patterns. We apply the method of stochastic stability to derive the thermodynamics of the model, by considering a formal extension of this technique to the case of multiple sets of stored patterns, which may act as a benchmark for the study of correlated sets. Our results imply that simulating the dynamics of a Hopfield network, requiring the update of N neurons and the storage of N(N-1)/2 synapses, can be accomplished by a hybrid Boltzmann Machine, requiring the update of N+P neurons but the storage of only NP synapses. In addition, the well known glass transition of the Hopfield network has a counterpart in the Boltzmann Machine: It corresponds to an optimum criterion for selecting the relative sizes of the hidden and visible layers, resolving the trade-off between flexibility and generality of the model. The low storage phase of the Hopfield model corresponds to few hidden units and hence a overly constrained RBM, while the spin-glass phase (too many hidden units) corresponds to unconstrained RBM prone to overfitting of the observed data.\n    ",
        "submission_date": "2011-05-13T00:00:00",
        "last_modified_date": "2012-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.2868",
        "title": "Semantic Vector Machines",
        "authors": [
            "Etter Vincent"
        ],
        "abstract": "We first present our work in machine translation, during which we used aligned sentences to train a neural network to embed n-grams of different languages into an $d$-dimensional space, such that n-grams that are the translation of each other are close with respect to some metric. Good n-grams to n-grams translation results were achieved, but full sentences translation is still problematic. We realized that learning semantics of sentences and documents was the key for solving a lot of natural language processing problems, and thus moved to the second part of our work: sentence compression. We introduce a flexible neural network architecture for learning embeddings of words and sentences that extract their semantics, propose an efficient implementation in the Torch framework and present embedding results comparable to the ones obtained with classical neural language models, while being more powerful.\n    ",
        "submission_date": "2011-05-14T00:00:00",
        "last_modified_date": "2011-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.2943",
        "title": "Feature Selection for MAUC-Oriented Classification Systems",
        "authors": [
            "Rui Wang",
            "Ke Tang"
        ],
        "abstract": "Feature selection is an important pre-processing step for many pattern classification tasks. Traditionally, feature selection methods are designed to obtain a feature subset that can lead to high classification accuracy. However, classification accuracy has recently been shown to be an inappropriate performance metric of classification systems in many cases. Instead, the Area Under the receiver operating characteristic Curve (AUC) and its multi-class extension, MAUC, have been proved to be better alternatives. Hence, the target of classification system design is gradually shifting from seeking a system with the maximum classification accuracy to obtaining a system with the maximum AUC/MAUC. Previous investigations have shown that traditional feature selection methods need to be modified to cope with this new objective. These methods most often are restricted to binary classification problems only. In this study, a filter feature selection method, namely MAUC Decomposition based Feature Selection (MDFS), is proposed for multi-class classification problems. To the best of our knowledge, MDFS is the first method specifically designed to select features for building classification systems with maximum MAUC. Extensive empirical results demonstrate the advantage of MDFS over several compared feature selection methods.\n    ",
        "submission_date": "2011-05-15T00:00:00",
        "last_modified_date": "2011-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.4318",
        "title": "Correction of Noisy Sentences using a Monolingual Corpus",
        "authors": [
            "Diptesh Chatterhee"
        ],
        "abstract": "Correction of Noisy Natural Language Text is an important and well studied problem in Natural Language Processing. It has a number of applications in domains like Statistical Machine Translation, Second Language Learning and Natural Language Generation. In this work, we consider some statistical techniques for Text Correction. We define the classes of errors commonly found in text and describe algorithms to correct them. The data has been taken from a poorly trained Machine Translation system. The algorithms use only a language model in the target language in order to correct the sentences. We use phrase based correction methods in both the algorithms. The phrases are replaced and combined to give us the final corrected sentence. We also present the methods to model different kinds of errors, in addition to results of the working of the algorithms on the test set. We show that one of the approaches fail to achieve the desired goal, whereas the other succeeds well. In the end, we analyze the possible reasons for such a trend in performance.\n    ",
        "submission_date": "2011-05-22T00:00:00",
        "last_modified_date": "2011-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.4394",
        "title": "Integrating Testing and Interactive Theorem Proving",
        "authors": [
            "Harsh Raju Chamarthi",
            "Peter C. Dillinger",
            "Matt Kaufmann",
            "Panagiotis Manolios"
        ],
        "abstract": "Using an interactive theorem prover to reason about programs involves a sequence of interactions where the user challenges the theorem prover with conjectures. Invariably, many of the conjectures posed are in fact false, and users often spend considerable effort examining the theorem prover's output before realizing this. We present a synergistic integration of testing with theorem proving, implemented in the ACL2 Sedan (ACL2s), for automatically generating concrete counterexamples.  Our method uses the full power of the theorem prover and associated libraries to simplify conjectures; this simplification can transform conjectures for which finding counterexamples is hard into conjectures where finding counterexamples is trivial. In fact, our approach even leads to better theorem proving, e.g. if testing shows that a generalization step leads to a false conjecture, we force the theorem prover to backtrack, allowing it to pursue more fruitful options that may yield a proof. The focus of the paper is on the engineering of a synergistic integration of testing with interactive theorem proving; this includes extending ACL2 with new functionality that we expect to be of general interest.  We also discuss our experience in using ACL2s to teach freshman students how to reason about their programs.\n    ",
        "submission_date": "2011-05-23T00:00:00",
        "last_modified_date": "2011-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5129",
        "title": "A Quantitative Version of the Gibbard-Satterthwaite Theorem for Three Alternatives",
        "authors": [
            "Ehud Friedgut",
            "Gil Kalai",
            "Nathan Keller",
            "Noam Nisan"
        ],
        "abstract": "The Gibbard-Satterthwaite theorem states that every non-dictatorial election rule among at least three alternatives can be strategically manipulated. We prove a quantitative version of the Gibbard-Satterthwaite theorem: a random manipulation by a single random voter will succeed with a non-negligible probability for any election rule among three alternatives that is far from being a dictatorship and from having only two alternatives in its range.\n    ",
        "submission_date": "2011-05-25T00:00:00",
        "last_modified_date": "2011-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5464",
        "title": "Learning to Order Things",
        "authors": [
            "W. W. Cohen",
            "R. E. Schapire",
            "Y. Singer"
        ],
        "abstract": "There are many applications in which it is desirable to    order rather than classify instances. Here we consider the problem of    learning how to order instances given feedback in the form of    preference judgments, i.e., statements to the effect that one instance    should be ranked ahead of another.  We outline a two-stage approach in    which one first learns by conventional means a binary preference    function indicating whether it is advisable to rank one instance    before another. Here we consider an on-line algorithm for learning    preference functions that is based on Freund and Schapire's 'Hedge'    algorithm.  In the second stage, new instances are ordered so as to    maximize agreement with the learned preference function.  We show that    the problem of finding the ordering that agrees best with a learned    preference function is NP-complete.  Nevertheless, we describe simple    greedy algorithms that are guaranteed to find a good approximation.    Finally, we show how metasearch can be formulated as an ordering    problem, and present experimental results on learning a combination of    'search experts', each of which is a domain-specific query expansion    strategy for a web search engine.\n    ",
        "submission_date": "2011-05-27T00:00:00",
        "last_modified_date": "2011-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0221",
        "title": "Evolutionary Algorithms for Reinforcement Learning",
        "authors": [
            "J. J. Grefenstette",
            "D. E. Moriarty",
            "A. C. Schultz"
        ],
        "abstract": "There are two distinct approaches to solving reinforcement    learning problems, namely, searching in value function space and    searching in policy space.  Temporal difference methods and    evolutionary algorithms are well-known examples of these approaches.    Kaelbling, Littman and Moore recently provided an informative survey    of temporal difference methods.  This article focuses on the    application of evolutionary algorithms to the reinforcement learning    problem, emphasizing alternative policy representations, credit    assignment methods, and problem-specific genetic operators.  Strengths    and weaknesses of the evolutionary approach to reinforcement learning    are presented, along with a survey of representative applications.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0223",
        "title": "Decentralized Markets versus Central Control: A Comparative Study",
        "authors": [
            "H. Akkermans",
            "F. Ygge"
        ],
        "abstract": "Multi-Agent Systems (MAS) promise to offer solutions to    problems where established, older paradigms fall short. In order to validate such claims that are repeatedly made in software agent publications, empirical in-depth studies of advantages and weaknesses of multi-agent solutions versus conventional ones in practical applications are needed. Climate control in large buildings is one application area where multi-agent systems, and market-oriented programming in particular, have been reported to be very successful, although central control solutions are still the standard practice. We have therefore constructed and implemented a variety of market designs for this problem, as well as different standard control engineering solutions. This article gives a detailed analysis and comparison, so as to learn about differences between standard versus agent approaches, and yielding new insights about benefits and limitations of computational markets. An important outcome is that \"local information plus market communication produces global control\".\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0235",
        "title": "Robust Agent Teams via Socially-Attentive Monitoring",
        "authors": [
            "G. A. Kaminka",
            "M. Tambe"
        ],
        "abstract": "Agents in dynamic multi-agent environments must monitor    their peers to execute individual and group plans. A key open question    is how much monitoring of other agents' states is required to be    effective: The Monitoring Selectivity Problem.  We investigate this    question in the context of detecting failures in teams of cooperating    agents, via Socially-Attentive Monitoring, which focuses on monitoring    for failures in the social relationships between the agents. We    empirically and analytically explore a family of socially-attentive    teamwork monitoring algorithms in two dynamic, complex, multi-agent    domains, under varying conditions of task distribution and    uncertainty. We show that a centralized scheme using a complex    algorithm trades correctness for completeness and requires monitoring    all teammates. In contrast, a simple distributed teamwork monitoring    algorithm results in correct and complete detection of teamwork    failures, despite relying on limited, uncertain knowledge, and    monitoring only key agents in a team.  In addition, we report on the    design of a socially-attentive monitoring system and demonstrate its    generality in monitoring several coordination relationships,    diagnosing detected failures, and both on-line and off-line applications.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0357",
        "title": "Learning Hierarchical Sparse Representations using Iterative Dictionary Learning and Dimension Reduction",
        "authors": [
            "Mohamad Tarifi",
            "Meera Sitharam",
            "Jeffery Ho"
        ],
        "abstract": "This paper introduces an elemental building block which combines Dictionary Learning and Dimension Reduction (DRDL). We show how this foundational element can be used to iteratively construct a Hierarchical Sparse Representation (HSR) of a sensory stream. We compare our approach to existing models showing the generality of our simple prescription. We then perform preliminary experiments using this framework, illustrating with the example of an object recognition task using standard datasets. This work introduces the very first steps towards an integrated framework for designing and analyzing various computational tasks from learning to attention to action. The ultimate goal is building a mathematically rigorous, integrated theory of intelligence.\n    ",
        "submission_date": "2011-06-02T00:00:00",
        "last_modified_date": "2011-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0676",
        "title": "Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System",
        "authors": [
            "M. Kearns",
            "D. Litman",
            "S. Singh",
            "M. Walker"
        ],
        "abstract": "Designing the dialogue policy of a spoken dialogue system    involves many nontrivial choices.  This paper presents a reinforcement    learning approach for automatically optimizing a dialogue policy,    which addresses the technical challenges in applying reinforcement    learning to a working dialogue system with human users.  We report on    the design, construction and empirical evaluation of NJFun, an    experimental spoken dialogue system that provides users with access to    information about fun things to do in New Jersey.  Our results show    that by optimizing its performance via reinforcement learning, NJFun    measurably improves system performance.\n    ",
        "submission_date": "2011-06-03T00:00:00",
        "last_modified_date": "2011-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0681",
        "title": "Accelerating Reinforcement Learning through Implicit Imitation",
        "authors": [
            "C. Boutilier",
            "B. Price"
        ],
        "abstract": "Imitation can be viewed as a means of enhancing learning in multiagent environments.  It augments an agent's ability to learn useful behaviors by making intelligent use of the knowledge implicit in behaviors demonstrated by cooperative teachers or other more experienced agents.  We propose and study a formal model of implicit imitation that can accelerate reinforcement learning dramatically in certain cases.  Roughly, by observing a mentor, a reinforcement-learning agent can extract information about its own capabilities in, and the relative value of, unvisited parts of the state space.  We study two specific instantiations of this model, one in which the learning agent and the mentor have identical abilities, and one designed to deal with agents and mentors with different action sets.  We illustrate the benefits of implicit imitation by integrating it with prioritized sweeping, and demonstrating improved performance and convergence through observation of single and multiple mentors. Though we make some stringent assumptions regarding observability and possible interactions, we briefly comment on extensions of the model that relax these restricitions.\n    ",
        "submission_date": "2011-06-03T00:00:00",
        "last_modified_date": "2011-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0707",
        "title": "Efficient Reinforcement Learning Using Recursive Least-Squares Methods",
        "authors": [
            "H. He",
            "D. Hu",
            "X. Xu"
        ],
        "abstract": "The recursive least-squares (RLS) algorithm is one of the    most well-known algorithms used in adaptive filtering, system    identification and adaptive control. Its popularity is mainly due to    its fast convergence speed, which is considered to be optimal in    practice. In this paper, RLS methods are used to solve reinforcement    learning problems, where two new reinforcement learning algorithms    using linear value function approximators are proposed and    analyzed. The two algorithms are called RLS-TD(lambda) and Fast-AHC    (Fast Adaptive Heuristic Critic), respectively. RLS-TD(lambda) can be    viewed as the extension of RLS-TD(0) from lambda=0 to general lambda    within interval [0,1], so it is a multi-step temporal-difference (TD)    learning algorithm using RLS methods. The convergence with probability    one and the limit of convergence of RLS-TD(lambda) are proved for    ergodic Markov chains. Compared to the existing LS-TD(lambda)    algorithm, RLS-TD(lambda) has advantages in computation and is more    suitable for online learning. The effectiveness of RLS-TD(lambda) is    analyzed and verified by learning prediction experiments of Markov    chains with a wide range of parameter settings.        The Fast-AHC algorithm is derived by applying the proposed    RLS-TD(lambda) algorithm in the critic network of the adaptive    heuristic critic method. Unlike conventional AHC algorithm, Fast-AHC    makes use of RLS methods to improve the learning-prediction efficiency    in the critic. Learning control experiments of the cart-pole balancing    and the acrobot swing-up problems are conducted to compare the data    efficiency of Fast-AHC with conventional AHC. From the experimental    results, it is shown that the data efficiency of learning control can    also be improved by using RLS methods in the learning-prediction    process of the critic. The performance of Fast-AHC is also compared    with that of the AHC method using LS-TD(lambda). Furthermore, it is    demonstrated in the experiments that different initial values of the    variance matrix in RLS-TD(lambda) are required to get better    performance not only in learning prediction but also in learning    control. The experimental results are analyzed based on the existing    theoretical work on the transient phase of forgetting factor RLS    methods.\n    ",
        "submission_date": "2011-06-03T00:00:00",
        "last_modified_date": "2011-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0823",
        "title": "Recovering Epipolar Geometry from Images of Smooth Surfaces",
        "authors": [
            "Oleg Kupervasser"
        ],
        "abstract": "We present four methods for recovering the epipolar geometry from images of smooth surfaces. In the existing methods for recovering epipolar geometry corresponding feature points are used that cannot be found in such images. The first method is based on finding corresponding characteristic points created by illumination (ICPM - illumination characteristic points' method (PM)). The second method is based on correspondent tangency points created by tangents from epipoles to outline of smooth bodies (OTPM - outline tangent PM). These two methods are exact and give correct results for real images, because positions of the corresponding illumination characteristic points and corresponding outline are known with small errors. But the second method is limited either to special type of scenes or to restricted camera motion. We also consider two more methods which are termed CCPM (curve characteristic PM) and CTPM (curve tangent PM), for searching epipolar geometry for images of smooth bodies based on a set of level curves with constant illumination intensity. The CCPM method is based on searching correspondent points on isophoto curves with the help of correlation of curvatures between these lines. The CTPM method is based on property of the tangential to isophoto curve epipolar line to map into the tangential to correspondent isophoto curves epipolar line. The standard method (SM) based on knowledge of pairs of the almost exact correspondent points. The methods have been implemented and tested by SM on pairs of real images. Unfortunately, the last two methods give us only a finite subset of solutions including \"good\" solution. Exception is \"epipoles in infinity\". The main reason is inaccuracy of assumption of constant brightness for smooth bodies. But outline and illumination characteristic points are not influenced by this inaccuracy. So, the first pair of methods gives exact results.\n    ",
        "submission_date": "2011-06-04T00:00:00",
        "last_modified_date": "2012-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0987",
        "title": "Nearest Prime Simplicial Complex for Object Recognition",
        "authors": [
            "Junping Zhang",
            "Ziyu Xie",
            "Stan Z. Li"
        ],
        "abstract": "The structure representation of data distribution plays an important role in understanding the underlying mechanism of generating data. In this paper, we propose nearest prime simplicial complex approaches (NSC) by utilizing persistent homology to capture such structures. Assuming that each class is represented with a prime simplicial complex, we classify unlabeled samples based on the nearest projection distances from the samples to the simplicial complexes. We also extend the extrapolation ability of these complexes with a projection constraint term. Experiments in simulated and practical datasets indicate that compared with several published algorithms, the proposed NSC approaches achieve promising performance without losing the structure representation.\n    ",
        "submission_date": "2011-06-06T00:00:00",
        "last_modified_date": "2011-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1157",
        "title": "Bayesian and L1 Approaches to Sparse Unsupervised Learning",
        "authors": [
            "Shakir Mohamed",
            "Katherine Heller",
            "Zoubin Ghahramani"
        ],
        "abstract": "The use of L1 regularisation for sparse learning has generated immense research interest, with successful application in such diverse areas as signal acquisition, image coding, genomics and collaborative filtering. While existing work highlights the many advantages of L1 methods, in this paper we find that L1 regularisation often dramatically underperforms in terms of predictive performance when compared with other methods for inferring sparsity. We focus on unsupervised latent variable models, and develop L1 minimising factor models, Bayesian variants of \"L1\", and Bayesian models with a stronger L0-like sparsity induced through spike-and-slab distributions. These spike-and-slab Bayesian factor models encourage sparsity while accounting for uncertainty in a principled manner and avoiding unnecessary shrinkage of non-zero values. We demonstrate on a number of data sets that in practice spike-and-slab Bayesian methods outperform L1 minimisation, even on a computational budget. We thus highlight the need to re-assess the wide use of L1 methods in sparsity-reliant applications, particularly when we care about generalising to previously unseen data, and provide an alternative that, over many varying conditions, provides improved generalisation performance.\n    ",
        "submission_date": "2011-06-06T00:00:00",
        "last_modified_date": "2012-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.2363",
        "title": "Random design analysis of ridge regression",
        "authors": [
            "Daniel Hsu",
            "Sham M. Kakade",
            "Tong Zhang"
        ],
        "abstract": "This work gives a simultaneous analysis of both the ordinary least squares estimator and the ridge regression estimator in the random design setting under mild assumptions on the covariate/response distributions. In particular, the analysis provides sharp results on the ``out-of-sample'' prediction error, as opposed to the ``in-sample'' (fixed design) error. The analysis also reveals the effect of errors in the estimated covariance structure, as well as the effect of modeling errors, neither of which effects are present in the fixed design setting. The proofs of the main results are based on a simple decomposition lemma combined with concentration inequalities for random vectors and matrices.\n    ",
        "submission_date": "2011-06-13T00:00:00",
        "last_modified_date": "2014-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.2369",
        "title": "Efficient Optimal Learning for Contextual Bandits",
        "authors": [
            "Miroslav Dudik",
            "Daniel Hsu",
            "Satyen Kale",
            "Nikos Karampatziakis",
            "John Langford",
            "Lev Reyzin",
            "Tong Zhang"
        ],
        "abstract": "We address the problem of learning in an online setting where the learner repeatedly observes features, selects among a set of actions, and receives reward for the action taken. We provide the first efficient algorithm with an optimal regret. Our algorithm uses a cost sensitive classification learner as an oracle and has a running time $\\mathrm{polylog}(N)$, where $N$ is the number of classification rules among which the oracle might choose. This is exponentially faster than all previous algorithms that achieve optimal regret in this setting. Our formulation also enables us to create an algorithm with regret that is additive rather than multiplicative in feedback delay as in all previous work.\n    ",
        "submission_date": "2011-06-13T00:00:00",
        "last_modified_date": "2011-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.2489",
        "title": "Eliciting Forecasts from Self-interested Experts: Scoring Rules for Decision Makers",
        "authors": [
            "Craig Boutilier"
        ],
        "abstract": "Scoring rules for eliciting expert predictions of random variables are usually developed assuming that experts derive utility only from the quality of their predictions (e.g., score awarded by the rule, or payoff in a prediction market). We study a more realistic setting in which (a) the principal is a decision maker and will take a decision based on the expert's prediction; and (b) the expert has an inherent interest in the decision. For example, in a corporate decision market, the expert may derive different levels of utility from the actions taken by her manager. As a consequence the expert will usually have an incentive to misreport her forecast to influence the choice of the decision maker if typical scoring rules are used. We develop a general model for this setting and introduce the concept of a compensation rule. When combined with the expert's inherent utility for decisions, a compensation rule induces a net scoring rule that behaves like a normal scoring rule. Assuming full knowledge of expert utility, we provide a complete characterization of all (strictly) proper compensation rules. We then analyze the situation where the expert's utility function is not fully known to the decision maker. We show bounds on: (a) expert incentive to misreport; (b) the degree to which an expert will misreport; and (c) decision maker loss in utility due to such uncertainty. These bounds depend in natural ways on the degree of uncertainty, the local degree of convexity of net scoring function, and natural properties of the decision maker's utility function. They also suggest optimization procedures for the design of compensation rules. Finally, we briefly discuss the use of compensation rules as market scoring rules for self-interested experts in a prediction market.\n    ",
        "submission_date": "2011-06-13T00:00:00",
        "last_modified_date": "2011-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.2662",
        "title": "Learning Equilibria with Partial Information in Decentralized Wireless Networks",
        "authors": [
            "Luca Rose",
            "Samir M. Perlaza",
            "Samson Lasaulce",
            "M\u00e9rouane Debbah"
        ],
        "abstract": "In this article, a survey of several important equilibrium concepts for decentralized networks is presented. The term decentralized is used here to refer to scenarios where decisions (e.g., choosing a power allocation policy) are taken autonomously by devices interacting with each other (e.g., through mutual interference). The iterative long-term interaction is characterized by stable points of the wireless network called equilibria. The interest in these equilibria stems from the relevance of network stability and the fact that they can be achieved by letting radio devices to repeatedly interact over time. To achieve these equilibria, several learning techniques, namely, the best response dynamics, fictitious play, smoothed fictitious play, reinforcement learning algorithms, and regret matching, are discussed in terms of information requirements and convergence properties. Most of the notions introduced here, for both equilibria and learning schemes, are illustrated by a simple case study, namely, an interference channel with two transmitter-receiver pairs.\n    ",
        "submission_date": "2011-06-14T00:00:00",
        "last_modified_date": "2011-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.3457",
        "title": "Extensional Higher-Order Logic Programming",
        "authors": [
            "A. Charalambidis",
            "K. Handjopoulos",
            "P. Rondogiannis",
            "W. W. Wadge"
        ],
        "abstract": "We propose a purely extensional semantics for higher-order logic programming. In this semantics program predicates denote sets of ordered tuples, and two predicates are equal iff they are equal as sets. Moreover, every program has a unique minimum Herbrand model which is the greatest lower bound of all Herbrand models of the program and the least fixed-point of an immediate consequence operator. We also propose an SLD-resolution proof procedure which is proven sound and complete with respect to the minimum model semantics. In other words, we provide a purely extensional theoretical framework for higher-order logic programming which generalizes the familiar theory of classical (first-order) logic programming.\n    ",
        "submission_date": "2011-06-17T00:00:00",
        "last_modified_date": "2011-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.3600",
        "title": "How Insight Emerges in a Distributed, Content-addressable Memory",
        "authors": [
            "Liane Gabora",
            "Apara Ranjan"
        ],
        "abstract": "We begin this chapter with the bold claim that it provides a neuroscientific explanation of the magic of creativity. Creativity presents a formidable challenge for neuroscience. Neuroscience generally involves studying what happens in the brain when someone engages in a task that involves responding to a stimulus, or retrieving information from memory and using it the right way, or at the right time. If the relevant information is not already encoded in memory, the task generally requires that the individual make systematic use of information that is encoded in memory. But creativity is different. It paradoxically involves studying how someone pulls out of their brain something that was never put into it! Moreover, it must be something both new and useful, or appropriate to the task at hand. The ability to pull out of memory something new and appropriate that was never stored there in the first place is what we refer to as the magic of creativity. Even if we are so fortunate as to determine which areas of the brain are active and how these areas interact during creative thought, we will not have an answer to the question of how the brain comes up with solutions and artworks that are new and appropriate. On the other hand, since the representational capacity of neurons emerges at a level that is higher than that of the individual neurons themselves, the inner workings of neurons is too low a level to explain the magic of creativity. Thus we look to a level that is midway between gross brain regions and neurons. Since creativity generally involves combining concepts from different domains, or seeing old ideas from new perspectives, we focus our efforts on the neural mechanisms underlying the representation of concepts and ideas. Thus we ask questions about the brain at the level that accounts for its representational capacity, i.e. at the level of distributed aggregates of neurons.\n    ",
        "submission_date": "2011-06-18T00:00:00",
        "last_modified_date": "2019-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.3655",
        "title": "Bayesian multitask inverse reinforcement learning",
        "authors": [
            "Christos Dimitrakakis",
            "Constantin Rothkopf"
        ],
        "abstract": "We generalise the problem of inverse reinforcement learning to multiple tasks, from multiple demonstrations. Each one may represent one expert trying to solve a different task, or as different experts trying to solve the same task. Our main contribution is to formalise the problem as statistical preference elicitation, via a number of structured priors, whose form captures our biases about the relatedness of different tasks or expert policies. In doing so, we introduce a prior on policy optimality, which is more natural to specify. We show that our framework allows us not only to learn to efficiently from multiple experts but to also effectively differentiate between the goals of each. Possible applications include analysing the intrinsic motivations of subjects in behavioural experiments and learning from multiple teachers.\n    ",
        "submission_date": "2011-06-18T00:00:00",
        "last_modified_date": "2011-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.3703",
        "title": "Prediction and Modularity in Dynamical Systems",
        "authors": [
            "Artemy Kolchinsky",
            "Luis M. Rocha"
        ],
        "abstract": "Identifying and understanding modular organizations is centrally important in the study of complex systems. Several approaches to this problem have been advanced, many framed in information-theoretic terms. Our treatment starts from the complementary point of view of statistical modeling and prediction of dynamical systems. It is known that for finite amounts of training data, simpler models can have greater predictive power than more complex ones. We use the trade-off between model simplicity and predictive accuracy to generate optimal multiscale decompositions of dynamical networks into weakly-coupled, simple modules. State-dependent and causal versions of our method are also proposed.\n    ",
        "submission_date": "2011-06-19T00:00:00",
        "last_modified_date": "2015-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.3862",
        "title": "On Kinds of Indiscernibility in Logic and Metaphysics",
        "authors": [
            "Adam Caulton",
            "Jeremy Butterfield"
        ],
        "abstract": "Using the Hilbert-Bernays account as a spring-board, we first define four ways in which two objects can be discerned from one another, using the non-logical vocabulary of the language concerned. (These definitions are based on definitions made by Quine and Saunders.) Because of our use of the Hilbert-Bernays account, these definitions are in terms of the syntax of the language. But we also relate our definitions to the idea of permutations on the domain of quantification, and their being symmetries. These relations turn out to be subtle---some natural conjectures about them are false. We will see in particular that the idea of symmetry meshes with a species of indiscernibility that we will call `absolute indiscernibility'. We then report all the logical implications between our four kinds of discernibility. We use these four kinds as a resource for stating four metaphysical theses about identity. Three of these theses articulate two traditional philosophical themes: viz. the principle of the identity of indiscernibles (which will come in two versions), and haecceitism. The fourth is recent. Its most notable feature is that it makes diversity (i.e. non-identity) weaker than what we will call individuality (being an individual): two objects can be distinct but not individuals. For this reason, it has been advocated both for quantum particles and for spacetime points. Finally, we locate this fourth metaphysical thesis in a broader position, which we call structuralism. We conclude with a discussion of the semantics suitable for a structuralist, with particular reference to physical theories as well as elementary model theory.\n    ",
        "submission_date": "2011-06-20T00:00:00",
        "last_modified_date": "2011-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4090",
        "title": "Discovery of Invariants through Automated Theory Formation",
        "authors": [
            "Maria Teresa Llano",
            "Andrew Ireland",
            "Alison Pease"
        ],
        "abstract": "Refinement is a powerful mechanism for mastering the complexities that arise when formally modelling systems. Refinement also brings with it additional proof obligations -- requiring a developer to discover properties relating to their design decisions. With the goal of reducing this burden, we have investigated how a general purpose theory formation tool, HR, can be used to automate the discovery of such properties within the context of Event-B. Here we develop a heuristic approach to the automatic discovery of invariants and report upon a  series of experiments that we undertook in order to evaluate our approach. The set of heuristics developed provides systematic guidance in tailoring HR for a given Event-B development. These heuristics are based upon proof-failure analysis, and have given rise to some promising results. \n    ",
        "submission_date": "2011-06-21T00:00:00",
        "last_modified_date": "2011-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4333",
        "title": "Residual Component Analysis",
        "authors": [
            "Alfredo A. Kalaitzis",
            "Neil D. Lawrence"
        ],
        "abstract": "Probabilistic principal component analysis (PPCA) seeks a low dimensional representation of a data set in the presence of independent spherical Gaussian noise, Sigma = (sigma^2)*I. The maximum likelihood solution for the model is an eigenvalue problem on the sample covariance matrix. In this paper we consider the situation where the data variance is already partially explained by other factors, e.g. covariates of interest, or temporal correlations leaving some residual variance. We decompose the residual variance into its components through a generalized eigenvalue problem, which we call residual component analysis (RCA). We show that canonical covariates analysis (CCA) is a special case of our algorithm and explore a range of new algorithms that arise from the framework. We illustrate the ideas on a gene expression time series data set and the recovery of human pose from silhouette.\n    ",
        "submission_date": "2011-06-21T00:00:00",
        "last_modified_date": "2011-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4570",
        "title": "Competitive Safety Analysis: Robust Decision-Making in Multi-Agent Systems",
        "authors": [
            "M. Tennenholtz"
        ],
        "abstract": "Much work in AI deals with the selection of proper actions in a given (known or unknown) environment. However, the way to select a proper action when facing other agents is quite unclear. Most work in AI adopts classical game-theoretic equilibrium analysis to predict agent behavior in such settings. This approach however does not provide us with any guarantee for the agent. In this paper we introduce competitive safety analysis. This approach bridges the gap between the desired normative AI approach, where a strategy should be selected in order to guarantee a desired payoff, and equilibrium analysis. We show that a safety level strategy is able to guarantee the value obtained in a Nash equilibrium, in several classical computer science settings. Then, we discuss the concept of competitive safety strategies, and illustrate its use in a decentralized load balancing setting, typical to network problems. In particular, we show that when we have many agents, it is possible to guarantee an expected payoff which is a factor of 8/9 of the payoff obtained in a Nash equilibrium. Our discussion of competitive safety analysis for decentralized load balancing is further developed to deal with many communication links and arbitrary speeds. Finally, we discuss the extension of the above concepts to Bayesian games, and illustrate their use in a basic auctions setup.\n    ",
        "submission_date": "2011-06-22T00:00:00",
        "last_modified_date": "2011-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4571",
        "title": "Acquiring Word-Meaning Mappings for Natural Language Interfaces",
        "authors": [
            "C. Thompson"
        ],
        "abstract": "This paper focuses on a system, WOLFIE (WOrd Learning From    Interpreted Examples), that acquires a semantic lexicon from a corpus    of sentences paired with semantic representations.  The lexicon    learned consists of phrases paired with meaning representations.    WOLFIE is part of an integrated system that learns to transform    sentences into representations such as logical database queries.        Experimental results are presented demonstrating WOLFIE's ability to    learn useful lexicons for a database interface in four different    natural languages.  The usefulness of the lexicons learned by WOLFIE    are compared to those acquired by a similar system, with results    favorable to WOLFIE.  A second set of experiments demonstrates    WOLFIE's ability to scale to larger and more difficult, albeit    artificially generated, corpora.           In natural language acquisition, it is difficult to gather the    annotated data needed for supervised learning; however, unannotated    data is fairly plentiful.  Active learning methods attempt to select    for annotation and training only the most informative examples, and    therefore are potentially very useful in natural language    applications.  However, most results to date for active learning have    only considered standard classification tasks.  To reduce annotation    effort while maintaining accuracy, we apply active learning to    semantic lexicons.  We show that active learning can significantly    reduce the number of annotated examples required to achieve a given    level of performance.\n    ",
        "submission_date": "2011-06-22T00:00:00",
        "last_modified_date": "2011-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4577",
        "title": "Interactive Execution Monitoring of Agent Teams",
        "authors": [
            "P. Berry",
            "T. J. Lee",
            "D. E. Wilkins"
        ],
        "abstract": "There is an increasing need for automated support for humans    monitoring the activity of distributed teams of cooperating agents,    both human and machine. We characterize the domain-independent    challenges posed by this problem, and describe how properties of    domains influence the challenges and their solutions. We will    concentrate on dynamic, data-rich domains where humans are ultimately    responsible for team behavior. Thus, the automated aid should    interactively support effective and timely decision making by the    human. We present a domain-independent categorization of the types of    alerts a plan-based monitoring system might issue to a user, where    each type generally requires different monitoring techniques. We    describe a monitoring framework for integrating many domain-specific    and task-specific monitoring techniques and then using the concept of    value of an alert to avoid operator overload.       We use this framework to describe an execution monitoring approach we    have used to implement Execution Assistants (EAs) in two different    dynamic, data-rich, real-world domains to assist a human in    monitoring team behavior. One domain (Army small unit operations) has    hundreds of mobile, geographically distributed agents, a combination    of humans, robots, and vehicles. The other domain (teams of unmanned    ground and air vehicles) has a handful of cooperating robots. Both    domains involve unpredictable adversaries in the vicinity. Our    approach customizes monitoring behavior for each specific task, plan,    and situation, as well as for user preferences. Our EAs alert the    human controller when reported events threaten plan execution or    physically threaten team members. Alerts were generated in a timely    manner without inundating the user with too many alerts (less than    10 percent of alerts are unwanted, as judged by domain experts).\n    ",
        "submission_date": "2011-06-22T00:00:00",
        "last_modified_date": "2011-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4632",
        "title": "Inferring 3D Articulated Models for Box Packaging Robot",
        "authors": [
            "Heran Yang",
            "Tiffany Low",
            "Matthew Cong",
            "Ashutosh Saxena"
        ],
        "abstract": "Given a point cloud, we consider inferring kinematic models of 3D articulated objects such as boxes for the purpose of manipulating them. While previous work has shown how to extract a planar kinematic model (often represented as a linear chain), such planar models do not apply to 3D objects that are composed of segments often linked to the other segments in cyclic configurations. We present an approach for building a model that captures the relation between the input point cloud features and the object segment as well as the relation between the neighboring object segments. We use a conditional random field that allows us to model the dependencies between different segments of the object. We test our approach on inferring the kinematic structure from partial and noisy point cloud data for a wide variety of boxes including cake boxes, pizza boxes, and cardboard cartons of several sizes. The inferred structure enables our robot to successfully close these boxes by manipulating the flaps.\n    ",
        "submission_date": "2011-06-23T00:00:00",
        "last_modified_date": "2011-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4862",
        "title": "Translation of Pronominal Anaphora between English and Spanish: Discrepancies and Evaluation",
        "authors": [
            "A. Ferrandez",
            "J. Peral"
        ],
        "abstract": "This paper evaluates the different tasks carried out in the translation of pronominal anaphora in a machine translation (MT) system. The MT interlingua approach named AGIR (Anaphora Generation with an Interlingua Representation) improves upon other proposals presented to date because it is able to translate intersentential anaphors, detect co-reference chains, and translate Spanish zero pronouns into English---issues hardly considered by other systems. The paper presents the resolution and evaluation of these anaphora problems in AGIR with the use of different kinds of knowledge (lexical, morphological, syntactic, and semantic). The translation of English and Spanish anaphoric third-person personal pronouns (including Spanish zero pronouns) into the target language has been evaluated on unrestricted corpora. We have obtained a precision of 80.4% and 84.8% in the translation of Spanish and English pronouns, respectively. Although we have only studied the Spanish and English languages, our approach can be easily extended to other languages such as Portuguese, Italian, or Japanese.\n    ",
        "submission_date": "2011-06-24T00:00:00",
        "last_modified_date": "2011-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4925",
        "title": "Belief-propagation algorithm and the Ising model on networks with arbitrary distributions of motifs",
        "authors": [
            "S. Yoon",
            "A. V. Goltsev",
            "S. N. Dorogovtsev",
            "J. F. F. Mendes"
        ],
        "abstract": "We generalize the belief-propagation algorithm to sparse random networks with arbitrary distributions of motifs (triangles, loops, etc.). Each vertex in these networks belongs to a given set of motifs (generalization of the configuration model). These networks can be treated as sparse uncorrelated hypergraphs in which hyperedges represent motifs. Here a hypergraph is a generalization of a graph, where a hyperedge can connect any number of vertices. These uncorrelated hypergraphs are tree-like (hypertrees), which crucially simplify the problem and allow us to apply the belief-propagation algorithm to these loopy networks with arbitrary motifs. As natural examples, we consider motifs in the form of finite loops and cliques. We apply the belief-propagation algorithm to the ferromagnetic Ising model on the resulting random networks. We obtain an exact solution of this model on networks with finite loops or cliques as motifs. We find an exact critical temperature of the ferromagnetic phase transition and demonstrate that with increasing the clustering coefficient and the loop size, the critical temperature increases compared to ordinary tree-like complex networks. Our solution also gives the birth point of the giant connected component in these loopy networks.\n    ",
        "submission_date": "2011-06-24T00:00:00",
        "last_modified_date": "2012-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5341",
        "title": "Pose Estimation from a Single Depth Image for Arbitrary Kinematic Skeletons",
        "authors": [
            "Daniel L. Ly",
            "Ashutosh Saxena",
            "Hod Lipson"
        ],
        "abstract": "We present a method for estimating pose information from a single depth image given an arbitrary kinematic structure without prior training. For an arbitrary skeleton and depth image, an evolutionary algorithm is used to find the optimal kinematic configuration to explain the observed image. Results show that our approach can correctly estimate poses of 39 and 78 degree-of-freedom models from a single depth image, even in cases of significant self-occlusion.\n    ",
        "submission_date": "2011-06-27T00:00:00",
        "last_modified_date": "2011-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5601",
        "title": "Class-based Rough Approximation with Dominance Principle",
        "authors": [
            "Junyi Chai",
            "James N.K. Liu"
        ],
        "abstract": "Dominance-based Rough Set Approach (DRSA), as the extension of Pawlak's Rough Set theory, is effective and fundamentally important in Multiple Criteria Decision Analysis (MCDA). In previous DRSA models, the definitions of the upper and lower approximations are preserving the class unions rather than the singleton class. In this paper, we propose a new Class-based Rough Approximation with respect to a series of previous DRSA models, including Classical DRSA model, VC-DRSA model and VP-DRSA model. In addition, the new class-based reducts are investigated.\n    ",
        "submission_date": "2011-06-28T00:00:00",
        "last_modified_date": "2011-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5829",
        "title": "Active Classification: Theory and Application to Underwater Inspection",
        "authors": [
            "Geoffrey A. Hollinger",
            "Urbashi Mitra",
            "Gaurav S. Sukhatme"
        ],
        "abstract": "We discuss the problem in which an autonomous vehicle must classify an object based on multiple views. We focus on the active classification setting, where the vehicle controls which views to select to best perform the classification. The problem is formulated as an extension to Bayesian active learning, and we show connections to recent theoretical guarantees in this area. We formally analyze the benefit of acting adaptively as new information becomes available. The analysis leads to a probabilistic algorithm for determining the best views to observe based on information theoretic costs. We validate our approach in two ways, both related to underwater inspection: 3D polyhedra recognition in synthetic depth maps and ship hull inspection with imaging sonar. These tasks encompass both the planning and recognition aspects of the active classification problem. The results demonstrate that actively planning for informative views can reduce the number of necessary views by up to 80% when compared to passive methods.\n    ",
        "submission_date": "2011-06-29T00:00:00",
        "last_modified_date": "2011-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.6024",
        "title": "The Rate of Convergence of AdaBoost",
        "authors": [
            "Indraneel Mukherjee",
            "Cynthia Rudin",
            "Robert E. Schapire"
        ],
        "abstract": "The AdaBoost algorithm was designed to combine many \"weak\" hypotheses that perform slightly better than random guessing into a \"strong\" hypothesis that has very low error. We study the rate at which AdaBoost iteratively converges to the minimum of the \"exponential loss.\" Unlike previous work, our proofs do not require a weak-learning assumption, nor do they require that minimizers of the exponential loss are finite. Our first result shows that at iteration $t$, the exponential loss of AdaBoost's computed parameter vector will be at most $\\epsilon$ more than that of any parameter vector of $\\ell_1$-norm bounded by $B$ in a number of rounds that is at most a polynomial in $B$ and $1/\\epsilon$. We also provide lower bounds showing that a polynomial dependence on these parameters is necessary. Our second result is that within $C/\\epsilon$ iterations, AdaBoost achieves a value of the exponential loss that is at most $\\epsilon$ more than the best possible value, where $C$ depends on the dataset. We show that this dependence of the rate on $\\epsilon$ is optimal up to constant factors, i.e., at least $\\Omega(1/\\epsilon)$ rounds are necessary to achieve within $\\epsilon$ of the optimal exponential loss.\n    ",
        "submission_date": "2011-06-29T00:00:00",
        "last_modified_date": "2011-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.6251",
        "title": "Kernels for Vector-Valued Functions: a Review",
        "authors": [
            "Mauricio A. Alvarez",
            "Lorenzo Rosasco",
            "Neil D. Lawrence"
        ],
        "abstract": "Kernel methods are among the most popular techniques in machine learning. From a frequentist/discriminative perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a Bayesian/generative perspective they are the key in the context of Gaussian processes, where the kernel function is also known as the covariance function. Traditionally, kernel methods have been used in supervised learning problem with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partly by frameworks like multitask learning. In this paper, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2012-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.6341",
        "title": "Vision-Based Navigation III: Pose and Motion from Omnidirectional Optical Flow and a Digital Terrain Map",
        "authors": [
            "Ronen Lerner",
            "Oleg Kupervasser",
            "Ehud Rivlin"
        ],
        "abstract": "An algorithm for pose and motion estimation using corresponding features in omnidirectional images and a digital terrain map is proposed. In previous paper, such algorithm for regular camera was considered. Using a Digital Terrain (or Digital Elevation) Map (DTM/DEM) as a global reference enables recovering the absolute position and orientation of the camera. In order to do this, the DTM is used to formulate a constraint between corresponding features in two consecutive frames. In this paper, these constraints are extended to handle non-central projection, as is the case with many omnidirectional systems. The utilization of omnidirectional data is shown to improve the robustness and accuracy of the navigation algorithm. The feasibility of this algorithm is established through lab experimentation with two kinds of omnidirectional acquisition systems. The first one is polydioptric cameras while the second is catadioptric camera.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0022",
        "title": "K-Implementation",
        "authors": [
            "D. Monderer",
            "M. Tennenholtz"
        ],
        "abstract": "This paper discusses an interested party who wishes to influence the behavior of agents in a game (multi-agent interaction), which is not under his control. The interested party cannot design a new game, cannot enforce agents' behavior, cannot enforce payments by the agents, and cannot prohibit strategies available to the agents. However, he can influence the outcome of the game by committing to non-negative monetary transfers for the different strategy profiles that may be selected by the agents.  The interested party assumes that agents are rational in the commonly agreed sense that they do not use dominated strategies. Hence, a certain subset of outcomes is implemented in a given game if by adding non-negative payments, rational players will necessarily produce an outcome in this subset. Obviously, by making sufficiently big payments one can implement any desirable outcome. The question is what is the cost of implementation? In this paper we introduce the notion of k-implementation of a desired set of strategy profiles, where k stands for the amount of payment that need to be actually made in order to implement desirable outcomes. A major point in k-implementation is that monetary offers need not necessarily materialize when following desired behaviors.  We define and study k-implementation in the contexts of games with complete and incomplete information. In the latter case we mainly focus on the VCG games. Our setting is later extended to deal with mixed strategies using correlation devices. Together, the paper introduces and studies the implementation of desirable outcomes by a reliable party who cannot modify game rules (i.e. provide protocols), complementing previous work in mechanism design, while making it more applicable to many realistic CS settings.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0029",
        "title": "A Personalized System for Conversational Recommendations",
        "authors": [
            "M. H. Goker",
            "P. Langley",
            "C. A. Thompson"
        ],
        "abstract": "Searching for and making decisions about information is becoming increasingly difficult as the amount of information and number of choices increases.  Recommendation systems help users find items of interest of a particular type, such as movies or restaurants, but are still somewhat awkward to use.  Our solution is to take advantage of the complementary strengths of personalized recommendation systems and dialogue systems, creating personalized aides.  We present a  system -- the Adaptive Place Advisor -- that treats item selection as an interactive, conversational process, with the program inquiring about item attributes and the user responding.  Individual, long-term user preferences are unobtrusively obtained in the course of normal recommendation dialogues and used to direct future conversations with the same user.  We present a novel user model that influences both item search and the questions asked during a conversation.  We demonstrate the effectiveness of our system in significantly reducing the time and number of interactions required to find a satisfactory item, as compared to a control group of users interacting with a non-adaptive version of the system.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0082",
        "title": "A case of combination of evidence in the Dempster-Shafer theory inconsistent with evaluation of probabilities",
        "authors": [
            "Andrzej K. Brodzik",
            "Robert H. Enders"
        ],
        "abstract": "The Dempster-Shafer theory of evidence accumulation is one of the main tools for combining data obtained from multiple sources. In this paper a special case of combination of two bodies of evidence with non-zero conflict coefficient is considered. It is shown that application of the Dempster-Shafer rule of combination in this case leads to an evaluation of masses of the combined bodies that is different from the evaluation of the corresponding probabilities obtained by application of the law of total probability. This finding supports the view that probabilistic interpretation of results of the Dempster-Shafer analysis in the general case is not appropriate.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0089",
        "title": "Towards a Reliable Framework of Uncertainty-Based Group Decision Support System",
        "authors": [
            "Junyi Chai",
            "James N.K. Liu"
        ],
        "abstract": "This study proposes a framework of Uncertainty-based Group Decision Support System (UGDSS). It provides a platform for multiple criteria decision analysis in six aspects including (1) decision environment, (2) decision problem, (3) decision group, (4) decision conflict, (5) decision schemes and (6) group negotiation. Based on multiple artificial intelligent technologies, this framework provides reliable support for the comprehensive manipulation of applications and advanced decision approaches through the design of an integrated multi-agents architecture.\n    ",
        "submission_date": "2011-07-01T00:00:00",
        "last_modified_date": "2011-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0098",
        "title": "A Probabilistic Attack on NP-complete Problems",
        "authors": [
            "Alexander Y. Davydov"
        ],
        "abstract": "Using the probability theory-based approach, this paper reveals the equivalence of an arbitrary NP-complete problem to a problem of checking whether a level set of a specifically constructed harmonic cost function (with all diagonal entries of its Hessian matrix equal to zero) intersects with a unit hypercube in many-dimensional Euclidean space. This connection suggests the possibility that methods of continuous mathematics can provide crucial insights into the most intriguing open questions in modern complexity theory.\n    ",
        "submission_date": "2011-07-01T00:00:00",
        "last_modified_date": "2012-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0399",
        "title": "Vision-Based Navigation I: A navigation filter for fusing DTM/correspondence updates",
        "authors": [
            "Oleg Kupervasser",
            "Vladimir Voronov"
        ],
        "abstract": "An algorithm for pose and motion estimation using corresponding features in images and a digital terrain map is proposed. Using a Digital Terrain (or Digital Elevation) Map (DTM/DEM) as a global reference enables recovering the absolute position and orientation of the camera. In order to do this, the DTM is used to formulate a constraint between corresponding features in two consecutive frames. The utilization of data is shown to improve the robustness and accuracy of the inertial navigation algorithm. Extended Kalman filter was used to combine results of inertial navigation algorithm and proposed vision-based navigation algorithm. The feasibility of this algorithms is established through numerical simulations.\n    ",
        "submission_date": "2011-07-02T00:00:00",
        "last_modified_date": "2012-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0998",
        "title": "An Information Theoretic Representation of Agent Dynamics as Set Intersections",
        "authors": [
            "Samuel Epstein",
            "Margrit Betke"
        ],
        "abstract": "We represent agents as sets of strings. Each string encodes a potential interaction with another agent or environment. We represent the total set of dynamics between two agents as the intersection of their respective strings, we prove complexity properties of player interactions using Algorithmic Information Theory. We show how the proposed construction is compatible with Universal Artificial Intelligence, in that the AIXI model can be seen as universal with respect to interaction.\n    ",
        "submission_date": "2011-07-05T00:00:00",
        "last_modified_date": "2011-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.1470",
        "title": "Vision-Based Navigation II: Error Analysis for a Navigation Algorithm based on Optical-Flow and a Digital Terrain Map",
        "authors": [
            "Oleg Kupervasser",
            "Ronen Lerner",
            "Ehud Rivlin",
            "Hector Rotstein"
        ],
        "abstract": "The paper deals with the error analysis of a navigation algorithm that uses as input a sequence of images acquired by a moving camera and a Digital Terrain Map (DTM) of the region been imaged by the camera during the motion. The main sources of error are more or less straightforward to identify: camera resolution, structure of the observed terrain and DTM accuracy, field of view and camera trajectory. After characterizing and modeling these error sources in the framework of the CDTM algorithm, a closed form expression for their effect on the pose and motion errors of the camera can be found. The analytic expression provides a priori measurements for the accuracy in terms of the parameters mentioned above.\n    ",
        "submission_date": "2011-07-07T00:00:00",
        "last_modified_date": "2011-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.1805",
        "title": "Loss-sensitive Training of Probabilistic Conditional Random Fields",
        "authors": [
            "Maksims N. Volkovs",
            "Hugo Larochelle",
            "Richard S. Zemel"
        ],
        "abstract": "We consider the problem of training probabilistic conditional random fields (CRFs) in the context of a task where performance is measured using a specific loss function. While maximum likelihood is the most common approach to training CRFs, it ignores the inherent structure of the task's loss function. We describe alternatives to maximum likelihood which take that loss into account. These include a novel adaptation of a loss upper bound from the structured SVMs literature to the CRF context, as well as a new loss-inspired KL divergence objective which relies on the probabilistic nature of CRFs. These loss-sensitive objectives are compared to maximum likelihood using ranking as a benchmark task. This comparison confirms the importance of incorporating loss information in the probabilistic training of CRFs, with the loss-inspired KL outperforming all other objectives.\n    ",
        "submission_date": "2011-07-09T00:00:00",
        "last_modified_date": "2011-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.1851",
        "title": "Task swapping networks in distributed systems",
        "authors": [
            "Dohan Kim"
        ],
        "abstract": "In this paper we propose task swapping networks for task reassignments by using task swappings in distributed systems. Some classes of task reassignments are achieved by using iterative local task swappings between software agents in distributed systems. We use group-theoretic methods to find a minimum-length sequence of adjacent task swappings needed from a source task assignment to a target task assignment in a task swapping network of several well-known topologies.\n    ",
        "submission_date": "2011-07-10T00:00:00",
        "last_modified_date": "2013-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.1932",
        "title": "Current State and Challenges of Automatic Planning in Web Service Composition",
        "authors": [
            "Sleiman Rabah",
            "Dan Ni",
            "Payam Jahanshahi",
            "Luis Felipe Guzman"
        ],
        "abstract": "This paper gives a survey on the current state of Web Service Compositions and the difficulties and solutions to automated Web Service Compositions. This first gives a definition of Web Service Composition and the motivation and goal of it. It then explores into why we need automated Web Service Compositions and formally defines the domains. Techniques and solutions are proposed by the papers we surveyed to solve the current difficulty of automated Web Service Composition. Verification and future work is discussed at the end to further extend the topic.\n    ",
        "submission_date": "2011-07-11T00:00:00",
        "last_modified_date": "2011-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.1956",
        "title": "Informledge System: A Modified Knowledge Network with Autonomous Nodes using Multi-lateral Links",
        "authors": [
            "Dr T.R. Gopalakrishnan Nair",
            "Meenakshi Malhotra"
        ],
        "abstract": "Research in the field of Artificial Intelligence is continually progressing to simulate the human knowledge into automated intelligent knowledge base, which can encode and retrieve knowledge efficiently along with the capability of being is consistent and scalable at all times. However, there is no system at hand that can match the diversified abilities of human knowledge base. In this position paper, we put forward a theoretical model of a different system that intends to integrate pieces of knowledge, Informledge System (ILS). ILS would encode the knowledge, by virtue of knowledge units linked across diversified domains. The proposed ILS comprises of autonomous knowledge units termed as Knowledge Network Node (KNN), which would help in efficient cross-linking of knowledge units to encode fresh knowledge. These links are reasoned and inferred by the Parser and Link Manager, which are part of KNN.\n    ",
        "submission_date": "2011-07-11T00:00:00",
        "last_modified_date": "2011-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2126",
        "title": "Strong Solutions of the Fuzzy Linear Systems",
        "authors": [
            "\u015eahin Emrah Amrahov",
            "Iman N. Askerzade"
        ],
        "abstract": "We consider a fuzzy linear system with crisp coefficient matrix and with an arbitrary fuzzy number in parametric form on the right-hand side. It is known that the well-known existence and uniqueness theorem of a strong fuzzy solution is equivalent to the following: The coefficient matrix is the product of a permutation matrix and a diagonal matrix. This means that this theorem can be applicable only for a special form of linear systems, namely, only when the system consists of equations, each of which has exactly one variable. We prove an existence and uniqueness theorem, which can be use on more general systems. The necessary and sufficient conditions of the theorem are dependent on both the coefficient matrix and the right-hand side. This theorem is a generalization of the well-known existence and uniqueness theorem for the strong solution.\n    ",
        "submission_date": "2011-07-11T00:00:00",
        "last_modified_date": "2011-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2699",
        "title": "Linear Latent Force Models using Gaussian Processes",
        "authors": [
            "Mauricio A. \u00c1lvarez",
            "David Luengo",
            "Neil D. Lawrence"
        ],
        "abstract": "Purely data driven approaches for machine learning present difficulties when data is scarce relative to the complexity of the model or when the model is forced to extrapolate. On the other hand, purely mechanistic approaches need to identify and specify all the interactions in the problem at hand (which may not be feasible) and still leave the issue of how to parameterize the system. In this paper, we present a hybrid approach using Gaussian processes and differential equations to combine data driven modelling with a physical model of the system. We show how different, physically-inspired, kernel functions can be developed through sensible, simple, mechanistic assumptions about the underlying system. The versatility of our approach is illustrated with three case studies from motion capture, computational biology and geostatistics.\n    ",
        "submission_date": "2011-07-13T00:00:00",
        "last_modified_date": "2020-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2788",
        "title": "Diverse Consequences of Algorithmic Probability",
        "authors": [
            "Eray \u00d6zkural"
        ],
        "abstract": "We reminisce and discuss applications of algorithmic probability to a wide range of problems in artificial intelligence, philosophy and technological society. We propose that Solomonoff has effectively axiomatized the field of artificial intelligence, therefore establishing it as a rigorous scientific discipline. We also relate to our own work in incremental machine learning and philosophy of complexity.\n    ",
        "submission_date": "2011-07-14T00:00:00",
        "last_modified_date": "2011-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2822",
        "title": "A Survey on how Description Logic Ontologies Benefit from Formal Concept Analysis",
        "authors": [
            "Baris Sertkaya"
        ],
        "abstract": "Although the notion of a concept as a collection of objects sharing certain properties, and the notion of a conceptual hierarchy are fundamental to both Formal Concept Analysis and Description Logics, the ways concepts are described and obtained differ significantly between these two research areas. Despite these differences, there have been several attempts to bridge the gap between these two formalisms, and attempts to apply methods from one field in the other. The present work aims to give an overview on the research done in combining Description Logics and Formal Concept Analysis.\n    ",
        "submission_date": "2011-07-14T00:00:00",
        "last_modified_date": "2011-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.3342",
        "title": "Computing Strong Game-Theoretic Strategies in Jotto",
        "authors": [
            "Sam Ganzfried"
        ],
        "abstract": "We develop a new approach that computes approximate equilibrium strategies in Jotto, a popular word game. Jotto is an extremely large two-player game of imperfect information; its game tree has many orders of magnitude more states than games previously studied, including no-limit Texas hold 'em. To address the fact that the game is so large, we propose a novel strategy representation called oracular form, in which we do not explicitly represent a strategy, but rather appeal to an oracle that quickly outputs a sample move from the strategy's distribution. Our overall approach is based on an extension of the fictitious play algorithm to this oracular setting. We demonstrate the superiority of our computed strategies over the strategies computed by a benchmark algorithm, both in terms of head-to-head and worst-case performance.\n    ",
        "submission_date": "2011-07-17T00:00:00",
        "last_modified_date": "2016-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.3383",
        "title": "Evolutionary Quantum Logic Synthesis of Boolean Reversible Logic Circuits Embedded in Ternary Quantum Space using Heuristics",
        "authors": [
            "Maarti nLukac",
            "Marek Perkowski",
            "Michitaka Kameyama"
        ],
        "abstract": "It has been experimentally proven that realizing universal quantum gates using higher-radices logic is practically and technologically possible. We developed a Parallel Genetic Algorithm that synthesizes Boolean reversible circuits realized with a variety of quantum gates on qudits with various radices. In order to allow synthesizing circuits of medium sizes in the higher radix quantum space we performed the experiments using a GPU accelerated Genetic Algorithm. Using the accelerated GA we compare heuristic improvements to the mutation process based on cost minimization, on the adaptive cost of the primitives and improvements due to Baldwinian vs. Lamarckian GA. We also describe various fitness function formulations that allowed for various realizations of well known universal Boolean reversible or quantum-probabilistic circuits.\n    ",
        "submission_date": "2011-07-18T00:00:00",
        "last_modified_date": "2011-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.3792",
        "title": "Influence and Dynamic Behavior in Random Boolean Networks",
        "authors": [
            "C. Seshadhri",
            "Yevgeniy Vorobeychik",
            "Jackson R. Mayo",
            "Robert C. Armstrong",
            "Joseph R. Ruthruff"
        ],
        "abstract": "We present a rigorous mathematical framework for analyzing dynamics of a broad class of Boolean network models. We use this framework to provide the first formal proof of many of the standard critical transition results in Boolean network analysis, and offer analogous characterizations for novel classes of random Boolean networks. We precisely connect the short-run dynamic behavior of a Boolean network to the average influence of the transfer functions. We show that some of the assumptions traditionally made in the more common mean-field analysis of Boolean networks do not hold in general.\n",
        "submission_date": "2011-07-19T00:00:00",
        "last_modified_date": "2011-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4212",
        "title": "On the Undecidability of Fuzzy Description Logics with GCIs with Lukasiewicz t-norm",
        "authors": [
            "Marco Cerami",
            "Umberto Straccia"
        ],
        "abstract": "Recently there have been some unexpected results concerning Fuzzy Description Logics (FDLs) with General Concept Inclusions (GCIs). They show that, unlike the classical case, the DL ALC with GCIs does not have the finite model property under Lukasiewicz Logic or Product Logic and, specifically, knowledge base satisfiability is an undecidable problem for Product Logic. We complete here the analysis by showing that knowledge base satisfiability is also an undecidable problem for Lukasiewicz Logic.\n    ",
        "submission_date": "2011-07-21T00:00:00",
        "last_modified_date": "2011-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4570",
        "title": "Consistent Query Answering via ASP from Different Perspectives: Theory and Practice",
        "authors": [
            "Marco Manna",
            "Francesco Ricca",
            "Giorgio Terracina"
        ],
        "abstract": "A data integration system provides transparent access to different data sources by suitably combining their data, and providing the user with a unified view of them, called global schema. However, source data are generally not under the control of the data integration process, thus integrated data may violate global integrity constraints even in presence of locally-consistent data sources. In this scenario, it may be anyway interesting to retrieve as much consistent information as possible. The process of answering user queries under global constraint violations is called consistent query answering (CQA). Several notions of CQA have been proposed, e.g., depending on whether integrated information is assumed to be sound, complete, exact or a variant of them. This paper provides a contribution in this setting: it uniforms solutions coming from different perspectives under a common ASP-based core, and provides query-driven optimizations designed for isolating and eliminating inefficiencies of the general approach for computing consistent answers. Moreover, the paper introduces some new theoretical results enriching existing knowledge on decidability and complexity of the considered problems. The effectiveness of the approach is evidenced by experimental results.\n",
        "submission_date": "2011-07-22T00:00:00",
        "last_modified_date": "2011-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4651",
        "title": "Higher Order Programming to Mine Knowledge for a Modern Medical Expert System",
        "authors": [
            "Nittaya Kerdprasop",
            "Kittisak Kerdprasop"
        ],
        "abstract": "Knowledge mining is the process of deriving new and useful knowledge from vast volumes of data and background knowledge. Modern healthcare organizations regularly generate huge amount of electronic data stored in the databases. These data are a valuable resource for mining useful knowledge to help medical practitioners making appropriate and accurate decision on the diagnosis and treatment of diseases. In this paper, we propose the design of a novel medical expert system based on a logic-programming framework. The proposed system includes a knowledge-mining component as a repertoire of tools for discovering useful knowledge. The implementation of classification and association mining tools based on the higher order and meta-level programming schemes using Prolog has been presented to express the power of logic-based language. Such language also provides a pattern matching facility, which is an essential function for the development of knowledge-intensive tasks. Besides the major goal of medical decision support, the knowledge discovered by our logic-based knowledge-mining component can also be deployed as background knowledge to pre-treatment data from other sources as well as to guard the data repositories against constraint violation. A framework for knowledge deployment is also presented.\n    ",
        "submission_date": "2011-07-23T00:00:00",
        "last_modified_date": "2011-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4967",
        "title": "Normative design using inductive learning",
        "authors": [
            "Domenico Corapi",
            "Alessandra Russo",
            "Marina De Vos",
            "Julian Padget",
            "Ken Satoh"
        ],
        "abstract": "In this paper we propose a use-case-driven iterative design methodology for normative frameworks, also called virtual institutions, which are used to govern open systems. Our computational model represents the normative framework as a logic program under answer set semantics (ASP). By means of an inductive logic programming approach, implemented using ASP, it is possible to synthesise new rules and revise the existing ones. The learning mechanism is guided by the designer who describes the desired properties of the framework through use cases, comprising (i) event traces that capture possible scenarios, and (ii) a state that describes the desired outcome. The learning process then proposes additional rules, or changes to current rules, to satisfy the constraints expressed in the use cases. Thus, the contribution of this paper is a process for the elaboration and revision of a normative framework by means of a semi-automatic and iterative process driven from specifications of (un)desirable behaviour. The process integrates a novel and general methodology for theory revision based on ASP.\n    ",
        "submission_date": "2011-07-25T00:00:00",
        "last_modified_date": "2011-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4969",
        "title": "An end-to-end machine learning system for harmonic analysis of music",
        "authors": [
            "Yizhao Ni",
            "Matt Mcvicar",
            "Raul Santos-Rodriguez",
            "Tijl De Bie"
        ],
        "abstract": "We present a new system for simultaneous estimation of keys, chords, and bass notes from music audio. It makes use of a novel chromagram representation of audio that takes perception of loudness into account. Furthermore, it is fully based on machine learning (instead of expert knowledge), such that it is potentially applicable to a wider range of genres as long as training data is available. As compared to other models, the proposed system is fast and memory efficient, while achieving state-of-the-art performance.\n    ",
        "submission_date": "2011-07-25T00:00:00",
        "last_modified_date": "2011-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4985",
        "title": "Variational Gaussian Process Dynamical Systems",
        "authors": [
            "Andreas C. Damianou",
            "Michalis K. Titsias",
            "Neil D. Lawrence"
        ],
        "abstract": "High dimensional time series are endemic in applications of machine learning such as robotics (sensor data), computational biology (gene expression data), vision (video sequences) and graphics (motion capture data). Practical nonlinear probabilistic approaches to this data are required. In this paper we introduce the variational Gaussian process dynamical system. Our work builds on recent variational approximations for Gaussian process latent variable models to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical prior in the latent space. The approach also allows for the appropriate dimensionality of the latent space to be automatically determined. We demonstrate the model on a human motion capture data set and a series of high resolution video sequences.\n    ",
        "submission_date": "2011-07-25T00:00:00",
        "last_modified_date": "2011-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.5000",
        "title": "An iterative feature selection method for GRNs inference by exploring topological properties",
        "authors": [
            "Fabr\u00edcio Martins Lopes",
            "David C. Martins-Jr",
            "Junior Barrera",
            "Roberto M. Cesar-Jr"
        ],
        "abstract": "An important problem in bioinformatics is the inference of gene regulatory networks (GRN) from temporal expression profiles. In general, the main limitations faced by GRN inference methods is the small number of samples with huge dimensionalities and the noisy nature of the expression measurements. In face of these limitations, alternatives are needed to get better accuracy on the GRNs inference problem. This work addresses this problem by presenting an alternative feature selection method that applies prior knowledge on its search strategy, called SFFS-BA. The proposed search strategy is based on the Sequential Floating Forward Selection (SFFS) algorithm, with the inclusion of a scale-free (Barab\u00e1si-Albert) topology information in order to guide the search process to improve inference. The proposed algorithm explores the scale-free property by pruning the search space and using a power law as a weight for reducing it. In this way, the search space traversed by the SFFS-BA method combines a breadth-first search when the number of combinations is small (<k> <= 2) with a depth-first search when the number of combinations becomes explosive (<k> >= 3), being guided by the scale-free prior information. Experimental results show that the SFFS-BA provides a better inference similarities than SFS and SFFS, keeping the robustness of the SFS and SFFS methods, thus presenting very good results.\n    ",
        "submission_date": "2011-07-25T00:00:00",
        "last_modified_date": "2011-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.5236",
        "title": "Submodular Optimization for Efficient Semi-supervised Support Vector Machines",
        "authors": [
            "Wael Emara",
            "Mehmed Kantardzic"
        ],
        "abstract": "In this work we present a quadratic programming approximation of the Semi-Supervised Support Vector Machine (S3VM) problem, namely approximate QP-S3VM, that can be efficiently solved using off the shelf optimization packages. We prove that this approximate formulation establishes a relation between the low density separation and the graph-based models of semi-supervised learning (SSL) which is important to develop a unifying framework for semi-supervised learning methods. Furthermore, we propose the novel idea of representing SSL problems as submodular set functions and use efficient submodular optimization algorithms to solve them. Using this new idea we develop a representation of the approximate QP-S3VM as a maximization of a submodular set function which makes it possible to optimize using efficient greedy algorithms. We demonstrate that the proposed methods are accurate and provide significant improvement in time complexity over the state of the art in the literature.\n    ",
        "submission_date": "2011-07-26T00:00:00",
        "last_modified_date": "2011-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.5242",
        "title": "ALPprolog --- A New Logic Programming Method for Dynamic Domains",
        "authors": [
            "Conrad Drescher",
            "Michael Thielscher"
        ],
        "abstract": "Logic programming is a powerful paradigm for programming autonomous agents in dynamic domains, as witnessed by languages such as Golog and Flux. In this work we present ALPprolog, an expressive, yet efficient, logic programming language for the online control of agents that have to reason about incomplete information and sensing actions.\n    ",
        "submission_date": "2011-07-26T00:00:00",
        "last_modified_date": "2011-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.5387",
        "title": "Controlling wheelchairs by body motions: A learning framework for the adaptive remapping of space",
        "authors": [
            "Tauseef Gulrez",
            "Alessandro Tognetti",
            "Alon Fishbach",
            "Santiago Acosta",
            "Christopher Scharver",
            "Danilo De Rossi",
            "Ferdinando A. Mussa-Ivaldi"
        ],
        "abstract": "Learning to operate a vehicle is generally accomplished by forming a new cognitive map between the body motions and extrapersonal space. Here, we consider the challenge of remapping movement-to-space representations in survivors of spinal cord injury, for the control of powered wheelchairs. Our goal is to facilitate this remapping by developing interfaces between residual body motions and navigational commands that exploit the degrees of freedom that disabled individuals are most capable to coordinate. We present a new framework for allowing spinal cord injured persons to control powered wheelchairs through signals derived from their residual mobility. The main novelty of this approach lies in substituting the more common joystick controllers of powered wheelchairs with a sensor shirt. This allows the whole upper body of the user to operate as an adaptive joystick. Considerations about learning and risks have lead us to develop a safe testing environment in 3D Virtual Reality. A Personal Augmented Reality Immersive System (PARIS) allows us to analyse learning skills and provide users with an adequate training to control a simulated wheelchair through the signals generated by body motions in a safe environment. We provide a description of the basic theory, of the development phases and of the operation of the complete system. We also present preliminary results illustrating the processing of the data and supporting of the feasibility of this approach.\n    ",
        "submission_date": "2011-07-27T00:00:00",
        "last_modified_date": "2011-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.5742",
        "title": "Complex Optimization in Answer Set Programming",
        "authors": [
            "Martin Gebser",
            "Roland Kaminski",
            "Torsten Schaub"
        ],
        "abstract": "Preference handling and optimization are indispensable means for addressing non-trivial applications in Answer Set Programming (ASP). However, their implementation becomes difficult whenever they bring about a significant increase in computational complexity. As a consequence, existing ASP systems do not offer complex optimization capacities, supporting, for instance, inclusion-based minimization or Pareto efficiency. Rather, such complex criteria are typically addressed by resorting to dedicated modeling techniques, like saturation. Unlike the ease of common ASP modeling, however, these techniques are rather involved and hardly usable by ASP laymen. We address this problem by developing a general implementation technique by means of meta-programming, thus reusing existing ASP systems to capture various forms of qualitative preferences among answer sets. In this way, complex preferences and optimization capacities become readily available for ASP applications.\n    ",
        "submission_date": "2011-07-28T00:00:00",
        "last_modified_date": "2011-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.0476",
        "title": "Specifying and Staging Mixed-Initiative Dialogs with Program Generation and Transformation",
        "authors": [
            "Saverio Perugini"
        ],
        "abstract": "Specifying and implementing flexible human-computer dialogs, such as those used in kiosks and smart phone apps, is challenging because of the numerous and varied directions in which each user might steer a dialog. The objective of this research is to improve dialog specification and implementation. To do so we enriched a notation based on concepts from programming languages, especially partial evaluation, for specifying a variety of unsolicited reporting, mixed-initiative dialogs in a concise representation that serves as a design for dialog implementation. We also built a dialog mining system that extracts a specification in this notation from requirements. To demonstrate that such a specification provides a design for dialog implementation, we built a system that automatically generates an implementation of the dialog, called a stager, from it. These two components constitute a dialog modeling toolkit that automates dialog specification and implementation. These results provide a proof of concept and demonstrate the study of dialog specification and implementation from a programming languages perspective. The ubiquity of dialogs in domains such as travel, education, and health care combined with the demand for smart phone apps provide a landscape for further investigation of these results.\n    ",
        "submission_date": "2011-08-02T00:00:00",
        "last_modified_date": "2015-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.0502",
        "title": "An Efficient Real Time Method of Fingertip Detection",
        "authors": [
            "Jagdish Lal Raheja",
            "Karen Das",
            "Ankit Chaudhary"
        ],
        "abstract": "Fingertips detection has been used in many applications, and it is very popular and commonly used in the area of Human Computer Interaction these days. This paper presents a novel time efficient method that will lead to fingertip detection after cropping the irrelevant parts of input image. Binary silhouette of the input image is generated using HSV color space based skin filter and hand cropping done based on histogram of the hand image. The cropped image will be used to figure out the fingertips.\n    ",
        "submission_date": "2011-08-02T00:00:00",
        "last_modified_date": "2011-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.1170",
        "title": "Convex Optimization without Projection Steps",
        "authors": [
            "Martin Jaggi"
        ],
        "abstract": "For the general problem of minimizing a convex function over a compact convex domain, we will investigate a simple iterative approximation algorithm based on the method by Frank & Wolfe 1956, that does not need projection steps in order to stay inside the optimization domain. Instead of a projection step, the linearized problem defined by a current subgradient is solved, which gives a step direction that will naturally stay in the domain. Our framework generalizes the sparse greedy algorithm of Frank & Wolfe and its primal-dual analysis by Clarkson 2010 (and the low-rank SDP approach by Hazan 2008) to arbitrary convex domains. We give a convergence proof guaranteeing {\\epsilon}-small duality gap after O(1/{\\epsilon}) iterations.\n",
        "submission_date": "2011-08-04T00:00:00",
        "last_modified_date": "2011-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.2054",
        "title": "Uncertain Nearest Neighbor Classification",
        "authors": [
            "Fabrizio Angiulli",
            "Fabio Fassetti"
        ],
        "abstract": "This work deals with the problem of classifying uncertain data. With this aim the Uncertain Nearest Neighbor (UNN) rule is here introduced, which represents the generalization of the deterministic nearest neighbor rule to the case in which uncertain objects are available. The UNN rule relies on the concept of nearest neighbor class, rather than on that of nearest neighbor object. The nearest neighbor class of a test object is the class that maximizes the probability of providing its nearest neighbor. It is provided evidence that the former concept is much more powerful than the latter one in the presence of uncertainty, in that it correctly models the right semantics of the nearest neighbor decision rule when applied to the uncertain scenario. An effective and efficient algorithm to perform uncertain nearest neighbor classification of a generic (un)certain test object is designed, based on properties that greatly reduce the temporal cost associated with nearest neighbor class probability computation. Experimental results are presented, showing that the UNN rule is effective and efficient in classifying uncertain data.\n    ",
        "submission_date": "2011-08-09T00:00:00",
        "last_modified_date": "2011-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.2989",
        "title": "A theory of multiclass boosting",
        "authors": [
            "Indraneel Mukherjee",
            "Robert E. Schapire"
        ],
        "abstract": "Boosting combines weak classifiers to form highly accurate predictors. Although the case of binary classification is well understood, in the multiclass setting, the \"correct\" requirements on the weak classifier, or the notion of the most efficient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classifier, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements.\n    ",
        "submission_date": "2011-08-15T00:00:00",
        "last_modified_date": "2011-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.3235",
        "title": "Comparing System Dynamics and Agent-Based Simulation for Tumour Growth and its Interactions with Effector Cells",
        "authors": [
            "Grazziela P. Figueredo",
            "Uwe Aickelin"
        ],
        "abstract": "There is little research concerning comparisons and combination of System Dynamics Simulation (SDS) and Agent Based Simulation (ABS). ABS is a paradigm used in many levels of abstraction, including those levels covered by SDS. We believe that the establishment of frameworks for the choice between these two simulation approaches would contribute to the simulation research. Hence, our work aims for the establishment of directions for the choice between SDS and ABS approaches for immune system-related problems. Previously, we compared the use of ABS and SDS for modelling agents' behaviour in an environment with nomovement or interactions between these agents. We concluded that for these types of agents it is preferable to use SDS, as it takes up less computational resources and produces the same results as those obtained by the ABS model. In order to move this research forward, our next research question is: if we introduce interactions between these agents will SDS still be the most appropriate paradigm to be used? To answer this question for immune system simulation problems, we will use, as case studies, models involving interactions between tumour cells and immune effector cells. Experiments show that there are cases where SDS and ABS can not be used interchangeably, and therefore, their comparison is not straightforward.\n    ",
        "submission_date": "2011-08-16T00:00:00",
        "last_modified_date": "2011-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.3259",
        "title": "A review and comparison of strategies for multi-step ahead time series forecasting based on the NN5 forecasting competition",
        "authors": [
            "Souhaib Ben Taieb",
            "Gianluca Bontempi",
            "Amir Atiya",
            "Antti Sorjamaa"
        ],
        "abstract": "Multi-step ahead forecasting is still an open challenge in time series forecasting. Several approaches that deal with this complex problem have been proposed in the literature but an extensive comparison on a large number of tasks is still missing. This paper aims to fill this gap by reviewing existing strategies for multi-step ahead forecasting and comparing them in theoretical and practical terms. To attain such an objective, we performed a large scale comparison of these different strategies using a large experimental benchmark (namely the 111 series from the NN5 forecasting competition). In addition, we considered the effects of deseasonalization, input variable selection, and forecast combination on these strategies and on multi-step ahead forecasting at large. The following three findings appear to be consistently supported by the experimental results: Multiple-Output strategies are the best performing approaches, deseasonalization leads to uniformly improved forecast accuracy, and input selection is more effective when performed in conjunction with deseasonalization.\n    ",
        "submission_date": "2011-08-16T00:00:00",
        "last_modified_date": "2011-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.3298",
        "title": "A Machine Learning Perspective on Predictive Coding with PAQ",
        "authors": [
            "Byron Knoll",
            "Nando de Freitas"
        ],
        "abstract": "PAQ8 is an open source lossless data compression algorithm that currently achieves the best compression rates on many benchmarks. This report presents a detailed description of PAQ8 from a statistical machine learning perspective. It shows that it is possible to understand some of the modules of PAQ8 and use this understanding to improve the method. However, intuitive statistical explanations of the behavior of other modules remain elusive. We hope the description in this report will be a starting point for discussions that will increase our understanding, lead to improvements to PAQ8, and facilitate a transfer of knowledge from PAQ8 to other machine learning methods, such a recurrent neural networks and stochastic memoizers. Finally, the report presents a broad range of new applications of PAQ to machine learning tasks including language modeling and adaptive text prediction, adaptive game playing, classification, and compression using features from the field of deep learning.\n    ",
        "submission_date": "2011-08-16T00:00:00",
        "last_modified_date": "2011-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.3372",
        "title": "Overlapping Mixtures of Gaussian Processes for the Data Association Problem",
        "authors": [
            "Miguel L\u00e1zaro-Gredilla",
            "Steven Van Vaerenbergh",
            "Neil Lawrence"
        ],
        "abstract": "In this work we introduce a mixture of GPs to address the data association problem, i.e. to label a group of observations according to the sources that generated them. Unlike several previously proposed GP mixtures, the novel mixture has the distinct characteristic of using no gating function to determine the association of samples and mixture components. Instead, all the GPs in the mixture are global and samples are clustered following \"trajectories\" across input space. We use a non-standard variational Bayesian algorithm to efficiently recover sample labels and learn the hyperparameters. We show how multi-object tracking problems can be disambiguated and also explore the characteristics of the model in traditional regression settings.\n    ",
        "submission_date": "2011-08-16T00:00:00",
        "last_modified_date": "2011-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.3446",
        "title": "Premise Selection for Mathematics by Corpus Analysis and Kernel Methods",
        "authors": [
            "Jesse Alama",
            "Tom Heskes",
            "Daniel K\u00fchlwein",
            "Evgeni Tsivtsivadze",
            "Josef Urban"
        ],
        "abstract": "Smart premise selection is essential when using automated reasoning as a tool for large-theory formal proof development. A good method for premise selection in complex mathematical libraries is the application of machine learning to large corpora of proofs. This work develops learning-based premise selection in two ways. First, a newly available minimal dependency analysis of existing high-level formal mathematical proofs is used to build a large knowledge base of proof dependencies, providing precise data for ATP-based re-verification and for training premise selection algorithms. Second, a new machine learning algorithm for premise selection based on kernel methods is proposed and implemented. To evaluate the impact of both techniques, a benchmark consisting of 2078 large-theory mathematical problems is constructed,extending the older MPTP Challenge benchmark. The combined effect of the techniques results in a 50% improvement on the benchmark over the Vampire/SInE state-of-the-art system for automated reasoning in large theories.\n    ",
        "submission_date": "2011-08-17T00:00:00",
        "last_modified_date": "2012-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.3850",
        "title": "Solving puzzles described in English by automated translation to answer set programming and learning how to do that translation",
        "authors": [
            "Chitta Baral",
            "Juraj Dzifcak"
        ],
        "abstract": "We present a system capable of automatically solving combinatorial logic puzzles given in (simplified) English. It involves translating the English descriptions of the puzzles into answer set programming(ASP) and using ASP solvers to provide solutions of the puzzles. To translate the descriptions, we use a lambda-calculus based approach using Probabilistic Combinatorial Categorial Grammars (PCCG) where the meanings of words are associated with parameters to be able to distinguish between multiple meanings of the same word. Meaning of many words and the parameters are learned. The puzzles are represented in ASP using an ontology which is applicable to a large set of logic puzzles.\n    ",
        "submission_date": "2011-08-18T00:00:00",
        "last_modified_date": "2011-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.4440",
        "title": "Promoting scientific thinking with robots",
        "authors": [
            "Juan Pablo Carbajal",
            "Dorit Assaf",
            "Emanuel Benker"
        ],
        "abstract": "This article describes an exemplary robot exercise which was conducted in a class for mechatronics students. The goal of this exercise was to engage students in scientific thinking and reasoning, activities which do not always play an important role in their curriculum. The robotic platform presented here is simple in its construction and is customizable to the needs of the teacher. Therefore, it can be used for exercises in many different fields of science, not necessarily related to robotics. Here we present a situation where the robot is used like an alien creature from which we want to understand its behavior, resembling an ethological research activity. This robot exercise is suited for a wide range of courses, from general introduction to science, to hardware oriented lectures.\n    ",
        "submission_date": "2011-08-22T00:00:00",
        "last_modified_date": "2011-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.4973",
        "title": "Learning from Complex Systems: On the Roles of Entropy and Fisher Information in Pairwise Isotropic Gaussian Markov Random Fields",
        "authors": [
            "Alexandre L. M. Levada"
        ],
        "abstract": "Markov Random Field models are powerful tools for the study of complex systems. However, little is known about how the interactions between the elements of such systems are encoded, especially from an information-theoretic perspective. In this paper, our goal is to enlight the connection between Fisher information, Shannon entropy, information geometry and the behavior of complex systems modeled by isotropic pairwise Gaussian Markov random fields. We propose analytical expressions to compute local and global versions of these measures using Besag's pseudo-likelihood function, characterizing the system's behavior through its \\emph{Fisher curve}, a parametric trajectory accross the information space that provides a geometric representation for the study of complex systems. Computational experiments show how the proposed tools can be useful in extrating relevant information from complex patterns. The obtained results quantify and support our main conclusion, which is: in terms of information, moving towards higher entropy states (A --> B) is different from moving towards lower entropy states (B --> A), since the \\emph{Fisher curves} are not the same given a natural orientation (the direction of time).\n    ",
        "submission_date": "2011-08-25T00:00:00",
        "last_modified_date": "2013-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.5016",
        "title": "Une analyse bas\u00e9e sur la S-DRT pour la mod\u00e9lisation de dialogues pathologiques",
        "authors": [
            "Maxime Amblard",
            "Musiol Michel",
            "Rebuschi Manuel"
        ],
        "abstract": "In this article, we present a corpus of dialogues between a schizophrenic speaker and an interlocutor who drives the dialogue. We had identified specific discontinuities for paranoid schizophrenics. We propose a modeling of these discontinuities with S-DRT (its pragmatic part)\n    ",
        "submission_date": "2011-08-25T00:00:00",
        "last_modified_date": "2011-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.5017",
        "title": "Event in Compositional Dynamic Semantics",
        "authors": [
            "Sai Qian",
            "Maxime Amblard"
        ],
        "abstract": "We present a framework which constructs an event-style dis- course semantics. The discourse dynamics are encoded in continuation semantics and various rhetorical relations are embedded in the resulting interpretation of the framework. We assume discourse and sentence are distinct semantic objects, that play different roles in meaning evalua- tion. Moreover, two sets of composition functions, for handling different discourse relations, are introduced. The paper first gives the necessary background and motivation for event and dynamic semantics, then the framework with detailed examples will be introduced.\n    ",
        "submission_date": "2011-08-25T00:00:00",
        "last_modified_date": "2011-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.5027",
        "title": "Encoding Phases using Commutativity and Non-commutativity in a Logical Framework",
        "authors": [
            "Maxime Amblard"
        ],
        "abstract": "This article presents an extension of Minimalist Categorial Gram- mars (MCG) to encode Chomsky's phases. These grammars are based on Par- tially Commutative Logic (PCL) and encode properties of Minimalist Grammars (MG) of Stabler. The first implementation of MCG were using both non- commutative properties (to respect the linear word order in an utterance) and commutative ones (to model features of different constituents). Here, we pro- pose to adding Chomsky's phases with the non-commutative tensor product of the logic. Then we could give account of the PIC just by using logical prop- erties of the framework.\n    ",
        "submission_date": "2011-08-25T00:00:00",
        "last_modified_date": "2011-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.5710",
        "title": "Generalized Fast Approximate Energy Minimization via Graph Cuts: Alpha-Expansion Beta-Shrink Moves",
        "authors": [
            "Mark Schmidt",
            "Karteek Alahari"
        ],
        "abstract": "We present alpha-expansion beta-shrink moves, a simple generalization of the widely-used alpha-beta swap and alpha-expansion algorithms for approximate energy minimization. We show that in a certain sense, these moves dominate both alpha-beta-swap and alpha-expansion moves, but unlike previous generalizations the new moves require no additional assumptions and are still solvable in polynomial-time. We show promising experimental results with the new moves, which we believe could be used in any context where alpha-expansions are currently employed.\n    ",
        "submission_date": "2011-08-29T00:00:00",
        "last_modified_date": "2011-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.6223",
        "title": "Towards Configuration of applied Web-based information system",
        "authors": [
            "Mark Sh. Levin"
        ],
        "abstract": "In the paper, combinatorial synthesis of structure for applied Web-based systems is described. The problem is considered as a combination of selected design alternatives for system parts/components into a resultant composite decision (i.e., system configuration design). The solving framework is based on Hierarchical Morphological Multicriteria Design (HMMD) approach: (i) multicriteria selection of alternatives for system parts, (ii) composing the selected alternatives into a resultant combination (while taking into account ordinal quality of the alternatives above and their compatibility). A lattice-based discrete space is used to evaluate (to integrate) quality of the resultant combinations (i.e., composite system decisions or system configurations). In addition, a simplified solving framework based on multicriteria multiple choice problem is considered. A multistage design process to obtain a system trajectory is described as well. The basic applied example is targeted to an applied Web-based system for a communication service provider. Two other applications are briefly described (corporate system and information system for academic application).\n    ",
        "submission_date": "2011-08-31T00:00:00",
        "last_modified_date": "2011-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.6274",
        "title": "Every Formula-Based Logic Program Has a Least Infinite-Valued Model",
        "authors": [
            "Rainer L\u00fcdecke"
        ],
        "abstract": "Every definite logic program has as its meaning a least Herbrand model with respect to the program-independent ordering \"set-inclusion\". In the case of normal logic programs there do not exist least models in general. However, according to a recent approach by Rondogiannis and Wadge, who consider infinite-valued models, every normal logic program does have a least model with respect to a program-independent ordering. We show that this approach can be extended to formula-based logic programs (i.e., finite sets of rules of the form A\\leftarrowF where A is an atom and F an arbitrary first-order formula). We construct for a given program P an interpretation M_P and show that it is the least of all models of P. Keywords: Logic programming, semantics of programs, negation-as-failure, infinite-valued logics, set theory\n    ",
        "submission_date": "2011-08-31T00:00:00",
        "last_modified_date": "2011-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.0333",
        "title": "A KIF Formalization for the IFF Category Theory Ontology",
        "authors": [
            "Robert E. Kent"
        ],
        "abstract": "This paper begins the discussion of how the Information Flow Framework can be used to provide a principled foundation for the metalevel (or structural level) of the Standard Upper Ontology (SUO). This SUO structural level can be used as a logical framework for manipulating collections of ontologies in the object level of the SUO or other middle level or domain ontologies. From the Information Flow perspective, the SUO structural level resolves into several metalevel ontologies. This paper discusses a KIF formalization for one of those metalevel categories, the Category Theory Ontology. In particular, it discusses its category and colimit sub-namespaces.\n    ",
        "submission_date": "2011-09-02T00:00:00",
        "last_modified_date": "2011-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.0616",
        "title": "ATP and Presentation Service for Mizar Formalizations",
        "authors": [
            "Josef Urban",
            "Piotr Rudnicki",
            "Geoff Sutcliffe"
        ],
        "abstract": "This paper describes the Automated Reasoning for Mizar (MizAR) service, which integrates several automated reasoning, artificial intelligence, and presentation tools with Mizar and its authoring environment. The service provides ATP assistance to Mizar authors in finding and explaining proofs, and offers generation of Mizar problems as challenges to ATP systems. The service is based on a sound translation from the Mizar language to that of first-order ATP systems, and relies on the recent progress in application of ATP systems in large theories containing tens of thousands of available facts. We present the main features of MizAR services, followed by an account of initial experiments in finding proofs with the ATP assistance. Our initial experience indicates that the tool offers substantial help in exploring the Mizar library and in preparing new Mizar articles.\n    ",
        "submission_date": "2011-09-03T00:00:00",
        "last_modified_date": "2012-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.0633",
        "title": "Eliciting implicit assumptions of proofs in the MIZAR Mathematical Library by property omission",
        "authors": [
            "Jesse Alama"
        ],
        "abstract": "When formalizing proofs with interactive theorem provers, it often happens that extra background knowledge (declarative or procedural) about mathematical concepts is employed without the formalizer explicitly invoking it, to help the formalizer focus on the relevant details of the proof. In the contexts of producing and studying a formalized mathematical argument, such mechanisms are clearly valuable. But we may not always wish to suppress background knowledge. For certain purposes, it is important to know, as far as possible, precisely what background knowledge was implicitly employed in a formal proof. In this note we describe an experiment conducted on the MIZAR Mathematical Library of formal mathematical proofs to elicit one such class of implicitly employed background knowledge: properties of functions and relations (e.g., commutativity, asymmetry, etc.).\n    ",
        "submission_date": "2011-09-03T00:00:00",
        "last_modified_date": "2011-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.0820",
        "title": "ShareBoost: Efficient Multiclass Learning with Feature Sharing",
        "authors": [
            "Shai Shalev-Shwartz",
            "Yonatan Wexler",
            "Amnon Shashua"
        ],
        "abstract": "Multiclass prediction is the problem of classifying an object into a relevant target class. We consider the problem of learning a multiclass predictor that uses only few features, and in particular, the number of used features should increase sub-linearly with the number of possible classes. This implies that features should be shared by several classes. We describe and analyze the ShareBoost algorithm for learning a multiclass predictor that uses few shared features. We prove that ShareBoost efficiently finds a predictor that uses few shared features (if such a predictor exists) and that it has a small generalization error. We also describe how to use ShareBoost for learning a non-linear predictor that has a fast evaluation time. In a series of experiments with natural data sets we demonstrate the benefits of ShareBoost and evaluate its success relatively to other state-of-the-art approaches.\n    ",
        "submission_date": "2011-09-05T00:00:00",
        "last_modified_date": "2011-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1133",
        "title": "Color Texture Classification Approach Based on Combination of Primitive Pattern Units and Statistical Features",
        "authors": [
            "Shervan Fekri Ershad"
        ],
        "abstract": "Texture classification became one of the problems which has been paid much attention on by image processing scientists since late 80s. Consequently, since now many different methods have been proposed to solve this problem. In most of these methods the researchers attempted to describe and discriminate textures based on linear and non-linear patterns. The linear and non-linear patterns on any window are based on formation of Grain Components in a particular order. Grain component is a primitive unit of morphology that most meaningful information often appears in the form of occurrence of that. The approach which is proposed in this paper could analyze the texture based on its grain components and then by making grain components histogram and extracting statistical features from that would classify the textures. Finally, to increase the accuracy of classification, proposed approach is expanded to color images to utilize the ability of approach in analyzing each RGB channels, individually. Although, this approach is a general one and it could be used in different applications, the method has been tested on the stone texture and the results can prove the quality of approach.\n    ",
        "submission_date": "2011-09-06T00:00:00",
        "last_modified_date": "2011-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1317",
        "title": "Lifted Unit Propagation for Effective Grounding",
        "authors": [
            "Pashootan Vaezipoor",
            "David Mitchell",
            "Maarten Mari\u00ebn"
        ],
        "abstract": "A grounding of a formula $\\phi$ over a given finite domain is a ground formula which is equivalent to $\\phi$ on that domain. Very effective propositional solvers have made grounding-based methods for problem solving increasingly important, however for realistic problem domains and instances, the size of groundings is often problematic. A key technique in ground (e.g., SAT) solvers is unit propagation, which often significantly reduces ground formula size even before search begins. We define a \"lifted\" version of unit propagation which may be carried out prior to grounding, and describe integration of the resulting technique into grounding algorithms. We describe an implementation of the method in a bottom-up grounder, and an experimental study of its performance.\n    ",
        "submission_date": "2011-09-06T00:00:00",
        "last_modified_date": "2011-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1525",
        "title": "Conceptual Knowledge Markup Language: The central core",
        "authors": [
            "Robert E. Kent"
        ],
        "abstract": "The conceptual knowledge framework OML/CKML needs several components for a successful design. One important, but previously overlooked, component is the central core of OML/CKML. The central core provides a theoretical link between the ontological specification in OML and the conceptual knowledge representation in CKML. This paper discusses the formal semantics and syntactic styles of the central core, and also the important role it plays in defining interoperability between OML/CKML, RDF/S and Ontolingua.\n    ",
        "submission_date": "2011-09-07T00:00:00",
        "last_modified_date": "2011-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1841",
        "title": "Digital Libraries, Conceptual Knowledge Systems, and the Nebula Interface",
        "authors": [
            "Robert E. Kent",
            "C. Mic Bowman"
        ],
        "abstract": "Concept Analysis provides a principled approach to effective management of wide area information systems, such as the Nebula File System and Interface. This not only offers evidence to support the assertion that a digital library is a bounded collection of incommensurate information sources in a logical space, but also sheds light on techniques for collaboration through coordinated access to the shared organization of knowledge.\n    ",
        "submission_date": "2011-09-08T00:00:00",
        "last_modified_date": "2011-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.3650",
        "title": "Bi-Objective Community Detection (BOCD) in Networks using Genetic Algorithm",
        "authors": [
            "Rohan Agrawal"
        ],
        "abstract": "A lot of research effort has been put into community detection from all corners of academic interest such as physics, mathematics and computer science. In this paper I have proposed a Bi-Objective Genetic Algorithm for community detection which maximizes modularity and community score. Then the results obtained for both benchmark and real life data sets are compared with other algorithms using the modularity and MNI performance metrics. The results show that the BOCD algorithm is capable of successfully detecting community structure in both real life and synthetic datasets, as well as improving upon the performance of previous techniques.\n    ",
        "submission_date": "2011-09-16T00:00:00",
        "last_modified_date": "2011-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.3940",
        "title": "Learning Discriminative Metrics via Generative Models and Kernel Learning",
        "authors": [
            "Yuan Shi",
            "Yung-Kyun Noh",
            "Fei Sha",
            "Daniel D. Lee"
        ],
        "abstract": "Metrics specifying distances between data points can be learned in a discriminative manner or from generative models. In this paper, we show how to unify generative and discriminative learning of metrics via a kernel learning framework. Specifically, we learn local metrics optimized from parametric generative models. These are then used as base kernels to construct a global kernel that minimizes a discriminative training criterion. We consider both linear and nonlinear combinations of local metric kernels. Our empirical results show that these combinations significantly improve performance on classification tasks. The proposed learning algorithm is also very efficient, achieving order of magnitude speedup in training time compared to previous discriminative baseline methods.\n    ",
        "submission_date": "2011-09-19T00:00:00",
        "last_modified_date": "2011-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.3989",
        "title": "The SeaLion has Landed: An IDE for Answer-Set Programming---Preliminary Report",
        "authors": [
            "Johannes Oetsch",
            "J\u00f6rg P\u00fchrer",
            "Hans Tompits"
        ],
        "abstract": "We report about the current state and designated features of the tool SeaLion, aimed to serve as an integrated development environment (IDE) for answer-set programming (ASP). A main goal of SeaLion is to provide a user-friendly environment for supporting a developer to write, evaluate, debug, and test answer-set programs. To this end, new support techniques have to be developed that suit the requirements of the answer-set semantics and meet the constraints of practical applicability. In this respect, SeaLion benefits from the research results of a project on methods and methodologies for answer-set program development in whose context SeaLion is realised. Currently, the tool provides source-code editors for the languages of Gringo and DLV that offer syntax highlighting, syntax checking, and a visual program outline. Further implemented features are support for external solvers and visualisation as well as visual editing of answer sets. SeaLion comes as a plugin of the popular Eclipse platform and provides itself interfaces for future extensions of the IDE.\n    ",
        "submission_date": "2011-09-19T00:00:00",
        "last_modified_date": "2011-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.4095",
        "title": "Kara: A System for Visualising and Visual Editing of Interpretations for Answer-Set Programs",
        "authors": [
            "Christian Kloim\u00fcllner",
            "Johannes Oetsch",
            "J\u00f6rg P\u00fchrer",
            "Hans Tompits"
        ],
        "abstract": "In answer-set programming (ASP), the solutions of a problem are encoded in dedicated models, called answer sets, of a logical theory. These answer sets are computed from the program that represents the theory by means of an ASP solver and returned to the user as sets of ground first-order literals. As this type of representation is often cumbersome for the user to interpret, tools like ASPVIZ and IDPDraw were developed that allow for visualising answer sets. The tool Kara, introduced in this paper, follows these approaches, using ASP itself as a language for defining visualisations of interpretations. Unlike existing tools that position graphic primitives according to static coordinates only, Kara allows for more high-level specifications, supporting graph structures, grids, and relative positioning of graphical elements. Moreover, generalising the functionality of previous tools, Kara provides modifiable visualisations such that interpretations can be manipulated by graphically editing their visualisations. This is realised by resorting to abductive reasoning techniques. Kara is part of SeaLion, a forthcoming integrated development environment (IDE) for ASP.\n    ",
        "submission_date": "2011-09-19T00:00:00",
        "last_modified_date": "2011-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.4609",
        "title": "Memristive fuzzy edge detector",
        "authors": [
            "Farnood Merrikh-Bayat",
            "Saeed Bagheri Shouraki"
        ],
        "abstract": "Fuzzy inference systems always suffer from the lack of efficient structures or platforms for their hardware implementation. In this paper, we tried to overcome this problem by proposing new method for the implementation of those fuzzy inference systems which use fuzzy rule base to make inference. To achieve this goal, we have designed a multi-layer neuro-fuzzy computing system based on the memristor crossbar structure by introducing some new concepts like fuzzy minterms. Although many applications can be realized through the use of our proposed system, in this study we show how the fuzzy XOR function can be constructed and how it can be used to extract edges from grayscale images. Our memristive fuzzy edge detector (implemented in analog form) compared with other common edge detectors has this advantage that it can extract edges of any given image all at once in real-time.\n    ",
        "submission_date": "2011-09-21T00:00:00",
        "last_modified_date": "2011-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.4979",
        "title": "Latent Semantic Learning with Structured Sparse Representation for Human Action Recognition",
        "authors": [
            "Zhiwu Lu",
            "Yuxin Peng"
        ],
        "abstract": "This paper proposes a novel latent semantic learning method for extracting high-level features (i.e. latent semantics) from a large vocabulary of abundant mid-level features (i.e. visual keywords) with structured sparse representation, which can help to bridge the semantic gap in the challenging task of human action recognition. To discover the manifold structure of midlevel features, we develop a spectral embedding approach to latent semantic learning based on L1-graph, without the need to tune any parameter for graph construction as a key step of manifold learning. More importantly, we construct the L1-graph with structured sparse representation, which can be obtained by structured sparse coding with its structured sparsity ensured by novel L1-norm hypergraph regularization over mid-level features. In the new embedding space, we learn latent semantics automatically from abundant mid-level features through spectral clustering. The learnt latent semantics can be readily used for human action recognition with SVM by defining a histogram intersection kernel. Different from the traditional latent semantic analysis based on topic models, our latent semantic learning method can explore the manifold structure of mid-level features in both L1-graph construction and spectral embedding, which results in compact but discriminative high-level features. The experimental results on the commonly used KTH action dataset and unconstrained YouTube action dataset show the superior performance of our method.\n    ",
        "submission_date": "2011-09-23T00:00:00",
        "last_modified_date": "2011-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.5370",
        "title": "Higher-Order Markov Tag-Topic Models for Tagged Documents and Images",
        "authors": [
            "Jia Zeng",
            "Wei Feng",
            "William K. Cheung",
            "Chun-Hung Li"
        ],
        "abstract": "This paper studies the topic modeling problem of tagged documents and images. Higher-order relations among tagged documents and images are major and ubiquitous characteristics, and play positive roles in extracting reliable and interpretable topics. In this paper, we propose the tag-topic models (TTM) to depict such higher-order topic structural dependencies within the Markov random field (MRF) framework. First, we use the novel factor graph representation of latent Dirichlet allocation (LDA)-based topic models from the MRF perspective, and present an efficient loopy belief propagation (BP) algorithm for approximate inference and parameter estimation. Second, we propose the factor hypergraph representation of TTM, and focus on both pairwise and higher-order relation modeling among tagged documents and images. Efficient loopy BP algorithm is developed to learn TTM, which encourages the topic labeling smoothness among tagged documents and images. Extensive experimental results confirm the incorporation of higher-order relations to be effective in enhancing the overall topic modeling performance, when compared with current state-of-the-art topic models, in many text and image mining tasks of broad interests such as word and link prediction, document classification, and tag recommendation.\n    ",
        "submission_date": "2011-09-25T00:00:00",
        "last_modified_date": "2011-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.5404",
        "title": "Towards Optimal Learning of Chain Graphs",
        "authors": [
            "Jose M. Pe\u00f1a"
        ],
        "abstract": "In this paper, we extend Meek's conjecture (Meek 1997) from directed and acyclic graphs to chain graphs, and prove that the extended conjecture is true. Specifically, we prove that if a chain graph H is an independence map of the independence model induced by another chain graph G, then (i) G can be transformed into H by a sequence of directed and undirected edge additions and feasible splits and mergings, and (ii) after each operation in the sequence H remains an independence map of the independence model induced by G. Our result has the same important consequence for learning chain graphs from data as the proof of Meek's conjecture in (Chickering 2002) had for learning Bayesian networks from data: It makes it possible to develop efficient and asymptotically correct learning algorithms under mild assumptions.\n    ",
        "submission_date": "2011-09-25T00:00:00",
        "last_modified_date": "2011-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6401",
        "title": "An Interpretation of Belief Functions by means of a Probabilistic Multi-modal Logic",
        "authors": [
            "Frederic Dambreville"
        ],
        "abstract": "While belief functions may be seen formally as a generalization of probabilistic distributions, the question of the interactions between belief functions and probability is still an issue in practice. This question is difficult, since the contexts of use of these theory are notably different and the semantics behind these theories are not exactly the same. A prominent issue is increasingly regarded by the community, that is the management of the conflicting information. Recent works have introduced new rules for handling the conflict redistribution while combining belief functions. The notion of conflict, or its cancellation by an hypothesis of open world, seems by itself to prevent a direct interpretation of belief function in a probabilistic framework. This paper addresses the question of a probabilistic interpretation of belief functions. It first introduces and implements a theoretically grounded rule, which is in essence an adaptive conjunctive rule. It is shown, how this rule is derived from a logical interpretation of the belief functions by means of a probabilistic multimodal logic; in addition, a concept of source independence is introduced, based on a principle of entropy maximization.\n    ",
        "submission_date": "2011-09-29T00:00:00",
        "last_modified_date": "2011-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6402",
        "title": "Extension of Boolean algebra by a Bayesian operator; application to the definition of a Deterministic Bayesian Logic",
        "authors": [
            "Frederic Dambreville"
        ],
        "abstract": "This work contributes to the domains of Boolean algebra and of Bayesian probability, by proposing an algebraic extension of Boolean algebras, which implements an operator for the Bayesian conditional inference and is closed under this operator. It is known since the work of Lewis (Lewis' triviality) that it is not possible to construct such conditional operator within the space of events. Nevertheless, this work proposes an answer which complements Lewis' triviality, by the construction of a conditional operator outside the space of events, thus resulting in an algebraic extension. In particular, it is proved that any probability defined on a Boolean algebra may be extended to its algebraic extension in compliance with the multiplicative definition of the conditional probability. In the last part of this paper, a new bivalent logic is introduced on the basis of this algebraic extension, and basic properties are derived.\n    ",
        "submission_date": "2011-09-29T00:00:00",
        "last_modified_date": "2011-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6638",
        "title": "The Statistical Inefficiency of Sparse Coding for Images (or, One Gabor to Rule them All)",
        "authors": [
            "James Bergstra",
            "Aaron Courville",
            "Yoshua Bengio"
        ],
        "abstract": "Sparse coding is a proven principle for learning compact representations of images. However, sparse coding by itself often leads to very redundant dictionaries. With images, this often takes the form of similar edge detectors which are replicated many times at various positions, scales and orientations. An immediate consequence of this observation is that the estimation of the dictionary components is not statistically efficient. We propose a factored model in which factors of variation (e.g. position, scale and orientation) are untangled from the underlying Gabor-like filters. There is so much redundancy in sparse codes for natural images that our model requires only a single dictionary element (a Gabor-like edge detector) to outperform standard sparse coding. Our model scales naturally to arbitrary-sized images while achieving much greater statistical efficiency during learning. We validate this claim with a number of experiments showing, in part, superior compression of out-of-sample data using a sparse coding dictionary learned with only a single image.\n    ",
        "submission_date": "2011-09-29T00:00:00",
        "last_modified_date": "2011-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6838",
        "title": "Distributed Air Traffic Control : A Human Safety Perspective",
        "authors": [
            "Sarvesh Nikumbh",
            "Joeprakash Nathaman",
            "Rahul Vartak"
        ],
        "abstract": "The issues in air traffic control have so far been addressed with the intent to improve resource utilization and achieve an optimized solution with respect to fuel comsumption of aircrafts, efficient usage of the available airspace with minimal congestion related losses under various dynamic constraints. So the focus has almost always been more on smarter management of traffic to increase profits while human safety, though achieved in the process, we believe, has remained less seriously attended. This has become all the more important given that we have overburdened and overstressed air traffic controllers managing hundreds of airports and thousands of aircrafts per day.\n",
        "submission_date": "2011-09-30T00:00:00",
        "last_modified_date": "2011-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0107",
        "title": "Learning to relate images: Mapping units, complex cells and simultaneous eigenspaces",
        "authors": [
            "Roland Memisevic"
        ],
        "abstract": "A fundamental operation in many vision tasks, including motion understanding, stereopsis, visual odometry, or invariant recognition, is establishing correspondences between images or between images and data from other modalities. We present an analysis of the role that multiplicative interactions play in learning such correspondences, and we show how learning and inferring relationships between images can be viewed as detecting rotations in the eigenspaces shared among a set of orthogonal matrices. We review a variety of recent multiplicative sparse coding methods in light of this observation. We also review how the squaring operation performed by energy models and by models of complex cells can be thought of as a way to implement multiplicative interactions. This suggests that the main utility of including complex cells in computational models of vision may be that they can encode relations not invariances.\n    ",
        "submission_date": "2011-10-01T00:00:00",
        "last_modified_date": "2012-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0214",
        "title": "Eclectic Extraction of Propositional Rules from Neural Networks",
        "authors": [
            "Ridwan Al Iqbal"
        ],
        "abstract": "Artificial Neural Network is among the most popular algorithm for supervised learning. However, Neural Networks have a well-known drawback of being a \"Black Box\" learner that is not comprehensible to the Users. This lack of transparency makes it unsuitable for many high risk tasks such as medical diagnosis that requires a rational justification for making a decision. Rule Extraction methods attempt to curb this limitation by extracting comprehensible rules from a trained Network. Many such extraction algorithms have been developed over the years with their respective strengths and weaknesses. They have been broadly categorized into three types based on their approach to use internal model of the Network. Eclectic Methods are hybrid algorithms that combine the other approaches to attain more performance. In this paper, we present an Eclectic method called HERETIC. Our algorithm uses Inductive Decision Tree learning combined with information of the neural network structure for extracting logical rules. Experiments and theoretical analysis show HERETIC to be better in terms of speed and performance.\n    ",
        "submission_date": "2011-10-02T00:00:00",
        "last_modified_date": "2011-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0593",
        "title": "Two Projection Pursuit Algorithms for Machine Learning under Non-Stationarity",
        "authors": [
            "Duncan A. J. Blythe"
        ],
        "abstract": "This thesis derives, tests and applies two linear projection algorithms for machine learning under non-stationarity. The first finds a direction in a linear space upon which a data set is maximally non-stationary. The second aims to robustify two-way classification against non-stationarity. The algorithm is tested on a key application scenario, namely Brain Computer Interfacing.\n    ",
        "submission_date": "2011-10-04T00:00:00",
        "last_modified_date": "2011-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0623",
        "title": "On the Parameterized Complexity of Default Logic and Autoepistemic Logic",
        "authors": [
            "Arne Meier",
            "Johannes Schmidt",
            "Michael Thomas",
            "Heribert Vollmer"
        ],
        "abstract": "We investigate the application of Courcelle's Theorem and the logspace version of Elberfeld etal. in the context of the implication problem for propositional sets of formulae, the extension existence problem for default logic, as well as the expansion existence problem for autoepistemic logic and obtain fixed-parameter time and space efficient algorithms for these problems. On the other hand, we exhibit, for each of the above problems, families of instances of a very simple structure that, for a wide range of different parameterizations, do not have efficient fixed-parameter algorithms (even in the sense of the large class XPnu), unless P=NP.\n    ",
        "submission_date": "2011-10-04T00:00:00",
        "last_modified_date": "2011-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0624",
        "title": "Autonomous Agents Coordination: Action Languages meet CLP(FD) and Linda",
        "authors": [
            "Agostino Dovier",
            "Andrea Formisano",
            "Enrico Pontelli"
        ],
        "abstract": "The paper presents a knowledge representation formalism, in the form of a high-level Action Description Language for multi-agent systems, where autonomous agents reason and act in a shared environment. Agents are autonomously pursuing individual goals, but are capable of interacting through a shared knowledge repository. In their interactions through shared portions of the world, the agents deal with problems of synchronization and concurrency; the action language allows the description of strategies to ensure a consistent global execution of the agents' autonomously derived plans. A distributed planning problem is formalized by providing the declarative specifications of the portion of the problem pertaining a single agent. Each of these specifications is executable by a stand-alone CLP-based planner. The coordination among agents exploits a Linda infrastructure. The proposal is validated in a prototype implementation developed in SICStus Prolog.\n",
        "submission_date": "2011-10-04T00:00:00",
        "last_modified_date": "2011-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0879",
        "title": "Linearized Additive Classifiers",
        "authors": [
            "Subhransu Maji"
        ],
        "abstract": "We revisit the additive model learning literature and adapt a penalized spline formulation due to Eilers and Marx, to train additive classifiers efficiently. We also propose two new embeddings based two classes of orthogonal basis with orthogonal derivatives, which can also be used to efficiently learn additive classifiers. This paper follows the popular theme in the current literature where kernel SVMs are learned much more efficiently using a approximate embedding and linear machine. In this paper we show that spline basis are especially well suited for learning additive models because of their sparsity structure and the ease of computing the embedding which enables one to train these models in an online manner, without incurring the memory overhead of precomputing the storing the embeddings. We show interesting connections between B-Spline basis and histogram intersection kernel and show that for a particular choice of regularization and degree of the B-Splines, our proposed learning algorithm closely approximates the histogram intersection kernel SVM. This enables one to learn additive models with almost no memory overhead compared to fast a linear solver, such as LIBLINEAR, while being only 5-6X slower on average. On two large scale image classification datasets, MNIST and Daimler Chrysler pedestrians, the proposed additive classifiers are as accurate as the kernel SVM, while being two orders of magnitude faster to train.\n    ",
        "submission_date": "2011-10-05T00:00:00",
        "last_modified_date": "2011-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0999",
        "title": "Generalization Strategies for the Verification of Infinite State Systems",
        "authors": [
            "Fabio Fioravanti",
            "Alberto Pettorossi",
            "Maurizio Proietti",
            "Valerio Senni"
        ],
        "abstract": "We present a method for the automated verification of temporal properties of infinite state systems. Our verification method is based on the specialization of constraint logic programs (CLP) and works in two phases: (1) in the first phase, a CLP specification of an infinite state system is specialized with respect to the initial state of the system and the temporal property to be verified, and (2) in the second phase, the specialized program is evaluated by using a bottom-up strategy. The effectiveness of the method strongly depends on the generalization strategy which is applied during the program specialization phase. We consider several generalization strategies obtained by combining techniques already known in the field of program analysis and program transformation, and we also introduce some new strategies. Then, through many verification experiments, we evaluate the effectiveness of the generalization strategies we have considered. Finally, we compare the implementation of our specialization-based verification method to other constraint-based model checking tools. The experimental results show that our method is competitive with the methods used by those other tools. To appear in Theory and Practice of Logic Programming (TPLP).\n    ",
        "submission_date": "2011-10-05T00:00:00",
        "last_modified_date": "2011-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.1228",
        "title": "Order-distance and other metric-like functions on jointly distributed random variables",
        "authors": [
            "Ehtibar N. Dzhafarov",
            "Janne V. Kujala"
        ],
        "abstract": "We construct a class of real-valued nonnegative binary functions on a set of jointly distributed random variables, which satisfy the triangle inequality and vanish at identical arguments (pseudo-quasi-metrics). These functions are useful in dealing with the problem of selective probabilistic causality encountered in behavioral sciences and in quantum physics. The problem reduces to that of ascertaining the existence of a joint distribution for a set of variables with known distributions of certain subsets of this set. Any violation of the triangle inequality or its consequences by one of our functions when applied to such a set rules out the existence of this joint distribution. We focus on an especially versatile and widely applicable pseudo-quasi-metric called an order-distance and its special case called a classification distance.\n    ",
        "submission_date": "2011-10-06T00:00:00",
        "last_modified_date": "2012-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.1259",
        "title": "Characterizing and Improving Generalized Belief Propagation Algorithms on the 2D Edwards-Anderson Model",
        "authors": [
            "E. Dominguez",
            "A. Lage-Castellanos",
            "R. Mulet",
            "F. Ricci-Tersenghi",
            "T. Rizzo"
        ],
        "abstract": "We study the performance of different message passing algorithms in the two dimensional Edwards Anderson model. We show that the standard Belief Propagation (BP) algorithm converges only at high temperature to a paramagnetic solution. Then, we test a Generalized Belief Propagation (GBP) algorithm, derived from a Cluster Variational Method (CVM) at the plaquette level. We compare its performance with BP and with other algorithms derived under the same approximation: Double Loop (DL) and a two-ways message passing algorithm (HAK). The plaquette-CVM approximation improves BP in at least three ways: the quality of the paramagnetic solution at high temperatures, a better estimate (lower) for the critical temperature, and the fact that the GBP message passing algorithm converges also to non paramagnetic solutions. The lack of convergence of the standard GBP message passing algorithm at low temperatures seems to be related to the implementation details and not to the appearance of long range order. In fact, we prove that a gauge invariance of the constrained CVM free energy can be exploited to derive a new message passing algorithm which converges at even lower temperatures. In all its region of convergence this new algorithm is faster than HAK and DL by some orders of magnitude.\n    ",
        "submission_date": "2011-10-06T00:00:00",
        "last_modified_date": "2011-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.1301",
        "title": "Predicting User Actions in Software Processes",
        "authors": [
            "Michael Deynet"
        ],
        "abstract": "This paper describes an approach for user (e.g. SW architect) assisting in software processes. The approach observes the user's action and tries to predict his next step. For this we use approaches in the area of machine learning (sequence learning) and adopt these for the use in software processes.\n",
        "submission_date": "2011-10-06T00:00:00",
        "last_modified_date": "2011-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.1303",
        "title": "Discovering patterns of correlation and similarities in software project data with the Circos visualization tool",
        "authors": [
            "Makrina Viola Kosti",
            "Sofia Lazaridou",
            "Nikoleta Bourazani",
            "Lefteris Angelis"
        ],
        "abstract": "Software cost estimation based on multivariate data from completed projects requires the building of efficient models. These models essentially describe relations in the data, either on the basis of correlations between variables or of similarities between the projects. The continuous growth of the amount of data gathered and the need to perform preliminary analysis in order to discover patterns able to drive the building of reasonable models, leads the researchers towards intelligent and time-saving tools which can effectively describe data and their relationships. The goal of this paper is to suggest an innovative visualization tool, widely used in bioinformatics, which represents relations in data in an aesthetic and intelligent way. In order to illustrate the capabilities of the tool, we use a well known dataset from software engineering projects.\n    ",
        "submission_date": "2011-10-06T00:00:00",
        "last_modified_date": "2011-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.1328",
        "title": "Bayesian Locality Sensitive Hashing for Fast Similarity Search",
        "authors": [
            "Venu Satuluri",
            "Srinivasan Parthasarathy"
        ],
        "abstract": "Given a collection of objects and an associated similarity measure, the all-pairs similarity search problem asks us to find all pairs of objects with similarity greater than a certain user-specified threshold. Locality-sensitive hashing (LSH) based methods have become a very popular approach for this problem. However, most such methods only use LSH for the first phase of similarity search - i.e. efficient indexing for candidate generation. In this paper, we present BayesLSH, a principled Bayesian algorithm for the subsequent phase of similarity search - performing candidate pruning and similarity estimation using LSH. A simpler variant, BayesLSH-Lite, which calculates similarities exactly, is also presented. BayesLSH is able to quickly prune away a large majority of the false positive candidate pairs, leading to significant speedups over baseline approaches. For BayesLSH, we also provide probabilistic guarantees on the quality of the output, both in terms of accuracy and recall. Finally, the quality of BayesLSH's output can be easily tuned and does not require any manual setting of the number of hashes to use for similarity estimation, unlike standard approaches. For two state-of-the-art candidate generation algorithms, AllPairs and LSH, BayesLSH enables significant speedups, typically in the range 2x-20x for a wide variety of datasets.\n    ",
        "submission_date": "2011-10-06T00:00:00",
        "last_modified_date": "2012-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.1391",
        "title": "A Comparison of Different Machine Transliteration Models",
        "authors": [
            "K. Choi",
            "H. Isahara",
            "J. Oh"
        ],
        "abstract": "Machine transliteration is a method for automatically converting words in one language into phonetically equivalent ones in another language. Machine transliteration plays an important role in natural language applications such as information retrieval and machine translation, especially for handling proper nouns and technical terms. Four machine transliteration models -- grapheme-based transliteration model, phoneme-based transliteration model, hybrid transliteration model, and correspondence-based transliteration model -- have been proposed by several researchers. To date, however, there has been little research on a framework in which multiple transliteration models can operate simultaneously. Furthermore, there has been no comparison of the four models within the same framework and using the same data. We addressed these problems by 1) modeling the four models within the same framework, 2) comparing them under the same conditions, and 3) developing a way to improve machine transliteration through this comparison. Our comparison showed that the hybrid and correspondence-based models were the most effective and that the four models can be used in a complementary manner to improve machine transliteration performance.\n    ",
        "submission_date": "2011-10-06T00:00:00",
        "last_modified_date": "2011-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.1394",
        "title": "Learning Sentence-internal Temporal Relations",
        "authors": [
            "M. Lapata",
            "A. Lascarides"
        ],
        "abstract": "In this paper we propose a data intensive approach for inferring sentence-internal temporal relations. Temporal inference is relevant for practical NLP applications which either extract or synthesize temporal information (e.g., summarisation, question answering).  Our method bypasses the need for manual coding by exploiting the presence of markers like after\", which overtly signal a temporal relation. We first show that models trained on main and subordinate clauses connected with a temporal marker achieve good performance on a pseudo-disambiguation task simulating temporal inference (during testing the temporal marker is treated as unseen and the models must select the right marker from a set of possible candidates).  Secondly, we assess whether the proposed approach holds promise for the semi-automatic creation of temporal annotations.  Specifically, we use a model trained on noisy and approximate data (i.e., main and subordinate clauses) to predict intra-sentential relations present in TimeBank, a corpus annotated rich temporal information.  Our experiments compare and contrast several probabilistic models differing in their feature space, linguistic assumptions and data requirements.  We evaluate performance against gold standard corpora and also against human subjects.\n    ",
        "submission_date": "2011-10-06T00:00:00",
        "last_modified_date": "2011-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.1416",
        "title": "The matrices of argumentation frameworks",
        "authors": [
            "Xu Yuming"
        ],
        "abstract": "We introduce matrix and its block to the Dung's theory of argumentation frameworks. It is showed that each argumentation framework has a matrix representation, and the common extension-based semantics of argumentation framework can be characterized by blocks of matrix and their relations. In contrast with traditional method of directed graph, the matrix way has the advantage of computability. Therefore, it has an extensive perspective to bring the theory of matrix into the research of argumentation frameworks and related areas.\n    ",
        "submission_date": "2011-10-07T00:00:00",
        "last_modified_date": "2011-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.1892",
        "title": "Confidence-based Reasoning in Stochastic Constraint Programming",
        "authors": [
            "Roberto Rossi",
            "Brahim Hnich",
            "S. Armagan Tarim",
            "Steven Prestwich"
        ],
        "abstract": "In this work we introduce a novel approach, based on sampling, for finding assignments that are likely to be solutions to stochastic constraint satisfaction problems and constraint optimisation problems. Our approach reduces the size of the original problem being analysed; by solving this reduced problem, with a given confidence probability, we obtain assignments that satisfy the chance constraints in the original model within prescribed error tolerance thresholds. To achieve this, we blend concepts from stochastic constraint programming and statistics. We discuss both exact and approximate variants of our method. The framework we introduce can be immediately employed in concert with existing approaches for solving stochastic constraint programs. A thorough computational study on a number of stochastic combinatorial optimisation problems demonstrates the effectiveness of our approach.\n    ",
        "submission_date": "2011-10-09T00:00:00",
        "last_modified_date": "2015-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.1992",
        "title": "Open Source Software: How Can Design Metrics Facilitate Architecture Recovery?",
        "authors": [
            "Eleni Constantinou",
            "George Kakarontzas",
            "Ioannis Stamelos"
        ],
        "abstract": "Modern software development methodologies include reuse of open source code. Reuse can be facilitated by architectural knowledge of the software, not necessarily provided in the documentation of open source software. The effort required to comprehend the system's source code and discover its architecture can be considered a major drawback in reuse. In a recent study we examined the correlations between design metrics and classes' architecture layer. In this paper, we apply our methodology in more open source projects to verify the applicability of our method. Keywords: system understanding; program comprehension; object-oriented; reuse; architecture layer; design metrics;\n    ",
        "submission_date": "2011-10-10T00:00:00",
        "last_modified_date": "2011-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2211",
        "title": "Learning Symbolic Models of Stochastic Domains",
        "authors": [
            "L. P. Kaelbling",
            "H. M. Pasula",
            "L. S. Zettlemoyer"
        ],
        "abstract": "In this article, we work towards the goal of developing agents that can learn to act in complex worlds.  We develop a probabilistic, relational planning rule representation that compactly models noisy, nondeterministic action effects, and show how such rules can be effectively learned.  Through experiments in simple planning domains and a 3D simulated blocks world with realistic physics, we demonstrate that this learning algorithm allows agents to effectively model world dynamics.\n    ",
        "submission_date": "2011-10-10T00:00:00",
        "last_modified_date": "2011-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2341",
        "title": "Multiple ant-bee colony optimization for load balancing in packet-switched networks",
        "authors": [
            "Mehdi Kashefikia",
            "Nasser Nematbakhsh",
            "Reza Askari Moghadam"
        ],
        "abstract": "One of the important issues in computer networks is \"Load Balancing\" which leads to efficient use of the network resources. To achieve a balanced network it is necessary to find different routes between the source and destination. In the current paper we propose a new approach to find different routes using swarm intelligence techniques and multi colony algorithms. In the proposed algorithm that is an improved version of MACO algorithm, we use different colonies of ants and bees and appoint these colony members as intelligent agents to monitor the network and update the routing information. The survey includes comparison and critiques of MACO. The simulation results show a tangible improvement in the aforementioned approach.\n    ",
        "submission_date": "2011-10-11T00:00:00",
        "last_modified_date": "2011-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2407",
        "title": "Bi-modal G\u00f6del logic over [0,1]-valued Kripke frames",
        "authors": [
            "Xavier Caicedo",
            "Ricardo Oscar Rodriguez"
        ],
        "abstract": "We consider the G\u00f6del bi-modal logic determined by fuzzy Kripke models where both the propositions and the accessibility relation are infinitely valued over the standard G\u00f6del algebra [0,1] and prove strong completeness of Fischer Servi intuitionistic modal logic IK plus the prelinearity axiom with respect to this semantics. We axiomatize also the bi-modal analogues of $T,$ $S4,$ and $S5$ obtained by restricting to models over frames satisfying the [0,1]-valued versions of the structural properties which characterize these logics. As application of the completeness theorems we obtain a representation theorem for bi-modal G\u00f6del algebras.\n    ",
        "submission_date": "2011-10-11T00:00:00",
        "last_modified_date": "2011-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2733",
        "title": "Auctions with Severely Bounded Communication",
        "authors": [
            "L. Blumrosen",
            "N. Nisan",
            "I. Segal"
        ],
        "abstract": "We study auctions with severe bounds on the communication allowed: each bidder  may only transmit t bits of information to the auctioneer. We consider both welfare- and profit-maximizing auctions under this communication restriction. For both measures, we determine the optimal auction and show that the loss incurred relative to unconstrained auctions is mild. We prove non-surprising properties of these kinds of auctions, e.g., that in optimal mechanisms bidders  simply report the interval in which their valuation lies in,  as well as some surprising properties, e.g., that asymmetric auctions  are better than symmetric ones and that multi-round auctions reduce the communication complexity only by a linear factor.\n    ",
        "submission_date": "2011-10-12T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2765",
        "title": "Multi-Issue Negotiation with Deadlines",
        "authors": [
            "S. S. Fatima",
            "N. R. Jennings",
            "M. J. Wooldridge"
        ],
        "abstract": "This paper studies bilateral multi-issue negotiation between self-interested autonomous agents. Now, there are a number of different procedures that can be used for this process; the three main ones being the package deal procedure in which all the issues are bundled and discussed together, the simultaneous procedure in which the issues are discussed simultaneously but independently of each other, and the sequential procedure in which the issues are discussed one after another. Since each of them yields a different outcome, a key problem is to decide which one to use in which circumstances. Specifically, we consider this question for a model in which the agents have time constraints (in the form of both deadlines and discount factors) and information uncertainty (in that the agents do not know the opponents utility function). For this model, we consider issues that are both independent and those that are interdependent and determine equilibria for each case for each procedure. In so doing, we show that the package deal is in fact the optimal procedure for each party. We then go on to show that, although the package deal may be computationally more complex than the other two procedures, it generates Pareto optimal outcomes (unlike the other two), it has similar earliest and latest possible times of agreement to the simultaneous procedure (which is better than the sequential procedure), and that it (like the other two procedures) generates a unique outcome only under certain conditions (which we define).\n    ",
        "submission_date": "2011-10-12T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2767",
        "title": "Resource Allocation Among Agents with MDP-Induced Preferences",
        "authors": [
            "D. A. Dolgov",
            "E. H. Durfee"
        ],
        "abstract": "Allocating scarce resources among agents to maximize global utility is, in general, computationally challenging.  We focus on problems where resources enable agents to execute actions in stochastic environments, modeled as Markov decision processes (MDPs), such that the value of a resource bundle is defined as the expected value of the optimal MDP policy realizable given these resources.  We present an algorithm that simultaneously solves the resource-allocation and the policy-optimization problems.  This allows us to avoid explicitly representing utilities over exponentially many resource bundles, leading to drastic (often exponential) reductions in computational complexity.  We then use this algorithm in the context of self-interested agents to design a combinatorial auction for allocating resources. We empirically demonstrate the effectiveness of our approach by showing that it can, in minutes, optimally solve problems for which a straightforward combinatorial resource-allocation technique would require the agents to enumerate up to 2^100 resource bundles and the auctioneer to solve an NP-complete problem with an input of that size.\n    ",
        "submission_date": "2011-10-12T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.3239",
        "title": "Improving parameter learning of Bayesian nets from incomplete data",
        "authors": [
            "Giorgio Corani",
            "Cassio P. De Campos"
        ],
        "abstract": "This paper addresses the estimation of parameters of a Bayesian network from incomplete data. The task is usually tackled by running the Expectation-Maximization (EM) algorithm several times in order to obtain a high log-likelihood estimate. We argue that choosing the maximum log-likelihood estimate (as well as the maximum penalized log-likelihood and the maximum a posteriori estimate) has severe drawbacks, being affected both by overfitting and model uncertainty. Two ideas are discussed to overcome these issues: a maximum entropy approach and a Bayesian model averaging approach. Both ideas can be easily applied on top of EM, while the entropy idea can be also implemented in a more sophisticated way, through a dedicated non-linear solver. A vast set of experiments shows that these ideas produce significantly better estimates and inferences than the traditional and widely used maximum (penalized) log-likelihood and maximum a posteriori estimates. In particular, if EM is adopted as optimization engine, the model averaging approach is the best performing one; its performance is matched by the entropy approach when implemented using the non-linear solver. The results suggest that the applicability of these ideas is immediate (they are easy to implement and to integrate in currently available inference engines) and that they constitute a better way to learn Bayesian network parameters.\n    ",
        "submission_date": "2011-10-12T00:00:00",
        "last_modified_date": "2011-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.3907",
        "title": "AOSO-LogitBoost: Adaptive One-Vs-One LogitBoost for Multi-Class Problem",
        "authors": [
            "Peng Sun",
            "Mark D. Reid",
            "Jie Zhou"
        ],
        "abstract": "This paper presents an improvement to model learning when using multi-class LogitBoost for classification. Motivated by the statistical view, LogitBoost can be seen as additive tree regression. Two important factors in this setting are: 1) coupled classifier output due to a sum-to-zero constraint, and 2) the dense Hessian matrices that arise when computing tree node split gain and node value fittings. In general, this setting is too complicated for a tractable model learning algorithm. However, too aggressive simplification of the setting may lead to degraded performance. For example, the original LogitBoost is outperformed by ABC-LogitBoost due to the latter's more careful treatment of the above two factors.\n",
        "submission_date": "2011-10-18T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.3961",
        "title": "A Dynamic Framework of Reputation Systems for an Agent Mediated e-market",
        "authors": [
            "Vibha Gaur",
            "Neeraj Kumar Sharma"
        ],
        "abstract": "The success of an agent mediated e-market system lies in the underlying reputation management system to improve the quality of services in an information asymmetric e-market. Reputation provides an operatable metric for establishing trustworthiness between mutually unknown online entities. Reputation systems encourage honest behaviour and discourage malicious behaviour of participating agents in the e-market. A dynamic reputation model would provide virtually instantaneous knowledge about the changing e-market environment and would utilise Internets' capacity for continuous interactivity for reputation computation. This paper proposes a dynamic reputation framework using reinforcement learning and fuzzy set theory that ensures judicious use of information sharing for inter-agent cooperation. This framework is sensitive to the changing parameters of e-market like the value of transaction and the varying experience of agents with the purpose of improving inbuilt defense mechanism of the reputation system against various attacks so that e-market reaches an equilibrium state and dishonest agents are weeded out of the market.\n    ",
        "submission_date": "2011-10-18T00:00:00",
        "last_modified_date": "2011-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.4099",
        "title": "The Complexification of Engineering",
        "authors": [
            "Carlos Eduardo Maldonado",
            "Nelson Alfonso G\u00f3mez-Cruz"
        ],
        "abstract": "This paper deals with the arrow of complexification of engineering. We claim that the complexification of engineering consists in (a) that shift throughout which engineering becomes a science; thus it ceases to be a (mere) praxis or profession; (b) becoming a science, engineering can be considered as one of the sciences of complexity. In reality, the complexification of engineering is the process by which engineering can be studied, achieved and understood in terms of knowledge, and not of goods and services any longer. Complex engineered systems and bio-inspired engineering are so far the two expressions of a complex engineering.\n    ",
        "submission_date": "2011-10-18T00:00:00",
        "last_modified_date": "2011-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.5102",
        "title": "Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models",
        "authors": [
            "Congcong Li",
            "Adarsh Kowdle",
            "Ashutosh Saxena",
            "Tsuhan Chen"
        ],
        "abstract": "Scene understanding includes many related sub-tasks, such as scene categorization, depth estimation, object detection, etc. Each of these sub-tasks is often notoriously hard, and state-of-the-art classifiers already exist for many of them. These classifiers operate on the same raw image and provide correlated outputs. It is desirable to have an algorithm that can capture such correlation without requiring any changes to the inner workings of any classifier.\n",
        "submission_date": "2011-10-24T00:00:00",
        "last_modified_date": "2011-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.6200",
        "title": "TopicViz: Semantic Navigation of Document Collections",
        "authors": [
            "Jacob Eisenstein",
            "Duen Horng \"Polo\" Chau",
            "Aniket Kittur",
            "Eric P. Xing"
        ],
        "abstract": "When people explore and manage information, they think in terms of topics and themes. However, the software that supports information exploration sees text at only the surface level. In this paper we show how topic modeling -- a technique for identifying latent themes across large collections of documents -- can support semantic exploration. We present TopicViz, an interactive environment for information exploration. TopicViz combines traditional search and citation-graph functionality with a range of novel interactive visualizations, centered around a force-directed layout that links documents to the latent themes discovered by the topic model. We describe several use scenarios in which TopicViz supports rapid sensemaking on large document collections.\n    ",
        "submission_date": "2011-10-27T00:00:00",
        "last_modified_date": "2011-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.6384",
        "title": "Backdoors to Acyclic SAT",
        "authors": [
            "Serge Gaspers",
            "Stefan Szeider"
        ],
        "abstract": "Backdoor sets, a notion introduced by Williams et al. in 2003, are certain sets of key variables of a CNF formula F that make it easy to solve the formula; by assigning truth values to the variables in a backdoor set, the formula gets reduced to one or several polynomial-time solvable formulas. More specifically, a weak backdoor set of F is a set X of variables such that there exits a truth assignment t to X that reduces F to a satisfiable formula F[t] that belongs to a polynomial-time decidable base class C. A strong backdoor set is a set X of variables such that for all assignments t to X, the reduced formula F[t] belongs to C.\n",
        "submission_date": "2011-10-28T00:00:00",
        "last_modified_date": "2012-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.6387",
        "title": "Backdoors to Satisfaction",
        "authors": [
            "Serge Gaspers",
            "Stefan Szeider"
        ],
        "abstract": "A backdoor set is a set of variables of a propositional formula such that fixing the truth values of the variables in the backdoor set moves the formula into some polynomial-time decidable class. If we know a small backdoor set we can reduce the question of whether the given formula is satisfiable to the same question for one or several easy formulas that belong to the tractable class under consideration. In this survey we review parameterized complexity results for problems that arise in the context of backdoor sets, such as the problem of finding a backdoor set of size at most k, parameterized by k. We also discuss recent results on backdoor sets for problems that are beyond NP.\n    ",
        "submission_date": "2011-10-28T00:00:00",
        "last_modified_date": "2011-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.6437",
        "title": "Anthropic decision theory",
        "authors": [
            "Stuart Armstrong"
        ],
        "abstract": "This paper sets out to resolve how agents ought to act in the Sleeping Beauty problem and various related anthropic (self-locating belief) problems, not through the calculation of anthropic probabilities, but through finding the correct decision to make. It creates an anthropic decision theory (ADT) that decides these problems from a small set of principles. By doing so, it demonstrates that the attitude of agents with regards to each other (selfish or altruistic) changes the decisions they reach, and that it is very important to take this into account. To illustrate ADT, it is then applied to two major anthropic problems and paradoxes, the Presumptuous Philosopher and Doomsday problems, thus resolving some issues about the probability of human extinction.\n    ",
        "submission_date": "2011-10-28T00:00:00",
        "last_modified_date": "2017-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.6483",
        "title": "Iris Codes Classification Using Discriminant and Witness Directions",
        "authors": [
            "N. Popescu-Bodorin",
            "V. E. Balas",
            "I. M. Motoc"
        ],
        "abstract": "The main topic discussed in this paper is how to use intelligence for biometric decision defuzzification. A neural training model is proposed and tested here as a possible solution for dealing with natural fuzzification that appears between the intra- and inter-class distribution of scores computed during iris recognition tests. It is shown here that the use of proposed neural network support leads to an improvement in the artificial perception of the separation between the intra- and inter-class score distributions by moving them away from each other.\n    ",
        "submission_date": "2011-10-28T00:00:00",
        "last_modified_date": "2011-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0045",
        "title": "Query-time Entity Resolution",
        "authors": [
            "I. Bhattacharya",
            "L. Getoor"
        ],
        "abstract": "Entity resolution is the problem of reconciling database references corresponding to the same real-world entities. Given the abundance of publicly available databases that have unresolved entities, we motivate the problem of query-time entity resolution quick and accurate resolution for answering queries over such unclean databases at query-time.  Since collective entity resolution approaches --- where related references are resolved jointly --- have been shown to be more accurate than independent attribute-based resolution for off-line entity resolution, we focus on developing new algorithms for collective resolution for answering entity resolution queries at query-time.  For this purpose, we first formally show that, for collective resolution, precision and recall for individual entities follow a geometric progression as neighbors at increasing distances are considered. Unfolding this progression leads naturally to a two stage expand and resolve query processing strategy. In this strategy, we first extract the related records for a query using two novel expansion operators, and then resolve the extracted records collectively. We then show how the same strategy can be adapted for query-time entity resolution by identifying and resolving only those database references that are the most helpful for processing the query. We validate our approach on two large real-world publication databases where we show the usefulness of collective resolution and at the same time demonstrate the need for adaptive strategies for query processing. We then show how the same queries can be answered in real-time using our adaptive approach while preserving the gains of collective resolution. In addition to experiments on real datasets, we use synthetically generated data to empirically demonstrate the validity of the performance trends predicted by our analysis of collective entity resolution over a wide range of structural characteristics in the data.\n    ",
        "submission_date": "2011-10-31T00:00:00",
        "last_modified_date": "2011-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0158",
        "title": "Applying Fuzzy ID3 Decision Tree for Software Effort Estimation",
        "authors": [
            "Sanaa Elyassami",
            "Ali Idri"
        ],
        "abstract": "Web Effort Estimation is a process of predicting the efforts and cost in terms of money, schedule and staff for any software project system. Many estimation models have been proposed over the last three decades and it is believed that it is a must for the purpose of: Budgeting, risk analysis, project planning and control, and project improvement investment analysis. In this paper, we investigate the use of Fuzzy ID3 decision tree for software cost estimation; it is designed by integrating the principles of ID3 decision tree and the fuzzy set-theoretic concepts, enabling the model to handle uncertain and imprecise data when describing the software projects, which can improve greatly the accuracy of obtained estimates. MMRE and Pred are used as measures of prediction accuracy for this study. A series of experiments is reported using two different software projects datasets namely, Tukutuku and COCOMO'81 datasets. The results are compared with those produced by the crisp version of the ID3 decision tree.\n    ",
        "submission_date": "2011-11-01T00:00:00",
        "last_modified_date": "2011-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0432",
        "title": "Approximate Stochastic Subgradient Estimation Training for Support Vector Machines",
        "authors": [
            "Sangkyun Lee",
            "Stephen J. Wright"
        ],
        "abstract": "Subgradient algorithms for training support vector machines have been quite successful for solving large-scale and online learning problems. However, they have been restricted to linear kernels and strongly convex formulations. This paper describes efficient subgradient approaches without such limitations. Our approaches make use of randomized low-dimensional approximations to nonlinear kernels, and minimization of a reduced primal formulation using an algorithm based on robust stochastic approximation, which do not require strong convexity. Experiments illustrate that our approaches produce solutions of comparable prediction accuracy with the solutions acquired from existing SVM solvers, but often in much shorter time. We also suggest efficient prediction schemes that depend only on the dimension of kernel approximation, not on the number of support vectors.\n    ",
        "submission_date": "2011-11-02T00:00:00",
        "last_modified_date": "2011-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0466",
        "title": "Kernel diff-hash",
        "authors": [
            "Michael M Bronstein"
        ],
        "abstract": "This paper presents a kernel formulation of the recently introduced diff-hash algorithm for the construction of similarity-sensitive hash functions. Our kernel diff-hash algorithm that shows superior performance on the problem of image feature descriptor matching.\n    ",
        "submission_date": "2011-11-02T00:00:00",
        "last_modified_date": "2011-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0708",
        "title": "Bayesian Causal Induction",
        "authors": [
            "Pedro A. Ortega"
        ],
        "abstract": "Discovering causal relationships is a hard task, often hindered by the need for intervention, and often requiring large amounts of data to resolve statistical uncertainty. However, humans quickly arrive at useful causal relationships. One possible reason is that humans extrapolate from past experience to new, unseen situations: that is, they encode beliefs over causal invariances, allowing for sound generalization from the observations they obtain from directly acting in the world.\n",
        "submission_date": "2011-11-03T00:00:00",
        "last_modified_date": "2011-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0712",
        "title": "Online Learning with Preference Feedback",
        "authors": [
            "Pannagadatta K. Shivaswamy",
            "Thorsten Joachims"
        ],
        "abstract": "We propose a new online learning model for learning with preference feedback. The model is especially suited for applications like web search and recommender systems, where preference data is readily available from implicit user feedback (e.g. clicks). In particular, at each time step a potentially structured object (e.g. a ranking) is presented to the user in response to a context (e.g. query), providing him or her with some unobserved amount of utility. As feedback the algorithm receives an improved object that would have provided higher utility. We propose a learning algorithm with provable regret bounds for this online learning setting and demonstrate its effectiveness on a web-search application. The new learning model also applies to many other interactive learning problems and admits several interesting extensions.\n    ",
        "submission_date": "2011-11-03T00:00:00",
        "last_modified_date": "2011-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.1784",
        "title": "UPAL: Unbiased Pool Based Active Learning",
        "authors": [
            "Ravi Ganti",
            "Alexander Gray"
        ],
        "abstract": "In this paper we address the problem of pool based active learning, and provide an algorithm, called UPAL, that works by minimizing the unbiased estimator of the risk of a hypothesis in a given hypothesis space. For the space of linear classifiers and the squared loss we show that UPAL is equivalent to an exponentially weighted average forecaster. Exploiting some recent results regarding the spectra of random matrices allows us to establish consistency of UPAL when the true hypothesis is a linear hypothesis. Empirical comparison with an active learner implementation in Vowpal Wabbit, and a previously proposed pool based active learner implementation show good empirical performance and better scalability.\n    ",
        "submission_date": "2011-11-08T00:00:00",
        "last_modified_date": "2011-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.2221",
        "title": "Scaling Up Estimation of Distribution Algorithms For Continuous Optimization",
        "authors": [
            "Weishan Dong",
            "Tianshi Chen",
            "Peter Tino",
            "Xin Yao"
        ],
        "abstract": "Since Estimation of Distribution Algorithms (EDA) were proposed, many attempts have been made to improve EDAs' performance in the context of global optimization. So far, the studies or applications of multivariate probabilistic model based continuous EDAs are still restricted to rather low dimensional problems (smaller than 100D). Traditional EDAs have difficulties in solving higher dimensional problems because of the curse of dimensionality and their rapidly increasing computational cost. However, scaling up continuous EDAs for higher dimensional optimization is still necessary, which is supported by the distinctive feature of EDAs: Because a probabilistic model is explicitly estimated, from the learnt model one can discover useful properties or features of the problem. Besides obtaining a good solution, understanding of the problem structure can be of great benefit, especially for black box optimization. We propose a novel EDA framework with Model Complexity Control (EDA-MCC) to scale up EDAs. By using Weakly dependent variable Identification (WI) and Subspace Modeling (SM), EDA-MCC shows significantly better performance than traditional EDAs on high dimensional problems. Moreover, the computational cost and the requirement of large population sizes can be reduced in EDA-MCC. In addition to being able to find a good solution, EDA-MCC can also produce a useful problem structure characterization. EDA-MCC is the first successful instance of multivariate model based EDAs that can be effectively applied a general class of up to 500D problems. It also outperforms some newly developed algorithms designed specifically for large scale optimization. In order to understand the strength and weakness of EDA-MCC, we have carried out extensive computational studies of EDA-MCC. Our results have revealed when EDA-MCC is likely to outperform others on what kind of benchmark functions.\n    ",
        "submission_date": "2011-11-09T00:00:00",
        "last_modified_date": "2011-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.2988",
        "title": "Application of PSO, Artificial Bee Colony and Bacterial Foraging Optimization algorithms to economic load dispatch: An analysis",
        "authors": [
            "Anant Baijal",
            "Vikram Singh Chauhan",
            "T. Jayabarathi"
        ],
        "abstract": "This paper illustrates successful implementation of three evolutionary algorithms, namely- Particle Swarm Optimization(PSO), Artificial Bee Colony (ABC) and Bacterial Foraging Optimization (BFO) algorithms to economic load dispatch problem (ELD). Power output of each generating unit and optimum fuel cost obtained using all three algorithms have been compared. The results obtained show that ABC and BFO algorithms converge to optimal fuel cost with reduced computational time when compared to PSO for the two example problems considered.\n    ",
        "submission_date": "2011-11-13T00:00:00",
        "last_modified_date": "2011-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.3270",
        "title": "Mining Biclusters of Similar Values with Triadic Concept Analysis",
        "authors": [
            "Mehdi Kaytoue",
            "Sergei O. Kuznetsov",
            "Juraj Macko",
            "Wagner Meira",
            "Amedeo Napoli"
        ],
        "abstract": "Biclustering numerical data became a popular data-mining task in the beginning of 2000's, especially for analysing gene expression data. A bicluster reflects a strong association between a subset of objects and a subset of attributes in a numerical object/attribute data-table. So called biclusters of similar values can be thought as maximal sub-tables with close values. Only few methods address a complete, correct and non redundant enumeration of such patterns, which is a well-known intractable problem, while no formal framework exists. In this paper, we introduce important links between biclustering and formal concept analysis. More specifically, we originally show that Triadic Concept Analysis (TCA), provides a nice mathematical framework for biclustering. Interestingly, existing algorithms of TCA, that usually apply on binary data, can be used (directly or with slight modifications) after a preprocessing step for extracting maximal biclusters of similar values.\n    ",
        "submission_date": "2011-11-14T00:00:00",
        "last_modified_date": "2011-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.3735",
        "title": "A Bayesian Model for Plan Recognition in RTS Games applied to StarCraft",
        "authors": [
            "Gabriel Synnaeve",
            "Pierre Bessi\u00e8re"
        ],
        "abstract": "The task of keyhole (unobtrusive) plan recognition is central to adaptive game AI. \"Tech trees\" or \"build trees\" are the core of real-time strategy (RTS) game strategic (long term) planning. This paper presents a generic and simple Bayesian model for RTS build tree prediction from noisy observations, which parameters are learned from replays (game logs). This unsupervised machine learning approach involves minimal work for the game developers as it leverage players' data (com- mon in RTS). We applied it to StarCraft1 and showed that it yields high quality and robust predictions, that can feed an adaptive AI.\n    ",
        "submission_date": "2011-11-16T00:00:00",
        "last_modified_date": "2011-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.4930",
        "title": "Comparative study of Financial Time Series Prediction by Artificial Neural Network with Gradient Descent Learning",
        "authors": [
            "Arka Ghosh"
        ],
        "abstract": "Financial forecasting is an example of a signal processing problem which is challenging due to Small sample sizes, high noise, non-stationarity, and non-linearity,but fast forecasting of stock market price is very important for strategic business ",
        "submission_date": "2011-11-21T00:00:00",
        "last_modified_date": "2012-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.5296",
        "title": "Analytical and Learning-Based Spectrum Sensing Time Optimization in Cognitive Radio Systems",
        "authors": [
            "Hossein Shokri-Ghadikolaei",
            "Younes Abdi",
            "Masoumeh Nasiri-Kenari"
        ],
        "abstract": "Powerful spectrum sensing schemes enable cognitive radios (CRs) to find transmission opportunities in spectral resources allocated exclusively to the primary users. In this paper, maximizing the average throughput of a secondary user by optimizing its spectrum sensing time is formulated assuming that a prior knowledge of the presence and absence probabilities of the primary users is available. The energy consumed for finding a transmission opportunity is evaluated and a discussion on the impact of the number of the primary users on the secondary user throughput and consumed energy is presented. In order to avoid the challenges associated with the analytical method, as a second solution, a systematic neural network-based sensing time optimization approach is also proposed in this paper. The proposed adaptive scheme is able to find the optimum value of the channel sensing time without any prior knowledge or assumption about the wireless environment. The structure, performance, and cooperation of the artificial neural networks used in the proposed method are disclosed in detail and a set of illustrative simulation results is presented to validate the analytical results as well as the performance of the proposed learning-based optimization scheme.\n    ",
        "submission_date": "2011-11-20T00:00:00",
        "last_modified_date": "2011-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.5358",
        "title": "Contextually Guided Semantic Labeling and Search for 3D Point Clouds",
        "authors": [
            "Abhishek Anand",
            "Hema Swetha Koppula",
            "Thorsten Joachims",
            "Ashutosh Saxena"
        ],
        "abstract": "RGB-D cameras, which give an RGB image to- gether with depths, are becoming increasingly popular for robotic perception. In this paper, we address the task of detecting commonly found objects in the 3D point cloud of indoor scenes obtained from such cameras. Our method uses a graphical model that captures various features and contextual relations, including the local visual appearance and shape cues, object co-occurence relationships and geometric relationships. With a large number of object classes and relations, the model's parsimony becomes important and we address that by using multiple types of edge potentials. We train the model using a maximum-margin learning approach. In our experiments over a total of 52 3D scenes of homes and offices (composed from about 550 views), we get a performance of 84.06% and 73.38% in labeling office and home scenes respectively for 17 object classes each. We also present a method for a robot to search for an object using the learned model and the contextual information available from the current labelings of the scene. We applied this algorithm successfully on a mobile robot for the task of finding 12 object classes in 10 different offices and achieved a precision of 97.56% with 78.43% recall.\n    ",
        "submission_date": "2011-11-22T00:00:00",
        "last_modified_date": "2012-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.6387",
        "title": "3D Model Retrieval Based on Semantic and Shape Indexes",
        "authors": [
            "My Abdellah Kassimi",
            "Omar El beqqali"
        ],
        "abstract": "The size of 3D models used on the web or stored in databases is becoming increasingly high. Then, an efficient method that allows users to find similar 3D objects for a given 3D model query has become necessary. Keywords and the geometry of a 3D model cannot meet the needs of users' retrieval because they do not include the semantic information. In this paper, a new method has been proposed to 3D models retrieval using semantic concepts combined with shape indexes. To obtain these concepts, we use the machine learning methods to label 3D models by k-means algorithm in measures and shape indexes space. Moreover, semantic concepts have been organized and represented by ontology language OWL and spatial relationships are used to disambiguate among models of similar appearance. The SPARQL query language has been used to question the information displayed in this language and to compute the similarity between two 3D models. We interpret our results using the Princeton Shape Benchmark Database and the results show the performance of the proposed new approach to retrieval 3D models. Keywords: 3D Model, 3D retrieval, measures, shape indexes, semantic, ontology\n    ",
        "submission_date": "2011-11-28T00:00:00",
        "last_modified_date": "2011-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.6983",
        "title": "Aggregation of Composite Solutions: strategies, models, examples",
        "authors": [
            "Mark Sh. Levin"
        ],
        "abstract": "The paper addresses aggregation issues for composite (modular) solutions. A systemic view point is suggested for various aggregation problems. Several solution structures are considered: sets, set morphologies, trees, etc. Mainly, the aggregation approach is targeted to set morphologies. The aggregation problems are based on basic structures as substructure, superstructure, median/consensus, and extended median/consensus. In the last case, preliminary structure is built (e.g., substructure, median/consensus) and addition of solution elements is considered while taking into account profit of the additional elements and total resource constraint. Four aggregation strategies are examined: (i) extension strategy (designing a substructure of initial solutions as \"system kernel\" and extension of the substructure by additional elements); (ii) compression strategy (designing a superstructure of initial solutions and deletion of some its elements); (iii) combined strategy; and (iv) new design strategy to build a new solution over an extended domain of solution elements. Numerical real-world examples (e.g., telemetry system, communication protocol, student plan, security system, Web-based information system, investment, educational courses) illustrate the suggested aggregation approach.\n    ",
        "submission_date": "2011-11-29T00:00:00",
        "last_modified_date": "2011-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.0698",
        "title": "Machine Learning with Operational Costs",
        "authors": [
            "Theja Tulabandhula",
            "Cynthia Rudin"
        ],
        "abstract": "This work proposes a way to align statistical modeling with decision making. We provide a method that propagates the uncertainty in predictive modeling to the uncertainty in operational cost, where operational cost is the amount spent by the practitioner in solving the problem. The method allows us to explore the range of operational costs associated with the set of reasonable statistical models, so as to provide a useful way for practitioners to understand uncertainty. To do this, the operational cost is cast as a regularization term in a learning algorithm's objective function, allowing either an optimistic or pessimistic view of possible costs, depending on the regularization parameter. From another perspective, if we have prior knowledge about the operational cost, for instance that it should be low, this knowledge can help to restrict the hypothesis space, and can help with generalization. We provide a theoretical generalization bound for this scenario. We also show that learning with operational costs is related to robust optimization.\n    ",
        "submission_date": "2011-12-03T00:00:00",
        "last_modified_date": "2013-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.0791",
        "title": "Strong Equivalence of Qualitative Optimization Problems",
        "authors": [
            "Wolfgang Faber",
            "Miros\u0142aw Truszczy\u0144ski",
            "Stefan Woltran"
        ],
        "abstract": "We introduce the framework of qualitative optimization problems (or, simply, optimization problems) to represent preference theories. The formalism uses separate modules to describe the space of outcomes to be compared (the generator) and the preferences on outcomes (the selector). We consider two types of optimization problems. They differ in the way the generator, which we model by a propositional theory, is interpreted: by the standard propositional logic semantics, and by the equilibrium-model (answer-set) semantics. Under the latter interpretation of generators, optimization problems directly generalize answer-set optimization programs proposed previously. We study strong equivalence of optimization problems, which guarantees their interchangeability within any larger context. We characterize several versions of strong equivalence obtained by restricting the class of optimization problems that can be used as extensions and establish the complexity of associated reasoning tasks. Understanding strong equivalence is essential for modular representation of optimization problems and rewriting techniques to simplify them without changing their inherent properties.\n    ",
        "submission_date": "2011-12-04T00:00:00",
        "last_modified_date": "2011-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.0922",
        "title": "Extending Object-Oriented Languages by Declarative Specifications of Complex Objects using Answer-Set Programming",
        "authors": [
            "Johannes Oetsch",
            "J\u00f6rg P\u00fchrer",
            "Hans Tompits"
        ],
        "abstract": "Many applications require complexly structured data objects. Developing new or adapting existing algorithmic solutions for creating such objects can be a non-trivial and costly task if the considered objects are subject to different application-specific constraints. Often, however, it is comparatively easy to declaratively describe the required objects. In this paper, we propose to use answer-set programming (ASP)---a well-established declarative programming paradigm from the area of artificial intelligence---for instantiating objects in standard object-oriented programming languages. In particular, we extend Java with declarative specifications from which the required objects can be automatically generated using available ASP solver technology.\n    ",
        "submission_date": "2011-12-05T00:00:00",
        "last_modified_date": "2011-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.1217",
        "title": "Entropy Search for Information-Efficient Global Optimization",
        "authors": [
            "Philipp Hennig",
            "Christian J. Schuler"
        ],
        "abstract": "Contemporary global optimization algorithms are based on local measures of utility, rather than a probability measure over location and value of the optimum. They thus attempt to collect low function values, not to learn about the optimum. The reason for the absence of probabilistic global optimizers is that the corresponding inference problem is intractable in several ways. This paper develops desiderata for probabilistic optimization algorithms, then presents a concrete algorithm which addresses each of the computational intractabilities with a sequence of approximations and explicitly adresses the decision problem of maximizing information gain from each evaluation.\n    ",
        "submission_date": "2011-12-06T00:00:00",
        "last_modified_date": "2011-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.1330",
        "title": "Emotional control - conditio sine qua non for advanced artificial intelligences?",
        "authors": [
            "Claudius Gros"
        ],
        "abstract": "Humans dispose of two intertwined information processing pathways, cognitive information processing via neural firing patterns and diffusive volume control via neuromodulation. The cognitive information processing in the brain is traditionally considered to be the prime neural correlate of human intelligence, clinical studies indicate that human emotions intrinsically correlate with the activation of the neuromodulatory system.\n",
        "submission_date": "2011-12-06T00:00:00",
        "last_modified_date": "2011-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.1937",
        "title": "Bootstrapping Intrinsically Motivated Learning with Human Demonstrations",
        "authors": [
            "Sao Mai Nguyen",
            "Adrien Baranes",
            "Pierre-Yves Oudeyer"
        ],
        "abstract": "This paper studies the coupling of internally guided learning and social interaction, and more specifically the improvement owing to demonstrations of the learning by intrinsic motivation. We present Socially Guided Intrinsic Motivation by Demonstration (SGIM-D), an algorithm for learning in continuous, unbounded and non-preset environments. After introducing social learning and intrinsic motivation, we describe the design of our algorithm, before showing through a fishing experiment that SGIM-D efficiently combines the advantages of social learning and intrinsic motivation to gain a wide repertoire while being specialised in specific subspaces.\n    ",
        "submission_date": "2011-12-08T00:00:00",
        "last_modified_date": "2011-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.1996",
        "title": "KL-learning: Online solution of Kullback-Leibler control problems",
        "authors": [
            "Joris Bierkens",
            "Bert Kappen"
        ],
        "abstract": "We introduce a stochastic approximation method for the solution of an ergodic Kullback-Leibler control problem. A Kullback-Leibler control problem is a Markov decision process on a finite state space in which the control cost is proportional to a Kullback-Leibler divergence of the controlled transition probabilities with respect to the uncontrolled transition probabilities. The algorithm discussed in this work allows for a sound theoretical analysis using the ODE method. In a numerical experiment the algorithm is shown to be comparable to the power method and the related Z-learning algorithm in terms of convergence speed. It may be used as the basis of a reinforcement learning style algorithm for Markov decision problems.\n    ",
        "submission_date": "2011-12-09T00:00:00",
        "last_modified_date": "2012-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.2408",
        "title": "Maximum Production of Transmission Messages Rate for Service Discovery Protocols",
        "authors": [
            "Intisar Al-Mejibli",
            "Martin Colley"
        ],
        "abstract": "Minimizing the number of dropped User Datagram Protocol (UDP) messages in a network is regarded as a challenge by researchers. This issue represents serious problems for many protocols particularly those that depend on sending messages as part of their strategy, such us service discovery protocols. This paper proposes and evaluates an algorithm to predict the minimum period of time required between two or more consecutive messages and suggests the minimum queue sizes for the routers, to manage the traffic and minimise the number of dropped messages that has been caused by either congestion or queue overflow or both together. The algorithm has been applied to the Universal Plug and Play (UPnP) protocol using ns2 simulator. It was tested when the routers were connected in two configurations; as a centralized and de centralized. The message length and bandwidth of the links among the routers were taken in the consideration. The result shows Better improvement in number of dropped messages `among the routers.\n    ",
        "submission_date": "2011-12-11T00:00:00",
        "last_modified_date": "2011-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.2410",
        "title": "Networks Utilization Improvements for Service Discovery Performance",
        "authors": [
            "Intisar Al-Mejibli",
            "Martin Colley Salah Al-Majeed"
        ],
        "abstract": "Service discovery requests' messages have a vital role in sharing and locating resources in many of service discovery protocols. Sending more messages than a link can handle may cause congestion and loss of messages which dramatically influences the performance of these protocols. Re-send the lost messages result in latency and inefficiency in performing the tasks which user(s) require from the connected nodes. This issue become a serious problem in two cases: first, when the number of clients which performs a service discovery request is increasing, as this result in increasing in the number of sent discovery messages; second, when the network resources such as bandwidth capacity are consumed by other applications. These two cases lead to network congestion and loss of messages. This paper propose an algorithm to improve the services discovery protocols performance by separating each consecutive burst of messages with a specific period of time which calculated regarding the available network resources. It was tested when the routers were connected in two configurations; decentralised and centralised .In addition, this paper explains the impact of increasing the number of clients and the consumed network resources on the proposed algorithm.\n    ",
        "submission_date": "2011-12-11T00:00:00",
        "last_modified_date": "2011-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.2679",
        "title": "Truncated Power Method for Sparse Eigenvalue Problems",
        "authors": [
            "Xiao-Tong Yuan",
            "Tong Zhang"
        ],
        "abstract": "This paper considers the sparse eigenvalue problem, which is to extract dominant (largest) sparse eigenvectors with at most $k$ non-zero components. We propose a simple yet effective solution called truncated power method that can approximately solve the underlying nonconvex optimization problem. A strong sparse recovery result is proved for the truncated power method, and this theory is our key motivation for developing the new algorithm. The proposed method is tested on applications such as sparse principal component analysis and the densest $k$-subgraph problem. Extensive experiments on several synthetic and real-world large scale datasets demonstrate the competitive empirical performance of our method.\n    ",
        "submission_date": "2011-12-12T00:00:00",
        "last_modified_date": "2011-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.3115",
        "title": "The Diversity Paradox: How Nature Resolves an Evolutionary Dilemma",
        "authors": [
            "James M. Whitacre",
            "Sergei P. Atamas"
        ],
        "abstract": "Adaptation to changing environments is a hallmark of biological systems. Diversity in traits is necessary for adaptation and can influence the survival of a population faced with novelty. In habitats that remain stable over many generations, stabilizing selection reduces trait differences within populations, thereby appearing to remove the diversity needed for heritable adaptive responses in new environments. Paradoxically, field studies have documented numerous populations under long periods of stabilizing selection and evolutionary stasis that have rapidly evolved under changed environmental conditions. In this article, we review how cryptic genetic variation (CGV) resolves this diversity paradox by allowing populations in a stable environment to gradually accumulate hidden genetic diversity that is revealed as trait differences when environments change. Instead of being in conflict, environmental stasis supports CGV accumulation and thus appears to facilitate rapid adaptation in new environments as suggested by recent CGV studies. Similarly, degeneracy has been found to support both genetic and non-genetic adaptation at many levels of biological organization. Degenerate, as opposed to diverse or redundant, ensembles appear functionally redundant in certain environmental contexts but functionally diverse in others. CGV and degeneracy paradigms for adaptation are integrated in this review, revealing a common set of principles that support adaptation at multiple levels of biological organization. Though a discussion of simulation studies, molecular-based experimental systems, principles from population genetics, and field experiments, we demonstrate that CGV and degeneracy reflect complementary top-down and bottom-up, respectively, conceptualizations of the same basic phenomenon and arguably capture a universal feature of biological adaptive processes.\n    ",
        "submission_date": "2011-12-14T00:00:00",
        "last_modified_date": "2011-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.3117",
        "title": "Pervasive Flexibility in Living Technologies through Degeneracy Based Design",
        "authors": [
            "James Whitacre",
            "Axel Bender"
        ],
        "abstract": "The capacity to adapt can greatly influence the success of systems that need to compensate for damaged parts, learn how to achieve robust performance in new environments, or exploit novel opportunities that originate from new technological interfaces or emerging markets. Many of the conditions in which technology is required to adapt cannot be anticipated during its design stage, creating a significant challenge for the designer. Inspired by the study of a range of biological systems, we propose that degeneracy - the realization of multiple, functionally versatile components with contextually overlapping functional redundancy - will support adaptation in technologies because it effects pervasive flexibility, evolutionary innovation, and homeostatic robustness. We provide examples of degeneracy in a number of rudimentary living technologies from military socio-technical systems to swarm robotics and we present design principles - including protocols, loose regulatory coupling, and functional versatility - that allow degeneracy to arise in both biological and man-made systems.\n    ",
        "submission_date": "2011-12-14T00:00:00",
        "last_modified_date": "2011-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.4031",
        "title": "Application of Data Mining Techniques to a Selected Business Organisation with Special Reference to Buying Behaviour",
        "authors": [
            "Tejaswini Hilage",
            "R.V.Kulkarni"
        ],
        "abstract": "Data mining is a new concept & an exploration and analysis of large data sets, in order to discover meaningful patterns and rules. Many organizations are now using the data mining techniques to find out meaningful patterns from the database. The present paper studies how data mining techniques can be apply to the large database. These data mining techniques give certain behavioral pattern from the database. The results which come after analysis of the database are useful for organization. This paper examines the result after applying association rule mining technique, rule induction technique and Apriori algorithm. These techniques are applied to the database of shopping mall. Market basket analysis is performing by the above mentioned techniques and some important results are found such as buying behavior.\n    ",
        "submission_date": "2011-12-17T00:00:00",
        "last_modified_date": "2011-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.4628",
        "title": "Using Artificial Bee Colony Algorithm for MLP Training on Earthquake Time Series Data Prediction",
        "authors": [
            "Habib Shah",
            "Rozaida Ghazali",
            "Nazri Mohd Nawi"
        ],
        "abstract": "Nowadays, computer scientists have shown the interest in the study of social insect's behaviour in neural networks area for solving different combinatorial and statistical problems. Chief among these is the Artificial Bee Colony (ABC) algorithm. This paper investigates the use of ABC algorithm that simulates the intelligent foraging behaviour of a honey bee swarm. Multilayer Perceptron (MLP) trained with the standard back propagation algorithm normally utilises computationally intensive training algorithms. One of the crucial problems with the backpropagation (BP) algorithm is that it can sometimes yield the networks with suboptimal weights because of the presence of many local optima in the solution space. To overcome ABC algorithm used in this work to train MLP learning the complex behaviour of earthquake time series data trained by BP, the performance of MLP-ABC is benchmarked against MLP training with the standard BP. The experimental result shows that MLP-ABC performance is better than MLP-BP for time series data.\n    ",
        "submission_date": "2011-12-20T00:00:00",
        "last_modified_date": "2011-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.5493",
        "title": "Critical Data Compression",
        "authors": [
            "John Scoville"
        ],
        "abstract": "A new approach to data compression is developed and applied to multimedia content. This method separates messages into components suitable for both lossless coding and 'lossy' or statistical coding techniques, compressing complex objects by separately encoding signals and noise. This is demonstrated by compressing the most significant bits of data exactly, since they are typically redundant and compressible, and either fitting a maximally likely noise function to the residual bits or compressing them using lossy methods. Upon decompression, the significant bits are decoded and added to a noise function, whether sampled from a noise model or decompressed from a lossy code. This results in compressed data similar to the original. For many test images, a two-part image code using JPEG2000 for lossy coding and PAQ8l for lossless coding produces less mean-squared error than an equal length of JPEG2000. Computer-generated images typically compress better using this method than through direct lossy coding, as do many black and white photographs and most color photographs at sufficiently high quality levels. Examples applying the method to audio and video coding are also demonstrated. Since two-part codes are efficient for both periodic and chaotic data, concatenations of roughly similar objects may be encoded efficiently, which leads to improved inference. Applications to artificial intelligence are demonstrated, showing that signals using an economical lossless code have a critical level of redundancy which leads to better description-based inference than signals which encode either insufficient data or too much detail.\n    ",
        "submission_date": "2011-12-23T00:00:00",
        "last_modified_date": "2011-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.5505",
        "title": "A Study on Using Uncertain Time Series Matching Algorithms in MapReduce Applications",
        "authors": [
            "Nikzad Babaii Rizvandi",
            "Javid Taheri",
            "Albert Y. Zomaya",
            "Reza Moraveji"
        ],
        "abstract": "In this paper, we study CPU utilization time patterns of several Map-Reduce applications. After extracting running patterns of several applications, the patterns with their statistical information are saved in a reference database to be later used to tweak system parameters to efficiently execute unknown applications in future. To achieve this goal, CPU utilization patterns of new applications along with its statistical information are compared with the already known ones in the reference database to find/predict their most probable execution patterns. Because of different patterns lengths, the Dynamic Time Warping (DTW) is utilized for such comparison; a statistical analysis is then applied to DTWs' outcomes to select the most suitable candidates. Moreover, under a hypothesis, another algorithm is proposed to classify applications under similar CPU utilization patterns. Three widely used text processing applications (WordCount, Distributed Grep, and Terasort) and another application (Exim Mainlog parsing) are used to evaluate our hypothesis in tweaking system parameters in executing similar applications. Results were very promising and showed effectiveness of our approach on 5-node Map-Reduce platform\n    ",
        "submission_date": "2011-12-23T00:00:00",
        "last_modified_date": "2013-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.6219",
        "title": "Document Clustering based on Topic Maps",
        "authors": [
            "Muhammad Rafi",
            "M. Shahid Shaikh",
            "Amir Farooq"
        ],
        "abstract": "Importance of document clustering is now widely acknowledged by researchers for better management, smart navigation, efficient filtering, and concise summarization of large collection of documents like World Wide Web (WWW). The next challenge lies in semantically performing clustering based on the semantic contents of the document. The problem of document clustering has two main components: (1) to represent the document in such a form that inherently captures semantics of the text. This may also help to reduce dimensionality of the document, and (2) to define a similarity measure based on the semantic representation such that it assigns higher numerical values to document pairs which have higher semantic relationship. Feature space of the documents can be very challenging for document clustering. A document may contain multiple topics, it may contain a large set of class-independent general-words, and a handful class-specific core-words. With these features in mind, traditional agglomerative clustering algorithms, which are based on either Document Vector model (DVM) or Suffix Tree model (STC), are less efficient in producing results with high cluster quality. This paper introduces a new approach for document clustering based on the Topic Map representation of the documents. The document is being transformed into a compact form. A similarity measure is proposed based upon the inferred information through topic maps data and structures. The suggested method is implemented using agglomerative hierarchal clustering and tested on standard Information retrieval (IR) datasets. The comparative experiment reveals that the proposed approach is effective in improving the cluster quality.\n    ",
        "submission_date": "2011-12-29T00:00:00",
        "last_modified_date": "2011-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.6222",
        "title": "A comparison of two suffix tree-based document clustering algorithms",
        "authors": [
            "Muhammad Rafi",
            "M. Maujood",
            "M. M. Fazal",
            "S. M. Ali"
        ],
        "abstract": "Document clustering as an unsupervised approach extensively used to navigate, filter, summarize and manage large collection of document repositories like the World Wide Web (WWW). Recently, focuses in this domain shifted from traditional vector based document similarity for clustering to suffix tree based document similarity, as it offers more semantic representation of the text present in the document. In this paper, we compare and contrast two recently introduced approaches to document clustering based on suffix tree data model. The first is an Efficient Phrase based document clustering, which extracts phrases from documents to form compact document representation and uses a similarity measure based on common suffix tree to cluster the documents. The second approach is a frequent word/word meaning sequence based document clustering, it similarly extracts the common word sequence from the document and uses the common sequence/ common word meaning sequence to perform the compact representation, and finally, it uses document clustering approach to cluster the compact documents. These algorithms are using agglomerative hierarchical document clustering to perform the actual clustering step, the difference in these approaches are mainly based on extraction of phrases, model representation as a compact document, and the similarity measures used for clustering. This paper investigates the computational aspect of the two algorithms, and the quality of results they produced.\n    ",
        "submission_date": "2011-12-29T00:00:00",
        "last_modified_date": "2012-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.6371",
        "title": "Multi-q Analysis of Image Patterns",
        "authors": [
            "Ricardo Fabbri",
            "Wesley N. Gon\u00e7alves",
            "Francisco J. P. Lopes",
            "Odemir M. Bruno"
        ],
        "abstract": "This paper studies the use of the Tsallis Entropy versus the classic Boltzmann-Gibbs-Shannon entropy for classifying image patterns. Given a database of 40 pattern classes, the goal is to determine the class of a given image sample. Our experiments show that the Tsallis entropy encoded in a feature vector for different $q$ indices has great advantage over the Boltzmann-Gibbs-Shannon entropy for pattern classification, boosting recognition rates by a factor of 3. We discuss the reasons behind this success, shedding light on the usefulness of the Tsallis entropy.\n    ",
        "submission_date": "2011-12-29T00:00:00",
        "last_modified_date": "2011-12-29T00:00:00"
    }
]