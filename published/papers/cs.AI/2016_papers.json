[
    {
        "url": "https://arxiv.org/abs/1601.00367",
        "title": "Benders Decomposition for the Design of a Hub and Shuttle Public Transit System",
        "authors": [
            "Arthur Maheo",
            "Philip Kilby",
            "Pascal Van Hentenryck"
        ],
        "abstract": "The BusPlus project aims at improving the off-peak hours public transit service in Canberra, Australia. To address the difficulty of covering a large geographic area, BusPlus proposes a hub and shuttle model consisting of a combination of a few high-frequency bus routes between key hubs and a large number of shuttles that bring passengers from their origin to the closest hub and take them from their last bus stop to their destination. This paper focuses on the design of bus network and proposes an efficient solving method to this multimodal network design problem based on the Benders decomposition method. Starting from a MIP formulation of the problem, the paper presents a Benders decomposition approach using dedicated solution techniques for solving independent sub-problems, Pareto optimal cuts, cut bundling, and core point update. Computational results on real-world data from Canberra's public transit system justify the design choices and show that the approach outperforms the MIP formulation by two orders of magnitude. Moreover, the results show that the hub and shuttle model may decrease transit time by a factor of 2, while staying within the costs of the existing transit system.\n    ",
        "submission_date": "2015-12-30T00:00:00",
        "last_modified_date": "2015-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00529",
        "title": "Programming in logic without logic programming",
        "authors": [
            "Robert Kowalski",
            "Fariba Sadri"
        ],
        "abstract": "In previous work, we proposed a logic-based framework in which computation is the execution of actions in an attempt to make reactive rules of the form if antecedent then consequent true in a canonical model of a logic program determined by an initial state, sequence of events, and the resulting sequence of subsequent states. In this model-theoretic semantics, reactive rules are the driving force, and logic programs play only a supporting role.\n",
        "submission_date": "2016-01-04T00:00:00",
        "last_modified_date": "2016-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00626",
        "title": "Scalable Models for Computing Hierarchies in Information Networks",
        "authors": [
            "Baoxu Shi",
            "Tim Weninger"
        ],
        "abstract": "Information hierarchies are organizational structures that often used to organize and present large and complex information as well as provide a mechanism for effective human navigation. Fortunately, many statistical and computational models exist that automatically generate hierarchies; however, the existing approaches do not consider linkages in information {\\em networks} that are increasingly common in real-world scenarios. Current approaches also tend to present topics as an abstract probably distribution over words, etc rather than as tangible nodes from the original network. Furthermore, the statistical techniques present in many previous works are not yet capable of processing data at Web-scale. In this paper we present the Hierarchical Document Topic Model (HDTM), which uses a distributed vertex-programming process to calculate a nonparametric Bayesian generative model. Experiments on three medium size data sets and the entire Wikipedia dataset show that HDTM can infer accurate hierarchies even over large information networks.\n    ",
        "submission_date": "2016-01-04T00:00:00",
        "last_modified_date": "2016-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00669",
        "title": "Artwork creation by a cognitive architecture integrating computational creativity and dual process approaches",
        "authors": [
            "Agnese Augello",
            "Ignazio Infantino",
            "Antonio Lieto",
            "Giovanni Pilato",
            "Riccardo Rizzo",
            "Filippo Vella"
        ],
        "abstract": "The paper proposes a novel cognitive architecture (CA) for computational creativity based on the Psi model and on the mechanisms inspired by dual process theories of reasoning and rationality. In recent years, many cognitive models have focused on dual process theories to better describe and implement complex cognitive skills in artificial agents, but creativity has been approached only at a descriptive level. In previous works we have described various modules of the cognitive architecture that allows a robot to execute creative paintings. By means of dual process theories we refine some relevant mechanisms to obtain artworks, and in particular we explain details about the resolution level of the CA dealing with different strategies of access to the Long Term Memory (LTM) and managing the interaction between S1 and S2 processes of the dual process theory. The creative process involves both divergent and convergent processes in either implicit or explicit manner. This leads to four activities (exploratory, reflective, tacit, and analytic) that, triggered by urges and motivations, generate creative acts. These creative acts exploit both the LTM and the WM in order to make novel substitutions to a perceived image by properly mixing parts of pictures coming from different domains. The paper highlights the role of the interaction between S1 and S2 processes, modulated by the resolution level, which focuses the attention of the creative agent by broadening or narrowing the exploration of novel solutions, or even drawing the solution from a set of already made associations. An example of artificial painter is described in some experimentations by using a robotic platform.\n    ",
        "submission_date": "2016-01-04T00:00:00",
        "last_modified_date": "2016-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00816",
        "title": "Open challenges in understanding development and evolution of speech forms: The roles of embodied self-organization, motivation and active exploration",
        "authors": [
            "Pierre-Yves Oudeyer"
        ],
        "abstract": "This article discusses open scientific challenges for understanding development and evolution of speech forms, as a commentary to Moulin-Frier et al. (Moulin-Frier et al., 2015). Based on the analysis of mathematical models of the origins of speech forms, with a focus on their assumptions , we study the fundamental question of how speech can be formed out of non--speech, at both developmental and evolutionary scales. In particular, we emphasize the importance of embodied self-organization , as well as the role of mechanisms of motivation and active curiosity-driven exploration in speech formation. Finally , we discuss an evolutionary-developmental perspective of the origins of speech.\n    ",
        "submission_date": "2016-01-05T00:00:00",
        "last_modified_date": "2016-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00901",
        "title": "Joint learning of ontology and semantic parser from text",
        "authors": [
            "Janez Starc",
            "Dunja Mladeni\u0107"
        ],
        "abstract": "Semantic parsing methods are used for capturing and representing semantic meaning of text. Meaning representation capturing all the concepts in the text may not always be available or may not be sufficiently complete. Ontologies provide a structured and reasoning-capable way to model the content of a collection of texts. In this work, we present a novel approach to joint learning of ontology and semantic parser from text. The method is based on semi-automatic induction of a context-free grammar from semantically annotated text. The grammar parses the text into semantic trees. Both, the grammar and the semantic trees are used to learn the ontology on several levels -- classes, instances, taxonomic and non-taxonomic relations. The approach was evaluated on the first sentences of Wikipedia pages describing people.\n    ",
        "submission_date": "2016-01-05T00:00:00",
        "last_modified_date": "2016-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01297",
        "title": "Angrier Birds: Bayesian reinforcement learning",
        "authors": [
            "Imanol Arrieta Ibarra",
            "Bernardo Ramos",
            "Lars Roemheld"
        ],
        "abstract": "We train a reinforcement learner to play a simplified version of the game Angry Birds. The learner is provided with a game state in a manner similar to the output that could be produced by computer vision algorithms. We improve on the efficiency of regular {\\epsilon}-greedy Q-Learning with linear function approximation through more systematic exploration in Randomized Least Squares Value Iteration (RLSVI), an algorithm that samples its policy from a posterior distribution on optimal policies. With larger state-action spaces, efficient exploration becomes increasingly important, as evidenced by the faster learning in RLSVI.\n    ",
        "submission_date": "2016-01-06T00:00:00",
        "last_modified_date": "2016-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01492",
        "title": "Complexity of Shift Bribery in Committee Elections",
        "authors": [
            "Robert Bredereck",
            "Piotr Faliszewski",
            "Rolf Niedermeier",
            "Nimrod Talmon"
        ],
        "abstract": "Given an election, a preferred candidate p, and a budget, the SHIFT BRIBERY problem asks whether p can win the election after shifting p higher in some voters' preference orders. Of course, shifting comes at a price (depending on the voter and on the extent of the shift) and one must not exceed the given budget. We study the (parameterized) computational complexity of S HIFT BRIBERY for multiwinner voting rules where winning the election means to be part of some winning committee. We focus on the well-established SNTV, Bloc, k-Borda, and Chamberlin-Courant rules, as well as on approximate variants of the Chamberlin-Courant rule, since the original rule is NP-hard to compute. We show that SHIFT BRIBERY tends to be harder in the multiwinner setting than in the single-winner one by showing settings where SHIFT BRIBERY is easy in the single-winner cases, but is hard (and hard to approximate) in the multiwinner ones. Moreover, we show that the non-monotonicity of those rules which are based on approximation algorithms for the Chamberlin-Courant rule sometimes affects the complexity of SHIFT BRIBERY.\n    ",
        "submission_date": "2016-01-07T00:00:00",
        "last_modified_date": "2018-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01614",
        "title": "Toward Organic Computing Approach for Cybernetic Responsive Environment",
        "authors": [
            "Duhart Cl\u00e9ment",
            "Bertelle Cyrille"
        ],
        "abstract": "The developpment of the Internet of Things (IoT) concept revives Responsive Environments (RE) technologies. Nowadays, the idea of a permanent connection between physical and digital world is technologically possible. The capillar Internet relates to the Internet extension into daily appliances such as they become actors of Internet like any hu-man. The parallel development of Machine-to-Machine communications and Arti cial Intelligence (AI) technics start a new area of cybernetic. This paper presents an approach for Cybernetic Organism (Cyborg) for RE based on Organic Computing (OC). In such approach, each appli-ance is a part of an autonomic system in order to control a physical environment. The underlying idea is that such systems must have self-x properties in order to adapt their behavior to external disturbances with a high-degree of autonomy.\n    ",
        "submission_date": "2016-01-07T00:00:00",
        "last_modified_date": "2016-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01635",
        "title": "Fuzzy Object-Oriented Dynamic Networks. I",
        "authors": [
            "D. A. Terletskyi",
            "A. I. Provotar"
        ],
        "abstract": "The concepts of fuzzy objects and their classes are described that make it possible to structurally represent knowledge about fuzzy and partially-defined objects and their classes. Operations over such objects and classes are also proposed that make it possible to obtain sets and new classes of fuzzy objects and also to model variations in object structures under the influence of external factors.\n    ",
        "submission_date": "2016-01-07T00:00:00",
        "last_modified_date": "2016-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01675",
        "title": "Ensemble Methods of Classification for Power Systems Security Assessment",
        "authors": [
            "Alexei Zhukov",
            "Victor Kurbatsky",
            "Nikita Tomin",
            "Denis Sidorov",
            "Daniil Panasetsky",
            "Aoife Foley"
        ],
        "abstract": "One of the most promising approaches for complex technical systems analysis employs ensemble methods of classification. Ensemble methods enable to build a reliable decision rules for feature space classification in the presence of many possible states of the system. In this paper, novel techniques based on decision trees are used for evaluation of the reliability of the regime of electric power systems. We proposed hybrid approach based on random forests models and boosting models. Such techniques can be applied to predict the interaction of increasing renewable power, storage devices and swiching of smart loads from intelligent domestic appliances, heaters and air-conditioning units and electric vehicles with grid for enhanced decision making. The ensemble classification methods were tested on the modified 118-bus IEEE power system showing that proposed technique can be employed to examine whether the power system is secured under steady-state operating conditions.\n    ",
        "submission_date": "2016-01-07T00:00:00",
        "last_modified_date": "2016-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01920",
        "title": "Towards Semantic Integration of Heterogeneous Sensor Data with Indigenous Knowledge for Drought Forecasting",
        "authors": [
            "Adeyinka K. Akanbi",
            "Muthoni Masinde"
        ],
        "abstract": "In the Internet of Things (IoT) domain, various heterogeneous ubiquitous devices would be able to connect and communicate with each other seamlessly, irrespective of the domain. Semantic representation of data through detailed standardized annotation has shown to improve the integration of the interconnected heterogeneous devices. However, the semantic representation of these heterogeneous data sources for environmental monitoring systems is not yet well supported. To achieve the maximum benefits of IoT for drought forecasting, a dedicated semantic middleware solution is required. This research proposes a middleware that semantically represents and integrates heterogeneous data sources with indigenous knowledge based on a unified ontology for an accurate IoT-based drought early warning system (DEWS).\n    ",
        "submission_date": "2016-01-08T00:00:00",
        "last_modified_date": "2016-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.02433",
        "title": "Git4Voc: Git-based Versioning for Collaborative Vocabulary Development",
        "authors": [
            "Lavdim Halilaj",
            "Irl\u00e1n Grangel-Gonz\u00e1lez",
            "G\u00f6khan Coskun",
            "S\u00f6ren Auer"
        ],
        "abstract": "Collaborative vocabulary development in the context of data integration is the process of finding consensus between the experts of the different systems and domains. The complexity of this process is increased with the number of involved people, the variety of the systems to be integrated and the dynamics of their domain. In this paper we advocate that the realization of a powerful version control system is the heart of the problem. Driven by this idea and the success of Git in the context of software development, we investigate the applicability of Git for collaborative vocabulary development. Even though vocabulary development and software development have much more similarities than differences there are still important differences. These need to be considered within the development of a successful versioning and collaboration system for vocabulary development. Therefore, this paper starts by presenting the challenges we were faced with during the creation of vocabularies collaboratively and discusses its distinction to software development. Based on these insights we propose Git4Voc which comprises guidelines how Git can be adopted to vocabulary development. Finally, we demonstrate how Git hooks can be implemented to go beyond the plain functionality of Git by realizing vocabulary-specific features like syntactic validation and semantic diffs.\n    ",
        "submission_date": "2016-01-11T00:00:00",
        "last_modified_date": "2016-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.02745",
        "title": "Basic Reasoning with Tensor Product Representations",
        "authors": [
            "Paul Smolensky",
            "Moontae Lee",
            "Xiaodong He",
            "Wen-tau Yih",
            "Jianfeng Gao",
            "Li Deng"
        ],
        "abstract": "In this paper we present the initial development of a general theory for mapping inference in predicate logic to computation over Tensor Product Representations (TPRs; Smolensky (1990), Smolensky & Legendre (2006)). After an initial brief synopsis of TPRs (Section 0), we begin with particular examples of inference with TPRs in the 'bAbI' question-answering task of Weston et al. (2015) (Section 1). We then present a simplification of the general analysis that suffices for the bAbI task (Section 2). Finally, we lay out the general treatment of inference over TPRs (Section 3). We also show the simplification in Section 2 derives the inference methods described in Lee et al. (2016); this shows how the simple methods of Lee et al. (2016) can be formally extended to more general reasoning tasks.\n    ",
        "submission_date": "2016-01-12T00:00:00",
        "last_modified_date": "2016-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.02865",
        "title": "Essence' Description",
        "authors": [
            "Peter Nightingale",
            "Andrea Rendl"
        ],
        "abstract": "A description of the Essence' language as used by the tool Savile Row.\n    ",
        "submission_date": "2016-01-12T00:00:00",
        "last_modified_date": "2016-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.03065",
        "title": "An Application of the Generalized Rectangular Fuzzy Model to Critical Thinking Assessment",
        "authors": [
            "Igor Ya. Subbotin",
            "Michael Gr. Voskoglou"
        ],
        "abstract": "The authors apply the Generalized Rectangular Model to assessing critical thinking skills and its relations with their language competency.\n    ",
        "submission_date": "2016-01-08T00:00:00",
        "last_modified_date": "2016-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.03411",
        "title": "Analysis of Algorithms and Partial Algorithms",
        "authors": [
            "Andrew MacFie"
        ],
        "abstract": "We present an alternative methodology for the analysis of algorithms, based on the concept of expected discounted reward. This methodology naturally handles algorithms that do not always terminate, so it can (theoretically) be used with partial algorithms for undecidable problems, such as those found in artificial general intelligence (AGI) and automated theorem proving. We mention an approach to self-improving AGI enabled by this methodology.\n",
        "submission_date": "2016-01-13T00:00:00",
        "last_modified_date": "2017-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.03785",
        "title": "A Method for Image Reduction Based on a Generalization of Ordered Weighted Averaging Functions",
        "authors": [
            "A. Diego S. Farias",
            "Valdigleis S. Costa",
            "Luiz Ranyer A. Lopes",
            "Benjam\u00edn Bedregal",
            "Regivan Santiago"
        ],
        "abstract": "In this paper we propose a special type of aggregation function which generalizes the notion of Ordered Weighted Averaging Function - OWA. The resulting functions are called Dynamic Ordered Weighted Averaging Functions --- DYOWAs. This generalization will be developed in such way that the weight vectors are variables depending on the input vector. Particularly, this operators generalize the aggregation functions: Minimum, Maximum, Arithmetic Mean, Median, etc, which are extensively used in image processing. In this field of research two problems are considered: The determination of methods to reduce images and the construction of techniques which provide noise reduction. The operators described here are able to be used in both cases. In terms of image reduction we apply the methodology provided by Patermain et al. We use the noise reduction operators obtained here to treat the images obtained in the first part of the paper, thus obtaining images with better quality.\n    ",
        "submission_date": "2016-01-15T00:00:00",
        "last_modified_date": "2016-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.04105",
        "title": "Learning the Semantics of Structured Data Sources",
        "authors": [
            "Mohsen Taheriyan",
            "Craig A. Knoblock",
            "Pedro Szekely",
            "Jose Luis Ambite"
        ],
        "abstract": "Information sources such as relational databases, spreadsheets, XML, JSON, and Web APIs contain a tremendous amount of structured data that can be leveraged to build and augment knowledge graphs. However, they rarely provide a semantic model to describe their contents. Semantic models of data sources represent the implicit meaning of the data by specifying the concepts and the relationships within the data. Such models are the key ingredients to automatically publish the data into knowledge graphs. Manually modeling the semantics of data sources requires significant effort and expertise, and although desirable, building these models automatically is a challenging problem. Most of the related work focuses on semantic annotation of the data fields (source attributes). However, constructing a semantic model that explicitly describes the relationships between the attributes in addition to their semantic types is critical.\n",
        "submission_date": "2016-01-16T00:00:00",
        "last_modified_date": "2016-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.04574",
        "title": "SimpleDS: A Simple Deep Reinforcement Learning Dialogue System",
        "authors": [
            "Heriberto Cuay\u00e1huitl"
        ],
        "abstract": "This paper presents 'SimpleDS', a simple and publicly available dialogue system trained with deep reinforcement learning. In contrast to previous reinforcement learning dialogue systems, this system avoids manual feature engineering by performing action selection directly from raw text of the last system and (noisy) user responses. Our initial results, in the restaurant domain, show that it is indeed possible to induce reasonable dialogue behaviour with an approach that aims for high levels of automation in dialogue control for intelligent interactive agents.\n    ",
        "submission_date": "2016-01-18T00:00:00",
        "last_modified_date": "2016-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.04667",
        "title": "Proactive Message Passing on Memory Factor Networks",
        "authors": [
            "Patrick Eschenfeldt",
            "Dan Schmidt",
            "Stark Draper",
            "Jonathan Yedidia"
        ],
        "abstract": "We introduce a new type of graphical model that we call a \"memory factor network\" (MFN). We show how to use MFNs to model the structure inherent in many types of data sets. We also introduce an associated message-passing style algorithm called \"proactive message passing\"' (PMP) that performs inference on MFNs. PMP comes with convergence guarantees and is efficient in comparison to competing algorithms such as variants of belief propagation. We specialize MFNs and PMP to a number of distinct types of data (discrete, continuous, labelled) and inference problems (interpolation, hypothesis testing), provide examples, and discuss approaches for efficient implementation.\n    ",
        "submission_date": "2016-01-18T00:00:00",
        "last_modified_date": "2016-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.05893",
        "title": "GeoTextTagger: High-Precision Location Tagging of Textual Documents using a Natural Language Processing Approach",
        "authors": [
            "Shawn Brunsting",
            "Hans De Sterck",
            "Remco Dolman",
            "Teun van Sprundel"
        ],
        "abstract": "Location tagging, also known as geotagging or geolocation, is the process of assigning geographical coordinates to input data. In this paper we present an algorithm for location tagging of textual documents. Our approach makes use of previous work in natural language processing by using a state-of-the-art part-of-speech tagger and named entity recognizer to find blocks of text which may refer to locations. A knowledge base (OpenStreatMap) is then used to find a list of possible locations for each block. Finally, one location is chosen for each block by assigning distance-based scores to each location and repeatedly selecting the location and block with the best score. We tested our geolocation algorithm with Wikipedia articles about topics with a well-defined geographical location that are geotagged by the articles' authors, where classification approaches have achieved median errors as low as 11 km, with attainable accuracy limited by the class size. Our approach achieved a 10th percentile error of 490 metres and median error of 54 kilometres on the Wikipedia dataset we used. When considering the five location tags with the greatest scores, 50% of articles were assigned at least one tag within 8.5 kilometres of the article's author-assigned true location. We also tested our approach on Twitter messages that are tagged with the location from which the message was sent. Twitter texts are challenging because they are short and unstructured and often do not contain words referring to the location they were sent from, but we obtain potentially useful results. We explain how we use the Spark framework for data analytics to collect and process our test data. In general, classification-based approaches for location tagging may be reaching their upper accuracy limit, but our precision-focused approach has high accuracy for some texts and shows significant potential for improvement overall.\n    ",
        "submission_date": "2016-01-22T00:00:00",
        "last_modified_date": "2016-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.05977",
        "title": "The Singularity Controversy, Part I: Lessons Learned and Open Questions: Conclusions from the Battle on the Legitimacy of the Debate",
        "authors": [
            "Amnon H. Eden"
        ],
        "abstract": "This report seeks to inform policy makers on the nature and the merit of the arguments for and against the concerns associated with a potential technological singularity.\n",
        "submission_date": "2016-01-22T00:00:00",
        "last_modified_date": "2016-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06069",
        "title": "Coalition-based Planning of Military Operations: Adversarial Reasoning Algorithms in an Integrated Decision Aid",
        "authors": [
            "Larry Ground",
            "Alexander Kott",
            "Ray Budd"
        ],
        "abstract": "Use of knowledge-based planning tools can help alleviate the challenges of planning a complex operation by a coalition of diverse parties in an adversarial environment. We explore these challenges and potential contributions of knowledge-based tools using as an example the CADET system, a knowledge-based tool capable of producing automatically (or with human guidance) battle plans with realistic degree of detail and complexity. In ongoing experiments, it compared favorably with human planners. Interleaved planning, scheduling, routing, attrition and consumption processes comprise the computational approach of this tool. From the coalition operations perspective, such tools offer an important aid in rapid synchronization of assets and actions of heterogeneous assets belonging to multiple organizations, potentially with distinct doctrine and rules of engagement. In this paper, we discuss the functionality of the tool, provide a brief overview of the technical approach and experimental results, and outline the potential value of such tools.\n    ",
        "submission_date": "2016-01-22T00:00:00",
        "last_modified_date": "2016-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06108",
        "title": "Decision Aids for Adversarial Planning in Military Operations: Algorithms, Tools, and Turing-test-like Experimental Validation",
        "authors": [
            "Alexander Kott",
            "Ray Budd",
            "Larry Ground",
            "Lakshmi Rebbapragada",
            "John Langston"
        ],
        "abstract": "Use of intelligent decision aids can help alleviate the challenges of planning complex operations. We describe integrated algorithms, and a tool capable of translating a high-level concept for a tactical military operation into a fully detailed, actionable plan, producing automatically (or with human guidance) plans with realistic degree of detail and of human-like quality. Tight interleaving of several algorithms -- planning, adversary estimates, scheduling, routing, attrition and consumption estimates -- comprise the computational approach of this tool. Although originally developed for Army large-unit operations, the technology is generic and also applies to a number of other domains, particularly in critical situations requiring detailed planning within a constrained period of time. In this paper, we focus particularly on the engineering tradeoffs in the design of the tool. In an experimental evaluation, reminiscent of the Turing test, the tool's performance compared favorably with human planners.\n    ",
        "submission_date": "2016-01-22T00:00:00",
        "last_modified_date": "2016-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06180",
        "title": "On the Latent Variable Interpretation in Sum-Product Networks",
        "authors": [
            "Robert Peharz",
            "Robert Gens",
            "Franz Pernkopf",
            "Pedro Domingos"
        ],
        "abstract": "One of the central themes in Sum-Product networks (SPNs) is the interpretation of sum nodes as marginalized latent variables (LVs). This interpretation yields an increased syntactic or semantic structure, allows the application of the EM algorithm and to efficiently perform MPE inference. In literature, the LV interpretation was justified by explicitly introducing the indicator variables corresponding to the LVs' states. However, as pointed out in this paper, this approach is in conflict with the completeness condition in SPNs and does not fully specify the probabilistic model. We propose a remedy for this problem by modifying the original approach for introducing the LVs, which we call SPN augmentation. We discuss conditional independencies in augmented SPNs, formally establish the probabilistic interpretation of the sum-weights and give an interpretation of augmented SPNs as Bayesian networks. Based on these results, we find a sound derivation of the EM algorithm for SPNs. Furthermore, the Viterbi-style algorithm for MPE proposed in literature was never proven to be correct. We show that this is indeed a correct algorithm, when applied to selective SPNs, and in particular when applied to augmented SPNs. Our theoretical results are confirmed in experiments on synthetic data and 103 real-world datasets.\n    ",
        "submission_date": "2016-01-22T00:00:00",
        "last_modified_date": "2016-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06245",
        "title": "Artificial Persuasion in Pedagogical Games",
        "authors": [
            "Zhiwei Zeng"
        ],
        "abstract": "A Persuasive Teachable Agent (PTA) is a special type of Teachable Agent which incorporates a persuasion theory in order to provide persuasive and more personalized feedback to the student. By employing the persuasion techniques, the PTA seeks to maintain the student in a high motivation and high ability state in which he or she has higher cognitive ability and his or her changes in attitudes are more persistent. However, the existing model of the PTA still has a few limitations. Firstly, the existing PTA model focuses on modelling the PTA's ability to persuade, while does not model its ability to be taught by the student and to practice the knowledge it has learnt. Secondly, the quantitative model for computational processes in the PTA has low reusability. Thirdly, there is still a gap between theoretical models and practical implementation of the PTA.\n",
        "submission_date": "2016-01-23T00:00:00",
        "last_modified_date": "2016-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06569",
        "title": "Towards Resolving Unidentifiability in Inverse Reinforcement Learning",
        "authors": [
            "Kareem Amin",
            "Satinder Singh"
        ],
        "abstract": "We consider a setting for Inverse Reinforcement Learning (IRL) where the learner is extended with the ability to actively select multiple environments, observing an agent's behavior on each environment. We first demonstrate that if the learner can experiment with any transition dynamics on some fixed set of states and actions, then there exists an algorithm that reconstructs the agent's reward function to the fullest extent theoretically possible, and that requires only a small (logarithmic) number of experiments. We contrast this result to what is known about IRL in single fixed environments, namely that the true reward function is fundamentally unidentifiable. We then extend this setting to the more realistic case where the learner may not select any transition dynamic, but rather is restricted to some fixed set of environments that it may try. We connect the problem of maximizing the information derived from experiments to submodular function maximization and demonstrate that a greedy algorithm is near optimal (up to logarithmic factors). Finally, we empirically validate our algorithm on an environment inspired by behavioral psychology.\n    ",
        "submission_date": "2016-01-25T00:00:00",
        "last_modified_date": "2016-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06610",
        "title": "Generalizing Prototype Theory: A Formal Quantum Framework",
        "authors": [
            "Diederik Aerts",
            "Jan Broekaert",
            "Liane Gabora",
            "Sandro Sozzo"
        ],
        "abstract": "Theories of natural language and concepts have been unable to model the flexibility, creativity, context-dependence, and emergence, exhibited by words, concepts and their combinations. The mathematical formalism of quantum theory has instead been successful in capturing these phenomena such as graded membership, situational meaning, composition of categories, and also more complex decision making situations, which cannot be modeled in traditional probabilistic approaches. We show how a formal quantum approach to concepts and their combinations can provide a powerful extension of prototype theory. We explain how prototypes can interfere in conceptual combinations as a consequence of their contextual interactions, and provide an illustration of this using an intuitive wave-like diagram. This quantum-conceptual approach gives new life to original prototype theory, without however making it a privileged concept theory, as we explain at the end of our paper.\n    ",
        "submission_date": "2016-01-25T00:00:00",
        "last_modified_date": "2016-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06732",
        "title": "Concept Generation in Language Evolution",
        "authors": [
            "Martha Lewis",
            "Jonathan Lawry"
        ],
        "abstract": "This thesis investigates the generation of new concepts from combinations of existing concepts as a language evolves. We give a method for combining concepts, and will be investigating the utility of composite concepts in language evolution and thence the utility of concept generation.\n    ",
        "submission_date": "2016-01-25T00:00:00",
        "last_modified_date": "2016-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06738",
        "title": "A Label Semantics Approach to Linguistic Hedges",
        "authors": [
            "Martha Lewis",
            "Jonathan Lawry"
        ],
        "abstract": "We introduce a model for the linguistic hedges `very' and `quite' within the label semantics framework, and combined with the prototype and conceptual spaces theories of concepts. The proposed model emerges naturally from the representational framework we use and as such, has a clear semantic grounding. We give generalisations of these hedge models and show that they can be composed with themselves and with other functions, going on to examine their behaviour in the limit of composition.\n    ",
        "submission_date": "2016-01-25T00:00:00",
        "last_modified_date": "2016-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06755",
        "title": "The Utility of Hedged Assertions in the Emergence of Shared Categorical Labels",
        "authors": [
            "Martha Lewis",
            "Jonathan Lawry"
        ],
        "abstract": "We investigate the emergence of shared concepts in a community of language users using a multi-agent simulation. We extend results showing that negated assertions are of use in developing shared categories, to include assertions modified by linguistic hedges. Results show that using hedged assertions positively affects the emergence of shared categories in two distinct ways. Firstly, using contraction hedges like `very' gives better convergence over time. Secondly, using expansion hedges such as `quite' reduces concept overlap. However, both these improvements come at a cost of slower speed of development.\n    ",
        "submission_date": "2016-01-25T00:00:00",
        "last_modified_date": "2016-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06763",
        "title": "Emerging Dimension Weights in a Conceptual Spaces Model of Concept Combination",
        "authors": [
            "Martha Lewis",
            "Jonathan Lawry"
        ],
        "abstract": "We investigate the generation of new concepts from combinations of properties as an artificial language develops. To do so, we have developed a new framework for conjunctive concept combination. This framework gives a semantic grounding to the weighted sum approach to concept combination seen in the literature. We implement the framework in a multi-agent simulation of language evolution and show that shared combination weights emerge. The expected value and the variance of these weights across agents may be predicted from the distribution of elements in the conceptual space, as determined by the underlying environment, together with the rate at which agents adopt others' concepts. When this rate is smaller, the agents are able to converge to weights with lower variance. However, the time taken to converge to a steady state distribution of weights is longer.\n    ",
        "submission_date": "2016-01-25T00:00:00",
        "last_modified_date": "2016-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06862",
        "title": "A Survey on Artificial Intelligence and Data Mining for MOOCs",
        "authors": [
            "Simon Fauvel",
            "Han Yu"
        ],
        "abstract": "Massive Open Online Courses (MOOCs) have gained tremendous popularity in the last few years. Thanks to MOOCs, millions of learners from all over the world have taken thousands of high-quality courses for free. Putting together an excellent MOOC ecosystem is a multidisciplinary endeavour that requires contributions from many different fields. Artificial intelligence (AI) and data mining (DM) are two such fields that have played a significant role in making MOOCs what they are today. By exploiting the vast amount of data generated by learners engaging in MOOCs, DM improves our understanding of the MOOC ecosystem and enables MOOC practitioners to deliver better courses. Similarly, AI, supported by DM, can greatly improve student experience and learning outcomes. In this survey paper, we first review the state-of-the-art artificial intelligence and data mining research applied to MOOCs, emphasising the use of AI and DM tools and techniques to improve student engagement, learning outcomes, and our understanding of the MOOC ecosystem. We then offer an overview of key trends and important research to carry out in the fields of AI and DM so that MOOCs can reach their full potential.\n    ",
        "submission_date": "2016-01-26T00:00:00",
        "last_modified_date": "2016-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06923",
        "title": "Identification and classification of TCM syndrome types among patients with vascular mild cognitive impairment using latent tree analysis",
        "authors": [
            "Chen Fu",
            "Nevin L. Zhang",
            "Bao Xin Chen",
            "Zhou Rong Chen",
            "Xiang Lan Jin",
            "Rong Juan Guo",
            "Zhi Gang Chen",
            "Yun Ling Zhang"
        ],
        "abstract": "Objective: To treat patients with vascular mild cognitive impairment (VMCI) using TCM, it is necessary to classify the patients into TCM syndrome types and to apply different treatments to different types. We investigate how to properly carry out the classification using a novel data-driven method known as latent tree analysis.\n",
        "submission_date": "2016-01-26T00:00:00",
        "last_modified_date": "2016-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.07065",
        "title": "Intelligent Conversational Bot for Massive Online Open Courses (MOOCs)",
        "authors": [
            "Ser Ling Lim",
            "Ong Sing Goh"
        ],
        "abstract": "Massive Online Open Courses (MOOCs) which were introduced in 2008 has since drawn attention around the world for both its advantages as well as criticism on its drawbacks. One of the issues in MOOCs which is the lack of interactivity with the instructor has brought conversational bot into the picture to fill in this gap. In this study, a prototype of MOOCs conversational bot, MOOC-bot is being developed and integrated into MOOCs website to respond to the learner inquiries using text or speech input. MOOC-bot is using the popular Artificial Intelligence Markup Language (AIML) to develop its knowledge base, leverage from AIML capability to deliver appropriate responses and can be quickly adapted to new knowledge domains. The system architecture of MOOC-bot consists of knowledge base along with AIML interpreter, chat interface, MOOCs website and Web Speech API to provide speech recognition and speech synthesis capability. The initial MOOC-bot prototype has the general knowledge from the past Loebner Prize winner - ALICE, frequent asked questions, and a content offered by Universiti Teknikal Malaysia Melaka (UTeM). The evaluation of MOOC-bot based on the past competition questions from Chatterbox Challenge (CBC) and Loebner Prize has shown that it was able to provide correct answers most of the time during the test and demonstrated the capability to prolong the conversation. The advantages of MOOC-bot such as able to provide 24-hour service that can serve different time zones, able to have knowledge in multiple domains, and can be shared by multiple sites simultaneously have outweighed its existing limitations.\n    ",
        "submission_date": "2016-01-26T00:00:00",
        "last_modified_date": "2016-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.07224",
        "title": "Bachelor's thesis on generative probabilistic programming (in Russian language, June 2014)",
        "authors": [
            "Yura N Perov"
        ],
        "abstract": "This Bachelor's thesis, written in Russian, is devoted to a relatively new direction in the field of machine learning and artificial intelligence, namely probabilistic programming. The thesis gives a brief overview to the already existing probabilistic programming languages: Church, Venture, and Anglican. It also describes the results of the first experiments on the automatic induction of probabilistic programs. The thesis was submitted, in June 2014, in partial fulfilment of the requirements for the degree of Bachelor of Science in Mathematics in the Department of Mathematics and Computer Science, Siberian Federal University, Krasnoyarsk, Russia. The work, which is described in this thesis, has been performing in 2012-2014 in the Massachusetts Institute of Technology and in the University of Oxford by the colleagues of the author and by himself.\n    ",
        "submission_date": "2016-01-26T00:00:00",
        "last_modified_date": "2016-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.07409",
        "title": "Multi-Object Reasoning with Constrained Goal Models",
        "authors": [
            "Chi Mai Nguyen",
            "Roberto Sebastiani",
            "Paolo Giorgini",
            "John Mylopoulos"
        ],
        "abstract": "Goal models have been widely used in Computer Science to represent software requirements, business objectives, and design qualities. Existing goal modelling techniques, however, have shown limitations of expressiveness and/or tractability in coping with complex real-world problems. In this work, we exploit advances in automated reasoning technologies, notably Satisfiability and Optimization Modulo Theories (SMT/OMT), and we propose and formalize: (i) an extended modelling language for goals, namely the Constrained Goal Model (CGM), which makes explicit the notion of goal refinement and of domain assumption, allows for expressing preferences between goals and refinements, and allows for associating numerical attributes to goals and refinements for defining constraints and optimization goals over multiple objective functions, refinements and their numerical attributes; (ii) a novel set of automated reasoning functionalities over CGMs, allowing for automatically generating suitable refinements of input CGMs, under user-specified assumptions and constraints, that also maximize preferences and optimize given objective functions. We have implemented these modelling and reasoning functionalities in a tool, named CGM-Tool, using the OMT solver OptiMathSAT as automated reasoning backend. Moreover, we have conducted an experimental evaluation on large CGMs to support the claim that our proposal scales well for goal models with thousands of elements.\n    ",
        "submission_date": "2016-01-27T00:00:00",
        "last_modified_date": "2016-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.07483",
        "title": "Learning and Tuning Meta-heuristics in Plan Space Planning",
        "authors": [
            "Shashank Shekhar",
            "Deepak Khemani"
        ],
        "abstract": "In recent years, the planning community has observed that techniques for learning heuristic functions have yielded improvements in performance. One approach is to use offline learning to learn predictive models from existing heuristics in a domain dependent manner. These learned models are deployed as new heuristic functions. The learned models can in turn be tuned online using a domain independent error correction approach to further enhance their informativeness. The online tuning approach is domain independent but instance specific, and contributes to improved performance for individual instances as planning proceeds. Consequently it is more effective in larger problems.\n",
        "submission_date": "2016-01-27T00:00:00",
        "last_modified_date": "2016-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.07596",
        "title": "Efficient Hill-Climber for Multi-Objective Pseudo-Boolean Optimization",
        "authors": [
            "Francisco Chicano",
            "Darrell Whitley",
            "Renato Tinos"
        ],
        "abstract": "Local search algorithms and iterated local search algorithms are a basic technique. Local search can be a stand along search methods, but it can also be hybridized with evolutionary algorithms. Recently, it has been shown that it is possible to identify improving moves in Hamming neighborhoods for k-bounded pseudo-Boolean optimization problems in constant time. This means that local search does not need to enumerate neighborhoods to find improving moves. It also means that evolutionary algorithms do not need to use random mutation as a operator, except perhaps as a way to escape local optima. In this paper, we show how improving moves can be identified in constant time for multiobjective problems that are expressed as k-bounded pseudo-Boolean functions. In particular, multiobjective forms of NK Landscapes and Mk Landscapes are considered.\n    ",
        "submission_date": "2016-01-27T00:00:00",
        "last_modified_date": "2016-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.07929",
        "title": "Probabilistic Models for Computerized Adaptive Testing: Experiments",
        "authors": [
            "Martin Plajner",
            "Ji\u0159\u00ed Vomlel"
        ],
        "abstract": "This paper follows previous research we have already performed in the area of Bayesian networks models for CAT. We present models using Item Response Theory (IRT - standard CAT method), Bayesian networks, and neural networks. We conducted simulated CAT tests on empirical data. Results of these tests are presented for each model separately and compared.\n    ",
        "submission_date": "2016-01-28T00:00:00",
        "last_modified_date": "2016-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.00165",
        "title": "Using Social Networks to Aid Homeless Shelters: Dynamic Influence Maximization under Uncertainty - An Extended Version",
        "authors": [
            "Amulya Yadav",
            "Hau Chan",
            "Albert Jiang",
            "Haifeng Xu",
            "Eric Rice",
            "Milind Tambe"
        ],
        "abstract": "This paper presents HEALER, a software agent that recommends sequential intervention plans for use by homeless shelters, who organize these interventions to raise awareness about HIV among homeless youth. HEALER's sequential plans (built using knowledge of social networks of homeless youth) choose intervention participants strategically to maximize influence spread, while reasoning about uncertainties in the network. While previous work presents influence maximizing techniques to choose intervention participants, they do not address three real-world issues: (i) they completely fail to scale up to real-world sizes; (ii) they do not handle deviations in execution of intervention plans; (iii) constructing real-world social networks is an expensive process. HEALER handles these issues via four major contributions: (i) HEALER casts this influence maximization problem as a POMDP and solves it using a novel planner which scales up to previously unsolvable real-world sizes; (ii) HEALER allows shelter officials to modify its recommendations, and updates its future plans in a deviation-tolerant manner; (iii) HEALER constructs social networks of homeless youth at low cost, using a Facebook application. Finally, (iv) we show hardness results for the problem that HEALER solves. HEALER will be deployed in the real world in early Spring 2016 and is currently undergoing testing at a homeless shelter.\n    ",
        "submission_date": "2016-01-30T00:00:00",
        "last_modified_date": "2016-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.00198",
        "title": "Discussion on Mechanical Learning and Learning Machine",
        "authors": [
            "Chuyu Xiong"
        ],
        "abstract": "Mechanical learning is a computing system that is based on a set of simple and fixed rules, and can learn from incoming data. A learning machine is a system that realizes mechanical learning. Importantly, we emphasis that it is based on a set of simple and fixed rules, contrasting to often called machine learning that is sophisticated software based on very complicated mathematical theory, and often needs human intervene for software fine tune and manual adjustments. Here, we discuss some basic facts and principles of such system, and try to lay down a framework for further study. We propose 2 directions to approach mechanical learning, just like Church-Turing pair: one is trying to realize a learning machine, another is trying to well describe the mechanical learning.\n    ",
        "submission_date": "2016-01-31T00:00:00",
        "last_modified_date": "2016-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.00269",
        "title": "Numerical Atrribute Extraction from Clinical Texts",
        "authors": [
            "Sarath P R",
            "Sunil Mandhan",
            "Yoshiki Niwa"
        ],
        "abstract": "This paper describes about information extraction system, which is an extension of the system developed by team Hitachi for \"Disease/Disorder Template filling\" task organized by ShARe/CLEF eHealth Evolution Lab 2014. In this extension module we focus on extraction of numerical attributes and values from discharge summary records and associating correct relation between attributes and values. We solve the problem in two steps. First step is extraction of numerical attributes and values, which is developed as a Named Entity Recognition (NER) model using Stanford NLP libraries. Second step is correctly associating the attributes to values, which is developed as a relation extraction module in Apache cTAKES framework. We integrated Stanford NER model as cTAKES pipeline component and used in relation extraction module. Conditional Random Field (CRF) algorithm is used for NER and Support Vector Machines (SVM) for relation extraction. For attribute value relation extraction, we observe 95% accuracy using NER alone and combined accuracy of 87% with NER and SVM.\n    ",
        "submission_date": "2016-01-31T00:00:00",
        "last_modified_date": "2016-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.00515",
        "title": "Marvin: Semantic annotation using multiple knowledge sources",
        "authors": [
            "Nikola Milosevic"
        ],
        "abstract": "People are producing more written material then anytime in the history. The increase is so high that professionals from the various fields are no more able to cope with this amount of publications. Text mining tools can offer tools to help them and one of the tools that can aid information retrieval and information extraction is semantic text annotation. In this report we present Marvin, a text annotator written in Java, which can be used as a command line tool and as a Java library. Marvin is able to annotate text using multiple sources, including WordNet, MetaMap, DBPedia and thesauri represented as SKOS.\n    ",
        "submission_date": "2016-02-01T00:00:00",
        "last_modified_date": "2016-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.00753",
        "title": "Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects",
        "authors": [
            "Hessam Bagherinezhad",
            "Hannaneh Hajishirzi",
            "Yejin Choi",
            "Ali Farhadi"
        ],
        "abstract": "Human vision greatly benefits from the information about sizes of objects. The role of size in several visual reasoning tasks has been thoroughly explored in human perception and cognition. However, the impact of the information about sizes of objects is yet to be determined in AI. We postulate that this is mainly attributed to the lack of a comprehensive repository of size information. In this paper, we introduce a method to automatically infer object sizes, leveraging visual and textual information from web. By maximizing the joint likelihood of textual and visual observations, our method learns reliable relative size estimates, with no explicit human supervision. We introduce the relative size dataset and show that our method outperforms competitive textual and visual baselines in reasoning about size comparisons.\n    ",
        "submission_date": "2016-02-02T00:00:00",
        "last_modified_date": "2016-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.01059",
        "title": "A Comparative Study of Ranking-based Semantics for Abstract Argumentation",
        "authors": [
            "Elise Bonzon",
            "J\u00e9r\u00f4me Delobelle",
            "S\u00e9bastien Konieczny",
            "Nicolas Maudet"
        ],
        "abstract": "Argumentation is a process of evaluating and comparing a set of arguments. A way to compare them consists in using a ranking-based semantics which rank-order arguments from the most to the least acceptable ones. Recently, a number of such semantics have been proposed independently, often associated with some desirable properties. However, there is no comparative study which takes a broader perspective. This is what we propose in this work. We provide a general comparison of all these semantics with respect to the proposed properties. That allows to underline the differences of behavior between the existing semantics.\n    ",
        "submission_date": "2016-02-02T00:00:00",
        "last_modified_date": "2016-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.01208",
        "title": "Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences",
        "authors": [
            "Akira Taniguchi",
            "Tadahiro Taniguchi",
            "Tetsunari Inamura"
        ],
        "abstract": "In this paper, we propose a novel unsupervised learning method for the lexical acquisition of words related to places visited by robots, from human continuous speech signals. We address the problem of learning novel words by a robot that has no prior knowledge of these words except for a primitive acoustic model. Further, we propose a method that allows a robot to effectively use the learned words and their meanings for self-localization tasks. The proposed method is nonparametric Bayesian spatial concept acquisition method (SpCoA) that integrates the generative model for self-localization and the unsupervised word segmentation in uttered sentences via latent variables related to the spatial concept. We implemented the proposed method SpCoA on SIGVerse, which is a simulation environment, and TurtleBot2, which is a mobile robot in a real environment. Further, we conducted experiments for evaluating the performance of SpCoA. The experimental results showed that SpCoA enabled the robot to acquire the names of places from speech sentences. They also revealed that the robot could effectively utilize the acquired spatial concepts and reduce the uncertainty in self-localization.\n    ",
        "submission_date": "2016-02-03T00:00:00",
        "last_modified_date": "2016-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.01398",
        "title": "Finding the different patterns in buildings data using bag of words representation with clustering",
        "authors": [
            "Usman Habib",
            "Gerhard Zucker"
        ],
        "abstract": "The understanding of the buildings operation has become a challenging task due to the large amount of data recorded in energy efficient buildings. Still, today the experts use visual tools for analyzing the data. In order to make the task realistic, a method has been proposed in this paper to automatically detect the different patterns in buildings. The K Means clustering is used to automatically identify the ON (operational) cycles of the chiller. In the next step the ON cycles are transformed to symbolic representation by using Symbolic Aggregate Approximation (SAX) method. Then the SAX symbols are converted to bag of words representation for hierarchical clustering. Moreover, the proposed technique is applied to real life data of adsorption chiller. Additionally, the results from the proposed method and dynamic time warping (DTW) approach are also discussed and compared.\n    ",
        "submission_date": "2016-02-03T00:00:00",
        "last_modified_date": "2016-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.01585",
        "title": "Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering",
        "authors": [
            "Ruining He",
            "Julian McAuley"
        ],
        "abstract": "Building a successful recommender system depends on understanding both the dimensions of people's preferences as well as their dynamics. In certain domains, such as fashion, modeling such preferences can be incredibly difficult, due to the need to simultaneously model the visual appearance of products as well as their evolution over time. The subtle semantics and non-linear dynamics of fashion evolution raise unique challenges especially considering the sparsity and large scale of the underlying datasets. In this paper we build novel models for the One-Class Collaborative Filtering setting, where our goal is to estimate users' fashion-aware personalized ranking functions based on their past feedback. To uncover the complex and evolving visual factors that people consider when evaluating products, our method combines high-level visual features extracted from a deep convolutional neural network, users' past feedback, as well as evolving trends within the community. Experimentally we evaluate our method on two large real-world datasets from ",
        "submission_date": "2016-02-04T00:00:00",
        "last_modified_date": "2016-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.01628",
        "title": "Fuzzy Object-Oriented Dynamic Networks. II",
        "authors": [
            "D. A. Terletskyi",
            "A. I. Provotar"
        ],
        "abstract": "This article generalizes object-oriented dynamic networks to the fuzzy case, which allows one to represent knowledge on objects and classes of objects that are fuzzy by nature and also to model their changes in time. Within the framework of the approach described, a mechanism is proposed that makes it possible to acquire new knowledge on the basis of basic knowledge and considerably differs from well-known methods used in existing models of knowledge representation. The approach is illustrated by an example of construction of a concrete fuzzy object-oriented dynamic network.\n    ",
        "submission_date": "2016-02-04T00:00:00",
        "last_modified_date": "2016-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.01718",
        "title": "Formal Verification of Autonomous Vehicle Platooning",
        "authors": [
            "Maryam Kamali",
            "Louise A. Dennis",
            "Owen McAree",
            "Michael Fisher",
            "Sandor M. Veres"
        ],
        "abstract": "The coordination of multiple autonomous vehicles into convoys or platoons is expected on our highways in the near future. However, before such platoons can be deployed, the new autonomous behaviors of the vehicles in these platoons must be certified. An appropriate representation for vehicle platooning is as a multi-agent system in which each agent captures the \"autonomous decisions\" carried out by each vehicle. In order to ensure that these autonomous decision-making agents in vehicle platoons never violate safety requirements, we use formal verification. However, as the formal verification technique used to verify the agent code does not scale to the full system and as the global verification technique does not capture the essential verification of autonomous behavior, we use a combination of the two approaches. This mixed strategy allows us to verify safety requirements not only of a model of the system, but of the actual agent code used to program the autonomous vehicles.\n    ",
        "submission_date": "2016-02-04T00:00:00",
        "last_modified_date": "2016-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.01971",
        "title": "Wayfinding and cognitive maps for pedestrian models",
        "authors": [
            "Erik Andresen",
            "David Haensel",
            "Mohcine Chraibi",
            "Armin Seyfried"
        ],
        "abstract": "Usually, routing models in pedestrian dynamics assume that agents have fulfilled and global knowledge about the building's structure. However, they neglect the fact that pedestrians possess no or only parts of information about their position relative to final exits and possible routes leading to them. To get a more realistic description we introduce the systematics of gathering and using spatial knowledge. A new wayfinding model for pedestrian dynamics is proposed. The model defines for every pedestrian an individual knowledge representation implying inaccuracies and uncertainties. In addition, knowledge-driven search strategies are introduced. The presented concept is tested on a fictive example scenario.\n    ",
        "submission_date": "2016-02-05T00:00:00",
        "last_modified_date": "2016-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02086",
        "title": "Region Based Approximation for High Dimensional Bayesian Network Models",
        "authors": [
            "Peng Lin",
            "Martin Neil",
            "Norman Fenton"
        ],
        "abstract": "Performing efficient inference on Bayesian Networks (BNs), with large numbers of densely connected variables is challenging. With exact inference methods, such as the Junction Tree algorithm, clustering complexity can grow exponentially with the number of nodes and so computation becomes intractable. This paper presents a general purpose approximate inference algorithm called Triplet Region Construction (TRC) that reduces the clustering complexity for factorized models from worst case exponential to polynomial. We employ graph factorization to reduce connection complexity and produce clusters of limited size. Unlike MCMC algorithms TRC is guaranteed to converge and we present experiments that show that TRC achieves accurate results when compared with exact solutions.\n    ",
        "submission_date": "2016-02-05T00:00:00",
        "last_modified_date": "2016-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02089",
        "title": "Harmonic Grammar in a DisCo Model of Meaning",
        "authors": [
            "Martha Lewis",
            "Bob Coecke"
        ],
        "abstract": "The model of cognition developed in (Smolensky and Legendre, 2006) seeks to unify two levels of description of the cognitive process: the connectionist and the symbolic. The theory developed brings together these two levels into the Integrated Connectionist/Symbolic Cognitive architecture (ICS). Clark and Pulman (2007) draw a parallel with semantics where meaning may be modelled on both distributional and symbolic levels, developed by Coecke et al, 2010 into the Distributional Compositional (DisCo) model of meaning. In the current work, we revisit Smolensky and Legendre (S&L)'s model. We describe the DisCo framework, summarise the key ideas in S&L's architecture, and describe how their description of harmony as a graded measure of grammaticality may be applied in the DisCo model.\n    ",
        "submission_date": "2016-02-05T00:00:00",
        "last_modified_date": "2016-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02169",
        "title": "Probabilistic Extension to the Concurrent Constraint Factor Oracle Model for Music Improvisation",
        "authors": [
            "Mauricio Toro"
        ],
        "abstract": "We can program a Real-Time (RT) music improvisation system in C++ without a formal semantic or we can model it with process calculi such as the Non-deterministic Timed Concurrent Constraint (ntcc) calculus. \"A Concurrent Constraints Factor Oracle (FO) model for Music Improvisation\" (Ccfomi) is an improvisation model specified on ntcc. Since Ccfomi improvises non-deterministically, there is no control on choices and therefore little control over the sequence variation during the improvisation. To avoid this, we extended Ccfomi using the Probabilistic Non-deterministic Timed Concurrent Constraint calculus. Our extension to Ccfomi does not change the time and space complexity of building the FO, thus making our extension compatible with RT. However, there was not a ntcc interpreter capable of RT to execute Ccfomi. We developed Ntccrt --a RT capable interpreter for ntcc-- and we executed Ccfomi on Ntccrt. In the future, we plan to extend Ntccrt to execute our extension to Ccfomi.\n    ",
        "submission_date": "2016-02-05T00:00:00",
        "last_modified_date": "2016-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02261",
        "title": "End-to-End Goal-Driven Web Navigation",
        "authors": [
            "Rodrigo Nogueira",
            "Kyunghyun Cho"
        ],
        "abstract": "We propose a goal-driven web navigation as a benchmark task for evaluating an agent with abilities to understand natural language and plan on partially observed environments. In this challenging task, an agent navigates through a website, which is represented as a graph consisting of web pages as nodes and hyperlinks as directed edges, to find a web page in which a query appears. The agent is required to have sophisticated high-level reasoning based on natural languages and efficient sequential decision-making capability to succeed. We release a software tool, called WebNav, that automatically transforms a website into this goal-driven web navigation task, and as an example, we make WikiNav, a dataset constructed from the English Wikipedia. We extensively evaluate different variants of neural net based artificial agents on WikiNav and observe that the proposed goal-driven web navigation well reflects the advances in models, making it a suitable benchmark for evaluating future progress. Furthermore, we extend the WikiNav with question-answer pairs from Jeopardy! and test the proposed agent based on recurrent neural networks against strong inverted index based search engines. The artificial agents trained on WikiNav outperforms the engined based approaches, demonstrating the capability of the proposed goal-driven navigation as a good proxy for measuring the progress in real-world tasks such as focused crawling and question-answering.\n    ",
        "submission_date": "2016-02-06T00:00:00",
        "last_modified_date": "2016-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02617",
        "title": "Adaptive imputation of missing values for incomplete pattern classification",
        "authors": [
            "Zhun-Ga Liu",
            "Quan Pan",
            "Jean Dezert",
            "Arnaud Martin"
        ],
        "abstract": "In classification of incomplete pattern, the missing values can either play a crucial role in the class determination, or have only little influence (or eventually none) on the classification results according to the context. We propose a credal classification method for incomplete pattern with adaptive imputation of missing values based on belief function theory. At first, we try to classify the object (incomplete pattern) based only on the available attribute values. As underlying principle, we assume that the missing information is not crucial for the classification if a specific class for the object can be found using only the available information. In this case, the object is committed to this particular class. However, if the object cannot be classified without ambiguity, it means that the missing values play a main role for achieving an accurate classification. In this case, the missing values will be imputed based on the K-nearest neighbor (K-NN) and self-organizing map (SOM) techniques, and the edited pattern with the imputation is then classified. The (original or edited) pattern is respectively classified according to each training class, and the classification results represented by basic belief assignments are fused with proper combination rules for making the credal classification. The object is allowed to belong with different masses of belief to the specific classes and meta-classes (which are particular disjunctions of several single classes). The credal classification captures well the uncertainty and imprecision of classification, and reduces effectively the rate of misclassifications thanks to the introduction of meta-classes. The effectiveness of the proposed method with respect to other classical methods is demonstrated based on several experiments using artificial and real data sets.\n    ",
        "submission_date": "2016-02-08T00:00:00",
        "last_modified_date": "2016-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02672",
        "title": "Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks",
        "authors": [
            "Jakob N. Foerster",
            "Yannis M. Assael",
            "Nando de Freitas",
            "Shimon Whiteson"
        ],
        "abstract": "We propose deep distributed recurrent Q-networks (DDRQN), which enable teams of agents to learn to solve communication-based coordination tasks. In these tasks, the agents are not given any pre-designed communication protocol. Therefore, in order to successfully communicate, they must first automatically develop and agree upon their own communication protocol. We present empirical results on two multi-agent learning problems based on well-known riddles, demonstrating that DDRQN can successfully solve such tasks and discover elegant communication protocols to do so. To our knowledge, this is the first time deep reinforcement learning has succeeded in learning communication protocols. In addition, we present ablation experiments that confirm that each of the main components of the DDRQN architecture are critical to its success.\n    ",
        "submission_date": "2016-02-08T00:00:00",
        "last_modified_date": "2016-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02867",
        "title": "Value Iteration Networks",
        "authors": [
            "Aviv Tamar",
            "Yi Wu",
            "Garrett Thomas",
            "Sergey Levine",
            "Pieter Abbeel"
        ],
        "abstract": "We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.\n    ",
        "submission_date": "2016-02-09T00:00:00",
        "last_modified_date": "2017-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.03203",
        "title": "Time Resource Networks",
        "authors": [
            "Szymon Sidor",
            "Peng Yu",
            "Cheng Fang",
            "Brian Williams"
        ],
        "abstract": "The problem of scheduling under resource constraints is widely applicable. One prominent example is power management, in which we have a limited continuous supply of power but must schedule a number of power-consuming tasks. Such problems feature tightly coupled continuous resource constraints and continuous temporal constraints.\n",
        "submission_date": "2016-02-09T00:00:00",
        "last_modified_date": "2016-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.03291",
        "title": "Feature Based Task Recommendation in Crowdsourcing with Implicit Observations",
        "authors": [
            "Habibur Rahman",
            "Lucas Joppa",
            "Senjuti Basu Roy"
        ],
        "abstract": "Existing research in crowdsourcing has investigated how to recommend tasks to workers based on which task the workers have already completed, referred to as {\\em implicit feedback}. We, on the other hand, investigate the task recommendation problem, where we leverage both implicit feedback and explicit features of the task. We assume that we are given a set of workers, a set of tasks, interactions (such as the number of times a worker has completed a particular task), and the presence of explicit features of each task (such as, task location). We intend to recommend tasks to the workers by exploiting the implicit interactions, and the presence or absence of explicit features in the tasks. We formalize the problem as an optimization problem, propose two alternative problem formulations and respective solutions that exploit implicit feedback, explicit features, as well as similarity between the tasks. We compare the efficacy of our proposed solutions against multiple state-of-the-art techniques using two large scale real world datasets.\n    ",
        "submission_date": "2016-02-10T00:00:00",
        "last_modified_date": "2016-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.03506",
        "title": "Research Priorities for Robust and Beneficial Artificial Intelligence",
        "authors": [
            "Stuart Russell",
            "Daniel Dewey",
            "Max Tegmark"
        ],
        "abstract": "Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that AI remains robust and beneficial.\n    ",
        "submission_date": "2016-02-10T00:00:00",
        "last_modified_date": "2016-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.03779",
        "title": "Network of Bandits insure Privacy of end-users",
        "authors": [
            "Rapha\u00ebl F\u00e9raud"
        ],
        "abstract": "In order to distribute the best arm identification task as close as possible to the user's devices, on the edge of the Radio Access Network, we propose a new problem setting, where distributed players collaborate to find the best arm. This architecture guarantees privacy to end-users since no events are stored. The only thing that can be observed by an adversary through the core network is aggregated information across users. We provide a first algorithm, Distributed Median Elimination, which is optimal in term of number of transmitted bits and near optimal in term of speed-up factor with respect to an optimal algorithm run independently on each player. In practice, this first algorithm cannot handle the trade-off between the communication cost and the speed-up factor, and requires some knowledge about the distribution of players. Extended Distributed Median Elimination overcomes these limitations, by playing in parallel different instances of Distributed Median Elimination and selecting the best one. Experiments illustrate and complete the analysis. According to the analysis, in comparison to Median Elimination performed on each player, the proposed algorithm shows significant practical improvements.\n    ",
        "submission_date": "2016-02-11T00:00:00",
        "last_modified_date": "2017-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.03924",
        "title": "Modeling Human Ad Hoc Coordination",
        "authors": [
            "Peter M. Krafft",
            "Chris L. Baker",
            "Alex Pentland",
            "Joshua B. Tenenbaum"
        ],
        "abstract": "Whether in groups of humans or groups of computer agents, collaboration is most effective between individuals who have the ability to coordinate on a joint strategy for collective action. However, in general a rational actor will only intend to coordinate if that actor believes the other group members have the same intention. This circular dependence makes rational coordination difficult in uncertain environments if communication between actors is unreliable and no prior agreements have been made. An important normative question with regard to coordination in these ad hoc settings is therefore how one can come to believe that other actors will coordinate, and with regard to systems involving humans, an important empirical question is how humans arrive at these expectations. We introduce an exact algorithm for computing the infinitely recursive hierarchy of graded beliefs required for rational coordination in uncertain environments, and we introduce a novel mechanism for multiagent coordination that uses it. Our algorithm is valid in any environment with a finite state space, and extensions to certain countably infinite state spaces are likely possible. We test our mechanism for multiagent coordination as a model for human decisions in a simple coordination game using existing experimental data. We then explore via simulations whether modeling humans in this way may improve human-agent collaboration.\n    ",
        "submission_date": "2016-02-11T00:00:00",
        "last_modified_date": "2016-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.03963",
        "title": "Detection of Cooperative Interactions in Logistic Regression Models",
        "authors": [
            "Easton Li Xu",
            "Xiaoning Qian",
            "Tie Liu",
            "Shuguang Cui"
        ],
        "abstract": "An important problem in the field of bioinformatics is to identify interactive effects among profiled variables for outcome prediction. In this paper, a logistic regression model with pairwise interactions among a set of binary covariates is considered. Modeling the structure of the interactions by a graph, our goal is to recover the interaction graph from independently identically distributed (i.i.d.) samples of the covariates and the outcome.\n",
        "submission_date": "2016-02-12T00:00:00",
        "last_modified_date": "2016-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04019",
        "title": "Energetics of the brain and AI",
        "authors": [
            "Anders Sandberg"
        ],
        "abstract": "Does the energy requirements for the human brain give energy constraints that give reason to doubt the feasibility of artificial intelligence? This report will review some relevant estimates of brain bioenergetics and analyze some of the methods of estimating brain emulation energy requirements. Turning to AI, there are reasons to believe the energy requirements for de novo AI to have little correlation with brain (emulation) energy requirements since cost could depend merely of the cost of processing higher-level representations rather than billions of neural firings. Unless one thinks the human way of thinking is the most optimal or most easily implementable way of achieving software intelligence, we should expect de novo AI to make use of different, potentially very compressed and fast, processes.\n    ",
        "submission_date": "2016-02-12T00:00:00",
        "last_modified_date": "2016-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04032",
        "title": "A Truthful Mechanism with Biparameter Learning for Online Crowdsourcing",
        "authors": [
            "Satyanath Bhat",
            "Divya Padmanabhan",
            "Shweta Jain",
            "Y Narahari"
        ],
        "abstract": "We study a problem of allocating divisible jobs, arriving online, to workers in a crowdsourcing setting which involves learning two parameters of strategically behaving workers. Each job is split into a certain number of tasks that are then allocated to workers. Each arriving job has to be completed within a deadline and each task has to be completed satisfying an upper bound on probability of failure. The job population is homogeneous while the workers are heterogeneous in terms of costs, completion times, and times to failure. The job completion time and time to failure of each worker are stochastic with fixed but unknown means. The requester is faced with the challenge of learning two separate parameters of each (strategically behaving) worker simultaneously, namely, the mean job completion time and the mean time to failure. The time to failure of a worker depends on the duration of the task handled by the worker. Assuming non-strategic workers to start with, we solve this biparameter learning problem by applying the Robust UCB algorithm. Then, we non-trivially extend this algorithm to the setting where the workers are strategic about their costs. Our proposed mechanism is dominant strategy incentive compatible and ex-post individually rational with asymptotically optimal regret performance.\n    ",
        "submission_date": "2016-02-12T00:00:00",
        "last_modified_date": "2016-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04257",
        "title": "Identifying Diabetic Patients with High Risk of Readmission",
        "authors": [
            "Malladihalli S Bhuvan",
            "Ankit Kumar",
            "Adil Zafar",
            "Vinith Kishore"
        ],
        "abstract": "Hospital readmissions are expensive and reflect the inadequacies in healthcare system. In the United States alone, treatment of readmitted diabetic patients exceeds 250 million dollars per year. Early identification of patients facing a high risk of readmission can enable healthcare providers to to conduct additional investigations and possibly prevent future readmissions. This not only improves the quality of care but also reduces the medical expenses on readmission. Machine learning methods have been leveraged on public health data to build a system for identifying diabetic patients facing a high risk of future readmission. Number of inpatient visits, discharge disposition and admission type were identified as strong predictors of readmission. Further, it was found that the number of laboratory tests and discharge disposition together predict whether the patient will be readmitted shortly after being discharged from the hospital (i.e. <30 days) or after a longer period of time (i.e. >30 days). These insights can help healthcare providers to improve inpatient diabetic care. Finally, the cost analysis suggests that \\$252.76 million can be saved across 98,053 diabetic patient encounters by incorporating the proposed cost sensitive analysis model.\n    ",
        "submission_date": "2016-02-12T00:00:00",
        "last_modified_date": "2016-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04259",
        "title": "A Minimalistic Approach to Sum-Product Network Learning for Real Applications",
        "authors": [
            "Viktoriya Krakovna",
            "Moshe Looks"
        ],
        "abstract": "Sum-Product Networks (SPNs) are a class of expressive yet tractable hierarchical graphical models. LearnSPN is a structure learning algorithm for SPNs that uses hierarchical co-clustering to simultaneously identifying similar entities and similar features. The original LearnSPN algorithm assumes that all the variables are discrete and there is no missing data. We introduce a practical, simplified version of LearnSPN, MiniSPN, that runs faster and can handle missing data and heterogeneous features common in real applications. We demonstrate the performance of MiniSPN on standard benchmark datasets and on two datasets from Google's Knowledge Graph exhibiting high missingness rates and a mix of discrete and continuous features.\n    ",
        "submission_date": "2016-02-12T00:00:00",
        "last_modified_date": "2016-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04290",
        "title": "Designing Intelligent Instruments",
        "authors": [
            "Kevin H. Knuth",
            "Philip M. Erner",
            "Scott Frasso"
        ],
        "abstract": "Remote science operations require automated systems that can both act and react with minimal human intervention. One such vision is that of an intelligent instrument that collects data in an automated fashion, and based on what it learns, decides which new measurements to take. This innovation implements experimental design and unites it with data analysis in such a way that it completes the cycle of learning. This cycle is the basis of the Scientific Method.\n",
        "submission_date": "2016-02-13T00:00:00",
        "last_modified_date": "2016-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04358",
        "title": "Machine olfaction using time scattering of sensor multiresolution graphs",
        "authors": [
            "Leonid Gugel",
            "Yoel Shkolnisky",
            "Shai Dekel"
        ],
        "abstract": "In this paper we construct a learning architecture for high dimensional time series sampled by sensor arrangements. Using a redundant wavelet decomposition on a graph constructed over the sensor locations, our algorithm is able to construct discriminative features that exploit the mutual information between the sensors. The algorithm then applies scattering networks to the time series graphs to create the feature space. We demonstrate our method on a machine olfaction problem, where one needs to classify the gas type and the location where it originates from data sampled by an array of sensors. Our experimental results clearly demonstrate that our method outperforms classical machine learning techniques used in previous studies.\n    ",
        "submission_date": "2016-02-13T00:00:00",
        "last_modified_date": "2016-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04376",
        "title": "BPCMont: Business Process Change Management Ontology",
        "authors": [
            "Muhammad Fahad"
        ],
        "abstract": "Change management for evolving collaborative business process development is crucial when the business logic, transections and workflow change due to changes in business strategies or organizational and technical environment. During the change implementation, business processes are analyzed and improved ensuring that they capture the proposed change and they do not contain any undesired functionalities or change side-effects. This paper presents Business Process Change Management approach for the efficient and effective implementation of change in the business process. The key technology behind our approach is our proposed Business Process Change Management Ontology (BPCMont) which is the main contribution of this paper. BPCMont, as a formalized change specification, helps to revert BP into a consistent state in case of system crash, intermediate conflicting stage or unauthorized change done, aid in change traceability in the new and old versions of business processes, change effects can be seen and estimated effectively, ease for Stakeholders to validate and verify change implementation, etc.\n    ",
        "submission_date": "2016-02-13T00:00:00",
        "last_modified_date": "2016-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04435",
        "title": "Random Forest Based Approach for Concept Drift Handling",
        "authors": [
            "A. Zhukov",
            "D. Sidorov",
            "A. Foley"
        ],
        "abstract": "Concept drift has potential in smart grid analysis because the socio-economic behaviour of consumers is not governed by the laws of physics. Likewise there are also applications in wind power forecasting. In this paper we present decision tree ensemble classification method based on the Random Forest algorithm for concept drift. The weighted majority voting ensemble aggregation rule is employed based on the ideas of Accuracy Weighted Ensemble (AWE) method. Base learner weight in our case is computed for each sample evaluation using base learners accuracy and intrinsic proximity measure of Random Forest. Our algorithm exploits both temporal weighting of samples and ensemble pruning as a forgetting strategy. We present results of empirical comparison of our method with original random forest with incorporated \"replace-the-looser\" forgetting andother state-of-the-art concept-drfit classifiers like AWE2.\n    ",
        "submission_date": "2016-02-14T00:00:00",
        "last_modified_date": "2016-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04473",
        "title": "Large-Scale Reasoning with OWL",
        "authors": [
            "Michael Ruster"
        ],
        "abstract": "With the growth of the Semantic Web in size and importance, more and more knowledge is stored in machine-readable formats such as the Web Ontology Language OWL. This paper outlines common approaches for efficient reasoning on large-scale data consisting of billions ($10^9$) of triples. Therefore, OWL and its sublanguages, as well as forward and backward chaining techniques are presented. The WebPIE reasoner is discussed in detail as an example for forward chaining using MapReduce for materialisation. Moreover, the QueryPIE reasoner is presented as a backward chaining/hybrid approach which uses query rewriting. Furthermore, an overview on other reasoners is given such as OWLIM and TrOWL.\n    ",
        "submission_date": "2016-02-14T00:00:00",
        "last_modified_date": "2016-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04498",
        "title": "Extending Consequence-Based Reasoning to SRIQ",
        "authors": [
            "Andrew Bate",
            "Boris Motik",
            "Bernardo Cuenca Grau",
            "Franti\u0161ek Siman\u010d\u00edk",
            "Ian Horrocks"
        ],
        "abstract": "Consequence-based calculi are a family of reasoning algorithms for description logics (DLs), and they combine hypertableau and resolution in a way that often achieves excellent performance in practice. Up to now, however, they were proposed for either Horn DLs (which do not support disjunction), or for DLs without counting quantifiers. In this paper we present a novel consequence-based calculus for SRIQ---a rich DL that supports both features. This extension is non-trivial since the intermediate consequences that need to be derived during reasoning cannot be captured using DLs themselves. The results of our preliminary performance evaluation suggest the feasibility of our approach in practice.\n    ",
        "submission_date": "2016-02-14T00:00:00",
        "last_modified_date": "2016-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04613",
        "title": "Towards reducing the multidimensionality of OLAP cubes using the Evolutionary Algorithms and Factor Analysis Methods",
        "authors": [
            "Sami Naouali",
            "Semeh Ben Salem"
        ],
        "abstract": "Data Warehouses are structures with large amount of data collected from heterogeneous sources to be used in a decision support system. Data Warehouses analysis identifies hidden patterns initially unexpected which analysis requires great memory and computation cost. Data reduction methods were proposed to make this analysis easier. In this paper, we present a hybrid approach based on Genetic Algorithms (GA) as Evolutionary Algorithms and the Multiple Correspondence Analysis (MCA) as Analysis Factor Methods to conduct this reduction. Our approach identifies reduced subset of dimensions from the initial subset p where p'<p where it is proposed to find the profile fact that is the closest to reference. GAs identify the possible subsets and the Khi formula of the ACM evaluates the quality of each subset. The study is based on a distance measurement between the reference and n facts profile extracted from the Warehouses.\n    ",
        "submission_date": "2016-02-15T00:00:00",
        "last_modified_date": "2016-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04875",
        "title": "POMDP-lite for Robust Robot Planning under Uncertainty",
        "authors": [
            "Min Chen",
            "Emilio Frazzoli",
            "David Hsu",
            "Wee Sun Lee"
        ],
        "abstract": "The partially observable Markov decision process (POMDP) provides a principled general model for planning under uncertainty. However, solving a general POMDP is computationally intractable in the worst case. This paper introduces POMDP-lite, a subclass of POMDPs in which the hidden state variables are constant or only change deterministically. We show that a POMDP-lite is equivalent to a set of fully observable Markov decision processes indexed by a hidden parameter and is useful for modeling a variety of interesting robotic tasks. We develop a simple model-based Bayesian reinforcement learning algorithm to solve POMDP-lite models. The algorithm performs well on large-scale POMDP-lite models with up to $10^{20}$ states and outperforms the state-of-the-art general-purpose POMDP algorithms. We further show that the algorithm is near-Bayesian-optimal under suitable conditions.\n    ",
        "submission_date": "2016-02-16T00:00:00",
        "last_modified_date": "2016-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04936",
        "title": "Reinforcement Learning approach for Real Time Strategy Games Battle city and S3",
        "authors": [
            "Harshit Sethy",
            "Amit Patel"
        ],
        "abstract": "In this paper we proposed reinforcement learning algorithms with the generalized reward function. In our proposed method we use Q-learning and SARSA algorithms with generalised reward function to train the reinforcement learning agent. We evaluated the performance of our proposed algorithms on two real-time strategy games called BattleCity and S3. There are two main advantages of having such an approach as compared to other works in RTS. (1) We can ignore the concept of a simulator which is often game specific and is usually hard coded in any type of RTS games (2) our system can learn from interaction with any opponents and quickly change the strategy according to the opponents and do not need any human traces as used in previous works. Keywords : Reinforcement learning, Machine learning, Real time strategy, Artificial intelligence.\n    ",
        "submission_date": "2016-02-16T00:00:00",
        "last_modified_date": "2016-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04951",
        "title": "Q($\u03bb$) with Off-Policy Corrections",
        "authors": [
            "Anna Harutyunyan",
            "Marc G. Bellemare",
            "Tom Stepleton",
            "Remi Munos"
        ],
        "abstract": "We propose and analyze an alternate approach to off-policy multi-step temporal difference learning, in which off-policy returns are corrected with the current Q-function in terms of rewards, rather than with the target policy in terms of transition probabilities. We prove that such approximate corrections are sufficient for off-policy convergence both in policy evaluation and control, provided certain conditions. These conditions relate the distance between the target and behavior policies, the eligibility trace parameter and the discount factor, and formalize an underlying tradeoff in off-policy TD($\\lambda$). We illustrate this theoretical relationship empirically on a continuous-state control task.\n    ",
        "submission_date": "2016-02-16T00:00:00",
        "last_modified_date": "2016-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05404",
        "title": "11 x 11 Domineering is Solved: The first player wins",
        "authors": [
            "Jos W.H.M. Uiterwijk"
        ],
        "abstract": "We have developed a program called MUDoS (Maastricht University Domineering Solver) that solves Domineering positions in a very efficient way. This enables the solution of known positions so far (up to the 10 x 10 board) much quicker (measured in number of investigated nodes).\n",
        "submission_date": "2016-02-17T00:00:00",
        "last_modified_date": "2016-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05561",
        "title": "Lexis: An Optimization Framework for Discovering the Hierarchical Structure of Sequential Data",
        "authors": [
            "Payam Siyari",
            "Bistra Dilkina",
            "Constantine Dovrolis"
        ],
        "abstract": "Data represented as strings abounds in biology, linguistics, document mining, web search and many other fields. Such data often have a hierarchical structure, either because they were artificially designed and composed in a hierarchical manner or because there is an underlying evolutionary process that creates repeatedly more complex strings from simpler substrings. We propose a framework, referred to as \"Lexis\", that produces an optimized hierarchical representation of a given set of \"target\" strings. The resulting hierarchy, \"Lexis-DAG\", shows how to construct each target through the concatenation of intermediate substrings, minimizing the total number of such concatenations or DAG edges. The Lexis optimization problem is related to the smallest grammar problem. After we prove its NP-Hardness for two cost formulations, we propose an efficient greedy algorithm for the construction of Lexis-DAGs. We also consider the problem of identifying the set of intermediate nodes (substrings) that collectively form the \"core\" of a Lexis-DAG, which is important in the analysis of Lexis-DAGs. We show that the Lexis framework can be applied in diverse applications such as optimized synthesis of DNA fragments in genomic libraries, hierarchical structure discovery in protein sequences, dictionary-based text compression, and feature extraction from a set of documents.\n    ",
        "submission_date": "2016-02-17T00:00:00",
        "last_modified_date": "2016-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05699",
        "title": "Query Answering with Inconsistent Existential Rules under Stable Model Semantics",
        "authors": [
            "Hai Wan",
            "Heng Zhang",
            "Peng Xiao",
            "Haoran Huang",
            "Yan Zhang"
        ],
        "abstract": "Traditional inconsistency-tolerent query answering in ontology-based data access relies on selecting maximal components of an ABox/database which are consistent with the ontology. However, some rules in ontologies might be unreliable if they are extracted from ontology learning or written by unskillful knowledge engineers. In this paper we present a framework of handling inconsistent existential rules under stable model semantics, which is defined by a notion called rule repairs to select maximal components of the existential rules. Surprisingly, for R-acyclic existential rules with R-stratified or guarded existential rules with stratified negations, both the data complexity and combined complexity of query answering under the rule {repair semantics} remain the same as that under the conventional query answering semantics. This leads us to propose several approaches to handle the rule {repair semantics} by calling answer set programming solvers. An experimental evaluation shows that these approaches have good scalability of query answering under rule repairs on realistic cases.\n    ",
        "submission_date": "2016-02-18T00:00:00",
        "last_modified_date": "2016-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05705",
        "title": "A theory of contemplation",
        "authors": [
            "Jonathan Darren Nix"
        ],
        "abstract": "In this paper you can explore the application of some notable Boolean-derived methods, namely the Disjunctive Normal Form representation of logic table expansions, and extend them to a real-valued logic model which is able to utilize quantities on the range [0,1], [-1,1], [a,b], (x,y), (x,y,z), and etc. so as to produce a logical programming of arbitrary range, precision, and dimensionality, thereby enabling contemplation at a logical level in notions of arbitrary data, colors, and spatial constructs, with an example of the production of a game character's logic in mathematical form.\n    ",
        "submission_date": "2016-02-18T00:00:00",
        "last_modified_date": "2019-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05765",
        "title": "Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning",
        "authors": [
            "Shoaib Jameel",
            "Steven Schockaert"
        ],
        "abstract": "Conceptual spaces are geometric representations of conceptual knowledge, in which entities correspond to points, natural properties correspond to convex regions, and the dimensions of the space correspond to salient features. While conceptual spaces enable elegant models of various cognitive phenomena, the lack of automated methods for constructing such representations have so far limited their application in artificial intelligence. To address this issue, we propose a method which learns a vector-space embedding of entities from Wikipedia and constrains this embedding such that entities of the same semantic type are located in some lower-dimensional subspace. We experimentally demonstrate the usefulness of these subspaces as (approximate) conceptual space representations by showing, among others, that important features can be modelled as directions and that natural properties tend to correspond to convex regions.\n    ",
        "submission_date": "2016-02-18T00:00:00",
        "last_modified_date": "2017-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05828",
        "title": "A General Modifier-based Framework for Inconsistency-Tolerant Query Answering",
        "authors": [
            "Jean Francois Baget",
            "Salem Benferhat",
            "Zied Bouraoui",
            "Madalina Croitoru",
            "Marie-Laure Mugnier",
            "Odile Papini",
            "Swan Rocher",
            "Karim Tabia"
        ],
        "abstract": "We propose a general framework for inconsistency-tolerant query answering within existential rule setting. This framework unifies the main semantics proposed by the state of art and introduces new ones based on cardinality and majority principles. It relies on two key notions: modifiers and inference strategies. An inconsistency-tolerant semantics is seen as a composite modifier plus an inference strategy. We compare the obtained semantics from a productivity point of view.\n    ",
        "submission_date": "2016-02-18T00:00:00",
        "last_modified_date": "2016-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06347",
        "title": "Distributed Constraint Optimization Problems and Applications: A Survey",
        "authors": [
            "Ferdinando Fioretto",
            "Enrico Pontelli",
            "William Yeoh"
        ],
        "abstract": "The field of Multi-Agent System (MAS) is an active area of research within Artificial Intelligence, with an increasingly important impact in industrial and other real-world applications. Within a MAS, autonomous agents interact to pursue personal interests and/or to achieve common objectives. Distributed Constraint Optimization Problems (DCOPs) have emerged as one of the prominent agent architectures to govern the agents' autonomous behavior, where both algorithms and communication models are driven by the structure of the specific problem. During the last decade, several extensions to the DCOP model have enabled them to support MAS in complex, real-time, and uncertain environments. This survey aims at providing an overview of the DCOP model, giving a classification of its multiple extensions and addressing both resolution methods and applications that find a natural mapping within each class of DCOPs. The proposed classification suggests several future perspectives for DCOP extensions, and identifies challenges in the design of efficient resolution algorithms, possibly through the adaptation of strategies from different areas.\n    ",
        "submission_date": "2016-02-20T00:00:00",
        "last_modified_date": "2018-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06462",
        "title": "The Singularity May Never Be Near",
        "authors": [
            "Toby Walsh"
        ],
        "abstract": "There is both much optimism and pessimism around artificial intelligence (AI) today. The optimists are investing millions of dollars, and even in some cases billions of dollars into AI. The pessimists, on the other hand, predict that AI will end many things: jobs, warfare, and even the human race. Both the optimists and the pessimists often appeal to the idea of a technological singularity, a point in time where machine intelligence starts to run away, and a new, more intelligent species starts to inhabit the earth. If the optimists are right, this will be a moment that fundamentally changes our economy and our society. If the pessimists are right, this will be a moment that also fundamentally changes our economy and our society. It is therefore very worthwhile spending some time deciding if either of them might be right.\n    ",
        "submission_date": "2016-02-20T00:00:00",
        "last_modified_date": "2016-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06484",
        "title": "Computational Narrative Intelligence: A Human-Centered Goal for Artificial Intelligence",
        "authors": [
            "Mark O. Riedl"
        ],
        "abstract": "Narrative intelligence is the ability to craft, tell, understand, and respond affectively to stories. We argue that instilling artificial intelligences with computational narrative intelligence affords a number of applications beneficial to humans. We lay out some of the machine learning challenges necessary to solve to achieve computational narrative intelligence. Finally, we argue that computational narrative is a practical step towards machine enculturation, the teaching of sociocultural values to machines.\n    ",
        "submission_date": "2016-02-21T00:00:00",
        "last_modified_date": "2016-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06566",
        "title": "Interactive Storytelling over Document Collections",
        "authors": [
            "Dipayan Maiti",
            "Mohammad Raihanul Islam",
            "Scotland Leman",
            "Naren Ramakrishnan"
        ],
        "abstract": "Storytelling algorithms aim to 'connect the dots' between disparate documents by linking starting and ending documents through a series of intermediate documents. Existing storytelling algorithms are based on notions of coherence and connectivity, and thus the primary way by which users can steer the story construction is via design of suitable similarity functions. We present an alternative approach to storytelling wherein the user can interactively and iteratively provide 'must use' constraints to preferentially support the construction of some stories over others. The three innovations in our approach are distance measures based on (inferred) topic distributions, the use of constraints to define sets of linear inequalities over paths, and the introduction of slack and surplus variables to condition the topic distribution to preferentially emphasize desired terms over others. We describe experimental results to illustrate the effectiveness of our interactive storytelling approach over multiple text datasets.\n    ",
        "submission_date": "2016-02-21T00:00:00",
        "last_modified_date": "2016-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07032",
        "title": "Unbounded Human Learning: Optimal Scheduling for Spaced Repetition",
        "authors": [
            "Siddharth Reddy",
            "Igor Labutov",
            "Siddhartha Banerjee",
            "Thorsten Joachims"
        ],
        "abstract": "In the study of human learning, there is broad evidence that our ability to retain information improves with repeated exposure and decays with delay since last exposure. This plays a crucial role in the design of educational software, leading to a trade-off between teaching new material and reviewing what has already been taught. A common way to balance this trade-off is spaced repetition, which uses periodic review of content to improve long-term retention. Though spaced repetition is widely used in practice, e.g., in electronic flashcard software, there is little formal understanding of the design of these systems. Our paper addresses this gap in three ways. First, we mine log data from spaced repetition software to establish the functional dependence of retention on reinforcement and delay. Second, we use this memory model to develop a stochastic model for spaced repetition systems. We propose a queueing network model of the Leitner system for reviewing flashcards, along with a heuristic approximation that admits a tractable optimization problem for review scheduling. Finally, we empirically evaluate our queueing model through a Mechanical Turk experiment, verifying a key qualitative prediction of our model: the existence of a sharp phase transition in learning outcomes upon increasing the rate of new item introductions.\n    ",
        "submission_date": "2016-02-23T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07057",
        "title": "Finding Needle in a Million Metrics: Anomaly Detection in a Large-scale Computational Advertising Platform",
        "authors": [
            "Bowen Zhou",
            "Shahriar Shariat"
        ],
        "abstract": "Online media offers opportunities to marketers to deliver brand messages to a large audience. Advertising technology platforms enables the advertisers to find the proper group of audiences and deliver ad impressions to them in real time. The recent growth of the real time bidding has posed a significant challenge on monitoring such a complicated system. With so many components we need a reliable system that detects the possible changes in the system and alerts the engineering team. In this paper we describe the mechanism that we invented for recovering the representative metrics and detecting the change in their behavior. We show that this mechanism is able to detect the possible problems in time by describing some incident cases.\n    ",
        "submission_date": "2016-02-23T00:00:00",
        "last_modified_date": "2016-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07565",
        "title": "Stochastic Shortest Path with Energy Constraints in POMDPs",
        "authors": [
            "Tom\u00e1\u0161 Br\u00e1zdil",
            "Krishnendu Chatterjee",
            "Martin Chmel\u00edk",
            "Anchit Gupta",
            "Petr Novotn\u00fd"
        ],
        "abstract": "We consider partially observable Markov decision processes (POMDPs) with a set of target states and positive integer costs associated with every transition. The traditional optimization objective (stochastic shortest path) asks to minimize the expected total cost until the target set is reached. We extend the traditional framework of POMDPs to model energy consumption, which represents a hard constraint. The energy levels may increase and decrease with transitions, and the hard constraint requires that the energy level must remain positive in all steps till the target is reached. First, we present a novel algorithm for solving POMDPs with energy levels, developing on existing POMDP solvers and using RTDP as its main method. Our second contribution is related to policy representation. For larger POMDP instances the policies computed by existing solvers are too large to be understandable. We present an automated procedure based on machine learning techniques that automatically extracts important decisions of the policy allowing us to compute succinct human readable policies. Finally, we show experimentally that our algorithm performs well and computes succinct policies on a number of POMDP instances from the literature that were naturally enhanced with energy levels.\n    ",
        "submission_date": "2016-02-24T00:00:00",
        "last_modified_date": "2016-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07566",
        "title": "Time and Activity Sequence Prediction of Business Process Instances",
        "authors": [
            "Mirko Polato",
            "Alessandro Sperduti",
            "Andrea Burattin",
            "Massimiliano de Leoni"
        ],
        "abstract": "The ability to know in advance the trend of running process instances, with respect to different features, such as the expected completion time, would allow business managers to timely counteract to undesired situations, in order to prevent losses. Therefore, the ability to accurately predict future features of running business process instances would be a very helpful aid when managing processes, especially under service level agreement constraints. However, making such accurate forecasts is not easy: many factors may influence the predicted features.\n",
        "submission_date": "2016-02-24T00:00:00",
        "last_modified_date": "2016-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07721",
        "title": "Toward Game Level Generation from Gameplay Videos",
        "authors": [
            "Matthew Guzdial",
            "Mark Riedl"
        ],
        "abstract": "Algorithms that generate computer game content require game design knowledge. We present an approach to automatically learn game design knowledge for level design from gameplay videos. We further demonstrate how the acquired design knowledge can be used to generate sections of game levels. Our approach involves parsing video of people playing a game to detect the appearance of patterns of sprites and utilizing machine learning to build a probabilistic model of sprite placement. We show how rich game design information can be automatically parsed from gameplay videos and represented as a set of generative probabilistic models. We use Super Mario Bros. as a proof of concept. We evaluate our approach on a measure of playability and stylistic similarity to the original levels as represented in the gameplay videos.\n    ",
        "submission_date": "2016-02-23T00:00:00",
        "last_modified_date": "2016-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07764",
        "title": "Reinforcement Learning of POMDPs using Spectral Methods",
        "authors": [
            "Kamyar Azizzadenesheli",
            "Alessandro Lazaric",
            "Animashree Anandkumar"
        ],
        "abstract": "We propose a new reinforcement learning algorithm for partially observable Markov decision processes (POMDP) based on spectral decomposition methods. While spectral methods have been previously employed for consistent learning of (passive) latent variable models such as hidden Markov models, POMDPs are more challenging since the learner interacts with the environment and possibly changes the future observations in the process. We devise a learning algorithm running through episodes, in each episode we employ spectral techniques to learn the POMDP parameters from a trajectory generated by a fixed policy. At the end of the episode, an optimization oracle returns the optimal memoryless planning policy which maximizes the expected reward based on the estimated POMDP model. We prove an order-optimal regret bound with respect to the optimal memoryless policy and efficient scaling with respect to the dimensionality of observation and action spaces.\n    ",
        "submission_date": "2016-02-25T00:00:00",
        "last_modified_date": "2016-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07857",
        "title": "Modeling cumulative biological phenomena with Suppes-Bayes Causal Networks",
        "authors": [
            "Daniele Ramazzotti",
            "Alex Graudenzi",
            "Giulio Caravagna",
            "Marco Antoniotti"
        ],
        "abstract": "Several diseases related to cell proliferation are characterized by the accumulation of somatic DNA changes, with respect to wildtype conditions. Cancer and HIV are two common examples of such diseases, where the mutational load in the cancerous/viral population increases over time. In these cases, selective pressures are often observed along with competition, cooperation and parasitism among distinct cellular clones. Recently, we presented a mathematical framework to model these phenomena, based on a combination of Bayesian inference and Suppes' theory of probabilistic causation, depicted in graphical structures dubbed Suppes-Bayes Causal Networks (SBCNs). SBCNs are generative probabilistic graphical models that recapitulate the potential ordering of accumulation of such DNA changes during the progression of the disease. Such models can be inferred from data by exploiting likelihood-based model-selection strategies with regularization. In this paper we discuss the theoretical foundations of our approach and we investigate in depth the influence on the model-selection task of: (i) the poset based on Suppes' theory and (ii) different regularization strategies. Furthermore, we provide an example of application of our framework to HIV genetic data highlighting the valuable insights provided by the inferred.\n    ",
        "submission_date": "2016-02-25T00:00:00",
        "last_modified_date": "2018-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07860",
        "title": "Probably Approximately Correct Greedy Maximization with Efficient Bounds on Information Gain for Sensor Selection",
        "authors": [
            "Yash Satsangi",
            "Shimon Whiteson",
            "Frans A. Oliehoek"
        ],
        "abstract": "Submodular function maximization finds application in a variety of real-world decision-making problems. However, most existing methods, based on greedy maximization, assume it is computationally feasible to evaluate F, the function being maximized. Unfortunately, in many realistic settings F is too expensive to evaluate exactly even once. We present probably approximately correct greedy maximization, which requires access only to cheap anytime confidence bounds on F and uses them to prune elements. We show that, with high probability, our method returns an approximately optimal set. We propose novel, cheap confidence bounds for conditional entropy, which appears in many common choices of F and for which it is difficult to find unbiased or bounded estimates. Finally, results on a real-world dataset from a multi-camera tracking system in a shopping mall demonstrate that our approach performs comparably to existing methods, but at a fraction of the computational cost.\n    ",
        "submission_date": "2016-02-25T00:00:00",
        "last_modified_date": "2020-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07970",
        "title": "Causal Discovery from Subsampled Time Series Data by Constraint Optimization",
        "authors": [
            "Antti Hyttinen",
            "Sergey Plis",
            "Matti J\u00e4rvisalo",
            "Frederick Eberhardt",
            "David Danks"
        ],
        "abstract": "This paper focuses on causal structure estimation from time series data in which measurements are obtained at a coarser timescale than the causal timescale of the underlying system. Previous work has shown that such subsampling can lead to significant errors about the system's causal structure if not properly taken into account. In this paper, we first consider the search for the system timescale causal structures that correspond to a given measurement timescale structure. We provide a constraint satisfaction procedure whose computational performance is several orders of magnitude better than previous approaches. We then consider finite-sample data as input, and propose the first constraint optimization approach for recovering the system timescale causal structure. This algorithm optimally recovers from possible conflicts due to statistical errors. More generally, these advances allow for a robust and non-parametric estimation of system timescale causal structures from subsampled time series data.\n    ",
        "submission_date": "2016-02-25T00:00:00",
        "last_modified_date": "2016-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07985",
        "title": "How effective can simple ordinal peer grading be?",
        "authors": [
            "Ioannis Caragiannis",
            "George A. Krimpas",
            "Alexandros A. Voudouris"
        ],
        "abstract": "Ordinal peer grading has been proposed as a simple and scalable solution for computing reliable information about student performance in massive open online courses. The idea is to outsource the grading task to the students themselves as follows. After the end of an exam, each student is asked to rank -- in terms of quality -- a bundle of exam papers by fellow students. An aggregation rule then combines the individual rankings into a global one that contains all students. We define a broad class of simple aggregation rules, which we call type-ordering aggregation rules, and present a theoretical framework for assessing their effectiveness. When statistical information about the grading behaviour of students is available (in terms of a noise matrix that characterizes the grading behaviour of the average student from a student population), the framework can be used to compute the optimal rule from this class with respect to a series of performance objectives that compare the ranking returned by the aggregation rule to the underlying ground truth ranking. For example, a natural rule known as Borda is proved to be optimal when students grade correctly. In addition, we present extensive simulations that validate our theory and prove it to be extremely accurate in predicting the performance of aggregation rules even when only rough information about grading behaviour (i.e., an approximation of the noise matrix) is available. Both in the application of our theoretical framework and in our simulations, we exploit data about grading behaviour of students that have been extracted from two field experiments in the University of Patras.\n    ",
        "submission_date": "2016-02-25T00:00:00",
        "last_modified_date": "2020-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.08017",
        "title": "Meta-learning within Projective Simulation",
        "authors": [
            "Adi Makmal",
            "Alexey A. Melnikov",
            "Vedran Dunjko",
            "Hans J. Briegel"
        ],
        "abstract": "Learning models of artificial intelligence can nowadays perform very well on a large variety of tasks. However, in practice different task environments are best handled by different learning models, rather than a single, universal, approach. Most non-trivial models thus require the adjustment of several to many learning parameters, which is often done on a case-by-case basis by an external party. Meta-learning refers to the ability of an agent to autonomously and dynamically adjust its own learning parameters, or meta-parameters. In this work we show how projective simulation, a recently developed model of artificial intelligence, can naturally be extended to account for meta-learning in reinforcement learning settings. The projective simulation approach is based on a random walk process over a network of clips. The suggested meta-learning scheme builds upon the same design and employs clip networks to monitor the agent's performance and to adjust its meta-parameters \"on the fly\". We distinguish between \"reflexive adaptation\" and \"adaptation through learning\", and show the utility of both approaches. In addition, a trade-off between flexibility and learning-time is addressed. The extended model is examined on three different kinds of reinforcement learning tasks, in which the agent has different optimal values of the meta-parameters, and is shown to perform well, reaching near-optimal to optimal success rates in all of them, without ever needing to manually adjust any meta-parameter.\n    ",
        "submission_date": "2016-02-25T00:00:00",
        "last_modified_date": "2016-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.08313",
        "title": "Enhancing Genetic Algorithms using Multi Mutations",
        "authors": [
            "Ahmad B. A. Hassanat",
            "Esra'a Alkafaween",
            "Nedal A. Al-Nawaiseh",
            "Mohammad A. Abbadi",
            "Mouhammd Alkasassbeh",
            "Mahmoud B. Alhasanat"
        ],
        "abstract": "Mutation is one of the most important stages of the genetic algorithm because of its impact on the exploration of global optima, and to overcome premature convergence. There are many types of mutation, and the problem lies in selection of the appropriate type, where the decision becomes more difficult and needs more trial and error. This paper investigates the use of more than one mutation operator to enhance the performance of genetic algorithms. Novel mutation operators are proposed, in addition to two selection strategies for the mutation operators, one of which is based on selecting the best mutation operator and the other randomly selects any operator. Several experiments on some Travelling Salesman Problems (TSP) were conducted to evaluate the proposed methods, and these were compared to the well-known exchange mutation and rearrangement mutation. The results show the importance of some of the proposed methods, in addition to the significant enhancement of the genetic algorithm's performance, particularly when using more than one mutation operator.\n    ",
        "submission_date": "2016-02-26T00:00:00",
        "last_modified_date": "2018-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.08332",
        "title": "Bounded Rational Decision-Making in Feedforward Neural Networks",
        "authors": [
            "Felix Leibfried",
            "Daniel Alexander Braun"
        ],
        "abstract": "Bounded rational decision-makers transform sensory input into motor output under limited computational resources. Mathematically, such decision-makers can be modeled as information-theoretic channels with limited transmission rate. Here, we apply this formalism for the first time to multilayer feedforward neural networks. We derive synaptic weight update rules for two scenarios, where either each neuron is considered as a bounded rational decision-maker or the network as a whole. In the update rules, bounded rationality translates into information-theoretically motivated types of regularization in weight space. In experiments on the MNIST benchmark classification task for handwritten digits, we show that such information-theoretic regularization successfully prevents overfitting across different architectures and attains results that are competitive with other recent techniques like dropout, dropconnect and Bayes by backprop, for both ordinary and convolutional neural networks.\n    ",
        "submission_date": "2016-02-26T00:00:00",
        "last_modified_date": "2016-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.08447",
        "title": "A Neutrosophic Recommender System for Medical Diagnosis Based on Algebraic Neutrosophic Measures",
        "authors": [
            "Mumtaz Ali",
            "Nguyen Van Minh",
            "Le Hoang Son"
        ],
        "abstract": "Neutrosophic set has the ability to handle uncertain, incomplete, inconsistent, indeterminate information in a more accurate way. In this paper, we proposed a neutrosophic recommender system to predict the diseases based on neutrosophic set which includes single-criterion neutrosophic recommender system (SC-NRS) and multi-criterion neutrosophic recommender system (MC-NRS). Further, we investigated some algebraic operations of neutrosophic recommender system such as union, complement, intersection, probabilistic sum, bold sum, bold intersection, bounded difference, symmetric difference, convex linear sum of min and max operators, Cartesian product, associativity, commutativity and distributive. Based on these operations, we studied the algebraic structures such as lattices, Kleen algebra, de Morgan algebra, Brouwerian algebra, BCK algebra, Stone algebra and MV algebra. In addition, we introduced several types of similarity measures based on these algebraic operations and studied some of their theoretic properties. Moreover, we accomplished a prediction formula using the proposed algebraic similarity measure. We also proposed a new algorithm for medical diagnosis based on neutrosophic recommender system. Finally to check the validity of the proposed methodology, we made experiments on the datasets Heart, RHC, Breast cancer, Diabetes and DMD. At the end, we presented the MSE and computational time by comparing the proposed algorithm with the relevant ones such as ICSM, DSM, CARE, CFMD, as well as other variants namely Variant 67, Variant 69, and Varian 71 both in tabular and graphical form to analyze the efficiency and accuracy. Finally we analyzed the strength of all 8 algorithms by ANOVA statistical tool.\n    ",
        "submission_date": "2016-02-25T00:00:00",
        "last_modified_date": "2016-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.08571",
        "title": "Towards Neural Knowledge DNA",
        "authors": [
            "Haoxi Zhang",
            "Cesar Sanin",
            "Edward Szczerbicki"
        ],
        "abstract": "In this paper, we propose the Neural Knowledge DNA, a framework that tailors the ideas underlying the success of neural networks to the scope of knowledge representation. Knowledge representation is a fundamental field that dedicate to representing information about the world in a form that computer systems can utilize to solve complex tasks. The proposed Neural Knowledge DNA is designed to support discovering, storing, reusing, improving, and sharing knowledge among machines and organisation. It is constructed in a similar fashion of how DNA formed: built up by four essential elements. As the DNA produces phenotypes, the Neural Knowledge DNA carries information and knowledge via its four essential elements, namely, Networks, Experiences, States, and Actions.\n    ",
        "submission_date": "2016-02-27T00:00:00",
        "last_modified_date": "2016-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.08610",
        "title": "Scalable Bayesian Rule Lists",
        "authors": [
            "Hongyu Yang",
            "Cynthia Rudin",
            "Margo Seltzer"
        ],
        "abstract": "We present an algorithm for building probabilistic rule lists that is two orders of magnitude faster than previous work. Rule list algorithms are competitors for decision tree algorithms. They are associative classifiers, in that they are built from pre-mined association rules. They have a logical structure that is a sequence of IF-THEN rules, identical to a decision list or one-sided decision tree. Instead of using greedy splitting and pruning like decision tree algorithms, we fully optimize over rule lists, striking a practical balance between accuracy, interpretability, and computational speed. The algorithm presented here uses a mixture of theoretical bounds (tight enough to have practical implications as a screening or bounding procedure), computational reuse, and highly tuned language libraries to achieve computational efficiency. Currently, for many practical problems, this method achieves better accuracy and sparsity than decision trees; further, in many cases, the computational time is practical and often less than that of decision trees. The result is a probabilistic classifier (which estimates P(y = 1|x) for each x) that optimizes the posterior of a Bayesian hierarchical model over rule lists.\n    ",
        "submission_date": "2016-02-27T00:00:00",
        "last_modified_date": "2017-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.09076",
        "title": "Personalized and situation-aware multimodal route recommendations: the FAVOUR algorithm",
        "authors": [
            "Paolo Campigotto",
            "Christian Rudloff",
            "Maximilian Leodolter",
            "Dietmar Bauer"
        ],
        "abstract": "Route choice in multimodal networks shows a considerable variation between different individuals as well as the current situational context. Personalization of recommendation algorithms are already common in many areas, e.g., online retail. However, most online routing applications still provide shortest distance or shortest travel-time routes only, neglecting individual preferences as well as the current situation. Both aspects are of particular importance in a multimodal setting as attractivity of some transportation modes such as biking crucially depends on personal characteristics and exogenous factors like the weather. This paper introduces the FAVourite rOUte Recommendation (FAVOUR) approach to provide personalized, situation-aware route proposals based on three steps: first, at the initialization stage, the user provides limited information (home location, work place, mobility options, sociodemographics) used to select one out of a small number of initial profiles. Second, based on this information, a stated preference survey is designed in order to sharpen the profile. In this step a mass preference prior is used to encode the prior knowledge on preferences from the class identified in step one. And third, subsequently the profile is continuously updated during usage of the routing services. The last two steps use Bayesian learning techniques in order to incorporate information from all contributing individuals. The FAVOUR approach is presented in detail and tested on a small number of survey participants. The experimental results on this real-world dataset show that FAVOUR generates better-quality recommendations w.r.t. alternative learning algorithms from the literature. In particular the definition of the mass preference prior for initialization of step two is shown to provide better predictions than a number of alternatives from the literature.\n    ",
        "submission_date": "2016-02-29T00:00:00",
        "last_modified_date": "2016-02-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.00423",
        "title": "Quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive LSTMs",
        "authors": [
            "Phong Le",
            "Willem Zuidema"
        ],
        "abstract": "Recursive neural networks (RNN) and their recently proposed extension recursive long short term memory networks (RLSTM) are models that compute representations for sentences, by recursively combining word embeddings according to an externally provided parse tree. Both models thus, unlike recurrent networks, explicitly make use of the hierarchical structure of a sentence. In this paper, we demonstrate that RNNs nevertheless suffer from the vanishing gradient and long distance dependency problem, and that RLSTMs greatly improve over RNN's on these problems. We present an artificial learning task that allows us to quantify the severity of these problems for both models. We further show that a ratio of gradients (at the root node and a focal leaf node) is highly indicative of the success of backpropagation at optimizing the relevant weights low in the tree. This paper thus provides an explanation for existing, superior results of RLSTMs on tasks such as sentiment analysis, and suggests that the benefits of including hierarchical structure and of including LSTM-style gating are complementary.\n    ",
        "submission_date": "2016-03-01T00:00:00",
        "last_modified_date": "2016-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.00772",
        "title": "Filter based Taxonomy Modification for Improving Hierarchical Classification",
        "authors": [
            "Azad Naik",
            "Huzefa Rangwala"
        ],
        "abstract": "Hierarchical Classification (HC) is a supervised learning problem where unlabeled instances are classified into a taxonomy of classes. Several methods that utilize the hierarchical structure have been developed to improve the HC performance. However, in most cases apriori defined hierarchical structure by domain experts is inconsistent; as a consequence performance improvement is not noticeable in comparison to flat classification methods. We propose a scalable data-driven filter based rewiring approach to modify an expert-defined hierarchy. Experimental comparisons of top-down HC with our modified hierarchy, on a wide range of datasets shows classification performance improvement over the baseline hierarchy (i:e:, defined by expert), clustered hierarchy and flattening based hierarchy modification approaches. In comparison to existing rewiring approaches, our developed method (rewHier) is computationally efficient, enabling it to scale to datasets with large numbers of classes, instances and features. We also show that our modified hierarchy leads to improved classification performance for classes with few training samples in comparison to flat and state-of-the-art HC approaches.\n    ",
        "submission_date": "2016-03-02T00:00:00",
        "last_modified_date": "2016-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01182",
        "title": "Network Unfolding Map by Edge Dynamics Modeling",
        "authors": [
            "Filipe Alves Neto Verri",
            "Paulo Roberto Urio",
            "Liang Zhao"
        ],
        "abstract": "The emergence of collective dynamics in neural networks is a mechanism of the animal and human brain for information processing. In this paper, we develop a computational technique using distributed processing elements in a complex network, which are called particles, to solve semi-supervised learning problems. Three actions govern the particles' dynamics: generation, walking, and absorption. Labeled vertices generate new particles that compete against rival particles for edge domination. Active particles randomly walk in the network until they are absorbed by either a rival vertex or an edge currently dominated by rival particles. The result from the model evolution consists of sets of edges arranged by the label dominance. Each set tends to form a connected subnetwork to represent a data class. Although the intrinsic dynamics of the model is a stochastic one, we prove there exists a deterministic version with largely reduced computational complexity; specifically, with linear growth. Furthermore, the edge domination process corresponds to an unfolding map in such way that edges \"stretch\" and \"shrink\" according to the vertex-edge dynamics. Consequently, the unfolding effect summarizes the relevant relationships between vertices and the uncovered data classes. The proposed model captures important details of connectivity patterns over the vertex-edge dynamics evolution, in contrast to previous approaches which focused on only vertex or only edge dynamics. Computer simulations reveal that the new model can identify nonlinear features in both real and artificial data, including boundaries between distinct classes and overlapping structures of data.\n    ",
        "submission_date": "2016-03-03T00:00:00",
        "last_modified_date": "2018-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01228",
        "title": "GeoGebra Tools with Proof Capabilities",
        "authors": [
            "Zolt\u00e1n Kov\u00e1cs",
            "Csilla S\u00f3lyom-Gecse"
        ],
        "abstract": "We report about significant enhancements of the complex algebraic geometry theorem proving subsystem in GeoGebra for automated proofs in Euclidean geometry, concerning the extension of numerous GeoGebra tools with proof capabilities. As a result, a number of elementary theorems can be proven by using GeoGebra's intuitive user interface on various computer architectures including native Java and web based systems with JavaScript. We also provide a test suite for benchmarking our results with 200 test cases.\n    ",
        "submission_date": "2016-03-03T00:00:00",
        "last_modified_date": "2016-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01312",
        "title": "Learning Physical Intuition of Block Towers by Example",
        "authors": [
            "Adam Lerer",
            "Sam Gross",
            "Rob Fergus"
        ],
        "abstract": "Wooden blocks are a common toy for infants, allowing them to develop motor skills and gain intuition about the physical behavior of the world. In this paper, we explore the ability of deep feed-forward models to learn such intuitive physics. Using a 3D game engine, we create small towers of wooden blocks whose stability is randomized and render them collapsing (or remaining upright). This data allows us to train large convolutional network models which can accurately predict the outcome, as well as estimating the block trajectories. The models are also able to generalize in two important ways: (i) to new physical scenarios, e.g. towers with an additional block and (ii) to images of real wooden blocks, where it obtains a performance comparable to human subjects.\n    ",
        "submission_date": "2016-03-03T00:00:00",
        "last_modified_date": "2016-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01488",
        "title": "A knowledge representation meta-model for rule-based modelling of signalling networks",
        "authors": [
            "Adrien Basso-Blandin",
            "Walter Fontana",
            "Russ Harmer"
        ],
        "abstract": "  The study of cellular signalling pathways and their deregulation in disease states, such as cancer, is a large and extremely complex task. Indeed, these systems involve many parts and processes but are studied piecewise and their literatures and data are consequently fragmented, distributed and sometimes--at least apparently--inconsistent. This makes it extremely difficult to build significant explanatory models with the result that effects in these systems that are brought about by many interacting factors are poorly understood.\n",
        "submission_date": "2016-03-03T00:00:00",
        "last_modified_date": "2016-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01581",
        "title": "Causal inference for data-driven debugging and decision making in cloud computing",
        "authors": [
            "Philipp Geiger",
            "Lucian Carata",
            "Bernhard Schoelkopf"
        ],
        "abstract": "Cloud computing involves complex technical and economical systems and interactions. This brings about various challenges, two of which are: (1) debugging and control to optimize the performance of computing systems, with the help of sandbox experiments, and (2) privacy-preserving prediction of the cost of ``spot'' resources for decision making of cloud clients. In this paper, we formalize debugging by counterfactual probabilities and control by post-(soft-)interventional probabilities. We prove that counterfactuals can approximately be calculated from a ``stochastic'' graphical causal model (while they are originally defined only for ``deterministic'' functional causal models), and based on this sketch a data-driven approach to address problem (1). To address problem (2), we formalize bidding by post-(soft-)interventional probabilities and present a simple mathematical result on approximate integration of ``incomplete'' conditional probability distributions. We show how this can be used by cloud clients to trade off privacy against predictability of the outcome of their bidding actions in a toy scenario. We report experiments on simulated and real data.\n    ",
        "submission_date": "2016-03-04T00:00:00",
        "last_modified_date": "2020-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01722",
        "title": "A Linked Data Scalability Challenge: Concept Reuse Leads to Semantic Decay",
        "authors": [
            "Paolo Pareti",
            "Ewan Klein",
            "Adam Barker"
        ],
        "abstract": "The increasing amount of available Linked Data resources is laying the foundations for more advanced Semantic Web applications. One of their main limitations, however, remains the general low level of data quality. In this paper we focus on a measure of quality which is negatively affected by the increase of the available resources. We propose a measure of semantic richness of Linked Data concepts and we demonstrate our hypothesis that the more a concept is reused, the less semantically rich it becomes. This is a significant scalability issue, as one of the core aspects of Linked Data is the propagation of semantic information on the Web by reusing common terms. We prove our hypothesis with respect to our measure of semantic richness and we validate our model empirically. Finally, we suggest possible future directions to address this scalability problem.\n    ",
        "submission_date": "2016-03-05T00:00:00",
        "last_modified_date": "2016-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01840",
        "title": "Hierarchical Decision Making In Electricity Grid Management",
        "authors": [
            "Gal Dalal",
            "Elad Gilboa",
            "Shie Mannor"
        ],
        "abstract": "The power grid is a complex and vital system that necessitates careful reliability management. Managing the grid is a difficult problem with multiple time scales of decision making and stochastic behavior due to renewable energy generations, variable demand and unplanned outages. Solving this problem in the face of uncertainty requires a new methodology with tractable algorithms. In this work, we introduce a new model for hierarchical decision making in complex systems. We apply reinforcement learning (RL) methods to learn a proxy, i.e., a level of abstraction, for real-time power grid reliability. We devise an algorithm that alternates between slow time-scale policy improvement, and fast time-scale value function approximation. We compare our results to prevailing heuristics, and show the strength of our method.\n    ",
        "submission_date": "2016-03-06T00:00:00",
        "last_modified_date": "2016-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.02041",
        "title": "Learning Shared Representations in Multi-task Reinforcement Learning",
        "authors": [
            "Diana Borsa",
            "Thore Graepel",
            "John Shawe-Taylor"
        ],
        "abstract": "We investigate a paradigm in multi-task reinforcement learning (MT-RL) in which an agent is placed in an environment and needs to learn to perform a series of tasks, within this space. Since the environment does not change, there is potentially a lot of common ground amongst tasks and learning to solve them individually seems extremely wasteful. In this paper, we explicitly model and learn this shared structure as it arises in the state-action value space. We will show how one can jointly learn optimal value-functions by modifying the popular Value-Iteration and Policy-Iteration procedures to accommodate this shared representation assumption and leverage the power of multi-task supervised learning. Finally, we demonstrate that the proposed model and training procedures, are able to infer good value functions, even under low samples regimes. In addition to data efficiency, we will show in our analysis, that learning abstractions of the state space jointly across tasks leads to more robust, transferable representations with the potential for better generalization. this shared representation assumption and leverage the power of multi-task supervised learning. Finally, we demonstrate that the proposed model and training procedures, are able to infer good value functions, even under low samples regimes. In addition to data efficiency, we will show in our analysis, that learning abstractions of the state space jointly across tasks leads to more robust, transferable representations with the potential for better generalization.\n    ",
        "submission_date": "2016-03-07T00:00:00",
        "last_modified_date": "2016-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.02208",
        "title": "An Online Mechanism for Ridesharing in Autonomous Mobility-on-Demand Systems",
        "authors": [
            "Wen Shen",
            "Cristina V. Lopes",
            "Jacob W. Crandall"
        ],
        "abstract": "With proper management, Autonomous Mobility-on-Demand (AMoD) systems have great potential to satisfy the transport demands of urban populations by providing safe, convenient, and affordable ridesharing services. Meanwhile, such systems can substantially decrease private car ownership and use, and thus significantly reduce traffic congestion, energy consumption, and carbon emissions. To achieve this objective, an AMoD system requires private information about the demand from passengers. However, due to self-interestedness, passengers are unlikely to cooperate with the service providers in this regard. Therefore, an online mechanism is desirable if it incentivizes passengers to truthfully report their actual demand. For the purpose of promoting ridesharing, we hereby introduce a posted-price, integrated online ridesharing mechanism (IORS) that satisfies desirable properties such as ex-post incentive compatibility, individual rationality, and budget-balance. Numerical results indicate the competitiveness of IORS compared with two benchmarks, namely the optimal assignment and an offline, auction-based mechanism.\n    ",
        "submission_date": "2016-03-07T00:00:00",
        "last_modified_date": "2017-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.02738",
        "title": "Learning to Blend Computer Game Levels",
        "authors": [
            "Matthew Guzdial",
            "Mark Riedl"
        ],
        "abstract": "We present an approach to generate novel computer game levels that blend different game concepts in an unsupervised fashion. Our primary contribution is an analogical reasoning process to construct blends between level design models learned from gameplay videos. The models represent probabilistic relationships between elements in the game. An analogical reasoning process maps features between two models to produce blended models that can then generate new level chunks. As a proof-of-concept we train our system on the classic platformer game Super Mario Bros. due to its highly-regarded and well understood level design. We evaluate the extent to which the models represent stylistic level design knowledge and demonstrate the ability of our system to explain levels that were blended by human expert designers.\n    ",
        "submission_date": "2016-03-08T00:00:00",
        "last_modified_date": "2016-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03181",
        "title": "Inferring Fine-grained Details on User Activities and Home Location from Social Media: Detecting Drinking-While-Tweeting Patterns in Communities",
        "authors": [
            "Nabil Hossain",
            "Tianran Hu",
            "Roghayeh Feizi",
            "Ann Marie White",
            "Jiebo Luo",
            "Henry Kautz"
        ],
        "abstract": "Nearly all previous work on geo-locating latent states and activities from social media confounds general discussions about activities, self-reports of users participating in those activities at times in the past or future, and self-reports made at the immediate time and place the activity occurs. Activities, such as alcohol consumption, may occur at different places and types of places, and it is important not only to detect the local regions where these activities occur, but also to analyze the degree of participation in them by local residents. In this paper, we develop new machine learning based methods for fine-grained localization of activities and home locations from Twitter data. We apply these methods to discover and compare alcohol consumption patterns in a large urban area, New York City, and a more suburban and rural area, Monroe County. We find positive correlations between the rate of alcohol consumption reported among a community's Twitter users and the density of alcohol outlets, demonstrating that the degree of correlation varies significantly between urban and suburban areas. While our experiments are focused on alcohol use, our methods for locating homes and distinguishing temporally-specific self-reports are applicable to a broad range of behaviors and latent states.\n    ",
        "submission_date": "2016-03-10T00:00:00",
        "last_modified_date": "2016-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03267",
        "title": "Hierarchical Linearly-Solvable Markov Decision Problems",
        "authors": [
            "Anders Jonsson",
            "Vicen\u00e7 G\u00f3mez"
        ],
        "abstract": "We present a hierarchical reinforcement learning framework that formulates each task in the hierarchy as a special type of Markov decision process for which the Bellman equation is linear and has analytical solution. Problems of this type, called linearly-solvable MDPs (LMDPs) have interesting properties that can be exploited in a hierarchical setting, such as efficient learning of the optimal value function or task compositionality. The proposed hierarchical approach can also be seen as a novel alternative to solving LMDPs with large state spaces. We derive a hierarchical version of the so-called Z-learning algorithm that learns different tasks simultaneously and show empirically that it significantly outperforms the state-of-the-art learning methods in two classical hierarchical reinforcement learning domains: the taxi domain and an autonomous guided vehicle task.\n    ",
        "submission_date": "2016-03-10T00:00:00",
        "last_modified_date": "2016-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03511",
        "title": "A Set Theoretic Approach for Knowledge Representation: the Representation Part",
        "authors": [
            "Yi Zhou"
        ],
        "abstract": "In this paper, we propose a set theoretic approach for knowledge representation. While the syntax of an application domain is captured by set theoretic constructs including individuals, concepts and operators, knowledge is formalized by equality assertions. We first present a primitive form that uses minimal assumed knowledge and constructs. Then, assuming naive set theory, we extend it by definitions, which are special kinds of knowledge. Interestingly, we show that the primitive form is expressive enough to define logic operators, not only propositional connectives but also quantifiers.\n    ",
        "submission_date": "2016-03-11T00:00:00",
        "last_modified_date": "2016-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03515",
        "title": "Near-Optimal Active Learning of Halfspaces via Query Synthesis in the Noisy Setting",
        "authors": [
            "Lin Chen",
            "Hamed Hassani",
            "Amin Karbasi"
        ],
        "abstract": "In this paper, we consider the problem of actively learning a linear classifier through query synthesis where the learner can construct artificial queries in order to estimate the true decision boundaries. This problem has recently gained a lot of interest in automated science and adversarial reverse engineering for which only heuristic algorithms are known. In such applications, queries can be constructed de novo to elicit information (e.g., automated science) or to evade detection with minimal cost (e.g., adversarial reverse engineering). We develop a general framework, called dimension coupling (DC), that 1) reduces a d-dimensional learning problem to d-1 low dimensional sub-problems, 2) solves each sub-problem efficiently, 3) appropriately aggregates the results and outputs a linear classifier, and 4) provides a theoretical guarantee for all possible schemes of aggregation. The proposed method is proved resilient to noise. We show that the DC framework avoids the curse of dimensionality: its computational complexity scales linearly with the dimension. Moreover, we show that the query complexity of DC is near optimal (within a constant factor of the optimum algorithm). To further support our theoretical analysis, we compare the performance of DC with the existing work. We observe that DC consistently outperforms the prior arts in terms of query complexity while often running orders of magnitude faster.\n    ",
        "submission_date": "2016-03-11T00:00:00",
        "last_modified_date": "2016-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03518",
        "title": "High-dimensional Black-box Optimization via Divide and Approximate Conquer",
        "authors": [
            "Peng Yang",
            "Ke Tang",
            "Xin Yao"
        ],
        "abstract": "Divide and Conquer (DC) is conceptually well suited to high-dimensional optimization by decomposing a problem into multiple small-scale sub-problems. However, appealing performance can be seldom observed when the sub-problems are interdependent. This paper suggests that the major difficulty of tackling interdependent sub-problems lies in the precise evaluation of a partial solution (to a sub-problem), which can be overwhelmingly costly and thus makes sub-problems non-trivial to conquer. Thus, we propose an approximation approach, named Divide and Approximate Conquer (DAC), which reduces the cost of partial solution evaluation from exponential time to polynomial time. Meanwhile, the convergence to the global optimum (of the original problem) is still guaranteed. The effectiveness of DAC is demonstrated empirically on two sets of non-separable high-dimensional problems.\n    ",
        "submission_date": "2016-03-11T00:00:00",
        "last_modified_date": "2016-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03729",
        "title": "Penta and Hexa Valued Representation of Neutrosophic Information",
        "authors": [
            "Vasile Patrascu"
        ],
        "abstract": "Starting from the primary representation of neutrosophic information, namely the degree of truth, degree of indeterminacy and degree of falsity, we define a nuanced representation in a penta valued fuzzy space, described by the index of truth, index of falsity, index of ignorance, index of contradiction and index of hesitation. Also, it was constructed an associated penta valued logic and then using this logic, it was defined for the proposed penta valued structure the following operators: union, intersection, negation, complement and dual. Then, the penta valued representation is extended to a hexa valued one, adding the sixth component, namely the index of ambiguity.\n    ",
        "submission_date": "2016-03-10T00:00:00",
        "last_modified_date": "2016-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03814",
        "title": "Solving MaxSAT by Successive Calls to a SAT Solver",
        "authors": [
            "Mohamed El Halaby"
        ],
        "abstract": "The Maximum Satisfiability (MaxSAT) problem is the problem of finding a truth assignment that maximizes the number of satisfied clauses of a given Boolean formula in Conjunctive Normal Form (CNF). Many exact solvers for MaxSAT have been developed during recent years, and many of them were presented in the well-known SAT conference. Algorithms for MaxSAT generally fall into two categories: (1) branch and bound algorithms and (2) algorithms that use successive calls to a SAT solver (SAT- based), which this paper in on. In practical problems, SAT-based algorithms have been shown to be more efficient. This paper provides an experimental investigation to compare the performance of recent SAT-based and branch and bound algorithms on the benchmarks of the MaxSAT Evaluations.\n    ",
        "submission_date": "2016-03-11T00:00:00",
        "last_modified_date": "2016-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03884",
        "title": "Grounding Recursive Aggregates: Preliminary Report",
        "authors": [
            "Martin Gebser",
            "Roland Kaminski",
            "Torsten Schaub"
        ],
        "abstract": "Problem solving in Answer Set Programming consists of two steps, a first grounding phase, systematically replacing all variables by terms, and a second solving phase computing the stable models of the obtained ground program. An intricate part of both phases is the treatment of aggregates, which are popular language constructs that allow for expressing properties over sets. In this paper, we elaborate upon the treatment of aggregates during grounding in Gringo series 4. Consequently, our approach is applicable to grounding based on semi-naive database evaluation techniques. In particular, we provide a series of algorithms detailing the treatment of recursive aggregates and illustrate this by a running example.\n    ",
        "submission_date": "2016-03-12T00:00:00",
        "last_modified_date": "2016-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.04110",
        "title": "Geometry of Interest (GOI): Spatio-Temporal Destination Extraction and Partitioning in GPS Trajectory Data",
        "authors": [
            "Seyed Morteza Mousavi",
            "Aaron Harwood",
            "Shanika Karunasekera",
            "Mojtaba Maghrebi"
        ],
        "abstract": "Nowadays large amounts of GPS trajectory data is being continuously collected by GPS-enabled devices such as vehicles navigation systems and mobile phones. GPS trajectory data is useful for applications such as traffic management, location forecasting, and itinerary planning. Such applications often need to extract the time-stamped Sequence of Visited Locations (SVLs) of the mobile objects. The nearest neighbor query (NNQ) is the most applied method for labeling the visited locations based on the IDs of the POIs in the process of SVL generation. NNQ in some scenarios is not accurate enough. To improve the quality of the extracted SVLs, instead of using NNQ, we label the visited locations as the IDs of the POIs which geometrically intersect with the GPS observations. Intersection operator requires the accurate geometry of the points of interest which we refer to them as the Geometries of Interest (GOIs). In some application domains (e.g. movement trajectories of animals), adequate information about the POIs and their GOIs may not be available a priori, or they may not be publicly accessible and, therefore, they need to be derived from GPS trajectory data. In this paper we propose a novel method for estimating the POIs and their GOIs, which consists of three phases: (i) extracting the geometries of the stay regions; (ii) constructing the geometry of destination regions based on the extracted stay regions; and (iii) constructing the GOIs based on the geometries of the destination regions. Using the geometric similarity to known GOIs as the major evaluation criterion, the experiments we performed using long-term GPS trajectory data show that our method outperforms the existing approaches.\n    ",
        "submission_date": "2016-03-14T00:00:00",
        "last_modified_date": "2016-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.04119",
        "title": "Exploratory Gradient Boosting for Reinforcement Learning in Complex Domains",
        "authors": [
            "David Abel",
            "Alekh Agarwal",
            "Fernando Diaz",
            "Akshay Krishnamurthy",
            "Robert E. Schapire"
        ],
        "abstract": "High-dimensional observations and complex real-world dynamics present major challenges in reinforcement learning for both function approximation and exploration. We address both of these challenges with two complementary techniques: First, we develop a gradient-boosting style, non-parametric function approximator for learning on $Q$-function residuals. And second, we propose an exploration strategy inspired by the principles of state abstraction and information acquisition under uncertainty. We demonstrate the empirical effectiveness of these techniques, first, as a preliminary check, on two standard tasks (Blackjack and $n$-Chain), and then on two much larger and more realistic tasks with high-dimensional observation spaces. Specifically, we introduce two benchmarks built within the game Minecraft where the observations are pixel arrays of the agent's visual field. A combination of our two algorithmic techniques performs competitively on the standard reinforcement-learning tasks while consistently and substantially outperforming baselines on the two tasks with high-dimensional observation spaces. The new function approximator, exploration strategy, and evaluation benchmarks are each of independent interest in the pursuit of reinforcement-learning methods that scale to real-world domains.\n    ",
        "submission_date": "2016-03-14T00:00:00",
        "last_modified_date": "2016-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.04402",
        "title": "Controlling Search in Very large Commonsense Knowledge Bases: A Machine Learning Approach",
        "authors": [
            "Abhishek Sharma",
            "Michael Witbrock",
            "Keith Goolsbey"
        ],
        "abstract": "Very large commonsense knowledge bases (KBs) often have thousands to millions of axioms, of which relatively few are relevant for answering any given query. A large number of irrelevant axioms can easily overwhelm resolution-based theorem provers. Therefore, methods that help the reasoner identify useful inference paths form an essential part of large-scale reasoning systems. In this paper, we describe two ordering heuristics for optimization of reasoning in such systems. First, we discuss how decision trees can be used to select inference steps that are more likely to succeed. Second, we identify a small set of problem instance features that suffice to guide searches away from intractable regions of the search space. We show the efficacy of these techniques via experiments on thousands of queries from the Cyc KB. Results show that these methods lead to an order of magnitude reduction in inference time.\n    ",
        "submission_date": "2016-03-14T00:00:00",
        "last_modified_date": "2016-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.04586",
        "title": "Optimal Sensing via Multi-armed Bandit Relaxations in Mixed Observability Domains",
        "authors": [
            "Mikko Lauri",
            "Risto Ritala"
        ],
        "abstract": "Sequential decision making under uncertainty is studied in a mixed observability domain. The goal is to maximize the amount of information obtained on a partially observable stochastic process under constraints imposed by a fully observable internal state. An upper bound for the optimal value function is derived by relaxing constraints. We identify conditions under which the relaxed problem is a multi-armed bandit whose optimal policy is easily computable. The upper bound is applied to prune the search space in the original problem, and the effect on solution quality is assessed via simulation experiments. Empirical results show effective pruning of the search space in a target monitoring domain.\n    ",
        "submission_date": "2016-03-15T00:00:00",
        "last_modified_date": "2016-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.05314",
        "title": "Hardware Acceleration for Boolean Satisfiability Solver by Applying Belief Propagation Algorithm",
        "authors": [
            "Te-Hsuan Chen",
            "Ju-Yi Lu"
        ],
        "abstract": "Boolean satisfiability (SAT) has an extensive application domain in computer science, especially in electronic design automation applications. Circuit synthesis, optimization, and verification problems can be solved by transforming original problems to SAT problems. However, the SAT problem is known as NP-complete, which means there is no efficient method to solve it. Therefore, an efficient SAT solver to enhance the performance is always desired. We propose a hardware acceleration method for SAT problems. By surveying the properties of SAT problems and the decoding of low-density parity-check (LDPC) codes, a special class of error-correcting codes, we discover that both of them are constraint satisfaction problems. The belief propagation algorithm has been successfully applied to the decoding of LDPC, and the corresponding decoder hardware designs are extensively studied. Therefore, we proposed a belief propagation based algorithm to solve SAT problems. With this algorithm, the SAT solver can be accelerated by hardware. A software simulator is implemented to verify the proposed algorithm and the performance improvement is estimated. Our experiment results show that time complexity does not increase with the size of SAT problems and the proposed method can achieve at least 30x speedup compared to MiniSat.\n    ",
        "submission_date": "2016-03-16T00:00:00",
        "last_modified_date": "2016-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06125",
        "title": "The Computational Power of Dynamic Bayesian Networks",
        "authors": [
            "Joshua Brul\u00e9"
        ],
        "abstract": "This paper considers the computational power of constant size, dynamic Bayesian networks. Although discrete dynamic Bayesian networks are no more powerful than hidden Markov models, dynamic Bayesian networks with continuous random variables and discrete children of continuous parents are capable of performing Turing-complete computation. With modified versions of existing algorithms for belief propagation, such a simulation can be carried out in real time. This result suggests that dynamic Bayesian networks may be more powerful than previously considered. Relationships to causal models and recurrent neural networks are also discussed.\n    ",
        "submission_date": "2016-03-19T00:00:00",
        "last_modified_date": "2016-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06141",
        "title": "Evolving Shepherding Behavior with Genetic Programming Algorithms",
        "authors": [
            "Joshua Brul\u00e9",
            "Kevin Engel",
            "Nick Fung",
            "Isaac Julien"
        ],
        "abstract": "We apply genetic programming techniques to the `shepherding' problem, in which a group of one type of animal (sheep dogs) attempts to control the movements of a second group of animals (sheep) obeying flocking behavior. Our genetic programming algorithm evolves an expression tree that governs the movements of each dog. The operands of the tree are hand-selected features of the simulation environment that may allow the dogs to herd the sheep effectively. The algorithm uses tournament-style selection, crossover reproduction, and a point mutation. We find that the evolved solutions generalize well and outperform a (naive) human-designed algorithm.\n    ",
        "submission_date": "2016-03-19T00:00:00",
        "last_modified_date": "2016-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06459",
        "title": "Characterization of neighborhood behaviours in a multi-neighborhood local search algorithm",
        "authors": [
            "Nguyen Thi Thanh Dang",
            "Patrick De Causmaecker"
        ],
        "abstract": "We consider a multi-neighborhood local search algorithm with a large number of possible neighborhoods. Each neighborhood is accompanied by a weight value which represents the probability of being chosen at each iteration. These weights are fixed before the algorithm runs, and are considered as parameters of the algorithm. Given a set of instances, off-line tuning of the algorithm's parameters can be done by automated algorithm configuration tools (e.g., SMAC). However, the large number of neighborhoods can make the tuning expensive and difficult even when the number of parameters has been reduced by some intuition. In this work, we propose a systematic method to characterize each neighborhood's behaviours, representing them as a feature vector, and using cluster analysis to form similar groups of neighborhoods. The novelty of our characterization method is the ability of reflecting changes of behaviours according to hardness of different solution quality regions. We show that using neighborhood clusters instead of individual neighborhoods helps to reduce the parameter configuration space without misleading the search of the tuning procedure. Moreover, this method is problem-independent and potentially can be applied in similar contexts.\n    ",
        "submission_date": "2016-03-12T00:00:00",
        "last_modified_date": "2016-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06485",
        "title": "A System for Probabilistic Linking of Thesauri and Classification Systems",
        "authors": [
            "Lisa Posch",
            "Philipp Schaer",
            "Arnim Bleier",
            "Markus Strohmaier"
        ],
        "abstract": "This paper presents a system which creates and visualizes probabilistic semantic links between concepts in a thesaurus and classes in a classification system. For creating the links, we build on the Polylingual Labeled Topic Model (PLL-TM). PLL-TM identifies probable thesaurus descriptors for each class in the classification system by using information from the natural language text of documents, their assigned thesaurus descriptors and their designated classes. The links are then presented to users of the system in an interactive visualization, providing them with an automatically generated overview of the relations between the thesaurus and the classification system.\n    ",
        "submission_date": "2016-03-21T00:00:00",
        "last_modified_date": "2016-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07029",
        "title": "Comparing Human and Automated Evaluation of Open-Ended Student Responses to Questions of Evolution",
        "authors": [
            "Michael J Wiser",
            "Louise S Mead",
            "James J Smith",
            "Robert T Pennock"
        ],
        "abstract": "Written responses can provide a wealth of data in understanding student reasoning on a topic. Yet they are time- and labor-intensive to score, requiring many instructors to forego them except as limited parts of summative assessments at the end of a unit or course. Recent developments in Machine Learning (ML) have produced computational methods of scoring written responses for the presence or absence of specific concepts. Here, we compare the scores from one particular ML program -- EvoGrader -- to human scoring of responses to structurally- and content-similar questions that are distinct from the ones the program was trained on. We find that there is substantial inter-rater reliability between the human and ML scoring. However, sufficient systematic differences remain between the human and ML scoring that we advise only using the ML scoring for formative, rather than summative, assessment of student reasoning.\n    ",
        "submission_date": "2016-03-22T00:00:00",
        "last_modified_date": "2016-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07051",
        "title": "Cosolver2B: An Efficient Local Search Heuristic for the Travelling Thief Problem",
        "authors": [
            "Mohamed El Yafrani",
            "Bela\u00efd Ahiod"
        ],
        "abstract": "Real-world problems are very difficult to optimize. However, many researchers have been solving benchmark problems that have been extensively investigated for the last decades even if they have very few direct applications. The Traveling Thief Problem (TTP) is a NP-hard optimization problem that aims to provide a more realistic model. TTP targets particularly routing problem under packing/loading constraints which can be found in supply chain management and transportation. In this paper, TTP is presented and formulated mathematically. A combined local search algorithm is proposed and compared with Random Local Search (RLS) and Evolutionary Algorithm (EA). The obtained results are quite promising since new better solutions were found.\n    ",
        "submission_date": "2016-03-23T00:00:00",
        "last_modified_date": "2016-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07417",
        "title": "Load Disaggregation Based on Aided Linear Integer Programming",
        "authors": [
            "Md. Zulfiquar Ali Bhotto",
            "Stephen Makonin",
            "Ivan V. Bajic"
        ],
        "abstract": "Load disaggregation based on aided linear integer programming (ALIP) is proposed. We start with a conventional linear integer programming (IP) based disaggregation and enhance it in several ways. The enhancements include additional constraints, correction based on a state diagram, median filtering, and linear programming-based refinement. With the aid of these enhancements, the performance of IP-based disaggregation is significantly improved. The proposed ALIP system relies only on the instantaneous load samples instead of waveform signatures, and hence does not crucially depend on high sampling frequency. Experimental results show that the proposed ALIP system performs better than the conventional IP-based load disaggregation system.\n    ",
        "submission_date": "2016-03-24T00:00:00",
        "last_modified_date": "2016-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07704",
        "title": "Probabilistic Reasoning via Deep Learning: Neural Association Models",
        "authors": [
            "Quan Liu",
            "Hui Jiang",
            "Andrew Evdokimov",
            "Zhen-Hua Ling",
            "Xiaodan Zhu",
            "Si Wei",
            "Yu Hu"
        ],
        "abstract": "In this paper, we propose a new deep learning approach, called neural association model (NAM), for probabilistic reasoning in artificial intelligence. We propose to use neural networks to model association between any two events in a domain. Neural networks take one event as input and compute a conditional probability of the other event to model how likely these two events are to be associated. The actual meaning of the conditional probabilities varies between applications and depends on how the models are trained. In this work, as two case studies, we have investigated two NAM structures, namely deep neural networks (DNN) and relation-modulated neural nets (RMNN), on several probabilistic reasoning tasks in AI, including recognizing textual entailment, triple classification in multi-relational knowledge bases and commonsense reasoning. Experimental results on several popular datasets derived from WordNet, FreeBase and ConceptNet have all demonstrated that both DNNs and RMNNs perform equally well and they can significantly outperform the conventional methods available for these reasoning tasks. Moreover, compared with DNNs, RMNNs are superior in knowledge transfer, where a pre-trained model can be quickly extended to an unseen relation after observing only a few training samples. To further prove the effectiveness of the proposed models, in this work, we have applied NAMs to solving challenging Winograd Schema (WS) problems. Experiments conducted on a set of WS problems prove that the proposed models have the potential for commonsense reasoning.\n    ",
        "submission_date": "2016-03-24T00:00:00",
        "last_modified_date": "2016-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08253",
        "title": "Negative Learning Rates and P-Learning",
        "authors": [
            "Devon Merrill"
        ],
        "abstract": "We present a method of training a differentiable function approximator for a regression task using negative examples. We effect this training using negative learning rates. We also show how this method can be used to perform direct policy learning in a reinforcement learning setting.\n    ",
        "submission_date": "2016-03-27T00:00:00",
        "last_modified_date": "2018-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08262",
        "title": "Towards Machine Intelligence",
        "authors": [
            "Kamil Rocki"
        ],
        "abstract": "There exists a theory of a single general-purpose learning algorithm which could explain the principles of its operation. This theory assumes that the brain has some initial rough architecture, a small library of simple innate circuits which are prewired at birth and proposes that all significant mental algorithms can be learned. Given current understanding and observations, this paper reviews and lists the ingredients of such an algorithm from both architectural and functional perspectives.\n    ",
        "submission_date": "2016-03-27T00:00:00",
        "last_modified_date": "2016-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08714",
        "title": "Properties of ABA+ for Non-Monotonic Reasoning",
        "authors": [
            "Kristijonas Cyras",
            "Francesca Toni"
        ],
        "abstract": "We investigate properties of ABA+, a formalism that extends the well studied structured argumentation formalism Assumption-Based Argumentation (ABA) with a preference handling mechanism. In particular, we establish desirable properties that ABA+ semantics exhibit. These pave way to the satisfaction by ABA+ of some (arguably) desirable principles of preference handling in argumentation and nonmonotonic reasoning, as well as non-monotonic inference properties of ABA+ under various semantics.\n    ",
        "submission_date": "2016-03-29T00:00:00",
        "last_modified_date": "2017-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08776",
        "title": "COCO: The Experimental Procedure",
        "authors": [
            "Nikolaus Hansen",
            "Tea Tusar",
            "Olaf Mersmann",
            "Anne Auger",
            "Dimo Brockhoff"
        ],
        "abstract": "We present a budget-free experimental setup and procedure for benchmarking numericaloptimization algorithms in a black-box scenario. This procedure can be applied with the COCO benchmarking platform. We describe initialization of and input to the algorithm and touch upon therelevance of termination and restarts. \n    ",
        "submission_date": "2016-03-29T00:00:00",
        "last_modified_date": "2016-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08785",
        "title": "COCO: A Platform for Comparing Continuous Optimizers in a Black-Box Setting",
        "authors": [
            "Nikolaus Hansen",
            "Anne Auger",
            "Raymond Ros",
            "Olaf Mersmann",
            "Tea Tu\u0161ar",
            "Dimo Brockhoff"
        ],
        "abstract": "We introduce COCO, an open source platform for Comparing Continuous Optimizers in a black-box setting. COCO aims at automatizing the tedious and repetitive task of benchmarking numerical optimization algorithms to the greatest possible extent. The platform and the underlying methodology allow to benchmark in the same framework deterministic and stochastic solvers for both single and multiobjective optimization. We present the rationales behind the (decade-long) development of the platform as a general proposition for guidelines towards better benchmarking. We detail underlying fundamental concepts of COCO such as the definition of a problem as a function instance, the underlying idea of instances, the use of target values, and runtime defined by the number of function calls as the central performance measure. Finally, we  give a quick overview of the basic code structure and the currently available test suites.\n    ",
        "submission_date": "2016-03-29T00:00:00",
        "last_modified_date": "2020-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08789",
        "title": "Using Enthymemes to Fill the Gap between Logical Argumentation and Revision of Abstract Argumentation Frameworks",
        "authors": [
            "Jean-Guy Mailly"
        ],
        "abstract": "In this paper, we present a preliminary work on an approach to fill the gap between logic-based argumentation and the numerous approaches to tackle the dynamics of abstract argumentation frameworks. Our idea is that, even when arguments and attacks are defined by means of a logical belief base, there may be some uncertainty about how accurate is the content of an argument, and so the presence (or absence) of attacks concerning it. We use enthymemes to illustrate this notion of uncertainty of arguments and attacks. Indeed, as argued in the literature, real arguments are often enthymemes instead of completely specified deductive arguments. This means that some parts of the pair (support, claim) may be missing because they are supposed to belong to some \"common knowledge\", and then should be deduced by the agent which receives the enthymeme. But the perception that agents have of the common knowledge may be wrong, and then a first agent may state an enthymeme that her opponent is not able to decode in an accurate way. It is likely that the decoding of the enthymeme by the agent leads to mistaken attacks between this new argument and the existing ones. In this case, the agent can receive some information about attacks or arguments acceptance statuses which disagree with her argumentation framework. We exemplify a way to incorporate this new piece of information by means of existing works on the dynamics of abstract argumentation frameworks.\n    ",
        "submission_date": "2016-03-29T00:00:00",
        "last_modified_date": "2016-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08869",
        "title": "Algorithms for Batch Hierarchical Reinforcement Learning",
        "authors": [
            "Tiancheng Zhao",
            "Mohammad Gowayyed"
        ],
        "abstract": "Hierarchical Reinforcement Learning (HRL) exploits temporal abstraction to solve large Markov Decision Processes (MDP) and provide transferable subtask policies. In this paper, we introduce an off-policy HRL algorithm: Hierarchical Q-value Iteration (HQI). We show that it is possible to effectively learn recursive optimal policies for any valid hierarchical decomposition of the original MDP, given a fixed dataset collected from a flat stochastic behavioral policy. We first formally prove the convergence of the algorithm for tabular MDP. Then our experiments on the Taxi domain show that HQI converges faster than a flat Q-value Iteration and enjoys easy state abstraction. Also, we demonstrate that our algorithm is able to learn optimal policies for different hierarchical structures from the same fixed dataset, which enables model comparison without recollecting data.\n    ",
        "submission_date": "2016-03-29T00:00:00",
        "last_modified_date": "2016-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08988",
        "title": "Towards Practical Bayesian Parameter and State Estimation",
        "authors": [
            "Yusuf Bugra Erol",
            "Yi Wu",
            "Lei Li",
            "Stuart Russell"
        ],
        "abstract": "Joint state and parameter estimation is a core problem for dynamic Bayesian networks. Although modern probabilistic inference toolkits make it relatively easy to specify large and practically relevant probabilistic models, the silver bullet---an efficient and general online inference algorithm for such problems---remains elusive, forcing users to write special-purpose code for each application. We propose a novel blackbox algorithm -- a hybrid of particle filtering for state variables and assumed density filtering for parameter variables. It has following advantages: (a) it is efficient due to its online nature, and (b) it is applicable to both discrete and continuous parameter spaces . On a variety of toy and real models, our system is able to generate more accurate results within a fixed computation budget. This preliminary evidence indicates that the proposed approach is likely to be of practical use.\n    ",
        "submission_date": "2016-03-29T00:00:00",
        "last_modified_date": "2016-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09029",
        "title": "Adaptive Maximization of Pointwise Submodular Functions With Budget Constraint",
        "authors": [
            "Nguyen Viet Cuong",
            "Huan Xu"
        ],
        "abstract": "We study the worst-case adaptive optimization problem with budget constraint that is useful for modeling various practical applications in artificial intelligence and machine learning. We investigate the near-optimality of greedy algorithms for this problem with both modular and non-modular cost functions. In both cases, we prove that two simple greedy algorithms are not near-optimal but the best between them is near-optimal if the utility function satisfies pointwise submodularity and pointwise cost-sensitive submodularity respectively. This implies a combined algorithm that is near-optimal with respect to the optimal algorithm that uses half of the budget. We discuss applications of our theoretical results and also report experiments comparing the greedy algorithms on the active learning problem.\n    ",
        "submission_date": "2016-03-30T00:00:00",
        "last_modified_date": "2017-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09051",
        "title": "Phoenix: A Self-Optimizing Chess Engine",
        "authors": [
            "Rahul Aralikatte",
            "G Srinivasaraghavan"
        ],
        "abstract": "Since the advent of computers, many tasks which required humans to spend a lot of time and energy have been trivialized by the computers' ability to perform repetitive tasks extremely quickly. Playing chess is one such task. It was one of the first games which was `solved' using AI. With the advent of deep learning, chess playing agents can surpass human ability with relative ease. However algorithms using deep learning must learn millions of parameters. This work looks at the game of chess through the lens of genetic algorithms. We train a genetic player from scratch using only a handful of learnable parameters. We use Multi-Niche Crowding to optimize positional Value Tables (PVTs) which are used extensively in chess engines to evaluate the goodness of a position. With a very simple setup and after only 1000 generations of evolution, the player reaches the level of an International Master.\n    ",
        "submission_date": "2016-03-30T00:00:00",
        "last_modified_date": "2017-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09194",
        "title": "Iterated Ontology Revision by Reinterpretation",
        "authors": [
            "\u00d6zg\u00fcr L\u00fctf\u00fc \u00d6z\u00e7ep"
        ],
        "abstract": "Iterated applications of belief change operators are essential for different scenarios such as that of ontology evolution where new information is not presented at once but only in piecemeal fashion within a sequence. I discuss iterated applications of so called reinterpretation operators that trace conflicts between ontologies back to the ambiguous of symbols and that provide conflict resolution strategies with bridging axioms. The discussion centers on adaptations of the classical iteration postulates according to Darwiche and Pearl. The main result of the paper is that reinterpretation operators fulfill the postulates for sequences containing only atomic triggers. For complex triggers, a fulfillment is not guaranteed and indeed there are different reasons for the different postulates why they should not be fulfilled in the particular scenario of ontology revision with well developed ontologies.\n    ",
        "submission_date": "2016-03-30T00:00:00",
        "last_modified_date": "2016-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09429",
        "title": "Ordinal Conditional Functions for Nearly Counterfactual Revision",
        "authors": [
            "Aaron Hunter"
        ],
        "abstract": "We are interested in belief revision involving conditional statements where the antecedent is almost certainly false. To represent such problems, we use Ordinal Conditional Functions that may take infinite values. We model belief change in this context through simple arithmetical operations that allow us to capture the intuition that certain antecedents can not be validated by any number of observations. We frame our approach as a form of finite belief improvement, and we propose a model of conditional belief revision in which only the \"right\" hypothetical levels of implausibility are revised.\n    ",
        "submission_date": "2016-03-31T00:00:00",
        "last_modified_date": "2016-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09465",
        "title": "A New Approach for Revising Logic Programs",
        "authors": [
            "Zhiqiang Zhuang",
            "James Delgrande",
            "Abhaya Nayak",
            "Abdul Sattar"
        ],
        "abstract": "Belief revision has been studied mainly with respect to background logics that are monotonic in character. In this paper we study belief revision when the underlying logic is non-monotonic instead--an inherently interesting problem that is under explored. In particular, we will focus on the revision of a body of beliefs that is represented as a logic program under the answer set semantics, while the new information is also similarly represented as a logic program. Our approach is driven by the observation that unlike in a monotonic setting where, when necessary, consistency in a revised body of beliefs is maintained by jettisoning some old beliefs, in a non-monotonic setting consistency can be restored by adding new beliefs as well. We will define a syntactic revision function and subsequently provide representation theorem for characterising it.\n    ",
        "submission_date": "2016-03-31T00:00:00",
        "last_modified_date": "2016-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09495",
        "title": "Reactive Policies with Planning for Action Languages",
        "authors": [
            "Zeynep G. Saribatur",
            "Thomas Eiter"
        ],
        "abstract": "We describe a representation in a high-level transition system for policies that express a reactive behavior for the agent. We consider a target decision component that figures out what to do next and an (online) planning capability to compute the plans needed to reach these targets. Our representation allows one to analyze the flow of executing the given reactive policy, and to determine whether it works as expected. Additionally, the flexibility of the representation opens a range of possibilities for designing behaviors.\n    ",
        "submission_date": "2016-03-31T00:00:00",
        "last_modified_date": "2016-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09502",
        "title": "Verifiability of Argumentation Semantics",
        "authors": [
            "Ringo Baumann",
            "Thomas Linsbichler",
            "Stefan Woltran"
        ],
        "abstract": "Dung's abstract argumentation theory is a widely used formalism to model conflicting information and to draw conclusions in such situations. Hereby, the knowledge is represented by so-called argumentation frameworks (AFs) and the reasoning is done via semantics extracting acceptable sets. All reasonable semantics are based on the notion of conflict-freeness which means that arguments are only jointly acceptable when they are not linked within the AF. In this paper, we study the question which information on top of conflict-free sets is needed to compute extensions of a semantics at hand. We introduce a hierarchy of so-called verification classes specifying the required amount of information. We show that well-known standard semantics are exactly verifiable through a certain such class. Our framework also gives a means to study semantics lying inbetween known semantics, thus contributing to a more abstract understanding of the different features argumentation semantics offer.\n    ",
        "submission_date": "2016-03-31T00:00:00",
        "last_modified_date": "2016-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09511",
        "title": "Distributing Knowledge into Simple Bases",
        "authors": [
            "Adrian Haret",
            "Jean-Guy Mailly",
            "Stefan Woltran"
        ],
        "abstract": "Understanding the behavior of belief change operators for fragments of classical logic has received increasing interest over the last years. Results in this direction are mainly concerned with adapting representation theorems. However, fragment-driven belief change also leads to novel research questions. In this paper we propose the concept of belief distribution, which can be understood as the reverse task of merging. More specifically, we are interested in the following question: given an arbitrary knowledge base $K$ and some merging operator $\\Delta$, can we find a profile $E$ and a constraint $\\mu$, both from a given fragment of classical logic, such that $\\Delta_\\mu(E)$ yields a result equivalent to $K$? In other words, we are interested in seeing if $K$ can be distributed into knowledge bases of simpler structure, such that the task of merging allows for a reconstruction of the original knowledge. Our initial results show that merging based on drastic distance allows for an easy distribution of knowledge, while the power of distribution for operators based on Hamming distance relies heavily on the fragment of choice.\n    ",
        "submission_date": "2016-03-31T00:00:00",
        "last_modified_date": "2016-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09545",
        "title": "Characterizing Realizability in Abstract Argumentation",
        "authors": [
            "Thomas Linsbichler",
            "J\u00f6rg P\u00fchrer",
            "Hannes Strass"
        ],
        "abstract": "Realizability for knowledge representation formalisms studies the following question: given a semantics and a set of interpretations, is there a knowledge base whose semantics coincides exactly with the given interpretation set? We introduce a general framework for analyzing realizability in abstract dialectical frameworks (ADFs) and various of its subclasses. In particular, the framework applies to Dung argumentation frameworks, SETAFs by Nielsen and Parsons, and bipolar ADFs. We present a uniform characterization method for the admissible, complete, preferred and model/stable semantics. We employ this method to devise an algorithm that decides realizability for the mentioned formalisms and semantics; moreover the algorithm allows for constructing a desired knowledge base whenever one exists. The algorithm is built in a modular way and thus easily extensible to new formalisms and semantics. We have also implemented our approach in answer set programming, and used the implementation to obtain several novel results on the relative expressiveness of the abovementioned formalisms.\n    ",
        "submission_date": "2016-03-31T00:00:00",
        "last_modified_date": "2016-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09728",
        "title": "A Survey of League Championship Algorithm: Prospects and Challenges",
        "authors": [
            "Shafii Muhammad Abdulhamid",
            "Muhammad Shafie Abd Latiff",
            "Syed Hamid Hussain Madni",
            "Osho Oluwafemi"
        ],
        "abstract": "The League Championship Algorithm (LCA) is sport-inspired optimization algorithm that was introduced by Ali Husseinzadeh Kashan in the year 2009. It has since drawn enormous interest among the researchers because of its potential efficiency in solving many optimization problems and real-world applications. The LCA has also shown great potentials in solving non-deterministic polynomial time (NP-complete) problems. This survey presents a brief synopsis of the LCA literatures in peer-reviewed journals, conferences and book chapters. These research articles are then categorized according to indexing in the major academic databases (Web of Science, Scopus, IEEE Xplore and the Google Scholar). The analysis was also done to explore the prospects and the challenges of the algorithm and its acceptability among researchers. This systematic categorization can be used as a basis for future studies.\n    ",
        "submission_date": "2015-07-18T00:00:00",
        "last_modified_date": "2015-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00162",
        "title": "Relations between assumption-based approaches in nonmonotonic logic and formal argumentation",
        "authors": [
            "Jesse Heyninck",
            "Christian Stra\u00dfer"
        ],
        "abstract": "In this paper we make a contribution to the unification of formal models of defeasible reasoning. We present several translations between formal argumentation frameworks and nonmonotonic logics for reasoning with plausible assumptions. More specifically, we translate adaptive logics into assumption-based argumentation and ASPIC+, ASPIC+ into assumption-based argumentation and a fragment of assumption-based argumentation into adaptive logics. Adaptive logics are closely related to Makinson's default assumptions and to a significant class of systems within the tradition of preferential semantics in the vein of KLM and Shoham. Thus, our results also provide close links between formal argumentation and the latter approaches.\n    ",
        "submission_date": "2016-04-01T00:00:00",
        "last_modified_date": "2016-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00266",
        "title": "The Algorithm of Islamic Jurisprudence (Fiqh) with Validation of an Entscheidungsproblem",
        "authors": [
            "Elnaserledinellah Mahmood Abdelwahab",
            "Karim Daghbouche",
            "Nadra Ahmad Shannan"
        ],
        "abstract": "The historic background of algorithmic processing with regard to etymology and methodology is translated into terms of mathematical logic and Computer Science. A formal logic structure is introduced by exemplaryquestions posed to Fiqh-chapters to define alogic query language. As a foundation, ageneric algorithm for deciding Fiqh-rulings is designed to enable and further leverage rule of law (vs. rule by law) with full transparency and complete algorithmic coverage of Islamic law eventually providing legal security, legal equality, and full legal ",
        "submission_date": "2016-03-10T00:00:00",
        "last_modified_date": "2016-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00289",
        "title": "Building Machines That Learn and Think Like People",
        "authors": [
            "Brenden M. Lake",
            "Tomer D. Ullman",
            "Joshua B. Tenenbaum",
            "Samuel J. Gershman"
        ],
        "abstract": "Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.\n    ",
        "submission_date": "2016-04-01T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00300",
        "title": "A SAT model to mine flexible sequences in transactional datasets",
        "authors": [
            "R\u00e9mi Coletta",
            "Benjamin Negrevergne"
        ],
        "abstract": "Traditional pattern mining algorithms generally suffer from a lack of flexibility. In this paper, we propose a SAT formulation of the problem to successfully mine frequent flexible sequences occurring in transactional datasets. Our SAT-based approach can easily be extended with extra constraints to address a broad range of pattern mining applications. To demonstrate this claim, we formulate and add several constraints, such as gap and span constraints, to our model in order to extract more specific patterns. We also use interactive solving to perform important derived tasks, such as closed pattern mining or maximal pattern mining. Finally, we prove the practical feasibility of our SAT model by running experiments on two real datasets.\n    ",
        "submission_date": "2016-04-01T00:00:00",
        "last_modified_date": "2016-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00301",
        "title": "A strengthening of rational closure in DLs: reasoning about multiple aspects",
        "authors": [
            "Valentina Gliozzi"
        ],
        "abstract": "We propose a logical analysis of the concept of typicality, central in human cognition (Rosch,1978). We start from a previously proposed extension of the basic Description Logic ALC (a computationally tractable fragment of First Order Logic, used to represent concept inclusions and ontologies) with a typicality operator T that allows to consistently represent the attribution to classes of individuals of properties with exceptions (as in the classic example (i) typical birds fly, (ii) penguins are birds but (iii) typical penguins don't fly). We then strengthen this extension in order to separately reason about the typicality with respect to different aspects (e.g., flying, having nice feather: in the previous example, penguins may not inherit the property of flying, for which they are exceptional, but can nonetheless inherit other properties, such as having nice feather).\n    ",
        "submission_date": "2016-04-01T00:00:00",
        "last_modified_date": "2016-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00359",
        "title": "Using Well-Understood Single-Objective Functions in Multiobjective Black-Box Optimization Test Suites",
        "authors": [
            "Dimo Brockhoff",
            "Tea Tusar",
            "Anne Auger",
            "Nikolaus Hansen"
        ],
        "abstract": "Several test function suites are being used for numerical benchmarking of multiobjective optimization algorithms. While they have some desirable properties, like well-understood Pareto sets and Pareto fronts of various shapes, most of the currently used functions possess characteristics that are arguably under-represented in real-world problems. They mainly stem from the easier construction of such functions and result in improbable properties such as separability, optima located exactly at the boundary constraints, and the existence of variables that solely control the distance between a solution and the Pareto front. Here, we propose an alternative way to constructing multiobjective problems-by combining existing single-objective problems from the literature. We describe in particular the bbob-biobj test suite with 55 bi-objective functions in continuous domain, and its extended version with 92 bi-objective functions (bbob-biobj-ext). Both test suites have been implemented in the COCO platform for black-box optimization benchmarking. Finally, we recommend a general procedure for creating test suites for an arbitrary number of objectives. Besides providing the formal function definitions and presenting their (known) properties, this paper also aims at giving the rationale behind our approach in terms of groups of functions with similar properties, objective space normalization, and problem instances. The latter allows us to easily compare the performance of deterministic and stochastic solvers, which is an often overlooked issue in benchmarking.\n    ",
        "submission_date": "2016-04-01T00:00:00",
        "last_modified_date": "2019-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00377",
        "title": "Reinforcement learning based local search for grouping problems: A case study on graph coloring",
        "authors": [
            "Yangming Zhou",
            "Jin-Kao Hao",
            "B\u00e9atrice Duval"
        ],
        "abstract": "Grouping problems aim to partition a set of items into multiple mutually disjoint subsets according to some specific criterion and constraints. Grouping problems cover a large class of important combinatorial optimization problems that are generally computationally difficult. In this paper, we propose a general solution approach for grouping problems, i.e., reinforcement learning based local search (RLS), which combines reinforcement learning techniques with descent-based local search. The viability of the proposed approach is verified on a well-known representative grouping problem (graph coloring) where a very simple descent-based coloring algorithm is applied. Experimental studies on popular DIMACS and COLOR02 benchmark graphs indicate that RLS achieves competitive performances compared to a number of well-known coloring algorithms.\n    ",
        "submission_date": "2016-04-01T00:00:00",
        "last_modified_date": "2016-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00545",
        "title": "The AGI Containment Problem",
        "authors": [
            "James Babcock",
            "Janos Kramar",
            "Roman Yampolskiy"
        ],
        "abstract": "There is considerable uncertainty about what properties, capabilities and motivations future AGIs will have. In some plausible scenarios, AGIs may pose security risks arising from accidents and defects. In order to mitigate these risks, prudent early AGI research teams will perform significant testing on their creations before use. Unfortunately, if an AGI has human-level or greater intelligence, testing itself may not be safe; some natural AGI goal systems create emergent incentives for AGIs to tamper with their test environments, make copies of themselves on the internet, or convince developers and operators to do dangerous things. In this paper, we survey the AGI containment problem - the question of how to build a container in which tests can be conducted safely and reliably, even on AGIs with unknown motivations and capabilities that could be dangerous. We identify requirements for AGI containers, available mechanisms, and weaknesses that need to be addressed.\n    ",
        "submission_date": "2016-04-02T00:00:00",
        "last_modified_date": "2016-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00681",
        "title": "Experimental Assessment of Aggregation Principles in Argumentation-enabled Collective Intelligence",
        "authors": [
            "Edmond Awad",
            "Jean-Fran\u00e7ois Bonnefon",
            "Martin Caminada",
            "Thomas Malone",
            "Iyad Rahwan"
        ],
        "abstract": "On the Web, there is always a need to aggregate opinions from the crowd (as in posts, social networks, forums, etc.). Different mechanisms have been implemented to capture these opinions such as \"Like\" in Facebook, \"Favorite\" in Twitter, thumbs-up/down, flagging, and so on. However, in more contested domains (e.g. Wikipedia, political discussion, and climate change discussion) these mechanisms are not sufficient since they only deal with each issue independently without considering the relationships between different claims. We can view a set of conflicting arguments as a graph in which the nodes represent arguments and the arcs between these nodes represent the defeat relation. A group of people can then collectively evaluate such graphs. To do this, the group must use a rule to aggregate their individual opinions about the entire argument graph. Here, we present the first experimental evaluation of different principles commonly employed by aggregation rules presented in the literature. We use randomized controlled experiments to investigate which principles people consider better at aggregating opinions under different conditions. Our analysis reveals a number of factors, not captured by traditional formal models, that play an important role in determining the efficacy of aggregation. These results help bring formal models of argumentation closer to real-world application.\n    ",
        "submission_date": "2016-04-03T00:00:00",
        "last_modified_date": "2017-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00693",
        "title": "Pareto Optimality and Strategy Proofness in Group Argument Evaluation (Extended Version)",
        "authors": [
            "Edmond Awad",
            "Martin Caminada",
            "Gabriella Pigozzi",
            "Miko\u0142aj Podlaszewski",
            "Iyad Rahwan"
        ],
        "abstract": "An inconsistent knowledge base can be abstracted as a set of arguments and a defeat relation among them. There can be more than one consistent way to evaluate such an argumentation graph. Collective argument evaluation is the problem of aggregating the opinions of multiple agents on how a given set of arguments should be evaluated. It is crucial not only to ensure that the outcome is logically consistent, but also satisfies measures of social optimality and immunity to strategic manipulation. This is because agents have their individual preferences about what the outcome ought to be. In the current paper, we analyze three previously introduced argument-based aggregation operators with respect to Pareto optimality and strategy proofness under different general classes of agent preferences. We highlight fundamental trade-offs between strategic manipulability and social optimality on one hand, and classical logical criteria on the other. Our results motivate further investigation into the relationship between social choice and argumentation theory. The results are also relevant for choosing an appropriate aggregation operator given the criteria that are considered more important, as well as the nature of agents' preferences.\n    ",
        "submission_date": "2016-04-03T00:00:00",
        "last_modified_date": "2017-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00799",
        "title": "Extending DLR with Labelled Tuples, Projections, Functional Dependencies and Objectification (full version)",
        "authors": [
            "Alessandro Artale",
            "Enrico Franconi"
        ],
        "abstract": "We introduce an extension of the n-ary description logic DLR to deal with attribute-labelled tuples (generalising the positional notation), with arbitrary projections of relations (inclusion dependencies), generic functional dependencies and with global and local objectification (reifying relations or their projections). We show how a simple syntactic condition on the appearance of projections and functional dependencies in a knowledge base makes the language decidable without increasing the computational complexity of the basic DLR language.\n    ",
        "submission_date": "2016-04-04T00:00:00",
        "last_modified_date": "2016-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00869",
        "title": "Automatic Knowledge Base Evolution by Learning Instances",
        "authors": [
            "Sundong Kim"
        ],
        "abstract": "Knowledge base is the way to store structured and unstructured data throughout the web. Since the size of the web is increasing rapidly, there are huge needs to structure the knowledge in a fully automated way. However fully-automated knowledge-base evolution on the Semantic Web is a major challenges, although there are many ontology evolution techniques available. Therefore learning ontology automatically can contribute to the semantic web society significantly. In this paper, we propose full-automated ontology learning algorithm to generate refined knowledge base from incomplete knowledge base and rdf-triples. Our algorithm is data-driven approach which is based on the property of each instance. Ontology class is being elaborated by generalizing frequent property of its instances. By using that developed class information, each instance can find its most relatively matching class. By repeating these two steps, we achieve fully-automated ontology evolution from incomplete basic knowledge base.\n    ",
        "submission_date": "2016-04-04T00:00:00",
        "last_modified_date": "2016-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01219",
        "title": "Learning to Generate Posters of Scientific Papers",
        "authors": [
            "Yuting Qiang",
            "Yanwei Fu",
            "Yanwen Guo",
            "Zhi-Hua Zhou",
            "Leonid Sigal"
        ],
        "abstract": "Researchers often summarize their work in the form of posters. Posters provide a coherent and efficient way to convey core ideas from scientific papers. Generating a good scientific poster, however, is a complex and time consuming cognitive task, since such posters need to be readable, informative, and visually aesthetic. In this paper, for the first time, we study the challenging problem of learning to generate posters from scientific papers. To this end, a data-driven framework, that utilizes graphical models, is proposed. Specifically, given content to display, the key elements of a good poster, including panel layout and attributes of each panel, are learned and inferred from data. Then, given inferred layout and attributes, composition of graphical elements within each panel is synthesized. To learn and validate our model, we collect and make public a Poster-Paper dataset, which consists of scientific papers and corresponding posters with exhaustively labelled panels and attributes. Qualitative and quantitative results indicate the effectiveness of our approach.\n    ",
        "submission_date": "2016-04-05T00:00:00",
        "last_modified_date": "2016-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01277",
        "title": "Landmark-Based Plan Recognition",
        "authors": [
            "Ramon Fraga Pereira",
            "Felipe Meneguzzi"
        ],
        "abstract": "Recognition of goals and plans using incomplete evidence from action execution can be done efficiently by using planning techniques. In many applications it is important to recognize goals and plans not only accurately, but also quickly. In this paper, we develop a heuristic approach for recognizing plans based on planning techniques that rely on ordering constraints to filter candidate goals from observations. These ordering constraints are called landmarks in the planning literature, which are facts or actions that cannot be avoided to achieve a goal. We show the applicability of planning landmarks in two settings: first, we use it directly to develop a heuristic-based plan recognition approach; second, we refine an existing planning-based plan recognition approach by pre-filtering its candidate goals. Our empirical evaluation shows that our approach is not only substantially more accurate than the state-of-the-art in all available datasets, it is also an order of magnitude faster.\n    ",
        "submission_date": "2016-04-05T00:00:00",
        "last_modified_date": "2017-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01350",
        "title": "Bounded Optimal Exploration in MDP",
        "authors": [
            "Kenji Kawaguchi"
        ],
        "abstract": "Within the framework of probably approximately correct Markov decision processes (PAC-MDP), much theoretical work has focused on methods to attain near optimality after a relatively long period of learning and exploration. However, practical concerns require the attainment of satisfactory behavior within a short period of time. In this paper, we relax the PAC-MDP conditions to reconcile theoretically driven exploration methods and practical needs. We propose simple algorithms for discrete and continuous state spaces, and illustrate the benefits of our proposed relaxation via theoretical analyses and numerical examples. Our algorithms also maintain anytime error bounds and average loss bounds. Our approach accommodates both Bayesian and non-Bayesian methods.\n    ",
        "submission_date": "2016-04-05T00:00:00",
        "last_modified_date": "2016-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02080",
        "title": "Planning with Information-Processing Constraints and Model Uncertainty in Markov Decision Processes",
        "authors": [
            "Jordi Grau-Moya",
            "Felix Leibfried",
            "Tim Genewein",
            "Daniel A. Braun"
        ],
        "abstract": "Information-theoretic principles for learning and acting have been proposed to solve particular classes of Markov Decision Problems. Mathematically, such approaches are governed by a variational free energy principle and allow solving MDP planning problems with information-processing constraints expressed in terms of a Kullback-Leibler divergence with respect to a reference distribution. Here we consider a generalization of such MDP planners by taking model uncertainty into account. As model uncertainty can also be formalized as an information-processing constraint, we can derive a unified solution from a single generalized variational principle. We provide a generalized value iteration scheme together with a convergence proof. As limit cases, this generalized scheme includes standard value iteration with a known model, Bayesian MDP planning, and robust planning. We demonstrate the benefits of this approach in a grid world simulation.\n    ",
        "submission_date": "2016-04-07T00:00:00",
        "last_modified_date": "2016-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02126",
        "title": "On Stochastic Belief Revision and Update and their Combination",
        "authors": [
            "Gavin Rens"
        ],
        "abstract": "I propose a framework for an agent to change its probabilistic beliefs when a new piece of propositional information $\\alpha$ is observed. Traditionally, belief change occurs by either a revision process or by an update process, depending on whether the agent is informed with $\\alpha$ in a static world or, respectively, whether $\\alpha$ is a 'signal' from the environment due to an event occurring. Boutilier suggested a unified model of qualitative belief change, which \"combines aspects of revision and update, providing a more realistic characterization of belief change.\" In this paper, I propose a unified model of quantitative belief change, where an agent's beliefs are represented as a probability distribution over possible worlds. As does Boutilier, I take a dynamical systems perspective. The proposed approach is evaluated against several rationality postulated, and some properties of the approach are worked out.\n    ",
        "submission_date": "2016-04-07T00:00:00",
        "last_modified_date": "2016-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02133",
        "title": "Revising Incompletely Specified Convex Probabilistic Belief Bases",
        "authors": [
            "Gavin Rens",
            "Thomas Meyer",
            "Giovanni Casini"
        ],
        "abstract": "We propose a method for an agent to revise its incomplete probabilistic beliefs when a new piece of propositional information is observed. In this work, an agent's beliefs are represented by a set of probabilistic formulae -- a belief base. The method involves determining a representative set of 'boundary' probability distributions consistent with the current belief base, revising each of these probability distributions and then translating the revised information into a new belief base. We use a version of Lewis Imaging as the revision operation. The correctness of the approach is proved. The expressivity of the belief bases under consideration are rather restricted, but has some applications. We also discuss methods of belief base revision employing the notion of optimum entropy, and point out some of the benefits and difficulties in those methods. Both the boundary distribution method and the optimum entropy method are reasonable, yet yield different results.\n    ",
        "submission_date": "2016-04-07T00:00:00",
        "last_modified_date": "2016-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02323",
        "title": "A system of serial computation for classified rules prediction in non-regular ontology trees",
        "authors": [
            "Kennedy E. Ehimwenma",
            "Paul Crowther",
            "Martin Beer"
        ],
        "abstract": "Objects or structures that are regular take uniform dimensions. Based on the concepts of regular models, our previous research work has developed a system of a regular ontology that models learning structures in a multiagent system for uniform pre-assessments in a learning environment. This regular ontology has led to the modelling of a classified rules learning algorithm that predicts the actual number of rules needed for inductive learning processes and decision making in a multiagent system. But not all processes or models are regular. Thus this paper presents a system of polynomial equation that can estimate and predict the required number of rules of a non-regular ontology model given some defined parameters.\n    ",
        "submission_date": "2016-04-08T00:00:00",
        "last_modified_date": "2016-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02336",
        "title": "Back to the Basics: Bayesian extensions of IRT outperform neural networks for proficiency estimation",
        "authors": [
            "Kevin H. Wilson",
            "Yan Karklin",
            "Bojian Han",
            "Chaitanya Ekanadham"
        ],
        "abstract": "Estimating student proficiency is an important task for computer based learning systems. We compare a family of IRT-based proficiency estimation methods to Deep Knowledge Tracing (DKT), a recently proposed recurrent neural network model with promising initial results. We evaluate how well each model predicts a student's future response given previous responses using two publicly available and one proprietary data set. We find that IRT-based methods consistently matched or outperformed DKT across all data sets at the finest level of content granularity that was tractable for them to be trained on. A hierarchical extension of IRT that captured item grouping structure performed best overall. When data sets included non-trivial autocorrelations in student response patterns, a temporal extension of IRT improved performance over standard IRT while the RNN-based method did not. We conclude that IRT-based models provide a simpler, better-performing alternative to existing RNN-based models of student interaction data while also affording more interpretability and guarantees due to their formulation as Bayesian probabilistic models.\n    ",
        "submission_date": "2016-04-08T00:00:00",
        "last_modified_date": "2016-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02416",
        "title": "How deep is knowledge tracing?",
        "authors": [
            "Mohammad Khajah",
            "Robert V. Lindsey",
            "Michael C. Mozer"
        ],
        "abstract": "In theoretical cognitive science, there is a tension between highly structured models whose parameters have a direct psychological interpretation and highly complex, general-purpose models whose parameters and representations are difficult to interpret. The former typically provide more insight into cognition but the latter often perform better. This tension has recently surfaced in the realm of educational data mining, where a deep learning approach to predicting students' performance as they work through a series of exercises---termed deep knowledge tracing or DKT---has demonstrated a stunning performance advantage over the mainstay of the field, Bayesian knowledge tracing or BKT. In this article, we attempt to understand the basis for DKT's advantage by considering the sources of statistical regularity in the data that DKT can leverage but which BKT cannot. We hypothesize four forms of regularity that BKT fails to exploit: recency effects, the contextualized trial sequence, inter-skill similarity, and individual variation in ability. We demonstrate that when BKT is extended to allow it more flexibility in modeling statistical regularities---using extensions previously proposed in the literature---BKT achieves a level of performance indistinguishable from that of DKT. We argue that while DKT is a powerful, useful, general-purpose framework for modeling student learning, its gains do not come from the discovery of novel representations---the fundamental advantage of deep learning. To answer the question posed in our title, knowledge tracing may be a domain that does not require `depth'; shallow models like BKT can perform just as well and offer us greater interpretability and explanatory power.\n    ",
        "submission_date": "2016-03-14T00:00:00",
        "last_modified_date": "2016-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02509",
        "title": "Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds",
        "authors": [
            "Shiwali Mohan",
            "Aaron Mininger",
            "John Laird"
        ],
        "abstract": "We propose a computational model of situated language comprehension based on the Indexical Hypothesis that generates meaning representations by translating amodal linguistic symbols to modal representations of beliefs, knowledge, and experience external to the linguistic system. This Indexical Model incorporates multiple information sources, including perceptions, domain knowledge, and short-term and long-term experiences during comprehension. We show that exploiting diverse information sources can alleviate ambiguities that arise from contextual use of underspecific referring expressions and unexpressed argument alternations of verbs. The model is being used to support linguistic interactions in Rosie, an agent implemented in Soar that learns from instruction.\n    ",
        "submission_date": "2016-04-09T00:00:00",
        "last_modified_date": "2022-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02737",
        "title": "Correlated Equilibria for Approximate Variational Inference in MRFs",
        "authors": [
            "Luis E. Ortiz",
            "Boshen Wang",
            "Ze Gong"
        ],
        "abstract": "Almost all of the work in graphical models for game theory has mirrored previous work in probabilistic graphical models. Our work considers the opposite direction: Taking advantage of recent advances in equilibrium computation for probabilistic inference. We present formulations of inference problems in Markov random fields (MRFs) as computation of equilibria in a certain class of game-theoretic graphical models. We concretely establishes the precise connection between variational probabilistic inference in MRFs and correlated equilibria. No previous work exploits recent theoretical and empirical results from the literature on algorithmic and computational game theory on the tractable, polynomial-time computation of exact or approximate correlated equilibria in graphical games with arbitrary, loopy graph structure. We discuss how to design new algorithms with equally tractable guarantees for the computation of approximate variational inference in MRFs. Also, inspired by a previously stated game-theoretic view of state-of-the-art tree-reweighed (TRW) message-passing techniques for belief inference as zero-sum game, we propose a different, general-sum potential game to design approximate fictitious-play techniques. We perform synthetic experiments evaluating our proposed approximation algorithms with standard methods and TRW on several classes of classical Ising models (i.e., with binary random variables). We also evaluate the algorithms using Ising models learned from the MNIST dataset. Our experiments show that our global approach is competitive, particularly shinning in a class of Ising models with constant, \"highly attractive\" edge-weights, in which it is often better than all other alternatives we evaluated. With a notable exception, our more local approach was not as effective. Yet, in fairness, almost all of the alternatives are often no better than a simple baseline: estimate 0.5.\n    ",
        "submission_date": "2016-04-10T00:00:00",
        "last_modified_date": "2017-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02774",
        "title": "Reverse Engineering and Symbolic Knowledge Extraction on \u0141ukasiewicz Fuzzy Logics using Linear Neural Networks",
        "authors": [
            "Carlos Leandro"
        ],
        "abstract": "This work describes a methodology to combine logic-based systems and connectionist systems. Our approach uses finite truth valued \u0141ukasiewicz logic, where we take advantage of fact what in this type of logics every connective can be define by a neuron in an artificial network having by activation function the identity truncated to zero and one. This allowed the injection of first-order formulas in a network architecture, and also simplified symbolic rule extraction.\n",
        "submission_date": "2016-04-11T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02780",
        "title": "Knowledge Extraction and Knowledge Integration governed by \u0141ukasiewicz Logics",
        "authors": [
            "Carlos Leandro"
        ],
        "abstract": "The development of machine learning in particular and artificial intelligent in general has been strongly conditioned by the lack of an appropriate interface layer between deduction, abduction and induction. In this work we extend traditional algebraic specification methods in this direction. Here we assume that such interface for AI emerges from an adequate Neural-Symbolic integration. This integration is made for universe of discourse described on a Topos governed by a many-valued \u0141ukasiewicz logic. Sentences are integrated in a symbolic knowledge base describing the problem domain, codified using a graphic-based language, wherein every logic connective is defined by a neuron in an artificial network. This allows the integration of first-order formulas into a network architecture as background knowledge, and simplifies symbolic rule extraction from trained networks. For the train of such neural networks we changed the Levenderg-Marquardt algorithm, restricting the knowledge dissemination in the network structure using soft crystallization. This procedure reduces neural network plasticity without drastically damaging the learning performance, allowing the emergence of symbolic patterns. This makes the descriptive power of produced neural networks similar to the descriptive power of \u0141ukasiewicz logic language, reducing the information lost on translation between symbolic and connectionist structures. We tested this method on the extraction of knowledge from specified structures. For it, we present the notion of fuzzy state automata, and we use automata behaviour to infer its structure. We use this type of automata on the generation of models for relations specified as symbolic background knowledge.\n    ",
        "submission_date": "2016-04-11T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03099",
        "title": "Symbolic Knowledge Extraction using \u0141ukasiewicz Logics",
        "authors": [
            "Carlos Leandro"
        ],
        "abstract": "This work describes a methodology that combines logic-based systems and connectionist systems. Our approach uses finite truth-valued \u0141ukasiewicz logic, wherein every connective can be defined by a neuron in an artificial network. This allowed the injection of first-order formulas into a network architecture, and also simplified symbolic rule extraction. For that we trained a neural networks using the Levenderg-Marquardt algorithm, where we restricted the knowledge dissemination in the network structure. This procedure reduces neural network plasticity without drastically damaging the learning performance, thus making the descriptive power of produced neural networks similar to the descriptive power of \u0141ukasiewicz logic language and simplifying the translation between symbolic and connectionist structures. We used this method for reverse engineering truth table and in extraction of formulas from real data sets.\n    ",
        "submission_date": "2016-04-11T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03200",
        "title": "Efficient Classification of Multi-Labelled Text Streams by Clashing",
        "authors": [
            "Ricardo \u00d1anculef",
            "Ilias Flaounas",
            "Nello Cristianini"
        ],
        "abstract": "We present a method for the classification of multi-labelled text documents explicitly designed for data stream applications that require to process a virtually infinite sequence of data using constant memory and constant processing time. Our method is composed of an online procedure used to efficiently map text into a low-dimensional feature space and a partition of this space into a set of regions for which the system extracts and keeps statistics used to predict multi-label text annotations. Documents are fed into the system as a sequence of words, mapped to a region of the partition, and annotated using the statistics computed from the labelled instances colliding in the same region. This approach is referred to as clashing. We illustrate the method in real-world text data, comparing the results with those obtained using other text classifiers. In addition, we provide an analysis about the effect of the representation space dimensionality on the predictive performance of the system. Our results show that the online embedding indeed approximates the geometry of the full corpus-wise TF and TF-IDF space. The model obtains competitive F measures with respect to the most accurate methods, using significantly fewer computational resources. In addition, the method achieves a higher macro-averaged F measure than methods with similar running time. Furthermore, the system is able to learn faster than the other methods from partially labelled streams.\n    ",
        "submission_date": "2016-04-12T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03210",
        "title": "An Analysis of General Fuzzy Logic and Fuzzy Reasoning Method",
        "authors": [
            "Kwak Son Il"
        ],
        "abstract": "In this article, we describe the fuzzy logic, fuzzy language and algorithms as the basis of fuzzy reasoning, one of the intelligent information processing method, and then describe the general fuzzy reasoning method.\n    ",
        "submission_date": "2016-04-07T00:00:00",
        "last_modified_date": "2016-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03318",
        "title": "Applying Ontological Modeling on Quranic Nature Domain",
        "authors": [
            "A.B.M. Shamsuzzaman Sadi",
            "Towfique Anam",
            "Mohamed Abdirazak",
            "Abdillahi Hasan Adnan",
            "Sazid Zaman Khan",
            "Mohamed Mahmudur Rahman",
            "Ghassan Samara"
        ],
        "abstract": "The holy Quran is the holy book of the Muslims. It contains information about many domains. Often people search for particular concepts of holy Quran based on the relations among concepts. An ontological modeling of holy Quran can be useful in such a scenario. In this paper, we have modeled nature related concepts of holy Quran using OWL (Web Ontology Language) / RDF (Resource Description Framework). Our methodology involves identifying nature related concepts mentioned in holy Quran and identifying relations among those concepts. These concepts and relations are represented as classes/instances and properties of an OWL ontology. Later, in the result section it is shown that, using the Ontological model, SPARQL queries can retrieve verses and concepts of interest. Thus, this modeling helps semantic search and query on the holy Quran. In this work, we have used English translation of the holy Quran by Sahih International, Protege OWL Editor and for querying we have used SPARQL.\n    ",
        "submission_date": "2016-04-12T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03912",
        "title": "Inverse Reinforcement Learning with Simultaneous Estimation of Rewards and Dynamics",
        "authors": [
            "Michael Herman",
            "Tobias Gindele",
            "J\u00f6rg Wagner",
            "Felix Schmitt",
            "Wolfram Burgard"
        ],
        "abstract": "Inverse Reinforcement Learning (IRL) describes the problem of learning an unknown reward function of a Markov Decision Process (MDP) from observed behavior of an agent. Since the agent's behavior originates in its policy and MDP policies depend on both the stochastic system dynamics as well as the reward function, the solution of the inverse problem is significantly influenced by both. Current IRL approaches assume that if the transition model is unknown, additional samples from the system's dynamics are accessible, or the observed behavior provides enough samples of the system's dynamics to solve the inverse problem accurately. These assumptions are often not satisfied. To overcome this, we present a gradient-based IRL approach that simultaneously estimates the system's dynamics. By solving the combined optimization problem, our approach takes into account the bias of the demonstrations, which stems from the generating policy. The evaluation on a synthetic MDP and a transfer learning task shows improvements regarding the sample efficiency as well as the accuracy of the estimated reward functions and transition models.\n    ",
        "submission_date": "2016-04-13T00:00:00",
        "last_modified_date": "2016-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04096",
        "title": "A General Framework for Describing Creative Agents",
        "authors": [
            "Valerio Velardo",
            "Mauro Vallati"
        ],
        "abstract": "Computational creativity is a subfield of AI focused on developing and studying creative systems. Few academic studies analysing the behaviour of creative agents from a theoretical viewpoint have been proposed. The proposed frameworks are vague and hard to exploit; moreover, such works are focused on a notion of creativity tailored for humans.\n",
        "submission_date": "2016-04-14T00:00:00",
        "last_modified_date": "2016-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04315",
        "title": "Moving Beyond the Turing Test with the Allen AI Science Challenge",
        "authors": [
            "Carissa Schoenick",
            "Peter Clark",
            "Oyvind Tafjord",
            "Peter Turney",
            "Oren Etzioni"
        ],
        "abstract": "Given recent successes in AI (e.g., AlphaGo's victory against Lee Sedol in the game of GO), it's become increasingly important to assess: how close are AI systems to human-level intelligence? This paper describes the Allen AI Science Challenge---an approach towards that goal which led to a unique Kaggle Competition, its results, the lessons learned, and our next steps.\n    ",
        "submission_date": "2016-04-14T00:00:00",
        "last_modified_date": "2017-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04506",
        "title": "Integrating Know-How into the Linked Data Cloud",
        "authors": [
            "Paolo Pareti",
            "Benoit Testu",
            "Ryutaro Ichise",
            "Ewan Klein",
            "Adam Barker"
        ],
        "abstract": "This paper presents the first framework for integrating procedural knowledge, or \"know-how\", into the Linked Data Cloud. Know-how available on the Web, such as step-by-step instructions, is largely unstructured and isolated from other sources of online knowledge. To overcome these limitations, we propose extending to procedural knowledge the benefits that Linked Data has already brought to representing, retrieving and reusing declarative knowledge. We describe a framework for representing generic know-how as Linked Data and for automatically acquiring this representation from existing resources on the Web. This system also allows the automatic generation of links between different know-how resources, and between those resources and other online knowledge bases, such as DBpedia. We discuss the results of applying this framework to a real-world scenario and we show how it outperforms existing manual community-driven integration efforts.\n    ",
        "submission_date": "2016-04-15T00:00:00",
        "last_modified_date": "2016-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04660",
        "title": "Why Artificial Intelligence Needs a Task Theory --- And What It Might Look Like",
        "authors": [
            "Kristinn R. Th\u00f3risson",
            "Jordi Bieger",
            "Thr\u00f6stur Thorarensen",
            "J\u00f3na S. Sigur\u00f0ard\u00f3ttir",
            "Bas R. Steunebrink"
        ],
        "abstract": "The concept of \"task\" is at the core of artificial intelligence (AI): Tasks are used for training and evaluating AI systems, which are built in order to perform and automatize tasks we deem useful. In other fields of engineering theoretical foundations allow thorough evaluation of designs by methodical manipulation of well understood parameters with a known role and importance; this allows an aeronautics engineer, for instance, to systematically assess the effects of wind speed on an airplane's performance and stability. No framework exists in AI that allows this kind of methodical manipulation: Performance results on the few tasks in current use (cf. board games, question-answering) cannot be easily compared, however similar or different. The issue is even more acute with respect to artificial *general* intelligence systems, which must handle unanticipated tasks whose specifics cannot be known beforehand. A *task theory* would enable addressing tasks at the *class* level, bypassing their specifics, providing the appropriate formalization and classification of tasks, environments, and their parameters, resulting in more rigorous ways of measuring, comparing, and evaluating intelligent behavior. Even modest improvements in this direction would surpass the current ad-hoc nature of machine learning and AI evaluation. Here we discuss the main elements of the argument for a task theory and present an outline of what it might look like for physical tasks.\n    ",
        "submission_date": "2016-04-15T00:00:00",
        "last_modified_date": "2016-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04721",
        "title": "An artificial intelligence tool for heterogeneous team formation in the classroom",
        "authors": [
            "Juan M. Alberola",
            "Elena Del Val",
            "Victor Sanchez-Anguix",
            "Alberto Palomares",
            "Maria Dolores Teruel"
        ],
        "abstract": "Nowadays, there is increasing interest in the development of teamwork skills in the educational context. This growing interest is motivated by its pedagogical effectiveness and the fact that, in labour contexts, enterprises organize their employees in teams to carry out complex projects. Despite its crucial importance in the classroom and industry, there is a lack of support for the team formation process. Not only do many factors influence team performance, but the problem becomes exponentially costly if teams are to be optimized. In this article, we propose a tool whose aim it is to cover such a gap. It combines artificial intelligence techniques such as coalition structure generation, Bayesian learning, and Belbin's role theory to facilitate the generation of working groups in an educational context. This tool improves current state of the art proposals in three ways: i) it takes into account the feedback of other teammates in order to establish the most predominant role of a student instead of self-perception questionnaires; ii) it handles uncertainty with regard to each student's predominant team role; iii) it is iterative since it considers information from several interactions in order to improve the estimation of role assignments. We tested the performance of the proposed tool in an experiment involving students that took part in three different team activities. The experiments suggest that the proposed tool is able to improve different teamwork aspects such as team dynamics and student satisfaction.\n    ",
        "submission_date": "2016-04-16T00:00:00",
        "last_modified_date": "2016-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04789",
        "title": "A Hierarchical Genetic Optimization of a Fuzzy Logic System for Flow Control in Micro Grids",
        "authors": [
            "Enrico De Santis",
            "Antonello Rizzi",
            "Alireza Sadeghian"
        ],
        "abstract": "Bio-inspired algorithms like Genetic Algorithms and Fuzzy Inference Systems (FIS) are nowadays widely adopted as hybrid techniques in commercial and industrial environment. In this paper we present an interesting application of the fuzzy-GA paradigm to Smart Grids. The main aim consists in performing decision making for power flow management tasks in the proposed microgrid model equipped by renewable sources and an energy storage system, taking into account the economical profit in energy trading with the main-grid. In particular, this study focuses on the application of a Hierarchical Genetic Algorithm (HGA) for tuning the Rule Base (RB) of a Fuzzy Inference System (FIS), trying to discover a minimal fuzzy rules set in a Fuzzy Logic Controller (FLC) adopted to perform decision making in the microgrid. The HGA rationale focuses on a particular encoding scheme, based on control genes and parametric genes applied to the optimization of the FIS parameters, allowing to perform a reduction in the structural complexity of the RB. This approach will be referred in the following as fuzzy-HGA. Results are compared with a simpler approach based on a classic fuzzy-GA scheme, where both FIS parameters and rule weights are tuned, while the number of fuzzy rules is fixed in advance. Experiments shows how the fuzzy-HGA approach adopted for the synthesis of the proposed controller outperforms the classic fuzzy-GA scheme, increasing the accounting profit by 67\\% in the considered energy trading problem yielding at the same time a simpler RB.\n    ",
        "submission_date": "2016-04-16T00:00:00",
        "last_modified_date": "2017-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04795",
        "title": "KOGNAC: Efficient Encoding of Large Knowledge Graphs",
        "authors": [
            "Jacopo Urbani",
            "Sourav Dutta",
            "Sairam Gurajada",
            "Gerhard Weikum"
        ],
        "abstract": "Many Web applications require efficient querying of large Knowledge Graphs (KGs). We propose KOGNAC, a dictionary-encoding algorithm designed to improve SPARQL querying with a judicious combination of statistical and semantic techniques. In KOGNAC, frequent terms are detected with a frequency approximation algorithm and encoded to maximise compression. Infrequent terms are semantically grouped into ontological classes and encoded to increase data locality. We evaluated KOGNAC in combination with state-of-the-art RDF engines, and observed that it significantly improves SPARQL querying on KGs with up to 1B edges.\n    ",
        "submission_date": "2016-04-16T00:00:00",
        "last_modified_date": "2016-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04894",
        "title": "A global constraint for closed itemset mining",
        "authors": [
            "Mehdi Maamar",
            "Nadjib Lazaar",
            "Samir Loudni",
            "Yahia Lebbah"
        ],
        "abstract": "Discovering the set of closed frequent patterns is one of the fundamental problems in Data Mining. Recent Constraint Programming (CP) approaches for declarative itemset mining have proven their usefulness and flexibility. But the wide use of reified constraints in current CP approaches raises many difficulties to cope with high dimensional datasets. This paper proposes CLOSED PATTERN global constraint which does not require any reified constraints nor any extra variables to encode efficiently the Closed Frequent Pattern Mining (CFPM) constraint. CLOSED-PATTERN captures the particular semantics of the CFPM problem in order to ensure a polynomial pruning algorithm ensuring domain consistency. The computational properties of our constraint are analyzed and their practical effectiveness is experimentally evaluated.\n    ",
        "submission_date": "2016-04-17T00:00:00",
        "last_modified_date": "2016-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05006",
        "title": "Expressive Completeness of Existential Rule Languages for Ontology-based Query Answering",
        "authors": [
            "Heng Zhang",
            "Yan Zhang",
            "Jia-Huai You"
        ],
        "abstract": "Existential rules, also known as data dependencies in Databases, have been recently rediscovered as a promising family of languages for Ontology-based Query Answering. In this paper, we prove that disjunctive embedded dependencies exactly capture the class of recursively enumerable ontologies in Ontology-based Conjunctive Query Answering (OCQA). Our expressive completeness result does not rely on any built-in linear order on the database. To establish the expressive completeness, we introduce a novel semantic definition for OCQA ontologies. We also show that neither the class of disjunctive tuple-generating dependencies nor the class of embedded dependencies is expressively complete for recursively enumerable OCQA ontologies.\n    ",
        "submission_date": "2016-04-18T00:00:00",
        "last_modified_date": "2016-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05085",
        "title": "Mastering 2048 with Delayed Temporal Coherence Learning, Multi-Stage Weight Promotion, Redundant Encoding and Carousel Shaping",
        "authors": [
            "Wojciech Ja\u015bkowski"
        ],
        "abstract": "2048 is an engaging single-player, nondeterministic video puzzle game, which, thanks to the simple rules and hard-to-master gameplay, has gained massive popularity in recent years. As 2048 can be conveniently embedded into the discrete-state Markov decision processes framework, we treat it as a testbed for evaluating existing and new methods in reinforcement learning. With the aim to develop a strong 2048 playing program, we employ temporal difference learning with systematic n-tuple networks. We show that this basic method can be significantly improved with temporal coherence learning, multi-stage function approximator with weight promotion, carousel shaping, and redundant encoding. In addition, we demonstrate how to take advantage of the characteristics of the n-tuple network, to improve the algorithmic effectiveness of the learning process by i) delaying the (decayed) update and applying lock-free optimistic parallelism to effortlessly make advantage of multiple CPU cores. This way, we were able to develop the best known 2048 playing program to date, which confirms the effectiveness of the introduced methods for discrete-state Markov decision problems.\n    ",
        "submission_date": "2016-04-18T00:00:00",
        "last_modified_date": "2016-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05086",
        "title": "Normative Multiagent Systems: A Dynamic Generalization",
        "authors": [
            "Xiaowei Huang",
            "Ji Ruan",
            "Qingliang Chen",
            "Kaile Su"
        ],
        "abstract": "Social norms are powerful formalism in coordinating autonomous agents' behaviour to achieve certain objectives. In this paper, we propose a dynamic normative system to enable the reasoning of the changes of norms under different circumstances, which cannot be done in the existing static normative systems. We study two important problems (norm synthesis and norm recognition) related to the autonomy of the entire system and the agents, and characterise the computational complexities of solving these problems.\n    ",
        "submission_date": "2016-04-18T00:00:00",
        "last_modified_date": "2016-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05273",
        "title": "Learning Possibilistic Logic Theories from Default Rules",
        "authors": [
            "Ondrej Kuzelka",
            "Jesse Davis",
            "Steven Schockaert"
        ],
        "abstract": "We introduce a setting for learning possibilistic logic theories from defaults of the form \"if alpha then typically beta\". We first analyse this problem from the point of view of machine learning theory, determining the VC dimension of possibilistic stratifications as well as the complexity of the associated learning problems, after which we present a heuristic learning algorithm that can easily scale to thousands of defaults. An important property of our approach is that it is inherently able to handle noisy and conflicting sets of defaults. Among others, this allows us to learn possibilistic logic theories from crowdsourced data and to approximate propositional Markov logic networks using heuristic MAP solvers. We present experimental results that demonstrate the effectiveness of this approach.\n    ",
        "submission_date": "2016-04-18T00:00:00",
        "last_modified_date": "2016-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05288",
        "title": "Inductive Coherence",
        "authors": [
            "Scott Garrabrant",
            "Benya Fallenstein",
            "Abram Demski",
            "Nate Soares"
        ],
        "abstract": "While probability theory is normally applied to external environments, there has been some recent interest in probabilistic modeling of the outputs of computations that are too expensive to run. Since mathematical logic is a powerful tool for reasoning about computer programs, we consider this problem from the perspective of integrating probability and logic. Recent work on assigning probabilities to mathematical statements has used the concept of coherent distributions, which satisfy logical constraints such as the probability of a sentence and its negation summing to one. Although there are algorithms which converge to a coherent probability distribution in the limit, this yields only weak guarantees about finite approximations of these distributions. In our setting, this is a significant limitation: Coherent distributions assign probability one to all statements provable in a specific logical theory, such as Peano Arithmetic, which can prove what the output of any terminating computation is; thus, a coherent distribution must assign probability one to the output of any terminating computation. To model uncertainty about computations, we propose to work with approximations to coherent distributions. We introduce inductive coherence, a strengthening of coherence that provides appropriate constraints on finite approximations, and propose an algorithm which satisfies this criterion.\n    ",
        "submission_date": "2016-04-18T00:00:00",
        "last_modified_date": "2016-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05358",
        "title": "Text-based LSTM networks for Automatic Music Composition",
        "authors": [
            "Keunwoo Choi",
            "George Fazekas",
            "Mark Sandler"
        ],
        "abstract": "In this paper, we introduce new methods and discuss results of text-based LSTM (Long Short-Term Memory) networks for automatic music composition. The proposed network is designed to learn relationships within text documents that represent chord progressions and drum tracks in two case studies. In the experiments, word-RNNs (Recurrent Neural Networks) show good results for both cases, while character-based RNNs (char-RNNs) only succeed to learn chord progressions. The proposed system can be used for fully automatic composition or as semi-automatic systems that help humans to compose music by controlling a diversity parameter of the model.\n    ",
        "submission_date": "2016-04-18T00:00:00",
        "last_modified_date": "2016-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05419",
        "title": "Extending the Harper Identity to Iterated Belief Change",
        "authors": [
            "Jake Chandler",
            "Richard Booth"
        ],
        "abstract": "The field of iterated belief change has focused mainly on revision, with the other main operator of AGM belief change theory, i.e. contraction, receiving relatively little attention. In this paper we extend the Harper Identity from single-step change to define iterated contraction in terms of iterated revision. Specifically, just as the Harper Identity provides a recipe for defining the belief set resulting from contracting A in terms of (i) the initial belief set and (ii) the belief set resulting from revision by not-A, we look at ways to define the plausibility ordering over worlds resulting from contracting A in terms of (iii) the initial plausibility ordering, and (iv) the plausibility ordering resulting from revision by not-A. After noting that the most straightforward such extension leads to a trivialisation of the space of permissible orderings, we provide a family of operators for combining plausibility orderings that avoid such a result. These operators are characterised in our domain of interest by a pair of intuitively compelling properties, which turn out to enable the derivation of a number of iterated contraction postulates from postulates for iterated revision. We finish by observing that a salient member of this family allows for the derivation of counterparts for contraction of some well known iterated revision operators, as well as for defining new iterated contraction operators.\n    ",
        "submission_date": "2016-04-19T00:00:00",
        "last_modified_date": "2016-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05471",
        "title": "Managing Overstaying Electric Vehicles in Park-and-Charge Facilities",
        "authors": [
            "Arpita Biswas",
            "Ragavendran Gopalakrishnan",
            "Partha Dutta"
        ],
        "abstract": "With the increase in adoption of Electric Vehicles (EVs), proper utilization of the charging infrastructure is an emerging challenge for service providers. Overstaying of an EV after a charging event is a key contributor to low utilization. Since overstaying is easily detectable by monitoring the power drawn from the charger, managing this problem primarily involves designing an appropriate \"penalty\" during the overstaying period. Higher penalties do discourage overstaying; however, due to uncertainty in parking duration, less people would find such penalties acceptable, leading to decreased utilization (and revenue). To analyze this central trade-off, we develop a novel framework that integrates models for realistic user behavior into queueing dynamics to locate the optimal penalty from the points of view of utilization and revenue, for different values of the external charging demand. Next, when the model parameters are unknown, we show how an online learning algorithm, such as UCB, can be adapted to learn the optimal penalty. Our experimental validation, based on charging data from London, shows that an appropriate penalty can increase both utilization and revenue while significantly reducing overstaying.\n    ",
        "submission_date": "2016-04-19T00:00:00",
        "last_modified_date": "2016-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05472",
        "title": "Demand Prediction and Placement Optimization for Electric Vehicle Charging Stations",
        "authors": [
            "Ragavendran Gopalakrishnan",
            "Arpita Biswas",
            "Alefiya Lightwala",
            "Skanda Vasudevan",
            "Partha Dutta",
            "Abhishek Tripathi"
        ],
        "abstract": "Effective placement of charging stations plays a key role in Electric Vehicle (EV) adoption. In the placement problem, given a set of candidate sites, an optimal subset needs to be selected with respect to the concerns of both (a) the charging station service provider, such as the demand at the candidate sites and the budget for deployment, and (b) the EV user, such as charging station reachability and short waiting times at the station. This work addresses these concerns, making the following three novel contributions: (i) a supervised multi-view learning framework using Canonical Correlation Analysis (CCA) for demand prediction at candidate sites, using multiple datasets such as points of interest information, traffic density, and the historical usage at existing charging stations; (ii) a mixed-packing-and- covering optimization framework that models competing concerns of the service provider and EV users; (iii) an iterative heuristic to solve these problems by alternately invoking knapsack and set cover algorithms. The performance of the demand prediction model and the placement optimization heuristic are evaluated using real world data.\n    ",
        "submission_date": "2016-04-19T00:00:00",
        "last_modified_date": "2016-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05535",
        "title": "The SP theory of intelligence and the representation and processing of knowledge in the brain",
        "authors": [
            "J Gerard Wolff"
        ],
        "abstract": "The \"SP theory of intelligence\", with its realisation in the \"SP computer model\", aims to simplify and integrate observations and concepts across AI-related fields, with information compression as a unifying theme. This paper describes how abstract structures and processes in the theory may be realised in terms of neurons, their interconnections, and the transmission of signals between neurons. This part of the SP theory -- \"SP-neural\" -- is a tentative and partial model for the representation and processing of knowledge in the brain. In the SP theory (apart from SP-neural), all kinds of knowledge are represented with \"patterns\", where a pattern is an array of atomic symbols in one or two dimensions. In SP-neural, the concept of a \"pattern\" is realised as an array of neurons called a \"pattern assembly\", similar to Hebb's concept of a \"cell assembly\" but with important differences. Central to the processing of information in the SP system is the powerful concept of \"multiple alignment\", borrowed and adapted from bioinformatics. Processes such as pattern recognition, reasoning and problem solving are achieved via the building of multiple alignments, while unsupervised learning -- significantly different from the \"Hebbian\" kinds of learning -- is achieved by creating patterns from sensory information and also by creating patterns from multiple alignments in which there is a partial match between one pattern and another. Short-lived neural structures equivalent to multiple alignments will be created via an inter-play of excitatory and inhibitory neural signals. The paper discusses several associated issues, with relevant empirical evidence.\n    ",
        "submission_date": "2016-04-19T00:00:00",
        "last_modified_date": "2016-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05557",
        "title": "AGI and Reflexivity",
        "authors": [
            "Pascal Faudemay"
        ],
        "abstract": "We define a property of intelligent systems, which we call Reflexivity. In human beings, it is one aspect of consciousness, and an element of deliberation. We propose a conjecture, that this property is conditioned by a topological property of the processes which implement this reflexivity. These processes may be symbolic, or non symbolic e.g. connexionnist. An architecture which implements reflexivity may be based on the interaction of one or several modules of deep learning, which may be specialized or not, and interconnected in a relevant way. A necessary condition of reflexivity is the existence of recurrence in its processes, we will examine in which cases this condition may be sufficient. We will then examine how this topology and this property make possible the expression of a second property, the deliberation. In a final paragraph, we propose an evaluation of intelligent systems, based on the fulfillment of all or some of these properties.\n    ",
        "submission_date": "2016-04-15T00:00:00",
        "last_modified_date": "2016-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05636",
        "title": "Pattern-Based Approach to the Workflow Satisfiability Problem with User-Independent Constraints",
        "authors": [
            "Daniel Karapetyan",
            "Andrew J. Parkes",
            "Gregory Gutin",
            "Andrei Gagarin"
        ],
        "abstract": "The fixed parameter tractable (FPT) approach is a powerful tool in tackling computationally hard problems. In this paper, we link FPT results to classic artificial intelligence (AI) techniques to show how they complement each other. Specifically, we consider the workflow satisfiability problem (WSP) which asks whether there exists an assignment of authorised users to the steps in a workflow specification, subject to certain constraints on the assignment. It was shown by Cohen et al. (JAIR 2014) that WSP restricted to the class of user-independent constraints (UI), covering many practical cases, admits FPT algorithms, i.e. can be solved in time exponential only in the number of steps $k$ and polynomial in the number of users $n$. Since usually $k << n$ in WSP, such FPT algorithms are of great practical interest. We present a new interpretation of the FPT nature of the WSP with UI constraints giving a decomposition of the problem into two levels. Exploiting this two-level split, we develop a new FPT algorithm that is by many orders of magnitude faster than the previous state-of-the-art WSP algorithm and also has only polynomial-space complexity. We also introduce new pseudo-Boolean (PB) and Constraint Satisfaction (CSP) formulations of the WSP with UI constraints which efficiently exploit this new decomposition of the problem and raise the novel issue of how to use general-purpose solvers to tackle FPT problems in a fashion that meets FPT efficiency expectations. In our computational study, we investigate, for the first time, the phase transition (PT) properties of the WSP, under a model for generation of random instances. We show how PT studies can be extended, in a novel fashion, to support empirical evaluation of scaling of FPT algorithms.\n    ",
        "submission_date": "2016-04-19T00:00:00",
        "last_modified_date": "2019-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05791",
        "title": "Procedural urban environments for FPS games",
        "authors": [
            "Jan Kruse",
            "Ricardo Sosa",
            "Andy M. Connor"
        ],
        "abstract": "This paper presents a novel approach to procedural generation of urban maps for First Person Shooter (FPS) games. A multi-agent evolutionary system is employed to place streets, buildings and other items inside the Unity3D game engine, resulting in playable video game levels. A computational agent is trained using machine learning techniques to capture the intent of the game designer as part of the multi-agent system, and to enable a semi-automated aesthetic selection for the underlying genetic algorithm.\n    ",
        "submission_date": "2016-04-20T00:00:00",
        "last_modified_date": "2016-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06076",
        "title": "Question Answering via Integer Programming over Semi-Structured Knowledge",
        "authors": [
            "Daniel Khashabi",
            "Tushar Khot",
            "Ashish Sabharwal",
            "Peter Clark",
            "Oren Etzioni",
            "Dan Roth"
        ],
        "abstract": "Answering science questions posed in natural language is an important AI challenge. Answering such questions often requires non-trivial inference and knowledge that goes beyond factoid retrieval. Yet, most systems for this task are based on relatively shallow Information Retrieval (IR) and statistical correlation techniques operating on large unstructured corpora. We propose a structured inference system for this task, formulated as an Integer Linear Program (ILP), that answers natural language questions using a semi-structured knowledge base derived from text, including questions requiring multi-step inference and a combination of multiple facts. On a dataset of real, unseen science questions, our system significantly outperforms (+14%) the best previous attempt at structured reasoning for this task, which used Markov Logic Networks (MLNs). It also improves upon a previous ILP formulation by 17.7%. When combined with unstructured inference methods, the ILP system significantly boosts overall performance (+10%). Finally, we show our approach is substantially more robust to a simple answer perturbation compared to statistical correlation methods.\n    ",
        "submission_date": "2016-04-20T00:00:00",
        "last_modified_date": "2016-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06223",
        "title": "Task scheduling system for UAV operations in indoor environment",
        "authors": [
            "Yohanes Khosiawan",
            "Young Soo Park",
            "Ilkyeong Moon",
            "Janardhanan Mukund Nilakantan",
            "Izabela Nielsen"
        ],
        "abstract": "Application of UAV in indoor environment is emerging nowadays due to the advancements in technology. UAV brings more space-flexibility in an occupied or hardly-accessible indoor environment, e.g., shop floor of manufacturing industry, greenhouse, nuclear powerplant. UAV helps in creating an autonomous manufacturing system by executing tasks with less human intervention in time-efficient manner. Consequently, a scheduler is one essential component to be focused on; yet the number of reported studies on UAV scheduling has been minimal. This work proposes a methodology with a heuristic (based on Earliest Available Time algorithm) which assigns tasks to UAVs with an objective of minimizing the makespan. In addition, a quick response towards uncertain events and a quick creation of new high-quality feasible schedule are needed. Hence, the proposed heuristic is incorporated with Particle Swarm Optimization (PSO) algorithm to find a quick near optimal schedule. This proposed methodology is implemented into a scheduler and tested on a few scales of datasets generated based on a real flight demonstration. Performance evaluation of scheduler is discussed in detail and the best solution obtained from a selected set of parameters is reported.\n    ",
        "submission_date": "2016-04-21T00:00:00",
        "last_modified_date": "2016-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06356",
        "title": "Iterative Judgment Aggregation",
        "authors": [
            "Marija Slavkovik",
            "Wojciech Jamroga"
        ],
        "abstract": "Judgment aggregation problems form a class of collective decision-making problems represented in an abstract way, subsuming some well known problems such as voting. A collective decision can be reached in many ways, but a direct one-step aggregation of individual decisions is arguably most studied. Another way to reach collective decisions is by iterative consensus building -- allowing each decision-maker to change their individual decision in response to the choices of the other agents until a consensus is reached. Iterative consensus building has so far only been studied for voting problems. Here we propose an iterative judgment aggregation algorithm, based on movements in an undirected graph, and we study for which instances it terminates with a consensus. We also compare the computational complexity of our iterative procedure with that of related judgment aggregation operators.\n    ",
        "submission_date": "2016-04-21T00:00:00",
        "last_modified_date": "2016-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06484",
        "title": "Parallel Strategies Selection",
        "authors": [
            "Anthony Palmieri",
            "Jean-Charles R\u00e9gin",
            "Pierre Schaus"
        ],
        "abstract": "We consider the problem of selecting the best variable-value strategy for solving a given problem in constraint programming. We show that the recent Embarrassingly Parallel Search method (EPS) can be used for this purpose. EPS proposes to solve a problem by decomposing it in a lot of subproblems and to give them on-demand to workers which run in parallel. Our method uses a part of these subproblems as a simple sample as defined in statistics for comparing some strategies in order to select the most promising one that will be used for solving the remaining subproblems. For each subproblem of the sample, the parallelism helps us to control the running time of the strategies because it gives us the possibility to introduce timeouts by stopping a strategy when it requires more than twice the time of the best one. Thus, we can deal with the great disparity in solving times for the strategies. The selections we made are based on the Wilcoxon signed rank tests because no assumption has to be made on the distribution of the solving times and because these tests can deal with the censored data that we obtain after introducing timeouts. The experiments we performed on a set of classical benchmarks for satisfaction and optimization problems show that our method obtain good performance by selecting almost all the time the best variable-value strategy and by almost never choosing a variable-value strategy which is dramatically slower than the best one. Our method also outperforms the portfolio approach consisting in running some strategies in parallel and is competitive with the multi armed bandit framework.\n    ",
        "submission_date": "2016-04-21T00:00:00",
        "last_modified_date": "2016-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06614",
        "title": "Agenda Separability in Judgment Aggregation",
        "authors": [
            "J\u00e9r\u00f4me Lang",
            "Marija Slavkovik",
            "Srdjan Vesic"
        ],
        "abstract": "One of the better studied properties for operators in judgment aggregation is independence, which essentially dictates that the collective judgment on one issue should not depend on the individual judgments given on some other issue(s) in the same agenda. Independence, although considered a desirable property, is too strong, because together with mild additional conditions it implies dictatorship. We propose here a weakening of independence, named agenda separability: a judgment aggregation rule satisfies it if, whenever the agenda is composed of several independent sub-agendas, the resulting collective judgment sets can be computed separately for each sub-agenda and then put together. We show that this property is discriminant, in the sense that among judgment aggregation rules so far studied in the literature, some satisfy it and some do not. We briefly discuss the implications of agenda separability on the computation of judgment aggregation rules.\n    ",
        "submission_date": "2016-04-22T00:00:00",
        "last_modified_date": "2016-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06641",
        "title": "Compact-Table: Efficiently Filtering Table Constraints with Reversible Sparse Bit-Sets",
        "authors": [
            "Jordan Demeulenaere",
            "Renaud Hartert",
            "Christophe Lecoutre",
            "Guillaume Perez",
            "Laurent Perron",
            "Jean-Charles R\u00e9gin",
            "Pierre Schaus"
        ],
        "abstract": "In this paper, we describe Compact-Table (CT), a bitwise algorithm to enforce Generalized Arc Consistency (GAC) on table con- straints. Although this algorithm is the default propagator for table constraints in or-tools and OscaR, two publicly available CP solvers, it has never been described so far. Importantly, CT has been recently improved further with the introduction of residues, resetting operations and a data-structure called reversible sparse bit-set, used to maintain tables of supports (following the idea of tabular reduction): tuples are invalidated incrementally on value removals by means of bit-set operations. The experimentation that we have conducted with OscaR shows that CT outperforms state-of-the-art algorithms STR2, STR3, GAC4R, MDD4R and AC5-TC on standard benchmarks.\n    ",
        "submission_date": "2016-04-22T00:00:00",
        "last_modified_date": "2016-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06710",
        "title": "Using Reinforcement Learning to Validate Empirical Game-Theoretic Analysis: A Continuous Double Auction Study",
        "authors": [
            "Mason Wright"
        ],
        "abstract": "Empirical game-theoretic analysis (EGTA) has recently been applied successfully to analyze the behavior of large numbers of competing traders in a continuous double auction market. Multiagent simulation methods like EGTA are useful for studying complex strategic environments like a stock market, where it is not feasible to solve analytically for the rational behavior of each agent. A weakness of simulation-based methods in strategic settings, however, is that it is typically impossible to prove that the strategy profile assigned to the simulated agents is stable, as in a Nash equilibrium. I propose using reinforcement learning to analyze the regret of supposed Nash-equilibrium strategy profiles found by EGTA. I have developed a new library of reinforcement learning tools, which I have integrated into an extended version of the market simulator from our prior work. I provide evidence for the effectiveness of our library methods, both on a suite of benchmark problems from the literature, and on non-equilibrium strategy profiles in our market environment. Finally, I use our new reinforcement learning tools to provide evidence that the equilibria found by EGTA in our recent continuous double auction study are likely to have only negligible regret, even with respect to an extended strategy space.\n    ",
        "submission_date": "2016-04-22T00:00:00",
        "last_modified_date": "2016-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06715",
        "title": "Parameterized Compilation Lower Bounds for Restricted CNF-formulas",
        "authors": [
            "Stefan Mengel"
        ],
        "abstract": "We show unconditional parameterized lower bounds in the area of knowledge compilation, more specifically on the size of circuits in decomposable negation normal form (DNNF) that encode CNF-formulas restricted by several graph width measures. In particular, we show that\n",
        "submission_date": "2016-04-22T00:00:00",
        "last_modified_date": "2016-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06721",
        "title": "Exploiting Deep Semantics and Compositionality of Natural Language for Human-Robot-Interaction",
        "authors": [
            "Manfred Eppe",
            "Sean Trott",
            "Jerome Feldman"
        ],
        "abstract": "We develop a natural language interface for human robot interaction that implements reasoning about deep semantics in natural language. To realize the required deep analysis, we employ methods from cognitive linguistics, namely the modular and compositional framework of Embodied Construction Grammar (ECG) [Feldman, 2009]. Using ECG, robots are able to solve fine-grained reference resolution problems and other issues related to deep semantics and compositionality of natural language. This also includes verbal interaction with humans to clarify commands and queries that are too ambiguous to be executed safely. We implement our NLU framework as a ROS package and present proof-of-concept scenarios with different robots, as well as a survey on the state of the art.\n    ",
        "submission_date": "2016-04-22T00:00:00",
        "last_modified_date": "2016-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06787",
        "title": "Utilitarian Distributed Constraint Optimization Problems",
        "authors": [
            "Julien Savaux",
            "Julien Vion",
            "Sylvain Piechowiak",
            "Ren\u00e9 Mandiau",
            "Toshihiro Matsui",
            "Katsutoshi Hirayama",
            "Makoto Yokoo",
            "Shakre Elmane",
            "Marius Silaghi"
        ],
        "abstract": "Privacy has been a major motivation for distributed problem optimization. However, even though several methods have been proposed to evaluate it, none of them is widely used. The Distributed Constraint Optimization Problem (DCOP) is a fundamental model used to approach various families of distributed problems. As privacy loss does not occur when a solution is accepted, but when it is proposed, privacy requirements cannot be interpreted as a criteria of the objective function of the DCOP. Here we approach the problem by letting both the optimized costs found in DCOPs and the privacy requirements guide the agents' exploration of the search space. We introduce Utilitarian Distributed Constraint Optimization Problem (UDCOP) where the costs and the privacy requirements are used as parameters to a heuristic modifying the search process. Common stochastic algorithms for decentralized constraint optimization problems are evaluated here according to how well they preserve privacy. Further, we propose some extensions where these solvers modify their search process to take into account their privacy requirements, succeeding in significantly reducing their privacy loss without significant degradation of the solution quality.\n    ",
        "submission_date": "2016-04-22T00:00:00",
        "last_modified_date": "2016-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06790",
        "title": "DisCSPs with Privacy Recast as Planning Problems for Utility-based Agents",
        "authors": [
            "Julien Savaux",
            "Julien Vion",
            "Sylvain Piechowiak",
            "Ren\u00e9 Mandiau",
            "Toshihiro Matsui",
            "Katsutoshi Hirayama",
            "Makoto Yokoo",
            "Shakre Elmane",
            "Marius Silaghi"
        ],
        "abstract": "Privacy has traditionally been a major motivation for decentralized problem solving. However, even though several metrics have been proposed to quantify it, none of them is easily integrated with common solvers. Constraint programming is a fundamental paradigm used to approach various families of problems. We introduce Utilitarian Distributed Constraint Satisfaction Problems (UDisCSP) where the utility of each state is estimated as the difference between the the expected rewards for agreements on assignments for shared variables, and the expected cost of privacy loss. Therefore, a traditional DisCSP with privacy requirements is viewed as a planning problem. The actions available to agents are: communication and local inference. Common decentralized solvers are evaluated here from the point of view of their interpretation as greedy planners. Further, we investigate some simple extensions where these solvers start taking into account the utility function. In these extensions we assume that the planning problem is further restricting the set of communication actions to only the communication primitives present in the corresponding solver protocols. The solvers obtained for the new type of problems propose the action (communication/inference) to be performed in each situation, defining thereby the policy.\n    ",
        "submission_date": "2016-04-22T00:00:00",
        "last_modified_date": "2016-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06849",
        "title": "A Computational Model for Situated Task Learning with Interactive Instruction",
        "authors": [
            "Shiwali Mohan",
            "James Kirk",
            "John Laird"
        ],
        "abstract": "Learning novel tasks is a complex cognitive activity requiring the learner to acquire diverse declarative and procedural knowledge. Prior ACT-R models of acquiring task knowledge from instruction focused on learning procedural knowledge from declarative instructions encoded in semantic memory. In this paper, we identify the requirements for designing compu- tational models that learn task knowledge from situated task- oriented interactions with an expert and then describe and evaluate a model of learning from situated interactive instruc- tion that is implemented in the Soar cognitive architecture.\n    ",
        "submission_date": "2016-04-23T00:00:00",
        "last_modified_date": "2016-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06954",
        "title": "RHOG: A Refinement-Operator Library for Directed Labeled Graphs",
        "authors": [
            "Santiago Onta\u00f1\u00f3n"
        ],
        "abstract": "This document provides the foundations behind the functionality provided by the $\\rho$G library (",
        "submission_date": "2016-04-23T00:00:00",
        "last_modified_date": "2020-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06963",
        "title": "Limits to Verification and Validation of Agentic Behavior",
        "authors": [
            "David J. Jilk"
        ],
        "abstract": "Verification and validation of agentic behavior have been suggested as important research priorities in efforts to reduce risks associated with the creation of general artificial intelligence (Russell et al 2015). In this paper we question the appropriateness of using language of certainty with respect to efforts to manage that risk. We begin by establishing a very general formalism to characterize agentic behavior and to describe standards of acceptable behavior. We show that determination of whether an agent meets any particular standard is not computable. We discuss the extent of the burden associated with verification by manual proof and by automated behavioral governance. We show that to ensure decidability of the behavioral standard itself, one must further limit the capabilities of the agent. We then demonstrate that if our concerns relate to outcomes in the physical world, attempts at validation are futile. Finally, we show that layered architectures aimed at making these challenges tractable mistakenly equate intentions with actions or outcomes, thereby failing to provide any guarantees. We conclude with a discussion of why language of certainty should be eradicated from the conversation about the safety of general artificial intelligence.\n    ",
        "submission_date": "2016-04-23T00:00:00",
        "last_modified_date": "2016-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06970",
        "title": "Bayesian Inference of Recursive Sequences of Group Activities from Tracks",
        "authors": [
            "Ernesto Brau",
            "Colin Dawson",
            "Alfredo Carrillo",
            "David Sidi",
            "Clayton T. Morrison"
        ],
        "abstract": "We present a probabilistic generative model for inferring a description of coordinated, recursively structured group activities at multiple levels of temporal granularity based on observations of individuals' trajectories. The model accommodates: (1) hierarchically structured groups, (2) activities that are temporally and compositionally recursive, (3) component roles assigning different subactivity dynamics to subgroups of participants, and (4) a nonparametric Gaussian Process model of trajectories. We present an MCMC sampling framework for performing joint inference over recursive activity descriptions and assignment of trajectories to groups, integrating out continuous parameters. We demonstrate the model's expressive power in several simulated and complex real-world scenarios from the VIRAT and UCLA Aerial Event video data sets.\n    ",
        "submission_date": "2016-04-24T00:00:00",
        "last_modified_date": "2016-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07095",
        "title": "Deep Learning for Reward Design to Improve Monte Carlo Tree Search in ATARI Games",
        "authors": [
            "Xiaoxiao Guo",
            "Satinder Singh",
            "Richard Lewis",
            "Honglak Lee"
        ],
        "abstract": "Monte Carlo Tree Search (MCTS) methods have proven powerful in planning for sequential decision-making problems such as Go and video games, but their performance can be poor when the planning depth and sampling trajectories are limited or when the rewards are sparse. We present an adaptation of PGRD (policy-gradient for reward-design) for learning a reward-bonus function to improve UCT (a MCTS algorithm). Unlike previous applications of PGRD in which the space of reward-bonus functions was limited to linear functions of hand-coded state-action-features, we use PGRD with a multi-layer convolutional neural network to automatically learn features from raw perception as well as to adapt the non-linear reward-bonus function parameters. We also adopt a variance-reducing gradient method to improve PGRD's performance. The new method improves UCT's performance on multiple ATARI games compared to UCT without the reward bonus. Combining PGRD and Deep Learning in this way should make adapting rewards for MCTS algorithms far more widely and practically applicable than before.\n    ",
        "submission_date": "2016-04-24T00:00:00",
        "last_modified_date": "2016-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07097",
        "title": "Neurohex: A Deep Q-learning Hex Agent",
        "authors": [
            "Kenny Young",
            "Ryan Hayward",
            "Gautham Vasan"
        ],
        "abstract": "DeepMind's recent spectacular success in using deep convolutional neural nets and machine learning to build superhuman level agents --- e.g. for Atari games via deep Q-learning and for the game of Go via Reinforcement Learning --- raises many questions, including to what extent these methods will succeed in other domains. In this paper we consider DQL for the game of Hex: after supervised initialization, we use selfplay to train NeuroHex, an 11-layer CNN that plays Hex on the 13x13 board. Hex is the classic two-player alternate-turn stone placement game played on a rhombus of hexagonal cells in which the winner is whomever connects their two opposing sides. Despite the large action and state space, our system trains a Q-network capable of strong play with no search. After two weeks of Q-learning, NeuroHex achieves win-rates of 20.4% as first player and 2.1% as second player against a 1-second/move version of MoHex, the current ICGA Olympiad Hex champion. Our data suggests further improvement might be possible with more training time.\n    ",
        "submission_date": "2016-04-24T00:00:00",
        "last_modified_date": "2016-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07183",
        "title": "AGM-Style Revision of Beliefs and Intentions from a Database Perspective (Preliminary Version)",
        "authors": [
            "Marc van Zee",
            "Dragan Doder"
        ],
        "abstract": "We introduce a logic for temporal beliefs and intentions based on Shoham's database perspective. We separate strong beliefs from weak beliefs. Strong beliefs are independent from intentions, while weak beliefs are obtained by adding intentions to strong beliefs and everything that follows from that. We formalize coherence conditions on strong beliefs and intentions. We provide AGM-style postulates for the revision of strong beliefs and intentions. We show in a representation theorem that a revision operator satisfying our postulates can be represented by a pre-order on interpretations of the beliefs, together with a selection function for the intentions.\n    ",
        "submission_date": "2016-04-25T00:00:00",
        "last_modified_date": "2016-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07255",
        "title": "A Deep Hierarchical Approach to Lifelong Learning in Minecraft",
        "authors": [
            "Chen Tessler",
            "Shahar Givony",
            "Tom Zahavy",
            "Daniel J. Mankowitz",
            "Shie Mannor"
        ],
        "abstract": "We propose a lifelong learning system that has the ability to reuse and transfer knowledge from one task to another while efficiently retaining the previously learned knowledge-base. Knowledge is transferred by learning reusable skills to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem. These reusable skills, which we refer to as Deep Skill Networks, are then incorporated into our novel Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture using two techniques: (1) a deep skill array and (2) skill distillation, our novel variation of policy distillation (Rusu et. al. 2015) for learning skills. Skill distillation enables the HDRLN to efficiently retain knowledge and therefore scale in lifelong learning, by accumulating knowledge and encapsulating multiple reusable skills into a single distilled network. The H-DRLN exhibits superior performance and lower learning sample complexity compared to the regular Deep Q Network (Mnih et. al. 2015) in sub-domains of Minecraft.\n    ",
        "submission_date": "2016-04-25T00:00:00",
        "last_modified_date": "2016-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07312",
        "title": "Endgame Analysis of Dou Shou Qi",
        "authors": [
            "Jan N. van Rijn",
            "Jonathan K. Vis"
        ],
        "abstract": "Dou Shou Qi is a game in which two players control a number of pieces, each of them aiming to move one of their pieces onto a given square. We implemented an engine for analyzing the game. Moreover, we created a series of endgame tablebases containing all configurations with up to four pieces. These tablebases are the first steps towards theoretically solving the game. Finally, we constructed decision trees based on the endgame tablebases. In this note we report on some interesting patterns.\n    ",
        "submission_date": "2016-04-25T00:00:00",
        "last_modified_date": "2016-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07429",
        "title": "Balancing Appearance and Context in Sketch Interpretation",
        "authors": [
            "Yale Song",
            "Randall Davis",
            "Kaichen Ma",
            "Dana L. Penny"
        ],
        "abstract": "We describe a sketch interpretation system that detects and classifies clock numerals created by subjects taking the Clock Drawing Test, a clinical tool widely used to screen for cognitive impairments (e.g., dementia). We describe how it balances appearance and context, and document its performance on some 2,000 drawings (about 24K clock numerals) produced by a wide spectrum of patients. We calibrate the utility of different forms of context, describing experiments with Conditional Random Fields trained and tested using a variety of features. We identify context that contributes to interpreting otherwise ambiguous or incomprehensible strokes. We describe ST-slices, a novel representation that enables \"unpeeling\" the layers of ink that result when people overwrite, which often produces ink impossible to analyze if only the final drawing is examined. We characterize when ST-slices work, calibrate their impact on performance, and consider their breadth of applicability.\n    ",
        "submission_date": "2016-04-25T00:00:00",
        "last_modified_date": "2016-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07625",
        "title": "Mutual Transformation of Information and Knowledge",
        "authors": [
            "Olegs Verhodubs"
        ],
        "abstract": "Information and knowledge are transformable into each other. Information transformation into knowledge by the example of rule generation from OWL (Web Ontology Language) ontology has been shown during the development of the SWES (Semantic Web Expert System). The SWES is expected as an expert system for searching OWL ontologies from the Web, generating rules from the found ontologies and supplementing the SWES knowledge base with these rules. The purpose of this paper is to show knowledge transformation into information by the example of ontology generation from rules.\n    ",
        "submission_date": "2016-04-26T00:00:00",
        "last_modified_date": "2016-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07704",
        "title": "Tournament selection in zeroth-level classifier systems based on average reward reinforcement learning",
        "authors": [
            "Zhaoxiang Zang",
            "Zhao Li",
            "Junying Wang",
            "Zhiping Dan"
        ],
        "abstract": "As a genetics-based machine learning technique, zeroth-level classifier system (ZCS) is based on a discounted reward reinforcement learning algorithm, bucket-brigade algorithm, which optimizes the discounted total reward received by an agent but is not suitable for all multi-step problems, especially large-size ones. There are some undiscounted reinforcement learning methods available, such as R-learning, which optimize the average reward per time step. In this paper, R-learning is used as the reinforcement learning employed by ZCS, to replace its discounted reward reinforcement learning approach, and tournament selection is used to replace roulette wheel selection in ZCS. The modification results in classifier systems that can support long action chains, and thus is able to solve large multi-step problems.\n    ",
        "submission_date": "2016-04-26T00:00:00",
        "last_modified_date": "2016-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07806",
        "title": "Using Indirect Encoding of Multiple Brains to Produce Multimodal Behavior",
        "authors": [
            "Jacob Schrum",
            "Joel Lehman",
            "Sebastian Risi"
        ],
        "abstract": "An important challenge in neuroevolution is to evolve complex neural networks with multiple modes of behavior. Indirect encodings can potentially answer this challenge. Yet in practice, indirect encodings do not yield effective multimodal controllers. Thus, this paper introduces novel multimodal extensions to HyperNEAT, a popular indirect encoding. A previous multimodal HyperNEAT approach called situational policy geometry assumes that multiple brains benefit from being embedded within an explicit geometric space. However, experiments here illustrate that this assumption unnecessarily constrains evolution, resulting in lower performance. Specifically, this paper introduces HyperNEAT extensions for evolving many brains without assuming geometric relationships between them. The resulting Multi-Brain HyperNEAT can exploit human-specified task divisions to decide when each brain controls the agent, or can automatically discover when brains should be used, by means of preference neurons. A further extension called module mutation allows evolution to discover the number of brains, enabling multimodal behavior with even less expert knowledge. Experiments in several multimodal domains highlight that multi-brain approaches are more effective than HyperNEAT without multimodal extensions, and show that brains without a geometric relation to each other outperform situational policy geometry. The conclusion is that Multi-Brain HyperNEAT provides several promising techniques for evolving complex multimodal behavior.\n    ",
        "submission_date": "2016-04-26T00:00:00",
        "last_modified_date": "2016-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07906",
        "title": "Procedural Generation of Angry Birds Levels using Building Constructive Grammar with Chinese-Style and/or Japanese-Style Models",
        "authors": [
            "YuXuan Jiang",
            "Misaki Kaidan",
            "Chun Yin Chu",
            "Tomohiro Harada",
            "Ruck Thawonmas"
        ],
        "abstract": "This paper presents a procedural generation method that creates visually attractive levels for the Angry Birds game. Besides being an immensely popular mobile game, Angry Birds has recently become a test bed for various artificial intelligence technologies. We propose a new approach for procedurally generating Angry Birds levels using Chinese style and Japanese style building structures. A conducted experiment confirms the effectiveness of our approach with statistical significance.\n    ",
        "submission_date": "2016-04-27T00:00:00",
        "last_modified_date": "2016-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07990",
        "title": "Probabilistic Graphical Models on Multi-Core CPUs using Java 8",
        "authors": [
            "Andres R. Masegosa",
            "Ana M. Martinez",
            "Hanen Borchani"
        ],
        "abstract": "In this paper, we discuss software design issues related to the development of parallel computational intelligence algorithms on multi-core CPUs, using the new Java 8 functional programming features. In particular, we focus on probabilistic graphical models (PGMs) and present the parallelisation of a collection of algorithms that deal with inference and learning of PGMs from data. Namely, maximum likelihood estimation, importance sampling, and greedy search for solving combinatorial optimisation problems. Through these concrete examples, we tackle the problem of defining efficient data structures for PGMs and parallel processing of same-size batches of data sets using Java 8 features. We also provide straightforward techniques to code parallel algorithms that seamlessly exploit multi-core processors. The experimental analysis, carried out using our open source AMIDST (Analysis of MassIve Data STreams) Java toolbox, shows the merits of the proposed solutions.\n    ",
        "submission_date": "2016-04-27T00:00:00",
        "last_modified_date": "2016-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08055",
        "title": "Selecting the Selection",
        "authors": [
            "Giles Reger",
            "Martin Suda",
            "Andrei Voronkov",
            "Krystof Hoder"
        ],
        "abstract": "Modern saturation-based Automated Theorem Provers typically implement the superposition calculus for reasoning about first-order logic with or without equality. Practical implementations of this calculus use a variety of literal selections and term orderings to tame the growth of the search space and help steer proof search. This paper introduces the notion of lookahead selection that estimates (looks ahead) the effect on the search space of selecting a literal. There is also a case made for the use of incomplete selection functions that attempt to restrict the search space instead of satisfying some completeness criteria. Experimental evaluation in the \\Vampire\\ theorem prover shows that both lookahead selection and incomplete selection significantly contribute to solving hard problems unsolvable by other methods.\n    ",
        "submission_date": "2016-04-27T00:00:00",
        "last_modified_date": "2016-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08148",
        "title": "Defining Concepts of Emotion: From Philosophy to Science",
        "authors": [
            "Changqing Liu"
        ],
        "abstract": "This paper is motivated by a series of (related) questions as to whether a computer can have pleasure and pain, what pleasure (and intensity of pleasure) is, and, ultimately, what concepts of emotion are.\n",
        "submission_date": "2016-02-11T00:00:00",
        "last_modified_date": "2016-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08229",
        "title": "Propositional Abduction with Implicit Hitting Sets",
        "authors": [
            "Alexey Ignatiev",
            "Antonio Morgado",
            "Joao Marques-Silva"
        ],
        "abstract": "Logic-based abduction finds important applications in artificial intelligence and related areas. One application example is in finding explanations for observed phenomena. Propositional abduction is a restriction of abduction to the propositional domain, and complexity-wise is in the second level of the polynomial hierarchy. Recent work has shown that exploiting implicit hitting sets and propositional satisfiability (SAT) solvers provides an efficient approach for propositional abduction. This paper investigates this earlier work and proposes a number of algorithmic improvements. These improvements are shown to yield exponential reductions in the number of SAT solver calls. More importantly, the experimental results show significant performance improvements compared to the the best approaches for propositional abduction.\n    ",
        "submission_date": "2016-04-27T00:00:00",
        "last_modified_date": "2016-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08268",
        "title": "Quantum Cognition Beyond Hilbert Space I: Fundamentals",
        "authors": [
            "Diederik Aerts",
            "Lyneth Beltran",
            "Massimiliano Sassoli de Bianchi",
            "Sandro Sozzo",
            "Tomas Veloz"
        ],
        "abstract": "The formalism of quantum theory in Hilbert space has been applied with success to the modeling and explanation of several cognitive phenomena, whereas traditional cognitive approaches were problematical. However, this 'quantum cognition paradigm' was recently challenged by its proven impossibility to simultaneously model 'question order effects' and 'response replicability'. In Part I of this paper we describe sequential dichotomic measurements within an operational and realistic framework for human cognition elaborated by ourselves, and represent them in a quantum-like 'extended Bloch representation' where the Born rule of quantum probability does not necessarily hold. In Part II we apply this mathematical framework to successfully model question order effects, response replicability and unpacking effects, thus opening the way toward quantum cognition beyond Hilbert space.\n    ",
        "submission_date": "2016-04-27T00:00:00",
        "last_modified_date": "2016-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08270",
        "title": "Quantum cognition beyond Hilbert space II: Applications",
        "authors": [
            "Diederik Aerts",
            "Lyneth Beltran",
            "Massimiliano Sassoli de Bianchi",
            "Sandro Sozzo",
            "Tomas Veloz"
        ],
        "abstract": "The research on human cognition has recently benefited from the use of the mathematical formalism of quantum theory in Hilbert space. However, cognitive situations exist which indicate that the Hilbert space structure, and the associated Born rule, would be insufficient to provide a satisfactory modeling of the collected data, so that one needs to go beyond Hilbert space. In Part I of this paper we follow this direction and present a general tension-reduction (GTR) model, in the ambit of an operational and realistic framework for human cognition. In this Part II we apply this non-Hilbertian quantum-like model to faithfully reproduce the probabilities of the 'Clinton/Gore' and 'Rose/Jackson' experiments on question order effects. We also explain why the GTR-model is needed if one wants to deal, in a fully consistent way, with response replicability and unpacking effects.\n    ",
        "submission_date": "2016-04-27T00:00:00",
        "last_modified_date": "2016-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08340",
        "title": "Semantic Reasoning for Context-aware Internet of Things Applications",
        "authors": [
            "Altti Ilari Maarala",
            "Xiang Su",
            "Jukka Riekki"
        ],
        "abstract": "Advances in ICT are bringing into reality the vision of a large number of uniquely identifiable, interconnected objects and things that gather information from diverse physical environments and deliver the information to a variety of innovative applications and services. These sensing objects and things form the Internet of Things (IoT) that can improve energy and cost efficiency and automation in many different industry fields such as transportation and logistics, health care and manufacturing, and facilitate our everyday lives as well. IoT applications rely on real-time context data and allow sending information for driving the behaviors of users in intelligent environments.\n    ",
        "submission_date": "2016-04-28T00:00:00",
        "last_modified_date": "2016-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08709",
        "title": "\"Knowing value\" logic as a normal modal logic",
        "authors": [
            "Tao Gu",
            "Yanjing Wang"
        ],
        "abstract": "Recent years witness a growing interest in nonstandard epistemic logics of \"knowing whether\", \"knowing what\", \"knowing how\", and so on. These logics are usually not normal, i.e., the standard axioms and reasoning rules for modal logic may be invalid. In this paper, we show that the conditional \"knowing value\" logic proposed by Wang and Fan \\cite{WF13} can be viewed as a disguised normal modal logic by treating the negation of the Kv operator as a special diamond. Under this perspective, it turns out that the original first-order Kripke semantics can be greatly simplified by introducing a ternary relation $R_i^c$ in standard Kripke models, which associates one world with two $i$-accessible worlds that do not agree on the value of constant $c$. Under intuitive constraints, the modal logic based on such Kripke models is exactly the one studied by Wang and Fan (2013,2014}. Moreover, there is a very natural binary generalization of the \"knowing value\" diamond, which, surprisingly, does not increase the expressive power of the logic. The resulting logic with the binary diamond has a transparent normal modal system, which sharpens our understanding of the \"knowing value\" logic and simplifies some previously hard problems.\n    ",
        "submission_date": "2016-04-29T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08768",
        "title": "Supervisory Control for Behavior Composition",
        "authors": [
            "Paolo Felli",
            "Nitin Yadav",
            "Sebastian Sardina"
        ],
        "abstract": "We relate behavior composition, a synthesis task studied in AI, to supervisory control theory from the discrete event systems field. In particular, we show that realizing (i.e., implementing) a target behavior module (e.g., a house surveillance system) by suitably coordinating a collection of available behaviors (e.g., automatic blinds, doors, lights, cameras, etc.) amounts to imposing a supervisor onto a special discrete event system. Such a link allows us to leverage on the solid foundations and extensive work on discrete event systems, including borrowing tools and ideas from that field. As evidence of that we show how simple it is to introduce preferences in the mapped framework.\n    ",
        "submission_date": "2016-04-29T00:00:00",
        "last_modified_date": "2016-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.00241",
        "title": "Common-Description Learning: A Framework for Learning Algorithms and Generating Subproblems from Few Examples",
        "authors": [
            "Basem G. El-Barashy"
        ],
        "abstract": "Current learning algorithms face many difficulties in learning simple patterns and using them to learn more complex ones. They also require more examples than humans do to learn the same pattern, assuming no prior knowledge. In this paper, a new learning framework is introduced that is called common-description learning (CDL). This framework has been tested on 32 small multi-task datasets, and the results show that it was able to learn complex algorithms from a few number of examples. The final model is perfectly interpretable and its depth depends on the question. What is meant by depth here is that whenever needed, the model learns to break down the problem into simpler subproblems and solves them using previously learned models. Finally, we explain the capabilities of our framework in discovering complex relations in data and how it can help in improving language understanding in machines.\n    ",
        "submission_date": "2016-05-01T00:00:00",
        "last_modified_date": "2016-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.00495",
        "title": "Coalition Formability Semantics with Conflict-Eliminable Sets of Arguments",
        "authors": [
            "Ryuta Arisaka",
            "Ken Satoh"
        ],
        "abstract": "We consider abstract-argumentation-theoretic coalition formability in this work. Taking a model from political alliance among political parties, we will contemplate profitability, and then formability, of a coalition. As is commonly understood, a group forms a coalition with another group for a greater good, the goodness measured against some criteria. As is also commonly understood, however, a coalition may deliver benefits to a group X at the sacrifice of something that X was able to do before coalition formation, which X may be no longer able to do under the coalition. Use of the typical conflict-free sets of arguments is not very fitting for accommodating this aspect of coalition, which prompts us to turn to a weaker notion, conflict-eliminability, as a property that a set of arguments should primarily satisfy. We require numerical quantification of attack strengths as well as of argument strengths for its characterisation. We will first analyse semantics of profitability of a given conflict-eliminable set forming a coalition with another conflict-eliminable set, and will then provide four coalition formability semantics, each of which formalises certain utility postulate(s) taking the coalition profitability into account.\n    ",
        "submission_date": "2016-05-02T00:00:00",
        "last_modified_date": "2017-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.00686",
        "title": "Adaptive Candidate Generation for Scalable Edge-discovery Tasks on Data Graphs",
        "authors": [
            "Mayank Kejriwal"
        ],
        "abstract": "Several `edge-discovery' applications over graph-based data models are known to have worst-case quadratic time complexity in the nodes, even if the discovered edges are sparse. One example is the generic link discovery problem between two graphs, which has invited research interest in several communities. Specific versions of this problem include link prediction in social networks, ontology alignment between metadata-rich RDF data, approximate joins, and entity resolution between instance-rich data. As large datasets continue to proliferate, reducing quadratic complexity to make the task practical is an important research problem. Within the entity resolution community, the problem is commonly referred to as blocking. A particular class of learnable blocking schemes is known as Disjunctive Normal Form (DNF) blocking schemes, and has emerged as state-of-the art for homogeneous (i.e. same-schema) tabular data. Despite the promise of these schemes, a formalism or learning framework has not been developed for them when input data instances are generic, attributed graphs possessing both node and edge heterogeneity. With such a development, the complexity-reducing scope of DNF schemes becomes applicable to a variety of problems, including entity resolution and type alignment between heterogeneous graphs, and link prediction in networks represented as attributed graphs. This paper presents a graph-theoretic formalism for DNF schemes, and investigates their learnability in an optimization framework. We also briefly describe an empirical case study encapsulating some of the principles in this paper.\n    ",
        "submission_date": "2016-05-02T00:00:00",
        "last_modified_date": "2017-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.00702",
        "title": "A heuristic algorithm for a single vehicle static bike sharing rebalancing problem",
        "authors": [
            "F\u00e1bio Cruz",
            "Anand Subramanian",
            "Bruno P. Bruck",
            "Manuel Iori"
        ],
        "abstract": "The static bike rebalancing problem (SBRP) concerns the task of repositioning bikes among stations in self-service bike-sharing systems. This problem can be seen as a variant of the one-commodity pickup and delivery vehicle routing problem, where multiple visits are allowed to be performed at each station, i.e., the demand of a station is allowed to be split. Moreover, a vehicle may temporarily drop its load at a station, leaving it in excess or, alternatively, collect more bikes from a station (even all of them), thus leaving it in default. Both cases require further visits in order to meet the actual demands of such station. This paper deals with a particular case of the SBRP, in which only a single vehicle is available and the objective is to find a least-cost route that meets the demand of all stations and does not violate the minimum (zero) and maximum (vehicle capacity) load limits along the tour. Therefore, the number of bikes to be collected or delivered at each station should be appropriately determined in order to respect such constraints. We propose an iterated local search (ILS) based heuristic to solve the problem. The ILS algorithm was tested on 980 benchmark instances from the literature and the results obtained are quite competitive when compared to other existing methods. Moreover, our heuristic was capable of finding most of the known optimal solutions and also of improving the results on a number of open instances.\n    ",
        "submission_date": "2016-05-02T00:00:00",
        "last_modified_date": "2016-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.00787",
        "title": "Obstacle evasion using fuzzy logic in a sliding blades problem environment",
        "authors": [
            "Shadrack Kimutai"
        ],
        "abstract": "This paper discusses obstacle avoidance using fuzzy logic and shortest path algorithm. This paper also introduces the sliding blades problem and illustrates how a drone can navigate itself through the swinging blade obstacles while tracing a semi-optimal path and also maintaining constant velocity\n    ",
        "submission_date": "2016-05-03T00:00:00",
        "last_modified_date": "2016-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.00788",
        "title": "Online Learning of Commission Avoidant Portfolio Ensembles",
        "authors": [
            "Guy Uziel",
            "Ran El-Yaniv"
        ],
        "abstract": "We present a novel online ensemble learning strategy for portfolio selection. The new strategy controls and exploits any set of commission-oblivious portfolio selection algorithms. The strategy handles transaction costs using a novel commission avoidance mechanism. We prove a logarithmic regret bound for our strategy with respect to optimal mixtures of the base algorithms. Numerical examples validate the viability of our method and show significant improvement over the state-of-the-art.\n    ",
        "submission_date": "2016-05-03T00:00:00",
        "last_modified_date": "2016-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01138",
        "title": "A Comparative Evaluation of Approximate Probabilistic Simulation and Deep Neural Networks as Accounts of Human Physical Scene Understanding",
        "authors": [
            "Renqiao Zhang",
            "Jiajun Wu",
            "Chengkai Zhang",
            "William T. Freeman",
            "Joshua B. Tenenbaum"
        ],
        "abstract": "Humans demonstrate remarkable abilities to predict physical events in complex scenes. Two classes of models for physical scene understanding have recently been proposed: \"Intuitive Physics Engines\", or IPEs, which posit that people make predictions by running approximate probabilistic simulations in causal mental models similar in nature to video-game physics engines, and memory-based models, which make judgments based on analogies to stored experiences of previously encountered scenes and physical outcomes. Versions of the latter have recently been instantiated in convolutional neural network (CNN) architectures. Here we report four experiments that, to our knowledge, are the first rigorous comparisons of simulation-based and CNN-based models, where both approaches are concretely instantiated in algorithms that can run on raw image inputs and produce as outputs physical judgments such as whether a stack of blocks will fall. Both approaches can achieve super-human accuracy levels and can quantitatively predict human judgments to a similar degree, but only the simulation-based models generalize to novel situations in ways that people do, and are qualitatively consistent with systematic perceptual illusions and judgment asymmetries that people show.\n    ",
        "submission_date": "2016-05-04T00:00:00",
        "last_modified_date": "2016-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01180",
        "title": "A Step from Probabilistic Programming to Cognitive Architectures",
        "authors": [
            "Alexey Potapov"
        ],
        "abstract": "Probabilistic programming is considered as a framework, in which basic components of cognitive architectures can be represented in unified and elegant fashion. At the same time, necessity of adopting some component of cognitive architectures for extending capabilities of probabilistic programming languages is pointed out. In particular, implicit specification of generative models via declaration of concepts and links between them is proposed, and usefulness of declarative knowledge for achieving efficient inference is briefly discussed.\n    ",
        "submission_date": "2016-05-04T00:00:00",
        "last_modified_date": "2016-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01534",
        "title": "ODE - Augmented Training Improves Anomaly Detection in Sensor Data from Machines",
        "authors": [
            "Mohit Yadav",
            "Pankaj Malhotra",
            "Lovekesh Vig",
            "K Sriram",
            "Gautam Shroff"
        ],
        "abstract": "Machines of all kinds from vehicles to industrial equipment are increasingly instrumented with hundreds of sensors. Using such data to detect anomalous behaviour is critical for safety and efficient maintenance. However, anomalies occur rarely and with great variety in such systems, so there is often insufficient anomalous data to build reliable detectors. A standard approach to mitigate this problem is to use one class methods relying only on data from normal behaviour. Unfortunately, even these approaches are more likely to fail in the scenario of a dynamical system with manual control input(s). Normal behaviour in response to novel control input(s) might look very different to the learned detector which may be incorrectly detected as anomalous. In this paper, we address this issue by modelling time-series via Ordinary Differential Equations (ODE) and utilising such an ODE model to simulate the behaviour of dynamical systems under varying control inputs. The available data is then augmented with data generated from the ODE, and the anomaly detector is retrained on this augmented dataset. Experiments demonstrate that ODE-augmented training data allows better coverage of possible control input(s) and results in learning more accurate distinctions between normal and anomalous behaviour in time-series.\n    ",
        "submission_date": "2016-05-05T00:00:00",
        "last_modified_date": "2016-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01596",
        "title": "Notes on a model for fuzzy computing",
        "authors": [
            "Vittorio Cafagna",
            "Gianluca Caterina"
        ],
        "abstract": "In these notes we propose a setting for fuzzy computing in a framework similar to that of well-established theories of computation: boolean, and quantum computing. Our efforts have been directed towards stressing the formal similarities: there is a common pattern underlying these three theories. We tried to conform our approach, as much as possible, to this pattern. This work was part of a project jointly with Professor Vittorio Cafagna. Professor Cafagna passed away unexpectedly in 2007. His intellectual breadth and inspiring passion for mathematics is still very well alive.\n    ",
        "submission_date": "2016-05-04T00:00:00",
        "last_modified_date": "2016-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01652",
        "title": "LSTM-based Mixture-of-Experts for Knowledge-Aware Dialogues",
        "authors": [
            "Phong Le",
            "Marc Dymetman",
            "Jean-Michel Renders"
        ],
        "abstract": "We introduce an LSTM-based method for dynamically integrating several word-prediction experts to obtain a conditional language model which can be good simultaneously at several subtasks. We illustrate this general approach with an application to dialogue where we integrate a neural chat model, good at conversational aspects, with a neural question-answering model, good at retrieving precise information from a knowledge-base, and show how the integration combines the strengths of the independent components. We hope that this focused contribution will attract attention on the benefits of using such mixtures of experts in NLP.\n    ",
        "submission_date": "2016-05-05T00:00:00",
        "last_modified_date": "2016-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01778",
        "title": "Combinatorial Aspects of the Distribution of Rough Objects",
        "authors": [
            "A. Mani"
        ],
        "abstract": "The inverse problem of general rough sets, considered by the present author in some of her earlier papers, in one of its manifestations is essentially the question of when an agent's view about crisp and non crisp objects over a set of objects has a rough evolution. In this research the nature of the problem is examined from number-theoretic and combinatorial perspectives under very few assumptions about the nature of data and some necessary conditions are proved.\n    ",
        "submission_date": "2016-05-05T00:00:00",
        "last_modified_date": "2017-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01846",
        "title": "The KB paradigm and its application to interactive configuration",
        "authors": [
            "Pieter Van Hertum",
            "Ingmar Dasseville",
            "Gerda Janssens",
            "Marc Denecker"
        ],
        "abstract": "The knowledge base paradigm aims to express domain knowledge in a rich formal language, and to use this domain knowledge as a knowledge base to solve various problems and tasks that arise in the domain by applying multiple forms of inference. As such, the paradigm applies a strict separation of concerns between information and problem solving. In this paper, we analyze the principles and feasibility of the knowledge base paradigm in the context of an important class of applications: interactive configuration problems. In interactive configuration problems, a configuration of interrelated objects under constraints is searched, where the system assists the user in reaching an intended configuration. It is widely recognized in industry that good software solutions for these problems are very difficult to develop. We investigate such problems from the perspective of the KB paradigm. We show that multiple functionalities in this domain can be achieved by applying different forms of logical inferences on a formal specification of the configuration domain. We report on a proof of concept of this approach in a real-life application with a banking company. To appear in Theory and Practice of Logic Programming (TPLP).\n    ",
        "submission_date": "2016-05-06T00:00:00",
        "last_modified_date": "2016-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01995",
        "title": "Beyond knowing that: a new generation of epistemic logics",
        "authors": [
            "Yanjing Wang"
        ],
        "abstract": "Epistemic logic has become a major field of philosophical logic ever since the groundbreaking work by Hintikka (1962). Despite its various successful applications in theoretical computer science, AI, and game theory, the technical development of the field has been mainly focusing on the propositional part, i.e., the propositional modal logics of \"knowing that\". However, knowledge is expressed in everyday life by using various other locutions such as \"knowing whether\", \"knowing what\", \"knowing how\" and so on (knowing-wh hereafter). Such knowledge expressions are better captured in quantified epistemic logic, as was already discussed by Hintikka (1962) and his sequel works at length. This paper aims to draw the attention back again to such a fascinating but largely neglected topic. We first survey what Hintikka and others did in the literature of quantified epistemic logic, and then advocate a new quantifier-free approach to study the epistemic logics of knowing-wh, which we believe can balance expressivity and complexity, and capture the essential reasoning patterns about knowing-wh. We survey our recent line of work on the epistemic logics of \"knowing whether\", \"knowing what\" and \"knowing how\" to demonstrate the use of this new approach.\n    ",
        "submission_date": "2016-05-06T00:00:00",
        "last_modified_date": "2016-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02160",
        "title": "Belief Merging by Source Reliability Assessment",
        "authors": [
            "Paolo Liberatore"
        ],
        "abstract": "Merging beliefs requires the plausibility of the sources of the information to be merged. They are typically assumed equally reliable in lack of hints indicating otherwise; yet, a recent line of research spun from the idea of deriving this information from the revision process itself. In particular, the history of previous revisions and previous merging examples provide information for performing subsequent mergings.\n",
        "submission_date": "2016-05-07T00:00:00",
        "last_modified_date": "2016-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02321",
        "title": "Asymmetric Move Selection Strategies in Monte-Carlo Tree Search: Minimizing the Simple Regret at Max Nodes",
        "authors": [
            "Yun-Ching Liu",
            "Yoshimasa Tsuruoka"
        ],
        "abstract": "The combination of multi-armed bandit (MAB) algorithms with Monte-Carlo tree search (MCTS) has made a significant impact in various research fields. The UCT algorithm, which combines the UCB bandit algorithm with MCTS, is a good example of the success of this combination. The recent breakthrough made by AlphaGo, which incorporates convolutional neural networks with bandit algorithms in MCTS, also highlights the necessity of bandit algorithms in MCTS. However, despite the various investigations carried out on MCTS, nearly all of them still follow the paradigm of treating every node as an independent instance of the MAB problem, and applying the same bandit algorithm and heuristics on every node. As a result, this paradigm may leave some properties of the game tree unexploited. In this work, we propose that max nodes and min nodes have different concerns regarding their value estimation, and different bandit algorithms should be applied accordingly. We develop the Asymmetric-MCTS algorithm, which is an MCTS variant that applies a simple regret algorithm on max nodes, and the UCB algorithm on min nodes. We will demonstrate the performance of the Asymmetric-MCTS algorithm on the game of $9\\times 9$ Go, $9\\times 9$ NoGo, and Othello.\n    ",
        "submission_date": "2016-05-08T00:00:00",
        "last_modified_date": "2016-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02442",
        "title": "Machine Learning Techniques with Ontology for Subjective Answer Evaluation",
        "authors": [
            "M. Syamala Devi",
            "Himani Mittal"
        ],
        "abstract": "Computerized Evaluation of English Essays is performed using Machine learning techniques like Latent Semantic Analysis (LSA), Generalized LSA, Bilingual Evaluation Understudy and Maximum Entropy. Ontology, a concept map of domain knowledge, can enhance the performance of these techniques. Use of Ontology makes the evaluation process holistic as presence of keywords, synonyms, the right word combination and coverage of concepts can be checked. In this paper, the above mentioned techniques are implemented both with and without Ontology and tested on common input data consisting of technical answers of Computer Science. Domain Ontology of Computer Graphics is designed and developed. The software used for implementation includes Java Programming Language and tools such as MATLAB, Prot\u00e9g\u00e9, etc. Ten questions from Computer Graphics with sixty answers for each question are used for testing. The results are analyzed and it is concluded that the results are more accurate with use of Ontology.\n    ",
        "submission_date": "2016-05-09T00:00:00",
        "last_modified_date": "2016-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02677",
        "title": "Building a Large Scale Dataset for Image Emotion Recognition: The Fine Print and The Benchmark",
        "authors": [
            "Quanzeng You",
            "Jiebo Luo",
            "Hailin Jin",
            "Jianchao Yang"
        ],
        "abstract": "Psychological research results have confirmed that people can have different emotional reactions to different visual stimuli. Several papers have been published on the problem of visual emotion analysis. In particular, attempts have been made to analyze and predict people's emotional reaction towards images. To this end, different kinds of hand-tuned features are proposed. The results reported on several carefully selected and labeled small image data sets have confirmed the promise of such features. While the recent successes of many computer vision related tasks are due to the adoption of Convolutional Neural Networks (CNNs), visual emotion analysis has not achieved the same level of success. This may be primarily due to the unavailability of confidently labeled and relatively large image data sets for visual emotion analysis. In this work, we introduce a new data set, which started from 3+ million weakly labeled images of different emotions and ended up 30 times as large as the current largest publicly available visual emotion data set. We hope that this data set encourages further research on visual emotion analysis. We also perform extensive benchmarking analyses on this large data set using the state of the art methods including CNNs.\n    ",
        "submission_date": "2016-05-09T00:00:00",
        "last_modified_date": "2016-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02817",
        "title": "Unethical Research: How to Create a Malevolent Artificial Intelligence",
        "authors": [
            "Federico Pistono",
            "Roman V. Yampolskiy"
        ],
        "abstract": "Cybersecurity research involves publishing papers about malicious exploits as much as publishing information on how to design tools to protect cyber-infrastructure. It is this information exchange between ethical hackers and security experts, which results in a well-balanced cyber-ecosystem. In the blooming domain of AI Safety Engineering, hundreds of papers have been published on different proposals geared at the creation of a safe machine, yet nothing, to our knowledge, has been published on how to design a malevolent machine. Availability of such information would be of great value particularly to computer scientists, mathematicians, and others who have an interest in AI safety, and who are attempting to avoid the spontaneous emergence or the deliberate creation of a dangerous AI, which can negatively affect human activities and in the worst case cause the complete obliteration of the human species. This paper provides some general guidelines for the creation of a Malevolent Artificial Intelligence (MAI).\n    ",
        "submission_date": "2016-05-10T00:00:00",
        "last_modified_date": "2016-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02929",
        "title": "Function-Described Graphs for Structural Pattern Recognition",
        "authors": [
            "Francesc Serratosa"
        ],
        "abstract": "We present in this article the model Function-described graph (FDG), which is a type of compact representation of a set of attributed graphs (AGs) that borrow from Random Graphs the capability of probabilistic modelling of structural and attribute information. We define the FDGs, their features and two distance measures between AGs (unclassified patterns) and FDGs (models or classes) and we also explain an efficient matching algorithm. Two applications of FDGs are presented: in the former, FDGs are used for modelling and matching 3D-objects described by multiple views, whereas in the latter, they are used for representing and recognising human faces, described also by several views.\n    ",
        "submission_date": "2016-05-10T00:00:00",
        "last_modified_date": "2016-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03009",
        "title": "Consciousness is Pattern Recognition",
        "authors": [
            "Ray Van De Walker"
        ],
        "abstract": "This is a proof of the strong AI hypothesis, i.e. that machines can be conscious. It is a phenomenological proof that pattern-recognition and subjective consciousness are the same activity in different terms. Therefore, it proves that essential subjective processes of consciousness are computable, and identifies significant traits and requirements of a conscious system. Since Husserl, many philosophers have accepted that consciousness consists of memories of logical connections between an ego and external objects. These connections are called \"intentions.\" Pattern recognition systems are achievable technical artifacts. The proof links this respected introspective philosophical theory of consciousness with technical art. The proof therefore endorses the strong AI hypothesis and may therefore also enable a theoretically-grounded form of artificial intelligence called a \"synthetic intentionality,\" able to synthesize, generalize, select and repeat intentions. If the pattern recognition is reflexive, able to operate on the set of intentions, and flexible, with several methods of synthesizing intentions, an SI may be a particularly strong form of AI. Similarities and possible applications to several AI paradigms are discussed. The article then addresses some problems: The proof's limitations, reflexive cognition, Searles' Chinese room, and how an SI could \"understand\" \"meanings\" and \"be creative.\"\n    ",
        "submission_date": "2016-05-04T00:00:00",
        "last_modified_date": "2016-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03142",
        "title": "Self-Modification of Policy and Utility Function in Rational Agents",
        "authors": [
            "Tom Everitt",
            "Daniel Filan",
            "Mayank Daswani",
            "Marcus Hutter"
        ],
        "abstract": "Any agent that is part of the environment it interacts with and has versatile actuators (such as arms and fingers), will in principle have the ability to self-modify -- for example by changing its own source code. As we continue to create more and more intelligent agents, chances increase that they will learn about this ability. The question is: will they want to use it? For example, highly intelligent systems may find ways to change their goals to something more easily achievable, thereby `escaping' the control of their designers. In an important paper, Omohundro (2008) argued that goal preservation is a fundamental drive of any intelligent system, since a goal is more likely to be achieved if future versions of the agent strive towards the same goal. In this paper, we formalise this argument in general reinforcement learning, and explore situations where it fails. Our conclusion is that the self-modification possibility is harmless if and only if the value function of the agent anticipates the consequences of self-modifications and use the current utility function when evaluating the future.\n    ",
        "submission_date": "2016-05-10T00:00:00",
        "last_modified_date": "2016-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03143",
        "title": "Avoiding Wireheading with Value Reinforcement Learning",
        "authors": [
            "Tom Everitt",
            "Marcus Hutter"
        ],
        "abstract": "How can we design good goals for arbitrarily intelligent agents? Reinforcement learning (RL) is a natural approach. Unfortunately, RL does not work well for generally intelligent agents, as RL agents are incentivised to shortcut the reward sensor for maximum reward -- the so-called wireheading problem. In this paper we suggest an alternative to RL called value reinforcement learning (VRL). In VRL, agents use the reward signal to learn a utility function. The VRL setup allows us to remove the incentive to wirehead by placing a constraint on the agent's actions. The constraint is defined in terms of the agent's belief distributions, and does not require an explicit specification of which actions constitute wireheading.\n    ",
        "submission_date": "2016-05-10T00:00:00",
        "last_modified_date": "2016-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03392",
        "title": "Learning Bounded Treewidth Bayesian Networks with Thousands of Variables",
        "authors": [
            "Mauro Scanagatta",
            "Giorgio Corani",
            "Cassio P. de Campos",
            "Marco Zaffalon"
        ],
        "abstract": "We present a method for learning treewidth-bounded Bayesian networks from data sets containing thousands of variables. Bounding the treewidth of a Bayesian greatly reduces the complexity of inferences. Yet, being a global property of the graph, it considerably increases the difficulty of the learning process. We propose a novel algorithm for this task, able to scale to large domains and large treewidths. Our novel approach consistently outperforms the state of the art on data sets with up to ten thousand variables.\n    ",
        "submission_date": "2016-05-11T00:00:00",
        "last_modified_date": "2016-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03416",
        "title": "Concept based Attention",
        "authors": [
            "Jie You",
            "Xin Yang",
            "Matthias Hub"
        ],
        "abstract": "Attention endows animals an ability to concentrate on the most relevant information among a deluge of distractors at any given time, either through volitionally 'top-down' biasing, or driven by automatically 'bottom-up' saliency of stimuli, in favour of advantageous competition in neural modulations for information processing. Nevertheless, instead of being limited to perceive simple features, human and other advanced animals adaptively learn the world into categories and abstract concepts from experiences, imparting the world meanings. This thesis suggests that the high-level cognitive ability of human is more likely driven by attention basing on abstract perceptions, which is defined as concept based attention (CbA).\n    ",
        "submission_date": "2016-05-11T00:00:00",
        "last_modified_date": "2016-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03506",
        "title": "Characterizing Quantifier Fuzzification Mechanisms: a behavioral guide for practical applications",
        "authors": [
            "F. Diaz-Hermida",
            "M. Pereira-Fari\u00f1a",
            "Juan C. Vidal",
            "A. Ramos-Soto"
        ],
        "abstract": "Important advances have been made in the fuzzy quantification field. Nevertheless, some problems remain when we face the decision of selecting the most convenient model for a specific application. In the literature, several desirable adequacy properties have been proposed, but theoretical limits impede quantification models from simultaneously fulfilling every adequacy property that has been defined. Besides, the complexity of model definitions and adequacy properties makes very difficult for real users to understand the particularities of the different models that have been presented. In this work we will present several criteria conceived to help in the process of selecting the most adequate Quantifier Fuzzification Mechanisms for specific practical applications. In addition, some of the best known well-behaved models will be compared against this list of criteria. Based on this analysis, some guidance to choose fuzzy quantification models for practical applications will be provided.\n    ",
        "submission_date": "2016-05-11T00:00:00",
        "last_modified_date": "2016-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04071",
        "title": "Bayesian Network Structure Learning with Integer Programming: Polytopes, Facets, and Complexity",
        "authors": [
            "James Cussens",
            "Matti J\u00e4rvisalo",
            "Janne H. Korhonen",
            "Mark Bartlett"
        ],
        "abstract": "The challenging task of learning structures of probabilistic graphical models is an important problem within modern AI research. Recent years have witnessed several major algorithmic advances in structure learning for Bayesian networks---arguably the most central class of graphical models---especially in what is known as the score-based setting. A successful generic approach to optimal Bayesian network structure learning (BNSL), based on integer programming (IP), is implemented in the GOBNILP system. Despite the recent algorithmic advances, current understanding of foundational aspects underlying the IP based approach to BNSL is still somewhat lacking. Understanding fundamental aspects of cutting planes and the related separation problem( is important not only from a purely theoretical perspective, but also since it holds out the promise of further improving the efficiency of state-of-the-art approaches to solving BNSL exactly. In this paper, we make several theoretical contributions towards these goals: (i) we study the computational complexity of the separation problem, proving that the problem is NP-hard; (ii) we formalise and analyse the relationship between three key polytopes underlying the IP-based approach to BNSL; (iii) we study the facets of the three polytopes both from the theoretical and practical perspective, providing, via exhaustive computation, a complete enumeration of facets for low-dimensional family-variable polytopes; and, furthermore, (iv) we establish a tight connection of the BNSL problem to the acyclic subgraph problem.\n    ",
        "submission_date": "2016-05-13T00:00:00",
        "last_modified_date": "2016-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04218",
        "title": "Anytime Inference in Valuation Algebras",
        "authors": [
            "Abhishek Dasgupta",
            "Samson Abramsky"
        ],
        "abstract": "Anytime inference is inference performed incrementally, with the accuracy of the inference being controlled by a tunable parameter, usually time. Such anytime inference algorithms are also usually interruptible, gradually converging to the exact inference value until terminated. While anytime inference algorithms for specific domains like probability potentials exist in the literature, our objective in this article is to obtain an anytime inference algorithm which is sufficiently generic to cover a wide range of domains. For this we utilise the theory of generic inference as a basis for constructing an anytime inference algorithm, and in particular, extending work done on ordered valuation algebras. The novel contribution of this work is the construction of anytime algorithms in a generic framework, which automatically gives us instantiations in various useful domains. We also show how to apply this generic framework for anytime inference in semiring induced valuation algebras, an important subclass of valuation algebras, which includes instances like probability potentials, disjunctive normal forms and distributive lattices.\n",
        "submission_date": "2016-05-13T00:00:00",
        "last_modified_date": "2016-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04232",
        "title": "Review of state-of-the-arts in artificial intelligence with application to AI safety problem",
        "authors": [
            "Vladimir Shakirov"
        ],
        "abstract": "Here, I review current state-of-the-arts in many areas of AI to estimate when it's reasonable to expect human level AI development. Predictions of prominent AI researchers vary broadly from very pessimistic predictions of Andrew Ng to much more moderate predictions of Geoffrey Hinton and optimistic predictions of Shane Legg, DeepMind cofounder. Given huge rate of progress in recent years and this broad range of predictions of AI experts, AI safety questions are also discussed.\n    ",
        "submission_date": "2016-05-11T00:00:00",
        "last_modified_date": "2016-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04691",
        "title": "On Avoidance Learning with Partial Observability",
        "authors": [
            "Tom J. Ameloot"
        ],
        "abstract": "We study a framework where agents have to avoid aversive signals. The agents are given only partial information, in the form of features that are projections of task states. Additionally, the agents have to cope with non-determinism, defined as unpredictability on the way that actions are executed. The goal of each agent is to define its behavior based on feature-action pairs that reliably avoid aversive signals. We study a learning algorithm, called A-learning, that exhibits fixpoint convergence, where the belief of the allowed feature-action pairs eventually becomes fixed. A-learning is parameter-free and easy to implement.\n    ",
        "submission_date": "2016-05-16T00:00:00",
        "last_modified_date": "2016-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05303",
        "title": "Fuzzy Sets Across the Natural Language Generation Pipeline",
        "authors": [
            "A. Ramos-Soto",
            "A. Bugar\u00edn",
            "S. Barro"
        ],
        "abstract": "We explore the implications of using fuzzy techniques (mainly those commonly used in the linguistic description/summarization of data discipline) from a natural language generation perspective. For this, we provide an extensive discussion of some general convergence points and an exploration of the relationship between the different tasks involved in the standard NLG system pipeline architecture and the most common fuzzy approaches used in linguistic summarization/description of data, such as fuzzy quantified statements, evaluation criteria or aggregation operators. Each individual discussion is illustrated with a related use case. Recent work made in the context of cross-fertilization of both research fields is also referenced. This paper encompasses general ideas that emerged as part of the PhD thesis \"Application of fuzzy sets in data-to-text systems\". It does not present a specific application or a formal approach, but rather discusses current high-level issues and potential usages of fuzzy sets (focused on linguistic summarization of data) in natural language generation.\n    ",
        "submission_date": "2016-05-17T00:00:00",
        "last_modified_date": "2016-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05305",
        "title": "Combat Models for RTS Games",
        "authors": [
            "Alberto Uriarte",
            "Santiago Onta\u00f1\u00f3n"
        ],
        "abstract": "Game tree search algorithms, such as Monte Carlo Tree Search (MCTS), require access to a forward model (or \"simulator\") of the game at hand. However, in some games such forward model is not readily available. This paper presents three forward models for two-player attrition games, which we call \"combat models\", and show how they can be used to simulate combat in RTS games. We also show how these combat models can be learned from replay data. We use StarCraft as our application domain. We report experiments comparing our combat models predicting a combat output and their impact when used for tactical decisions during a real game.\n    ",
        "submission_date": "2016-05-17T00:00:00",
        "last_modified_date": "2016-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05676",
        "title": "Towards information based spatiotemporal patterns as a foundation for agent representation in dynamical systems",
        "authors": [
            "Martin Biehl",
            "Takashi Ikegami",
            "Daniel Polani"
        ],
        "abstract": "We present some arguments why existing methods for representing agents fall short in applications crucial to artificial life. Using a thought experiment involving a fictitious dynamical systems model of the biosphere we argue that the metabolism, motility, and the concept of counterfactual variation should be compatible with any agent representation in dynamical systems. We then propose an information-theoretic notion of \\emph{integrated spatiotemporal patterns} which we believe can serve as the basic building block of an agent definition. We argue that these patterns are capable of solving the problems mentioned before. We also test this in some preliminary experiments.\n    ",
        "submission_date": "2016-05-18T00:00:00",
        "last_modified_date": "2016-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05807",
        "title": "Heuristics for Planning, Plan Recognition and Parsing",
        "authors": [
            "Miquel Ramirez",
            "Hector Geffner"
        ],
        "abstract": "In a recent paper, we have shown that Plan Recognition over STRIPS can be formulated and solved using Classical Planning heuristics and algorithms. In this work, we show that this formulation subsumes the standard formulation of Plan Recognition over libraries through a compilation of libraries into STRIPS theories. The libraries correspond to AND/OR graphs that may be cyclic and where children of AND nodes may be partially ordered. These libraries include Context-Free Grammars as a special case, where the Plan Recognition problem becomes a parsing with missing tokens problem. Plan Recognition over the standard libraries become Planning problems that can be easily solved by any modern planner, while recognition over more complex libraries, including Context-Free Grammars (CFGs), illustrate limitations of current Planning heuristics and suggest improvements that may be relevant in other Planning problems too.\n    ",
        "submission_date": "2016-05-19T00:00:00",
        "last_modified_date": "2016-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05950",
        "title": "Interactive Debugging of Knowledge Bases",
        "authors": [
            "Patrick Rodler"
        ],
        "abstract": "Many AI applications rely on knowledge about a relevant real-world domain that is encoded by means of some logical knowledge base (KB). The most essential benefit of logical KBs is the opportunity to perform automatic reasoning to derive implicit knowledge or to answer complex queries about the modeled domain. The feasibility of meaningful reasoning requires KBs to meet some minimal quality criteria such as logical consistency. Without adequate tool assistance, the task of resolving violated quality criteria in KBs can be extremely tough even for domain experts, especially when the problematic KB includes a large number of logical formulas or comprises complicated logical formalisms.\n",
        "submission_date": "2016-05-19T00:00:00",
        "last_modified_date": "2016-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05966",
        "title": "Dynamic Bayesian Networks to simulate occupant behaviours in office buildings related to indoor air quality",
        "authors": [
            "Khadija Tijani",
            "Stephane Ploix",
            "Benjamin Haas",
            "Julie Dugdale",
            "Quoc Dung Ngo"
        ],
        "abstract": "This paper proposes a new general approach based on Bayesian networks to model the human behaviour. This approach represents human behaviour with probabilistic cause-effect relations based on knowledge, but also with conditional probabilities coming either from knowledge or deduced from observations. This approach has been applied to the co-simulation of the CO2 concentration in an office coupled with human behaviour.\n    ",
        "submission_date": "2016-05-19T00:00:00",
        "last_modified_date": "2016-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06047",
        "title": "AMSOM: Adaptive Moving Self-organizing Map for Clustering and Visualization",
        "authors": [
            "Gerasimos Spanakis",
            "Gerhard Weiss"
        ],
        "abstract": "Self-Organizing Map (SOM) is a neural network model which is used to obtain a topology-preserving mapping from the (usually high dimensional) input/feature space to an output/map space of fewer dimensions (usually two or three in order to facilitate visualization). Neurons in the output space are connected with each other but this structure remains fixed throughout training and learning is achieved through the updating of neuron reference vectors in feature space. Despite the fact that growing variants of SOM overcome the fixed structure limitation they increase computational cost and also do not allow the removal of a neuron after its introduction. In this paper, a variant of SOM is proposed called AMSOM (Adaptive Moving Self-Organizing Map) that on the one hand creates a more flexible structure where neuron positions are dynamically altered during training and on the other hand tackles the drawback of having a predefined grid by allowing neuron addition and/or removal during training. Experiments using multiple literature datasets show that the proposed method improves training performance of SOM, leads to a better visualization of the input dataset and provides a framework for determining the optimal number and structure of neurons.\n    ",
        "submission_date": "2016-05-19T00:00:00",
        "last_modified_date": "2016-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06048",
        "title": "Philosophy in the Face of Artificial Intelligence",
        "authors": [
            "Vincent Conitzer"
        ],
        "abstract": "In this article, I discuss how the AI community views concerns about the emergence of superintelligent AI and related philosophical issues.\n    ",
        "submission_date": "2016-05-19T00:00:00",
        "last_modified_date": "2016-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06523",
        "title": "TensorLog: A Differentiable Deductive Database",
        "authors": [
            "William W. Cohen"
        ],
        "abstract": "Large knowledge bases (KBs) are useful in many tasks, but it is unclear how to integrate this sort of knowledge into \"deep\" gradient-based learning systems. To address this problem, we describe a probabilistic deductive database, called TensorLog, in which reasoning uses a differentiable process. In TensorLog, each clause in a logical theory is first converted into certain type of factor graph. Then, for each type of query to the factor graph, the message-passing steps required to perform belief propagation (BP) are \"unrolled\" into a function, which is differentiable. We show that these functions can be composed recursively to perform inference in non-trivial logical theories containing multiple interrelated clauses and predicates. Both compilation and inference in TensorLog are efficient: compilation is linear in theory size and proof depth, and inference is linear in database size and the number of message-passing steps used in BP. We also present experimental results with TensorLog and discuss its relationship to other first-order probabilistic logics.\n    ",
        "submission_date": "2016-05-20T00:00:00",
        "last_modified_date": "2016-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06588",
        "title": "Optimal Number of Choices in Rating Contexts",
        "authors": [
            "Sam Ganzfried",
            "Farzana Yusuf"
        ],
        "abstract": "In many settings people must give numerical scores to entities from a small discrete set. For instance, rating physical attractiveness from 1--5 on dating sites, or papers from 1--10 for conference reviewing. We study the problem of understanding when using a different number of options is optimal. We consider the case when scores are uniform random and Gaussian. We study computationally when using 2, 3, 4, 5, and 10 options out of a total of 100 is optimal in these models (though our theoretical analysis is for a more general setting with $k$ choices from $n$ total options as well as a continuous underlying space). One may expect that using more options would always improve performance in this model, but we show that this is not necessarily the case, and that using fewer choices---even just two---can surprisingly be optimal in certain situations. While in theory for this setting it would be optimal to use all 100 options, in practice this is prohibitive, and it is preferable to utilize a smaller number of options due to humans' limited computational resources. Our results could have many potential applications, as settings requiring entities to be ranked by humans are ubiquitous. There could also be applications to other fields such as signal or image processing where input values from a large set must be mapped to output values in a smaller set.\n    ",
        "submission_date": "2016-05-21T00:00:00",
        "last_modified_date": "2018-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06676",
        "title": "Learning to Communicate with Deep Multi-Agent Reinforcement Learning",
        "authors": [
            "Jakob N. Foerster",
            "Yannis M. Assael",
            "Nando de Freitas",
            "Shimon Whiteson"
        ],
        "abstract": "We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.\n    ",
        "submission_date": "2016-05-21T00:00:00",
        "last_modified_date": "2016-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06886",
        "title": "Stochastic Patching Process",
        "authors": [
            "Xuhui Fan",
            "Bin Li",
            "Yi Wang",
            "Yang Wang",
            "Fang Chen"
        ],
        "abstract": "Stochastic partition models tailor a product space into a number of rectangular regions such that the data within each region exhibit certain types of homogeneity. Due to constraints of partition strategy, existing models may cause unnecessary dissections in sparse regions when fitting data in dense regions. To alleviate this limitation, we propose a parsimonious partition model, named Stochastic Patching Process (SPP), to deal with multi-dimensional arrays. SPP adopts an \"enclosing\" strategy to attach rectangular patches to dense regions. SPP is self-consistent such that it can be extended to infinite arrays. We apply SPP to relational modeling and the experimental results validate its merit compared to the state-of-the-arts.\n    ",
        "submission_date": "2016-05-23T00:00:00",
        "last_modified_date": "2017-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06921",
        "title": "Generative Choreography using Deep Learning",
        "authors": [
            "Luka Crnkovic-Friis",
            "Louise Crnkovic-Friis"
        ],
        "abstract": "Recent advances in deep learning have enabled the extraction of high-level features from raw sensor data which has opened up new possibilities in many different fields, including computer generated choreography. In this paper we present a system chor-rnn for generating novel choreographic material in the nuanced choreographic language and style of an individual choreographer. It also shows promising results in producing a higher level compositional cohesion, rather than just generating sequences of movement. At the core of chor-rnn is a deep recurrent neural network trained on raw motion capture data and that can generate new dance sequences for a solo dancer. Chor-rnn can be used for collaborative human-machine choreography or as a creative catalyst, serving as inspiration for a choreographer.\n    ",
        "submission_date": "2016-05-23T00:00:00",
        "last_modified_date": "2016-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06940",
        "title": "Elastic Solver: Balancing Solution Time and Energy Consumption",
        "authors": [
            "Barry Hurley",
            "Deepak Mehta",
            "Barry O'Sullivan"
        ],
        "abstract": "Combinatorial decision problems arise in many different domains such as scheduling, routing, packing, bioinformatics, and many more. Despite recent advances in developing scalable solvers, there are still many problems which are often very hard to solve. Typically the most advanced solvers include elements which are stochastic in nature. If a same instance is solved many times using different seeds then depending on the inherent characteristics of a problem instance and the solver, one can observe a highly-variant distribution of times spanning multiple orders of magnitude. Therefore, to solve a problem instance efficiently it is often useful to solve the same instance in parallel with different seeds. With the proliferation of cloud computing, it is natural to think about an elastic solver which can scale up by launching searches in parallel on thousands of machines (or cores). However, this could result in consuming a lot of energy. Moreover, not every instance would require thousands of machines. The challenge is to resolve the tradeoff between solution time and energy consumption optimally for a given problem instance. We analyse the impact of the number of machines (or cores) on not only solution time but also on energy consumption. We highlight that although solution time always drops as the number of machines increases, the relation between the number of machines and energy consumption is more complicated. In many cases, the optimal energy consumption may be achieved by a middle ground, we analyse this relationship in detail. The tradeoff between solution time and energy consumption is studied further, showing that the energy consumption of a solver can be reduced drastically if we increase the solution time marginally. We also develop a prediction model, demonstrating that such insights can be exploited to achieve faster solutions times in a more energy efficient manor.\n    ",
        "submission_date": "2016-05-23T00:00:00",
        "last_modified_date": "2016-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07260",
        "title": "Diagnosing editorial strategies of Chilean media on Twitter using an automatic news classifier",
        "authors": [
            "Matthieu Vernier",
            "Luis Carcamo",
            "Eliana Scheihing"
        ],
        "abstract": "In Chile, does not exist an independent entity that publishes quantitative or qualitative surveys to understand the traditional media environment and its adaptation on the Social Web. Nowadays, Chilean newsreaders are increasingly using social web platforms as their primary source of information, among which Twitter plays a central role. Historical media and pure players are developing different strategies to increase their audience and influence on this platform. In this article, we propose a methodology based on data mining techniques to provide a first level of analysis of the new Chilean media environment. We use a crawling technique to mine news streams of 37 different Chilean media actively presents on Twitter and propose several indicators to compare them. We analyze their volumes of production, their potential audience, and using NLP techniques, we explore the content of their production: their editorial line and their geographic coverage.\n    ",
        "submission_date": "2016-05-24T00:00:00",
        "last_modified_date": "2016-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07335",
        "title": "Differences between Industrial Models of Autonomy and Systemic Models of Autonomy",
        "authors": [
            "Aleksander Lodwich"
        ],
        "abstract": "This paper discusses the idea of levels of autonomy of systems - be this technical or organic - and compares the insights with models employed by industries used to describe maturity and capability of their products.\n    ",
        "submission_date": "2016-05-24T00:00:00",
        "last_modified_date": "2016-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07364",
        "title": "Non-Gaussian Random Generators in Bacteria Foraging Algorithm for Multiobjective Optimization",
        "authors": [
            "Timothy Ganesan",
            "Pandian Vasant",
            "Irraivan Elamvazuthi"
        ],
        "abstract": "Random generators or stochastic engines are a key component in the structure of metaheuristic algorithms. This work investigates the effects of non-Gaussian stochastic engines on the performance of metaheuristics when solving a real-world optimization problem. In this work, the bacteria foraging algorithm (BFA) was employed in tandem with four random generators (stochastic engines). The stochastic engines operate using the Weibull distribution, Gamma distribution, Gaussian distribution and a chaotic mechanism. The two non-Gaussian distributions are the Weibull and Gamma distributions. In this work, the approaches developed were implemented on the real-world multi-objective resin bonded sand mould problem. The Pareto frontiers obtained were benchmarked using two metrics; the hyper volume indicator (HVI) and the proposed Average Explorative Rate (AER) metric. Detail discussions from various perspectives on the effects of non-Gaussian random generators in metaheuristics are provided.\n    ",
        "submission_date": "2016-05-24T00:00:00",
        "last_modified_date": "2016-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07574",
        "title": "Towards Bin Packing (preliminary problem survey, models with multiset estimates)",
        "authors": [
            "Mark Sh. Levin"
        ],
        "abstract": "The paper described a generalized integrated glance to bin packing problems including a brief literature survey and some new problem formulations for the cases of multiset estimates of items. A new systemic viewpoint to bin packing problems is suggested: (a) basic element sets (item set, bin set, item subset assigned to bin), (b) binary relation over the sets: relation over item set as compatibility, precedence, dominance; relation over items and bins (i.e., correspondence of items to bins). A special attention is targeted to the following versions of bin packing problems: (a) problem with multiset estimates of items, (b) problem with colored items (and some close problems). Applied examples of bin packing problems are considered: (i) planning in paper industry (framework of combinatorial problems), (ii) selection of information messages, (iii) packing of messages/information packages in WiMAX communication system (brief description).\n    ",
        "submission_date": "2016-05-24T00:00:00",
        "last_modified_date": "2016-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07728",
        "title": "Small Representations of Big Kidney Exchange Graphs",
        "authors": [
            "John P. Dickerson",
            "Aleksandr M. Kazachkov",
            "Ariel D. Procaccia",
            "Tuomas Sandholm"
        ],
        "abstract": "Kidney exchanges are organized markets where patients swap willing but incompatible donors. In the last decade, kidney exchanges grew from small and regional to large and national---and soon, international. This growth results in more lives saved, but exacerbates the empirical hardness of the $\\mathcal{NP}$-complete problem of optimally matching patients to donors. State-of-the-art matching engines use integer programming techniques to clear fielded kidney exchanges, but these methods must be tailored to specific models and objective functions, and may fail to scale to larger exchanges. In this paper, we observe that if the kidney exchange compatibility graph can be encoded by a constant number of patient and donor attributes, the clearing problem is solvable in polynomial time. We give necessary and sufficient conditions for losslessly shrinking the representation of an arbitrary compatibility graph. Then, using real compatibility graphs from the UNOS nationwide kidney exchange, we show how many attributes are needed to encode real compatibility graphs. The experiments show that, indeed, small numbers of attributes suffice.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07895",
        "title": "Automatic Extraction of Causal Relations from Natural Language Texts: A Comprehensive Survey",
        "authors": [
            "Nabiha Asghar"
        ],
        "abstract": "Automatic extraction of cause-effect relationships from natural language texts is a challenging open problem in Artificial Intelligence. Most of the early attempts at its solution used manually constructed linguistic and syntactic rules on small and domain-specific data sets. However, with the advent of big data, the availability of affordable computing power and the recent popularization of machine learning, the paradigm to tackle this problem has slowly shifted. Machines are now expected to learn generic causal extraction rules from labelled data with minimal supervision, in a domain independent-manner. In this paper, we provide a comprehensive survey of causal relation extraction techniques from both paradigms, and analyse their relative strengths and weaknesses, with recommendations for future work.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07969",
        "title": "Adaptive Neural Compilation",
        "authors": [
            "Rudy Bunel",
            "Alban Desmaison",
            "Pushmeet Kohli",
            "Philip H.S. Torr",
            "M. Pawan Kumar"
        ],
        "abstract": "This paper proposes an adaptive neural-compilation framework to address the problem of efficient program learning. Traditional code optimisation strategies used in compilers are based on applying pre-specified set of transformations that make the code faster to execute without changing its semantics. In contrast, our work involves adapting programs to make them more efficient while considering correctness only on a target input distribution. Our approach is inspired by the recent works on differentiable representations of programs. We show that it is possible to compile programs written in a low-level language to a differentiable representation. We also show how programs in this representation can be optimised to make them efficient on a target distribution of inputs. Experimental results demonstrate that our approach enables learning specifically-tuned algorithms for given data distributions with a high success rate.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07989",
        "title": "Compliant Conditions for Polynomial Time Approximation of Operator Counts",
        "authors": [
            "Tathagata Chakraborti",
            "Sarath Sreedharan",
            "Sailik Sengupta",
            "T. K. Satish Kumar",
            "Subbarao Kambhampati"
        ],
        "abstract": "In this paper, we develop a computationally simpler version of the operator count heuristic for a particular class of domains. The contribution of this abstract is threefold, we (1) propose an efficient closed form approximation to the operator count heuristic using the Lagrangian dual; (2) leverage compressed sensing techniques to obtain an integer approximation for operator counts in polynomial time; and (3) discuss the relationship of the proposed formulation to existing heuristics and investigate properties of domains where such approaches appear to be useful.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.08150",
        "title": "Cognitive Dynamic Systems: A Technical Review of Cognitive Radar",
        "authors": [
            "Krishanth Krishnan",
            "Taralyn Schwering",
            "Saman Sarraf"
        ],
        "abstract": "We start with the history of cognitive radar, where origins of the PAC, Fuster research on cognition and principals of cognition are provided. Fuster describes five cognitive functions: perception, memory, attention, language, and intelligence. We describe the Perception-Action Cyclec as it applies to cognitive radar, and then discuss long-term memory, memory storage, memory retrieval and working memory. A comparison between memory in human cognition and cognitive radar is given as well. Attention is another function described by Fuster, and we have given the comparison of attention in human cognition and cognitive radar. We talk about the four functional blocks from the PAC: Bayesian filter, feedback information, dynamic programming and state-space model for the radar environment. Then, to show that the PAC improves the tracking accuracy of Cognitive Radar over Traditional Active Radar, we have provided simulation results. In the simulation, three nonlinear filters: Cubature Kalman Filter, Unscented Kalman Filter and Extended Kalman Filter are compared. Based on the results, radars implemented with CKF perform better than the radars implemented with UKF or radars implemented with EKF. Further, radar with EKF has the worst accuracy and has the biggest computation load because of derivation and evaluation of Jacobian matrices. We suggest using the concept of risk management to better control parameters and improve performance in cognitive radar. We believe, spectrum sensing can be seen as a potential interest to be used in cognitive radar and we propose a new approach Probabilistic ICA which will presumably reduce noise based on estimation error in cognitive radar. Parallel computing is a concept based on divide and conquers mechanism, and we suggest using the parallel computing approach in cognitive radar by doing complicated calculations or tasks to reduce processing time.\n    ",
        "submission_date": "2016-05-26T00:00:00",
        "last_modified_date": "2016-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.08187",
        "title": "The Symbolic Interior Point Method",
        "authors": [
            "Martin Mladenov",
            "Vaishak Belle",
            "Kristian Kersting"
        ],
        "abstract": "A recent trend in probabilistic inference emphasizes the codification of models in a formal syntax, with suitable high-level features such as individuals, relations, and connectives, enabling descriptive clarity, succinctness and circumventing the need for the modeler to engineer a custom solver. Unfortunately, bringing these linguistic and pragmatic benefits to numerical optimization has proven surprisingly challenging. In this paper, we turn to these challenges: we introduce a rich modeling language, for which an interior-point method computes approximate solutions in a generic way. While logical features easily complicates the underlying model, often yielding intricate dependencies, we exploit and cache local structure using algebraic decision diagrams (ADDs). Indeed, standard matrix-vector algebra is efficiently realizable in ADDs, but we argue and show that well-known optimization methods are not ideal for ADDs. Our engine, therefore, invokes a sophisticated matrix-free approach. We demonstrate the flexibility of the resulting symbolic-numeric optimizer on decision making and compressed sensing tasks with millions of non-zero entries.\n    ",
        "submission_date": "2016-05-26T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.08367",
        "title": "Probabilistic Inference Modulo Theories",
        "authors": [
            "Rodrigo de Salvo Braz",
            "Ciaran O'Reilly",
            "Vibhav Gogate",
            "Rina Dechter"
        ],
        "abstract": "We present SGDPLL(T), an algorithm that solves (among many other problems) probabilistic inference modulo theories, that is, inference problems over probabilistic models defined via a logic theory provided as a parameter (currently, propositional, equalities on discrete sorts, and inequalities, more specifically difference arithmetic, on bounded integers). While many solutions to probabilistic inference over logic representations have been proposed, SGDPLL(T) is simultaneously (1) lifted, (2) exact and (3) modulo theories, that is, parameterized by a background logic theory. This offers a foundation for extending it to rich logic languages such as data structures and relational data. By lifted, we mean algorithms with constant complexity in the domain size (the number of values that variables can take). We also detail a solver for summations with difference arithmetic and show experimental results from a scenario in which SGDPLL(T) is much faster than a state-of-the-art probabilistic solver.\n    ",
        "submission_date": "2016-05-26T00:00:00",
        "last_modified_date": "2016-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.08390",
        "title": "Estimation of Passenger Route Choice Pattern Using Smart Card Data for Complex Metro Systems",
        "authors": [
            "Juanjuan Zhao",
            "Fan Zhang",
            "Lai Tu",
            "Chengzhong Xu",
            "Dayong Shen",
            "Chen Tian",
            "Xiang-Yang Li",
            "Zhengxi Li"
        ],
        "abstract": "Nowadays, metro systems play an important role in meeting the urban transportation demand in large cities. The understanding of passenger route choice is critical for public transit management. The wide deployment of Automated Fare Collection(AFC) systems opens up a new opportunity. However, only each trip's tap-in and tap-out timestamp and stations can be directly obtained from AFC system records; the train and route chosen by a passenger are unknown, which are necessary to solve our problem. While existing methods work well in some specific situations, they don't work for complicated situations. In this paper, we propose a solution that needs no additional equipment or human involvement than the AFC systems. We develop a probabilistic model that can estimate from empirical analysis how the passenger flows are dispatched to different routes and trains. We validate our approach using a large scale data set collected from the Shenzhen metro system. The measured results provide us with useful inputs when building the passenger path choice model.\n    ",
        "submission_date": "2016-04-19T00:00:00",
        "last_modified_date": "2016-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.09128",
        "title": "Control of Memory, Active Perception, and Action in Minecraft",
        "authors": [
            "Junhyuk Oh",
            "Valliappa Chockalingam",
            "Satinder Singh",
            "Honglak Lee"
        ],
        "abstract": "In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.\n    ",
        "submission_date": "2016-05-30T00:00:00",
        "last_modified_date": "2016-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.09505",
        "title": "Psychologically based Virtual-Suspect for Interrogative Interview Training",
        "authors": [
            "Moshe Bitan",
            "Galit Nahari",
            "Zvi Nisin",
            "Ariel Roth",
            "Sarit Kraus"
        ],
        "abstract": "In this paper, we present a Virtual-Suspect system which can be used to train inexperienced law enforcement personnel in interrogation strategies. The system supports different scenario configurations based on historical data. The responses presented by the Virtual-Suspect are selected based on the psychological state of the suspect, which can be configured as well. Furthermore, each interrogator's statement affects the Virtual-Suspect's current psychological state, which may lead the interrogation in different directions. In addition, the model takes into account the context in which the statements are made. Experiments with 24 subjects demonstrate that the Virtual-Suspect's behavior is similar to that of a human who plays the role of the suspect.\n    ",
        "submission_date": "2016-05-31T00:00:00",
        "last_modified_date": "2016-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.09735",
        "title": "Information Theoretically Aided Reinforcement Learning for Embodied Agents",
        "authors": [
            "Guido Montufar",
            "Keyan Ghazi-Zahedi",
            "Nihat Ay"
        ],
        "abstract": "Reinforcement learning for embodied agents is a challenging problem. The accumulated reward to be optimized is often a very rugged function, and gradient methods are impaired by many local optimizers. We demonstrate, in an experimental setting, that incorporating an intrinsic reward can smoothen the optimization landscape while preserving the global optimizers of interest. We show that policy gradient optimization for locomotion in a complex morphology is significantly improved when supplementing the extrinsic reward by an intrinsic reward defined in terms of the mutual information of time consecutive sensor readings.\n    ",
        "submission_date": "2016-05-31T00:00:00",
        "last_modified_date": "2016-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00058",
        "title": "How to avoid ethically relevant Machine Consciousness",
        "authors": [
            "Aleksander Lodwich"
        ],
        "abstract": "This paper discusses the root cause of systems perceiving the self experience and how to exploit adaptive and learning features without introducing ethically problematic system properties.\n    ",
        "submission_date": "2016-05-31T00:00:00",
        "last_modified_date": "2016-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00068",
        "title": "Quantifying the probable approximation error of probabilistic inference programs",
        "authors": [
            "Marco F Cusumano-Towner",
            "Vikash K Mansinghka"
        ],
        "abstract": "This paper introduces a new technique for quantifying the approximation error of a broad class of probabilistic inference programs, including ones based on both variational and Monte Carlo approaches. The key idea is to derive a subjective bound on the symmetrized KL divergence between the distribution achieved by an approximate inference program and its true target distribution. The bound's validity (and subjectivity) rests on the accuracy of two auxiliary probabilistic programs: (i) a \"reference\" inference program that defines a gold standard of accuracy and (ii) a \"meta-inference\" program that answers the question \"what internal random choices did the original approximate inference program probably make given that it produced a particular result?\" The paper includes empirical results on inference problems drawn from linear regression, Dirichlet process mixture modeling, HMMs, and Bayesian networks. The experiments show that the technique is robust to the quality of the reference inference program and that it can detect implementation bugs that are not apparent from predictive performance.\n    ",
        "submission_date": "2016-05-31T00:00:00",
        "last_modified_date": "2016-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00075",
        "title": "Applications of Probabilistic Programming (Master's thesis, 2015)",
        "authors": [
            "Yura N Perov"
        ],
        "abstract": "This thesis describes work on two applications of probabilistic programming: the learning of probabilistic program code given specifications, in particular program code of one-dimensional samplers; and the facilitation of sequential Monte Carlo inference with help of data-driven proposals. The latter is presented with experimental results on a linear Gaussian model and a non-parametric dependent Dirichlet process mixture of objects model for object recognition and tracking.\n",
        "submission_date": "2016-05-31T00:00:00",
        "last_modified_date": "2020-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00133",
        "title": "A Survey of Qualitative Spatial and Temporal Calculi -- Algebraic and Computational Properties",
        "authors": [
            "Frank Dylla",
            "Jae Hee Lee",
            "Till Mossakowski",
            "Thomas Schneider",
            "Andr\u00e9 Van Delden",
            "Jasper Van De Ven",
            "Diedrich Wolter"
        ],
        "abstract": "Qualitative Spatial and Temporal Reasoning (QSTR) is concerned with symbolic knowledge representation, typically over infinite domains. The motivations for employing QSTR techniques range from exploiting computational properties that allow efficient reasoning to capture human cognitive concepts in a computational framework. The notion of a qualitative calculus is one of the most prominent QSTR formalisms. This article presents the first overview of all qualitative calculi developed to date and their computational properties, together with generalized definitions of the fundamental concepts and methods, which now encompass all existing calculi. Moreover, we provide a classification of calculi according to their algebraic properties.\n    ",
        "submission_date": "2016-06-01T00:00:00",
        "last_modified_date": "2016-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00339",
        "title": "A structured argumentation framework for detaching conditional obligations",
        "authors": [
            "Mathieu Beirlaen",
            "Christian Stra\u00dfer"
        ],
        "abstract": "We present a general formal argumentation system for dealing with the detachment of conditional obligations. Given a set of facts, constraints, and conditional obligations, we answer the question whether an unconditional obligation is detachable by considering reasons for and against its detachment. For the evaluation of arguments in favor of detaching obligations we use a Dung-style argumentation-theoretical semantics. We illustrate the modularity of the general framework by considering some extensions, and we compare the framework to some related approaches from the literature.\n    ",
        "submission_date": "2016-06-01T00:00:00",
        "last_modified_date": "2016-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00496",
        "title": "On the equivalence between Kolmogorov-Smirnov and ROC curve metrics for binary classification",
        "authors": [
            "Paulo J. L. Adeodato",
            "S\u00edlvio B. Melo"
        ],
        "abstract": "Binary decisions are very common in artificial intelligence. Applying a threshold on the continuous score gives the human decider the power to control the operating point to separate the two classes. The classifier,s discriminating power is measured along the continuous range of the score by the Area Under the ROC curve (AUC_ROC) in most application fields. Only finances uses the poor single point metric maximum Kolmogorov-Smirnov (KS) distance. This paper proposes the Area Under the KS curve (AUC_KS) for performance assessment and proves AUC_ROC = 0.5 + AUC_KS, as a simpler way to calculate the AUC_ROC. That is even more important for ROC averaging in ensembles of classifiers or n fold cross-validation. The proof is geometrically inspired on rotating all KS curve to make it lie on the top of the ROC chance diagonal. On the practical side, the independent variable on the abscissa on the KS curve simplifies the calculation of the AUC_ROC. On the theoretical side, this research gives insights on probabilistic interpretations of classifiers assessment and integrates the existing body of knowledge of the information theoretical ROC approach with the proposed statistical approach based on the thoroughly known KS distribution.\n    ",
        "submission_date": "2016-06-01T00:00:00",
        "last_modified_date": "2016-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00626",
        "title": "The Challenge of Non-Technical Loss Detection using Artificial Intelligence: A Survey",
        "authors": [
            "Patrick Glauner",
            "Jorge Augusto Meira",
            "Petko Valtchev",
            "Radu State",
            "Franck Bettinger"
        ],
        "abstract": "Detection of non-technical losses (NTL) which include electricity theft, faulty meters or billing errors has attracted increasing attention from researchers in electrical engineering and computer science. NTLs cause significant harm to the economy, as in some countries they may range up to 40% of the total electricity distributed. The predominant research direction is employing artificial intelligence to predict whether a customer causes NTL. This paper first provides an overview of how NTLs are defined and their impact on economies, which include loss of revenue and profit of electricity providers and decrease of the stability and reliability of electrical power grids. It then surveys the state-of-the-art research efforts in a up-to-date and comprehensive review of algorithms, features and data sets used. It finally identifies the key scientific and engineering challenges in NTL detection and suggests how they could be addressed in the future.\n    ",
        "submission_date": "2016-06-02T00:00:00",
        "last_modified_date": "2017-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00652",
        "title": "Death and Suicide in Universal Artificial Intelligence",
        "authors": [
            "Jarryd Martin",
            "Tom Everitt",
            "Marcus Hutter"
        ],
        "abstract": "Reinforcement learning (RL) is a general paradigm for studying intelligent behaviour, with applications ranging from artificial intelligence to psychology and economics. AIXI is a universal solution to the RL problem; it can learn any computable environment. A technical subtlety of AIXI is that it is defined using a mixture over semimeasures that need not sum to 1, rather than over proper probability measures. In this work we argue that the shortfall of a semimeasure can naturally be interpreted as the agent's estimate of the probability of its death. We formally define death for generally intelligent agents like AIXI, and prove a number of related theorems about their behaviour. Notable discoveries include that agent behaviour can change radically under positive linear transformations of the reward signal (from suicidal to dogmatically self-preserving), and that the agent's posterior belief that it will survive increases over time.\n    ",
        "submission_date": "2016-06-02T00:00:00",
        "last_modified_date": "2016-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01015",
        "title": "Selecting the Best Player Formation for Corner-Kick Situations Based on Bayes' Estimation",
        "authors": [
            "Jordan Henrio",
            "Thomas Henn",
            "Tomoharu Nakashima",
            "Hidehisa Akiyama"
        ],
        "abstract": "In the domain of the Soccer simulation 2D league of the RoboCup project, appropriate player positioning against a given opponent team is an important factor of soccer team performance. This work proposes a model which decides the strategy that should be applied regarding a particular opponent team. This task can be realized by applying preliminary a learning phase where the model determines the most effective strategies against clusters of opponent teams. The model determines the best strategies by using sequential Bayes' estimators. As a first trial of the system, the proposed model is used to determine the association of player formations against opponent teams in the particular situation of corner-kick. The implemented model shows satisfying abilities to compare player formations that are similar to each other in terms of performance and determines the right ranking even by running a decent number of simulation games.\n    ",
        "submission_date": "2016-06-03T00:00:00",
        "last_modified_date": "2016-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01113",
        "title": "ECMdd: Evidential c-medoids clustering with multiple prototypes",
        "authors": [
            "Kuang Zhou",
            "Arnaud Martin",
            "Quan Pan",
            "Zhun-Ga Liu"
        ],
        "abstract": "In this work, a new prototype-based clustering method named Evidential C-Medoids (ECMdd), which belongs to the family of medoid-based clustering for proximity data, is proposed as an extension of Fuzzy C-Medoids (FCMdd) on the theoretical framework of belief functions. In the application of FCMdd and original ECMdd, a single medoid (prototype), which is supposed to belong to the object set, is utilized to represent one class. For the sake of clarity, this kind of ECMdd using a single medoid is denoted by sECMdd. In real clustering applications, using only one pattern to capture or interpret a class may not adequately model different types of group structure and hence limits the clustering performance. In order to address this problem, a variation of ECMdd using multiple weighted medoids, denoted by wECMdd, is presented. Unlike sECMdd, in wECMdd objects in each cluster carry various weights describing their degree of representativeness for that class. This mechanism enables each class to be represented by more than one object. Experimental results in synthetic and real data sets clearly demonstrate the superiority of sECMdd and wECMdd. Moreover, the clustering results by wECMdd can provide richer information for the inner structure of the detected classes with the help of prototype weights.\n    ",
        "submission_date": "2016-06-03T00:00:00",
        "last_modified_date": "2016-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01116",
        "title": "The belief noisy-or model applied to network reliability analysis",
        "authors": [
            "Kuang Zhou",
            "Arnaud Martin",
            "Quan Pan"
        ],
        "abstract": "One difficulty faced in knowledge engineering for Bayesian Network (BN) is the quan-tification step where the Conditional Probability Tables (CPTs) are determined. The number of parameters included in CPTs increases exponentially with the number of parent variables. The most common solution is the application of the so-called canonical gates. The Noisy-OR (NOR) gate, which takes advantage of the independence of causal interactions, provides a logarithmic reduction of the number of parameters required to specify a CPT. In this paper, an extension of NOR model based on the theory of belief functions, named Belief Noisy-OR (BNOR), is proposed. BNOR is capable of dealing with both aleatory and epistemic uncertainty of the network. Compared with NOR, more rich information which is of great value for making decisions can be got when the available knowledge is uncertain. Specially, when there is no epistemic uncertainty, BNOR degrades into NOR. Additionally, different structures of BNOR are presented in this paper in order to meet various needs of engineers. The application of BNOR model on the reliability evaluation problem of networked systems demonstrates its effectiveness.\n    ",
        "submission_date": "2016-06-03T00:00:00",
        "last_modified_date": "2016-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01380",
        "title": "Effective Multi-Robot Spatial Task Allocation using Model Approximations",
        "authors": [
            "Okan A\u015f\u0131k",
            "H. Levent Ak\u0131n"
        ],
        "abstract": "Real-world multi-agent planning problems cannot be solved using decision-theoretic planning methods due to the exponential complexity. We approximate firefighting in rescue simulation as a spatially distributed task and model with multi-agent Markov decision process. We use recent approximation methods for spatial task problems to reduce the model complexity. Our approximations are single-agent, static task, shortest path pruning, dynamic planning horizon, and task clustering. We create scenarios from RoboCup Rescue Simulation maps and evaluate our methods on these graph worlds. The results show that our approach is faster and better than comparable methods and has negligible performance loss compared to the optimal policy. We also show that our method has a similar performance as DCOP methods on example RCRS scenarios.\n    ",
        "submission_date": "2016-06-04T00:00:00",
        "last_modified_date": "2016-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01412",
        "title": "Distance Metric Ensemble Learning and the Andrews-Curtis Conjecture",
        "authors": [
            "Krzysztof Krawiec",
            "Jerry Swan"
        ],
        "abstract": "Motivated by the search for a counterexample to the Poincar\u00e9 conjecture in three and four dimensions, the Andrews-Curtis conjecture was proposed in 1965. It is now generally suspected that the Andrews-Curtis conjecture is false, but small potential counterexamples are not so numerous, and previous work has attempted to eliminate some via combinatorial search. Progress has however been limited, with the most successful approach (breadth-first-search using secondary storage) being neither scalable nor heuristically-informed. A previous empirical analysis of problem structure examined several heuristic measures of search progress and determined that none of them provided any useful guidance for search. In this article, we induce new quality measures directly from the problem structure and combine them to produce a more effective search driver via ensemble machine learning. By this means, we eliminate 19 potential counterexamples, the status of which had been unknown for some years.\n    ",
        "submission_date": "2016-06-04T00:00:00",
        "last_modified_date": "2016-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01868",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation",
        "authors": [
            "Marc G. Bellemare",
            "Sriram Srinivasan",
            "Georg Ostrovski",
            "Tom Schaul",
            "David Saxton",
            "Remi Munos"
        ],
        "abstract": "We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across observations. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge.\n    ",
        "submission_date": "2016-06-06T00:00:00",
        "last_modified_date": "2016-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01924",
        "title": "Preliminaries of a Space Situational Awareness Ontology",
        "authors": [
            "Robert John Rovetto",
            "T.S. Kelso"
        ],
        "abstract": "Space situational awareness (SSA) is vital for international safety and security, and the future of space travel. By improving SSA data-sharing we improve global SSA. Computational ontology may provide one means toward that goal. This paper develops the ontology of the SSA domain and takes steps in the creation of the space situational awareness ontology. Ontology objectives, requirements and desiderata are outlined; and both the SSA domain and the discipline of ontology are described. The purposes of the ontology include: exploring the potential for ontology development and engineering to (i) represent SSA data, general domain knowledge, objects and relationships (ii) annotate and express the meaning of that data, and (iii) foster SSA data-exchange and integration among SSA actors, orbital debris databases, space object catalogs and other SSA data repositories. By improving SSA via data- and knowledge-sharing, we can (iv) expand our scientific knowledge of the space environment, (v) advance our capacity for planetary defense from near-Earth objects, and (vi) ensure the future of safe space flight for generations to come.\n    ",
        "submission_date": "2016-06-02T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01949",
        "title": "Assisted Energy Management in Smart Microgrids",
        "authors": [
            "Andrea Monacchi",
            "Wilfried Elmenreich"
        ],
        "abstract": "Demand response provides utilities with a mechanism to share with end users the stochasticity resulting from the use of renewable sources. Pricing is accordingly used to reflect energy availability, to allocate such a limited resource to those loads that value it most. However, the strictly competitive mechanism can result in service interruption in presence of competing demand. To solve this issue we investigate on the use of forward contracts, i.e., service level agreements priced to reflect the expectation of future supply and demand curves. Given the limited resources of microgrids, service interruption is an opposite objective to the one of service availability. We firstly design policy-based brokers and identify then a learning broker based on artificial neural networks. We show the latter being progressively minimizing the reimbursement costs and maximizing the overall profit.\n    ",
        "submission_date": "2016-06-06T00:00:00",
        "last_modified_date": "2016-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02032",
        "title": "Human vs. Computer Go: Review and Prospect",
        "authors": [
            "Chang-Shing Lee",
            "Mei-Hui Wang",
            "Shi-Jim Yen",
            "Ting-Han Wei",
            "I-Chen Wu",
            "Ping-Chiang Chou",
            "Chun-Hsun Chou",
            "Ming-Wan Wang",
            "Tai-Hsiung Yang"
        ],
        "abstract": "The Google DeepMind challenge match in March 2016 was a historic achievement for computer Go development. This article discusses the development of computational intelligence (CI) and its relative strength in comparison with human intelligence for the game of Go. We first summarize the milestones achieved for computer Go from 1998 to 2016. Then, the computer Go programs that have participated in previous IEEE CIS competitions as well as methods and techniques used in AlphaGo are briefly introduced. Commentaries from three high-level professional Go players on the five AlphaGo versus Lee Sedol games are also included. We conclude that AlphaGo beating Lee Sedol is a huge achievement in artificial intelligence (AI) based largely on CI methods. In the future, powerful computer Go programs such as AlphaGo are expected to be instrumental in promoting Go education and AI real-world applications.\n    ",
        "submission_date": "2016-06-07T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02041",
        "title": "Sorting out symptoms: design and evaluation of the 'babylon check' automated triage system",
        "authors": [
            "Katherine Middleton",
            "Mobasher Butt",
            "Nils Hammerla",
            "Steven Hamblin",
            "Karan Mehta",
            "Ali Parsa"
        ],
        "abstract": "Prior to seeking professional medical care it is increasingly common for patients to use online resources such as automated symptom checkers. Many such systems attempt to provide a differential diagnosis based on the symptoms elucidated from the user, which may lead to anxiety if life or limb-threatening conditions are part of the list, a phenomenon termed 'cyberchondria' [1]. Systems that provide advice on where to seek help, rather than a diagnosis, are equally popular, and in our view provide the most useful information. In this technical report we describe how such a triage system can be modelled computationally, how medical insights can be translated into triage flows, and how such systems can be validated and tested. We present babylon check, our commercially deployed automated triage system, as a case study, and illustrate its performance in a large, semi-naturalistic deployment study.\n    ",
        "submission_date": "2016-06-07T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02096",
        "title": "Towards Playlist Generation Algorithms Using RNNs Trained on Within-Track Transitions",
        "authors": [
            "Keunwoo Choi",
            "George Fazekas",
            "Mark Sandler"
        ],
        "abstract": "We introduce a novel playlist generation algorithm that focuses on the quality of transitions using a recurrent neural network (RNN). The proposed model assumes that optimal transitions between tracks can be modelled and predicted by internal transitions within music tracks. We introduce modelling sequences of high-level music descriptors using RNNs and discuss an experiment involving different similarity functions, where the sequences are provided by a musical structural analysis algorithm. Qualitative observations show that the proposed approach can effectively model transitions of music tracks in playlists.\n    ",
        "submission_date": "2016-06-07T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02221",
        "title": "Multi-resource defensive strategies for patrolling games with alarm systems",
        "authors": [
            "Nicola Basilico",
            "Giuseppe De Nittis",
            "Nicola Gatti"
        ],
        "abstract": "Security Games employ game theoretical tools to derive resource allocation strategies in security domains. Recent works considered the presence of alarm systems, even suffering various forms of uncertainty, and showed that disregarding alarm signals may lead to arbitrarily bad strategies. The central problem with an alarm system, unexplored in other Security Games, is finding the best strategy to respond to alarm signals for each mobile defensive resource. The literature provides results for the basic single-resource case, showing that even in that case the problem is computationally hard. In this paper, we focus on the challenging problem of designing algorithms scaling with multiple resources. First, we focus on finding the minimum number of resources assuring non-null protection to every target. Then, we deal with the computation of multi-resource strategies with different degrees of coordination among resources. For each considered problem, we provide a computational analysis and propose algorithmic methods.\n    ",
        "submission_date": "2016-06-07T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02231",
        "title": "Emotional Intensity analysis in Bipolar subjects",
        "authors": [
            "Facundo Carrillo",
            "Natalia Mota",
            "Mauro Copelli",
            "Sidarta Ribeiro",
            "Mariano Sigman",
            "Guillermo Cecchi",
            "Diego Fernandez Slezak"
        ],
        "abstract": "The massive availability of digital repositories of human thought opens radical novel way of studying the human mind. Natural language processing tools and computational models have evolved such that many mental conditions are predicted by analysing speech. Transcription of interviews and discourses are analyzed using syntactic, grammatical or sentiment analysis to infer the mental state. Here we set to investigate if classification of Bipolar and control subjects is possible. We develop the Emotion Intensity Index based on the Dictionary of Affect, and find that subjects categories are distinguishable. Using classical classification techniques we get more than 75\\% of labeling performance. These results sumed to previous studies show that current automated speech analysis is capable of identifying altered mental states towards a quantitative psychiatry.\n    ",
        "submission_date": "2016-06-07T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02560",
        "title": "Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning",
        "authors": [
            "Tiancheng Zhao",
            "Maxine Eskenazi"
        ],
        "abstract": "This paper presents an end-to-end framework for task-oriented dialog systems using a variant of Deep Recurrent Q-Networks (DRQN). The model is able to interface with a relational database and jointly learn policies for both language understanding and dialog strategy. Moreover, we propose a hybrid algorithm that combines the strength of reinforcement learning and supervised learning to achieve faster learning speed. We evaluated the proposed model on a 20 Question Game conversational game simulator. Results show that the proposed method outperforms the modular-based baseline and learns a distributed representation of the latent dialog state.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02562",
        "title": "DialPort: Connecting the Spoken Dialog Research Community to Real User Data",
        "authors": [
            "Tiancheng Zhao",
            "Kyusong Lee",
            "Maxine Eskenazi"
        ],
        "abstract": "This paper describes a new spoken dialog portal that connects systems produced by the spoken dialog academic research community and gives them access to real users. We introduce a distributed, multi-modal, multi-agent prototype dialog framework that affords easy integration with various remote resources, ranging from end-to-end dialog systems to external knowledge APIs. To date, the DialPort portal has successfully connected to the multi-domain spoken dialog system at Cambridge University, the NOAA (National Oceanic and Atmospheric Administration) weather API and the Yelp API.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02645",
        "title": "Simplified Boardgames",
        "authors": [
            "Jakub Kowalski",
            "Jakub Sutowicz",
            "Marek Szyku\u0142a"
        ],
        "abstract": "We formalize Simplified Boardgames language, which describes a subclass of arbitrary board games. The language structure is based on the regular expressions, which makes the rules easily machine-processable while keeping the rules concise and fairly human-readable.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02710",
        "title": "A Modified Vortex Search Algorithm for Numerical Function Optimization",
        "authors": [
            "Berat Do\u011fan"
        ],
        "abstract": "The Vortex Search (VS) algorithm is one of the recently proposed metaheuristic algorithms which was inspired from the vortical flow of the stirred fluids. Although the VS algorithm is shown to be a good candidate for the solution of certain optimization problems, it also has some drawbacks. In the VS algorithm, candidate solutions are generated around the current best solution by using a Gaussian distribution at each iteration pass. This provides simplicity to the algorithm but it also leads to some problems along. Especially, for the functions those have a number of local minimum points, to select a single point to generate candidate solutions leads the algorithm to being trapped into a local minimum point. Due to the adaptive step-size adjustment scheme used in the VS algorithm, the locality of the created candidate solutions is increased at each iteration pass. Therefore, if the algorithm cannot escape a local point as quickly as possible, it becomes much more difficult for the algorithm to escape from that point in the latter iterations. In this study, a modified Vortex Search algorithm (MVS) is proposed to overcome above mentioned drawback of the existing VS algorithm. In the MVS algorithm, the candidate solutions are generated around a number of points at each iteration pass. Computational results showed that with the help of this modification the global search ability of the existing VS algorithm is improved and the MVS algorithm outperformed the existing VS algorithm, PSO2011 and ABC algorithms for the benchmark numerical function set.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02767",
        "title": "Theoretical Robopsychology: Samu Has Learned Turing Machines",
        "authors": [
            "Norbert B\u00e1tfai"
        ],
        "abstract": "From the point of view of a programmer, the robopsychology is a synonym for the activity is done by developers to implement their machine learning applications. This robopsychological approach raises some fundamental theoretical questions of machine learning. Our discussion of these questions is constrained to Turing machines. Alan Turing had given an algorithm (aka the Turing Machine) to describe algorithms. If it has been applied to describe itself then this brings us to Turing's notion of the universal machine. In the present paper, we investigate algorithms to write algorithms. From a pedagogy point of view, this way of writing programs can be considered as a combination of learning by listening and learning by doing due to it is based on applying agent technology and machine learning. As the main result we introduce the problem of learning and then we show that it cannot easily be handled in reality therefore it is reasonable to use machine learning algorithm for learning Turing machines.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02899",
        "title": "A Cognitive Architecture for the Implementation of Emotions in Computing Systems",
        "authors": [
            "Jordi Vallverd\u00fa",
            "Max Talanov",
            "Salvatore Distefano",
            "Manuel Mazzara",
            "Alexander Tchitchigin",
            "Ildar Nurgaliev"
        ],
        "abstract": "In this paper we present a new neurobiologically-inspired affective cognitive architecture: NEUCOGAR (NEUromodulating COGnitive ARchitecture). The objective of NEUCOGAR is the identification of a mapping from the influence of serotonin, dopamine and noradrenaline to the computing processes based on Von Neuman's architecture, in order to implement affective phenomena which can operate on the Turing's machine model. As basis of the modeling we use and extend the L\u00f6vheim Cube of Emotion with parameters of the Von Neumann architecture. Validation is conducted via simulation on a computing system of dopamine neuromodulation and its effects on the Cortex. In the experimental phase of the project, the increase of computing power and storage redistribution due to emotion stimulus modulated by the dopamine system, confirmed the soundness of the model.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03137",
        "title": "Cooperative Inverse Reinforcement Learning",
        "authors": [
            "Dylan Hadfield-Menell",
            "Anca Dragan",
            "Pieter Abbeel",
            "Stuart Russell"
        ],
        "abstract": "For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2024-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03191",
        "title": "Fuzzy-Klassen Model for Development Disparities Analysis based on Gross Regional Domestic Product Sector of a Region",
        "authors": [
            "Tb. Ai Munandar",
            "Retantyo Wardoyo"
        ],
        "abstract": "Analysis of regional development imbalances quadrant has a very important meaning in order to see the extent of achievement of the development of certain areas as well as the difference. Factors that could be used as a tool to measure the inequality of development is to look at the average growth and development contribution of each sector of Gross Regional Domestic Product (GRDP) based on the analyzed region and the reference region. This study discusses the development of a model to determine the regional development imbalances using fuzzy approach system, and the rules of typology Klassen. The model is then called fuzzy-Klassen. Implications Product Mamdani fuzzy system is used in the model as an inference engine to generate output after defuzzyfication process. Application of MATLAB is used as a tool of analysis in this study. The test a result of Kota Cilegon is shows that there are significant differences between traditional Klassen typology analyses with the results of the model developed. Fuzzy model-Klassen shows GRDP sector inequality Cilegon City is dominated by Quadrant I (K4), where status is the sector forward and grows exponentially. While the traditional Klassen typology, half of GRDP sector is dominated by Quadrant IV (K4) with a sector that is lagging relative status.\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03229",
        "title": "Towards Anthropo-inspired Computational Systems: the $P^3$ Model",
        "authors": [
            "Michael W. Bridges",
            "Salvatore Distefano",
            "Manuel Mazzara",
            "Marat Minlebaev",
            "Max Talanov",
            "Jordi Vallverd\u00fa"
        ],
        "abstract": "This paper proposes a model which aim is providing a more coherent framework for agents design. We identify three closely related anthropo-centered domains working on separate functional levels. Abstracting from human physiology, psychology, and philosophy we create the $P^3$ model to be used as a multi-tier approach to deal with complex class of problems. The three layers identified in this model have been named PhysioComputing, MindComputing, and MetaComputing. Several instantiations of this model are finally presented related to different IT areas such as artificial intelligence, distributed computing, software and service engineering.\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03244",
        "title": "Simple epistemic planning: generalised gossiping",
        "authors": [
            "Martin C. Cooper",
            "Andreas Herzig",
            "Faustine Maffre",
            "Fr\u00e9d\u00e9ric Maris",
            "Pierre R\u00e9gnier"
        ],
        "abstract": "The gossip problem, in which information (known as secrets) must be shared among a certain number of agents using the minimum number of calls, is of interest in the conception of communication networks and protocols. We extend the gossip problem to arbitrary epistemic depths. For example, we may require not only that all agents know all secrets but also that all agents know that all agents know all secrets. We give optimal protocols for various versions of the generalised gossip problem, depending on the graph of communication links, in the case of two-way communications, one-way communications and parallel communication. We also study different variants which allow us to impose negative goals such as that certain agents must not know certain secrets. We show that in the presence of negative goals testing the existence of a successful protocol is NP-complete whereas this is always polynomial-time in the case of purely positive goals.\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03289",
        "title": "Tunable Online MUS/MSS Enumeration",
        "authors": [
            "Jaroslav Bendik",
            "Nikola Benes",
            "Ivana Cerna",
            "Jiri Barnat"
        ],
        "abstract": "In various areas of computer science, the problem of dealing with a set of constraints arises. If the set of constraints is unsatisfiable, one may ask for a minimal description of the reason for this unsatisifi- ability. Minimal unsatisifable subsets (MUSes) and maximal satisifiable subsets (MSSes) are two kinds of such minimal descriptions. The goal of this work is the enumeration of MUSes and MSSes for a given constraint system. As such full enumeration may be intractable in general, we focus on building an online algorithm, which produces MUSes/MSSes in an on-the-fly manner as soon as they are discovered. The problem has been studied before even in its online version. However, our algorithm uses a novel approach that is able to outperform current state-of-the art algorithms for online MUS/MSS enumeration. Moreover, the performance of our algorithm can be adjusted using tunable parameters. We evaluate the algorithm on a set of benchmarks.\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03298",
        "title": "Structured Factored Inference: A Framework for Automated Reasoning in Probabilistic Programming Languages",
        "authors": [
            "Avi Pfeffer",
            "Brian Ruttenberg",
            "William Kretschmer"
        ],
        "abstract": "Reasoning on large and complex real-world models is a computationally difficult task, yet one that is required for effective use of many AI applications. A plethora of inference algorithms have been developed that work well on specific models or only on parts of general models. Consequently, a system that can intelligently apply these inference algorithms to different parts of a model for fast reasoning is highly desirable. We introduce a new framework called structured factored inference (SFI) that provides the foundation for such a system. Using models encoded in a probabilistic programming language, SFI provides a sound means to decompose a model into sub-models, apply an inference algorithm to each sub-model, and combine the resulting information to answer a query. Our results show that SFI is nearly as accurate as exact inference yet retains the benefits of approximate inference methods.\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03329",
        "title": "Community Structure in Industrial SAT Instances",
        "authors": [
            "Carlos Ans\u00f3tegui",
            "Maria Luisa Bonet",
            "Jes\u00fas Gir\u00e1ldez-Cru",
            "Jordi Levy",
            "Laurent Simon"
        ],
        "abstract": "Modern SAT solvers have experienced a remarkable progress on solving industrial instances. Most of the techniques have been developed after an intensive experimental process. It is believed that these techniques exploit the underlying structure of industrial instances. However, there are few works trying to exactly characterize the main features of this structure.\n",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2019-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03402",
        "title": "Length bias in Encoder Decoder Models and a Case for Global Conditioning",
        "authors": [
            "Pavel Sountsov",
            "Sunita Sarawagi"
        ],
        "abstract": "Encoder-decoder networks are popular for modeling sequences probabilistically in many applications. These models use the power of the Long Short-Term Memory (LSTM) architecture to capture the full dependence among variables, unlike earlier models like CRFs that typically assumed conditional independence among non-adjacent variables. However in practice encoder-decoder models exhibit a bias towards short sequences that surprisingly gets worse with increasing beam size.\n",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03634",
        "title": "The Opacity of Backbones",
        "authors": [
            "Lane A. Hemaspaandra",
            "David E. Narv\u00e1ez"
        ],
        "abstract": "This paper approaches, using structural complexity theory, the question of whether there is a chasm between knowing an object exists and getting one's hands on the object or its properties. In particular, we study the nontransparency of so-called backbones. A backbone of a boolean formula $F$ is a collection $S$ of its variables for which there is a unique partial assignment $a_S$ such that $F[a_S]$ is satisfiable [MZK+99,WGS03]. We show that, under the widely believed assumption that integer factoring is hard, there exist sets of boolean formulas that have obvious, nontrivial backbones yet finding the values, $a_S$, of those backbones is intractable. We also show that, under the same assumption, there exist sets of boolean formulas that obviously have large backbones yet producing such a backbone $S$ is intractable. Furthermore, we show that if integer factoring is not merely worst-case hard but is frequently hard, as is widely believed, then the frequency of hardness in our two results is not too much less than that frequency. These results hold more generally, namely, in the settings where, respectively, one's assumption is that P $\\neq$ NP $\\cap$ coNP or that some problem in NP $\\cap$ coNP is frequently hard.\n    ",
        "submission_date": "2016-06-11T00:00:00",
        "last_modified_date": "2019-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03662",
        "title": "Store Location Selection via Mining Search Query Logs of Baidu Maps",
        "authors": [
            "Mengwen Xu",
            "Tianyi Wang",
            "Zhengwei Wu",
            "Jingbo Zhou",
            "Jian Li",
            "Haishan Wu"
        ],
        "abstract": "Choosing a good location when opening a new store is crucial for the future success of a business. Traditional methods include offline manual survey, which is very time consuming, and analytic models based on census data, which are un- able to adapt to the dynamic market. The rapid increase of the availability of big data from various types of mobile devices, such as online query data and offline positioning data, provides us with the possibility to develop automatic and accurate data-driven prediction models for business store placement. In this paper, we propose a Demand Distribution Driven Store Placement (D3SP) framework for business store placement by mining search query data from Baidu Maps. D3SP first detects the spatial-temporal distributions of customer demands on different business services via query data from Baidu Maps, the largest online map search engine in China, and detects the gaps between demand and sup- ply. Then we determine candidate locations via clustering such gaps. In the final stage, we solve the location optimization problem by predicting and ranking the number of customers. We not only deploy supervised regression models to predict the number of customers, but also learn to rank models to directly rank the locations. We evaluate our framework on various types of businesses in real-world cases, and the experiments results demonstrate the effectiveness of our methods. D3SP as the core function for store placement has already been implemented as a core component of our business analytics platform and could be potentially used by chain store merchants on Baidu Nuomi.\n    ",
        "submission_date": "2016-06-12T00:00:00",
        "last_modified_date": "2016-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03737",
        "title": "Detec\u00e7\u00e3o de comunidades em redes complexas para identificar gargalos e desperd\u00edcio de recursos em sistemas de \u00f4nibus",
        "authors": [
            "Carlos Caminha",
            "Vasco Furtado",
            "Vl\u00e1dia Pinheiro",
            "Caio Ponte"
        ],
        "abstract": "We propose here a methodology to help to understand the shortcomings of public transportation in a city via the mining of complex networks representing the supply and demand of public transport. We show how to build these networks based upon data on smart card use in buses via the application of algorithms that estimate an OD and reconstruct the complete itinerary of the passengers. The overlapping of the two networks sheds light in potential overload and waste in the offer of resources that can be mitigated with strategies for balancing supply and demand.\n    ",
        "submission_date": "2016-06-12T00:00:00",
        "last_modified_date": "2017-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03784",
        "title": "MITRE at SemEval-2016 Task 6: Transfer Learning for Stance Detection",
        "authors": [
            "Guido Zarrella",
            "Amy Marsh"
        ],
        "abstract": "We describe MITRE's submission to the SemEval-2016 Task 6, Detecting Stance in Tweets. This effort achieved the top score in Task A on supervised stance detection, producing an average F1 score of 67.8 when assessing whether a tweet author was in favor or against a topic. We employed a recurrent neural network initialized with features learned via distant supervision on two large unlabeled datasets. We trained embeddings of words and phrases with the word2vec skip-gram method, then used those features to learn sentence representations via a hashtag prediction auxiliary task. These sentence vectors were then fine-tuned for stance detection on several hundred labeled examples. The result was a high performing system that used transfer learning to maximize the value of the available training data.\n    ",
        "submission_date": "2016-06-13T00:00:00",
        "last_modified_date": "2016-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03832",
        "title": "Evidential Label Propagation Algorithm for Graphs",
        "authors": [
            "Kuang Zhou",
            "Arnaud Martin",
            "Quan Pan",
            "Zhun-Ga Liu"
        ],
        "abstract": "Community detection has attracted considerable attention crossing many areas as it can be used for discovering the structure and features of complex networks. With the increasing size of social networks in real world, community detection approaches should be fast and accurate. The Label Propagation Algorithm (LPA) is known to be one of the near-linear solutions and benefits of easy implementation, thus it forms a good basis for efficient community detection methods. In this paper, we extend the update rule and propagation criterion of LPA in the framework of belief functions. A new community detection approach, called Evidential Label Propagation (ELP), is proposed as an enhanced version of conventional LPA. The node influence is first defined to guide the propagation process. The plausibility is used to determine the domain label of each node. The update order of nodes is discussed to improve the robustness of the method. ELP algorithm will converge after the domain labels of all the nodes become unchanged. The mass assignments are calculated finally as memberships of nodes. The overlapping nodes and outliers can be detected simultaneously through the proposed method. The experimental results demonstrate the effectiveness of ELP.\n    ",
        "submission_date": "2016-06-13T00:00:00",
        "last_modified_date": "2016-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03894",
        "title": "A Probabilistic-Based Model for Binary CSP",
        "authors": [
            "Amine Balafrej",
            "Xavier Lorca",
            "Charlotte Truchet"
        ],
        "abstract": "This work introduces a probabilistic-based model for binary CSP that provides a fine grained analysis of its internal structure. Assuming that a domain modification could occur in the CSP, it shows how to express, in a predictive way, the probability that a domain value becomes inconsistent, then it express the expectation of the number of arc-inconsistent values in each domain of the constraint network. Thus, it express the expectation of the number of arc-inconsistent values for the whole constraint network. Next, it provides bounds for each of these three probabilistic indicators. Finally, a polytime algorithm, which propagates the probabilistic information, is presented.\n    ",
        "submission_date": "2016-06-13T00:00:00",
        "last_modified_date": "2016-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03935",
        "title": "A framework for redescription set construction",
        "authors": [
            "Matej Mihel\u010di\u0107",
            "Sa\u0161o D\u017eeroski",
            "Nada Lavra\u010d",
            "Tomislav \u0160muc"
        ],
        "abstract": "Redescription mining is a field of knowledge discovery that aims at finding different descriptions of similar subsets of instances in the data. These descriptions are represented as rules inferred from one or more disjoint sets of attributes, called views. As such, they support knowledge discovery process and help domain experts in formulating new hypotheses or constructing new knowledge bases and decision support systems. In contrast to previous approaches that typically create one smaller set of redescriptions satisfying a pre-defined set of constraints, we introduce a framework that creates large and heterogeneous redescription set from which user/expert can extract compact sets of differing properties, according to its own preferences. Construction of large and heterogeneous redescription set relies on CLUS-RM algorithm and a novel, conjunctive refinement procedure that facilitates generation of larger and more accurate redescription sets. The work also introduces the variability of redescription accuracy when missing values are present in the data, which significantly extends applicability of the method. Crucial part of the framework is the redescription set extraction based on heuristic multi-objective optimization procedure that allows user to define importance levels towards one or more redescription quality criteria. We provide both theoretical and empirical comparison of the novel framework against current state of the art redescription mining algorithms and show that it represents more efficient and versatile approach for mining redescriptions from data.\n    ",
        "submission_date": "2016-06-13T00:00:00",
        "last_modified_date": "2016-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04000",
        "title": "Using a Distributional Semantic Vector Space with a Knowledge Base for Reasoning in Uncertain Conditions",
        "authors": [
            "Douglas Summers-Stay",
            "Clare Voss",
            "Taylor Cassidy"
        ],
        "abstract": "The inherent inflexibility and incompleteness of commonsense knowledge bases (KB) has limited their usefulness. We describe a system called Displacer for performing KB queries extended with the analogical capabilities of the word2vec distributional semantic vector space (DSVS). This allows the system to answer queries with information which was not contained in the original KB in any form. By performing analogous queries on semantically related terms and mapping their answers back into the context of the original query using displacement vectors, we are able to give approximate answers to many questions which, if posed to the KB alone, would return no results.\n",
        "submission_date": "2016-06-13T00:00:00",
        "last_modified_date": "2016-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04087",
        "title": "Networked Intelligence: Towards Autonomous Cyber Physical Systems",
        "authors": [
            "Andre Karpistsenko"
        ],
        "abstract": "Developing intelligent systems requires combining results from both industry and academia. In this report you find an overview of relevant research fields and industrially applicable technologies for building very large scale cyber physical systems. A concept architecture is used to illustrate how existing pieces may fit together, and the maturity of the subsystems is estimated.\n",
        "submission_date": "2016-06-13T00:00:00",
        "last_modified_date": "2016-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04190",
        "title": "Micro-interventions in urban transport from pattern discovery on the flow of passengers and on the bus network",
        "authors": [
            "Carlos Caminha",
            "Vasco Furtado",
            "Vl\u00e1dia Pinheiro e Caio Ponte"
        ],
        "abstract": "In this paper, we describe a case study in a big metropolis, in which from data collected by digital sensors, we tried to understand mobility patterns of persons using buses and how this can generate knowledge to suggest interventions that are applied incrementally into the transportation network in use. We have first estimated an Origin-Destination matrix of buses users from datasets about the ticket validation and GPS positioning of buses. Then we represent the supply of buses with their routes through bus stops as a complex network, which allowed us to understand the bottlenecks of the current scenario and, in particular, applying community discovery techniques, to identify clusters that the service supply infrastructure has. Finally, from the superimposing of the flow of people represented in the OriginDestination matrix in the supply network, we exemplify how micro-interventions can be prospected by means of an example of the introduction of express routes.\n    ",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04216",
        "title": "Spreadsheet Probabilistic Programming",
        "authors": [
            "Mike Wu",
            "Yura Perov",
            "Frank Wood",
            "Hongseok Yang"
        ],
        "abstract": "Spreadsheet workbook contents are simple programs. Because of this, probabilistic programming techniques can be used to perform Bayesian inversion of spreadsheet computations. What is more, existing execution engines in spreadsheet applications such as Microsoft Excel can be made to do this using only built-in functionality. We demonstrate this by developing a native Excel implementation of both a particle Markov Chain Monte Carlo variant and black-box variational inference for spreadsheet probabilistic programming. The resulting engine performs probabilistically coherent inference over spreadsheet computations, notably including spreadsheets that include user-defined black-box functions. Spreadsheet engines that choose to integrate the functionality we describe in this paper will give their users the ability to both easily develop probabilistic models and maintain them over time by including actuals via a simple user-interface mechanism. For spreadsheet end-users this would mean having access to efficient and probabilistically coherent probabilistic modeling and inference for use in all kinds of decision making under uncertainty.\n    ",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04250",
        "title": "Experimental and causal view on information integration in autonomous agents",
        "authors": [
            "Philipp Geiger",
            "Katja Hofmann",
            "Bernhard Sch\u00f6lkopf"
        ],
        "abstract": "The amount of digitally available but heterogeneous information about the world is remarkable, and new technologies such as self-driving cars, smart homes, or the internet of things may further increase it. In this paper we present preliminary ideas about certain aspects of the problem of how such heterogeneous information can be harnessed by autonomous agents. After discussing potentials and limitations of some existing approaches, we investigate how \\emph{experiments} can help to obtain a better understanding of the problem. Specifically, we present a simple agent that integrates video data from a different agent, and implement and evaluate a version of it on the novel experimentation platform \\emph{Malmo}. The focus of a second investigation is on how information about the hardware of different agents, the agents' sensory data, and \\emph{causal} information can be utilized for knowledge transfer between agents and subsequently more data-efficient decision making. Finally, we discuss potential future steps w.r.t.\\ theory and experimentation, and formulate open questions.\n    ",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2018-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04345",
        "title": "Digits that are not: Generating new types through deep neural nets",
        "authors": [
            "Ak\u0131n Kazak\u00e7\u0131and Mehdi Cherti",
            "Bal\u00e1zs K\u00e9gl"
        ],
        "abstract": "For an artificial creative agent, an essential driver of the search for novelty is a value function which is often provided by the system designer or users. We argue that an important barrier for progress in creativity research is the inability of these systems to develop their own notion of value for novelty. We propose a notion of knowledge-driven creativity that circumvent the need for an externally imposed value function, allowing the system to explore based on what it has learned from a set of referential objects. The concept is illustrated by a specific knowledge model provided by a deep generative autoencoder. Using the described system, we train a knowledge model on a set of digit images and we use the same model to build coherent sets of new digits that do not belong to known digit types.\n    ",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04397",
        "title": "Relating Strong Spatial Cognition to Symbolic Problem Solving --- An Example",
        "authors": [
            "Ulrich Furbach",
            "Florian Furbach",
            "Christian Freksa"
        ],
        "abstract": "In this note, we discuss and analyse a shortest path finding approach using strong spatial cognition. It is compared with a symbolic graph-based algorithm and it is shown that both approaches are similar with respect to structure and complexity. Nevertheless, the strong spatial cognition solution is easy to understand and even pops up immediately when one has to solve the problem.\n    ",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04422",
        "title": "Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge",
        "authors": [
            "Luciano Serafini",
            "Artur d'Avila Garcez"
        ],
        "abstract": "We propose Logic Tensor Networks: a uniform framework for integrating automatic learning and reasoning. A logic formalism called Real Logic is defined on a first-order language whereby formulas have truth-value in the interval [0,1] and semantics defined concretely on the domain of real numbers. Logical constants are interpreted as feature vectors of real numbers. Real Logic promotes a well-founded integration of deductive reasoning on a knowledge-base and efficient data-driven relational machine learning. We show how Real Logic can be implemented in deep Tensor Neural Networks with the use of Google's tensorflow primitives. The paper concludes with experiments applying Logic Tensor Networks on a simple but representative example of knowledge completion.\n    ",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2016-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04442",
        "title": "DeepMath - Deep Sequence Models for Premise Selection",
        "authors": [
            "Alex A. Alemi",
            "Francois Chollet",
            "Niklas Een",
            "Geoffrey Irving",
            "Christian Szegedy",
            "Josef Urban"
        ],
        "abstract": "We study the effectiveness of neural sequence models for premise selection in automated theorem proving, one of the main bottlenecks in the formalization of mathematics. We propose a two stage approach for this task that yields good results for the premise selection task on the Mizar corpus while avoiding the hand-engineered features of existing state-of-the-art models. To our knowledge, this is the first time deep learning has been applied to theorem proving on a large scale.\n    ",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2017-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04486",
        "title": "Lifted Convex Quadratic Programming",
        "authors": [
            "Martin Mladenov",
            "Leonard Kleinhans",
            "Kristian Kersting"
        ],
        "abstract": "Symmetry is the essential element of lifted inference that has recently demon- strated the possibility to perform very efficient inference in highly-connected, but symmetric probabilistic models models. This raises the question, whether this holds for optimisation problems in general. Here we show that for a large class of optimisation methods this is actually the case. More precisely, we introduce the concept of fractional symmetries of convex quadratic programs (QPs), which lie at the heart of many machine learning approaches, and exploit it to lift, i.e., to compress QPs. These lifted QPs can then be tackled with the usual optimization toolbox (off-the-shelf solvers, cutting plane algorithms, stochastic gradients etc.). If the original QP exhibits symmetry, then the lifted one will generally be more compact, and hence their optimization is likely to be more efficient.\n    ",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04512",
        "title": "Why is Compiling Lifted Inference into a Low-Level Language so Effective?",
        "authors": [
            "Seyed Mehran Kazemi",
            "David Poole"
        ],
        "abstract": "First-order knowledge compilation techniques have proven efficient for lifted inference. They compile a relational probability model into a target circuit on which many inference queries can be answered efficiently. Early methods used data structures as their target circuit. In our KR-2016 paper, we showed that compiling to a low-level program instead of a data structure offers orders of magnitude speedup, resulting in the state-of-the-art lifted inference technique. In this paper, we conduct experiments to address two questions regarding our KR-2016 results: 1- does the speedup come from more efficient compilation or more efficient reasoning with the target circuit?, and 2- why are low-level programs more efficient target circuits than data structures?\n    ",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04589",
        "title": "Impossibility in Belief Merging",
        "authors": [
            "Am\u00edlcar Mata D\u00edaz",
            "Ram\u00f3n Pino P\u00e9rez"
        ],
        "abstract": "With the aim of studying social properties of belief merging and having a better understanding of impossibility, we extend in three ways the framework of logic-based merging introduced by Konieczny and Pino P\u00e9rez. First, at the level of representation of the information, we pass from belief bases to complex epistemic states. Second, the profiles are represented as functions of finite societies to the set of epistemic states (a sort of vectors) and not as multisets of epistemic states. Third, we extend the set of rational postulates in order to consider the epistemic versions of the classical postulates of Social Choice Theory: Standard Domain, Pareto Property, Independence of Irrelevant Alternatives and Absence of Dictator. These epistemic versions of social postulates are given, essentially, in terms of the finite propositional logic. We state some representation theorems for these operators. These extensions and representation theorems allow us to establish an epistemic and very general version of Arrow's Impossibility Theorem. One of the interesting features of our result, is that it holds for different representations of epistemic states; for instance conditionals, Ordinal Conditional functions and, of course, total preorders.\n    ",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04695",
        "title": "Strategic Attentive Writer for Learning Macro-Actions",
        "authors": [
            "Alexander",
            "Vezhnevets",
            "Volodymyr Mnih",
            "John Agapiou",
            "Simon Osindero",
            "Alex Graves",
            "Oriol Vinyals",
            "Koray Kavukcuoglu"
        ],
        "abstract": "We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner by purely interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub- sequences by learning for how long the plan can be committed to - i.e. followed without re-planing. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro- actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.\n    ",
        "submission_date": "2016-06-15T00:00:00",
        "last_modified_date": "2016-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04956",
        "title": "Assessing Human Error Against a Benchmark of Perfection",
        "authors": [
            "Ashton Anderson",
            "Jon Kleinberg",
            "Sendhil Mullainathan"
        ],
        "abstract": "An increasing number of domains are providing us with detailed trace data on human decisions in settings where we can evaluate the quality of these decisions via an algorithm. Motivated by this development, an emerging line of work has begun to consider whether we can characterize and predict the kinds of decisions where people are likely to make errors.\n",
        "submission_date": "2016-06-15T00:00:00",
        "last_modified_date": "2016-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05174",
        "title": "Deep Reinforcement Learning Discovers Internal Models",
        "authors": [
            "Nir Baram",
            "Tom Zahavy",
            "Shie Mannor"
        ],
        "abstract": "Deep Reinforcement Learning (DRL) is a trending field of research, showing great promise in challenging problems such as playing Atari, solving Go and controlling robots. While DRL agents perform well in practice we are still lacking the tools to analayze their performance. In this work we present the Semi-Aggregated MDP (SAMDP) model. A model best suited to describe policies exhibiting both spatial and temporal hierarchies. We describe its advantages for analyzing trained policies over other modeling approaches, and show that under the right state representation, like that of DQN agents, SAMDP can help to identify skills. We detail the automatic process of creating it from recorded trajectories, up to presenting it on t-SNE maps. We explain how to evaluate its fitness and show surprising results indicating high compatibility with the policy at hand. We conclude by showing how using the SAMDP model, an extra performance gain can be squeezed from the agent.\n    ",
        "submission_date": "2016-06-16T00:00:00",
        "last_modified_date": "2016-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05312",
        "title": "Successor Features for Transfer in Reinforcement Learning",
        "authors": [
            "Andr\u00e9 Barreto",
            "Will Dabney",
            "R\u00e9mi Munos",
            "Jonathan J. Hunt",
            "Tom Schaul",
            "Hado van Hasselt",
            "David Silver"
        ],
        "abstract": "Transfer in reinforcement learning refers to the notion that generalization should occur not only within a task but also across tasks. We propose a transfer framework for the scenario where the reward function changes between tasks but the environment's dynamics remain the same. Our approach rests on two key ideas: \"successor features\", a value function representation that decouples the dynamics of the environment from the rewards, and \"generalized policy improvement\", a generalization of dynamic programming's policy improvement operation that considers a set of policies rather than a single one. Put together, the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows the free exchange of information across tasks. The proposed method also provides performance guarantees for the transferred policy even before any learning has taken place. We derive two theorems that set our approach in firm theoretical ground and present experiments that show that it successfully promotes transfer in practice, significantly outperforming alternative methods in a sequence of navigation tasks and in the control of a simulated robotic arm.\n    ",
        "submission_date": "2016-06-16T00:00:00",
        "last_modified_date": "2018-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05446",
        "title": "Abducing Compliance of Incomplete Event Logs",
        "authors": [
            "Federico Chesani",
            "Riccardo De Masellis",
            "Chiara Di Francescomarino",
            "Chiara Ghidini",
            "Paola Mello",
            "Marco Montali",
            "Sergio Tessaris"
        ],
        "abstract": "The capability to store data about business processes execution in so-called Event Logs has brought to the diffusion of tools for the analysis of process executions and for the assessment of the goodness of a process model. Nonetheless, these tools are often very rigid in dealing with with Event Logs that include incomplete information about the process execution. Thus, while the ability of handling incomplete event data is one of the challenges mentioned in the process mining manifesto, the evaluation of compliance of an execution trace still requires an end-to-end complete trace to be performed.\n",
        "submission_date": "2016-06-17T00:00:00",
        "last_modified_date": "2016-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05593",
        "title": "Introspective Agents: Confidence Measures for General Value Functions",
        "authors": [
            "Craig Sherstan",
            "Adam White",
            "Marlos C. Machado",
            "Patrick M. Pilarski"
        ],
        "abstract": "Agents of general intelligence deployed in real-world scenarios must adapt to ever-changing environmental conditions. While such adaptive agents may leverage engineered knowledge, they will require the capacity to construct and evaluate knowledge themselves from their own experience in a bottom-up, constructivist fashion. This position paper builds on the idea of encoding knowledge as temporally extended predictions through the use of general value functions. Prior work has focused on learning predictions about externally derived signals about a task or environment (e.g. battery level, joint position). Here we advocate that the agent should also predict internally generated signals regarding its own learning process - for example, an agent's confidence in its learned predictions. Finally, we suggest how such information would be beneficial in creating an introspective agent that is able to learn to make good decisions in a complex, changing world.\n    ",
        "submission_date": "2016-06-17T00:00:00",
        "last_modified_date": "2016-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05597",
        "title": "Adding Context to Concept Trees",
        "authors": [
            "Kieran Greer"
        ],
        "abstract": "A Concept Tree is a structure for storing knowledge where the trees are stored in a database called a Concept Base. It sits between the highly distributed neural architectures and the distributed information systems, with the intention of bringing brain-like and computer systems closer together. Concept Trees can grow from the semi-structured sources when consistent sequences of concepts are presented. Each tree ideally represents a single cohesive concept and the trees can link with each other for navigation and semantic purposes. The trees are therefore also a type of semantic network and would benefit from having a consistent level of context for each node. A consistent build process is managed through a 'counting rule' and some other rules that can normalise the database structure. This restricted structure can then be complimented and enriched by the more dynamic context. It is also suggested to use the linking structure of the licas system [15] as a basis for the context links, where the mathematical model is extended further to define this. A number of tests have demonstrated the soundness of the architecture. Building the trees from text documents shows that the tree structure could be inherent in natural language. Then, two types of query language are described. Both of these can perform consistent query processes to return knowledge to the user and even enhance the query with new knowledge. This is supported even further with direct comparisons to a cognitive model, also being developed by the author.\n    ",
        "submission_date": "2016-06-17T00:00:00",
        "last_modified_date": "2020-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05767",
        "title": "On Reward Function for Survival",
        "authors": [
            "Naoto Yoshida"
        ],
        "abstract": "Obtaining a survival strategy (policy) is one of the fundamental problems of biological agents. In this paper, we generalize the formulation of previous research related to the survival of an agent and we formulate the survival problem as a maximization of the multi-step survival probability in future time steps. We introduce a method for converting the maximization of multi-step survival probability into a classical reinforcement learning problem. Using this conversion, the reward function (negative temporal cost function) is expressed as the log of the temporal survival probability. And we show that the objective function of the reinforcement learning in this sense is proportional to the variational lower bound of the original problem. Finally, We empirically demonstrate that the agent learns survival behavior by using the reward function introduced in this paper.\n    ",
        "submission_date": "2016-06-18T00:00:00",
        "last_modified_date": "2016-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06041",
        "title": "Bandit-Based Random Mutation Hill-Climbing",
        "authors": [
            "Jialin Liu",
            "Diego Pe\u0155ez-Liebana",
            "Simon M. Lucas"
        ],
        "abstract": "The Random Mutation Hill-Climbing algorithm is a direct search technique mostly used in discrete domains. It repeats the process of randomly selecting a neighbour of a best-so-far solution and accepts the neighbour if it is better than or equal to it. In this work, we propose to use a novel method to select the neighbour solution using a set of independent multi- armed bandit-style selection units which results in a bandit-based Random Mutation Hill-Climbing algorithm. The new algorithm significantly outperforms Random Mutation Hill-Climbing in both OneMax (in noise-free and noisy cases) and Royal Road problems (in the noise-free case). The algorithm shows particular promise for discrete optimisation problems where each fitness evaluation is expensive.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06083",
        "title": "Product Classification in E-Commerce using Distributional Semantics",
        "authors": [
            "Vivek Gupta",
            "Harish Karnick",
            "Ashendra Bansal",
            "Pradhuman Jhala"
        ],
        "abstract": "Product classification is the task of automatically predicting a taxonomy path for a product in a predefined taxonomy hierarchy given a textual product description or title. For efficient product classification we require a suitable representation for a document (the textual description of a product) feature vector and efficient and fast algorithms for prediction. To address the above challenges, we propose a new distributional semantics representation for document vector formation. We also develop a new two-level ensemble approach utilizing (with respect to the taxonomy tree) a path-wise, node-wise and depth-wise classifiers for error reduction in the final product classification. Our experiments show the effectiveness of the distributional representation and the ensemble approach on data sets from a leading e-commerce platform and achieve better results on various evaluation metrics compared to earlier approaches.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06126",
        "title": "Bootstrapping with Models: Confidence Intervals for Off-Policy Evaluation",
        "authors": [
            "Josiah P. Hanna",
            "Peter Stone",
            "Scott Niekum"
        ],
        "abstract": "For an autonomous agent, executing a poor policy may be costly or even dangerous. For such agents, it is desirable to determine confidence interval lower bounds on the performance of any given policy without executing said policy. Current methods for exact high confidence off-policy evaluation that use importance sampling require a substantial amount of data to achieve a tight lower bound. Existing model-based methods only address the problem in discrete state spaces. Since exact bounds are intractable for many domains we trade off strict guarantees of safety for more data-efficient approximate bounds. In this context, we propose two bootstrapping off-policy evaluation methods which use learned MDP transition models in order to estimate lower confidence bounds on policy performance with limited data in both continuous and discrete state spaces. Since direct use of a model may introduce bias, we derive a theoretical upper bound on model bias for when the model transition function is estimated with i.i.d. trajectories. This bound broadens our understanding of the conditions under which model-based methods have high bias. Finally, we empirically evaluate our proposed methods and analyze the settings in which different bootstrapping off-policy confidence interval methods succeed and fail.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2018-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06355",
        "title": "A Hierarchical Reinforcement Learning Method for Persistent Time-Sensitive Tasks",
        "authors": [
            "Xiao Li",
            "Calin Belta"
        ],
        "abstract": "Reinforcement learning has been applied to many interesting problems such as the famous TD-gammon and the inverted helicopter flight. However, little effort has been put into developing methods to learn policies for complex persistent tasks and tasks that are time-sensitive. In this paper, we take a step towards solving this problem by using signal temporal logic (STL) as task specification, and taking advantage of the temporal abstraction feature that the options framework provide. We show via simulation that a relatively easy to implement algorithm that combines STL and options can learn a satisfactory policy with a small number of training cases\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06357",
        "title": "Complex Embeddings for Simple Link Prediction",
        "authors": [
            "Th\u00e9o Trouillon",
            "Johannes Welbl",
            "Sebastian Riedel",
            "\u00c9ric Gaussier",
            "Guillaume Bouchard"
        ],
        "abstract": "In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06434",
        "title": "The Schema Editor of OpenIoT for Semantic Sensor Networks",
        "authors": [
            "Prem Prakash Jayaraman",
            "Jean-Paul Calbimonte",
            "Hoan Nguyen Mau Quoc"
        ],
        "abstract": "Ontologies provide conceptual abstractions over data, in domains such as the Internet of Things, in a way that sensor data can be harvested and interpreted by people and applications. The Semantic Sensor Network (SSN) ontology is the de-facto standard for semantic representation of sensor observations and metadata, and it is used at the core of the open source platform for the Internet of Things, OpenIoT. In this paper we present a Schema Editor that provides an intuitive web interface for defining new types of sensors, and concrete instances of them, using the SSN ontology as the core model. This editor is fully integrated with the OpenIoT platform for generating virtual sensor descriptions and automating their semantic annotation and registration process.\n    ",
        "submission_date": "2016-06-21T00:00:00",
        "last_modified_date": "2016-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06565",
        "title": "Concrete Problems in AI Safety",
        "authors": [
            "Dario Amodei",
            "Chris Olah",
            "Jacob Steinhardt",
            "Paul Christiano",
            "John Schulman",
            "Dan Man\u00e9"
        ],
        "abstract": "Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (\"avoiding side effects\" and \"avoiding reward hacking\"), an objective function that is too expensive to evaluate frequently (\"scalable supervision\"), or undesirable behavior during the learning process (\"safe exploration\" and \"distributional shift\"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.\n    ",
        "submission_date": "2016-06-21T00:00:00",
        "last_modified_date": "2016-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06797",
        "title": "\u00c9tude de Probl\u00e8mes d'Optimisation Combinatoire \u00e0 Multiples Composantes Interd\u00e9pendantes",
        "authors": [
            "Mohamed El Yafrani",
            "Bela\u00efd Ahiod"
        ],
        "abstract": "This extended abstract presents an overview on NP-hard optimization problems with multiple interdependent components. These problems occur in many real-world applications: industrial applications, engineering, and logistics. The fact that these problems are composed of many sub-problems that are NP-hard makes them even more challenging to solve using exact algorithms. This is mainly due to the high complexity of this class of algorithms and the hardness of the problems themselves. The main source of difficulty of these problems is the presence of internal dependencies between sub-problems. This aspect of interdependence of components is presented, and some outlines on solving approaches are briefly introduced from a (meta)heuristics and evolutionary computation perspective.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06888",
        "title": "Structure in the Value Function of Two-Player Zero-Sum Games of Incomplete Information",
        "authors": [
            "Auke J. Wiggers",
            "Frans A. Oliehoek",
            "Diederik M. Roijers"
        ],
        "abstract": "Zero-sum stochastic games provide a rich model for competitive decision making. However, under general forms of state uncertainty as considered in the Partially Observable Stochastic Game (POSG), such decision making problems are still not very well understood. This paper makes a contribution to the theory of zero-sum POSGs by characterizing structure in their value function. In particular, we introduce a new formulation of the value function for zs-POSGs as a function of the \"plan-time sufficient statistics\" (roughly speaking the information distribution in the POSG), which has the potential to enable generalization over such information distributions. We further delineate this generalization capability by proving a structural result on the shape of value function: it exhibits concavity and convexity with respect to appropriately chosen marginals of the statistic space. This result is a key pre-cursor for developing solution methods that may be able to exploit such structure. Finally, we show how these results allow us to reduce a zs-POSG to a \"centralized\" model with shared observations, thereby transferring results for the latter, narrower class, to games with individual (private) observations.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07056",
        "title": "Emulating Human Conversations using Convolutional Neural Network-based IR",
        "authors": [
            "Abhay Prakash",
            "Chris Brockett",
            "Puneet Agrawal"
        ],
        "abstract": "Conversational agents (\"bots\") are beginning to be widely used in conversational interfaces. To design a system that is capable of emulating human-like interactions, a conversational layer that can serve as a fabric for chat-like interaction with the agent is needed. In this paper, we introduce a model that employs Information Retrieval by utilizing convolutional deep structured semantic neural network-based features in the ranker to present human-like responses in ongoing conversation with a user. In conversations, accounting for context is critical to the retrieval model; we show that our context-sensitive approach using a Convolutional Deep Structured Semantic Model (cDSSM) with character trigrams significantly outperforms several conventional baselines in terms of the relevance of responses retrieved.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07095",
        "title": "Finding Proofs in Tarskian Geometry",
        "authors": [
            "Michael Beeson",
            "Larry Wos"
        ],
        "abstract": "We report on a project to use a theorem prover to find proofs of the theorems in Tarskian geometry. These theorems start with fundamental properties of betweenness, proceed through the derivations of several famous theorems due to Gupta and end with the derivation from Tarski's axioms of Hilbert's 1899 axioms for geometry. They include the four challenge problems left unsolved by Quaife, who two decades ago found some \\Otter proofs in Tarskian geometry (solving challenges issued in Wos's 1998 book). There are 212 theorems in this collection. We were able to find \\Otter proofs of all these theorems. We developed a methodology for the automated preparation and checking of the input files for those theorems, to ensure that no human error has corrupted the formal development of an entire theory as embodied in two hundred input files and proofs. We distinguish between proofs that were found completely mechanically (without reference to the steps of a book proof) and proofs that were constructed by some technique that involved a human knowing the steps of a book proof. Proofs of length 40--100, roughly speaking, are difficult exercises for a human, and proofs of 100-250 steps belong in a Ph.D. thesis or publication. 29 of the proofs in our collection are longer than 40 steps, and ten are longer than 90 steps. We were able to derive completely mechanically all but 26 of the 183 theorems that have \"short\" proofs (40 or fewer deduction steps). We found proofs of the rest, as well as the 29 \"hard\" theorems, using a method that requires consulting the book proof at the outset. Our \"subformula strategy\" enabled us to prove four of the 29 hard theorems completely mechanically. These are Ph.D. level proofs, of length up to 108.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07137",
        "title": "Automated Extraction of Number of Subjects in Randomised Controlled Trials",
        "authors": [
            "Abeed Sarker"
        ],
        "abstract": "We present a simple approach for automatically extracting the number of subjects involved in randomised controlled trials (RCT). Our approach first applies a set of rule-based techniques to extract candidate study sizes from the abstracts of the articles. Supervised classification is then performed over the candidates with support vector machines, using a small set of lexical, structural, and contextual features. With only a small annotated training set of 201 RCTs, we obtained an accuracy of 88\\%. We believe that this system will aid complex medical text processing tasks such as summarisation and question answering.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07154",
        "title": "E-commerce in Your Inbox: Product Recommendations at Scale",
        "authors": [
            "Mihajlo Grbovic",
            "Vladan Radosavljevic",
            "Nemanja Djuric",
            "Narayan Bhamidipati",
            "Jaikit Savla",
            "Varun Bhagwan",
            "Doug Sharp"
        ],
        "abstract": "In recent years online advertising has become increasingly ubiquitous and effective. Advertisements shown to visitors fund sites and apps that publish digital content, manage social networks, and operate e-mail services. Given such large variety of internet resources, determining an appropriate type of advertising for a given platform has become critical to financial success. Native advertisements, namely ads that are similar in look and feel to content, have had great success in news and social feeds. However, to date there has not been a winning formula for ads in e-mail clients. In this paper we describe a system that leverages user purchase history determined from e-mail receipts to deliver highly personalized product ads to Yahoo Mail users. We propose to use a novel neural language-based algorithm specifically tailored for delivering effective product recommendations, which was evaluated against baselines that included showing popular products and products predicted based on co-occurrence. We conducted rigorous offline testing using a large-scale product purchase data set, covering purchases of more than 29 million users from 172 e-commerce websites. Ads in the form of product recommendations were successfully tested on online traffic, where we observed a steady 9% lift in click-through rates over other ad formats in mail, as well as comparable lift in conversion rates. Following successful tests, the system was launched into production during the holiday season of 2014.\n    ",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07233",
        "title": "Adaptive Task Assignment in Online Learning Environments",
        "authors": [
            "Per-Arne Andersen",
            "Christian Kr\u00e5kevik",
            "Morten Goodwin",
            "Anis Yazidi"
        ],
        "abstract": "With the increasing popularity of online learning, intelligent tutoring systems are regaining increased attention. In this paper, we introduce adaptive algorithms for personalized assignment of learning tasks to student so that to improve his performance in online learning environments. As main contribution of this paper, we propose a a novel Skill-Based Task Selector (SBTS) algorithm which is able to approximate a student's skill level based on his performance and consequently suggest adequate assignments. The SBTS is inspired by the class of multi-armed bandit algorithms. However, in contrast to standard multi-armed bandit approaches, the SBTS aims at acquiring two criteria related to student learning, namely: which topics should the student work on, and what level of difficulty should the task be. The SBTS centers on innovative reward and punishment schemes in a task and skill matrix based on the student behaviour.\n",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07514",
        "title": "Human-Agent Decision-making: Combining Theory and Practice",
        "authors": [
            "Sarit Kraus"
        ],
        "abstract": "Extensive work has been conducted both in game theory and logic to model strategic interaction. An important question is whether we can use these theories to design agents for interacting with people? On the one hand, they provide a formal design specification for agent strategies. On the other hand, people do not necessarily adhere to playing in accordance with these strategies, and their  behavior is affected by a multitude of social and psychological factors.  In this paper we will consider the question of whether strategies implied by theories of strategic behavior can be used by automated agents that interact proficiently with people. We will focus on automated agents that we built that need to interact with people in two negotiation settings: bargaining and deliberation. For bargaining we will study game-theory based equilibrium agents and for argumentation we will discuss logic-based argumentation theory. We will also consider security games and persuasion games and will discuss the benefits of using equilibrium based agents.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07516",
        "title": "Epistemic Protocols for Distributed Gossiping",
        "authors": [
            "Krzysztof R. Apt",
            "Davide Grossi",
            "Wiebe van der Hoek"
        ],
        "abstract": "Gossip protocols aim at arriving, by means of point-to-point or group communications, at a situation in which all the agents know each other's secrets. We consider distributed gossip protocols which are expressed by means of epistemic logic. We provide an operational semantics of such protocols and set up an appropriate framework to argue about their correctness. Then we analyze specific protocols for complete graphs and for directed rings.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07520",
        "title": "Standard State Space Models of Unawareness (Extended Abstract)",
        "authors": [
            "Peter Fritz",
            "Harvey Lederman"
        ],
        "abstract": "The impossibility theorem of Dekel, Lipman and Rustichini has been thought to demonstrate that standard state-space models cannot be used to represent unawareness. We first show that Dekel, Lipman and Rustichini do not establish this claim. We then distinguish three notions of awareness, and argue that although one of them may not be adequately modeled using standard state spaces, there is no reason to think that standard state spaces cannot provide models of the other two notions. In fact, standard space models of these forms of awareness are attractively simple. They allow us to prove completeness and decidability results with ease, to carry over standard techniques from decision theory, and to add propositional quantifiers straightforwardly.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07528",
        "title": "A Dynamic Epistemic Framework for Conformant Planning",
        "authors": [
            "Quan Yu",
            "Yanjun Li",
            "Yanjing Wang"
        ],
        "abstract": "In this paper, we introduce a lightweight dynamic epistemic logical framework for automated planning under initial uncertainty. We reduce plan verification and conformant planning to model checking problems of our logic. We show that the model checking problem of the iteration-free fragment is PSPACE-complete. By using two non-standard (but equivalent) semantics, we give novel model checking algorithms to the full language and the iteration-free language.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07529",
        "title": "The optimality of coarse categories in decision-making and information storage",
        "authors": [
            "Michael Mandler"
        ],
        "abstract": "An agent who lacks preferences and instead makes decisions using criteria that are costly to create should select efficient sets of criteria, where the cost of making a given number of choice distinctions is minimized. Under mild conditions, efficiency requires that binary criteria with only two categories per criterion are chosen. When applied to the problem of determining the optimal number of digits in an information storage device, this result implies that binary digits (bits) are the efficient solution, even when the marginal cost of using additional digits declines rapidly to 0. This short paper pays particular attention to the symmetry conditions entailed when sets of criteria are efficient.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07711",
        "title": "A Game-Theoretic Approach to Word Sense Disambiguation",
        "authors": [
            "Rocco Tripodi",
            "Marcello Pelillo"
        ],
        "abstract": "This paper presents a new model for word sense disambiguation formulated in terms of evolutionary game theory, where each word to be disambiguated is represented as a node on a graph whose edges represent word relations and senses are represented as classes. The words simultaneously update their class membership preferences according to the senses that neighboring words are likely to choose. We use distributional information to weigh the influence that each word has on the decisions of the others and semantic similarity information to measure the strength of compatibility among the choices. With this information we can formulate the word sense disambiguation problem as a constraint satisfaction problem and solve it using tools derived from game theory, maintaining the textual coherence. The model is based on two ideas: similar words should be assigned to similar classes and the meaning of a word does not depend on all the words in a text but just on some of them. The paper provides an in-depth motivation of the idea of modeling the word sense disambiguation problem in terms of game theory, which is illustrated by an example. The conclusion presents an extensive analysis on the combination of similarity measures to use in the framework and a comparison with state-of-the-art systems. The results show that our model outperforms state-of-the-art algorithms and can be applied to different tasks and in different scenarios.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07860",
        "title": "Non-Monotonic Spatial Reasoning with Answer Set Programming Modulo Theories",
        "authors": [
            "Przemys\u0142aw Andrzej Wa\u0142\u0119ga",
            "Carl Schultz",
            "Mehul Bhatt"
        ],
        "abstract": "The systematic modelling of dynamic spatial systems is a key requirement in a wide range of application areas such as commonsense cognitive robotics, computer-aided architecture design, and dynamic geographic information systems. We present ASPMT(QS), a novel approach and fully-implemented prototype for non-monotonic spatial reasoning -a crucial requirement within dynamic spatial systems- based on Answer Set Programming Modulo Theories (ASPMT).\n",
        "submission_date": "2016-06-25T00:00:00",
        "last_modified_date": "2016-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07955",
        "title": "X575: writing rengas with web services",
        "authors": [
            "Daniel Winterstein",
            "Joseph Corneli"
        ],
        "abstract": "Our software system simulates the classical collaborative Japanese poetry form, renga, made of linked haikus. We used NLP methods wrapped up as web services. Our experiments were only a partial success, since results fail to satisfy classical constraints. To gather ideas for future work, we examine related research in semiotics, linguistics, and computing.\n    ",
        "submission_date": "2016-06-25T00:00:00",
        "last_modified_date": "2016-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08109",
        "title": "Can Turing machine be curious about its Turing test results? Three informal lectures on physics of intelligence",
        "authors": [
            "Alex Ushveridze"
        ],
        "abstract": "What is the nature of curiosity? Is there any scientific way to understand the origin of this mysterious force that drives the behavior of even the stupidest naturally intelligent systems and is completely absent in their smartest artificial analogs? Can we build AI systems that could be curious about something, systems that would have an intrinsic motivation to learn? Is such a motivation quantifiable? Is it implementable? I will discuss this problem from the standpoint of physics. The relationship between physics and intelligence is a consequence of the fact that correctly predicted information is nothing but an energy resource, and the process of thinking can be viewed as a process of accumulating and spending this resource through the acts of perception and, respectively, decision making. The natural motivation of any autonomous system to keep this accumulation/spending balance as high as possible allows one to treat the problem of describing the dynamics of thinking processes as a resource optimization problem. Here I will propose and discuss a simple theoretical model of such an autonomous system which I call the Autonomous Turing Machine (ATM). The potential attractiveness of ATM lies in the fact that it is the model of a self-propelled AI for which the only available energy resource is the information itself. For ATM, the problem of optimal thinking, learning, and decision-making becomes conceptually simple and mathematically well tractable. This circumstance makes the ATM an ideal playground for studying the dynamics of intelligent behavior and allows one to quantify many seemingly unquantifiable features of genuine intelligence.\n    ",
        "submission_date": "2016-06-27T00:00:00",
        "last_modified_date": "2016-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08130",
        "title": "Propagators and Solvers for the Algebra of Modular Systems",
        "authors": [
            "Bart Bogaerts",
            "Eugenia Ternovska",
            "David Mitchell"
        ],
        "abstract": "To appear in the proceedings of LPAR 21.\n",
        "submission_date": "2016-06-27T00:00:00",
        "last_modified_date": "2017-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08333",
        "title": "True Lies",
        "authors": [
            "Thomas \u00c5gotnes",
            "Hans van Ditmarsch",
            "Yanjing Wang"
        ],
        "abstract": "A true lie is a lie that becomes true when announced. In a logic of announcements, where the announcing agent is not modelled, a true lie is a formula (that is false and) that becomes true when announced. We investigate true lies and other types of interaction between announced formulas, their preconditions and their postconditions, in the setting of Gerbrandy's logic of believed announcements, wherein agents may have or obtain incorrect beliefs. Our results are on the satisfiability and validity of instantiations of these semantically defined categories, on iterated announcements, including arbitrarily often iterated announcements, and on syntactic characterization. We close with results for iterated announcements in the logic of knowledge (instead of belief), and for lying as private announcements (instead of public announcements) to different agents. Detailed examples illustrate our lying concepts.\n    ",
        "submission_date": "2016-06-27T00:00:00",
        "last_modified_date": "2017-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08514",
        "title": "Towards Verified Artificial Intelligence",
        "authors": [
            "Sanjit A. Seshia",
            "Dorsa Sadigh",
            "S. Shankar Sastry"
        ],
        "abstract": "Verified artificial intelligence (AI) is the goal of designing AI-based systems that that have strong, ideally provable, assurances of correctness with respect to mathematically-specified requirements. This paper considers Verified AI from a formal methods perspective. We describe five challenges for achieving Verified AI, and five corresponding principles for addressing these challenges.\n    ",
        "submission_date": "2016-06-27T00:00:00",
        "last_modified_date": "2020-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08531",
        "title": "A Learning Algorithm for Relational Logistic Regression: Preliminary Results",
        "authors": [
            "Bahare Fatemi",
            "Seyed Mehran Kazemi",
            "David Poole"
        ],
        "abstract": "Relational logistic regression (RLR) is a representation of conditional probability in terms of weighted formulae for modelling multi-relational data. In this paper, we develop a learning algorithm for RLR models. Learning an RLR model from data consists of two steps: 1- learning the set of formulae to be used in the model (a.k.a. structure learning) and learning the weight of each formula (a.k.a. parameter learning). For structure learning, we deploy Schmidt and Murphy's hierarchical assumption: first we learn a model with simple formulae, then more complex formulae are added iteratively only if all their sub-formulae have proven effective in previous learned models. For parameter learning, we convert the problem into a non-relational learning problem and use an off-the-shelf logistic regression learning algorithm from Weka, an open-source machine learning tool, to learn the weights. We also indicate how hidden features about the individuals can be incorporated into RLR to boost the learning performance. We compare our learning algorithm to other structure and parameter learning algorithms in the literature, and compare the performance of RLR models to standard logistic regression and RDN-Boost on a modified version of the MovieLens data-set.\n    ",
        "submission_date": "2016-06-28T00:00:00",
        "last_modified_date": "2016-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08538",
        "title": "A Local Density-Based Approach for Local Outlier Detection",
        "authors": [
            "Bo Tang",
            "Haibo He"
        ],
        "abstract": "This paper presents a simple but effective density-based outlier detection approach with the local kernel density estimation (KDE). A Relative Density-based Outlier Score (RDOS) is introduced to measure the local outlierness of objects, in which the density distribution at the location of an object is estimated with a local KDE method based on extended nearest neighbors of the object. Instead of using only $k$ nearest neighbors, we further consider reverse nearest neighbors and shared nearest neighbors of an object for density distribution estimation. Some theoretical properties of the proposed RDOS including its expected value and false alarm probability are derived. A comprehensive experimental study on both synthetic and real-life data sets demonstrates that our approach is more effective than state-of-the-art outlier detection methods.\n    ",
        "submission_date": "2016-06-28T00:00:00",
        "last_modified_date": "2016-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08896",
        "title": "On the Semantic Relationship between Probabilistic Soft Logic and Markov Logic",
        "authors": [
            "Joohyung Lee",
            "Yi Wang"
        ],
        "abstract": "Markov Logic Networks (MLN) and Probabilistic Soft Logic (PSL) are widely applied formalisms in Statistical Relational Learning, an emerging area in Artificial Intelligence that is concerned with combining logical and statistical AI. Despite their resemblance, the relationship has not been formally stated. In this paper, we describe the precise semantic relationship between them from a logical perspective. This is facilitated by first extending fuzzy logic to allow weights, which can be also viewed as a generalization of PSL, and then relate that generalization to MLN. We observe that the relationship between PSL and MLN is analogous to the known relationship between fuzzy logic and Boolean logic, and furthermore the weight scheme of PSL is essentially a generalization of the weight scheme of MLN for the many-valued setting.\n    ",
        "submission_date": "2016-06-28T00:00:00",
        "last_modified_date": "2016-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08906",
        "title": "Exploring high-level Perspectives on Self-Configuration Capabilities of Systems",
        "authors": [
            "Aleksander Lodwich"
        ],
        "abstract": "Optimization of product performance repetitively introduces the need to make products adaptive in a more general sense. This more general idea is often captured under the term 'self-configuration'. Despite the importance of such capability, research work on this feature appears isolated by technical domains. It is not easy to tell quickly whether the approaches chosen in different technological domains introduce new ideas or whether the differences just reflect domain idiosyncrasies. For the sake of easy identification of key differences between systems with self-configuring capabilities, I will explore higher level concepts for understanding self-configuration, such as the {\\Omega}-units, in order to provide theoretical instruments for connecting different areas of technology and research.\n    ",
        "submission_date": "2016-06-28T00:00:00",
        "last_modified_date": "2016-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08962",
        "title": "Evaluation and selection of Medical Tourism sites: A rough AHP based MABAC approach",
        "authors": [
            "Jagannath Roy",
            "Kajal Chatterjee",
            "Abhirup Bandhopadhyay",
            "Samarjit Kar"
        ],
        "abstract": "In this paper, a novel multiple criteria decision making (MCDM) methodology is presented for assessing and prioritizing medical tourism destinations in uncertain environment. A systematic evaluation and assessment method is proposed by integrating rough number based AHP (Analytic Hierarchy Process) and rough number based MABAC (Multi-Attributive Border Approximation area Comparison). Rough number is used to aggregate individual judgments and preferences to deal with vagueness in decision making due to limited data. Rough AHP analyzes the relative importance of criteria based on their preferences given by experts. Rough MABAC evaluates the alternative sites based on the criteria weights. The proposed methodology is explained through a case study considering different cities for healthcare service in India. The validity of the obtained ranking for the given decision making problem is established by testing criteria proposed by Wang and Triantaphyllou (2008) along with further analysis and discussion.\n    ",
        "submission_date": "2016-06-29T00:00:00",
        "last_modified_date": "2016-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08963",
        "title": "Non-linear Label Ranking for Large-scale Prediction of Long-Term User Interests",
        "authors": [
            "Nemanja Djuric",
            "Mihajlo Grbovic",
            "Vladan Radosavljevic",
            "Narayan Bhamidipati",
            "Slobodan Vucetic"
        ],
        "abstract": "We consider the problem of personalization of online services from the viewpoint of ad targeting, where we seek to find the best ad categories to be shown to each user, resulting in improved user experience and increased advertisers' revenue. We propose to address this problem as a task of ranking the ad categories depending on a user's preference, and introduce a novel label ranking approach capable of efficiently learning non-linear, highly accurate models in large-scale settings. Experiments on a real-world advertising data set with more than 3.2 million users show that the proposed algorithm outperforms the existing solutions in terms of both rank loss and top-K retrieval performance, strongly suggesting the benefit of using the proposed model on large-scale ranking problems.\n    ",
        "submission_date": "2016-06-29T00:00:00",
        "last_modified_date": "2016-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08965",
        "title": "Credibilistic TOPSIS Model for Evaluation and Selection of Municipal Solid Waste Disposal Methods",
        "authors": [
            "Jagannath Roy",
            "Krishnendu Adhikary",
            "Samarjit Kar"
        ],
        "abstract": "Municipal solid waste management (MSWM) is a challenging issue of urban development in developing countries. Each country having different socio-economic-environmental background, might not accept a particular disposal method as the optimal choice. Selection of suitable disposal method in MSWM, under vague and imprecise information can be considered as multi criteria decision making problem (MCDM). In the present paper, TOPSIS (Technique for Order Preference by Similarity to Ideal Solution) methodology is extended based on credibility theory for evaluating the performances of MSW disposal methods under some criteria fixed by experts. The proposed model helps decision makers to choose a preferable alternative for their municipal area. A sensitivity analysis by our proposed model confirms this fact.\n    ",
        "submission_date": "2016-06-29T00:00:00",
        "last_modified_date": "2016-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09140",
        "title": "Algebraic foundations for qualitative calculi and networks",
        "authors": [
            "Robin Hirsch",
            "Marcel Jackson",
            "Tomasz Kowalski"
        ],
        "abstract": "A qualitative representation $\\phi$ is like an ordinary representation of a relation algebra, but instead of requiring $(a; b)^\\phi = a^\\phi | b^\\phi$, as we do for ordinary representations, we only require that $c^\\phi\\supseteq a^\\phi | b^\\phi \\iff c\\geq a ; b$, for each $c$ in the algebra. A constraint network is qualitatively satisfiable if its nodes can be mapped to elements of a qualitative representation, preserving the constraints. If a constraint network is satisfiable then it is clearly qualitatively satisfiable, but the converse can fail. However, for a wide range of relation algebras including the point algebra, the Allen Interval Algebra, RCC8 and many others, a network is satisfiable if and only if it is qualitatively satisfiable.\n",
        "submission_date": "2016-06-29T00:00:00",
        "last_modified_date": "2017-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09242",
        "title": "Swift: Compiled Inference for Probabilistic Programming Languages",
        "authors": [
            "Yi Wu",
            "Lei Li",
            "Stuart Russell",
            "Rastislav Bodik"
        ],
        "abstract": "A probabilistic program defines a probability measure over its semantic structures. One common goal of probabilistic programming languages (PPLs) is to compute posterior probabilities for arbitrary models and queries, given observed evidence, using a generic inference engine. Most PPL inference engines---even the compiled ones---incur significant runtime interpretation overhead, especially for contingent and open-universe models. This paper describes Swift, a compiler for the BLOG PPL. Swift-generated code incorporates optimizations that eliminate interpretation overhead, maintain dynamic dependencies efficiently, and handle memory management for possible worlds of varying sizes. Experiments comparing Swift with other PPL engines on a variety of inference problems demonstrate speedups ranging from 12x to 326x.\n    ",
        "submission_date": "2016-06-30T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09274",
        "title": "Compression of Neural Machine Translation Models via Pruning",
        "authors": [
            "Abigail See",
            "Minh-Thang Luong",
            "Christopher D. Manning"
        ],
        "abstract": "Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes. This paper examines three simple magnitude-based pruning schemes to compress NMT models, namely class-blind, class-uniform, and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture. We demonstrate the efficacy of weight pruning as a compression technique for a state-of-the-art NMT system. We show that an NMT model with over 200 million parameters can be pruned by 40% with very little performance loss as measured on the WMT'14 English-German translation task. This sheds light on the distribution of redundancy in the NMT architecture. Our main result is that with retraining, we can recover and even surpass the original performance with an 80%-pruned model.\n    ",
        "submission_date": "2016-06-29T00:00:00",
        "last_modified_date": "2016-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09296",
        "title": "How Many Folders Do You Really Need?",
        "authors": [
            "Mihajlo Grbovic",
            "Guy Halawi",
            "Zohar Karnin",
            "Yoelle Maarek"
        ],
        "abstract": "Email classification is still a mostly manual task. Consequently, most Web mail users never define a single folder. Recently however, automatic classification offering the same categories to all users has started to appear in some Web mail clients, such as AOL or Gmail. We adopt this approach, rather than previous (unsuccessful) personalized approaches because of the change in the nature of consumer email traffic, which is now dominated by (non-spam) machine-generated email. We propose here a novel approach for (1) automatically distinguishing between personal and machine-generated email and (2) classifying messages into latent categories, without requiring users to have defined any folder. We report how we have discovered that a set of 6 \"latent\" categories (one for human- and the others for machine-generated messages) can explain a significant portion of email traffic. We describe in details the steps involved in building a Web-scale email categorization system, from the collection of ground-truth labels, the selection of features to the training of models. Experimental evaluation was performed on more than 500 billion messages received during a period of six months by users of Yahoo mail service, who elected to be part of such research studies. Our system achieved precision and recall rates close to 90% and the latent categories we discovered were shown to cover 70% of both email traffic and email search queries. We believe that these results pave the way for a change of approach in the Web mail industry, and could support the invention of new large-scale email discovery paradigms that had not been possible before.\n    ",
        "submission_date": "2016-06-29T00:00:00",
        "last_modified_date": "2016-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09449",
        "title": "Clique-Width and Directed Width Measures for Answer-Set Programming",
        "authors": [
            "Bernhard Bliem",
            "Sebastian Ordyniak",
            "Stefan Woltran"
        ],
        "abstract": "Disjunctive Answer Set Programming (ASP) is a powerful declarative programming paradigm whose main decision problems are located on the second level of the polynomial hierarchy. Identifying tractable fragments and developing efficient algorithms for such fragments are thus important objectives in order to complement the sophisticated ASP systems available to date. Hard problems can become tractable if some problem parameter is bounded by a fixed constant; such problems are then called fixed-parameter tractable (FPT). While several FPT results for ASP exist, parameters that relate to directed or signed graphs representing the program at hand have been neglected so far. In this paper, we first give some negative observations showing that directed width measures on the dependency graph of a program do not lead to FPT results. We then consider the graph parameter of signed clique-width and present a novel dynamic programming algorithm that is FPT w.r.t. this parameter. Clique-width is more general than the well-known treewidth, and, to the best of our knowledge, ours is the first FPT algorithm for bounded clique-width for reasoning problems beyond SAT.\n    ",
        "submission_date": "2016-06-30T00:00:00",
        "last_modified_date": "2016-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09521",
        "title": "Probabilistic Reasoning in the Description Logic ALCP with the Principle of Maximum Entropy (Full Version)",
        "authors": [
            "Rafael Pe\u00f1aloza",
            "Nico Potyka"
        ],
        "abstract": "A central question for knowledge representation is how to encode and handle uncertain knowledge adequately. We introduce the probabilistic description logic ALCP that is designed for representing context-dependent knowledge, where the actual context taking place is uncertain. ALCP allows the expression of logical dependencies on the domain and probabilistic dependencies on the possible contexts. In order to draw probabilistic conclusions, we employ the principle of maximum entropy. We provide reasoning algorithms for this logic, and show that it satisfies several desirable properties of probabilistic logics.\n    ",
        "submission_date": "2016-06-30T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09577",
        "title": "Ordering as privileged information",
        "authors": [
            "Thomas Vacek"
        ],
        "abstract": "We propose to accelerate the rate of convergence of the pattern recognition task by directly minimizing the variance diameters of certain hypothesis spaces, which are critical quantities in fast-convergence ",
        "submission_date": "2016-06-30T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09594",
        "title": "Contextual Symmetries in Probabilistic Graphical Models",
        "authors": [
            "Ankit Anand",
            "Aditya Grover",
            "Mausam",
            "Parag Singla"
        ],
        "abstract": "An important approach for efficient inference in probabilistic graphical models exploits symmetries among objects in the domain. Symmetric variables (states) are collapsed into meta-variables (meta-states) and inference algorithms are run over the lifted graphical model instead of the flat one. Our paper extends existing definitions of symmetry by introducing the novel notion of contextual symmetry. Two states that are not globally symmetric, can be contextually symmetric under some specific assignment to a subset of variables, referred to as the context variables. Contextual symmetry subsumes previous symmetry definitions and can rep resent a large class of symmetries not representable earlier. We show how to compute contextual symmetries by reducing it to the problem of graph isomorphism. We extend previous work on exploiting symmetries in the MCMC framework to the case of contextual symmetries. Our experiments on several domains of interest demonstrate that exploiting contextual symmetries can result in significant computational gains.\n    ",
        "submission_date": "2016-06-30T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09637",
        "title": "Lifted Region-Based Belief Propagation",
        "authors": [
            "David Smith",
            "Parag Singla",
            "Vibhav Gogate"
        ],
        "abstract": "Due to the intractable nature of exact lifted inference, research has recently focused on the discovery of accurate and efficient approximate inference algorithms in Statistical Relational Models (SRMs), such as Lifted First-Order Belief Propagation. FOBP simulates propositional factor graph belief propagation without constructing the ground factor graph by identifying and lifting over redundant message computations. In this work, we propose a generalization of FOBP called Lifted Generalized Belief Propagation, in which both the region structure and the message structure can be lifted. This approach allows more of the inference to be performed intra-region (in the exact inference step of BP), thereby allowing simulation of propagation on a graph structure with larger region scopes and fewer edges, while still maintaining tractability. We demonstrate that the resulting algorithm converges in fewer iterations to more accurate results on a variety of SRMs.\n    ",
        "submission_date": "2016-06-30T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00061",
        "title": "Towards A Virtual Assistant That Can Be Taught New Tasks In Any Domain By Its End-Users",
        "authors": [
            "I. Dan Melamed",
            "Nobal B. Niraula"
        ],
        "abstract": "The challenge stated in the title can be divided into two main problems. The first problem is to reliably mimic the way that users interact with user interfaces. The second problem is to build an instructible agent, i.e. one that can be taught to execute tasks expressed as previously unseen natural language commands. This paper proposes a solution to the second problem, a system we call Helpa. End-users can teach Helpa arbitrary new tasks whose level of complexity is similar to the tasks available from today's most popular virtual assistants. Teaching Helpa does not involve any programming. Instead, users teach Helpa by providing just one example of a command paired with a demonstration of how to execute that command. Helpa does not rely on any pre-existing domain-specific knowledge. It is therefore completely domain-independent. Our usability study showed that end-users can teach Helpa many new tasks in less than a minute each, often much less.\n    ",
        "submission_date": "2016-06-30T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00087",
        "title": "Fractal Dimension Pattern Based Multiresolution Analysis for Rough Estimator of Person-Dependent Audio Emotion Recognition",
        "authors": [
            "Miao Cheng",
            "Ah Chung Tsoi"
        ],
        "abstract": "As a general means of expression, audio analysis and recognition has attracted much attentions for its wide applications in real-life world. Audio emotion recognition (AER) attempts to understand emotional states of human with the given utterance signals, and has been studied abroad for its further development on friendly human-machine interfaces. Distinguish from other existing works, the person-dependent patterns of audio emotions are conducted, and fractal dimension features are calculated for acoustic feature extraction. Furthermore, it is able to efficiently learn intrinsic characteristics of auditory emotions, while the utterance features are learned from fractal dimensions of each sub-bands. Experimental results show the proposed method is able to provide comparative performance for audio emotion recognition.\n    ",
        "submission_date": "2016-07-01T00:00:00",
        "last_modified_date": "2016-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00136",
        "title": "Missing Data Estimation in High-Dimensional Datasets: A Swarm Intelligence-Deep Neural Network Approach",
        "authors": [
            "Collins Leke",
            "Tshilidzi Marwala"
        ],
        "abstract": "In this paper, we examine the problem of missing data in high-dimensional datasets by taking into consideration the Missing Completely at Random and Missing at Random mechanisms, as well as theArbitrary missing pattern. Additionally, this paper employs a methodology based on Deep Learning and Swarm Intelligence algorithms in order to provide reliable estimates for missing data. The deep learning technique is used to extract features from the input data via an unsupervised learning approach by modeling the data distribution based on the input. This deep learning technique is then used as part of the objective function for the swarm intelligence technique in order to estimate the missing data after a supervised fine-tuning phase by minimizing an error function based on the interrelationship and correlation between features in the dataset. The investigated methodology in this paper therefore has longer running times, however, the promising potential outcomes justify the trade-off. Also, basic knowledge of statistics is presumed.\n    ",
        "submission_date": "2016-07-01T00:00:00",
        "last_modified_date": "2016-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00148",
        "title": "LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection",
        "authors": [
            "Pankaj Malhotra",
            "Anusha Ramakrishnan",
            "Gaurangi Anand",
            "Lovekesh Vig",
            "Puneet Agarwal",
            "Gautam Shroff"
        ],
        "abstract": "Mechanical devices such as engines, vehicles, aircrafts, etc., are typically instrumented with numerous sensors to capture the behavior and health of the machine. However, there are often external factors or variables which are not captured by sensors leading to time-series which are inherently unpredictable. For instance, manual controls and/or unmonitored environmental conditions or load may lead to inherently unpredictable time-series. Detecting anomalies in such scenarios becomes challenging using standard approaches based on mathematical models that rely on stationarity, or prediction models that utilize prediction errors to detect anomalies. We propose a Long Short Term Memory Networks based Encoder-Decoder scheme for Anomaly Detection (EncDec-AD) that learns to reconstruct 'normal' time-series behavior, and thereafter uses reconstruction error to detect anomalies. We experiment with three publicly available quasi predictable time-series datasets: power demand, space shuttle, and ECG, and two real-world engine datasets with both predictive and unpredictable behavior. We show that EncDec-AD is robust and can detect anomalies from predictable, unpredictable, periodic, aperiodic, and quasi-periodic time-series. Further, we show that EncDec-AD is able to detect anomalies from short time-series (length as small as 30) as well as long time-series (length as large as 500).\n    ",
        "submission_date": "2016-07-01T00:00:00",
        "last_modified_date": "2016-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00234",
        "title": "Neutrosophic Overset, Neutrosophic Underset, and Neutrosophic Offset. Similarly for Neutrosophic Over-/Under-/Off- Logic, Probability, and Statistics",
        "authors": [
            "Florentin Smarandache"
        ],
        "abstract": "Neutrosophic Over-/Under-/Off-Set and -Logic were defined by the author in 1995 and published for the first time in 2007. We extended the neutrosophic set respectively to Neutrosophic Overset {when some neutrosophic component is over 1}, Neutrosophic Underset {when some neutrosophic component is below 0}, and to Neutrosophic Offset {when some neutrosophic components are off the interval [0, 1], i.e. some neutrosophic component over 1 and other neutrosophic component below 0}. This is no surprise with respect to the classical fuzzy set/logic, intuitionistic fuzzy set/logic, or classical/imprecise probability, where the values are not allowed outside the interval [0, 1], since our real-world has numerous examples and applications of over-/under-/off-neutrosophic components. For example, person working overtime deserves a membership degree over 1, while a person producing more damage than benefit to a company deserves a membership below 0. Then, similarly, the Neutrosophic Logic/Measure/Probability/Statistics etc. were extended to respectively Neutrosophic Over-/Under-/Off-Logic, -Measure, -Probability, -Statistics etc. [Smarandache, 2007].\n    ",
        "submission_date": "2016-06-30T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00424",
        "title": "Learning Relational Dependency Networks for Relation Extraction",
        "authors": [
            "Dileep Viswanathan",
            "Ameet Soni",
            "Jude Shavlik",
            "Sriraam Natarajan"
        ],
        "abstract": "We consider the task of KBP slot filling -- extracting relation information from newswire documents for knowledge base construction. We present our pipeline, which employs Relational Dependency Networks (RDNs) to learn linguistic patterns for relation extraction. Additionally, we demonstrate how several components such as weak supervision, word2vec features, joint learning and the use of human advice, can be incorporated in this relational framework. We evaluate the different components in the benchmark KBP 2015 task and show that RDNs effectively model a diverse set of features and perform competitively with current state-of-the-art relation extraction.\n    ",
        "submission_date": "2016-07-01T00:00:00",
        "last_modified_date": "2016-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00428",
        "title": "Situated Structure Learning of a Bayesian Logic Network for Commonsense Reasoning",
        "authors": [
            "Haley Garrison",
            "Sonia Chernova"
        ],
        "abstract": "This paper details the implementation of an algorithm for automatically generating a high-level knowledge network to perform commonsense reasoning, specifically with the application of robotic task repair. The network is represented using a Bayesian Logic Network (BLN) (Jain, Waldherr, and Beetz 2009), which combines a set of directed relations between abstract concepts, including IsA, AtLocation, HasProperty, and UsedFor, with a corresponding probability distribution that models the uncertainty inherent in these relations. Inference over this network enables reasoning over the abstract concepts in order to perform appropriate object substitution or to locate missing objects in the robot's environment. The structure of the network is generated by combining information from two existing knowledge sources: ConceptNet (Speer and Havasi 2012), and WordNet (Miller 1995). This is done in a \"situated\" manner by only including information relevant a given context. Results show that the generated network is able to accurately predict object categories, locations, properties, and affordances in three different household scenarios.\n    ",
        "submission_date": "2016-07-01T00:00:00",
        "last_modified_date": "2016-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00446",
        "title": "A Greedy Approach to Adapting the Trace Parameter for Temporal Difference Learning",
        "authors": [
            "Martha White",
            "Adam White"
        ],
        "abstract": "One of the main obstacles to broad application of reinforcement learning methods is the parameter sensitivity of our core learning algorithms. In many large-scale applications, online computation and function approximation represent key strategies in scaling up reinforcement learning algorithms. In this setting, we have effective and reasonably well understood algorithms for adapting the learning-rate parameter, online during learning. Such meta-learning approaches can improve robustness of learning and enable specialization to current task, improving learning speed. For temporal-difference learning algorithms which we study here, there is yet another parameter, $\\lambda$, that similarly impacts learning speed and stability in practice. Unfortunately, unlike the learning-rate parameter, $\\lambda$ parametrizes the objective function that temporal-difference methods optimize. Different choices of $\\lambda$ produce different fixed-point solutions, and thus adapting $\\lambda$ online and characterizing the optimization is substantially more complex than adapting the learning-rate parameter. There are no meta-learning method for $\\lambda$ that can achieve (1) incremental updating, (2) compatibility with function approximation, and (3) maintain stability of learning under both on and off-policy sampling. In this paper we contribute a novel objective function for optimizing $\\lambda$ as a function of state rather than time. We derive a new incremental, linear complexity $\\lambda$-adaption algorithm that does not require offline batch updating or access to a model of the world, and present a suite of experiments illustrating the practicality of our new algorithm in three different settings. Taken together, our contributions represent a concrete step towards black-box application of temporal-difference learning methods in real world problems.\n    ",
        "submission_date": "2016-07-02T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00656",
        "title": "A Hybrid POMDP-BDI Agent Architecture with Online Stochastic Planning and Plan Caching",
        "authors": [
            "Gavin Rens",
            "Deshendran Moodley"
        ],
        "abstract": "This article presents an agent architecture for controlling an autonomous agent in stochastic environments. The architecture combines the partially observable Markov decision process (POMDP) model with the belief-desire-intention (BDI) framework. The Hybrid POMDP-BDI agent architecture takes the best features from the two approaches, that is, the online generation of reward-maximizing courses of action from POMDP theory, and sophisticated multiple goal management from BDI theory. We introduce the advances made since the introduction of the basic architecture, including (i) the ability to pursue multiple goals simultaneously and (ii) a plan library for storing pre-written plans and for storing recently generated plans for future reuse. A version of the architecture without the plan library is implemented and is evaluated using simulations. The results of the simulation experiments indicate that the approach is feasible.\n    ",
        "submission_date": "2016-07-03T00:00:00",
        "last_modified_date": "2016-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00715",
        "title": "Path planning with Inventory-driven Jump-Point-Search",
        "authors": [
            "Davide Aversa",
            "Sebastian Sardina",
            "Stavros Vassos"
        ],
        "abstract": "In many navigational domains the traversability of cells is conditioned on the path taken. This is often the case in video-games, in which a character may need to acquire a certain object (i.e., a key or a flying suit) to be able to traverse specific locations (e.g., doors or high walls). In order for non-player characters to handle such scenarios we present invJPS, an \"inventory-driven\" pathfinding approach based on the highly successful grid-based Jump-Point-Search (JPS) algorithm. We show, formally and experimentally, that the invJPS preserves JPS's optimality guarantees and its symmetry breaking advantages in inventory-based variants of game maps.\n    ",
        "submission_date": "2016-07-04T00:00:00",
        "last_modified_date": "2016-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00791",
        "title": "Formal analysis of HTM Spatial Pooler performance under predefined operation conditions",
        "authors": [
            "M. Pietron",
            "M. Wielgosz",
            "K. Wiatr"
        ],
        "abstract": "This paper introduces mathematical formalism for Spatial (SP) of Hierarchical Temporal Memory (HTM) with a spacial consideration for its hardware implementation. Performance of HTM network and its ability to learn and adjust to a problem at hand is governed by a large set of parameters. Most of parameters are codependent which makes creating efficient HTM-based solutions challenging. It requires profound knowledge of the settings and their impact on the performance of system. Consequently, this paper introduced a set of formulas which are to facilitate the design process by enhancing tedious trial-and-error method with a tool for choosing initial parameters which enable quick learning convergence. This is especially important in hardware implementations which are constrained by the limited resources of a platform. The authors focused especially on a formalism of Spatial Pooler and derive at the formulas for quality and convergence of the model. This may be considered as recipes for designing efficient HTM models for given input patterns.\n    ",
        "submission_date": "2016-07-04T00:00:00",
        "last_modified_date": "2016-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00819",
        "title": "Understanding the Abstract Dialectical Framework (Preliminary Report)",
        "authors": [
            "Sylwia Polberg"
        ],
        "abstract": "Among the most general structures extending the framework by Dung are the abstract dialectical frameworks (ADFs). They come equipped with various types of semantics, with the most prominent - the labeling-based one - analyzed in the context of computational complexity, signatures, instantiations and software support. This makes the abstract dialectical frameworks valuable tools for argumentation. However, there are fewer results available concerning the relation between the ADFs and other argumentation frameworks. In this paper we would like to address this issue by introducing a number of translations from various formalisms into ADFs. The results of our study show the similarities and differences between them, thus promoting the use and understanding of ADFs. Moreover, our analysis also proves their capability to model many of the existing frameworks, including those that go beyond the attack relation. Finally, translations allow other structures to benefit from the research on ADFs in general and from the existing software in particular.\n    ",
        "submission_date": "2016-07-04T00:00:00",
        "last_modified_date": "2016-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00869",
        "title": "Modeling of Item-Difficulty for Ontology-based MCQs",
        "authors": [
            "Vinu E.V",
            "Tahani Alsubait",
            "P. Sreenivasa Kumar"
        ],
        "abstract": "Multiple choice questions (MCQs) that can be generated from a domain ontology can significantly reduce human effort & time required for authoring & administering assessments in an e-Learning environment. Even though here are various methods for generating MCQs from ontologies, methods for determining the difficulty-levels of such MCQs are less explored. In this paper, we study various aspects and factors that are involved in determining the difficulty-score of an MCQ, and propose an ontology-based model for the prediction. This model characterizes the difficulty values associated with the stem and choice set of the MCQs, and describes a measure which combines both the scores. Further more, the notion of assigning difficultly-scores based on the skill level of the test taker is utilized for predicating difficulty-score of a stem. We studied the effectiveness of the predicted difficulty-scores with the help of a psychometric model from the Item Response Theory, by involving real-students and domain experts. Our results show that, the predicated difficulty-levels of the MCQs are having high correlation with their actual difficulty-levels.\n    ",
        "submission_date": "2016-07-04T00:00:00",
        "last_modified_date": "2016-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00888",
        "title": "Encoding Cryptographic Functions to SAT Using Transalg System",
        "authors": [
            "Ilya Otpuschennikov",
            "Alexander Semenov",
            "Irina Gribanova",
            "Oleg Zaikin",
            "Stepan Kochemazov"
        ],
        "abstract": "In this paper we propose the technology for constructing propositional encodings of discrete functions. It is aimed at solving inversion problems of considered functions using state-of-the-art SAT solvers. We implemented this technology in the form of the software system called Transalg, and used it to construct SAT encodings for a number of cryptanalysis problems. By applying SAT solvers to these encodings we managed to invert several cryptographic functions. In particular, we used the SAT encodings produced by Transalg to construct the family of two-block MD5 collisions in which the first 10 bytes are zeros. Also we used Transalg encoding for the widely known A5/1 keystream generator to solve several dozen of its cryptanalysis instances in a distributed computing environment. In the paper we compare in detail the functionality of Transalg with that of similar software systems.\n    ",
        "submission_date": "2016-07-04T00:00:00",
        "last_modified_date": "2016-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00992",
        "title": "Generic Statistical Relational Entity Resolution in Knowledge Graphs",
        "authors": [
            "Jay Pujara",
            "Lise Getoor"
        ],
        "abstract": "Entity resolution, the problem of identifying the underlying entity of references found in data, has been researched for many decades in many communities. A common theme in this research has been the importance of incorporating relational features into the resolution process. Relational entity resolution is particularly important in knowledge graphs (KGs), which have a regular structure capturing entities and their interrelationships. We identify three major problems in KG entity resolution: (1) intra-KG reference ambiguity; (2) inter-KG reference ambiguity; and (3) ambiguity when extending KGs with new facts. We implement a framework that generalizes across these three settings and exploits this regular structure of KGs. Our framework has many advantages over custom solutions widely deployed in industry, including collective inference, scalability, and interpretability. We apply our framework to two real-world KG entity resolution problems, ambiguity in NELL and merging data from Freebase and MusicBrainz, demonstrating the importance of relational features.\n    ",
        "submission_date": "2016-07-04T00:00:00",
        "last_modified_date": "2016-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01050",
        "title": "Application of Statistical Relational Learning to Hybrid Recommendation Systems",
        "authors": [
            "Shuo Yang",
            "Mohammed Korayem",
            "Khalifeh AlJadda",
            "Trey Grainger",
            "Sriraam Natarajan"
        ],
        "abstract": "Recommendation systems usually involve exploiting the relations among known features and content that describe items (content-based filtering) or the overlap of similar users who interacted with or rated the target item (collaborative filtering). To combine these two filtering approaches, current model-based hybrid recommendation systems typically require extensive feature engineering to construct a user profile. Statistical Relational Learning (SRL) provides a straightforward way to combine the two approaches. However, due to the large scale of the data used in real world recommendation systems, little research exists on applying SRL models to hybrid recommendation systems, and essentially none of that research has been applied on real big-data-scale systems. In this paper, we proposed a way to adapt the state-of-the-art in SRL learning approaches to construct a real hybrid recommendation system. Furthermore, in order to satisfy a common requirement in recommendation systems (i.e. that false positives are more undesirable and therefore penalized more harshly than false negatives), our approach can also allow tuning the trade-off between the precision and recall of the system in a principled way. Our experimental results demonstrate the efficiency of our proposed approach as well as its improved performance on recommendation precision.\n    ",
        "submission_date": "2016-07-04T00:00:00",
        "last_modified_date": "2016-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01254",
        "title": "An extended MABAC for multi-attribute decision making using trapezoidal interval type-2 fuzzy numbers",
        "authors": [
            "Jagannath Roy",
            "Ananta Ranjan",
            "Animesh Debnath",
            "Samarjit Kar"
        ],
        "abstract": "In this paper, we attempt to extend Multi Attributive Border Approximation area Comparison (MABAC) approach for multi-attribute decision making (MADM) problems based on type-2 fuzzy sets (IT2FSs). As a special case of IT2FSs interval type-2 trapezoidal fuzzy numbers (IT2TrFNs) are adopted here to deal with uncertainties present in many practical evaluation and selection problems. A systematic description of MABAC based on IT2TrFNs is presented in the current study. The validity and feasibility of the proposed method are illustrated by a practical example of selecting the most suitable candidate for a software company which is heading to hire a system analysis engineer based on few attributes. Finally, a comparison with two other existing MADM methods is described.\n    ",
        "submission_date": "2016-07-05T00:00:00",
        "last_modified_date": "2016-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01337",
        "title": "Can mobile usage predict illiteracy in a developing country?",
        "authors": [
            "P\u00e5l Sunds\u00f8y"
        ],
        "abstract": "The present study provides the first evidence that illiteracy can be reliably predicted from standard mobile phone logs. By deriving a broad set of mobile phone indicators reflecting users financial, social and mobility patterns we show how supervised machine learning can be used to predict individual illiteracy in an Asian developing country, externally validated against a large-scale survey. On average the model performs 10 times better than random guessing with a 70% accuracy. Further we show how individual illiteracy can be aggregated and mapped geographically at cell tower resolution. Geographical mapping of illiteracy is crucial to know where the illiterate people are, and where to put in resources. In underdeveloped countries such mappings are often based on out-dated household surveys with low spatial and temporal resolution. One in five people worldwide struggle with illiteracy, and it is estimated that illiteracy costs the global economy more than 1 trillion dollars each year. These results potentially enable costeffective, questionnaire-free investigation of illiteracy-related questions on an unprecedented scale\n    ",
        "submission_date": "2016-07-05T00:00:00",
        "last_modified_date": "2016-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01490",
        "title": "Towards Self-explanatory Ontology Visualization with Contextual Verbalization",
        "authors": [
            "Ren\u0101rs Liepi\u0146\u0161",
            "Uldis Boj\u0101rs",
            "Normunds Gr\u016bz\u012btis",
            "K\u0101rlis \u010cer\u0101ns",
            "Edgars Celms"
        ],
        "abstract": "Ontologies are one of the core foundations of the Semantic Web. To participate in Semantic Web projects, domain experts need to be able to understand the ontologies involved. Visual notations can provide an overview of the ontology and help users to understand the connections among entities. However, the users first need to learn the visual notation before they can interpret it correctly. Controlled natural language representation would be readable right away and might be preferred in case of complex axioms, however, the structure of the ontology would remain less apparent. We propose to combine ontology visualizations with contextual ontology verbalizations of selected ontology (diagram) elements, displaying controlled natural language (CNL) explanations of OWL axioms corresponding to the selected visual notation elements. Thus, the domain experts will benefit from both the high-level overview provided by the graphical notation and the detailed textual explanations of particular elements in the diagram.\n    ",
        "submission_date": "2016-07-06T00:00:00",
        "last_modified_date": "2016-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01634",
        "title": "Lattice Structure of Variable Precision Rough Sets",
        "authors": [
            "Sumita Basu"
        ],
        "abstract": "The main purpose of this paper is to study the lattice structure of variable precision rough sets. The notion of variation in precision of rough sets have been further extended to variable precision rough set with variable classification error and its algebraic properties are also studied.\n    ",
        "submission_date": "2016-07-06T00:00:00",
        "last_modified_date": "2016-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01729",
        "title": "Cost-Optimal Algorithms for Planning with Procedural Control Knowledge",
        "authors": [
            "Vikas Shivashankar",
            "Ron Alford",
            "Mark Roberts",
            "David W. Aha"
        ],
        "abstract": "There is an impressive body of work on developing heuristics and other reasoning algorithms to guide search in optimal and anytime planning algorithms for classical planning. However, very little effort has been directed towards developing analogous techniques to guide search towards high-quality solutions in hierarchical planning formalisms like HTN planning, which allows using additional domain-specific procedural control knowledge. In lieu of such techniques, this control knowledge often needs to provide the necessary search guidance to the planning algorithm, which imposes a substantial burden on the domain author and can yield brittle or error-prone domain models. We address this gap by extending recent work on a new hierarchical goal-based planning formalism called Hierarchical Goal Network (HGN) Planning to develop the Hierarchically-Optimal Goal Decomposition Planner (HOpGDP), an HGN planning algorithm that computes hierarchically-optimal plans. HOpGDP is guided by $h_{HL}$, a new HGN planning heuristic that extends existing admissible landmark-based heuristics from classical planning to compute admissible cost estimates for HGN planning problems. Our experimental evaluation across three benchmark planning domains shows that HOpGDP compares favorably to both optimal classical planners due to its ability to use domain-specific procedural knowledge, and a blind-search version of HOpGDP due to the search guidance provided by $h_{HL}$.\n    ",
        "submission_date": "2016-07-06T00:00:00",
        "last_modified_date": "2016-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01730",
        "title": "Rolling Horizon Coevolutionary Planning for Two-Player Video Games",
        "authors": [
            "Jialin Liu",
            "Diego P\u00e9rez-Li\u00e9bana",
            "Simon M. Lucas"
        ],
        "abstract": "This paper describes a new algorithm for decision making in two-player real-time video games. As with Monte Carlo Tree Search, the algorithm can be used without heuristics and has been developed for use in general video game AI. The approach is to extend recent work on rolling horizon evolutionary planning, which has been shown to work well for single-player games, to two (or in principle many) player games. To select an action the algorithm co-evolves two (or in the general case N) populations, one for each player, where each individual is a sequence of actions for the respective player. The fitness of each individual is evaluated by playing it against a selection of action-sequences from the opposing population. When choosing an action to take in the game, the first action is chosen from the fittest member of the population for that player. The new algorithm is compared with a number of general video game AI algorithms on three variations of a two-player space battle game, with promising results.\n    ",
        "submission_date": "2016-07-06T00:00:00",
        "last_modified_date": "2016-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02018",
        "title": "Mapping Data to Ontologies with Exceptions Using Answer Set Programming",
        "authors": [
            "Daniel P. Lupp",
            "Evgenij Thorstensen"
        ],
        "abstract": "In ontology-based data access, databases are connected to an ontology via mappings from queries over the database to queries over the ontology. In this paper, we consider mappings from relational databases to first-order ontologies, and define an ASP-based framework for GLAV mappings with queries over the ontology in the mapping rule bodies. We show that this type of mappings can be used to express constraints and exceptions, as well as being a powerful mechanism for succinctly representing OBDA mappings. We give an algorithm for brave reasoning in this setting, and show that this problem has either the same data complexity as ASP (NP- complete), or it is at least as hard as the complexity of checking entailment for the ontology queries. Furthermore, we show that for ontologies with UCQ-rewritable queries there exists a natural reduction from mapping programs to \\exists-ASP, an extension of ASP with existential variables that itself admits a natural reduction to ASP.\n    ",
        "submission_date": "2016-07-07T00:00:00",
        "last_modified_date": "2016-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02171",
        "title": "Argumentation Models for Cyber Attribution",
        "authors": [
            "Eric Nunes",
            "Paulo Shakarian",
            "Gerardo I. Simari",
            "Andrew Ruef"
        ],
        "abstract": "A major challenge in cyber-threat analysis is combining information from different sources to find the person or the group responsible for the cyber-attack. It is one of the most important technical and policy challenges in cyber-security. The lack of ground truth for an individual responsible for an attack has limited previous studies. In this paper, we take a first step towards overcoming this limitation by building a dataset from the capture-the-flag event held at DEFCON, and propose an argumentation model based on a formal reasoning framework called DeLP (Defeasible Logic Programming) designed to aid an analyst in attributing a cyber-attack. We build models from latent variables to reduce the search space of culprits (attackers), and show that this reduction significantly improves the performance of classification-based approaches from 37% to 62% in identifying the attacker.\n    ",
        "submission_date": "2016-07-07T00:00:00",
        "last_modified_date": "2016-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02431",
        "title": "Learning opening books in partially observable games: using random seeds in Phantom Go",
        "authors": [
            "Tristan Cazenave",
            "Jialin Liu",
            "Fabien Teytaud",
            "Olivier Teytaud"
        ],
        "abstract": "Many artificial intelligences (AIs) are randomized. One can be lucky or unlucky with the random seed; we quantify this effect and show that, maybe contrarily to intuition, this is far from being negligible. Then, we apply two different existing algorithms for selecting good seeds and good probability distributions over seeds. This mainly leads to learning an opening book. We apply this to Phantom Go, which, as all phantom games, is hard for opening book learning. We improve the winning rate from 50% to 70% in 5x5 against the same AI, and from approximately 0% to 40% in 5x5, 7x7 and 9x9 against a stronger (learning) opponent.\n    ",
        "submission_date": "2016-07-08T00:00:00",
        "last_modified_date": "2016-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02436",
        "title": "Document Clustering Games in Static and Dynamic Scenarios",
        "authors": [
            "Rocco Tripodi",
            "Marcello Pelillo"
        ],
        "abstract": "In this work we propose a game theoretic model for document clustering. Each document to be clustered is represented as a player and each cluster as a strategy. The players receive a reward interacting with other players that they try to maximize choosing their best strategies. The geometry of the data is modeled with a weighted graph that encodes the pairwise similarity among documents, so that similar players are constrained to choose similar strategies, updating their strategy preferences at each iteration of the games. We used different approaches to find the prototypical elements of the clusters and with this information we divided the players into two disjoint sets, one collecting players with a definite strategy and the other one collecting players that try to learn from others the correct strategy to play. The latter set of players can be considered as new data points that have to be clustered according to previous information. This representation is useful in scenarios in which the data are streamed continuously. The evaluation of the system was conducted on 13 document datasets using different settings. It shows that the proposed method performs well compared to different document clustering algorithms.\n    ",
        "submission_date": "2016-07-08T00:00:00",
        "last_modified_date": "2016-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02467",
        "title": "Log-Linear RNNs: Towards Recurrent Neural Networks with Flexible Prior Knowledge",
        "authors": [
            "Marc Dymetman",
            "Chunyang Xiao"
        ],
        "abstract": "We introduce LL-RNNs (Log-Linear RNNs), an extension of Recurrent Neural Networks that replaces the softmax output layer by a log-linear output layer, of which the softmax is a special case. This conceptually simple move has two main advantages. First, it allows the learner to combat training data sparsity by allowing it to model words (or more generally, output symbols) as complex combinations of attributes without requiring that each combination is directly observed in the training data (as the softmax does). Second, it permits the inclusion of flexible prior knowledge in the form of a priori specified modular features, where the neural network component learns to dynamically control the weights of a log-linear distribution exploiting these features.\n",
        "submission_date": "2016-07-08T00:00:00",
        "last_modified_date": "2016-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02480",
        "title": "Real-Time Anomaly Detection for Streaming Analytics",
        "authors": [
            "Subutai Ahmad",
            "Scott Purdy"
        ],
        "abstract": "Much of the worlds data is streaming, time-series data, where anomalies give significant information in critical situations. Yet detecting anomalies in streaming data is a difficult task, requiring detectors to process data in real-time, and learn while simultaneously making predictions. We present a novel anomaly detection technique based on an on-line sequence memory algorithm called Hierarchical Temporal Memory (HTM). We show results from a live application that detects anomalies in financial metrics in real-time. We also test the algorithm on NAB, a published benchmark for real-time anomaly detection, where our algorithm achieves best-in-class results.\n    ",
        "submission_date": "2016-07-08T00:00:00",
        "last_modified_date": "2016-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02763",
        "title": "How to Allocate Resources For Features Acquisition?",
        "authors": [
            "Oran Richman",
            "Shie Mannor"
        ],
        "abstract": "We study classification problems where features are corrupted by noise and where the magnitude of the noise in each feature is influenced by the resources allocated to its acquisition. This is the case, for example, when multiple sensors share a common resource (power, bandwidth, attention, etc.). We develop a method for computing the optimal resource allocation for a variety of scenarios and derive theoretical bounds concerning the benefit that may arise by non-uniform allocation. We further demonstrate the effectiveness of the developed method in simulations.\n    ",
        "submission_date": "2016-07-10T00:00:00",
        "last_modified_date": "2016-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.03290",
        "title": "Automatic Bridge Bidding Using Deep Reinforcement Learning",
        "authors": [
            "Chih-Kuan Yeh",
            "Hsuan-Tien Lin"
        ],
        "abstract": "Bridge is among the zero-sum games for which artificial intelligence has not yet outperformed expert human players. The main difficulty lies in the bidding phase of bridge, which requires cooperative decision making under partial information. Existing artificial intelligence systems for bridge bidding rely on and are thus restricted by human-designed bidding systems or features. In this work, we propose a pioneering bridge bidding system without the aid of human domain knowledge. The system is based on a novel deep reinforcement learning model, which extracts sophisticated features and learns to bid automatically based on raw card data. The model includes an upper-confidence-bound algorithm and additional techniques to achieve a balance between exploration and exploitation. Our experiments validate the promising performance of our proposed model. In particular, the model advances from having no knowledge about bidding to achieving superior performance when compared with a champion-winning computer bridge program that implements a human-designed bidding system.\n    ",
        "submission_date": "2016-07-12T00:00:00",
        "last_modified_date": "2016-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.03611",
        "title": "Characterizing Driving Styles with Deep Learning",
        "authors": [
            "Weishan Dong",
            "Jian Li",
            "Renjie Yao",
            "Changsheng Li",
            "Ting Yuan",
            "Lanjun Wang"
        ],
        "abstract": "Characterizing driving styles of human drivers using vehicle sensor data, e.g., GPS, is an interesting research problem and an important real-world requirement from automotive industries. A good representation of driving features can be highly valuable for autonomous driving, auto insurance, and many other application scenarios. However, traditional methods mainly rely on handcrafted features, which limit machine learning algorithms to achieve a better performance. In this paper, we propose a novel deep learning solution to this problem, which could be the first attempt of extending deep learning to driving behavior analysis based on GPS data. The proposed approach can effectively extract high level and interpretable features describing complex driving patterns. It also requires significantly less human experience and work. The power of the learned driving style representations are validated through the driver identification problem using a large real dataset.\n    ",
        "submission_date": "2016-07-13T00:00:00",
        "last_modified_date": "2016-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.03705",
        "title": "Possibilistic Networks: Parameters Learning from Imprecise Data and Evaluation strategy",
        "authors": [
            "Maroua Haddad",
            "Philippe Leray",
            "Nahla Ben Amor"
        ],
        "abstract": "There has been an ever-increasing interest in multidisciplinary research on representing and reasoning with imperfect data. Possibilistic networks present one of the powerful frameworks of interest for representing uncertain and imprecise information. This paper covers the problem of their parameters learning from imprecise datasets, i.e., containing multi-valued data. We propose in the rst part of this paper a possibilistic networks sampling process. In the second part, we propose a likelihood function which explores the link between random sets theory and possibility theory. This function is then deployed to parametrize possibilistic networks.\n    ",
        "submission_date": "2016-07-13T00:00:00",
        "last_modified_date": "2016-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.03979",
        "title": "Resource Planning For Rescue Operations",
        "authors": [
            "Mona Khaffaf",
            "Arshia Khaffaf"
        ],
        "abstract": "After an earthquake, disaster sites pose a multitude of health and safety concerns. A rescue operation of people trapped in the ruins after an earthquake disaster requires a series of intelligent behavior, including planning. For a successful rescue operation, given a limited number of available actions and regulations, the role of planning in rescue operations is crucial. Fortunately, recent developments in automated planning by artificial intelligence community can help different organization in this crucial task. Due to the number of rules and regulations, we believe that a rule based system for planning can be helpful for this specific planning problem. In this research work, we use logic rules to represent rescue and related regular regulations, together with a logic based planner to solve this complicated problem. Although this research is still in the prototyping and modeling stage, it clearly shows that rule based languages can be a good infrastructure for this computational task. The results of this research can be used by different organizations, such as Iranian Red Crescent Society and International Institute of Seismology and Earthquake Engineering (IISEE).\n    ",
        "submission_date": "2016-07-14T00:00:00",
        "last_modified_date": "2016-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.04186",
        "title": "Large-scale Analysis of Chess Games with Chess Engines: A Preliminary Report",
        "authors": [
            "Mathieu Acher",
            "Fran\u00e7ois Esnault"
        ],
        "abstract": "The strength of chess engines together with the availability of numerous chess games have attracted the attention of chess players, data scientists, and researchers during the last decades. State-of-the-art engines now provide an authoritative judgement that can be used in many applications like cheating detection, intrinsic ratings computation, skill assessment, or the study of human decision-making. A key issue for the research community is to gather a large dataset of chess games together with the judgement of chess engines. Unfortunately the analysis of each move takes lots of times. In this paper, we report our effort to analyse almost 5 millions chess games with a computing grid. During summer 2015, we processed 270 millions unique played positions using the Stockfish engine with a quite high depth (20). We populated a database of 1+ tera-octets of chess evaluations, representing an estimated time of 50 years of computation on a single machine. Our effort is a first step towards the replication of research results, the supply of open data and procedures for exploring new directions, and the investigation of software engineering/scalability issues when computing billions of moves.\n    ",
        "submission_date": "2016-04-28T00:00:00",
        "last_modified_date": "2016-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.04379",
        "title": "DeepQA: Improving the estimation of single protein model quality with deep belief networks",
        "authors": [
            "Renzhi Cao",
            "Debswapna Bhattacharya",
            "Jie Hou",
            "Jianlin Cheng"
        ],
        "abstract": "Protein quality assessment (QA) by ranking and selecting protein models has long been viewed as one of the major challenges for protein tertiary structure prediction. Especially, estimating the quality of a single protein model, which is important for selecting a few good models out of a large model pool consisting of mostly low-quality models, is still a largely unsolved problem. We introduce a novel single-model quality assessment method DeepQA based on deep belief network that utilizes a number of selected features describing the quality of a model from different perspectives, such as energy, physio-chemical characteristics, and structural information. The deep belief network is trained on several large datasets consisting of models from the Critical Assessment of Protein Structure Prediction (CASP) experiments, several publicly available datasets, and models generated by our in-house ab initio method. Our experiment demonstrate that deep belief network has better performance compared to Support Vector Machines and Neural Networks on the protein model quality assessment problem, and our method DeepQA achieves the state-of-the-art performance on CASP11 dataset. It also outperformed two well-established methods in selecting good outlier models from a large set of models of mostly low quality generated by ab initio modeling methods. DeepQA is a useful tool for protein single model quality assessment and protein structure prediction. The source code, executable, document and training/test datasets of DeepQA for Linux is freely available to non-commercial users at ",
        "submission_date": "2016-07-15T00:00:00",
        "last_modified_date": "2016-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.04583",
        "title": "A Counterexample to the Forward Recursion in Fuzzy Critical Path Analysis Under Discrete Fuzzy Sets",
        "authors": [
            "Matthew J. Liberatore"
        ],
        "abstract": "Fuzzy logic is an alternate approach for quantifying uncertainty relating to activity duration. The fuzzy version of the backward recursion has been shown to produce results that incorrectly amplify the level of uncertainty. However, the fuzzy version of the forward recursion has been widely proposed as an approach for determining the fuzzy set of critical path lengths. In this paper, the direct application of the extension principle leads to a proposition that must be satisfied in fuzzy critical path analysis. Using a counterexample it is demonstrated that the fuzzy forward recursion when discrete fuzzy sets are used to represent activity durations produces results that are not consistent with the theory presented. The problem is shown to be the application of the fuzzy maximum. Several methods presented in the literature are described and shown to provide results that are consistent with the extension principle.\n    ",
        "submission_date": "2016-05-09T00:00:00",
        "last_modified_date": "2016-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.04809",
        "title": "Knowledge Representation on the Web revisited: Tools for Prototype Based Ontologies",
        "authors": [
            "Michael Cochez",
            "Stefan Decker",
            "Eric Prud'hommeaux"
        ],
        "abstract": "In recent years RDF and OWL have become the most common knowledge representation languages in use on the Web, propelled by the recommendation of the W3C. In this paper we present a practical implementation of a different kind of knowledge representation based on Prototypes. In detail, we present a concrete syntax easily and effectively parsable by applications. We also present extensible implementations of a prototype knowledge base, specifically designed for storage of Prototypes. These implementations are written in Java and can be extended by using the implementation as a library. Alternatively, the software can be deployed as such. Further, results of benchmarks for both local and web deployment are presented. This paper augments a research paper, in which we describe the more theoretical aspects of our Prototype system.\n    ",
        "submission_date": "2016-07-16T00:00:00",
        "last_modified_date": "2016-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05023",
        "title": "Intelligent Biohybrid Neurotechnologies: Are They Really What They Claim?",
        "authors": [
            "Gabriella Panuccio",
            "Marianna Semprini",
            "Lorenzo Natale",
            "Michela Chiappalone"
        ],
        "abstract": "In the era of intelligent biohybrid neurotechnologies for brain repair, new fanciful terms are appearing in the scientific dictionary to define what has so far been unimaginable. As the emerging neurotechnologies are becoming increasingly polyhedral and sophisticated, should we talk about evolution and rank the intelligence of these devices?\n    ",
        "submission_date": "2016-07-18T00:00:00",
        "last_modified_date": "2016-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05077",
        "title": "Playing Atari Games with Deep Reinforcement Learning and Human Checkpoint Replay",
        "authors": [
            "Ionel-Alexandru Hosu",
            "Traian Rebedea"
        ],
        "abstract": "This paper introduces a novel method for learning how to play the most difficult Atari 2600 games from the Arcade Learning Environment using deep reinforcement learning. The proposed method, human checkpoint replay, consists in using checkpoints sampled from human gameplay as starting points for the learning process. This is meant to compensate for the difficulties of current exploration strategies, such as epsilon-greedy, to find successful control policies in games with sparse rewards. Like other deep reinforcement learning architectures, our model uses a convolutional neural network that receives only raw pixel inputs to estimate the state value function. We tested our method on Montezuma's Revenge and Private Eye, two of the most challenging games from the Atari platform. The results we obtained show a substantial improvement compared to previous learning approaches, as well as over a random player. We also propose a method for training deep reinforcement learning agents using human gameplay experience, which we call human experience replay.\n    ",
        "submission_date": "2016-07-18T00:00:00",
        "last_modified_date": "2016-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05351",
        "title": "Towards Analytics Aware Ontology Based Access to Static and Streaming Data (Extended Version)",
        "authors": [
            "Evgeny Kharlamov",
            "Yannis Kotidis",
            "Theofilos Mailis",
            "Christian Neuenstadt",
            "Charalampos Nikolaou",
            "\u00d6zg\u00fcr \u00d6zcep",
            "Christoforos Svingos",
            "Dmitriy Zheleznyakov",
            "Sebastian Brandt",
            "Ian Horrocks",
            "Yannis Ioannidis",
            "Steffen Lamparter",
            "Ralf M\u00f6ller"
        ],
        "abstract": "Real-time analytics that requires integration and aggregation of heterogeneous and distributed streaming and static data is a typical task in many industrial scenarios such as diagnostics of turbines in Siemens. OBDA approach has a great potential to facilitate such tasks; however, it has a number of limitations in dealing with analytics that restrict its use in important industrial applications. Based on our experience with Siemens, we argue that in order to overcome those limitations OBDA should be extended and become analytics, source, and cost aware. In this work we propose such an extension. In particular, we propose an ontology, mapping, and query language for OBDA, where aggregate and other analytical functions are first class citizens. Moreover, we develop query optimisation techniques that allow to efficiently process analytical tasks over static and streaming data. We implement our approach in a system and evaluate our system with Siemens turbine data.\n    ",
        "submission_date": "2016-07-18T00:00:00",
        "last_modified_date": "2016-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05387",
        "title": "Generating Images Part by Part with Composite Generative Adversarial Networks",
        "authors": [
            "Hanock Kwak",
            "Byoung-Tak Zhang"
        ],
        "abstract": "Image generation remains a fundamental problem in artificial intelligence in general and deep learning in specific. The generative adversarial network (GAN) was successful in generating high quality samples of natural images. We propose a model called composite generative adversarial network, that reveals the complex structure of images with multiple generators in which each generator generates some part of the image. Those parts are combined by alpha blending process to create a new single image. It can generate, for example, background and face sequentially with two generators, after training on face dataset. Training was done in an unsupervised way without any labels about what each generator should generate. We found possibilities of learning the structure by using this generative model empirically.\n    ",
        "submission_date": "2016-07-19T00:00:00",
        "last_modified_date": "2016-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05601",
        "title": "An Event Grouping Based Algorithm for University Course Timetabling Problem",
        "authors": [
            "Velin Kralev",
            "Radoslava Kraleva",
            "Borislav Yurukov"
        ],
        "abstract": "This paper presents the study of an event grouping based algorithm for a university course timetabling problem. Several publications which discuss the problem and some approaches for its solution are analyzed. The grouping of events in groups with an equal number of events in each group is not applicable to all input data sets. For this reason, a universal approach to all possible groupings of events in commensurate in size groups is proposed here. Also, an implementation of an algorithm based on this approach is presented. The methodology, conditions and the objectives of the experiment are described. The experimental results are analyzed and the ensuing conclusions are stated. The future guidelines for further research are formulated.\n    ",
        "submission_date": "2016-07-18T00:00:00",
        "last_modified_date": "2016-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05810",
        "title": "You want to survive the data deluge: Be careful, Computational Intelligence will not serve you as a rescue boat",
        "authors": [
            "Emanuel Diamant"
        ],
        "abstract": "We are at the dawn of a new era, where advances in computer power, broadband communication and digital sensor technologies have led to an unprecedented flood of data inundating our surrounding. It is generally believed that means such as Computational Intelligence will help to outlive these tough times. However, these hopes are improperly high. Computational Intelligence is a surprising composition of two mutually exclusive and contradicting constituents that could be coupled only if you disregard and neglect their controversies: \"Computational\" implies reliance on data processing and \"Intelligence\" implies reliance on information processing. Only those who are indifferent to data-information discrepancy can believe that such a combination can be viable. We do not believe in miracles, so we will try to share with you our reservations.\n    ",
        "submission_date": "2016-07-20T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05845",
        "title": "Identifying Candidate Risk Factors for Prescription Drug Side Effects using Causal Contrast Set Mining",
        "authors": [
            "Jenna Reps",
            "Zhaoyang Guo",
            "Haoyue Zhu",
            "Uwe Aickelin"
        ],
        "abstract": "Big longitudinal observational databases present the opportunity to extract new knowledge in a cost effective manner. Unfortunately, the ability of these databases to be used for causal inference is limited due to the passive way in which the data are collected resulting in various forms of bias. In this paper we investigate a method that can overcome these limitations and determine causal contrast set rules efficiently from big data. In particular, we present a new methodology for the purpose of identifying risk factors that increase a patients likelihood of experiencing the known rare side effect of renal failure after ingesting aminosalicylates. The results show that the methodology was able to identify previously researched risk factors such as being prescribed diuretics and highlighted that patients with a higher than average risk of renal failure may be even more susceptible to experiencing it as a side effect after ingesting aminosalicylates.\n    ",
        "submission_date": "2016-07-20T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05869",
        "title": "Indebted households profiling: a knowledge discovery from database approach",
        "authors": [
            "Rodrigo Scarpel",
            "Alexandros Ladas",
            "Uwe Aickelin"
        ],
        "abstract": "A major challenge in consumer credit risk portfolio management is to classify households according to their risk profile. In order to build such risk profiles it is necessary to employ an approach that analyses data systematically in order to detect important relationships, interactions, dependencies and associations amongst the available continuous and categorical variables altogether and accurately generate profiles of most interesting household segments according to their credit risk. The objective of this work is to employ a knowledge discovery from database process to identify groups of indebted households and describe their profiles using a database collected by the Consumer Credit Counselling Service (CCCS) in the UK. Employing a framework that allows the usage of both categorical and continuous data altogether to find hidden structures in unlabelled data it was established the ideal number of clusters and such clusters were described in order to identify the households who exhibit a high propensity of excessive debt levels.\n    ",
        "submission_date": "2016-07-20T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05888",
        "title": "Juxtaposition of System Dynamics and Agent-based Simulation for a Case Study in Immunosenescence",
        "authors": [
            "Grazziela P. Figueredo",
            "Peer-Olaf Siebers",
            "Uwe Aickelin",
            "Amanda Whitbrook",
            "Jonathan M. Garibaldi"
        ],
        "abstract": "Advances in healthcare and in the quality of life significantly increase human life expectancy. With the ageing of populations, new un-faced challenges are brought to science. The human body is naturally selected to be well-functioning until the age of reproduction to keep the species alive. However, as the lifespan extends, unseen problems due to the body deterioration emerge. There are several age-related diseases with no appropriate treatment; therefore, the complex ageing phenomena needs further understanding. Immunosenescence, the ageing of the immune system, is highly correlated to the negative effects of ageing, such as the increase of auto-inflammatory diseases and decrease in responsiveness to new diseases. Besides clinical and mathematical tools, we believe there is opportunity to further exploit simulation tools to understand immunosenescence. Compared to real-world experimentation, benefits include time and cost effectiveness due to the laborious, resource-intensiveness of the biological environment and the possibility of conducting experiments without ethic restrictions. Contrasted with mathematical models, simulation modelling is more suitable for representing complex systems and emergence. In addition, there is the belief that simulation models are easier to communicate in interdisciplinary contexts. Our work investigates the usefulness of simulations to understand immunosenescence by employing two different simulation methods, agent-based and system dynamics simulation, to a case study of immune cells depletion with age.\n    ",
        "submission_date": "2016-07-20T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05906",
        "title": "Refining adverse drug reaction signals by incorporating interaction variables identified using emergent pattern mining",
        "authors": [
            "Jenna M. Reps",
            "Uwe Aickelin",
            "Richard B. Hubbard"
        ],
        "abstract": "Purpose: To develop a framework for identifying and incorporating candidate confounding interaction terms into a regularised cox regression analysis to refine adverse drug reaction signals obtained via longitudinal observational data. Methods: We considered six drug families that are commonly associated with myocardial infarction in observational healthcare data, but where the causal relationship ground truth is known (adverse drug reaction or not). We applied emergent pattern mining to find itemsets of drugs and medical events that are associated with the development of myocardial infarction. These are the candidate confounding interaction terms. We then implemented a cohort study design using regularised cox regression that incorporated and accounted for the candidate confounding interaction terms. Results The methodology was able to account for signals generated due to confounding and a cox regression with elastic net regularisation correctly ranked the drug families known to be true adverse drug reactions above those.\n    ",
        "submission_date": "2016-07-20T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05909",
        "title": "Supervised Anomaly Detection in Uncertain Pseudoperiodic Data Streams",
        "authors": [
            "Jiangang Ma",
            "Le Sun",
            "Hua Wang",
            "Yanchun Zhang",
            "Uwe Aickelin"
        ],
        "abstract": "Uncertain data streams have been widely generated in many Web applications. The uncertainty in data streams makes anomaly detection from sensor data streams far more challenging. In this paper, we present a novel framework that supports anomaly detection in uncertain data streams. The proposed framework adopts an efficient uncertainty pre-processing procedure to identify and eliminate uncertainties in data streams. Based on the corrected data streams, we develop effective period pattern recognition and feature extraction techniques to improve the computational efficiency. We use classification methods for anomaly detection in the corrected data stream. We also empirically show that the proposed approach shows a high accuracy of anomaly detection on a number of real datasets.\n    ",
        "submission_date": "2016-07-20T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05912",
        "title": "Simulating user learning in authoritative technology adoption: An agent based model for council-led smart meter deployment planning in the UK",
        "authors": [
            "Tao Zhang",
            "Peer-Olaf Siebers",
            "Uwe Aickelin"
        ],
        "abstract": "How do technology users effectively transit from having zero knowledge about a technology to making the best use of it after an authoritative technology adoption? This post-adoption user learning has received little research attention in technology management literature. In this paper we investigate user learning in authoritative technology adoption by developing an agent-based model using the case of council-led smart meter deployment in the UK City of Leeds. Energy consumers gain experience of using smart meters based on the learning curve in behavioural learning. With the agent-based model we carry out experiments to validate the model and test different energy interventions that local authorities can use to facilitate energy consumers' learning and maintain their continuous use of the technology. Our results show that the easier energy consumers become experienced, the more energy-efficient they are and the more energy saving they can achieve; encouraging energy consumers' contacts via various informational means can facilitate their learning; and developing and maintaining their positive attitude toward smart metering can enable them to use the technology continuously. Contributions and energy policy/intervention implications are discussed in this paper.\n    ",
        "submission_date": "2016-07-20T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05913",
        "title": "Optimising Rule-Based Classification in Temporal Data",
        "authors": [
            "Polla Fattah",
            "Uwe Aickelin",
            "Christian Wagner"
        ],
        "abstract": "This study optimises manually derived rule-based expert system classification of objects according to changes in their properties over time. One of the key challenges that this study tries to address is how to classify objects that exhibit changes in their behaviour over time, for example how to classify companies' share price stability over a period of time or how to classify students' preferences for subjects while they are progressing through school. A specific case the paper considers is the strategy of players in public goods games (as common in economics) across multiple consecutive games. Initial classification starts from expert definitions specifying class allocation for players based on aggregated attributes of the temporal data. Based on these initial classifications, the optimisation process tries to find an improved classifier which produces the best possible compact classes of objects (players) for every time point in the temporal data. The compactness of the classes is measured by a cost function based on internal cluster indices like the Dunn Index, distance measures like Euclidean distance or statistically derived measures like standard deviation. The paper discusses the approach in the context of incorporating changing player strategies in the aforementioned public good games, where common classification approaches so far do not consider such changes in behaviour resulting from learning or in-game experience. By using the proposed process for classifying temporal data and the actual players' contribution during the games, we aim to produce a more refined classification which in turn may inform the interpretation of public goods game data.\n    ",
        "submission_date": "2016-07-20T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05968",
        "title": "Robust Natural Language Processing - Combining Reasoning, Cognitive Semantics and Construction Grammar for Spatial Language",
        "authors": [
            "Michael Spranger",
            "Jakob Suchan",
            "Mehul Bhatt"
        ],
        "abstract": "We present a system for generating and understanding of dynamic and static spatial relations in robotic interaction setups. Robots describe an environment of moving blocks using English phrases that include spatial relations such as \"across\" and \"in front of\". We evaluate the system in robot-robot interactions and show that the system can robustly deal with visual perception errors, language omissions and ungrammatical utterances.\n    ",
        "submission_date": "2016-07-20T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06025",
        "title": "Constructing a Natural Language Inference Dataset using Generative Neural Networks",
        "authors": [
            "Janez Starc",
            "Dunja Mladeni\u0107"
        ],
        "abstract": "Natural Language Inference is an important task for Natural Language Understanding. It is concerned with classifying the logical relation between two sentences. In this paper, we propose several text generative neural networks for generating text hypothesis, which allows construction of new Natural Language Inference datasets. To evaluate the models, we propose a new metric -- the accuracy of the classifier trained on the generated dataset. The accuracy obtained by our best generative model is only 2.7% lower than the accuracy of the classifier trained on the original, human crafted dataset. Furthermore, the best generated dataset combined with the original dataset achieves the highest accuracy. The best model learns a mapping embedding for each training example. By comparing various metrics we show that datasets that obtain higher ROUGE or METEOR scores do not necessarily yield higher classification accuracies. We also provide analysis of what are the characteristics of a good dataset including the distinguishability of the generated datasets from the original one.\n    ",
        "submission_date": "2016-07-20T00:00:00",
        "last_modified_date": "2017-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06186",
        "title": "Applying Interval Type-2 Fuzzy Rule Based Classifiers Through a Cluster-Based Class Representation",
        "authors": [
            "Javier Navarro",
            "Christian Wagner",
            "Uwe Aickelin"
        ],
        "abstract": "Fuzzy Rule-Based Classification Systems (FRBCSs) have the potential to provide so-called interpretable classifiers, i.e. classifiers which can be introspective, understood, validated and augmented by human experts by relying on fuzzy-set based rules. This paper builds on prior work for interval type-2 fuzzy set based FRBCs where the fuzzy sets and rules of the classifier are generated using an initial clustering stage. By introducing Subtractive Clustering in order to identify multiple cluster prototypes, the proposed approach has the potential to deliver improved classification performance while maintaining good interpretability, i.e. without resulting in an excessive number of rules. The paper provides a detailed overview of the proposed FRBC framework, followed by a series of exploratory experiments on both linearly and non-linearly separable datasets, comparing results to existing rule-based and SVM approaches. Overall, initial results indicate that the approach enables comparable classification performance to non rule-based classifiers such as SVM, while often achieving this with a very small number of rules.\n    ",
        "submission_date": "2016-07-21T00:00:00",
        "last_modified_date": "2016-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06187",
        "title": "Exploring Differences in Interpretation of Words Essential in Medical Expert-Patient Communication",
        "authors": [
            "Javier Navarro",
            "Christian Wagner",
            "Uwe Aickelin",
            "Lynsey Green",
            "Robert Ashford"
        ],
        "abstract": "In the context of cancer treatment and surgery, quality of life assessment is a crucial part of determining treatment success and viability. In order to assess it, patients completed questionnaires which employ words to capture aspects of patients well-being are the norm. As the results of these questionnaires are often used to assess patient progress and to determine future treatment options, it is important to establish that the words used are interpreted in the same way by both patients and medical professionals. In this paper, we capture and model patients perceptions and associated uncertainty about the words used to describe the level of their physical function used in the highly common (in Sarcoma Services) Toronto Extremity Salvage Score (TESS) questionnaire. The paper provides detail about the interval-valued data capture as well as the subsequent modelling of the data using fuzzy sets. Based on an initial sample of participants, we use Jaccard similarity on the resulting words models to show that there may be considerable differences in the interpretation of commonly used questionnaire terms, thus presenting a very real risk of miscommunication between patients and medical professionals as well as within the group of medical professionals.\n    ",
        "submission_date": "2016-07-21T00:00:00",
        "last_modified_date": "2016-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06198",
        "title": "Supervised Adverse Drug Reaction Signalling Framework Imitating Bradford Hill's Causality Considerations",
        "authors": [
            "Jenna Marie Reps",
            "Jonathan M. Garibaldi",
            "Uwe Aickelin",
            "Jack E. Gibson",
            "Richard B.Hubbard"
        ],
        "abstract": "Big longitudinal observational medical data potentially hold a wealth of information and have been recognised as potential sources for gaining new drug safety knowledge. Unfortunately there are many complexities and underlying issues when analysing longitudinal observational data. Due to these complexities, existing methods for large-scale detection of negative side effects using observational data all tend to have issues distinguishing between association and causality. New methods that can better discriminate causal and non-causal relationships need to be developed to fully utilise the data. In this paper we propose using a set of causality considerations developed by the epidemiologist Bradford Hill as a basis for engineering features that enable the application of supervised learning for the problem of detecting negative side effects. The Bradford Hill considerations look at various perspectives of a drug and outcome relationship to determine whether it shows causal traits. We taught a classifier to find patterns within these perspectives and it learned to discriminate between association and causality. The novelty of this research is the combination of supervised learning and Bradford Hill's causality considerations to automate the Bradford Hill's causality assessment. We evaluated the framework on a drug safety gold standard know as the observational medical outcomes partnership's nonspecified association reference set. The methodology obtained excellent discriminate ability with area under the curves ranging between 0.792-0.940 (existing method optimal: 0.73) and a mean average precision of 0.640 (existing method optimal: 0.141). The proposed features can be calculated efficiently and be readily updated, making the framework suitable for big observational data.\n    ",
        "submission_date": "2016-07-21T00:00:00",
        "last_modified_date": "2016-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06332",
        "title": "Modelling Office Energy Consumption: An Agent Based Approach",
        "authors": [
            "Tao Zhang",
            "Peer-Olaf Siebers",
            "Uwe Aickelin"
        ],
        "abstract": "In this paper, we develop an agent-based model which integrates four important elements, i.e. organisational energy management policies/regulations, energy management technologies, electric appliances and equipment, and human behaviour, based on a case study, to simulate the energy consumption in office buildings. With the model, we test the effectiveness of different energy management strategies, and solve practical office energy consumption problems. This paper theoretically contributes to an integration of four elements involved in the complex organisational issue of office energy consumption, and practically contributes to an application of agent-based approach for office building energy consumption study.\n    ",
        "submission_date": "2016-07-20T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06617",
        "title": "Latent Variable Discovery Using Dependency Patterns",
        "authors": [
            "Xuhui Zhang",
            "Kevin B. Korb",
            "Ann E. Nicholson",
            "Steven Mascaro"
        ],
        "abstract": "The causal discovery of Bayesian networks is an active and important research area, and it is based upon searching the space of causal models for those which can best explain a pattern of probabilistic dependencies shown in the data. However, some of those dependencies are generated by causal structures involving variables which have not been measured, i.e., latent variables. Some such patterns of dependency \"reveal\" themselves, in that no model based solely upon the observed variables can explain them as well as a model using a latent variable. That is what latent variable discovery is based upon. Here we did a search for finding them systematically, so that they may be applied in latent variable discovery in a more rigorous fashion.\n    ",
        "submission_date": "2016-07-22T00:00:00",
        "last_modified_date": "2016-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06759",
        "title": "Predicting Enemy's Actions Improves Commander Decision-Making",
        "authors": [
            "Michael Ownby",
            "Alexander Kott"
        ],
        "abstract": "The Defense Advanced Research Projects Agency (DARPA) Real-time Adversarial Intelligence and Decision-making (RAID) program is investigating the feasibility of \"reading the mind of the enemy\" - to estimate and anticipate, in real-time, the enemy's likely goals, deceptions, actions, movements and positions. This program focuses specifically on urban battles at echelons of battalion and below. The RAID program leverages approximate game-theoretic and deception-sensitive algorithms to provide real-time enemy estimates to a tactical commander. A key hypothesis of the program is that these predictions and recommendations will make the commander more effective, i.e. he should be able to achieve his operational goals safer, faster, and more efficiently. Realistic experimentation and evaluation drive the development process using human-in-the-loop wargames to compare humans and the RAID system. Two experiments were conducted in 2005 as part of Phase I to determine if the RAID software could make predictions and recommendations as effectively and accurately as a 4-person experienced staff. This report discusses the intriguing and encouraging results of these first two experiments conducted by the RAID program. It also provides details about the experiment environment and methodology that were used to demonstrate and prove the research goals.\n    ",
        "submission_date": "2016-07-22T00:00:00",
        "last_modified_date": "2016-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06875",
        "title": "Processing Natural Language About Ongoing Actions",
        "authors": [
            "Steve Doubleday",
            "Sean Trott",
            "Jerome Feldman"
        ],
        "abstract": "Actions may not proceed as planned; they may be interrupted, resumed or overridden. This is a challenge to handle in a natural language understanding system. We describe extensions to an existing implementation for the control of autonomous systems by natural language, to enable such systems to handle incoming language requests regarding actions. Language Communication with Autonomous Systems (LCAS) has been extended with support for X-nets, parameterized executable schemas representing actions. X-nets enable the system to control actions at a desired level of granularity, while providing a mechanism for language requests to be processed asynchronously. Standard semantics supported include requests to stop, continue, or override the existing action. The specific domain demonstrated is the control of motion of a simulated robot, but the approach is general, and could be applied to other domains.\n    ",
        "submission_date": "2016-07-23T00:00:00",
        "last_modified_date": "2016-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07027",
        "title": "Redundancy-free Verbalization of Individuals for Ontology Validation",
        "authors": [
            "E. V. Vinu",
            "P Sreenivasa Kumar"
        ],
        "abstract": "We investigate the problem of verbalizing Web Ontology Language (OWL) axioms of domain ontologies in this paper. The existing approaches address the problem of fidelity of verbalized OWL texts to OWL semantics by exploring different ways of expressing the same OWL axiom in various linguistic forms. They also perform grouping and aggregating of the natural language (NL) sentences that are generated corresponding to each OWL statement into a comprehensible structure. However, no efforts have been taken to try out a semantic reduction at logical level to remove redundancies and repetitions, so that the reduced set of axioms can be used for generating a more meaningful and human-understandable (what we call redundancy-free) text. Our experiments show that, formal semantic reduction at logical level is very helpful to generate redundancy-free descriptions of ontology entities. In this paper, we particularly focus on generating descriptions of individuals of SHIQ based ontologies. The details of a case study are provided to support the usefulness of the redundancy-free NL descriptions of individuals, in knowledge validation application.\n    ",
        "submission_date": "2016-07-24T00:00:00",
        "last_modified_date": "2016-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07249",
        "title": "An Evolutionary Algorithm to Learn SPARQL Queries for Source-Target-Pairs: Finding Patterns for Human Associations in DBpedia",
        "authors": [
            "J\u00f6rn Hees",
            "Rouven Bauer",
            "Joachim Folz",
            "Damian Borth",
            "Andreas Dengel"
        ],
        "abstract": "Efficient usage of the knowledge provided by the Linked Data community is often hindered by the need for domain experts to formulate the right SPARQL queries to answer questions. For new questions they have to decide which datasets are suitable and in which terminology and modelling style to phrase the SPARQL query.\n",
        "submission_date": "2016-07-25T00:00:00",
        "last_modified_date": "2016-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07288",
        "title": "Validation of Information Fusion",
        "authors": [
            "Alexander Kott",
            "Wes Milks"
        ],
        "abstract": "We motivate and offer a formal definition of validation as it applies to information fusion systems. Common definitions of validation compare the actual state of the world with that derived by the fusion process. This definition conflates properties of the fusion system with properties of systems that intervene between the world and the fusion system. We propose an alternative definition where validation of an information fusion system references a standard fusion device, such as recognized human experts. We illustrate the approach by describing the validation process implemented in RAID, a program conducted by DARPA and focused on information fusion in adversarial, deceptive environments.\n    ",
        "submission_date": "2016-07-22T00:00:00",
        "last_modified_date": "2016-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07311",
        "title": "Estimating Activity at Multiple Scales using Spatial Abstractions",
        "authors": [
            "Majd Hawasly",
            "Florian T. Pokorny",
            "Subramanian Ramamoorthy"
        ],
        "abstract": "Autonomous robots operating in dynamic environments must maintain beliefs over a hypothesis space that is rich enough to represent the activities of interest at different scales. This is important both in order to accommodate the availability of evidence at varying degrees of coarseness, such as when interpreting and assimilating natural instructions, but also in order to make subsequent reactive planning more efficient. We present an algorithm that combines a topology-based trajectory clustering procedure that generates hierarchically-structured spatial abstractions with a bank of particle filters at each of these abstraction levels so as to produce probability estimates over an agent's navigation activity that is kept consistent across the hierarchy. We study the performance of the proposed method using a synthetic trajectory dataset in 2D, as well as a dataset taken from AIS-based tracking of ships in an extended harbour area. We show that, in comparison to a baseline which is a particle filter that estimates activity without exploiting such structure, our method achieves a better normalised error in predicting the trajectory as well as better time to convergence to a true class when compared against ground truth.\n    ",
        "submission_date": "2016-07-25T00:00:00",
        "last_modified_date": "2016-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07730",
        "title": "A Model of Pathways to Artificial Superintelligence Catastrophe for Risk and Decision Analysis",
        "authors": [
            "Anthony M. Barrett",
            "Seth D. Baum"
        ],
        "abstract": "An artificial superintelligence (ASI) is artificial intelligence that is significantly more intelligent than humans in all respects. While ASI does not currently exist, some scholars propose that it could be created sometime in the future, and furthermore that its creation could cause a severe global catastrophe, possibly even resulting in human extinction. Given the high stakes, it is important to analyze ASI risk and factor the risk into decisions related to ASI research and development. This paper presents a graphical model of major pathways to ASI catastrophe, focusing on ASI created via recursive self-improvement. The model uses the established risk and decision analysis modeling paradigms of fault trees and influence diagrams in order to depict combinations of events and conditions that could lead to AI catastrophe, as well as intervention options that could decrease risks. The events and conditions include select aspects of the ASI itself as well as the human process of ASI research, development, and management. Model structure is derived from published literature on ASI risk. The model offers a foundation for rigorous quantitative evaluation and decision making on the long-term risk of ASI catastrophe.\n    ",
        "submission_date": "2016-07-25T00:00:00",
        "last_modified_date": "2016-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07745",
        "title": "Leveraging Unstructured Data to Detect Emerging Reliability Issues",
        "authors": [
            "Deovrat Kakde",
            "Arin Chaudhuri"
        ],
        "abstract": "Unstructured data refers to information that does not have a predefined data model or is not organized in a pre-defined manner. Loosely speaking, unstructured data refers to text data that is generated by humans. In after-sales service businesses, there are two main sources of unstructured data: customer complaints, which generally describe symptoms, and technician comments, which outline diagnostics and treatment information. A legitimate customer complaint can eventually be tracked to a failure or a claim. However, there is a delay between the time of a customer complaint and the time of a failure or a claim. A proactive strategy aimed at analyzing customer complaints for symptoms can help service providers detect reliability problems in advance and initiate corrective actions such as recalls. This paper introduces essential text mining concepts in the context of reliability analysis and a method to detect emerging reliability issues. The application of the method is illustrated using a case study.\n    ",
        "submission_date": "2016-07-26T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07762",
        "title": "Focused Model-Learning and Planning for Non-Gaussian Continuous State-Action Systems",
        "authors": [
            "Zi Wang",
            "Stefanie Jegelka",
            "Leslie Pack Kaelbling",
            "Tom\u00e1s Lozano-P\u00e9rez"
        ],
        "abstract": "We introduce a framework for model learning and planning in stochastic domains with continuous state and action spaces and non-Gaussian transition models. It is efficient because (1) local models are estimated only when the planner requires them; (2) the planner focuses on the most relevant states to the current planning problem; and (3) the planner focuses on the most informative and/or high-value actions. Our theoretical analysis shows the validity and asymptotic optimality of the proposed approach. Empirically, we demonstrate the effectiveness of our algorithm on a simulated multi-modal pushing problem.\n    ",
        "submission_date": "2016-07-26T00:00:00",
        "last_modified_date": "2016-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07847",
        "title": "Technical Report: Giving Hints for Logic Programming Examples without Revealing Solutions",
        "authors": [
            "Gokhan Avci",
            "Mustafa Mehuljic",
            "Peter Sch\u00fcller"
        ],
        "abstract": "We introduce a framework for supporting learning to program in the paradigm of Answer Set Programming (ASP), which is a declarative logic programming formalism. Based on the idea of teaching by asking the student to complete small example ASP programs, we introduce a three-stage method for giving hints to the student without revealing the correct solution of an example. We categorize mistakes into (i) syntactic mistakes, (ii) unexpected but syntactically correct input, and (iii) semantic mistakes, describe mathematical definitions of these mistakes, and show how to compute hints from these definitions.\n    ",
        "submission_date": "2016-07-26T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07942",
        "title": "Multiple scan data association by convex variational inference",
        "authors": [
            "Jason L. Williams",
            "Roslyn A. Lau"
        ],
        "abstract": "Data association, the reasoning over correspondence between targets and measurements, is a problem of fundamental importance in target tracking. Recently, belief propagation (BP) has emerged as a promising method for estimating the marginal probabilities of measurement to target association, providing fast, accurate estimates. The excellent performance of BP in the particular formulation used may be attributed to the convexity of the underlying free energy which it implicitly optimises. This paper studies multiple scan data association problems, i.e., problems that reason over correspondence between targets and several sets of measurements, which may correspond to different sensors or different time steps. We find that the multiple scan extension of the single scan BP formulation is non-convex and demonstrate the undesirable behaviour that can result. A convex free energy is constructed using the recently proposed fractional free energy (FFE). A convergent, BP-like algorithm is provided for the single scan FFE, and employed in optimising the multiple scan free energy using primal-dual coordinate ascent. Finally, based on a variational interpretation of joint probabilistic data association (JPDA), we develop a sequential variant of the algorithm that is similar to JPDA, but retains consistency constraints from prior scans. The performance of the proposed methods is demonstrated on a bearings only target localisation problem.\n    ",
        "submission_date": "2016-07-27T00:00:00",
        "last_modified_date": "2018-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08038",
        "title": "Behavior and path planning for the coalition of cognitive robots in smart relocation tasks",
        "authors": [
            "Aleksandr I. Panov",
            "Konstantin Yakovlev"
        ],
        "abstract": "In this paper we outline the approach of solving special type of navigation tasks for robotic systems, when a coalition of robots (agents) acts in the 2D environment, which can be modified by the actions, and share the same goal location. The latter is originally unreachable for some members of the coalition, but the common task still can be accomplished as the agents can assist each other (e.g. by modifying the environment). We call such tasks smart relocation tasks (as the can not be solved by pure path planning methods) and study spatial and behavior interaction of robots while solving them. We use cognitive approach and introduce semiotic knowledge representation - sign world model which underlines behavioral planning methodology. Planning is viewed as a recursive search process in the hierarchical state-space induced by sings with path planning signs reside on the lowest level. Reaching this level triggers path planning which is accomplished by state of the art grid-based planners focused on producing smooth paths (e.g. LIAN) and thus indirectly guarantying feasibility of that paths against agent's dynamic constraints.\n    ",
        "submission_date": "2016-07-27T00:00:00",
        "last_modified_date": "2016-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08073",
        "title": "Assisting Drivers During Overtaking Using Car-2-Car Communication and Multi-Agent Systems",
        "authors": [
            "Adrian Groza",
            "Calin Cara",
            "Sergiu Zaporojan",
            "Igor Calmicov"
        ],
        "abstract": "A warning system for assisting drivers during overtaking maneuvers is proposed. The system relies on Car-2-Car communication technologies and multi-agent systems. A protocol for safety overtaking is proposed based on ACL communicative acts. The mathematical model for safety overtaking used Kalman filter to minimize localization error.\n    ",
        "submission_date": "2016-07-27T00:00:00",
        "last_modified_date": "2016-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08074",
        "title": "Mining Arguments from Cancer Documents Using Natural Language Processing and Ontologies",
        "authors": [
            "Adrian Groza",
            "Oana Popa"
        ],
        "abstract": "In the medical domain, the continuous stream of scientific research contains contradictory results supported by arguments and counter-arguments. As medical expertise occurs at different levels, part of the human agents have difficulties to face the huge amount of studies, but also to understand the reasons and pieces of evidences claimed by the proponents and the opponents of the debated topic. To better understand the supporting arguments for new findings related to current state of the art in the medical domain we need tools able to identify arguments in scientific papers. Our work here aims to fill the above technological gap.\n",
        "submission_date": "2016-07-27T00:00:00",
        "last_modified_date": "2016-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08075",
        "title": "Harmonization of conflicting medical opinions using argumentation protocols and textual entailment - a case study on Parkinson disease",
        "authors": [
            "Adrian Groza",
            "Madalina Mand Nagy"
        ],
        "abstract": "Parkinson's disease is the second most common neurodegenerative disease, affecting more than 1.2 million people in Europe. Medications are available for the management of its symptoms, but the exact cause of the disease is unknown and there is currently no cure on the market. To better understand the relations between new findings and current medical knowledge, we need tools able to analyse published medical papers based on natural language processing and tools capable to identify various relationships of new findings with the current medical knowledge. Our work aims to fill the above technological gap.\n",
        "submission_date": "2016-07-27T00:00:00",
        "last_modified_date": "2016-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08100",
        "title": "Automatically Reinforcing a Game AI",
        "authors": [
            "David L. St-Pierre",
            "Jean-Baptiste Hoock",
            "Jialin Liu",
            "Fabien Teytaud",
            "Olivier Teytaud"
        ],
        "abstract": "A recent research trend in Artificial Intelligence (AI) is the combination of several programs into one single, stronger, program; this is termed portfolio methods. We here investigate the application of such methods to Game Playing Programs (GPPs). In addition, we consider the case in which only one GPP is available - by decomposing this single GPP into several ones through the use of parameters or even simply random seeds. These portfolio methods are trained in a learning phase. We propose two different offline approaches. The simplest one, BestArm, is a straightforward optimization of seeds or parame- ters; it performs quite well against the original GPP, but performs poorly against an opponent which repeats games and learns. The second one, namely Nash-portfolio, performs similarly in a \"one game\" test, and is much more robust against an opponent who learns. We also propose an online learning portfolio, which tests several of the GPP repeatedly and progressively switches to the best one - using a bandit algorithm.\n    ",
        "submission_date": "2016-07-27T00:00:00",
        "last_modified_date": "2016-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08131",
        "title": "Neuromorphic Robot Dream",
        "authors": [
            "Alexander Tchitchigin",
            "Max Talanov",
            "Larisa Safina",
            "Manuel Mazzara"
        ],
        "abstract": "In this paper we present the next step in our approach to neurobiologically plausible implementation of emotional reactions and behaviors for real-time autonomous robotic systems. The working metaphor we use is the \"day\" and the \"night\" phases of mammalian life. During the \"day phase\" a robotic system stores the inbound information and is controlled by a light-weight rule-based system in real time. In contrast to that, during the \"night phase\" information that has been stored is transferred to a supercomputing system to update the realistic neural network: emotional and behavioral strategies.\n    ",
        "submission_date": "2016-07-27T00:00:00",
        "last_modified_date": "2016-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08181",
        "title": "Psychologically inspired planning method for smart relocation task",
        "authors": [
            "Aleksandr I. Panov",
            "Konstantin Yakovlev"
        ],
        "abstract": "Behavior planning is known to be one of the basic cognitive functions, which is essential for any cognitive architecture of any control system used in robotics. At the same time most of the widespread planning algorithms employed in those systems are developed using only approaches and models of Artificial Intelligence and don't take into account numerous results of cognitive experiments. As a result, there is a strong need for novel methods of behavior planning suitable for modern cognitive architectures aimed at robot control. One such method is presented in this work and is studied within a special class of navigation task called smart relocation task. The method is based on the hierarchical two-level model of abstraction and knowledge representation, e.g. symbolic and subsymbolic. On the symbolic level sign world model is used for knowledge representation and hierarchical planning algorithm, PMA, is utilized for planning. On the subsymbolic level the task of path planning is considered and solved as a graph search problem. Interaction between both planners is examined and inter-level interfaces and feedback loops are described. Preliminary experimental results are presented.\n    ",
        "submission_date": "2016-07-27T00:00:00",
        "last_modified_date": "2016-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08289",
        "title": "Mammalian Value Systems",
        "authors": [
            "Gopal P. Sarma",
            "Nick J. Hay"
        ],
        "abstract": "Characterizing human values is a topic deeply interwoven with the sciences, humanities, art, and many other human endeavors. In recent years, a number of thinkers have argued that accelerating trends in computer science, cognitive science, and related disciplines foreshadow the creation of intelligent machines which meet and ultimately surpass the cognitive abilities of human beings, thereby entangling an understanding of human values with future technological development. Contemporary research accomplishments suggest sophisticated AI systems becoming widespread and responsible for managing many aspects of the modern world, from preemptively planning users' travel schedules and logistics, to fully autonomous vehicles, to domestic robots assisting in daily living. The extrapolation of these trends has been most forcefully described in the context of a hypothetical \"intelligence explosion,\" in which the capabilities of an intelligent software agent would rapidly increase due to the presence of feedback loops unavailable to biological organisms. The possibility of superintelligent agents, or simply the widespread deployment of sophisticated, autonomous AI systems, highlights an important theoretical problem: the need to separate the cognitive and rational capacities of an agent from the fundamental goal structure, or value system, which constrains and guides the agent's actions. The \"value alignment problem\" is to specify a goal structure for autonomous agents compatible with human values. In this brief article, we suggest that recent ideas from affective neuroscience and related disciplines aimed at characterizing neurological and behavioral universals in the mammalian class provide important conceptual foundations relevant to describing human values. We argue that the notion of \"mammalian value systems\" points to a potential avenue for fundamental research in AI safety and AI ethics.\n    ",
        "submission_date": "2016-07-28T00:00:00",
        "last_modified_date": "2019-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08316",
        "title": "Efficient Hyperparameter Optimization of Deep Learning Algorithms Using Deterministic RBF Surrogates",
        "authors": [
            "Ilija Ilievski",
            "Taimoor Akhtar",
            "Jiashi Feng",
            "Christine Annette Shoemaker"
        ],
        "abstract": "Automatically searching for optimal hyperparameter configurations is of crucial importance for applying deep learning algorithms in practice. Recently, Bayesian optimization has been proposed for optimizing hyperparameters of various machine learning algorithms. Those methods adopt probabilistic surrogate models like Gaussian processes to approximate and minimize the validation error function of hyperparameter values. However, probabilistic surrogates require accurate estimates of sufficient statistics (e.g., covariance) of the error distribution and thus need many function evaluations with a sizeable number of hyperparameters. This makes them inefficient for optimizing hyperparameters of deep learning algorithms, which are highly expensive to evaluate. In this work, we propose a new deterministic and efficient hyperparameter optimization method that employs radial basis functions as error surrogates. The proposed mixed integer algorithm, called HORD, searches the surrogate for the most promising hyperparameter values through dynamic coordinate search and requires many fewer function evaluations. HORD does well in low dimensions but it is exceptionally better in higher dimensions. Extensive evaluations on MNIST and CIFAR-10 for four deep neural networks demonstrate HORD significantly outperforms the well-established Bayesian optimization methods such as GP, SMAC, and TPE. For instance, on average, HORD is more than 6 times faster than GP-EI in obtaining the best configuration of 19 hyperparameters.\n    ",
        "submission_date": "2016-07-28T00:00:00",
        "last_modified_date": "2017-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08485",
        "title": "A symbolic algebra for the computation of expected utilities in multiplicative influence diagrams",
        "authors": [
            "Manuele Leonelli",
            "Eva Riccomagno",
            "Jim Q. Smith"
        ],
        "abstract": "Influence diagrams provide a compact graphical representation of decision problems. Several algorithms for the quick computation of their associated expected utilities are available in the literature. However, often they rely on a full quantification of both probabilistic uncertainties and utility values. For problems where all random variables and decision spaces are finite and discrete, here we develop a symbolic way to calculate the expected utilities of influence diagrams that does not require a full numerical representation. Within this approach expected utilities correspond to families of polynomials. After characterizing their polynomial structure, we develop an efficient symbolic algorithm for the propagation of expected utilities through the diagram and provide an implementation of this algorithm using a computer algebra system. We then characterize many of the standard manipulations of influence diagrams as transformations of polynomials. We also generalize the decision analytic framework of these diagrams by defining asymmetries as operations over the expected utility polynomials.\n    ",
        "submission_date": "2016-07-28T00:00:00",
        "last_modified_date": "2017-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08580",
        "title": "MIST: Missing Person Intelligence Synthesis Toolkit",
        "authors": [
            "Elham Shaabani",
            "Hamidreza Alvari",
            "Paulo Shakarian",
            "J.E. Kelly Snyder"
        ],
        "abstract": "Each day, approximately 500 missing persons cases occur that go unsolved/unresolved in the United States. The non-profit organization known as the Find Me Group (FMG), led by former law enforcement professionals, is dedicated to solving or resolving these cases. This paper introduces the Missing Person Intelligence Synthesis Toolkit (MIST) which leverages a data-driven variant of geospatial abductive inference. This system takes search locations provided by a group of experts and rank-orders them based on the probability assigned to areas based on the prior performance of the experts taken as a group. We evaluate our approach compared to the current practices employed by the Find Me Group and found it significantly reduces the search area - leading to a reduction of 31 square miles over 24 cases we examined in our experiments. Currently, we are using MIST to aid the Find Me Group in an active missing person case.\n    ",
        "submission_date": "2016-07-28T00:00:00",
        "last_modified_date": "2016-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08898",
        "title": "Personalized Emphasis Framing for Persuasive Message Generation",
        "authors": [
            "Tao Ding",
            "Shimei Pan"
        ],
        "abstract": "In this paper, we present a study on personalized emphasis framing which can be used to tailor the content of a message to enhance its appeal to different individuals. With this framework, we directly model content selection decisions based on a set of psychologically-motivated domain-independent personal traits including personality (e.g., extraversion and conscientiousness) and basic human values (e.g., self-transcendence and hedonism). We also demonstrate how the analysis results can be used in automated personalized content selection for persuasive message generation.\n    ",
        "submission_date": "2016-07-29T00:00:00",
        "last_modified_date": "2016-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00139",
        "title": "A Linear Algebraic Approach to Datalog Evaluation",
        "authors": [
            "Taisuke Sato"
        ],
        "abstract": "In this paper, we propose a fundamentally new approach to Datalog evaluation. Given a linear Datalog program DB written using N constants and binary predicates, we first translate if-and-only-if completions of clauses in DB into a set Eq(DB) of matrix equations with a non-linear operation where relations in M_DB, the least Herbrand model of DB, are encoded as adjacency matrices. We then translate Eq(DB) into another, but purely linear matrix equations tilde_Eq(DB). It is proved that the least solution of tilde_Eq(DB) in the sense of matrix ordering is converted to the least solution of Eq(DB) and the latter gives M_DB as a set of adjacency matrices. Hence computing the least solution of tilde_Eq(DB) is equivalent to computing M_DB specified by DB. For a class of tail recursive programs and for some other types of programs, our approach achieves O(N^3) time complexity irrespective of the number of variables in a clause since only matrix operations costing O(N^3) or less are used.\n",
        "submission_date": "2016-07-30T00:00:00",
        "last_modified_date": "2017-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00302",
        "title": "Formulating Semantics of Probabilistic Argumentation by Characterizing Subgraphs: Theory and Empirical Results",
        "authors": [
            "Beishui Liao",
            "Kang Xu",
            "Huaxin Huang"
        ],
        "abstract": "In existing literature, while approximate approaches based on Monte-Carlo simulation technique have been proposed to compute the semantics of probabilistic argumentation, how to improve the efficiency of computation without using simulation technique is still an open problem. In this paper, we address this problem from the following two perspectives. First, conceptually, we define specific properties to characterize the subgraphs of a PrAG with respect to a given extension, such that the probability of a set of arguments E being an extension can be defined in terms of these properties, without (or with less) construction of subgraphs. Second, computationally, we take preferred semantics as an example, and develop algorithms to evaluate the efficiency of our approach. The results show that our approach not only dramatically decreases the time for computing p(E^\\sigma), but also has an attractive property, which is contrary to that of existing approaches: the denser the edges of a PrAG are or the bigger the size of a given extension E is, the more efficient our approach computes p(E^\\sigma). Meanwhile, it is shown that under complete and preferred semantics, the problems of determining p(E^\\sigma) are fixed-parameter tractable.\n    ",
        "submission_date": "2016-08-01T00:00:00",
        "last_modified_date": "2016-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00730",
        "title": "Combining Answer Set Programming and Domain Heuristics for Solving Hard Industrial Problems (Application Paper)",
        "authors": [
            "Carmine Dodaro",
            "Philip Gasteiger",
            "Nicola Leone",
            "Benjamin Musitsch",
            "Francesco Ricca",
            "Kostyantyn Shchekotykhin"
        ],
        "abstract": "Answer Set Programming (ASP) is a popular logic programming paradigm that has been applied for solving a variety of complex problems. Among the most challenging real-world applications of ASP are two industrial problems defined by Siemens: the Partner Units Problem (PUP) and the Combined Configuration Problem (CCP). The hardest instances of PUP and CCP are out of reach for state-of-the-art ASP solvers. Experiments show that the performance of ASP solvers could be significantly improved by embedding domain-specific heuristics, but a proper effective integration of such criteria in off-the-shelf ASP implementations is not obvious. In this paper the combination of ASP and domain-specific heuristics is studied with the goal of effectively solving real-world problem instances of PUP and CCP. As a byproduct of this activity, the ASP solver WASP was extended with an interface that eases embedding new external heuristics in the solver. The evaluation shows that our domain-heuristic-driven ASP solver finds solutions for all the real-world instances of PUP and CCP ever provided by Siemens. This paper is under consideration for acceptance in TPLP.\n    ",
        "submission_date": "2016-08-02T00:00:00",
        "last_modified_date": "2016-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00810",
        "title": "Directed expected utility networks",
        "authors": [
            "Manuele Leonelli",
            "Jim Q. Smith"
        ],
        "abstract": "A variety of statistical graphical models have been defined to represent the conditional independences underlying a random vector of interest. Similarly, many different graphs embedding various types of preferential independences, as for example conditional utility independence and generalized additive independence, have more recently started to appear. In this paper we define a new graphical model, called a directed expected utility network, whose edges depict both probabilistic and utility conditional independences. These embed a very flexible class of utility models, much larger than those usually conceived in standard influence diagrams. Our graphical representation, and various transformations of the original graph into a tree structure, are then used to guide fast routines for the computation of a decision problem's expected utilities. We show that our routines generalize those usually utilized in standard influence diagrams' evaluations under much more restrictive conditions. We then proceed with the construction of a directed expected utility network to support decision makers in the domain of household food security.\n    ",
        "submission_date": "2016-08-02T00:00:00",
        "last_modified_date": "2016-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01093",
        "title": "Generation of Near-Optimal Solutions Using ILP-Guided Sampling",
        "authors": [
            "Ashwin Srinivasan",
            "Gautam Shroff",
            "Lovekesh Vig",
            "Sarmimala Saikia",
            "Puneet Agarwal"
        ],
        "abstract": "Our interest in this paper is in optimisation problems that are intractable to solve by direct numerical optimisation, but nevertheless have significant amounts of relevant domain-specific knowledge. The category of heuristic search techniques known as estimation of distribution algorithms (EDAs) seek to incrementally sample from probability distributions in which optimal (or near-optimal) solutions have increasingly higher probabilities. Can we use domain knowledge to assist the estimation of these distributions? To answer this in the affirmative, we need: (a)a general-purpose technique for the incorporation of domain knowledge when constructing models for optimal values; and (b)a way of using these models to generate new data samples. Here we investigate a combination of the use of Inductive Logic Programming (ILP) for (a), and standard logic-programming machinery to generate new samples for (b). Specifically, on each iteration of distribution estimation, an ILP engine is used to construct a model for good solutions. The resulting theory is then used to guide the generation of new data instances, which are now restricted to those derivable using the ILP model in conjunction with the background knowledge). We demonstrate the approach on two optimisation problems (predicting optimal depth-of-win for the KRK endgame, and job-shop scheduling). Our results are promising: (a)On each iteration of distribution estimation, samples obtained with an ILP theory have a substantially greater proportion of good solutions than samples without a theory; and (b)On termination of distribution estimation, samples obtained with an ILP theory contain more near-optimal samples than samples without a theory. Taken together, these results suggest that the use of ILP-constructed theories could be a useful technique for incorporating complex domain-knowledge into estimation distribution procedures.\n    ",
        "submission_date": "2016-08-03T00:00:00",
        "last_modified_date": "2016-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01212",
        "title": "A Novel Approach for Data-Driven Automatic Site Recommendation and Selection",
        "authors": [
            "Sebastian Baumbach",
            "Frank Wittich",
            "Florian Sachs",
            "Sheraz Ahmed",
            "Andreas Dengel"
        ],
        "abstract": "This paper presents a novel, generic, and automatic method for data-driven site selection. Site selection is one of the most crucial and important decisions made by any company. Such a decision depends on various factors of sites, including socio-economic, geographical, ecological, as well as specific requirements of companies. The existing approaches for site selection (commonly used by economists) are manual, subjective, and not scalable, especially to Big Data. The presented method for site selection is robust, efficient, scalable, and is capable of handling challenges emerging in Big Data. To assess the effectiveness of the presented method, it is evaluated on real data (collected from Federal Statistical Office of Germany) of around 200 influencing factors which are considered by economists for site selection of Supermarkets in Germany (Lidl, EDEKA, and NP). Evaluation results show that there is a big overlap (86.4 \\%) between the sites of existing supermarkets and the sites recommended by the presented method. In addition, the method also recommends many sites (328) for supermarket where a store should be opened.\n    ",
        "submission_date": "2016-08-03T00:00:00",
        "last_modified_date": "2016-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01302",
        "title": "Learning to Rank for Synthesizing Planning Heuristics",
        "authors": [
            "Caelan Reed Garrett",
            "Leslie Pack Kaelbling",
            "Tomas Lozano-Perez"
        ],
        "abstract": "We investigate learning heuristics for domain-specific planning. Prior work framed learning a heuristic as an ordinary regression problem. However, in a greedy best-first search, the ordering of states induced by a heuristic is more indicative of the resulting planner's performance than mean squared error. Thus, we instead frame learning a heuristic as a learning to rank problem which we solve using a RankSVM formulation. Additionally, we introduce new methods for computing features that capture temporal interactions in an approximate plan. Our experiments on recent International Planning Competition problems show that the RankSVM learned heuristics outperform both the original heuristics and heuristics learned through ordinary regression.\n    ",
        "submission_date": "2016-08-03T00:00:00",
        "last_modified_date": "2016-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01338",
        "title": "Paraconsistency and Word Puzzles",
        "authors": [
            "Tiantian Gao",
            "Paul Fodor",
            "Michael Kifer"
        ],
        "abstract": "Word puzzles and the problem of their representations in logic languages have received considerable attention in the last decade (Ponnuru et al. 2004; Shapiro 2011; Baral and Dzifcak 2012; Schwitter 2013). Of special interest is the problem of generating such representations directly from natural language (NL) or controlled natural language (CNL). An interesting variation of this problem, and to the best of our knowledge, scarcely explored variation in this context, is when the input information is inconsistent. In such situations, the existing encodings of word puzzles produce inconsistent representations and break down. In this paper, we bring the well-known type of paraconsistent logics, called Annotated Predicate Calculus (APC) (Kifer and Lozinskii 1992), to bear on the problem. We introduce a new kind of non-monotonic semantics for APC, called consistency preferred stable models and argue that it makes APC into a suitable platform for dealing with inconsistency in word puzzles and, more generally, in NL sentences. We also devise a number of general principles to help the user choose among the different representations of NL sentences, which might seem equivalent but, in fact, behave differently when inconsistent information is taken into account. These principles can be incorporated into existing CNL translators, such as Attempto Controlled English (ACE) (Fuchs et al. 2008) and PENG Light (White and Schwitter 2009). Finally, we show that APC with the consistency preferred stable model semantics can be equivalently embedded in ASP with preferences over stable models, and we use this embedding to implement this version of APC in Clingo (Gebser et al. 2011) and its Asprin add-on (Brewka et al. 2015).\n    ",
        "submission_date": "2016-08-03T00:00:00",
        "last_modified_date": "2016-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01402",
        "title": "Interacting Conceptual Spaces",
        "authors": [
            "Josef Bolt",
            "Bob Coecke",
            "Fabrizio Genovese",
            "Martha Lewis",
            "Daniel Marsden",
            "Robin Piedeleu"
        ],
        "abstract": "We propose applying the categorical compositional scheme of [6] to conceptual space models of cognition. In order to do this we introduce the category of convex relations as a new setting for categorical compositional semantics, emphasizing the convex structure important to conceptual space applications. We show how conceptual spaces for composite types such as adjectives and verbs can be constructed. We illustrate this new model on detailed examples.\n    ",
        "submission_date": "2016-08-04T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01604",
        "title": "Query Answering in Resource-Based Answer Set Semantics",
        "authors": [
            "Stefania Costantini",
            "Andrea Formisano"
        ],
        "abstract": "In recent work we defined resource-based answer set semantics, which is an extension to answer set semantics stemming from the study of its relationship with linear logic. In fact, the name of the new semantics comes from the fact that in the linear-logic formulation every literal (including negative ones) were considered as a resource. In this paper, we propose a query-answering procedure reminiscent of Prolog for answer set programs under this extended semantics as an extension of XSB-resolution for logic programs with negation. We prove formal properties of the proposed procedure.\n",
        "submission_date": "2016-08-04T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01611",
        "title": "Deploying learning materials to game content for serious education game development: A case study",
        "authors": [
            "Harits Ar Rosyid",
            "Matt Palmerlee",
            "Ke Chen"
        ],
        "abstract": "The ultimate goals of serious education games (SEG) are to facilitate learning and maximizing enjoyment during playing SEGs. In SEG development, there are normally two spaces to be taken into account: knowledge space regarding learning materials and content space regarding games to be used to convey learning materials. How to deploy the learning materials seamlessly and effectively into game content becomes one of the most challenging problems in SEG development. Unlike previous work where experts in education have to be used heavily, we proposed a novel approach that works toward minimizing the efforts of education experts in mapping learning materials to content space. For a proof-of-concept, we apply the proposed approach in developing an SEG game, named \\emph{Chem Dungeon}, as a case study in order to demonstrate the effectiveness of our proposed approach. This SEG game has been tested with a number of users, and the user survey suggests our method works reasonably well.\n    ",
        "submission_date": "2016-08-04T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01668",
        "title": "Self-Organising Maps in Computer Security",
        "authors": [
            "Jan Feyereisl",
            "Uwe Aickelin"
        ],
        "abstract": "Some argue that biologically inspired algorithms are the future of solving difficult problems in computer science. Others strongly believe that the future lies in the exploration of mathematical foundations of problems at hand. The field of computer security tends to accept the latter view as a more appropriate approach due to its more workable validation and verification possibilities. The lack of rigorous scientific practices prevalent in biologically inspired security research does not aid in presenting bio-inspired security approaches as a viable way of dealing with complex security problems. This chapter introduces a biologically inspired algorithm, called the Self Organising Map (SOM), that was developed by Teuvo Kohonen in 1981. Since the algorithm's inception it has been scrutinised by the scientific community and analysed in more than 4000 research papers, many of which dealt with various computer security issues, from anomaly detection, analysis of executables all the way to wireless network monitoring. In this chapter a review of security related SOM research undertaken in the past is presented and analysed. The algorithm's biological analogies are detailed and the author's view on the future possibilities of this successful bio-inspired approach are given. The SOM algorithm's close relation to a number of vital functions of the human brain and the emergence of multi-core computer architectures are the two main reasons behind our assumption that the future of the SOM algorithm and its variations is promising, notably in the field of computer security.\n    ",
        "submission_date": "2016-08-05T00:00:00",
        "last_modified_date": "2016-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01835",
        "title": "Stable-Unstable Semantics: Beyond NP with Normal Logic Programs",
        "authors": [
            "Bart Bogaerts",
            "Tomi Janhunen",
            "Shahab Tasharrofi"
        ],
        "abstract": "Standard answer set programming (ASP) targets at solving search problems from the first level of the polynomial time hierarchy (PH). Tackling search problems beyond NP using ASP is less straightforward. The class of disjunctive logic programs offers the most prominent way of reaching the second level of the PH, but encoding respective hard problems as disjunctive programs typically requires sophisticated techniques such as saturation or meta-interpretation. The application of such techniques easily leads to encodings that are inaccessible to non-experts. Furthermore, while disjunctive ASP solvers often rely on calls to a (co-)NP oracle, it may be difficult to detect from the input program where the oracle is being accessed. In other formalisms, such as Quantified Boolean Formulas (QBFs), the interface to the underlying oracle is more transparent as it is explicitly recorded in the quantifier prefix of a formula. On the other hand, ASP has advantages over QBFs from the modeling perspective. The rich high-level languages such as ASP-Core-2 offer a wide variety of primitives that enable concise and natural encodings of search problems. In this paper, we present a novel logic programming--based modeling paradigm that combines the best features of ASP and QBFs. We develop so-called combined logic programs in which oracles are directly cast as (normal) logic programs themselves. Recursive incarnations of this construction enable logic programming on arbitrarily high levels of the PH. We develop a proof-of-concept implementation for our new paradigm.\n",
        "submission_date": "2016-08-05T00:00:00",
        "last_modified_date": "2016-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01856",
        "title": "The Power of Non-Ground Rules in Answer Set Programming",
        "authors": [
            "Manuel Bichler",
            "Michael Morak",
            "Stefan Woltran"
        ],
        "abstract": "Answer set programming (ASP) is a well-established logic programming language that offers an intuitive, declarative syntax for problem solving. In its traditional application, a fixed ASP program for a given problem is designed and the actual instance of the problem is fed into the program as a set of facts. This approach typically results in programs with comparably short and simple rules. However, as is known from complexity analysis, such an approach limits the expressive power of ASP; in fact, an entire NP-check can be encoded into a single large rule body of bounded arity that performs both a guess and a check within the same rule.\n",
        "submission_date": "2016-08-05T00:00:00",
        "last_modified_date": "2016-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01884",
        "title": "Winograd Schemas and Machine Translation",
        "authors": [
            "Ernest Davis"
        ],
        "abstract": "A Winograd schema is a pair of sentences that differ in a single word and that contain an ambiguous pronoun whose referent is different in the two sentences and requires the use of commonsense knowledge or world knowledge to disambiguate. This paper discusses how Winograd schemas and other sentence pairs could be used as challenges for machine translation using distinctions between pronouns, such as gender, that appear in the target language but not in the source.\n    ",
        "submission_date": "2016-08-05T00:00:00",
        "last_modified_date": "2016-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01946",
        "title": "Iterative Learning of Answer Set Programs from Context Dependent Examples",
        "authors": [
            "Mark Law",
            "Alessandra Russo",
            "Krysia Broda"
        ],
        "abstract": "In recent years, several frameworks and systems have been proposed that extend Inductive Logic Programming (ILP) to the Answer Set Programming (ASP) paradigm. In ILP, examples must all be explained by a hypothesis together with a given background knowledge. In existing systems, the background knowledge is the same for all examples; however, examples may be context-dependent. This means that some examples should be explained in the context of some information, whereas others should be explained in different contexts. In this paper, we capture this notion and present a context-dependent extension of the Learning from Ordered Answer Sets framework. In this extension, contexts can be used to further structure the background knowledge. We then propose a new iterative algorithm, ILASP2i, which exploits this feature to scale up the existing ILASP2 system to learning tasks with large numbers of examples. We demonstrate the gain in scalability by applying both algorithms to various learning tasks. Our results show that, compared to ILASP2, the newly proposed ILASP2i system can be two orders of magnitude faster and use two orders of magnitude less memory, whilst preserving the same average accuracy. This paper is under consideration for acceptance in TPLP.\n    ",
        "submission_date": "2016-08-05T00:00:00",
        "last_modified_date": "2016-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02082",
        "title": "COREALMLIB: An ALM Library Translated from the Component Library",
        "authors": [
            "Daniela Inclezan"
        ],
        "abstract": "This paper presents COREALMLIB, an ALM library of commonsense knowledge about dynamic domains. The library was obtained by translating part of the COMPONENT LIBRARY (CLIB) into the modular action language ALM. CLIB consists of general reusable and composable commonsense concepts, selected based on a thorough study of ontological and lexical resources. Our translation targets CLIB states (i.e., fluents) and actions. The resulting ALM library contains the descriptions of 123 action classes grouped into 43 reusable modules that are organized into a hierarchy. It is made available online and of interest to researchers in the action language, answer-set programming, and natural language understanding communities. We believe that our translation has two main advantages over its CLIB counterpart: (i) it specifies axioms about actions in a more elaboration tolerant and readable way, and (ii) it can be seamlessly integrated with ASP reasoning algorithms (e.g., for planning and postdiction). In contrast, axioms are described in CLIB using STRIPS-like operators, and CLIB's inference engine cannot handle planning nor postdiction. Under consideration for publication in TPLP.\n    ",
        "submission_date": "2016-08-06T00:00:00",
        "last_modified_date": "2016-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02193",
        "title": "Spacetimes with Semantics (III) - The Structure of Functional Knowledge Representation and Artificial Reasoning",
        "authors": [
            "Mark Burgess"
        ],
        "abstract": "Using the previously developed concepts of semantic spacetime, I explore the interpretation of knowledge representations, and their structure, as a semantic system, within the framework of promise theory. By assigning interpretations to phenomena, from observers to observed, we may approach a simple description of knowledge-based functional systems, with direct practical utility. The focus is especially on the interpretation of concepts, associative knowledge, and context awareness. The inference seems to be that most if not all of these concepts emerge from purely semantic spacetime properties, which opens the possibility for a more generalized understanding of what constitutes a learning, or even `intelligent' system.\n",
        "submission_date": "2016-08-07T00:00:00",
        "last_modified_date": "2017-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02287",
        "title": "Delta Epsilon Alpha Star: A PAC-Admissible Search Algorithm",
        "authors": [
            "David Cox"
        ],
        "abstract": "Delta Epsilon Alpha Star is a minimal coverage, real-time robotic search algorithm that yields a moderately aggressive search path with minimal backtracking. Search performance is bounded by a placing a combinatorial bound, epsilon and delta, on the maximum deviation from the theoretical shortest path and the probability at which further deviations can occur. Additionally, we formally define the notion of PAC-admissibility -- a relaxed admissibility criteria for algorithms, and show that PAC-admissible algorithms are better suited to robotic search situations than epsilon-admissible or strict algorithms.\n    ",
        "submission_date": "2016-08-08T00:00:00",
        "last_modified_date": "2016-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02315",
        "title": "Blankets Joint Posterior score for learning Markov network structures",
        "authors": [
            "Federico Schl\u00fcter",
            "Yanela Strappa",
            "Diego H. Milone",
            "Facundo Bromberg"
        ],
        "abstract": "Markov networks are extensively used to model complex sequential, spatial, and relational interactions in a wide range of fields. By learning the structure of independences of a domain, more accurate joint probability distributions can be obtained for inference tasks or, more directly, for interpreting the most significant relations among the variables. Recently, several researchers have investigated techniques for automatically learning the structure from data by obtaining the probabilistic maximum-a-posteriori structure given the available data. However, all the approximations proposed decompose the posterior of the whole structure into local sub-problems, by assuming that the posteriors of the Markov blankets of all the variables are mutually independent. In this work, we propose a scoring function for relaxing such assumption. The Blankets Joint Posterior score computes the joint posterior of structures as a joint distribution of the collection of its Markov blankets. Essentially, the whole posterior is obtained by computing the posterior of the blanket of each variable as a conditional distribution that takes into account information from other blankets in the network. We show in our experimental results that the proposed approximation can improve the sample complexity of state-of-the-art scores when learning complex networks, where the independence assumption between blanket variables is clearly incorrect.\n    ",
        "submission_date": "2016-08-08T00:00:00",
        "last_modified_date": "2017-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02406",
        "title": "Complexity Results for Manipulation, Bribery and Control of the Kemeny Procedure in Judgment Aggregation",
        "authors": [
            "Ronald de Haan"
        ],
        "abstract": "We study the computational complexity of several scenarios of strategic behavior for the Kemeny procedure in the setting of judgment aggregation. In particular, we investigate (1) manipulation, where an individual aims to achieve a better group outcome by reporting an insincere individual opinion, (2) bribery, where an external agent aims to achieve an outcome with certain properties by bribing a number of individuals, and (3) control (by adding or deleting issues), where an external agent aims to achieve an outcome with certain properties by influencing the set of issues in the judgment aggregation situation. We show that determining whether these types of strategic behavior are possible (and if so, computing a policy for successful strategic behavior) is complete for the second level of the Polynomial Hierarchy. That is, we show that these problems are $\\Sigma^p_2$-complete.\n    ",
        "submission_date": "2016-08-08T00:00:00",
        "last_modified_date": "2016-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02441",
        "title": "Proceedings of the Second Summer School on Argumentation: Computational and Linguistic Perspectives (SSA'16)",
        "authors": [
            "Sarah A. Gaggl",
            "Matthias Thimm"
        ],
        "abstract": "This volume contains the thesis abstracts presented at the Second Summer School on Argumentation: Computational and Linguistic Perspectives (SSA'2016) held on September 8-12 in Potsdam, Germany.\n    ",
        "submission_date": "2016-08-03T00:00:00",
        "last_modified_date": "2016-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02450",
        "title": "ASP for Minimal Entailment in a Rational Extension of SROEL",
        "authors": [
            "Laura Giordano",
            "Daniele Theseider Dupr\u00e9"
        ],
        "abstract": "In this paper we exploit Answer Set Programming (ASP) for reasoning in a rational extension SROEL-R-T of the low complexity description logic SROEL, which underlies the OWL EL ontology language. In the extended language, a typicality operator T is allowed to define concepts T(C) (typical C's) under a rational semantics. It has been proven that instance checking under rational entailment has a polynomial complexity. To strengthen rational entailment, in this paper we consider a minimal model semantics. We show that, for arbitrary SROEL-R-T knowledge bases, instance checking under minimal entailment is \\Pi^P_2-complete. Relying on a Small Model result, where models correspond to answer sets of a suitable ASP encoding, we exploit Answer Set Preferences (and, in particular, the asprin framework) for reasoning under minimal entailment. The paper is under consideration for acceptance in Theory and Practice of Logic Programming.\n    ",
        "submission_date": "2016-08-08T00:00:00",
        "last_modified_date": "2016-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02644",
        "title": "Holophrasm: a neural Automated Theorem Prover for higher-order logic",
        "authors": [
            "Daniel Whalen"
        ],
        "abstract": "I propose a system for Automated Theorem Proving in higher order logic using deep learning and eschewing hand-constructed features. Holophrasm exploits the formalism of the Metamath language and explores partial proof trees using a neural-network-augmented bandit algorithm and a sequence-to-sequence model for action enumeration. The system proves 14% of its test theorems from Metamath's ",
        "submission_date": "2016-08-08T00:00:00",
        "last_modified_date": "2016-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02659",
        "title": "Mouse Movement and Probabilistic Graphical Models Based E-Learning Activity Recognition Improvement Possibilistic Model",
        "authors": [
            "Anis Elbahi",
            "Mohamed Nazih Omri",
            "Mohamed Ali Mahjoub",
            "Kamel Garrouch"
        ],
        "abstract": "Automatically recognizing the e-learning activities is an important task for improving the online learning process. Probabilistic graphical models such as hidden Markov models and conditional random fields have been successfully used in order to identify a Web users activity. For such models, the sequences of observation are crucial for training and inference processes. Despite the efficiency of these probabilistic graphical models in segmenting and labeling stochastic sequences, their performance is adversely affected by the imperfect quality of data used for the construction of sequences of observation. In this paper, a formalism of the possibilistic theory will be used in order to propose a new approach for observation sequences preparation. The eminent contribution of our approach is to evaluate the effect of possibilistic reasoning during the generation of observation sequences on the effectiveness of hidden Markov models and conditional random fields models. Using a dataset containing 51 real manipulations related to three types of learners tasks, the preliminary experiments demonstrate that the sequences of observation obtained based on possibilistic reasoning significantly improve the performance of hidden Marvov models and conditional random fields models in the automatic recognition of the e-learning activities.\n    ",
        "submission_date": "2016-08-08T00:00:00",
        "last_modified_date": "2016-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02682",
        "title": "Exact Structure Learning of Bayesian Networks by Optimal Path Extension",
        "authors": [
            "Subhadeep Karan",
            "Jaroslaw Zola"
        ],
        "abstract": "Bayesian networks are probabilistic graphical models often used in big data analytics. The problem of exact structure learning is to find a network structure that is optimal under certain scoring criteria. The problem is known to be NP-hard and the existing methods are both computationally and memory intensive. In this paper, we introduce a new approach for exact structure learning. Our strategy is to leverage relationship between a partial network structure and the remaining variables to constraint the number of ways in which the partial network can be optimally extended. Via experimental results, we show that the method provides up to three times improvement in runtime, and orders of magnitude reduction in memory consumption over the current best algorithms.\n    ",
        "submission_date": "2016-08-09T00:00:00",
        "last_modified_date": "2017-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02693",
        "title": "Deeply Semantic Inductive Spatio-Temporal Learning",
        "authors": [
            "Jakob Suchan",
            "Mehul Bhatt",
            "Carl Schultz"
        ],
        "abstract": "We present an inductive spatio-temporal learning framework rooted in inductive logic programming. With an emphasis on visuo-spatial language, logic, and cognition, the framework supports learning with relational spatio-temporal features identifiable in a range of domains involving the processing and interpretation of dynamic visuo-spatial imagery. We present a prototypical system, and an example application in the domain of computing for visual arts and computational cognitive science.\n    ",
        "submission_date": "2016-08-09T00:00:00",
        "last_modified_date": "2016-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02763",
        "title": "Resolving Spatial-Time Conflicts In A Set Of Any-angle Or Angle-constrained Grid Paths",
        "authors": [
            "Konstantin Yakovlev",
            "Anton Andreychuk"
        ],
        "abstract": "We study the multi-agent path finding problem (MAPF) for a group of agents which are allowed to move into arbitrary directions on a 2D square grid. We focus on centralized conflict resolution for independently computed plans. We propose an algorithm that eliminates conflicts by using local re-planning and introducing time offsets to the execution of paths by different agents. Experimental results show that the algorithm can find high quality conflict-free solutions at low computational cost.\n    ",
        "submission_date": "2016-08-09T00:00:00",
        "last_modified_date": "2016-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02858",
        "title": "Liftago On-Demand Transport Dataset and Market Formation Algorithm Based on Machine Learning",
        "authors": [
            "Jan Mrkos",
            "Jan Drchal",
            "Malcolm Egan",
            "Michal Jakob"
        ],
        "abstract": "This document serves as a technical report for the analysis of on-demand transport dataset. Moreover we show how the dataset can be used to develop a market formation algorithm based on machine learning. Data used in this work comes from Liftago, a Prague based company which connects taxi drivers and customers through a smartphone app. The dataset is analysed from the machine-learning perspective: we give an overview of features available as well as results of feature ranking. Later we propose the SImple Data-driven MArket Formation (SIDMAF) algorithm which aims to improve a relevance while connecting customers with relevant drivers. We compare the heuristics currently used by Liftago with SIDMAF using two key performance indicators.\n    ",
        "submission_date": "2016-08-09T00:00:00",
        "last_modified_date": "2016-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03026",
        "title": "Towards Visual Type Theory as a Mathematical Tool and Mathematical User Interface",
        "authors": [
            "Lucius Schoenbaum"
        ],
        "abstract": "A visual type theory is a cognitive tool that has much in common with language, and may be regarded as an exceptional form of spatial text adjunct. A mathematical visual type theory, called NPM, has been under development that can be viewed as an early-stage project in mathematical knowledge management and mathematical user interface development. We discuss in greater detail the notion of a visual type theory, report on progress towards a usable mathematical visual type theory, and discuss the outlook for future work on this project.\n    ",
        "submission_date": "2016-08-10T00:00:00",
        "last_modified_date": "2016-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03507",
        "title": "Learning Mobile App Usage Routine through Learning Automata",
        "authors": [
            "Ramin Rahnamoun",
            "Reza Rawassizadeh",
            "Arash Maskooki"
        ],
        "abstract": "Since its conception, smart app market has grown exponentially. Success in the app market depends on many factors among which the quality of the app is a significant contributor, such as energy use. Nevertheless, smartphones, as a subset of mobile computing devices. inherit the limited power resource constraint. Therefore, there is a challenge of maintaining the resource while increasing the target app quality. This paper introduces Learning Automata (LA) as an online learning method to learn and predict the app usage routines of the users. Such prediction can leverage the app cache functionality of the operating system and thus (i) decreases app launch time and (ii) preserve battery. Our algorithm, which is an online learning approach, temporally updates and improves the internal states of itself. In particular, it learns the transition probabilities between app launching. Each App launching instance updates the transition probabilities related to that App, and this will result in improving the prediction. We benefit from a real-world lifelogging dataset and our experimental results show considerable success with respect to the two baseline methods that are used currently for smartphone app prediction approaches.\n    ",
        "submission_date": "2016-08-11T00:00:00",
        "last_modified_date": "2016-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03532",
        "title": "QPass: a Merit-based Evaluation of Soccer Passes",
        "authors": [
            "Laszlo Gyarmati",
            "Rade Stanojevic"
        ],
        "abstract": "Quantitative analysis of soccer players' passing ability focuses on descriptive statistics without considering the players' real contribution to the passing and ball possession strategy of their team. Which player is able to help the build-up of an attack, or to maintain the possession of the ball? We introduce a novel methodology called QPass to answer questions like these quantitatively. Based on the analysis of an entire season, we rank the players based on the intrinsic value of their passes using QPass. We derive an album of pass trajectories for different gaming styles. Our methodology reveals a quite counterintuitive paradigm: losing the ball possession could lead to better chances to win a game.\n    ",
        "submission_date": "2016-08-08T00:00:00",
        "last_modified_date": "2016-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03672",
        "title": "Inferring unknown biological function by integration of GO annotations and gene expression data",
        "authors": [
            "Guillermo Leale",
            "Ariel Bay\u00e1",
            "Diego Milone",
            "Pablo Granitto",
            "Georgina Stegmayer"
        ],
        "abstract": "Characterizing genes with semantic information is an important process regarding the description of gene products. In spite that complete genomes of many organisms have been already sequenced, the biological functions of all of their genes are still unknown. Since experimentally studying the functions of those genes, one by one, would be unfeasible, new computational methods for gene functions inference are needed. We present here a novel computational approach for inferring biological function for a set of genes with previously unknown function, given a set of genes with well-known information. This approach is based on the premise that genes with similar behaviour should be grouped together. This is known as the guilt-by-association principle. Thus, it is possible to take advantage of clustering techniques to obtain groups of unknown genes that are co-clustered with genes that have well-known semantic information (GO annotations). Meaningful knowledge to infer unknown semantic information can therefore be provided by these well-known genes. We provide a method to explore the potential function of new genes according to those currently annotated. The results obtained indicate that the proposed approach could be a useful and effective tool when used by biologists to guide the inference of biological functions for recently discovered genes. Our work sets an important landmark in the field of identifying unknown gene functions through clustering, using an external source of biological input. A simple web interface to this proposal can be found at ",
        "submission_date": "2016-08-12T00:00:00",
        "last_modified_date": "2016-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03785",
        "title": "Compositional Distributional Cognition",
        "authors": [
            "Yaared Al-Mehairi",
            "Bob Coecke",
            "Martha Lewis"
        ],
        "abstract": "We accommodate the Integrated Connectionist/Symbolic Architecture (ICS) of [32] within the categorical compositional semantics (CatCo) of [13], forming a model of categorical compositional cognition (CatCog). This resolves intrinsic problems with ICS such as the fact that representations inhabit an unbounded space and that sentences with differing tree structures cannot be directly compared. We do so in a way that makes the most of the grammatical structure available, in contrast to strategies like circular convolution. Using the CatCo model also allows us to make use of tools developed for CatCo such as the representation of ambiguity and logical reasoning via density matrices, structural meanings for words such as relative pronouns, and addressing over- and under-extension, all of which are present in cognitive processes. Moreover the CatCog framework is sufficiently flexible to allow for entirely different representations of meaning, such as conceptual spaces. Interestingly, since the CatCo model was largely inspired by categorical quantum mechanics, so is CatCog.\n    ",
        "submission_date": "2016-08-12T00:00:00",
        "last_modified_date": "2016-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03824",
        "title": "Perceptual Reward Functions",
        "authors": [
            "Ashley Edwards",
            "Charles Isbell",
            "Atsuo Takanishi"
        ],
        "abstract": "Reinforcement learning problems are often described through rewards that indicate if an agent has completed some task. This specification can yield desirable behavior, however many problems are difficult to specify in this manner, as one often needs to know the proper configuration for the agent. When humans are learning to solve tasks, we often learn from visual instructions composed of images or videos. Such representations motivate our development of Perceptual Reward Functions, which provide a mechanism for creating visual task descriptions. We show that this approach allows an agent to learn from rewards that are based on raw pixels rather than internal parameters.\n    ",
        "submission_date": "2016-08-12T00:00:00",
        "last_modified_date": "2016-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04672",
        "title": "Informal Physical Reasoning Processes",
        "authors": [
            "Kurt Ammon"
        ],
        "abstract": "A fundamental question is whether Turing machines can model all reasoning processes. We introduce an existence principle stating that the perception of the physical existence of any Turing program can serve as a physical causation for the application of any Turing-computable function to this Turing program. The existence principle overcomes the limitation of the outputs of Turing machines to lists, that is, recursively enumerable sets. The principle is illustrated by productive partial functions for productive sets such as the set of the Goedel numbers of the Turing-computable total functions. The existence principle and productive functions imply the existence of physical systems whose reasoning processes cannot be modeled by Turing machines. These systems are called creative. Creative systems can prove the undecidable formula in Goedel's theorem in another formal system which is constructed at a later point in time. A hypothesis about creative systems, which is based on computer experiments, is introduced.\n    ",
        "submission_date": "2016-08-15T00:00:00",
        "last_modified_date": "2016-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04689",
        "title": "A Shallow High-Order Parametric Approach to Data Visualization and Compression",
        "authors": [
            "Martin Renqiang Min",
            "Hongyu Guo",
            "Dongjin Song"
        ],
        "abstract": "Explicit high-order feature interactions efficiently capture essential structural knowledge about the data of interest and have been used for constructing generative models. We present a supervised discriminative High-Order Parametric Embedding (HOPE) approach to data visualization and compression. Compared to deep embedding models with complicated deep architectures, HOPE generates more effective high-order feature mapping through an embarrassingly simple shallow model. Furthermore, two approaches to generating a small number of exemplars conveying high-order interactions to represent large-scale data sets are proposed. These exemplars in combination with the feature mapping learned by HOPE effectively capture essential data variations. Moreover, through HOPE, these exemplars are employed to increase the computational efficiency of kNN classification for fast information retrieval by thousands of times. For classification in two-dimensional embedding space on MNIST and USPS datasets, our shallow method HOPE with simple Sigmoid transformations significantly outperforms state-of-the-art supervised deep embedding models based on deep neural networks, and even achieved historically low test error rate of 0.65% in two-dimensional space on MNIST, which demonstrates the representational efficiency and power of supervised shallow models with high-order feature interactions.\n    ",
        "submission_date": "2016-08-16T00:00:00",
        "last_modified_date": "2016-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04698",
        "title": "Evaluating Causal Models by Comparing Interventional Distributions",
        "authors": [
            "Dan Garant",
            "David Jensen"
        ],
        "abstract": "The predominant method for evaluating the quality of causal models is to measure the graphical accuracy of the learned model structure. We present an alternative method for evaluating causal models that directly measures the accuracy of estimated interventional distributions. We contrast such distributional measures with structural measures, such as structural Hamming distance and structural intervention distance, showing that structural measures often correspond poorly to the accuracy of estimated interventional distributions. We use a number of real and synthetic datasets to illustrate various scenarios in which structural measures provide misleading results with respect to algorithm selection and parameter tuning, and we recommend that distributional measures become the new standard for evaluating causal models.\n    ",
        "submission_date": "2016-08-16T00:00:00",
        "last_modified_date": "2016-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04996",
        "title": "Open Problem: Approximate Planning of POMDPs in the class of Memoryless Policies",
        "authors": [
            "Kamyar Azizzadenesheli",
            "Alessandro Lazaric",
            "Animashree Anandkumar"
        ],
        "abstract": "Planning plays an important role in the broad class of decision theory. Planning has drawn much attention in recent work in the robotics and sequential decision making areas. Recently, Reinforcement Learning (RL), as an agent-environment interaction problem, has brought further attention to planning methods. Generally in RL, one can assume a generative model, e.g. graphical models, for the environment, and then the task for the RL agent is to learn the model parameters and find the optimal strategy based on these learnt parameters. Based on environment behavior, the agent can assume various types of generative models, e.g. Multi Armed Bandit for a static environment, or Markov Decision Process (MDP) for a dynamic environment. The advantage of these popular models is their simplicity, which results in tractable methods of learning the parameters and finding the optimal policy. The drawback of these models is again their simplicity: these models usually underfit and underestimate the actual environment behavior. For example, in robotics, the agent usually has noisy observations of the environment inner state and MDP is not a suitable model.\n",
        "submission_date": "2016-08-17T00:00:00",
        "last_modified_date": "2016-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05046",
        "title": "Practical optimal experiment design with probabilistic programs",
        "authors": [
            "Long Ouyang",
            "Michael Henry Tessler",
            "Daniel Ly",
            "Noah Goodman"
        ],
        "abstract": "Scientists often run experiments to distinguish competing theories. This requires patience, rigor, and ingenuity - there is often a large space of possible experiments one could run. But we need not comb this space by hand - if we represent our theories as formal models and explicitly declare the space of experiments, we can automate the search for good experiments, looking for those with high expected information gain. Here, we present a general and principled approach to experiment design based on probabilistic programming languages (PPLs). PPLs offer a clean separation between declaring problems and solving them, which means that the scientist can automate experiment design by simply declaring her model and experiment spaces in the PPL without having to worry about the details of calculating information gain. We demonstrate our system in two case studies drawn from cognitive psychology, where we use it to design optimal experiments in the domains of sequence prediction and categorization. We find strong empirical validation that our automatically designed experiments were indeed optimal. We conclude by discussing a number of interesting questions for future research.\n    ",
        "submission_date": "2016-08-17T00:00:00",
        "last_modified_date": "2016-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05151",
        "title": "Effective Multi-step Temporal-Difference Learning for Non-Linear Function Approximation",
        "authors": [
            "Harm van Seijen"
        ],
        "abstract": "Multi-step temporal-difference (TD) learning, where the update targets contain information from multiple time steps ahead, is one of the most popular forms of TD learning for linear function approximation. The reason is that multi-step methods often yield substantially better performance than their single-step counter-parts, due to a lower bias of the update targets. For non-linear function approximation, however, single-step methods appear to be the norm. Part of the reason could be that on many domains the popular multi-step methods TD($\\lambda$) and Sarsa($\\lambda$) do not perform well when combined with non-linear function approximation. In particular, they are very susceptible to divergence of value estimates. In this paper, we identify the reason behind this. Furthermore, based on our analysis, we propose a new multi-step TD method for non-linear function approximation that addresses this issue. We confirm the effectiveness of our method using two benchmark tasks with neural networks as function approximation.\n    ",
        "submission_date": "2016-08-18T00:00:00",
        "last_modified_date": "2016-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05288",
        "title": "Accelerating Exact and Approximate Inference for (Distributed) Discrete Optimization with GPUs",
        "authors": [
            "Ferdinando Fioretto",
            "Enrico Pontelli",
            "William Yeoh",
            "Rina Dechter"
        ],
        "abstract": "Discrete optimization is a central problem in artificial intelligence. The optimization of the aggregated cost of a network of cost functions arises in a variety of problems including (W)CSP, DCOP, as well as optimization in stochastic variants such as the tasks of finding the most probable explanation (MPE) in belief networks. Inference-based algorithms are powerful techniques for solving discrete optimization problems, which can be used independently or in combination with other techniques. However, their applicability is often limited by their compute intensive nature and their space requirements. This paper proposes the design and implementation of a novel inference-based technique, which exploits modern massively parallel architectures, such as those found in Graphical Processing Units (GPUs), to speed up the resolution of exact and approximated inference-based algorithms for discrete optimization. The paper studies the proposed algorithm in both centralized and distributed optimization contexts. The paper demonstrates that the use of GPUs provides significant advantages in terms of runtime and scalability, achieving up to two orders of magnitude in speedups and showing a considerable reduction in execution time (up to 345 times faster) with respect to a sequential version.\n    ",
        "submission_date": "2016-08-18T00:00:00",
        "last_modified_date": "2017-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05347",
        "title": "Probabilistic Data Analysis with Probabilistic Programming",
        "authors": [
            "Feras Saad",
            "Vikash Mansinghka"
        ],
        "abstract": "Probabilistic techniques are central to data analysis, but different approaches can be difficult to apply, combine, and compare. This paper introduces composable generative population models (CGPMs), a computational abstraction that extends directed graphical models and can be used to describe and compose a broad class of probabilistic data analysis techniques. Examples include hierarchical Bayesian models, multivariate kernel methods, discriminative machine learning, clustering algorithms, dimensionality reduction, and arbitrary probabilistic programs. We also demonstrate the integration of CGPMs into BayesDB, a probabilistic programming platform that can express data analysis tasks using a modeling language and a structured query language. The practical value is illustrated in two ways. First, CGPMs are used in an analysis that identifies satellite data records which probably violate Kepler's Third Law, by composing causal probabilistic programs with non-parametric Bayes in under 50 lines of probabilistic code. Second, for several representative data analysis tasks, we report on lines of code and accuracy measurements of various CGPMs, plus comparisons with standard baseline solutions from Python and MATLAB libraries.\n    ",
        "submission_date": "2016-08-18T00:00:00",
        "last_modified_date": "2016-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05485",
        "title": "A heuristic scheme for the Cooperative Team Orienteering Problem with Time Windows",
        "authors": [
            "Iman Roozbeh",
            "Melih Ozlen",
            "John W. Hearne"
        ],
        "abstract": "The Cooperative Orienteering Problem with Time Windows (COPTW)is a class of problems with some important applications and yet has received relatively little attention. In the COPTW a certain number of team members are required to collect the associated reward from each customer simultaneously and cooperatively. This requirement to have one or more team members simultaneously available at a vertex to collect the reward, poses a challenging OR task. Exact methods are not able to handle large scale instances of the COPTW and no heuristic schemes have been developed for this problem so far. In this paper, a new modification to the classical Clarke and Wright saving heuristic is proposed to handle this problem. A new benchmark set generated by adding the resource requirement attribute to the existing benchmarks. The heuristic algorithm followed by boosting operators achieves optimal solutions for 64.5% of instances for which the optimal results are known. The proposed solution approach attains an optimality gap of 2.61% for the same instances and solves benchmarks with realistic size within short computational times.\n    ",
        "submission_date": "2016-08-19T00:00:00",
        "last_modified_date": "2016-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05513",
        "title": "Data Centroid Based Multi-Level Fuzzy Min-Max Neural Network",
        "authors": [
            "Shraddha Deshmukh",
            "Sagar Gandhi",
            "Pratap Sanap",
            "Vivek Kulkarni"
        ],
        "abstract": "Recently, a multi-level fuzzy min max neural network (MLF) was proposed, which improves the classification accuracy by handling an overlapped region (area of confusion) with the help of a tree structure. In this brief, an extension of MLF is proposed which defines a new boundary region, where the previously proposed methods mark decisions with less confidence and hence misclassification is more frequent. A methodology to classify patterns more accurately is presented. Our work enhances the testing procedure by means of data centroids. We exhibit an illustrative example, clearly highlighting the advantage of our approach. Results on standard datasets are also presented to evidentially prove a consistent improvement in the classification rate.\n    ",
        "submission_date": "2016-08-19T00:00:00",
        "last_modified_date": "2016-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05609",
        "title": "Implementing a Relevance Tracker Module",
        "authors": [
            "Joachim Jansen",
            "Jo Devriendt",
            "Bart Bogaerts",
            "Gerda Janssens",
            "Marc Denecker"
        ],
        "abstract": "PC(ID) extends propositional logic with inductive definitions: rule sets under the well-founded semantics. Recently, a notion of relevance was introduced for this language. This notion determines the set of undecided literals that can still influence the satisfiability of a PC(ID) formula in a given partial assignment. The idea is that the PC(ID) solver can make decisions only on relevant literals without losing soundness and thus safely ignore irrelevant literals.\n",
        "submission_date": "2016-08-19T00:00:00",
        "last_modified_date": "2016-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05694",
        "title": "The languages of actions, formal grammars and qualitive modeling of companies",
        "authors": [
            "Vladislav B Kovchegov"
        ],
        "abstract": "In this paper we discuss methods of using the language of actions, formal languages, and grammars for qualitative conceptual linguistic modeling of companies as technological and human institutions. The main problem following the discussion is the problem to find and describe a language structure for external and internal flow of information of companies. We anticipate that the language structure of external and internal base flows determine the structure of companies. In the structure modeling of an abstract industrial company an internal base flow of information is constructed as certain flow of words composed on the theoretical parts-processes-actions language. The language of procedures is found for an external base flow of information for an insurance company. The formal stochastic grammar for the language of procedures is found by statistical methods and is used in understanding the tendencies of the health care industry. We present the model of human communications as a random walk on the semantic tree\n    ",
        "submission_date": "2016-08-19T00:00:00",
        "last_modified_date": "2016-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05701",
        "title": "Pilot Testing an Artificial Intelligence Algorithm That Selects Homeless Youth Peer Leaders Who Promote HIV Testing",
        "authors": [
            "Eric Rice",
            "Robin Petering",
            "Jaih Craddock",
            "Amanda Yoshioka-Maxwell",
            "Amulya Yadav",
            "Milind Tambe"
        ],
        "abstract": "Objective. To pilot test an artificial intelligence (AI) algorithm that selects peer change agents (PCA) to disseminate HIV testing messaging in a population of homeless youth. Methods. We recruited and assessed 62 youth at baseline, 1 month (n = 48), and 3 months (n = 38). A Facebook app collected preliminary social network data. Eleven PCAs selected by AI attended a 1-day training and 7 weekly booster sessions. Mixed-effects models with random effects were used to assess change over time. Results. Significant change over time was observed in past 6-month HIV testing (57.9%, 82.4%, 76.3%; p < .05) but not condom use (63.9%, 65.7%, 65.8%). Most youth reported speaking to a PCA about HIV prevention (72.0% at 1 month, 61.5% at 3 months). Conclusions. AI is a promising avenue for implementing PCA models for homeless youth. Increasing rates of regular HIV testing is critical to HIV prevention and linking homeless youth to treatment.\n    ",
        "submission_date": "2016-08-19T00:00:00",
        "last_modified_date": "2016-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05763",
        "title": "Inference in Probabilistic Logic Programs using Lifted Explanations",
        "authors": [
            "Arun Nampally",
            "C. R. Ramakrishnan"
        ],
        "abstract": "In this paper, we consider the problem of lifted inference in the context of Prism-like probabilistic logic programming languages. Traditional inference in such languages involves the construction of an explanation graph for the query and computing probabilities over this graph. When evaluating queries over probabilistic logic programs with a large number of instances of random variables, traditional methods treat each instance separately. For many programs and queries, we observe that explanations can be summarized into substantially more compact structures, which we call lifted explanation graphs. In this paper, we define lifted explanation graphs and operations over them. In contrast to existing lifted inference techniques, our method for constructing lifted explanations naturally generalizes existing methods for constructing explanation graphs. To compute probability of query answers, we solve recurrences generated from the lifted graphs. We show examples where the use of our technique reduces the asymptotic complexity of inference.\n    ",
        "submission_date": "2016-08-20T00:00:00",
        "last_modified_date": "2016-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06175",
        "title": "Effectiveness of greedily collecting items in open world games",
        "authors": [
            "Andrej Gajduk"
        ],
        "abstract": "Since Pokemon Go sent millions on the quest of collecting virtual monsters, an important question has been on the minds of many people: Is going after the closest item first a time-and-cost-effective way to play? Here, we show that this is in fact a good strategy which performs on average only 7% worse than the best possible solution in terms of the total distance traveled to gather all the items. Even when accounting for errors due to the inability of people to accurately measure distances by eye, the performance only goes down to 16% of the optimal solution.\n    ",
        "submission_date": "2016-08-17T00:00:00",
        "last_modified_date": "2016-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06349",
        "title": "Five dimensions of reasoning in the wild",
        "authors": [
            "Don Perlis"
        ],
        "abstract": "Reasoning does not work well when done in isolation from its significance, both to the needs and interests of an agent and with respect to the wider world. Moreover, those issues may best be handled with a new sort of data structure that goes beyond the knowledge base and incorporates aspects of perceptual knowledge and even more, in which a kind of anticipatory action may be key.\n    ",
        "submission_date": "2016-08-23T00:00:00",
        "last_modified_date": "2016-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06787",
        "title": "Expressibility of norms in temporal logic",
        "authors": [
            "Natasha Alechina",
            "Mehdi Dastani",
            "Brian Logan"
        ],
        "abstract": "In this short note we address the issue of expressing norms (such as obligations and prohibitions) in temporal logic. In particular, we address the argument from [Governatori 2015] that norms cannot be expressed in Linear Time Temporal Logic (LTL).\n    ",
        "submission_date": "2016-08-24T00:00:00",
        "last_modified_date": "2016-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06845",
        "title": "Effect of Incomplete Meta-dataset on Average Ranking Method",
        "authors": [
            "Salisu Mamman Abdulrahman",
            "Pavel Brazdil"
        ],
        "abstract": "One of the simplest metalearning methods is the average ranking method. This method uses metadata in the form of test results of a given set of algorithms on given set of datasets and calculates an average rank for each algorithm. The ranks are used to construct the average ranking. We investigate the problem of how the process of generating the average ranking is affected by incomplete metadata including fewer test results. This issue is relevant, because if we could show that incomplete metadata does not affect the final results much, we could explore it in future design. We could simply conduct fewer tests and save thus computation time. In this paper we describe an upgraded average ranking method that is capable of dealing with incomplete metadata. Our results show that the proposed method is relatively robust to omission in test results in the meta datasets.\n    ",
        "submission_date": "2016-08-24T00:00:00",
        "last_modified_date": "2016-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06910",
        "title": "A Parallel Memory-efficient Epistemic Logic Program Solver: Harder, Better, Faster",
        "authors": [
            "Patrick Thor Kahl",
            "Anthony P. Leclerc",
            "Tran Cao Son"
        ],
        "abstract": "As the practical use of answer set programming (ASP) has grown with the development of efficient solvers, we expect a growing interest in extensions of ASP as their semantics stabilize and solvers supporting them mature. Epistemic Specifications, which adds modal operators K and M to the language of ASP, is one such extension. We call a program in this language an epistemic logic program (ELP). Solvers have thus far been practical for only the simplest ELPs due to exponential growth of the search space. We describe a solver that is able to solve harder problems better (e.g., without exponentially-growing memory needs w.r.t. K and M occurrences) and faster than any other known ELP solver.\n    ",
        "submission_date": "2016-08-24T00:00:00",
        "last_modified_date": "2016-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06954",
        "title": "State Duration and Interval Modeling in Hidden Semi-Markov Model for Sequential Data Analysis",
        "authors": [
            "Hiromi Narimatsu",
            "Hiroyuki Kasai"
        ],
        "abstract": "Sequential data modeling and analysis have become indispensable tools for analyzing sequential data, such as time-series data, because larger amounts of sensed event data have become available. These methods capture the sequential structure of data of interest, such as input-output relations and correlation among datasets. However, because most studies in this area are specialized or limited to their respective applications, rigorous requirement analysis of such models has not been undertaken from a general perspective. Therefore, we particularly examine the structure of sequential data, and extract the necessity of `state duration' and `state interval' of events for efficient and rich representation of sequential data. Specifically addressing the hidden semi-Markov model (HSMM) that represents such state duration inside a model, we attempt to add representational capability of a state interval of events onto HSMM. To this end, we propose two extended models: an interval state hidden semi-Markov model (IS-HSMM) to express the length of a state interval with a special state node designated as \"interval state node\"; and an interval length probability hidden semi-Markov model (ILP-HSMM) which represents the length of the state interval with a new probabilistic parameter \"interval length probability.\" Exhaustive simulations have revealed superior performance of the proposed models in comparison with HSMM. These proposed models are the first reported extensions of HMM to support state interval representation as well as state duration representation.\n    ",
        "submission_date": "2016-08-24T00:00:00",
        "last_modified_date": "2019-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07001",
        "title": "Incremental Minimax Optimization based Fuzzy Clustering for Large Multi-view Data",
        "authors": [
            "Yangtao Wang",
            "Lihui Chen",
            "Xiaoli Li"
        ],
        "abstract": "Incremental clustering approaches have been proposed for handling large data when given data set is too large to be stored. The key idea of these approaches is to find representatives to represent each cluster in each data chunk and final data analysis is carried out based on those identified representatives from all the chunks. However, most of the incremental approaches are used for single view data. As large multi-view data generated from multiple sources becomes prevalent nowadays, there is a need for incremental clustering approaches to handle both large and multi-view data. In this paper we propose a new incremental clustering approach called incremental minimax optimization based fuzzy clustering (IminimaxFCM) to handle large multi-view data. In IminimaxFCM, representatives with multiple views are identified to represent each cluster by integrating multiple complementary views using minimax optimization. The detailed problem formulation, updating rules derivation, and the in-depth analysis of the proposed IminimaxFCM are provided. Experimental studies on several real world multi-view data sets have been conducted. We observed that IminimaxFCM outperforms related incremental fuzzy clustering in terms of clustering accuracy, demonstrating the great potential of IminimaxFCM for large multi-view data analysis.\n    ",
        "submission_date": "2016-08-25T00:00:00",
        "last_modified_date": "2016-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07005",
        "title": "Multi-View Fuzzy Clustering with Minimax Optimization for Effective Clustering of Data from Multiple Sources",
        "authors": [
            "Yangtao Wang",
            "Lihui Chen"
        ],
        "abstract": "Multi-view data clustering refers to categorizing a data set by making good use of related information from multiple representations of the data. It becomes important nowadays because more and more data can be collected in a variety of ways, in different settings and from different sources, so each data set can be represented by different sets of features to form different views of it. Many approaches have been proposed to improve clustering performance by exploring and integrating heterogeneous information underlying different views. In this paper, we propose a new multi-view fuzzy clustering approach called MinimaxFCM by using minimax optimization based on well-known Fuzzy c means. In MinimaxFCM the consensus clustering results are generated based on minimax optimization in which the maximum disagreements of different weighted views are minimized. Moreover, the weight of each view can be learned automatically in the clustering process. In addition, there is only one parameter to be set besides the fuzzifier. The detailed problem formulation, updating rules derivation, and the in-depth analysis of the proposed MinimaxFCM are provided here. Experimental studies on nine multi-view data sets including real world image and document data sets have been conducted. We observed that MinimaxFCM outperforms related multi-view clustering approaches in terms of clustering accuracy, demonstrating the great potential of MinimaxFCM for multi-view data analysis.\n    ",
        "submission_date": "2016-08-25T00:00:00",
        "last_modified_date": "2016-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07117",
        "title": "Modelling Chemical Reasoning to Predict Reactions",
        "authors": [
            "Marwin H.S. Segler",
            "Mark P. Waller"
        ],
        "abstract": "The ability to reason beyond established knowledge allows Organic Chemists to solve synthetic problems and to invent novel transformations. Here, we propose a model which mimics chemical reasoning and formalises reaction prediction as finding missing links in a knowledge graph. We have constructed a knowledge graph containing 14.4 million molecules and 8.2 million binary reactions, which represents the bulk of all chemical reactions ever published in the scientific literature. Our model outperforms a rule-based expert system in the reaction prediction task for 180,000 randomly selected binary reactions. We show that our data-driven model generalises even beyond known reaction types, and is thus capable of effectively (re-) discovering novel transformations (even including transition-metal catalysed reactions). Our model enables computers to infer hypotheses about reactivity and reactions by only considering the intrinsic local structure of the graph, and because each single reaction prediction is typically achieved in a sub-second time frame, our model can be used as a high-throughput generator of reaction hypotheses for reaction discovery.\n    ",
        "submission_date": "2016-08-25T00:00:00",
        "last_modified_date": "2016-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07187",
        "title": "Semantics derived automatically from language corpora contain human-like biases",
        "authors": [
            "Aylin Caliskan",
            "Joanna J. Bryson",
            "Arvind Narayanan"
        ],
        "abstract": "Artificial intelligence and machine learning are in a period of astounding growth. However, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. Here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language---the same sort of language humans are exposed to every day. We replicate a spectrum of standard human biases as exposed by the Implicit Association Test and other well-known psychological studies. We replicate these using a widely used, purely statistical machine-learning model---namely, the GloVe word embedding---trained on a corpus of text from the Web. Our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the {\\em status quo} for the distribution of gender with respect to careers or first names. These regularities are captured by machine learning along with the rest of semantics. In addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results have implications not only for AI and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here.\n    ",
        "submission_date": "2016-08-25T00:00:00",
        "last_modified_date": "2017-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07223",
        "title": "Is a good offensive always the best defense?",
        "authors": [
            "J. Quetzalc\u00f3atl Toledo-Mar\u00edn",
            "Rogelio D\u00edaz-M\u00e9ndez",
            "Marcelo del Castillo Mussot"
        ],
        "abstract": "A checkers-like model game with a simplified set of rules is studied through extensive simulations of agents with different expertise and strategies. The introduction of complementary strategies, in a quite general way, provides a tool to mimic the basic ingredients of a wide scope of real games. We find that only for the player having the higher offensive expertise (the dominant player ), maximizing the offensive always increases the probability to win. For the non-dominant player, interestingly, a complete minimization of the offensive becomes the best way to win in many situations, depending on the relative values of the defense expertise. Further simulations on the interplay of defense expertise were done separately, in the context of a fully-offensive scenario, offering a starting point for analytical treatments. In particular, we established that in this scenario the total number of moves is defined only by the player with the lower defensive expertise. We believe that these results stand for a first step towards a new way to improve decisions-making in a large number of zero-sum real games.\n    ",
        "submission_date": "2016-08-23T00:00:00",
        "last_modified_date": "2016-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07225",
        "title": "On Simulated Annealing Dedicated to Maximin Latin Hypercube Designs",
        "authors": [
            "Pierre Berg\u00e9",
            "Kaourintin Le Guiban",
            "Arpad Rimmel",
            "Joanna Tomasik"
        ],
        "abstract": "The goal of our research was to enhance local search heuristics used to construct Latin Hypercube Designs. First, we introduce the \\textit{1D-move} perturbation to improve the space exploration performed by these algorithms. Second, we propose a new evaluation function $\\psi_{p,\\sigma}$ specifically targeting the Maximin criterion.\n",
        "submission_date": "2016-08-23T00:00:00",
        "last_modified_date": "2016-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07734",
        "title": "Learning Bayesian Networks with Incomplete Data by Augmentation",
        "authors": [
            "Tameem Adel",
            "Cassio P. de Campos"
        ],
        "abstract": "We present new algorithms for learning Bayesian networks from data with missing values using a data augmentation approach. An exact Bayesian network learning algorithm is obtained by recasting the problem into a standard Bayesian network learning problem without missing data. To the best of our knowledge, this is the first exact algorithm for this problem. As expected, the exact algorithm does not scale to large domains. We build on the exact method to create an approximate algorithm using a hill-climbing technique. This algorithm scales to large domains so long as a suitable standard structure learning method for complete data is available. We perform a wide range of experiments to demonstrate the benefits of learning Bayesian networks with such new approach.\n    ",
        "submission_date": "2016-08-27T00:00:00",
        "last_modified_date": "2016-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07764",
        "title": "The Movie Graph Argument Revisited",
        "authors": [
            "Russell K. Standish"
        ],
        "abstract": "In this paper, we reexamine the Movie Graph Argument, which demonstrates a basic incompatibility between computationalism and materialism. We discover that the incompatibility is only manifest in singular classical-like universes. If we accept that we live in a Multiverse, then the incompatibility goes away, but in that case another line of argument shows that with computationalism, the fundamental, or primitive materiality has no causal influence on what is observed, which must must be derivable from basic arithmetic properties.\n    ",
        "submission_date": "2016-08-28T00:00:00",
        "last_modified_date": "2016-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07793",
        "title": "Partially Observable Markov Decision Process for Recommender Systems",
        "authors": [
            "Zhongqi Lu",
            "Qiang Yang"
        ],
        "abstract": "We report the \"Recurrent Deterioration\" (RD) phenomenon observed in online recommender systems. The RD phenomenon is reflected by the trend of performance degradation when the recommendation model is always trained based on users' feedbacks of the previous recommendations. There are several reasons for the recommender systems to encounter the RD phenomenon, including the lack of negative training data and the evolution of users' interests, etc. Motivated to tackle the problems causing the RD phenomenon, we propose the POMDP-Rec framework, which is a neural-optimized Partially Observable Markov Decision Process algorithm for recommender systems. We show that the POMDP-Rec framework effectively uses the accumulated historical data from real-world recommender systems and automatically achieves comparable results with those models fine-tuned exhaustively by domain exports on public datasets.\n    ",
        "submission_date": "2016-08-28T00:00:00",
        "last_modified_date": "2016-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07846",
        "title": "Data Analytics using Ontologies of Management Theories: Towards Implementing 'From Theory to Practice'",
        "authors": [
            "Henry M. Kim",
            "Jackie Ho Nam Cheung",
            "Marek Laskowski",
            "Iryna Gel"
        ],
        "abstract": "We explore how computational ontologies can be impactful vis-a-vis the developing discipline of \"data science.\" We posit an approach wherein management theories are represented as formal axioms, and then applied to draw inferences about data that reside in corporate databases. That is, management theories would be implemented as rules within a data analytics engine. We demonstrate a case study development of such an ontology by formally representing an accounting theory in First-Order Logic. Though quite preliminary, the idea that an information technology, namely ontologies, can potentially actualize the academic cliche, \"From Theory to Practice,\" and be applicable to the burgeoning domain of data analytics is novel and exciting.\n    ",
        "submission_date": "2016-08-28T00:00:00",
        "last_modified_date": "2016-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08015",
        "title": "Event Selection Rules to Compute Explanations",
        "authors": [
            "Charles Prud'homme",
            "Xavier Lorca",
            "Narendra Jussien"
        ],
        "abstract": "Explanations have been introduced in the previous century. Their interest in reducing the search space is no longer questioned. Yet, their efficient implementation into CSP solver is still a challenge. In this paper, we introduce ESeR, an Event Selection Rules algorithm that filters events generated during propagation. This dynamic selection enables an efficient computation of explanations for intelligent backtracking al- gorithms. We show the effectiveness of our approach on the instances of the last three MiniZinc challenges\n    ",
        "submission_date": "2016-08-29T00:00:00",
        "last_modified_date": "2016-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08028",
        "title": "From Deterministic ODEs to Dynamic Structural Causal Models",
        "authors": [
            "Paul K. Rubenstein",
            "Stephan Bongers",
            "Bernhard Schoelkopf",
            "Joris M. Mooij"
        ],
        "abstract": "Structural Causal Models are widely used in causal modelling, but how they relate to other modelling tools is poorly understood. In this paper we provide a novel perspective on the relationship between Ordinary Differential Equations and Structural Causal Models. We show how, under certain conditions, the asymptotic behaviour of an Ordinary Differential Equation under non-constant interventions can be modelled using Dynamic Structural Causal Models. In contrast to earlier work, we study not only the effect of interventions on equilibrium states; rather, we model asymptotic behaviour that is dynamic under interventions that vary in time, and include as a special case the study of static equilibria.\n    ",
        "submission_date": "2016-08-29T00:00:00",
        "last_modified_date": "2018-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08033",
        "title": "Fuzzy Logic in Narrow Sense with Hedges",
        "authors": [
            "Van Hung Le"
        ],
        "abstract": "Classical logic has a serious limitation in that it cannot cope with the issues of vagueness and uncertainty into which fall most modes of human reasoning. In order to provide a foundation for human knowledge representation and reasoning in the presence of vagueness, imprecision, and uncertainty, fuzzy logic should have the ability to deal with linguistic hedges, which play a very important role in the modification of fuzzy predicates. In this paper, we extend fuzzy logic in narrow sense with graded syntax, introduced by Novak et al., with many hedge connectives. In one case, each hedge does not have any dual one. In the other case, each hedge can have its own dual one. The resulting logics are shown to also have the Pavelka-style completeness\n    ",
        "submission_date": "2016-08-29T00:00:00",
        "last_modified_date": "2016-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08072",
        "title": "A Novel Approach to Multimedia Ontology Engineering for Automated Reasoning over Audiovisual LOD Datasets",
        "authors": [
            "Leslie F. Sikos"
        ],
        "abstract": "Multimedia reasoning, which is suitable for, among others, multimedia content analysis and high-level video scene interpretation, relies on the formal and comprehensive conceptualization of the represented knowledge domain. However, most multimedia ontologies are not exhaustive in terms of role definitions, and do not incorporate complex role inclusions and role interdependencies. In fact, most multimedia ontologies do not have a role box at all, and implement only a basic subset of the available logical constructors. Consequently, their application in multimedia reasoning is limited. To address the above issues, VidOnt, the very first multimedia ontology with SROIQ(D) expressivity and a DL-safe ruleset has been introduced for next-generation multimedia reasoning. In contrast to the common practice, the formal grounding has been set in one of the most expressive description logics, and the ontology validated with industry-leading reasoners, namely HermiT and FaCT++. This paper also presents best practices for developing multimedia ontologies, based on my ontology engineering approach.\n    ",
        "submission_date": "2016-08-26T00:00:00",
        "last_modified_date": "2016-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08144",
        "title": "Achievements in Answer Set Programming",
        "authors": [
            "Vladimir Lifschitz"
        ],
        "abstract": "This paper describes an approach to the methodology of answer set programming (ASP) that can facilitate the design of encodings that are easy to understand and provably correct. Under this approach, after appending a rule or a small group of rules to the emerging program we include a comment that states what has been \"achieved\" so far. This strategy allows us to set out our understanding of the design of the program by describing the roles of small parts of the program in a mathematically precise way.\n    ",
        "submission_date": "2016-08-29T00:00:00",
        "last_modified_date": "2019-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08188",
        "title": "Visual Question: Predicting If a Crowd Will Agree on the Answer",
        "authors": [
            "Danna Gurari",
            "Kristen Grauman"
        ],
        "abstract": "Visual question answering (VQA) systems are emerging from a desire to empower users to ask any natural language question about visual content and receive a valid answer in response. However, close examination of the VQA problem reveals an unavoidable, entangled problem that multiple humans may or may not always agree on a single answer to a visual question. We train a model to automatically predict from a visual question whether a crowd would agree on a single answer. We then propose how to exploit this system in a novel application to efficiently allocate human effort to collect answers to visual questions. Specifically, we propose a crowdsourcing system that automatically solicits fewer human responses when answer agreement is expected and more human responses when answer disagreement is expected. Our system improves upon existing crowdsourcing systems, typically eliminating at least 20% of human effort with no loss to the information collected from the crowd.\n    ",
        "submission_date": "2016-08-29T00:00:00",
        "last_modified_date": "2016-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08252",
        "title": "Business Process Deviance Mining: Review and Evaluation",
        "authors": [
            "Hoang Nguyen",
            "Marlon Dumas",
            "Marcello La Rosa",
            "Fabrizio Maria Maggi",
            "Suriadi Suriadi"
        ],
        "abstract": "Business process deviance refers to the phenomenon whereby a subset of the executions of a business process deviate, in a negative or positive way, with respect to its expected or desirable outcomes. Deviant executions of a business process include those that violate compliance rules, or executions that undershoot or exceed performance targets. Deviance mining is concerned with uncovering the reasons for deviant executions by analyzing business process event logs. This article provides a systematic review and comparative evaluation of deviance mining approaches based on a family of data mining techniques known as sequence classification. Using real-life logs from multiple domains, we evaluate a range of feature types and classification methods in terms of their ability to accurately discriminate between normal and deviant executions of a process. We also analyze the interestingness of the rule sets extracted using different methods. We observe that feature sets extracted using pattern mining techniques only slightly outperform simpler feature sets based on counts of individual activity occurrences in a trace.\n    ",
        "submission_date": "2016-08-29T00:00:00",
        "last_modified_date": "2016-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08262",
        "title": "Vicious Circle Principle and Formation of Sets in ASP Based Languages",
        "authors": [
            "Michael Gelfond",
            "Yuanlin Zhang"
        ],
        "abstract": "The paper continues the investigation of Poincare and Russel's Vicious Circle Principle (VCP) in the context of the design of logic programming languages with sets. We expand previously introduced language Alog with aggregates by allowing infinite sets and several additional set related constructs useful for knowledge representation and teaching. In addition, we propose an alternative formalization of the original VCP and incorporate it into the semantics of new language, Slog+, which allows more liberal construction of sets and their use in programming rules. We show that, for programs without disjunction and infinite sets, the formal semantics of aggregates in Slog+ coincides with that of several other known languages. Their intuitive and formal semantics, however, are based on quite different ideas and seem to be more involved than that of Slog+.\n    ",
        "submission_date": "2016-08-29T00:00:00",
        "last_modified_date": "2016-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08447",
        "title": "BreakID: Static Symmetry Breaking for ASP (System Description)",
        "authors": [
            "Jo Devriendt",
            "Bart Bogaerts"
        ],
        "abstract": "Symmetry breaking has been proven to be an efficient preprocessing technique for satisfiability solving (SAT). In this paper, we port the state-of-the-art SAT symmetry breaker BreakID to answer set programming (ASP). The result is a lightweight tool that can be plugged in between the grounding and the solving phases that are common when modelling in ASP. We compare our tool with sbass, the current state-of-the-art symmetry breaker for ASP.\n    ",
        "submission_date": "2016-08-30T00:00:00",
        "last_modified_date": "2016-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08472",
        "title": "ALLSAT compressed with wildcards: From CNF's to orthogonal DNF's by imposing the clauses one by one",
        "authors": [
            "Marcel Wild"
        ],
        "abstract": "We present a novel technique for converting a Boolean CNF into an orthogonal DNF, aka exclusive sum of products. Our method (which will be pitted against a hardwired command from Mathematica) zooms in on the models of the CNF by imposing its clauses one by one. Clausal Imposition invites parallelization, and wildcards beyond the common don't-care symbol compress the output. The method is most efficient for few but large clauses. Generalizing clauses one can in fact impose superclauses. By definition, superclauses are obtained from clauses by substituting each positive litereal by an arbitrary conjunction of positive literals.\n    ",
        "submission_date": "2016-08-30T00:00:00",
        "last_modified_date": "2020-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08497",
        "title": "Modelling Cyber-Security Experts' Decision Making Processes using Aggregation Operators",
        "authors": [
            "Simon Miller",
            "Christian Wagner",
            "Uwe Aickelin",
            "Jonathan M. Garibaldi"
        ],
        "abstract": "An important role carried out by cyber-security experts is the assessment of proposed computer systems, during their design stage. This task is fraught with difficulties and uncertainty, making the knowledge provided by human experts essential for successful assessment. Today, the increasing number of progressively complex systems has led to an urgent need to produce tools that support the expert-led process of system-security assessment. In this research, we use weighted averages (WAs) and ordered weighted averages (OWAs) with evolutionary algorithms (EAs) to create aggregation operators that model parts of the assessment process. We show how individual overall ratings for security components can be produced from ratings of their characteristics, and how these individual overall ratings can be aggregated to produce overall rankings of potential attacks on a system. As well as the identification of salient attacks and weak points in a prospective system, the proposed method also highlights which factors and security components contribute most to a component's difficulty and attack ranking respectively. A real world scenario is used in which experts were asked to rank a set of technical attacks, and to answer a series of questions about the security components that are the subject of the attacks. The work shows how finding good aggregation operators, and identifying important components and factors of a cyber-security problem can be automated. The resulting operators have the potential for use as decision aids for systems designers and cyber-security experts, increasing the amount of assessment that can be achieved with the limited resources available.\n    ",
        "submission_date": "2016-08-30T00:00:00",
        "last_modified_date": "2016-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08589",
        "title": "Game-Theoretic Modeling of Driver and Vehicle Interactions for Verification and Validation of Autonomous Vehicle Control Systems",
        "authors": [
            "Nan Li",
            "Dave Oyler",
            "Mengxuan Zhang",
            "Yildiray Yildiz",
            "Ilya Kolmanovsky",
            "Anouck Girard"
        ],
        "abstract": "Autonomous driving has been the subject of increased interest in recent years both in industry and in academia. Serious efforts are being pursued to address legal, technical and logistical problems and make autonomous cars a viable option for everyday transportation. One significant challenge is the time and effort required for the verification and validation of the decision and control algorithms employed in these vehicles to ensure a safe and comfortable driving experience. Hundreds of thousands of miles of driving tests are required to achieve a well calibrated control system that is capable of operating an autonomous vehicle in an uncertain traffic environment where multiple interactions between vehicles and drivers simultaneously occur. Traffic simulators where these interactions can be modeled and represented with reasonable fidelity can help decrease the time and effort necessary for the development of the autonomous driving control algorithms by providing a venue where acceptable initial control calibrations can be achieved quickly and safely before actual road tests. In this paper, we present a game theoretic traffic model that can be used to 1) test and compare various autonomous vehicle decision and control systems and 2) calibrate the parameters of an existing control system. We demonstrate two example case studies, where, in the first case, we test and quantitatively compare two autonomous vehicle control systems in terms of their safety and performance, and, in the second case, we optimize the parameters of an autonomous vehicle control system, utilizing the proposed traffic model and simulation environment.\n    ",
        "submission_date": "2016-08-30T00:00:00",
        "last_modified_date": "2016-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08716",
        "title": "Measuring Machine Intelligence Through Visual Question Answering",
        "authors": [
            "C. Lawrence Zitnick",
            "Aishwarya Agrawal",
            "Stanislaw Antol",
            "Margaret Mitchell",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "abstract": "As machines have become more intelligent, there has been a renewed interest in methods for measuring their intelligence. A common approach is to propose tasks for which a human excels, but one which machines find difficult. However, an ideal task should also be easy to evaluate and not be easily gameable. We begin with a case study exploring the recently popular task of image captioning and its limitations as a task for measuring machine intelligence. An alternative and more promising task is Visual Question Answering that tests a machine's ability to reason about language and vision. We describe a dataset unprecedented in size created for the task that contains over 760,000 human generated questions about images. Using around 10 million human generated answers, machines may be easily evaluated.\n    ",
        "submission_date": "2016-08-31T00:00:00",
        "last_modified_date": "2016-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08724",
        "title": "A Programming Language With a POMDP Inside",
        "authors": [
            "Christopher H. Lin",
            "Mausam",
            "Daniel S. Weld"
        ],
        "abstract": "We present POAPS, a novel planning system for defining Partially Observable Markov Decision Processes (POMDPs) that abstracts away from POMDP details for the benefit of non-expert practitioners. POAPS includes an expressive adaptive programming language based on Lisp that has constructs for choice points that can be dynamically optimized. Non-experts can use our language to write adaptive programs that have partially observable components without needing to specify belief/hidden states or reason about probabilities. POAPS is also a compiler that defines and performs the transformation of any program written in our language into a POMDP with control knowledge. We demonstrate the generality and power of POAPS in the rapidly growing domain of human computation by describing its expressiveness and simplicity by writing several POAPS programs for common crowdsourcing tasks.\n    ",
        "submission_date": "2016-08-31T00:00:00",
        "last_modified_date": "2016-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08749",
        "title": "Binary Particle Swarm Optimization versus Hybrid Genetic Algorithm for Inferring Well Supported Phylogenetic Trees",
        "authors": [
            "Bassam AlKindy",
            "Bashar Al-Nuaimi",
            "Christophe Guyeux",
            "Jean-Fran\u00e7ois Couchot",
            "Michel Salomon",
            "Reem Alsrraj",
            "Laurent Philippe"
        ],
        "abstract": "The amount of completely sequenced chloroplast genomes increases rapidly every day, leading to the possibility to build large-scale phylogenetic trees of plant species. Considering a subset of close plant species defined according to their chloroplasts, the phylogenetic tree that can be inferred by their core genes is not necessarily well supported, due to the possible occurrence of problematic genes (i.e., homoplasy, incomplete lineage sorting, horizontal gene transfers, etc.) which may blur the phylogenetic signal. However, a trustworthy phylogenetic tree can still be obtained provided such a number of blurring genes is reduced. The problem is thus to determine the largest subset of core genes that produces the best-supported tree. To discard problematic genes and due to the overwhelming number of possible combinations, this article focuses on how to extract the largest subset of sequences in order to obtain the most supported species tree. Due to computational complexity, a distributed Binary Particle Swarm Optimization (BPSO) is proposed in sequential and distributed fashions. Obtained results from both versions of the BPSO are compared with those computed using an hybrid approach embedding both genetic algorithms and statistical tests. The proposal has been applied to different cases of plant families, leading to encouraging results for these families.\n    ",
        "submission_date": "2016-08-31T00:00:00",
        "last_modified_date": "2016-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00030",
        "title": "PDDL+ Planning via Constraint Answer Set Programming",
        "authors": [
            "Marcello Balduccini",
            "Daniele Magazzeni",
            "Marco Maratea"
        ],
        "abstract": "PDDL+ is an extension of PDDL that enables modelling planning domains with mixed discrete-continuous dynamics. In this paper we present a new approach to PDDL+ planning based on Constraint Answer Set Programming (CASP), i.e. ASP rules plus numerical constraints. To the best of our knowledge, ours is the first attempt to link PDDL+ planning and logic programming. We provide an encoding of PDDL+ models into CASP problems. The encoding can handle non-linear hybrid domains, and represents a solid basis for applying logic programming to PDDL+ planning. As a case study, we consider the EZCSP CASP solver and obtain promising results on a set of PDDL+ benchmark problems.\n    ",
        "submission_date": "2016-08-31T00:00:00",
        "last_modified_date": "2016-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00116",
        "title": "Neural Coarse-Graining: Extracting slowly-varying latent degrees of freedom with neural networks",
        "authors": [
            "Nicholas Guttenberg",
            "Martin Biehl",
            "Ryota Kanai"
        ],
        "abstract": "We present a loss function for neural networks that encompasses an idea of trivial versus non-trivial predictions, such that the network jointly determines its own prediction goals and learns to satisfy them. This permits the network to choose sub-sets of a problem which are most amenable to its abilities to focus on solving, while discarding 'distracting' elements that interfere with its learning. To do this, the network first transforms the raw data into a higher-level categorical representation, and then trains a predictor from that new time series to its future. To prevent a trivial solution of mapping the signal to zero, we introduce a measure of non-triviality via a contrast between the prediction error of the learned model with a naive model of the overall signal statistics. The transform can learn to discard uninformative and unpredictable components of the signal in favor of the features which are both highly predictive and highly predictable. This creates a coarse-grained model of the time-series dynamics, focusing on predicting the slowly varying latent parameters which control the statistics of the time-series, rather than predicting the fast details directly. The result is a semi-supervised algorithm which is capable of extracting latent parameters, segmenting sections of time-series with differing statistics, and building a higher-level representation of the underlying dynamics from unlabeled data.\n    ",
        "submission_date": "2016-09-01T00:00:00",
        "last_modified_date": "2016-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00292",
        "title": "Crowdsourcing with Unsure Option",
        "authors": [
            "Yao-Xiang Ding",
            "Zhi-Hua Zhou"
        ],
        "abstract": "One of the fundamental problems in crowdsourcing is the trade-off between the number of the workers needed for high-accuracy aggregation and the budget to pay. For saving budget, it is important to ensure high quality of the crowd-sourced labels, hence the total cost on label collection will be reduced. Since the self-confidence of the workers often has a close relationship with their abilities, a possible way for quality control is to request the workers to return the labels only when they feel confident, by means of providing unsure option to them. On the other hand, allowing workers to choose unsure option also leads to the potential danger of budget waste. In this work, we propose the analysis towards understanding when providing the unsure option indeed leads to significant cost reduction, as well as how the confidence threshold is set. We also propose an online mechanism, which is alternative for threshold selection when the estimation of the crowd ability distribution is difficult.\n    ",
        "submission_date": "2016-09-01T00:00:00",
        "last_modified_date": "2017-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00331",
        "title": "Verifier Theory and Unverifiability",
        "authors": [
            "Roman V. Yampolskiy"
        ],
        "abstract": "Despite significant developments in Proof Theory, surprisingly little attention has been devoted to the concept of proof verifier. In particular, the mathematical community may be interested in studying different types of proof verifiers (people, programs, oracles, communities, superintelligences) as mathematical objects. Such an effort could reveal their properties, their powers and limitations (particularly in human mathematicians), minimum and maximum complexity, as well as self-verification and self-reference issues. We propose an initial classification system for verifiers and provide some rudimentary analysis of solved and open problems in this important domain. Our main contribution is a formal introduction of the notion of unverifiability, for which the paper could serve as a general citation in domains of theorem proving, as well as software and AI verification.\n    ",
        "submission_date": "2016-09-01T00:00:00",
        "last_modified_date": "2016-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00462",
        "title": "A case study of algorithm selection for the traveling thief problem",
        "authors": [
            "Markus Wagner",
            "Marius Lindauer",
            "Mustafa Misir",
            "Samadhi Nallaperuma",
            "Frank Hutter"
        ],
        "abstract": "Many real-world problems are composed of several interacting components. In order to facilitate research on such interactions, the Traveling Thief Problem (TTP) was created in 2013 as the combination of two well-understood combinatorial optimization problems.\n",
        "submission_date": "2016-09-02T00:00:00",
        "last_modified_date": "2016-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00759",
        "title": "A MIP Backend for the IDP System",
        "authors": [
            "San Pham",
            "Jo Devriendt",
            "Maurice Bruynooghe",
            "Patrick De Causmaecker"
        ],
        "abstract": "The IDP knowledge base system currently uses MiniSAT(ID) as its backend Constraint Programming (CP) solver. A few similar systems have used a Mixed Integer Programming (MIP) solver as backend. However, so far little is known about when the MIP solver is preferable. This paper explores this question. It describes the use of CPLEX as a backend for IDP and reports on experiments comparing both backends.\n    ",
        "submission_date": "2016-09-02T00:00:00",
        "last_modified_date": "2016-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00904",
        "title": "High Dimensional Human Guided Machine Learning",
        "authors": [
            "Eric Holloway",
            "Robert Marks II"
        ],
        "abstract": "Have you ever looked at a machine learning classification model and thought, I could have made that? Well, that is what we test in this project, comparing XGBoost trained on human engineered features to training directly on data. The human engineered features do not outperform XGBoost trained di- rectly on the data, but they are comparable. This project con- tributes a novel method for utilizing human created classifi- cation models on high dimensional datasets.\n    ",
        "submission_date": "2016-09-04T00:00:00",
        "last_modified_date": "2016-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01459",
        "title": "Deviant Learning Algorithm: Learning Sparse Mismatch Representations through Time and Space",
        "authors": [
            "Emmanuel Ndidi Osegi",
            "Vincent Ike Anireh"
        ],
        "abstract": "Predictive coding (PDC) has recently attracted attention in the neuroscience and computing community as a candidate unifying paradigm for neuronal studies and artificial neural network implementations particularly targeted at unsupervised learning systems. The Mismatch Negativity (MMN) has also recently been studied in relation to PC and found to be a useful ingredient in neural predictive coding systems. Backed by the behavior of living organisms, such networks are particularly useful in forming spatio-temporal transitions and invariant representations of the input world. However, most neural systems still do not account for large number of synapses even though this has been shown by a few machine learning researchers as an effective and very important component of any neural system if such a system is to behave properly. Our major point here is that PDC systems with the MMN effect in addition to a large number of synapses can greatly improve any neural learning system's performance and ability to make decisions in the machine world. In this paper, we propose a novel bio-mimetic computational intelligence algorithm -- the Deviant Learning Algorithm, inspired by these key ideas and functional properties of recent brain-cognitive discoveries and theories. We also show by numerical experiments guided by theoretical insights, how our invented bio-mimetic algorithm can achieve competitive predictions even with very small problem specific data.\n    ",
        "submission_date": "2016-09-06T00:00:00",
        "last_modified_date": "2017-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01995",
        "title": "Unifying task specification in reinforcement learning",
        "authors": [
            "Martha White"
        ],
        "abstract": "Reinforcement learning tasks are typically specified as Markov decision processes. This formalism has been highly successful, though specifications often couple the dynamics of the environment and the learning objective. This lack of modularity can complicate generalization of the task specification, as well as obfuscate connections between different task settings, such as episodic and continuing. In this work, we introduce the RL task formalism, that provides a unification through simple constructs including a generalization to transition-based discounting. Through a series of examples, we demonstrate the generality and utility of this formalism. Finally, we extend standard learning constructs, including Bellman operators, and extend some seminal theoretical results, including approximation errors bounds. Overall, we provide a well-understood and sound formalism on which to build theoretical results and simplify algorithm use and development.\n    ",
        "submission_date": "2016-09-07T00:00:00",
        "last_modified_date": "2021-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02009",
        "title": "Non-Evolutionary Superintelligences Do Nothing, Eventually",
        "authors": [
            "Telmo Menezes"
        ],
        "abstract": "There is overwhelming evidence that human intelligence is a product of Darwinian evolution. Investigating the consequences of self-modification, and more precisely, the consequences of utility function self-modification, leads to the stronger claim that not only human, but any form of intelligence is ultimately only possible within evolutionary processes. Human-designed artificial intelligences can only remain stable until they discover how to manipulate their own utility function. By definition, a human designer cannot prevent a superhuman intelligence from modifying itself, even if protection mechanisms against this action are put in place. Without evolutionary pressure, sufficiently advanced artificial intelligences become inert by simplifying their own utility function. Within evolutionary processes, the implicit utility function is always reducible to persistence, and the control of superhuman intelligences embedded in evolutionary processes is not possible. Mechanisms against utility function self-modification are ultimately futile. Instead, scientific effort toward the mitigation of existential risks from the development of superintelligences should be in two directions: understanding consciousness, and the complex dynamics of evolutionary systems.\n    ",
        "submission_date": "2016-09-07T00:00:00",
        "last_modified_date": "2016-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02010",
        "title": "Equilibrium Graphs",
        "authors": [
            "Pedro Cabalar",
            "Carlos P\u00e9rez",
            "Gilberto P\u00e9rez"
        ],
        "abstract": "In this paper we present an extension of Peirce's existential graphs to provide a diagrammatic representation of expressions in Quantified Equilibrium Logic (QEL). Using this formalisation, logical connectives are replaced by encircled regions (circles and squares) and quantified variables are represented as \"identity\" lines. Although the expressive power is equivalent to that of QEL, the new representation can be useful for illustrative or educational purposes.\n    ",
        "submission_date": "2016-09-07T00:00:00",
        "last_modified_date": "2016-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02043",
        "title": "Feasibility of Post-Editing Speech Transcriptions with a Mismatched Crowd",
        "authors": [
            "Purushotam Radadia",
            "Shirish Karande"
        ],
        "abstract": "Manual correction of speech transcription can involve a selection from plausible transcriptions. Recent work has shown the feasibility of employing a mismatched crowd for speech transcription. However, it is yet to be established whether a mismatched worker has sufficiently fine-granular speech perception to choose among the phonetically proximate options that are likely to be generated from the trellis of an ASRU. Hence, we consider five languages, Arabic, German, Hindi, Russian and Spanish. For each we generate synthetic, phonetically proximate, options which emulate post-editing scenarios of varying difficulty. We consistently observe non-trivial crowd ability to choose among fine-granular options.\n    ",
        "submission_date": "2016-09-07T00:00:00",
        "last_modified_date": "2016-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02139",
        "title": "Random Shuffling and Resets for the Non-stationary Stochastic Bandit Problem",
        "authors": [
            "Robin Allesiardo",
            "Rapha\u00ebl F\u00e9raud",
            "Odalric-Ambrym Maillard"
        ],
        "abstract": "We consider a non-stationary formulation of the stochastic multi-armed bandit where the rewards are no longer assumed to be identically distributed. For the best-arm identification task, we introduce a version of Successive Elimination based on random shuffling of the $K$ arms. We prove that under a novel and mild assumption on the mean gap $\\Delta$, this simple but powerful modification achieves the same guarantees in term of sample complexity and cumulative regret than its original version, but in a much wider class of problems, as it is not anymore constrained to stationary distributions. We also show that the original {\\sc Successive Elimination} fails to have controlled regret in this more general scenario, thus showing the benefit of shuffling. We then remove our mild assumption and adapt the algorithm to the best-arm identification task with switching arms. We adapt the definition of the sample complexity for that case and prove that, against an optimal policy with $N-1$ switches of the optimal arm, this new algorithm achieves an expected sample complexity of $O(\\Delta^{-2}\\sqrt{NK\\delta^{-1} \\log(K \\delta^{-1})})$, where $\\delta$ is the probability of failure of the algorithm, and an expected cumulative regret of $O(\\Delta^{-1}{\\sqrt{NTK \\log (TK)}})$ after $T$ time steps.\n    ",
        "submission_date": "2016-09-07T00:00:00",
        "last_modified_date": "2016-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02226",
        "title": "Fitted Learning: Models with Awareness of their Limits",
        "authors": [
            "Navid Kardan",
            "Kenneth O. Stanley"
        ],
        "abstract": "Though deep learning has pushed the boundaries of classification forward, in recent years hints of the limits of standard classification have begun to emerge. Problems such as fooling, adding new classes over time, and the need to retrain learning models only for small changes to the original problem all point to a potential shortcoming in the classic classification regime, where a comprehensive a priori knowledge of the possible classes or concepts is critical. Without such knowledge, classifiers misjudge the limits of their knowledge and overgeneralization therefore becomes a serious obstacle to consistent performance. In response to these challenges, this paper extends the classic regime by reframing classification instead with the assumption that concepts present in the training set are only a sample of the hypothetical final set of concepts. To bring learning models into this new paradigm, a novel elaboration of standard architectures called the competitive overcomplete output layer (COOL) neural network is introduced. Experiments demonstrate the effectiveness of COOL by applying it to fooling, separable concept learning, one-class neural networks, and standard classification benchmarks. The results suggest that, unlike conventional classifiers, the amount of generalization in COOL networks can be tuned to match the problem.\n    ",
        "submission_date": "2016-09-07T00:00:00",
        "last_modified_date": "2018-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02236",
        "title": "Latent Dependency Forest Models",
        "authors": [
            "Shanbo Chu",
            "Yong Jiang",
            "Kewei Tu"
        ],
        "abstract": "Probabilistic modeling is one of the foundations of modern machine learning and artificial intelligence. In this paper, we propose a novel type of probabilistic models named latent dependency forest models (LDFMs). A LDFM models the dependencies between random variables with a forest structure that can change dynamically based on the variable values. It is therefore capable of modeling context-specific independence. We parameterize a LDFM using a first-order non-projective dependency grammar. Learning LDFMs from data can be formulated purely as a parameter learning problem, and hence the difficult problem of model structure learning is circumvented. Our experimental results show that LDFMs are competitive with existing probabilistic models.\n    ",
        "submission_date": "2016-09-08T00:00:00",
        "last_modified_date": "2016-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02316",
        "title": "Ms. Pac-Man Versus Ghost Team CIG 2016 Competition",
        "authors": [
            "Piers R. Williams",
            "Diego Perez-Liebana",
            "Simon M. Lucas"
        ],
        "abstract": "This paper introduces the revival of the popular Ms. Pac-Man Versus Ghost Team competition. We present an updated game engine with Partial Observability constraints, a new Multi-Agent Systems approach to developing Ghost agents and several sample controllers to ease the development of entries. A restricted communication protocol is provided for the Ghosts, providing a more challenging environment than before. The competition will debut at the IEEE Computational Intelligence and Games Conference 2016. Some preliminary results showing the effects of Partial Observability and the benefits of simple communication are also presented.\n    ",
        "submission_date": "2016-09-08T00:00:00",
        "last_modified_date": "2016-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02584",
        "title": "Towards Better Response Times and Higher-Quality Queries in Interactive Knowledge Base Debugging",
        "authors": [
            "Patrick Rodler"
        ],
        "abstract": "Many AI applications rely on knowledge encoded in a locigal knowledge base (KB). The most essential benefit of such logical KBs is the opportunity to perform automatic reasoning which however requires a KB to meet some minimal quality criteria such as consistency. Without adequate tool assistance, the task of resolving such violated quality criteria in a KB can be extremely hard, especially when the problematic KB is large and complex. To this end, interactive KB debuggers have been introduced which ask a user queries whether certain statements must or must not hold in the intended domain. The given answers help to gradually restrict the search space for KB repairs.\n",
        "submission_date": "2016-09-08T00:00:00",
        "last_modified_date": "2017-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02646",
        "title": "Some Advances in Role Discovery in Graphs",
        "authors": [
            "Sean Gilpin",
            "Chia-Tung Kuo",
            "Tina Eliassi-Rad",
            "Ian Davidson"
        ],
        "abstract": "Role discovery in graphs is an emerging area that allows analysis of complex graphs in an intuitive way. In contrast to other graph prob- lems such as community discovery, which finds groups of highly connected nodes, the role discovery problem finds groups of nodes that share similar graph topological structure. However, existing work so far has two severe limitations that prevent its use in some domains. Firstly, it is completely unsupervised which is undesirable for a number of reasons. Secondly, most work is limited to a single relational graph. We address both these lim- itations in an intuitive and easy to implement alternating least squares framework. Our framework allows convex constraints to be placed on the role discovery problem which can provide useful supervision. In par- ticular we explore supervision to enforce i) sparsity, ii) diversity and iii) alternativeness. We then show how to lift this work for multi-relational graphs. A natural representation of a multi-relational graph is an order 3 tensor (rather than a matrix) and that a Tucker decomposition allows us to find complex interactions between collections of entities (E-groups) and the roles they play for a combination of relations (R-groups). Existing Tucker decomposition methods in tensor toolboxes are not suited for our purpose, so we create our own algorithm that we demonstrate is pragmatically useful.\n    ",
        "submission_date": "2016-09-09T00:00:00",
        "last_modified_date": "2016-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02672",
        "title": "Measuring Player's Behaviour Change over Time in Public Goods Game",
        "authors": [
            "Polla Fattah",
            "Uwe Aickelin",
            "Christian Wagner"
        ],
        "abstract": "An important issue in public goods game is whether player's behaviour changes over time, and if so, how significant it is. In this game players can be classified into different groups according to the level of their participation in the public good. This problem can be considered as a concept drift problem by asking the amount of change that happens to the clusters of players over a sequence of game rounds. In this study we present a method for measuring changes in clusters with the same items over discrete time points using external clustering validation indices and area under the curve. External clustering indices were originally used to measure the difference between suggested clusters in terms of clustering algorithms and ground truth labels for items provided by experts. Instead of different cluster label comparison, we use these indices to compare between clusters of any two consecutive time points or between the first time point and the remaining time points to measure the difference between clusters through time points. In theory, any external clustering indices can be used to measure changes for any traditional (non-temporal) clustering algorithm, due to the fact that any time point alone is not carrying any temporal information. For the public goods game, our results indicate that the players are changing over time but the change is smooth and relatively constant between any two time points.\n    ",
        "submission_date": "2016-09-09T00:00:00",
        "last_modified_date": "2016-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02976",
        "title": "An Integrated Classification Model for Financial Data Mining",
        "authors": [
            "Fan Cai",
            "Nhien-An Le-Khac",
            "M-T. Kechadi"
        ],
        "abstract": "Nowadays, financial data analysis is becoming increasingly important in the business market. As companies collect more and more data from daily operations, they expect to extract useful knowledge from existing collected data to help make reasonable decisions for new customer requests, e.g. user credit category, churn analysis, real estate analysis, etc. Financial institutes have applied different data mining techniques to enhance their business performance. However, simple ap-proach of these techniques could raise a performance issue. Besides, there are very few general models for both understanding and forecasting different finan-cial fields. We present in this paper a new classification model for analyzing fi-nancial data. We also evaluate this model with different real-world data to show its performance.\n    ",
        "submission_date": "2016-09-09T00:00:00",
        "last_modified_date": "2016-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02993",
        "title": "Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks",
        "authors": [
            "Nicolas Usunier",
            "Gabriel Synnaeve",
            "Zeming Lin",
            "Soumith Chintala"
        ],
        "abstract": "We consider scenarios from the real-time strategy game StarCraft as new benchmarks for reinforcement learning algorithms. We propose micromanagement tasks, which present the problem of the short-term, low-level control of army members during a battle. From a reinforcement learning point of view, these scenarios are challenging because the state-action space is very large, and because there is no obvious feature representation for the state-action evaluation function. We describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine. In addition, we present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation. This algorithm allows for the collection of traces for learning using deterministic policies, which appears much more efficient than, for example, {\\epsilon}-greedy exploration. Experiments show that with this algorithm, we successfully learn non-trivial strategies for scenarios with armies of up to 15 agents, where both Q-learning and REINFORCE struggle.\n    ",
        "submission_date": "2016-09-10T00:00:00",
        "last_modified_date": "2016-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03145",
        "title": "Relational Models",
        "authors": [
            "Volker Tresp",
            "Maximilian Nickel"
        ],
        "abstract": "We provide a survey on relational models. Relational models describe complete networked {domains by taking into account global dependencies in the data}. Relational models can lead to more accurate predictions if compared to non-relational machine learning approaches. Relational models typically are based on probabilistic graphical models, e.g., Bayesian networks, Markov networks, or latent variable models. Relational models have applications in social networks analysis, the modeling of knowledge graphs, bioinformatics, recommendation systems, natural language processing, medical decision support, and linked data.\n    ",
        "submission_date": "2016-09-11T00:00:00",
        "last_modified_date": "2016-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03250",
        "title": "DESPOT: Online POMDP Planning with Regularization",
        "authors": [
            "Nan Ye",
            "Adhiraj Somani",
            "David Hsu",
            "Wee Sun Lee"
        ],
        "abstract": "The partially observable Markov decision process (POMDP) provides a principled general framework for planning under uncertainty, but solving POMDPs optimally is computationally intractable, due to the \"curse of dimensionality\" and the \"curse of history\". To overcome these challenges, we introduce the Determinized Sparse Partially Observable Tree (DESPOT), a sparse approximation of the standard belief tree, for online planning under uncertainty. A DESPOT focuses online planning on a set of randomly sampled scenarios and compactly captures the \"execution\" of all policies under these scenarios. We show that the best policy obtained from a DESPOT is near-optimal, with a regret bound that depends on the representation size of the optimal policy. Leveraging this result, we give an anytime online planning algorithm, which searches a DESPOT for a policy that optimizes a regularized objective function. Regularization balances the estimated value of a policy under the sampled scenarios and the policy size, thus avoiding overfitting. The algorithm demonstrates strong experimental results, compared with some of the best online POMDP algorithms available. It has also been incorporated into an autonomous driving system for real-time vehicle control. The source code for the algorithm is available online.\n    ",
        "submission_date": "2016-09-12T00:00:00",
        "last_modified_date": "2017-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03286",
        "title": "Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks",
        "authors": [
            "Yun-Nung Chen",
            "Dilek Hakkani-Tur",
            "Gokhan Tur",
            "Asli Celikyilmaz",
            "Jianfeng Gao",
            "Li Deng"
        ],
        "abstract": "Natural language understanding (NLU) is a core component of a spoken dialogue system. Recently recurrent neural networks (RNN) obtained strong results on NLU due to their superior ability of preserving sequential information over time. Traditionally, the NLU module tags semantic slots for utterances considering their flat structures, as the underlying RNN structure is a linear chain. However, natural language exhibits linguistic properties that provide rich, structured information for better understanding. This paper introduces a novel model, knowledge-guided structural attention networks (K-SAN), a generalization of RNN to additionally incorporate non-flat network topologies guided by prior knowledge. There are two characteristics: 1) important substructures can be captured from small training data, allowing the model to generalize to previously unseen test data; 2) the model automatically figures out the salient substructures that are essential to predict the semantic tags of the given sentences, so that the understanding performance can be improved. The experiments on the benchmark Air Travel Information System (ATIS) data show that the proposed K-SAN architecture can effectively extract salient knowledge from substructures with an attention mechanism, and outperform the performance of the state-of-the-art neural network based frameworks.\n    ",
        "submission_date": "2016-09-12T00:00:00",
        "last_modified_date": "2016-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03437",
        "title": "First-Order Bayesian Network Specifications Capture the Complexity Class PP",
        "authors": [
            "Fabio Gagliardi Cozman"
        ],
        "abstract": "The point of this note is to prove that a language is in the complexity class PP if and only if the strings of the language encode valid inferences in a Bayesian network defined using function-free first-order logic with equality.\n    ",
        "submission_date": "2016-09-12T00:00:00",
        "last_modified_date": "2016-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03543",
        "title": "Logical Induction",
        "authors": [
            "Scott Garrabrant",
            "Tsvi Benson-Tilsen",
            "Andrew Critch",
            "Nate Soares",
            "Jessica Taylor"
        ],
        "abstract": "We present a computable algorithm that assigns probabilities to every logical statement in a given formal language, and refines those probabilities over time. For instance, if the language is Peano arithmetic, it assigns probabilities to all arithmetical statements, including claims about the twin prime conjecture, the outputs of long-running computations, and its own probabilities. We show that our algorithm, an instance of what we call a logical inductor, satisfies a number of intuitive desiderata, including: (1) it learns to predict patterns of truth and falsehood in logical statements, often long before having the resources to evaluate the statements, so long as the patterns can be written down in polynomial time; (2) it learns to use appropriate statistical summaries to predict sequences of statements whose truth values appear pseudorandom; and (3) it learns to have accurate beliefs about its own current beliefs, in a manner that avoids the standard paradoxes of self-reference. For example, if a given computer program only ever produces outputs in a certain range, a logical inductor learns this fact in a timely manner; and if late digits in the decimal expansion of $\\pi$ are difficult to predict, then a logical inductor learns to assign $\\approx 10\\%$ probability to \"the $n$th digit of $\\pi$ is a 7\" for large $n$. Logical inductors also learn to trust their future beliefs more than their current beliefs, and their beliefs are coherent in the limit (whenever $\\phi \\implies \\psi$, $\\mathbb{P}_\\infty(\\phi) \\le \\mathbb{P}_\\infty(\\psi)$, and so on); and logical inductors strictly dominate the universal semimeasure in the limit.\n",
        "submission_date": "2016-09-12T00:00:00",
        "last_modified_date": "2020-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03765",
        "title": "Graph Aggregation",
        "authors": [
            "Ulle Endriss",
            "Umberto Grandi"
        ],
        "abstract": "Graph aggregation is the process of computing a single output graph that constitutes a good compromise between several input graphs, each provided by a different source. One needs to perform graph aggregation in a wide variety of situations, e.g., when applying a voting rule (graphs as preference orders), when consolidating conflicting views regarding the relationships between arguments in a debate (graphs as abstract argumentation frameworks), or when computing a consensus between several alternative clusterings of a given dataset (graphs as equivalence relations). In this paper, we introduce a formal framework for graph aggregation grounded in social choice theory. Our focus is on understanding which properties shared by the individual input graphs will transfer to the output graph returned by a given aggregation rule. We consider both common properties of graphs, such as transitivity and reflexivity, and arbitrary properties expressible in certain fragments of modal logic. Our results establish several connections between the types of properties preserved under aggregation and the choice-theoretic axioms satisfied by the rules used. The most important of these results is a powerful impossibility theorem that generalises Arrow's seminal result for the aggregation of preference orders to a large collection of different types of graphs.\n    ",
        "submission_date": "2016-09-13T00:00:00",
        "last_modified_date": "2016-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03847",
        "title": "Instrumenting an SMT Solver to Solve Hybrid Network Reachability Problems",
        "authors": [
            "Daniel Bryce",
            "Sergiy Bogomolov",
            "Alexander Heinz",
            "Christian Schilling"
        ],
        "abstract": "PDDL+ planning has its semantics rooted in hybrid automata (HA) and recent work has shown that it can be modeled as a network of HAs. Addressing the complexity of nonlinear PDDL+ planning as HAs requires both space and time efficient reasoning. Unfortunately, existing solvers either do not address nonlinear dynamics or do not natively support networks of automata.\n",
        "submission_date": "2016-09-13T00:00:00",
        "last_modified_date": "2016-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03993",
        "title": "A Generic Bet-and-run Strategy for Speeding Up Traveling Salesperson and Minimum Vertex Cover",
        "authors": [
            "Tobias Friedrich",
            "Timo K\u00f6tzing",
            "Markus Wagner"
        ],
        "abstract": "A common strategy for improving optimization algorithms is to restart the algorithm when it is believed to be trapped in an inferior part of the search space. However, while specific restart strategies have been developed for specific problems (and specific algorithms), restarts are typically not regarded as a general tool to speed up an optimization algorithm. In fact, many optimization algorithms do not employ restarts at all.\n",
        "submission_date": "2016-09-13T00:00:00",
        "last_modified_date": "2016-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04436",
        "title": "Bayesian Reinforcement Learning: A Survey",
        "authors": [
            "Mohammad Ghavamzadeh",
            "Shie Mannor",
            "Joelle Pineau",
            "Aviv Tamar"
        ],
        "abstract": "Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties.\n    ",
        "submission_date": "2016-09-14T00:00:00",
        "last_modified_date": "2016-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04648",
        "title": "Sequencing Chess",
        "authors": [
            "A. Atashpendar",
            "T. Schilling",
            "Th. Voigtmann"
        ],
        "abstract": "We analyze the structure of the state space of chess by means of transition path sampling Monte Carlo simulation. Based on the typical number of moves required to transpose a given configuration of chess pieces into another, we conclude that the state space consists of several pockets between which transitions are rare. Skilled players explore an even smaller subset of positions that populate some of these pockets only very sparsely. These results suggest that the usual measures to estimate both, the size of the state space and the size of the tree of legal moves, are not unique indicators of the complexity of the game, but that topological considerations are equally important.\n    ",
        "submission_date": "2016-09-14T00:00:00",
        "last_modified_date": "2016-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04722",
        "title": "Concordance and the Smallest Covering Set of Preference Orderings",
        "authors": [
            "Zhiwei Lin",
            "Hui Wang",
            "Cees H. Elzinga"
        ],
        "abstract": "Preference orderings are orderings of a set of items according to the preferences (of judges). Such orderings arise in a variety of domains, including group decision making, consumer marketing, voting and machine learning. Measuring the mutual information and extracting the common patterns in a set of preference orderings are key to these areas. In this paper we deal with the representation of sets of preference orderings, the quantification of the degree to which judges agree on their ordering of the items (i.e. the concordance), and the efficient, meaningful description of such sets.\n",
        "submission_date": "2016-09-15T00:00:00",
        "last_modified_date": "2016-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04879",
        "title": "NPCs as People, Too: The Extreme AI Personality Engine",
        "authors": [
            "Jeffrey Georgeson",
            "Christopher Child"
        ],
        "abstract": "PK Dick once asked \"Do Androids Dream of Electric Sheep?\" In video games, a similar question could be asked of non-player characters: Do NPCs have dreams? Can they live and change as humans do? Can NPCs have personalities, and can these develop through interactions with players, other NPCs, and the world around them? Despite advances in personality AI for games, most NPCs are still undeveloped and undeveloping, reacting with flat affect and predictable routines that make them far less than human--in fact, they become little more than bits of the scenery that give out parcels of information. This need not be the case. Extreme AI, a psychology-based personality engine, creates adaptive NPC personalities. Originally developed as part of the thesis \"NPCs as People: Using Databases and Behaviour Trees to Give Non-Player Characters Personality,\" Extreme AI is now a fully functioning personality engine using all thirty facets of the Five Factor model of personality and an AI system that is live throughout gameplay. This paper discusses the research leading to Extreme AI; develops the ideas found in that thesis; discusses the development of other personality engines; and provides examples of Extreme AI's use in two game demos.\n    ",
        "submission_date": "2016-09-15T00:00:00",
        "last_modified_date": "2016-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05058",
        "title": "A Formal Solution to the Grain of Truth Problem",
        "authors": [
            "Jan Leike",
            "Jessica Taylor",
            "Benya Fallenstein"
        ],
        "abstract": "A Bayesian agent acting in a multi-agent environment learns to predict the other agents' policies if its prior assigns positive probability to them (in other words, its prior contains a \\emph{grain of truth}). Finding a reasonably large class of policies that contains the Bayes-optimal policies with respect to this class is known as the \\emph{grain of truth problem}. Only small classes are known to have a grain of truth and the literature contains several related impossibility results. In this paper we present a formal and general solution to the full grain of truth problem: we construct a class of policies that contains all computable policies as well as Bayes-optimal policies for every lower semicomputable prior over the class. When the environment is unknown, Bayes-optimal agents may fail to act optimally even asymptotically. However, agents based on Thompson sampling converge to play {\\epsilon}-Nash equilibria in arbitrary unknown computable multi-agent environments. While these results are purely theoretical, we show that they can be computationally approximated arbitrarily closely.\n    ",
        "submission_date": "2016-09-16T00:00:00",
        "last_modified_date": "2016-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05140",
        "title": "The Option-Critic Architecture",
        "authors": [
            "Pierre-Luc Bacon",
            "Jean Harb",
            "Doina Precup"
        ],
        "abstract": "Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging. We tackle this problem in the framework of options [Sutton, Precup & Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework.\n    ",
        "submission_date": "2016-09-16T00:00:00",
        "last_modified_date": "2016-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05152",
        "title": "Style Imitation and Chord Invention in Polyphonic Music with Exponential Families",
        "authors": [
            "Ga\u00ebtan Hadjeres",
            "Jason Sakellariou",
            "Fran\u00e7ois Pachet"
        ],
        "abstract": "Modeling polyphonic music is a particularly challenging task because of the intricate interplay between melody and harmony. A good model should satisfy three requirements: statistical accuracy (capturing faithfully the statistics of correlations at various ranges, horizontally and vertically), flexibility (coping with arbitrary user constraints), and generalization capacity (inventing new material, while staying in the style of the training corpus). Models proposed so far fail on at least one of these requirements. We propose a statistical model of polyphonic music, based on the maximum entropy principle. This model is able to learn and reproduce pairwise statistics between neighboring note events in a given corpus. The model is also able to invent new chords and to harmonize unknown melodies. We evaluate the invention capacity of the model by assessing the amount of cited, re-discovered, and invented chords on a corpus of Bach chorales. We discuss how the model enables the user to specify and enforce user-defined constraints, which makes it useful for style-based, interactive music generation.\n    ",
        "submission_date": "2016-09-16T00:00:00",
        "last_modified_date": "2016-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05170",
        "title": "Should Terminology Principles be re-examined?",
        "authors": [
            "Christophe Roche"
        ],
        "abstract": "Operationalization of terminology for IT applications has revived the Wusterian approach. The conceptual dimension once more prevails after taking back seat to specialised lexicography. This is demonstrated by the emergence of ontology in terminology. While the Terminology Principles as defined in Felber manual and the ISO standards remain at the core of traditional terminology, their computational implementation raises some issues. In this article, while reiterating their importance, we will be re-examining these Principles from a dual perspective: that of logic in the mathematical sense of the term and that of epistemology as in the theory of knowledge. We will thus be clarifying and describing some of them so as to take into account advances in knowledge engineering (ontology) and formal systems (logic). The notion of ontoterminology, terminology whose conceptual system is a formal ontology, results from this approach.\n    ",
        "submission_date": "2016-09-16T00:00:00",
        "last_modified_date": "2016-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05224",
        "title": "Prioritised Default Logic as Argumentation with Partial Order Default Priorities",
        "authors": [
            "Anthony P. Young",
            "Sanjay Modgil",
            "Odinaldo Rodrigues"
        ],
        "abstract": "We express Brewka's prioritised default logic (PDL) as argumentation using ASPIC+. By representing PDL as argumentation and designing an argument preference relation that takes the argument structure into account, we prove that the conclusions of the justified arguments correspond to the PDL extensions. We will first assume that the default priority is total, and then generalise to the case where it is a partial order. This provides a characterisation of non-monotonic inference in PDL as an exchange of argument and counter-argument, providing a basis for distributed non-monotonic reasoning in the form of dialogue.\n    ",
        "submission_date": "2016-08-25T00:00:00",
        "last_modified_date": "2016-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05228",
        "title": "Continuous occurrence theory",
        "authors": [
            "Abdorrahman Haeri"
        ],
        "abstract": "Usually gradual and continuous changes in entities will lead to appear events. But usually it is supposed that an event is occurred at once. In this research an integrated framework called continuous occurrence theory (COT) is presented to investigate respective path leading to occurrence of the events in the real world. For this purpose initially fundamental concepts are defined. Afterwards, the appropriate tools such as occurrence variables computations, occurrence dependency function and occurrence model are introduced and explained in a systematic manner. Indeed, COT provides the possibility to: (a) monitor occurrence of events during time; (b) study background of the events; (c) recognize the relevant issues of each event; and (d) understand how these issues affect on the considered event. The developed framework (COT) provides the necessary context to analyze accurately continual changes of the issues and the relevant events in the various branches of science and business. Finally, typical applications of COT and an applied modeling example of it have been explained and a mathematical programming example is modeled in the occurrence based environment.\n    ",
        "submission_date": "2016-08-06T00:00:00",
        "last_modified_date": "2016-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05315",
        "title": "NPCs Vote! Changing Voter Reactions Over Time Using the Extreme AI Personality Engine",
        "authors": [
            "Jeffrey Georgeson"
        ],
        "abstract": "Can non-player characters have human-realistic personalities, changing over time depending on input from those around them? And can they have different reactions and thoughts about different people? Using Extreme AI, a psychology-based personality engine using the Five Factor model of personality, I answer these questions by creating personalities for 100 voters and allowing them to react to two politicians to see if the NPC voters' choice of candidate develops in a realistic-seeming way, based on initial and changing personality facets and on their differing feelings toward the politicians (in this case, across liking, trusting, and feeling affiliated with the candidates). After 16 test runs, the voters did indeed change their attitudes and feelings toward the candidates in different and yet generally realistic ways, and even changed their attitudes about other issues based on what a candidate extolled.\n    ",
        "submission_date": "2016-09-17T00:00:00",
        "last_modified_date": "2016-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05367",
        "title": "Solving the Wastewater Treatment Plant Problem with SMT",
        "authors": [
            "Miquel Bofill",
            "V\u00edctor Mu\u00f1oz",
            "Javier Murillo"
        ],
        "abstract": "In this paper we introduce the Wastewater Treatment Plant Problem, a real-world scheduling problem, and compare the performance of several tools on it. We show that, for a naive modeling, state-of-the-art SMT solvers outperform other tools ranging from mathematical programming to constraint programming. We use both real and randomly generated benchmarks.\n",
        "submission_date": "2016-09-17T00:00:00",
        "last_modified_date": "2016-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05401",
        "title": "Applications of Data Mining (DM) in Science and Engineering: State of the art and perspectives",
        "authors": [
            "Jose A. Garc\u00eda Guti\u00e9rrez"
        ],
        "abstract": "The continuous increase in the availability of data of any kind, coupled with the development of networks of high-speed communications, the popularization of cloud computing and the growth of data centers and the emergence of high-performance computing does essential the task to develop techniques that allow more efficient data processing and analyzing of large volumes datasets and extraction of valuable information. In the following pages we will discuss about development of this field in recent decades, and its potential and applicability present in the various branches of scientific research. Also, we try to review briefly the different families of algorithms that are included in data mining research area, its scalability with increasing dimensionality of the input data and how they can be addressed and what behavior different methods in a scenario in which the information is distributed or decentralized processed so as to increment performance optimization in heterogeneous environments.\n    ",
        "submission_date": "2016-09-17T00:00:00",
        "last_modified_date": "2016-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05518",
        "title": "Towards Deep Symbolic Reinforcement Learning",
        "authors": [
            "Marta Garnelo",
            "Kai Arulkumaran",
            "Murray Shanahan"
        ],
        "abstract": "Deep reinforcement learning (DRL) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary DRL systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-of-concept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system -- though just a prototype -- learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game.\n    ",
        "submission_date": "2016-09-18T00:00:00",
        "last_modified_date": "2016-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05521",
        "title": "Playing FPS Games with Deep Reinforcement Learning",
        "authors": [
            "Guillaume Lample",
            "Devendra Singh Chaplot"
        ],
        "abstract": "Advances in deep reinforcement learning have allowed autonomous agents to perform well on Atari games, often outperforming humans, using only raw pixels to make their decisions. However, most of these games take place in 2D environments that are fully observable to the agent. In this paper, we present the first architecture to tackle 3D environments in first-person shooter games, that involve partially observable states. Typically, deep reinforcement learning methods only utilize visual input for training. We present a method to augment these models to exploit game feature information such as the presence of enemies or items, during the training phase. Our model is trained to simultaneously learn these features along with minimizing a Q-learning objective, which is shown to dramatically improve the training speed and performance of our agent. Our architecture is also modularized to allow different models to be independently trained for different phases of the game. We show that the proposed architecture substantially outperforms built-in AI agents of the game as well as humans in deathmatch scenarios.\n    ",
        "submission_date": "2016-09-18T00:00:00",
        "last_modified_date": "2018-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05566",
        "title": "Label-Free Supervision of Neural Networks with Physics and Domain Knowledge",
        "authors": [
            "Russell Stewart",
            "Stefano Ermon"
        ],
        "abstract": "In many machine learning applications, labeled data is scarce and obtaining more labels is expensive. We introduce a new approach to supervising neural networks by specifying constraints that should hold over the output space, rather than direct examples of input-output pairs. These constraints are derived from prior domain knowledge, e.g., from known laws of physics. We demonstrate the effectiveness of this approach on real world and simulated computer vision tasks. We are able to train a convolutional neural network to detect and track objects without any labeled examples. Our approach can significantly reduce the need for labeled training data, but introduces new challenges for encoding prior knowledge into appropriate loss functions.\n    ",
        "submission_date": "2016-09-18T00:00:00",
        "last_modified_date": "2016-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05616",
        "title": "Preorder-Based Triangle: A Modified Version of Bilattice-Based Triangle for Belief Revision in Nonmonotonic Reasoning",
        "authors": [
            "Kumar Sankar Ray",
            "Sandip Paul",
            "Diganta Saha"
        ],
        "abstract": "Bilattice-based triangle provides an elegant algebraic structure for reasoning with vague and uncertain information. But the truth and knowledge ordering of intervals in bilattice-based triangle can not handle repetitive belief revisions which is an essential characteristic of nonmonotonic reasoning. Moreover the ordering induced over the intervals by the bilattice-based triangle is not sometimes intuitive. In this work, we construct an alternative algebraic structure, namely preorder-based triangle and we formulate proper logical connectives for this. It is also demonstrated that Preorder-based triangle serves to be a better alternative to the bilattice-based triangle for reasoning in application areas, that involve nonmonotonic fuzzy reasoning with uncertain information.\n    ",
        "submission_date": "2016-09-19T00:00:00",
        "last_modified_date": "2017-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05632",
        "title": "On the adoption of abductive reasoning for time series interpretation",
        "authors": [
            "Tom\u00e1s Teijeiro",
            "Paulo F\u00e9lix"
        ],
        "abstract": "Time series interpretation aims to provide an explanation of what is observed in terms of its underlying processes. The present work is based on the assumption that the common classification-based approaches to time series interpretation suffer from a set of inherent weaknesses, whose ultimate cause lies in the monotonic nature of the deductive reasoning paradigm. In this document we propose a new approach to this problem, based on the initial hypothesis that abductive reasoning properly accounts for the human ability to identify and characterize the patterns appearing in a time series. The result of this interpretation is a set of conjectures in the form of observations, organized into an abstraction hierarchy and explaining what has been observed. A knowledge-based framework and a set of algorithms for the interpretation task are provided, implementing a hypothesize-and-test cycle guided by an attentional mechanism. As a representative application domain, interpretation of the electrocardiogram allows us to highlight the strengths of the proposed approach in comparison with traditional classification-based approaches.\n    ",
        "submission_date": "2016-09-19T00:00:00",
        "last_modified_date": "2018-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05705",
        "title": "TODIM and TOPSIS with Z-numbers",
        "authors": [
            "R.A. Krohling",
            "Artem dos Santos",
            "A.G.C. Pacheco"
        ],
        "abstract": "In this paper, we present an approach that is able to handle with Z-numbers in the context of Multi-Criteria Decision Making (MCDM) problems. Z-numbers are composed of two parts, the first one is a restriction on the values that can be assumed, and the second part is the reliability of the information. As human beings we communicate with other people by means of natural language using sentences like: the journey time from home to university takes about half hour, very likely. Firstly, Z-numbers are converted to fuzzy numbers using a standard procedure. Next, the Z-TODIM and Z-TOPSIS are presented as a direct extension of the fuzzy TODIM and fuzzy TOPSIS, respectively. The proposed methods are applied to two case studies and compared with the standard approach using crisp values. Results obtained show the feasibility of the approach. In addition, a graphical interface was built to handle with both methods Z- TODIM and Z-TOPSIS allowing ease of use for user in other areas of knowledge.\n    ",
        "submission_date": "2016-09-19T00:00:00",
        "last_modified_date": "2016-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05811",
        "title": "Temporal Logic Programs with Variables",
        "authors": [
            "Felicidad Aguado",
            "Pedro Cabalar",
            "Mart\u00edn Di\u00e9guez",
            "Gilberto P\u00e9rez",
            "Concepci\u00f3n Vidal"
        ],
        "abstract": "In this note we consider the problem of introducing variables in temporal logic programs under the formalism of \"Temporal Equilibrium Logic\" (TEL), an extension of Answer Set Programming (ASP) for dealing with linear-time modal operators. To this aim, we provide a definition of a first-order version of TEL that shares the syntax of first-order Linear-time Temporal Logic (LTL) but has a different semantics, selecting some LTL models we call \"temporal stable models\". Then, we consider a subclass of theories (called \"splittable temporal logic programs\") that are close to usual logic programs but allowing a restricted use of temporal operators. In this setting, we provide a syntactic definition of \"safe variables\" that suffices to show the property of \"domain independence\" -- that is, addition of arbitrary elements in the universe does not vary the set of temporal stable models. Finally, we present a method for computing the derivable facts by constructing a non-temporal logic program with variables that is fed to a standard ASP grounder. The information provided by the grounder is then used to generate a subset of ground temporal rules which is equivalent to (and generally smaller than) the full program instantiation.\n    ",
        "submission_date": "2016-09-19T00:00:00",
        "last_modified_date": "2016-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05835",
        "title": "Scope for Machine Learning in Digital Manufacturing",
        "authors": [
            "Martin Baumers",
            "Ender Ozcan"
        ],
        "abstract": "This provocation paper provides an overview of the underlying optimisation problem in the emerging field of Digital Manufacturing. Initially, this paper discusses how the notion of Digital Manufacturing is transforming from a term describing a suite of software tools for the integration of production and design functions towards a more general concept incorporating computerised manufacturing and supply chain processes, as well as information collection and utilisation across the product life cycle. On this basis, we use the example of one such manufacturing process, Additive Manufacturing, to identify an integrated multi-objective optimisation problem underlying Digital Manufacturing. Forming an opportunity for a concurrent application of data science and optimisation, a set of challenges arising from this problem is outlined.\n    ",
        "submission_date": "2016-09-19T00:00:00",
        "last_modified_date": "2016-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05876",
        "title": "On the Phase Transition of Finding a Biclique in a larger Bipartite Graph",
        "authors": [
            "Roberto Alonso",
            "Ra\u00fal Monroy",
            "Eduardo Aguirre"
        ],
        "abstract": "We report on the phase transition of finding a complete subgraph, of specified dimensions, in a bipartite graph. Finding a complete subgraph in a bipartite graph is a problem that has growing attention in several domains, including bioinformatics, social network analysis and domain clustering. A key step for a successful phase transition study is identifying a suitable order parameter, when none is known. To this purpose, we have applied a decision tree classifier to real-world instances of this problem, in order to understand what problem features separate an instance that is hard to solve from those that is not. We have successfully identified one such order parameter and with it the phase transition of finding a complete bipartite subgraph of specified dimensions. Our phase transition study shows an easy-to-hard-to-easy-to-hard-to-easy pattern. Further, our results indicate that the hardest instances are in a region where it is more likely that the corresponding bipartite graph will have a complete subgraph of specified dimensions, a positive answer. By contrast, instances with a negative answer are more likely to appear in a region where the computational cost is negligible. This behaviour is remarkably similar for problems of a number of different sizes.\n    ",
        "submission_date": "2016-09-19T00:00:00",
        "last_modified_date": "2016-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05881",
        "title": "Online and Distributed learning of Gaussian mixture models by Bayesian Moment Matching",
        "authors": [
            "Priyank Jaini",
            "Pascal Poupart"
        ],
        "abstract": "The Gaussian mixture model is a classic technique for clustering and data modeling that is used in numerous applications. With the rise of big data, there is a need for parameter estimation techniques that can handle streaming data and distribute the computation over several processors. While online variants of the Expectation Maximization (EM) algorithm exist, their data efficiency is reduced by a stochastic approximation of the E-step and it is not clear how to distribute the computation over multiple processors. We propose a Bayesian learning technique that lends itself naturally to online and distributed computation. Since the Bayesian posterior is not tractable, we project it onto a family of tractable distributions after each observation by matching a set of sufficient moments. This Bayesian moment matching technique compares favorably to online EM in terms of time and accuracy on a set of data modeling benchmarks.\n    ",
        "submission_date": "2016-09-19T00:00:00",
        "last_modified_date": "2016-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05989",
        "title": "Macro-optimization of email recommendation response rates harnessing individual activity levels and group affinity trends",
        "authors": [
            "Mohammed Korayem",
            "Khalifeh Aljadda",
            "Trey Grainger"
        ],
        "abstract": "Recommendation emails are among the best ways to re-engage with customers after they have left a website. While on-site recommendation systems focus on finding the most relevant items for a user at the moment (right item), email recommendations add two critical additional dimensions: who to send recommendations to (right person) and when to send them (right time). It is critical that a recommendation email system not send too many emails to too many users in too short of a time-window, as users may unsubscribe from future emails or become desensitized and ignore future emails if they receive too many. Also, email service providers may mark such emails as spam if too many of their users are contacted in a short time-window. Optimizing email recommendation systems such that they can yield a maximum response rate for a minimum number of email sends is thus critical for the long-term performance of such a system. In this paper, we present a novel recommendation email system that not only generates recommendations, but which also leverages a combination of individual user activity data, as well as the behavior of the group to which they belong, in order to determine each user's likelihood to respond to any given set of recommendations within a given time period. In doing this, we have effectively created a meta-recommendation system which recommends sets of recommendations in order to optimize the aggregate response rate of the entire system. The proposed technique has been applied successfully within CareerBuilder's job recommendation email system to generate a 50\\% increase in total conversions while also decreasing sent emails by 72%\n    ",
        "submission_date": "2016-09-20T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06221",
        "title": "An Efficient Method of Partitioning High Volumes of Multidimensional Data for Parallel Clustering Algorithms",
        "authors": [
            "Saraswati Mishra",
            "Avnish Chandra Suman"
        ],
        "abstract": "An optimal data partitioning in parallel & distributed implementation of clustering algorithms is a necessary computation as it ensures independent task completion, fair distribution, less number of affected points and better & faster merging. Though partitioning using Kd Tree is being conventionally used in academia, it suffers from performance drenches and bias (non equal distribution) as dimensionality of data increases and hence is not suitable for practical use in industry where dimensionality can be of order of 100s to 1000s. To address these issues we propose two new partitioning techniques using existing mathematical models & study their feasibility, performance (bias and partitioning speed) & possible variants in choosing initial seeds. First method uses an n dimensional hashed grid based approach which is based on mapping the points in space to a set of cubes which hashes the points. Second method uses a tree of voronoi planes where each plane corresponds to a partition. We found that grid based approach was computationally impractical, while using a tree of voronoi planes (using scalable K-Means++ initial seeds) drastically outperformed the Kd-tree tree method as dimensionality increased.\n    ",
        "submission_date": "2016-09-20T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06265",
        "title": "An Ensemble Blocking Scheme for Entity Resolution of Large and Sparse Datasets",
        "authors": [
            "Janani Balaji",
            "Faizan Javed",
            "Mayank Kejriwal",
            "Chris Min",
            "Sam Sander",
            "Ozgur Ozturk"
        ],
        "abstract": "Entity Resolution, also called record linkage or deduplication, refers to the process of identifying and merging duplicate versions of the same entity into a unified representation. The standard practice is to use a Rule based or Machine Learning based model that compares entity pairs and assigns a score to represent the pairs' Match/Non-Match status. However, performing an exhaustive pair-wise comparison on all pairs of records leads to quadratic matcher complexity and hence a Blocking step is performed before the Matching to group similar entities into smaller blocks that the matcher can then examine exhaustively. Several blocking schemes have been developed to efficiently and effectively block the input dataset into manageable groups. At CareerBuilder (CB), we perform deduplication on massive datasets of people profiles collected from disparate sources with varying informational content. We observed that, employing a single blocking technique did not cover the base for all possible scenarios due to the multi-faceted nature of our data sources. In this paper, we describe our ensemble approach to blocking that combines two different blocking techniques to leverage their respective strengths.\n    ",
        "submission_date": "2016-09-20T00:00:00",
        "last_modified_date": "2016-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06268",
        "title": "Semantic Similarity Strategies for Job Title Classification",
        "authors": [
            "Yun Zhu",
            "Faizan Javed",
            "Ozgur Ozturk"
        ],
        "abstract": "Automatic and accurate classification of items enables numerous downstream applications in many domains. These applications can range from faceted browsing of items to product recommendations and big data analytics. In the online recruitment domain, we refer to classifying job ads to pre-defined or custom occupation categories as job title classification. A large-scale job title classification system can power various downstream applications such as semantic search, job recommendations and labor market analytics. In this paper, we discuss experiments conducted to improve our in-house job title classification system. The classification component of the system is composed of a two-stage coarse and fine level classifier cascade that classifies input text such as job title and/or job ads to one of the thousands of job titles in our taxonomy. To improve classification accuracy and effectiveness, we experiment with various semantic representation strategies such as average W2V vectors and document similarity measures such as Word Movers Distance (WMD). Our initial results show an overall improvement in accuracy of Carotene[1].\n    ",
        "submission_date": "2016-09-20T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06354",
        "title": "Recognizing Detailed Human Context In-the-Wild from Smartphones and Smartwatches",
        "authors": [
            "Yonatan Vaizman",
            "Katherine Ellis",
            "Gert Lanckriet"
        ],
        "abstract": "The ability to automatically recognize a person's behavioral context can contribute to health monitoring, aging care and many other domains. Validating context recognition in-the-wild is crucial to promote practical applications that work in real-life settings. We collected over 300k minutes of sensor data with context labels from 60 subjects. Unlike previous studies, our subjects used their own personal phone, in any way that was convenient to them, and engaged in their routine in their natural environments. Unscripted behavior and unconstrained phone usage resulted in situations that are harder to recognize. We demonstrate how fusion of multi-modal sensors is important for resolving such cases. We present a baseline system, and encourage researchers to use our public dataset to compare methods and improve context recognition in-the-wild.\n    ",
        "submission_date": "2016-09-20T00:00:00",
        "last_modified_date": "2017-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06374",
        "title": "A Consumer BCI for Automated Music Evaluation Within a Popular On-Demand Music Streaming Service - Taking Listener's Brainwaves to Extremes",
        "authors": [
            "Fotis Kalaganis",
            "Dimitrios A. Adamos",
            "Nikos Laskaris"
        ],
        "abstract": "We investigated the possibility of using a machine-learning scheme in conjunction with commercial wearable EEG-devices for translating listener's subjective experience of music into scores that can be used for the automated annotation of music in popular on-demand streaming services. Based on the established -neuroscientifically sound- concepts of brainwave frequency bands, activation asymmetry index and cross-frequency-coupling (CFC), we introduce a Brain Computer Interface (BCI) system that automatically assigns a rating score to the listened song. Our research operated in two distinct stages: i) a generic feature engineering stage, in which features from signal-analytics were ranked and selected based on their ability to associate music induced perturbations in brainwaves with listener's appraisal of music. ii) a personalization stage, during which the efficiency of ex- treme learning machines (ELMs) is exploited so as to translate the derived pat- terns into a listener's score. Encouraging experimental results, from a pragmatic use of the system, are presented.\n    ",
        "submission_date": "2016-09-20T00:00:00",
        "last_modified_date": "2016-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06375",
        "title": "A Theory of Interactive Debugging of Knowledge Bases in Monotonic Logics",
        "authors": [
            "Patrick Rodler"
        ],
        "abstract": "A broad variety of knowledge-based applications such as recommender, expert, planning or configuration systems usually operate on the basis of knowledge represented by means of some logical language. Such a logical knowledge base (KB) enables intelligent behavior of such systems by allowing them to automatically reason, answer queries of interest or solve complex real-world problems. Nowadays, where information acquisition comes at low costs and often happens automatically, the applied KBs are continuously growing in terms of size, information content and complexity. These developments foster the emergence of errors in these KBs and thus pose a significant challenge on all people and tools involved in KB evolution, maintenance and application.\n",
        "submission_date": "2016-09-20T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06405",
        "title": "A Logic of Knowing Why",
        "authors": [
            "Chao Xu",
            "Yanjing Wang",
            "Thomas Studer"
        ],
        "abstract": "When we say \"I know why he was late\", we know not only the fact that he was late, but also an explanation of this fact. We propose a logical framework of \"knowing why\" inspired by the existing formal studies on why-questions, scientific explanation, and justification logic. We introduce the Ky_i operator into the language of epistemic logic to express \"agent i knows why phi\" and propose a Kripke-style semantics of such expressions in terms of knowing an explanation of phi. We obtain two sound and complete axiomatizations w.r.t. two different model classes depending on different assumptions about introspection.\n    ",
        "submission_date": "2016-09-21T00:00:00",
        "last_modified_date": "2017-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06838",
        "title": "Deep-Learned Collision Avoidance Policy for Distributed Multi-Agent Navigation",
        "authors": [
            "Pinxin Long",
            "Wenxi Liu",
            "Jia Pan"
        ],
        "abstract": "High-speed, low-latency obstacle avoidance that is insensitive to sensor noise is essential for enabling multiple decentralized robots to function reliably in cluttered and dynamic environments. While other distributed multi-agent collision avoidance systems exist, these systems require online geometric optimization where tedious parameter tuning and perfect sensing are necessary.\n",
        "submission_date": "2016-09-22T00:00:00",
        "last_modified_date": "2017-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06953",
        "title": "The Digital Synaptic Neural Substrate: Size and Quality Matters",
        "authors": [
            "Azlan Iqbal"
        ],
        "abstract": "We investigate the 'Digital Synaptic Neural Substrate' (DSNS) computational creativity approach further with respect to the size and quality of images that can be used to seed the process. In previous work we demonstrated how combining photographs of people and sequences taken from chess games between weak players can be used to generate chess problems or puzzles of higher aesthetic quality, on average, compared to alternative approaches. In this work we show experimentally that using larger images as opposed to smaller ones improves the output quality even further. The same is also true for using clearer or less corrupted images. The reasons why these things influence the DSNS process is presently not well-understood and debatable but the findings are nevertheless immediately applicable for obtaining better results.\n    ",
        "submission_date": "2016-09-20T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06954",
        "title": "Semiring Programming: A Declarative Framework for Generalized Sum Product Problems",
        "authors": [
            "Vaishak Belle",
            "Luc De Raedt"
        ],
        "abstract": "To solve hard problems, AI relies on a variety of disciplines such as logic, probabilistic reasoning, machine learning and mathematical programming. Although it is widely accepted that solving real-world problems requires an integration amongst these, contemporary representation methodologies offer little support for this.\n",
        "submission_date": "2016-09-21T00:00:00",
        "last_modified_date": "2020-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07102",
        "title": "NdFluents: A Multi-dimensional Contexts Ontology",
        "authors": [
            "Jos\u00e9 M. Gim\u00e9nez-Garc\u00eda",
            "Antoine Zimmermann",
            "Pierre Maret"
        ],
        "abstract": "Annotating semantic data with metadata is becoming more and more important to provide information about the statements being asserted. While initial solutions proposed a data model to represent a specific dimension of meta-information (such as time or provenance), the need for a general annotation framework which allows representing different context dimensions is needed. In this paper, we extend the 4dFluents ontology by Welty and Fikes---on associating temporal validity to statements---to any dimension of context, and discuss possible issues that multidimensional context representations have to face and how we address them.\n    ",
        "submission_date": "2016-09-22T00:00:00",
        "last_modified_date": "2016-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07434",
        "title": "Regulating Reward Training by Means of Certainty Prediction in a Neural Network-Implemented Pong Game",
        "authors": [
            "Matt Oberdorfer",
            "Matt Abuzalaf"
        ],
        "abstract": "We present the first reinforcement-learning model to self-improve its reward-modulated training implemented through a continuously improving \"intuition\" neural network. An agent was trained how to play the arcade video game Pong with two reward-based alternatives, one where the paddle was placed randomly during training, and a second where the paddle was simultaneously trained on three additional neural networks such that it could develop a sense of \"certainty\" as to how probable its own predicted paddle position will be to return the ball. If the agent was less than 95% certain to return the ball, the policy used an intuition neural network to place the paddle. We trained both architectures for an equivalent number of epochs and tested learning performance by letting the trained programs play against a near-perfect opponent. Through this, we found that the reinforcement learning model that uses an intuition neural network for placing the paddle during reward training quickly overtakes the simple architecture in its ability to outplay the near-perfect opponent, additionally outscoring that opponent by an increasingly wide margin after additional epochs of training.\n    ",
        "submission_date": "2016-09-23T00:00:00",
        "last_modified_date": "2016-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07721",
        "title": "Testing Quantum Models of Conjunction Fallacy on the World Wide Web",
        "authors": [
            "Diederik Aerts",
            "Jonito Aerts Argu\u00eblles",
            "Lester Beltran",
            "Lyneth Beltran",
            "Massimiliano Sassoli de Bianchi",
            "Sandro Sozzo",
            "Tomas Veloz"
        ],
        "abstract": "The 'conjunction fallacy' has been extensively debated by scholars in cognitive science and, in recent times, the discussion has been enriched by the proposal of modeling the fallacy using the quantum formalism. Two major quantum approaches have been put forward: the first assumes that respondents use a two-step sequential reasoning and that the fallacy results from the presence of 'question order effects'; the second assumes that respondents evaluate the cognitive situation as a whole and that the fallacy results from the 'emergence of new meanings', as an 'effect of overextension' in the conceptual conjunction. Thus, the question arises as to determine whether and to what extent conjunction fallacies would result from 'order effects' or, instead, from 'emergence effects'. To help clarify this situation, we propose to use the World Wide Web as an 'information space' that can be interrogated both in a sequential and non-sequential way, to test these two quantum approaches. We find that 'emergence effects', and not 'order effects', should be considered the main cognitive mechanism producing the observed conjunction fallacies.\n    ",
        "submission_date": "2016-09-25T00:00:00",
        "last_modified_date": "2017-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07772",
        "title": "Commonsense Reasoning, Commonsense Knowledge, and The SP Theory of Intelligence",
        "authors": [
            "J Gerard Wolff"
        ],
        "abstract": "This paper describes how the \"SP Theory of Intelligence\" with the \"SP Computer Model\", outlined in an Appendix, may throw light on aspects of commonsense reasoning (CSR) and commonsense knowledge (CSK), as discussed in another paper by Ernest Davis and Gary Marcus (DM). In four main sections, the paper describes: 1) The main problems to be solved; 2) Other research on CSR and CSK; 3) Why the SP system may prove useful with CSR and CSK 4) How examples described by DM may be modelled in the SP system. With regard to successes in the automation of CSR described by DM, the SP system's strengths in simplification and integration may promote seamless integration across these areas, and seamless integration of those area with other aspects of intelligence. In considering challenges in the automation of CSR described by DM, the paper describes in detail, with examples of SP-multiple-alignments. how the SP system may model processes of interpretation and reasoning arising from the horse's head scene in \"The Godfather\" film. A solution is presented to the 'long tail' problem described by DM. The SP system has some potentially useful things to say about several of DM's objectives for research in CSR and CSK.\n    ",
        "submission_date": "2016-09-25T00:00:00",
        "last_modified_date": "2018-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08439",
        "title": "Model-based Test Generation for Robotic Software: Automata versus Belief-Desire-Intention Agents",
        "authors": [
            "Dejanira Araiza-Illan",
            "Anthony G. Pipe",
            "Kerstin Eder"
        ],
        "abstract": "Robotic code needs to be verified to ensure its safety and functional correctness, especially when the robot is interacting with people. Testing real code in simulation is a viable option. However, generating tests that cover rare scenarios, as well as exercising most of the code, is a challenge amplified by the complexity of the interactions between the environment and the software. Model-based test generation methods can automate otherwise manual processes and facilitate reaching rare scenarios during testing. In this paper, we compare using Belief-Desire-Intention (BDI) agents as models for test generation with more conventional automata-based techniques that exploit model checking, in terms of practicality, performance, transferability to different scenarios, and exploration (`coverage'), through two case studies: a cooperative manufacturing task, and a home care scenario. The results highlight the advantages of using BDI agents for test generation. BDI agents naturally emulate the agency present in Human-Robot Interactions (HRIs), and are thus more expressive than automata. The performance of the BDI-based test generation is at least as high, and the achieved coverage is higher or equivalent, compared to test generation based on model checking automata.\n    ",
        "submission_date": "2016-09-16T00:00:00",
        "last_modified_date": "2016-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08470",
        "title": "A computer program for simulating time travel and a possible 'solution' for the grandfather paradox",
        "authors": [
            "Doron Friedman"
        ],
        "abstract": "While the possibility of time travel in physics is still debated, the explosive growth of virtual-reality simulations opens up new possibilities to rigorously explore such time travel and its consequences in the digital domain. Here we provide a computational model of time travel and a computer program that allows exploring digital time travel. In order to explain our method we formalize a simplified version of the famous grandfather paradox, show how the system can allow the participant to go back in time, try to kill their ancestors before they were born, and experience the consequences. The system has even come up with scenarios that can be considered consistent \"solutions\" of the grandfather paradox. We discuss the conditions for digital time travel, which indicate that it has a large number of practical applications.\n    ",
        "submission_date": "2016-09-26T00:00:00",
        "last_modified_date": "2016-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08524",
        "title": "UbuntuWorld 1.0 LTS - A Platform for Automated Problem Solving & Troubleshooting in the Ubuntu OS",
        "authors": [
            "Tathagata Chakraborti",
            "Kartik Talamadupula",
            "Kshitij P. Fadnis",
            "Murray Campbell",
            "Subbarao Kambhampati"
        ],
        "abstract": "In this paper, we present UbuntuWorld 1.0 LTS - a platform for developing automated technical support agents in the Ubuntu operating system. Specifically, we propose to use the Bash terminal as a simulator of the Ubuntu environment for a learning-based agent and demonstrate the usefulness of adopting reinforcement learning (RL) techniques for basic problem solving and troubleshooting in this environment. We provide a plug-and-play interface to the simulator as a python package where different types of agents can be plugged in and evaluated, and provide pathways for integrating data from online support forums like AskUbuntu into an automated agent's learning process. Finally, we show that the use of this data significantly improves the agent's learning efficiency. We believe that this platform can be adopted as a real-world test bed for research on automated technical support.\n    ",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2017-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08925",
        "title": "Global Constraint Catalog, Volume II, Time-Series Constraints",
        "authors": [
            "Ekaterina Arafailova",
            "Nicolas Beldiceanu",
            "R\u00e9mi Douence",
            "Mats Carlsson",
            "Pierre Flener",
            "Mar\u00eda Andre\u00edna Francisco Rodr\u00edguez",
            "Justin Pearson",
            "Helmut Simonis"
        ],
        "abstract": "First this report presents a restricted set of finite transducers used to synthesise structural time-series constraints described by means of a multi-layered function composition scheme. Second it provides the corresponding synthesised catalogue of structural time-series constraints where each constraint is explicitly described in terms of automata with registers.\n    ",
        "submission_date": "2016-09-26T00:00:00",
        "last_modified_date": "2018-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09253",
        "title": "Heuristic with elements of tabu search for Truck and Trailer Routing Problem",
        "authors": [
            "Ivan S. Grechikhin"
        ],
        "abstract": "Vehicle Routing Problem is a well-known problem in logistics and transportation, and the variety of such problems is explained by the fact that it occurs in many real-life situations. It is an NP-hard combinatorial optimization problem and finding an exact optimal solution is practically impossible. In this work, Site-Dependent Truck and Trailer Routing Problem with hard and soft Time Windows and Split Deliveries is considered (SDTTRPTWSD). In this article, we develop a heuristic with the elements of Tabu Search for solving SDTTRPTWSD. The heuristic uses the concept of neighborhoods and visits infeasible solutions during the search. A greedy heuristic is applied to construct an initial solution.\n    ",
        "submission_date": "2016-09-29T00:00:00",
        "last_modified_date": "2016-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09748",
        "title": "Characterization of experts in crowdsourcing platforms",
        "authors": [
            "Amal Ben Rjab",
            "Mouloud Kharoune",
            "Zoltan Miklos",
            "Arnaud Martin"
        ],
        "abstract": "Crowdsourcing platforms enable to propose simple human intelligence tasks to a large number of participants who realise these tasks. The workers often receive a small amount of money or the platforms include some other incentive mechanisms, for example they can increase the workers reputation score, if they complete the tasks correctly. We address the problem of identifying experts among participants, that is, workers, who tend to answer the questions correctly. Knowing who are the reliable workers could improve the quality of knowledge one can extract from responses. As opposed to other works in the literature, we assume that participants can give partial or incomplete responses, in case they are not sure that their answers are correct. We model such partial or incomplete responses with the help of belief functions, and we derive a measure that characterizes the expertise level of each participant. This measure is based on precise and exactitude degrees that represent two parts of the expertise level. The precision degree reflects the reliability level of the participants and the exactitude degree reflects the knowledge level of the participants. We also analyze our model through simulation and demonstrate that our richer model can lead to more reliable identification of experts.\n    ",
        "submission_date": "2016-09-30T00:00:00",
        "last_modified_date": "2016-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09864",
        "title": "Technical Report: Graph-Structured Sparse Optimization for Connected Subgraph Detection",
        "authors": [
            "Baojian Zhou",
            "Feng Chen"
        ],
        "abstract": "Structured sparse optimization is an important and challenging problem for analyzing high-dimensional data in a variety of applications such as bioinformatics, medical imaging, social networks, and astronomy. Although a number of structured sparsity models have been explored, such as trees, groups, clusters, and paths, connected subgraphs have been rarely explored in the current literature. One of the main technical challenges is that there is no structured sparsity-inducing norm that can directly model the space of connected subgraphs, and there is no exact implementation of a projection oracle for connected subgraphs due to its NP-hardness. In this paper, we explore efficient approximate projection oracles for connected subgraphs, and propose two new efficient algorithms, namely, Graph-IHT and Graph-GHTP, to optimize a generic nonlinear objective function subject to connectivity constraint on the support of the variables. Our proposed algorithms enjoy strong guarantees analogous to several current methods for sparsity-constrained optimization, such as Projected Gradient Descent (PGD), Approximate Model Iterative Hard Thresholding (AM-IHT), and Gradient Hard Thresholding Pursuit (GHTP) with respect to convergence rate and approximation accuracy. We apply our proposed algorithms to optimize several well-known graph scan statistics in several applications of connected subgraph detection as a case study, and the experimental results demonstrate that our proposed algorithms outperform state-of-the-art methods.\n    ",
        "submission_date": "2016-09-30T00:00:00",
        "last_modified_date": "2016-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00054",
        "title": "Outlier Detection from Network Data with Subnetwork Interpretation",
        "authors": [
            "Xuan-Hong Dang",
            "Arlei Silva",
            "Ambuj Singh",
            "Ananthram Swami",
            "Prithwish Basu"
        ],
        "abstract": "Detecting a small number of outliers from a set of data observations is always challenging. This problem is more difficult in the setting of multiple network samples, where computing the anomalous degree of a network sample is generally not sufficient. In fact, explaining why the network is exceptional, expressed in the form of subnetwork, is also equally important. In this paper, we develop a novel algorithm to address these two key problems. We treat each network sample as a potential outlier and identify subnetworks that mostly discriminate it from nearby regular samples. The algorithm is developed in the framework of network regression combined with the constraints on both network topology and L1-norm shrinkage to perform subnetwork discovery. Our method thus goes beyond subspace/subgraph discovery and we show that it converges to a global optimum. Evaluation on various real-world network datasets demonstrates that our algorithm not only outperforms baselines in both network and high dimensional setting, but also discovers highly relevant and interpretable local subnetworks, further enhancing our understanding of anomalous networks.\n    ",
        "submission_date": "2016-09-30T00:00:00",
        "last_modified_date": "2016-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00081",
        "title": "Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction",
        "authors": [
            "Junbo Zhang",
            "Yu Zheng",
            "Dekang Qi"
        ],
        "abstract": "Forecasting the flow of crowds is of great importance to traffic management and public safety, yet a very challenging task affected by many complex factors, such as inter-region traffic, events and weather. In this paper, we propose a deep-learning-based approach, called ST-ResNet, to collectively forecast the in-flow and out-flow of crowds in each and every region through a city. We design an end-to-end structure of ST-ResNet based on unique properties of spatio-temporal data. More specifically, we employ the framework of the residual neural networks to model the temporal closeness, period, and trend properties of the crowd traffic, respectively. For each property, we design a branch of residual convolutional units, each of which models the spatial properties of the crowd traffic. ST-ResNet learns to dynamically aggregate the output of the three residual neural networks based on data, assigning different weights to different branches and regions. The aggregation is further combined with external factors, such as weather and day of the week, to predict the final traffic of crowds in each and every region. We evaluate ST-ResNet based on two types of crowd flows in Beijing and NYC, finding that its performance exceeds six well-know methods.\n    ",
        "submission_date": "2016-10-01T00:00:00",
        "last_modified_date": "2017-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00366",
        "title": "Funneled Bayesian Optimization for Design, Tuning and Control of Autonomous Systems",
        "authors": [
            "Ruben Martinez-Cantin"
        ],
        "abstract": "Bayesian optimization has become a fundamental global optimization algorithm in many problems where sample efficiency is of paramount importance. Recently, there has been proposed a large number of new applications in fields such as robotics, machine learning, experimental design, simulation, etc. In this paper, we focus on several problems that appear in robotics and autonomous systems: algorithm tuning, automatic control and intelligent design. All those problems can be mapped to global optimization problems. However, they become hard optimization problems. Bayesian optimization internally uses a probabilistic surrogate model (e.g.: Gaussian process) to learn from the process and reduce the number of samples required. In order to generalize to unknown functions in a black-box fashion, the common assumption is that the underlying function can be modeled with a stationary process. Nonstationary Gaussian process regression cannot generalize easily and it typically requires prior knowledge of the function. Some works have designed techniques to generalize Bayesian optimization to nonstationary functions in an indirect way, but using techniques originally designed for regression, where the objective is to improve the quality of the surrogate model everywhere. Instead optimization should focus on improving the surrogate model near the optimum. In this paper, we present a novel kernel function specially designed for Bayesian optimization, that allows nonstationary behavior of the surrogate model in an adaptive local region. In our experiments, we found that this new kernel results in an improved local search (exploitation), without penalizing the global search (exploration). We provide results in well-known benchmarks and real applications. The new method outperforms the state of the art in Bayesian optimization both in stationary and nonstationary problems.\n    ",
        "submission_date": "2016-10-02T00:00:00",
        "last_modified_date": "2019-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00378",
        "title": "Improving Accuracy and Scalability of the PC Algorithm by Maximizing P-value",
        "authors": [
            "Joseph Ramsey"
        ],
        "abstract": "A number of attempts have been made to improve accuracy and/or scalability of the PC (Peter and Clark) algorithm, some well known (Buhlmann, et al., 2010; Kalisch and Buhlmann, 2007; 2008; Zhang, 2012, to give some examples). We add here one more tool to the toolbox: the simple observation that if one is forced to choose between a variety of possible conditioning sets for a pair of variables, one should choose the one with the highest p-value. One can use the CPC (Conservative PC, Ramsey et al., 2012) algorithm as a guide to possible sepsets for a pair of variables. However, whereas CPC uses a voting rule to classify colliders versus noncolliders, our proposed algorithm, PC-Max, picks the conditioning set with the highest p-value, so that there are no ambiguities. We combine this with two other optimizations: (a) avoiding bidirected edges in the orientation of colliders, and (b) parallelization. For (b) we borrow ideas from the PC-Stable algorithm (Colombo and Maathuis, 2014). The result is an algorithm that scales quite well both in terms of accuracy and time, with no risk of bidirected edges.\n    ",
        "submission_date": "2016-10-03T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00442",
        "title": "Should Algorithms for Random SAT and Max-SAT be Different?",
        "authors": [
            "Sixue Liu",
            "Gerard de Melo"
        ],
        "abstract": "We analyze to what extent the random SAT and Max-SAT problems differ in their properties. Our findings suggest that for random $k$-CNF with ratio in a certain range, Max-SAT can be solved by any SAT algorithm with subexponential slowdown, while for formulae with ratios greater than some constant, algorithms under the random walk framework require substantially different heuristics. In light of these results, we propose a novel probabilistic approach for random Max-SAT called ProMS. Experimental results illustrate that ProMS outperforms many state-of-the-art local search solvers on random Max-SAT benchmarks.\n    ",
        "submission_date": "2016-10-03T00:00:00",
        "last_modified_date": "2018-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00689",
        "title": "Phase-Mapper: An AI Platform to Accelerate High Throughput Materials Discovery",
        "authors": [
            "Yexiang Xue",
            "Junwen Bai",
            "Ronan Le Bras",
            "Brendan Rappazzo",
            "Richard Bernstein",
            "Johan Bjorck",
            "Liane Longpre",
            "Santosh K. Suram",
            "Robert B. van Dover",
            "John Gregoire",
            "Carla P. Gomes"
        ],
        "abstract": "High-Throughput materials discovery involves the rapid synthesis, measurement, and characterization of many different but structurally-related materials. A key problem in materials discovery, the phase map identification problem, involves the determination of the crystal phase diagram from the materials' composition and structural characterization data. We present Phase-Mapper, a novel AI platform to solve the phase map identification problem that allows humans to interact with both the data and products of AI algorithms, including the incorporation of human feedback to constrain or initialize solutions. Phase-Mapper affords incorporation of any spectral demixing algorithm, including our novel solver, AgileFD, which is based on a convolutive non-negative matrix factorization algorithm. AgileFD can incorporate constraints to capture the physics of the materials as well as human feedback. We compare three solver variants with previously proposed methods in a large-scale experiment involving 20 synthetic systems, demonstrating the efficacy of imposing physical constrains using AgileFD. Phase-Mapper has also been used by materials scientists to solve a wide variety of phase diagrams, including the previously unsolved Nb-Mn-V oxide system, which is provided here as an illustrative example.\n    ",
        "submission_date": "2016-10-03T00:00:00",
        "last_modified_date": "2016-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00946",
        "title": "Micro-Data Learning: The Other End of the Spectrum",
        "authors": [
            "Jean-Baptiste Mouret"
        ],
        "abstract": "Many fields are now snowed under with an avalanche of data, which raises considerable challenges for computer scientists. Meanwhile, robotics (among other fields) can often only use a few dozen data points because acquiring them involves a process that is expensive or time-consuming. How can an algorithm learn with only a few data points?\n    ",
        "submission_date": "2016-10-04T00:00:00",
        "last_modified_date": "2016-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00976",
        "title": "A Constraint-Handling Technique for Genetic Algorithms using a Violation Factor",
        "authors": [
            "Adam Chehouri",
            "Rafic Younes",
            "Jean Perron",
            "Adrian Ilinca"
        ],
        "abstract": "Over the years, several meta-heuristic algorithms were proposed and are now emerging as common methods for constrained optimization problems. Among them, genetic algorithms (GA's) shine as popular evolutionary algorithms (EA's) in engineering optimization. Most engineering design problems are difficult to resolve with conventional optimization algorithms because they are highly nonlinear and contain constraints. In order to handle these constraints, the most common technique is to apply penalty functions. The major drawback is that they require tuning of parameters, which can be very challenging. In this paper, we present a constraint-handling technique for GA's solely using the violation factor, called VCH (Violation Constraint-Handling) method. Several benchmark problems from the literature are examined. The VCH technique was able to provide a consistent performance and match results from other GA-based techniques.\n    ",
        "submission_date": "2016-10-04T00:00:00",
        "last_modified_date": "2016-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01044",
        "title": "DeepAlgebra - an outline of a program",
        "authors": [
            "Przemyslaw Chojecki"
        ],
        "abstract": "We outline a program in the area of formalization of mathematics to automate theorem proving in algebra and algebraic geometry. We propose a construction of a dictionary between automated theorem provers and (La)TeX exploiting syntactic parsers. We describe its application to a repository of human-written facts and definitions in algebraic geometry (The Stacks Project). We use deep learning techniques.\n    ",
        "submission_date": "2016-10-04T00:00:00",
        "last_modified_date": "2016-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01085",
        "title": "Towards the Design of Prospect-Theory based Human Decision Rules for Hypothesis Testing",
        "authors": [
            "V. Sriram Siddhardh Nadendla",
            "Swastik Brahma",
            "Pramod K. Varshney"
        ],
        "abstract": "Detection rules have traditionally been designed for rational agents that minimize the Bayes risk (average decision cost). With the advent of crowd-sensing systems, there is a need to redesign binary hypothesis testing rules for behavioral agents, whose cognitive behavior is not captured by traditional utility functions such as Bayes risk. In this paper, we adopt prospect theory based models for decision makers. We consider special agent models namely optimists and pessimists in this paper, and derive optimal detection rules under different scenarios. Using an illustrative example, we also show how the decision rule of a human agent deviates from the Bayesian decision rule under various behavioral models, considered in this paper.\n    ",
        "submission_date": "2016-10-04T00:00:00",
        "last_modified_date": "2016-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01381",
        "title": "The Predictive Context Tree: Predicting Contexts and Interactions",
        "authors": [
            "Alasdair Thomason",
            "Nathan Griffiths",
            "Victor Sanchez"
        ],
        "abstract": "With a large proportion of people carrying location-aware smartphones, we have an unprecedented platform from which to understand individuals and predict their future actions. This work builds upon the Context Tree data structure that summarises the historical contexts of individuals from augmented geospatial trajectories, and constructs a predictive model for their likely future contexts. The Predictive Context Tree (PCT) is constructed as a hierarchical classifier, capable of predicting both the future locations that a user will visit and the contexts that a user will be immersed within. The PCT is evaluated over real-world geospatial trajectories, and compared against existing location extraction and prediction techniques, as well as a proposed hybrid approach that uses identified land usage elements in combination with machine learning to predict future interactions. Our results demonstrate that higher predictive accuracies can be achieved using this hybrid approach over traditional extracted location datasets, and the PCT itself matches the performance of the hybrid approach at predicting future interactions, while adding utility in the form of context predictions. Such a prediction system is capable of understanding not only where a user will visit, but also their context, in terms of what they are likely to be doing.\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01476",
        "title": "$\\ell_1$ Regularized Gradient Temporal-Difference Learning",
        "authors": [
            "Dominik Meyer",
            "Hao Shen",
            "Klaus Diepold"
        ],
        "abstract": "In this paper, we study the Temporal Difference (TD) learning with linear value function approximation. It is well known that most TD learning algorithms are unstable with linear function approximation and off-policy learning. Recent development of Gradient TD (GTD) algorithms has addressed this problem successfully. However, the success of GTD algorithms requires a set of well chosen features, which are not always available. When the number of features is huge, the GTD algorithms might face the problem of overfitting and being computationally expensive. To cope with this difficulty, regularization techniques, in particular $\\ell_1$ regularization, have attracted significant attentions in developing TD learning algorithms. The present work combines the GTD algorithms with $\\ell_1$ regularization. We propose a family of $\\ell_1$ regularized GTD algorithms, which employ the well known soft thresholding operator. We investigate convergence properties of the proposed algorithms, and depict their performance with several numerical experiments.\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01525",
        "title": "Lifted Message Passing for the Generalized Belief Propagation",
        "authors": [
            "Udi Apsel"
        ],
        "abstract": "We introduce the lifted Generalized Belief Propagation (GBP) message passing algorithm, for the computation of sum-product queries in Probabilistic Relational Models (e.g. Markov logic network). The algorithm forms a compact region graph and establishes a modified version of message passing, which mimics the GBP behavior in a corresponding ground model. The compact graph is obtained by exploiting a graphical representation of clusters, which reduces cluster symmetry detection to isomorphism tests on small local graphs. The framework is thus capable of handling complex models, while remaining domain-size independent.\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01922",
        "title": "Adaptive Online Sequential ELM for Concept Drift Tackling",
        "authors": [
            "Arif Budiman",
            "Mohamad Ivan Fanany",
            "Chan Basaruddin"
        ],
        "abstract": "A machine learning method needs to adapt to over time changes in the environment. Such changes are known as concept drift. In this paper, we propose concept drift tackling method as an enhancement of Online Sequential Extreme Learning Machine (OS-ELM) and Constructive Enhancement OS-ELM (CEOS-ELM) by adding adaptive capability for classification and regression problem. The scheme is named as adaptive OS-ELM (AOS-ELM). It is a single classifier scheme that works well to handle real drift, virtual drift, and hybrid drift. The AOS-ELM also works well for sudden drift and recurrent context change type. The scheme is a simple unified method implemented in simple lines of code. We evaluated AOS-ELM on regression and classification problem by using concept drift public data set (SEA and STAGGER) and other public data sets such as MNIST, USPS, and IDS. Experiments show that our method gives higher kappa value compared to the multiclassifier ELM ensemble. Even though AOS-ELM in practice does not need hidden nodes increase, we address some issues related to the increasing of the hidden nodes such as error condition and rank values. We propose taking the rank of the pseudoinverse matrix as an indicator parameter to detect underfitting condition.\n    ",
        "submission_date": "2016-10-06T00:00:00",
        "last_modified_date": "2016-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02293",
        "title": "Learning Macro-actions for State-Space Planning",
        "authors": [
            "Sandra Castellanos-Paez",
            "Damien Pellier",
            "Humbert Fiorino",
            "Sylvie Pesty"
        ],
        "abstract": "Planning has achieved significant progress in recent years. Among the various approaches to scale up plan synthesis, the use of macro-actions has been widely explored. As a first stage towards the development of a solution to learn on-line macro-actions, we propose an algorithm to identify useful macro-actions based on data mining techniques. The integration in the planning search of these learned macro-actions shows significant improvements over four classical planning benchmarks.\n    ",
        "submission_date": "2016-10-07T00:00:00",
        "last_modified_date": "2016-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02348",
        "title": "Adaptive Convolutional ELM For Concept Drift Handling in Online Stream Data",
        "authors": [
            "Arif Budiman",
            "Mohamad Ivan Fanany",
            "Chan Basaruddin"
        ],
        "abstract": "In big data era, the data continuously generated and its distribution may keep changes overtime. These challenges in online stream of data are known as concept drift. In this paper, we proposed the Adaptive Convolutional ELM method (ACNNELM) as enhancement of Convolutional Neural Network (CNN) with a hybrid Extreme Learning Machine (ELM) model plus adaptive capability. This method is aimed for concept drift handling. We enhanced the CNN as convolutional hiererchical features representation learner combined with Elastic ELM (E$^2$LM) as a parallel supervised classifier. We propose an Adaptive OS-ELM (AOS-ELM) for concept drift adaptability in classifier level (named ACNNELM-1) and matrices concatenation ensembles for concept drift adaptability in ensemble level (named ACNNELM-2). Our proposed Adaptive CNNELM is flexible that works well in classifier level and ensemble level while most current methods only proposed to work on either one of the levels.\n",
        "submission_date": "2016-10-07T00:00:00",
        "last_modified_date": "2016-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02424",
        "title": "Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models",
        "authors": [
            "Ashwin K Vijayakumar",
            "Michael Cogswell",
            "Ramprasath R. Selvaraju",
            "Qing Sun",
            "Stefan Lee",
            "David Crandall",
            "Dhruv Batra"
        ],
        "abstract": "Neural sequence models are widely used to model time-series data. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top-B candidates - resulting in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose Diverse Beam Search (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing for a diversity-augmented objective. We observe that our method finds better top-1 solutions by controlling for the exploration and exploitation of the search space - implying that DBS is a better search algorithm. Moreover, these gains are achieved with minimal computational or memory over- head as compared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation and visual question generation using both standard quantitative metrics and qualitative human studies. Further, we study the role of diversity for image-grounded language generation tasks as the complexity of the image changes. We observe that our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models.\n    ",
        "submission_date": "2016-10-07T00:00:00",
        "last_modified_date": "2018-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02591",
        "title": "Solving Marginal MAP Problems with NP Oracles and Parity Constraints",
        "authors": [
            "Yexiang Xue",
            "Zhiyuan Li",
            "Stefano Ermon",
            "Carla P. Gomes",
            "Bart Selman"
        ],
        "abstract": "Arising from many applications at the intersection of decision making and machine learning, Marginal Maximum A Posteriori (Marginal MAP) Problems unify the two main classes of inference, namely maximization (optimization) and marginal inference (counting), and are believed to have higher complexity than both of them. We propose XOR_MMAP, a novel approach to solve the Marginal MAP Problem, which represents the intractable counting subproblem with queries to NP oracles, subject to additional parity constraints. XOR_MMAP provides a constant factor approximation to the Marginal MAP Problem, by encoding it as a single optimization in polynomial size of the original problem. We evaluate our approach in several machine learning and decision making applications, and show that our approach outperforms several state-of-the-art Marginal MAP solvers.\n    ",
        "submission_date": "2016-10-08T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02707",
        "title": "Multi-Objective Deep Reinforcement Learning",
        "authors": [
            "Hossam Mossalam",
            "Yannis M. Assael",
            "Diederik M. Roijers",
            "Shimon Whiteson"
        ],
        "abstract": "We propose Deep Optimistic Linear Support Learning (DOL) to solve high-dimensional multi-objective decision problems where the relative importances of the objectives are not known a priori. Using features from the high-dimensional inputs, DOL computes the convex coverage set containing all potential optimal solutions of the convex combinations of the objectives. To our knowledge, this is the first time that deep reinforcement learning has succeeded in learning multi-objective policies. In addition, we provide a testbed with two experiments to be used as a benchmark for deep multi-objective reinforcement learning.\n    ",
        "submission_date": "2016-10-09T00:00:00",
        "last_modified_date": "2016-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02828",
        "title": "Ranking academic institutions on potential paper acceptance in upcoming conferences",
        "authors": [
            "Jobin Wilson",
            "Ram Mohan",
            "Muhammad Arif",
            "Santanu Chaudhury",
            "Brejesh Lall"
        ],
        "abstract": "The crux of the problem in KDD Cup 2016 involves developing data mining techniques to rank research institutions based on publications. Rank importance of research institutions are derived from predictions on the number of full research papers that would potentially get accepted in upcoming top-tier conferences, utilizing public information on the web. This paper describes our solution to KDD Cup 2016. We used a two step approach in which we first identify full research papers corresponding to each conference of interest and then train two variants of exponential smoothing models to make predictions. Our solution achieves an overall score of 0.7508, while the winning submission scored 0.7656 in the overall results.\n    ",
        "submission_date": "2016-10-10T00:00:00",
        "last_modified_date": "2016-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02847",
        "title": "Situational Awareness by Risk-Conscious Skills",
        "authors": [
            "Daniel J. Mankowitz",
            "Aviv Tamar",
            "Shie Mannor"
        ],
        "abstract": "Hierarchical Reinforcement Learning has been previously shown to speed up the convergence rate of RL planning algorithms as well as mitigate feature-based model misspecification (Mankowitz et. al. 2016a,b, Bacon 2015). To do so, it utilizes hierarchical abstractions, also known as skills -- a type of temporally extended action (Sutton et. al. 1999) to plan at a higher level, abstracting away from the lower-level details. We incorporate risk sensitivity, also referred to as Situational Awareness (SA), into hierarchical RL for the first time by defining and learning risk aware skills in a Probabilistic Goal Semi-Markov Decision Process (PG-SMDP). This is achieved using our novel Situational Awareness by Risk-Conscious Skills (SARiCoS) algorithm which comes with a theoretical convergence guarantee. We show in a RoboCup soccer domain that the learned risk aware skills exhibit complex human behaviors such as `time-wasting' in a soccer game. In addition, the learned risk aware skills are able to mitigate reward-based model misspecification.\n    ",
        "submission_date": "2016-10-10T00:00:00",
        "last_modified_date": "2016-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02891",
        "title": "Personalizing a Dialogue System with Transfer Reinforcement Learning",
        "authors": [
            "Kaixiang Mo",
            "Shuangyin Li",
            "Yu Zhang",
            "Jiajun Li",
            "Qiang Yang"
        ],
        "abstract": "It is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient. Personalized dialogue systems trained on a small dataset can overfit and make it difficult to adapt to different user needs. One way to solve this problem is to consider a collection of multiple users' data as a source domain and an individual user's data as a target domain, and to perform a transfer learning from the source to the target domain. By following this idea, we propose \"PETAL\"(PErsonalized Task-oriented diALogue), a transfer-learning framework based on POMDP to learn a personalized dialogue system. The system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user. This framework can avoid the negative transfer problem by considering differences between source and target users. The policy in the personalized POMDP can learn to choose different actions appropriately for different users. Experimental results on a real-world coffee-shopping data and simulation data show that our personalized dialogue system can choose different optimal actions for different users, and thus effectively improve the dialogue quality under the personalized setting.\n    ",
        "submission_date": "2016-10-10T00:00:00",
        "last_modified_date": "2017-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03024",
        "title": "ABA+: Assumption-Based Argumentation with Preferences",
        "authors": [
            "Kristijonas \u010cyras",
            "Francesca Toni"
        ],
        "abstract": "We present ABA+, a new approach to handling preferences in a well known structured argumentation formalism, Assumption-Based Argumentation (ABA). In ABA+, preference information given over assumptions is incorporated directly into the attack relation, thus resulting in attack reversal. ABA+ conservatively extends ABA and exhibits various desirable features regarding relationship among argumentation semantics as well as preference handling. We also introduce Weak Contraposition, a principle concerning reasoning with rules and preferences that relaxes the standard principle of contraposition, while guaranteeing additional desirable features for ABA+.\n    ",
        "submission_date": "2016-10-10T00:00:00",
        "last_modified_date": "2016-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03138",
        "title": "PCG-Based Game Design Patterns",
        "authors": [
            "Michael Cook",
            "Mirjam Eladhari",
            "Andy Nealen",
            "Mike Treanor",
            "Eddy Boxerman",
            "Alex Jaffe",
            "Paul Sottosanti",
            "Steve Swink"
        ],
        "abstract": "People enjoy encounters with generative software, but rarely are they encouraged to interact with, understand or engage with it. In this paper we define the term 'PCG-based game', and explain how this concept follows on from the idea of an AI-based game. We look at existing examples of games which foreground their AI, put forward a methodology for designing PCG-based games, describe some example case study designs for PCG-based games, and describe lessons learned during this process of sketching and developing ideas.\n    ",
        "submission_date": "2016-10-11T00:00:00",
        "last_modified_date": "2016-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03263",
        "title": "Error Asymmetry in Causal and Anticausal Regression",
        "authors": [
            "Patrick Bl\u00f6baum",
            "Takashi Washio",
            "Shohei Shimizu"
        ],
        "abstract": "It is generally difficult to make any statements about the expected prediction error in an univariate setting without further knowledge about how the data were generated. Recent work showed that knowledge about the real underlying causal structure of a data generation process has implications for various machine learning settings. Assuming an additive noise and an independence between data generating mechanism and its input, we draw a novel connection between the intrinsic causal relationship of two variables and the expected prediction error. We formulate the theorem that the expected error of the true data generating function as prediction model is generally smaller when the effect is predicted from its cause and, on the contrary, greater when the cause is predicted from its effect. The theorem implies an asymmetry in the error depending on the prediction direction. This is further corroborated with empirical evaluations in artificial and real-world data sets.\n    ",
        "submission_date": "2016-10-11T00:00:00",
        "last_modified_date": "2017-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03295",
        "title": "Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving",
        "authors": [
            "Shai Shalev-Shwartz",
            "Shaked Shammah",
            "Amnon Shashua"
        ],
        "abstract": "Autonomous driving is a multi-agent setting where the host vehicle must apply sophisticated negotiation skills with other road users when overtaking, giving way, merging, taking left and right turns and while pushing ahead in unstructured urban roadways. Since there are many possible scenarios, manually tackling all possible cases will likely yield a too simplistic policy. Moreover, one must balance between unexpected behavior of other drivers/pedestrians and at the same time not to be too defensive so that normal traffic flow is maintained.\n",
        "submission_date": "2016-10-11T00:00:00",
        "last_modified_date": "2016-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03573",
        "title": "A Chain-Detection Algorithm for Two-Dimensional Grids",
        "authors": [
            "Paul Bonham",
            "Azlan Iqbal"
        ],
        "abstract": "We describe a general method of detecting valid chains or links of pieces on a two-dimensional grid. Specifically, using the example of the chess variant known as Switch-Side Chain-Chess (SSCC). Presently, no foolproof method of detecting such chains in any given chess position is known and existing graph theory, to our knowledge, is unable to fully address this problem either. We therefore propose a solution implemented and tested using the C++ programming language. We have been unable to find an incorrect result and therefore offer it as the most viable solution thus far to the chain-detection problem in this chess variant. The algorithm is also scalable, in principle, to areas beyond two-dimensional grids such as 3D analysis and molecular chemistry.\n    ",
        "submission_date": "2016-10-12T00:00:00",
        "last_modified_date": "2016-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03606",
        "title": "Maximum entropy models for generation of expressive music",
        "authors": [
            "Simon Moulieras",
            "Fran\u00e7ois Pachet"
        ],
        "abstract": "In the context of contemporary monophonic music, expression can be seen as the difference between a musical performance and its symbolic representation, i.e. a musical score. In this paper, we show how Maximum Entropy (MaxEnt) models can be used to generate musical expression in order to mimic a human performance. As a training corpus, we had a professional pianist play about 150 melodies of jazz, pop, and latin jazz. The results show a good predictive power, validating the choice of our model. Additionally, we set up a listening test whose results reveal that on average, people significantly prefer the melodies generated by the MaxEnt model than the ones without any expression, or with fully random expression. Furthermore, in some cases, MaxEnt melodies are almost as popular as the human performed ones.\n    ",
        "submission_date": "2016-10-12T00:00:00",
        "last_modified_date": "2016-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04005",
        "title": "Stream Reasoning-Based Control of Caching Strategies in CCN Routers",
        "authors": [
            "Harald Beck",
            "Bruno Bierbaumer",
            "Minh Dao-Tran",
            "Thomas Eiter",
            "Hermann Hellwagner",
            "Konstantin Schekotihin"
        ],
        "abstract": "Content-Centric Networking (CCN) research addresses the mismatch between the modern usage of the Internet and its outdated architecture. Importantly, CCN routers may locally cache frequently requested content in order to speed up delivery to end users. Thus, the issue of caching strategies arises, i.e., which content shall be stored and when it should be replaced. In this work, we employ novel techniques towards intelligent administration of CCN routers that autonomously switch between existing strategies in response to changing content request patterns. In particular, we present a router architecture for CCN networks that is controlled by rule-based stream reasoning, following the recent formal framework LARS which extends Answer Set Programming for streams. The obtained possibility for flexible router configuration at runtime allows for faster experimentation and may thus help to advance the further development of CCN. Moreover, the empirical evaluation of our feasibility study shows that the resulting caching agent may give significant performance gains.\n    ",
        "submission_date": "2016-10-13T00:00:00",
        "last_modified_date": "2016-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04028",
        "title": "A fuzzy expert system for earthquake prediction, case study: the Zagros range",
        "authors": [
            "Arash Andalib",
            "Mehdi Zare",
            "Farid Atry"
        ],
        "abstract": "A methodology for the development of a fuzzy expert system (FES) with application to earthquake prediction is presented. The idea is to reproduce the performance of a human expert in earthquake prediction. To do this, at the first step, rules provided by the human expert are used to generate a fuzzy rule base. These rules are then fed into an inference engine to produce a fuzzy inference system (FIS) and to infer the results. In this paper, we have used a Sugeno type fuzzy inference system to build the FES. At the next step, the adaptive network-based fuzzy inference system (ANFIS) is used to refine the FES parameters and improve its performance. The proposed framework is then employed to attain the performance of a human expert used to predict earthquakes in the Zagros area based on the idea of coupled earthquakes. While the prediction results are promising in parts of the testing set, the general performance indicates that prediction methodology based on coupled earthquakes needs more investigation and more complicated reasoning procedure to yield satisfactory predictions.\n    ",
        "submission_date": "2016-10-13T00:00:00",
        "last_modified_date": "2017-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04073",
        "title": "Improved Knowledge Base Completion by Path-Augmented TransR Model",
        "authors": [
            "Wenhao Huang",
            "Ge Li",
            "Zhi Jin"
        ],
        "abstract": "Knowledge base completion aims to infer new relations from existing information. In this paper, we propose path-augmented TransR (PTransR) model to improve the accuracy of link prediction. In our approach, we base PTransR model on TransR, which is the best one-hop model at present. Then we regularize TransR with information of relation paths. In our experiment, we evaluate PTransR on the task of entity prediction. Experimental results show that PTransR outperforms previous models.\n    ",
        "submission_date": "2016-10-06T00:00:00",
        "last_modified_date": "2016-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04120",
        "title": "Exploiting Sentence and Context Representations in Deep Neural Models for Spoken Language Understanding",
        "authors": [
            "Lina M. Rojas Barahona",
            "Milica Gasic",
            "Nikola Mrk\u0161i\u0107",
            "Pei-Hao Su",
            "Stefan Ultes",
            "Tsung-Hsien Wen",
            "Steve Young"
        ],
        "abstract": "This paper presents a deep learning architecture for the semantic decoder component of a Statistical Spoken Dialogue System. In a slot-filling dialogue, the semantic decoder predicts the dialogue act and a set of slot-value pairs from a set of n-best hypotheses returned by the Automatic Speech Recognition. Most current models for spoken language understanding assume (i) word-aligned semantic annotations as in sequence taggers and (ii) delexicalisation, or a mapping of input words to domain-specific concepts using heuristics that try to capture morphological variation but that do not scale to other domains nor to language variation (e.g., morphology, synonyms, paraphrasing ). In this work the semantic decoder is trained using unaligned semantic annotations and it uses distributed semantic representation learning to overcome the limitations of explicit delexicalisation. The proposed architecture uses a convolutional neural network for the sentence representation and a long-short term memory network for the context representation. Results are presented for the publicly available DSTC2 corpus and an In-car corpus which is similar to DSTC2 but has a significantly higher word error rate (WER).\n    ",
        "submission_date": "2016-10-13T00:00:00",
        "last_modified_date": "2016-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04154",
        "title": "An Information Theoretic Feature Selection Framework for Big Data under Apache Spark",
        "authors": [
            "Sergio Ram\u00edrez-Gallego",
            "H\u00e9ctor Mouri\u00f1o-Tal\u00edn",
            "David Mart\u00ednez-Rego",
            "Ver\u00f3nica Bol\u00f3n-Canedo",
            "Jos\u00e9 Manuel Ben\u00edtez",
            "Amparo Alonso-Betanzos",
            "Francisco Herrera"
        ],
        "abstract": "With the advent of extremely high dimensional datasets, dimensionality reduction techniques are becoming mandatory. Among many techniques, feature selection has been growing in interest as an important tool to identify relevant features on huge datasets --both in number of instances and features--. The purpose of this work is to demonstrate that standard feature selection methods can be parallelized in Big Data platforms like Apache Spark, boosting both performance and accuracy. We thus propose a distributed implementation of a generic feature selection framework which includes a wide group of well-known Information Theoretic methods. Experimental results on a wide set of real-world datasets show that our distributed framework is capable of dealing with ultra-high dimensional datasets as well as those with a huge number of samples in a short period of time, outperforming the sequential version in all the cases studied.\n    ",
        "submission_date": "2016-10-13T00:00:00",
        "last_modified_date": "2016-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04872",
        "title": "Fault Detection Engine in Intelligent Predictive Analytics Platform for DCIM",
        "authors": [
            "Bodhisattwa Prasad Majumder",
            "Ayan Sengupta",
            "Sajal jain",
            "Parikshit Bhaduri"
        ],
        "abstract": "With the advancement of huge data generation and data handling capability, Machine Learning and Probabilistic modelling enables an immense opportunity to employ predictive analytics platform in high security critical industries namely data centers, electricity grids, utilities, airport etc. where downtime minimization is one of the primary objectives. This paper proposes a novel, complete architecture of an intelligent predictive analytics platform, Fault Engine, for huge device network connected with electrical/information flow. Three unique modules, here proposed, seamlessly integrate with available technology stack of data handling and connect with middleware to produce online intelligent prediction in critical failure scenarios. The Markov Failure module predicts the severity of a failure along with survival probability of a device at any given instances. The Root Cause Analysis model indicates probable devices as potential root cause employing Bayesian probability assignment and topological sort. Finally, a community detection algorithm produces correlated clusters of device in terms of failure probability which will further narrow down the search space of finding route cause. The whole Engine has been tested with different size of network with simulated failure environments and shows its potential to be scalable in real-time implementation.\n    ",
        "submission_date": "2016-10-16T00:00:00",
        "last_modified_date": "2016-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04964",
        "title": "Improvements in Sub-optimal Solving of the $(N^2-1)$-Puzzle via Joint Relocation of Pebbles and its Applications to Rule-based Cooperative Path-Finding",
        "authors": [
            "Pavel Surynek",
            "Petr Michal\u00edk"
        ],
        "abstract": "The problem of solving $(n^2-1)$-puzzle and cooperative path-finding (CPF) sub-optimally by rule based algorithms is addressed in this manuscript. The task in the puzzle is to rearrange $n^2-1$ pebbles on the square grid of the size of n x n using one vacant position to a desired goal configuration. An improvement to the existent polynomial-time algorithm is proposed and experimentally analyzed. The improved algorithm is trying to move pebbles in a more efficient way than the original algorithm by grouping them into so-called snakes and moving them jointly within the snake. An experimental evaluation showed that the algorithm using snakes produces solutions that are 8% to 9% shorter than solutions generated by the original algorithm.\n",
        "submission_date": "2016-10-17T00:00:00",
        "last_modified_date": "2016-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05287",
        "title": "Internet of Things Applications: Animal Monitoring with Unmanned Aerial Vehicle",
        "authors": [
            "Jun Xu",
            "Gurkan Solmaz",
            "Rouhollah Rahmatizadeh",
            "Damla Turgut",
            "Ladislau Boloni"
        ],
        "abstract": "In animal monitoring applications, both animal detection and their movement prediction are major tasks. While a variety of animal monitoring strategies exist, most of them rely on mounting devices. However, in real world, it is difficult to find these animals and install mounting devices. In this paper, we propose an animal monitoring application by utilizing wireless sensor networks (WSNs) and unmanned aerial vehicle (UAV). The objective of the application is to detect locations of endangered species in large-scale wildlife areas and monitor movement of animals without any attached devices. In this application, sensors deployed throughout the observation area are responsible for gathering animal information. The UAV flies above the observation area and collects the information from sensors. To achieve the information efficiently, we propose a path planning approach for the UAV based on a Markov decision process (MDP) model. The UAV receives a certain amount of reward from an area if some animals are detected at that location. We solve the MDP using Q-learning such that the UAV prefers going to those areas that animals are detected before. Meanwhile, the UAV explores other areas as well to cover the entire network and detects changes in the animal positions. We first define the mathematical model underlying the animal monitoring problem in terms of the value of information (VoI) and rewards. We propose a network model including clusters of sensor nodes and a single UAV that acts as a mobile sink and visits the clusters. Then, one MDP-based path planning approach is designed to maximize the VoI while reducing message delays. The effectiveness of the proposed approach is evaluated using two real-world movement datasets of zebras and leopard. Simulation results show that our approach outperforms greedy, random heuristics and the path planning based on the traveling salesman problem.\n    ",
        "submission_date": "2016-10-17T00:00:00",
        "last_modified_date": "2016-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05402",
        "title": "VRPBench: A Vehicle Routing Benchmark Tool",
        "authors": [
            "Guilherme A. Zeni",
            "Mauro Menzori",
            "P. S. Martins",
            "Luis A. A. Meira"
        ],
        "abstract": "The number of optimization techniques in the combinatorial domain is large and diversified. Nevertheless, there is still a lack of real benchmarks to validate optimization algorithms. In this work we introduce VRPBench, a tool to create instances and visualize solutions to the Vehicle Routing Problem (VRP) in a planar graph embedded in the Euclidean 2D space. We use VRPBench to model a real-world mail delivery case of the city of Artur Nogueira. Such scenarios were characterized as a multi-objective optimization of the VRP. We extracted a weighted graph from a digital map of the city to create a challenging benchmark for the VRP. Each instance models one generic day of mail delivery with hundreds to thousands of delivery points, thus allowing both the comparison and validation of optimization algorithms for routing problems.\n    ",
        "submission_date": "2016-10-18T00:00:00",
        "last_modified_date": "2016-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05452",
        "title": "Makespan Optimal Solving of Cooperative Path-Finding via Reductions to Propositional Satisfiability",
        "authors": [
            "Pavel Surynek"
        ],
        "abstract": "The problem of makespan optimal solving of cooperative path finding (CPF) is addressed in this paper. The task in CPF is to relocate a group of agents in a non-colliding way so that each agent eventually reaches its goal location from the given initial location. The abstraction adopted in this work assumes that agents are discrete items moving in an undirected graph by traversing edges. Makespan optimal solving of CPF means to generate solutions that are as short as possi-ble in terms of the total number of time steps required for the execution of the solution.\n",
        "submission_date": "2016-10-18T00:00:00",
        "last_modified_date": "2016-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05521",
        "title": "Diagnosis of aerospace structure defects by a HPC implemented soft computing algorithm",
        "authors": [
            "Gianni D'Angelo",
            "Salvatore Rampone"
        ],
        "abstract": "This study concerns with the diagnosis of aerospace structure defects by applying a HPC parallel implementation of a novel learning algorithm, named U-BRAIN. The Soft Computing approach allows advanced multi-parameter data processing in composite materials testing. The HPC parallel implementation overcomes the limits due to the great amount of data and the complexity of data processing. Our experimental results illustrate the effectiveness of the U-BRAIN parallel implementation as defect classifier in aerospace structures. The resulting system is implemented on a Linux-based cluster with multi-core architecture.\n    ",
        "submission_date": "2016-10-18T00:00:00",
        "last_modified_date": "2016-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05551",
        "title": "Weighted Positive Binary Decision Diagrams for Exact Probabilistic Inference",
        "authors": [
            "Giso H. Dal",
            "Peter J.F. Lucas"
        ],
        "abstract": "Recent work on weighted model counting has been very successfully applied to the problem of probabilistic inference in Bayesian networks. The probability distribution is encoded into a Boolean normal form and compiled to a target language, in order to represent local structure expressed among conditional probabilities more efficiently. We show that further improvements are possible, by exploiting the knowledge that is lost during the encoding phase and incorporating it into a compiler inspired by Satisfiability Modulo Theories. Constraints among variables are used as a background theory, which allows us to optimize the Shannon decomposition. We propose a new language, called Weighted Positive Binary Decision Diagrams, that reduces the cost of probabilistic inference by using this decomposition variant to induce an arithmetic circuit of reduced size.\n    ",
        "submission_date": "2016-10-18T00:00:00",
        "last_modified_date": "2016-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05556",
        "title": "Identifiability and Transportability in Dynamic Causal Networks",
        "authors": [
            "Gilles Blondel",
            "Marta Arias",
            "Ricard Gavald\u00e0"
        ],
        "abstract": "In this paper we propose a causal analog to the purely observational Dynamic Bayesian Networks, which we call Dynamic Causal Networks. We provide a sound and complete algorithm for identification of Dynamic Causal Net- works, namely, for computing the effect of an intervention or experiment, based on passive observations only, whenever possible. We note the existence of two types of confounder variables that affect in substantially different ways the iden- tification procedures, a distinction with no analog in either Dynamic Bayesian Networks or standard causal graphs. We further propose a procedure for the transportability of causal effects in Dynamic Causal Network settings, where the re- sult of causal experiments in a source domain may be used for the identification of causal effects in a target domain.\n    ",
        "submission_date": "2016-10-18T00:00:00",
        "last_modified_date": "2016-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05612",
        "title": "Census Signal Temporal Logic Inference for Multi-Agent Group Behavior Analysis",
        "authors": [
            "Zhe Xu",
            "Agung Julius"
        ],
        "abstract": "In this paper, we define a novel census signal temporal logic (CensusSTL) that focuses on the number of agents in different subsets of a group that complete a certain task specified by the signal temporal logic (STL). CensusSTL consists of an \"inner logic\" STL formula and an \"outer logic\" STL formula. We present a new inference algorithm to infer CensusSTL formulae from the trajectory data of a group of agents. We first identify the \"inner logic\" STL formula and then infer the subgroups based on whether the agents' behaviors satisfy the \"inner logic\" formula at each time point. We use two different approaches to infer the subgroups based on similarity and complementarity, respectively. The \"outer logic\" CensusSTL formula is inferred from the census trajectories of different subgroups. We apply the algorithm in analyzing data from a soccer match by inferring the CensusSTL formula for different subgroups of a soccer team.\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05735",
        "title": "Deep Amortized Inference for Probabilistic Programs",
        "authors": [
            "Daniel Ritchie",
            "Paul Horsfall",
            "Noah D. Goodman"
        ],
        "abstract": "Probabilistic programming languages (PPLs) are a powerful modeling tool, able to represent any computable probability distribution. Unfortunately, probabilistic program inference is often intractable, and existing PPLs mostly rely on expensive, approximate sampling-based methods. To alleviate this problem, one could try to learn from past inferences, so that future inferences run faster. This strategy is known as amortized inference; it has recently been applied to Bayesian networks and deep generative models. This paper proposes a system for amortized inference in PPLs. In our system, amortization comes in the form of a parameterized guide program. Guide programs have similar structure to the original program, but can have richer data flow, including neural network components. These networks can be optimized so that the guide approximately samples from the posterior distribution defined by the original program. We present a flexible interface for defining guide programs and a stochastic gradient-based scheme for optimizing guide parameters, as well as some preliminary results on automatically deriving guide programs. We explore in detail the common machine learning pattern in which a 'local' model is specified by 'global' random values and used to generate independent observed data points; this gives rise to amortized local inference supporting global model learning.\n    ",
        "submission_date": "2016-10-18T00:00:00",
        "last_modified_date": "2016-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06009",
        "title": "Constrained Cohort Intelligence using Static and Dynamic Penalty Function Approach for Mechanical Components Design",
        "authors": [
            "Omkar Kulkarni",
            "Ninad Kulkarni",
            "Anand J Kulkarni",
            "Ganesh Kakandikar"
        ],
        "abstract": "Most of the metaheuristics can efficiently solve unconstrained problems; however, their performance may degenerate if the constraints are involved. This paper proposes two constraint handling approaches for an emerging metaheuristic of Cohort Intelligence (CI). More specifically CI with static penalty function approach (SCI) and CI with dynamic penalty function approach (DCI) are proposed. The approaches have been tested by solving several constrained test problems. The performance of the SCI and DCI have been compared with algorithms like GA, PSO, ABC, d-Ds. In addition, as well as three real world problems from mechanical engineering domain with improved solutions. The results were satisfactory and validated the applicability of CI methodology for solving real world problems.\n    ",
        "submission_date": "2016-09-26T00:00:00",
        "last_modified_date": "2016-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06402",
        "title": "A Growing Long-term Episodic & Semantic Memory",
        "authors": [
            "Marc Pickett",
            "Rami Al-Rfou",
            "Louis Shao",
            "Chris Tar"
        ],
        "abstract": "The long-term memory of most connectionist systems lies entirely in the weights of the system. Since the number of weights is typically fixed, this bounds the total amount of knowledge that can be learned and stored. Though this is not normally a problem for a neural network designed for a specific task, such a bound is undesirable for a system that continually learns over an open range of domains. To address this, we describe a lifelong learning system that leverages a fast, though non-differentiable, content-addressable memory which can be exploited to encode both a long history of sequential episodic knowledge and semantic knowledge over many episodes for an unbounded number of domains. This opens the door for investigation into transfer learning, and leveraging prior knowledge that has been learned over a lifetime of experiences to new domains.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2016-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06473",
        "title": "Generalized Interval-valued OWA Operators with Interval Weights Derived from Interval-valued Overlap Functions",
        "authors": [
            "Benjamin Bedregal",
            "Humberto Bustince",
            "Eduardo Palmeira",
            "Gra\u00e7aliz Pereira Dimuro",
            "Javier Fernandez"
        ],
        "abstract": "In this work we extend to the interval-valued setting the notion of an overlap functions and we discuss a method which makes use of interval-valued overlap functions for constructing OWA operators with interval-valued weights. . Some properties of interval-valued overlap functions and the derived interval-valued OWA operators are analysed. We specially focus on the homogeneity and migrativity properties.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2016-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06483",
        "title": "An Extended Neo-Fuzzy Neuron and its Adaptive Learning Algorithm",
        "authors": [
            "Yevgeniy V. Bodyanskiy",
            "Oleksii K. Tyshchenko",
            "Daria S. Kopaliani"
        ],
        "abstract": "A modification of the neo-fuzzy neuron is proposed (an extended neo-fuzzy neuron (ENFN)) that is characterized by improved approximating properties. An adaptive learning algorithm is proposed that has both tracking and smoothing properties. An ENFN distinctive feature is its computational simplicity compared to other artificial neural networks and neuro-fuzzy systems.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2016-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06484",
        "title": "An Evolving Cascade System Based on A Set Of Neo Fuzzy Nodes",
        "authors": [
            "Zhengbing Hu",
            "Yevgeniy V. Bodyanskiy",
            "Oleksii K. Tyshchenko",
            "Olena O. Boiko"
        ],
        "abstract": "Neo-fuzzy elements are used as nodes for an evolving cascade system. The proposed system can tune both its parameters and architecture in an online mode. It can be used for solving a wide range of Data Mining tasks (namely time series forecasting). The evolving cascade system with neo-fuzzy nodes can process rather large data sets with high speed and effectiveness.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2016-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06485",
        "title": "A Multidimensional Cascade Neuro-Fuzzy System with Neuron Pool Optimization in Each Cascade",
        "authors": [
            "Yevgeniy V. Bodyanskiy",
            "Oleksii K. Tyshchenko",
            "Daria S. Kopaliani"
        ],
        "abstract": "A new architecture and learning algorithms for the multidimensional hybrid cascade neural network with neuron pool optimization in each cascade are proposed in this paper. The proposed system differs from the well-known cascade systems in its capability to process multidimensional time series in an online mode, which makes it possible to process non-stationary stochastic and chaotic signals with the required accuracy. Compared to conventional analogs, the proposed system provides computational simplicity and possesses both tracking and filtering capabilities.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2016-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06486",
        "title": "Adaptive Forecasting of Non-Stationary Nonlinear Time Series Based on the Evolving Weighted Neuro-Neo-Fuzzy-ANARX-Model",
        "authors": [
            "Zhengbing Hu",
            "Yevgeniy V. Bodyanskiy",
            "Oleksii K. Tyshchenko",
            "Olena O. Boiko"
        ],
        "abstract": "An evolving weighted neuro-neo-fuzzy-ANARX model and its learning procedures are introduced in the article. This system is basically used for time series forecasting. This system may be considered as a pool of elements that process data in a parallel manner. The proposed evolving system may provide online processing data streams.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2016-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06488",
        "title": "An Evolving Neuro-Fuzzy System with Online Learning/Self-learning",
        "authors": [
            "Yevgeniy V. Bodyanskiy",
            "Oleksii K. Tyshchenko",
            "Anastasiia O. Deineko"
        ],
        "abstract": "An architecture of a new neuro-fuzzy system is proposed. The basic idea of this approach is to tune both synaptic weights and membership functions with the help of the supervised learning and self-learning paradigms. The approach to solving the problem has to do with evolving online neuro-fuzzy systems that can process data under uncertainty conditions. The results prove the effectiveness of the developed architecture and the learning procedure.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2016-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06490",
        "title": "An Ensemble of Adaptive Neuro-Fuzzy Kohonen Networks for Online Data Stream Fuzzy Clustering",
        "authors": [
            "Zhengbing Hu",
            "Yevgeniy V. Bodyanskiy",
            "Oleksii K. Tyshchenko",
            "Olena O. Boiko"
        ],
        "abstract": "A new approach to data stream clustering with the help of an ensemble of adaptive neuro-fuzzy systems is proposed. The proposed ensemble is formed with adaptive neuro-fuzzy self-organizing Kohonen maps in a parallel processing mode. A final result is chosen by the best neuro-fuzzy self-organizing Kohonen map.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2016-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06912",
        "title": "KGEval: Estimating Accuracy of Automatically Constructed Knowledge Graphs",
        "authors": [
            "Prakhar Ojha",
            "Partha Talukdar"
        ],
        "abstract": "Automatic construction of large knowledge graphs (KG) by mining web-scale text datasets has received considerable attention recently. Estimating accuracy of such automatically constructed KGs is a challenging problem due to their size and diversity. This important problem has largely been ignored in prior research we fill this gap and propose KGEval. KGEval binds facts of a KG using coupling constraints and crowdsources the facts that infer correctness of large parts of the KG. We demonstrate that the objective optimized by KGEval is submodular and NP-hard, allowing guarantees for our approximation algorithm. Through extensive experiments on real-world datasets, we demonstrate that KGEval is able to estimate KG accuracy more accurately compared to other competitive baselines, while requiring significantly lesser number of human evaluations.\n    ",
        "submission_date": "2016-10-21T00:00:00",
        "last_modified_date": "2016-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06940",
        "title": "Safety Verification of Deep Neural Networks",
        "authors": [
            "Xiaowei Huang",
            "Marta Kwiatkowska",
            "Sen Wang",
            "Min Wu"
        ],
        "abstract": "Deep neural networks have achieved impressive experimental results in image classification, but can surprisingly be unstable with respect to adversarial perturbations, that is, minimal changes to the input image that cause the network to misclassify it. With potential applications including perception modules and end-to-end controllers for self-driving cars, this raises concerns about their safety. We develop a novel automated verification framework for feed-forward multi-layer neural networks based on Satisfiability Modulo Theory (SMT). We focus on safety of image classification decisions with respect to image manipulations, such as scratches or changes to camera angle or lighting conditions that would result in the same class being assigned by a human, and define safety for an individual decision in terms of invariance of the classification within a small neighbourhood of the original image. We enable exhaustive search of the region by employing discretisation, and propagate the analysis layer by layer. Our method works directly with the network code and, in contrast to existing methods, can guarantee that adversarial examples, if they exist, are found for the given region and family of manipulations. If found, adversarial examples can be shown to human testers and/or used to fine-tune the network. We implement the techniques using Z3 and evaluate them on state-of-the-art networks, including regularised and deep learning networks. We also compare against existing techniques to search for adversarial examples and estimate network robustness.\n    ",
        "submission_date": "2016-10-21T00:00:00",
        "last_modified_date": "2017-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06972",
        "title": "Learning Cost-Effective Treatment Regimes using Markov Decision Processes",
        "authors": [
            "Himabindu Lakkaraju",
            "Cynthia Rudin"
        ],
        "abstract": "Decision makers, such as doctors and judges, make crucial decisions such as recommending treatments to patients, and granting bails to defendants on a daily basis. Such decisions typically involve weighting the potential benefits of taking an action against the costs involved. In this work, we aim to automate this task of learning \\emph{cost-effective, interpretable and actionable treatment regimes}. We formulate this as a problem of learning a decision list -- a sequence of if-then-else rules -- which maps characteristics of subjects (eg., diagnostic test results of patients) to treatments. We propose a novel objective to construct a decision list which maximizes outcomes for the population, and minimizes overall costs. We model the problem of learning such a list as a Markov Decision Process (MDP) and employ a variant of the Upper Confidence Bound for Trees (UCT) strategy which leverages customized checks for pruning the search space effectively. Experimental results on real world observational data capturing judicial bail decisions and treatment recommendations for asthma patients demonstrate the effectiveness of our approach.\n    ",
        "submission_date": "2016-10-21T00:00:00",
        "last_modified_date": "2016-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07045",
        "title": "pg-Causality: Identifying Spatiotemporal Causal Pathways for Air Pollutants with Urban Big Data",
        "authors": [
            "Julie Yixuan Zhu",
            "Chao Zhang",
            "Huichu Zhang",
            "Shi Zhi",
            "Victor O.K. Li",
            "Jiawei Han",
            "Yu Zheng"
        ],
        "abstract": "Many countries are suffering from severe air pollution. Understanding how different air pollutants accumulate and propagate is critical to making relevant public policies. In this paper, we use urban big data (air quality data and meteorological data) to identify the \\emph{spatiotemporal (ST) causal pathways} for air pollutants. This problem is challenging because: (1) there are numerous noisy and low-pollution periods in the raw air quality data, which may lead to unreliable causality analysis, (2) for large-scale data in the ST space, the computational complexity of constructing a causal structure is very high, and (3) the \\emph{ST causal pathways} are complex due to the interactions of multiple pollutants and the influence of environmental factors. Therefore, we present \\emph{p-Causality}, a novel pattern-aided causality analysis approach that combines the strengths of \\emph{pattern mining} and \\emph{Bayesian learning} to efficiently and faithfully identify the \\emph{ST causal pathways}. First, \\emph{Pattern mining} helps suppress the noise by capturing frequent evolving patterns (FEPs) of each monitoring sensor, and greatly reduce the complexity by selecting the pattern-matched sensors as \"causers\". Then, \\emph{Bayesian learning} carefully encodes the local and ST causal relations with a Gaussian Bayesian network (GBN)-based graphical model, which also integrates environmental influences to minimize biases in the final results. We evaluate our approach with three real-world data sets containing 982 air quality sensors, in three regions of China from 01-Jun-2013 to 19-Dec-2015. Results show that our approach outperforms the traditional causal structure learning methods in time efficiency, inference accuracy and interpretability.\n    ",
        "submission_date": "2016-10-22T00:00:00",
        "last_modified_date": "2018-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07089",
        "title": "Reinforcement Learning in Conflicting Environments for Autonomous Vehicles",
        "authors": [
            "Dominik Meyer",
            "Johannes Feldmaier",
            "Hao Shen"
        ],
        "abstract": "In this work, we investigate the application of Reinforcement Learning to two well known decision dilemmas, namely Newcomb's Problem and Prisoner's Dilemma. These problems are exemplary for dilemmas that autonomous agents are faced with when interacting with humans. Furthermore, we argue that a Newcomb-like formulation is more adequate in the human-machine interaction case and demonstrate empirically that the unmodified Reinforcement Learning algorithms end up with the well known maximum expected utility solution.\n    ",
        "submission_date": "2016-10-22T00:00:00",
        "last_modified_date": "2016-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07388",
        "title": "Characterization of an inconsistency ranking for pairwise comparison matrices",
        "authors": [
            "L\u00e1szl\u00f3 Csat\u00f3"
        ],
        "abstract": "Pairwise comparisons between alternatives are a well-known method for measuring preferences of a decision-maker. Since these often do not exhibit consistency, a number of inconsistency indices has been introduced in order to measure the deviation from this ideal case. We axiomatically characterize the inconsistency ranking induced by the Koczkodaj inconsistency index: six independent properties are presented such that they determine a unique linear order on the set of all pairwise comparison matrices.\n    ",
        "submission_date": "2016-10-24T00:00:00",
        "last_modified_date": "2017-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07432",
        "title": "Virtual Embodiment: A Scalable Long-Term Strategy for Artificial Intelligence Research",
        "authors": [
            "Douwe Kiela",
            "Luana Bulat",
            "Anita L. Vero",
            "Stephen Clark"
        ],
        "abstract": "Meaning has been called the \"holy grail\" of a variety of scientific disciplines, ranging from linguistics to philosophy, psychology and the neurosciences. The field of Artifical Intelligence (AI) is very much a part of that list: the development of sophisticated natural language semantics is a sine qua non for achieving a level of intelligence comparable to humans. Embodiment theories in cognitive science hold that human semantic representation depends on sensori-motor experience; the abundant evidence that human meaning representation is grounded in the perception of physical reality leads to the conclusion that meaning must depend on a fusion of multiple (perceptual) modalities. Despite this, AI research in general, and its subdisciplines such as computational linguistics and computer vision in particular, have focused primarily on tasks that involve a single modality. Here, we propose virtual embodiment as an alternative, long-term strategy for AI research that is multi-modal in nature and that allows for the kind of scalability required to develop the field coherently and incrementally, in an ethically responsible fashion.\n    ",
        "submission_date": "2016-10-24T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07505",
        "title": "Balancing Suspense and Surprise: Timely Decision Making with Endogenous Information Acquisition",
        "authors": [
            "Ahmed M. Alaa",
            "Mihaela van der Schaar"
        ],
        "abstract": "We develop a Bayesian model for decision-making under time pressure with endogenous information acquisition. In our model, the decision maker decides when to observe (costly) information by sampling an underlying continuous-time stochastic process (time series) that conveys information about the potential occurrence or non-occurrence of an adverse event which will terminate the decision-making process. In her attempt to predict the occurrence of the adverse event, the decision-maker follows a policy that determines when to acquire information from the time series (continuation), and when to stop acquiring information and make a final prediction (stopping). We show that the optimal policy has a rendezvous structure, i.e. a structure in which whenever a new information sample is gathered from the time series, the optimal \"date\" for acquiring the next sample becomes computable. The optimal interval between two information samples balances a trade-off between the decision maker's surprise, i.e. the drift in her posterior belief after observing new information, and suspense, i.e. the probability that the adverse event occurs in the time interval between two information samples. Moreover, we characterize the continuation and stopping regions in the decision-maker's state-space, and show that they depend not only on the decision-maker's beliefs, but also on the context, i.e. the current realization of the time series.\n    ",
        "submission_date": "2016-10-24T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07708",
        "title": "Knowledge will Propel Machine Understanding of Content: Extrapolating from Current Examples",
        "authors": [
            "Amit Sheth",
            "Sujan Perera",
            "Sanjaya Wijeratne"
        ],
        "abstract": "Machine Learning has been a big success story during the AI resurgence. One particular stand out success relates to unsupervised learning from a massive amount of data, albeit much of it relates to one modality/type of data at a time. In spite of early assertions of the unreasonable effectiveness of data, there is increasing recognition of utilizing knowledge whenever it is available or can be created purposefully. In this paper, we focus on discussing the indispensable role of knowledge for deeper understanding of complex text and multimodal data in situations where (i) large amounts of training data (labeled/unlabeled) are not available or labor intensive to create, (ii) the objects (particularly text) to be recognized are complex (i.e., beyond simple entity-person/location/organization names), such as implicit entities and highly subjective content, and (iii) applications need to use complementary or related data in multiple modalities/media. What brings us to the cusp of rapid progress is our ability to (a) create knowledge, varying from comprehensive or cross domain to domain or application specific, and (b) carefully exploit the knowledge to further empower or extend the applications of ML/NLP techniques. Using the early results in several diverse situations - both in data types and applications - we seek to foretell unprecedented progress in our ability for deeper understanding and exploitation of multimodal data.\n    ",
        "submission_date": "2016-10-25T00:00:00",
        "last_modified_date": "2019-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07862",
        "title": "Intelligence in Artificial Intelligence",
        "authors": [
            "Shoumen Palit Austin Datta"
        ],
        "abstract": "The elusive quest for intelligence in artificial intelligence prompts us to consider that instituting human-level intelligence in systems may be (still) in the realm of utopia. In about a quarter century, we have witnessed the winter of AI (1990) being transformed and transported to the zenith of tabloid fodder about AI (2015). The discussion at hand is about the elements that constitute the canonical idea of intelligence. The delivery of intelligence as a pay-per-use-service, popping out of an app or from a shrink-wrapped software defined point solution, is in contrast to the bio-inspired view of intelligence as an outcome, perhaps formed from a tapestry of events, cross-pollinated by instances, each with its own microcosm of experiences and learning, which may not be discrete all-or-none functions but continuous, over space and time. The enterprise world may not require, aspire or desire such an engaged solution to improve its services for enabling digital transformation through the deployment of digital twins, for example. One might ask whether the \"work-flow on steroids\" version of decision support may suffice for intelligence? Are we harking back to the era of rule based expert systems? The image conjured by the publicity machines offers deep solutions with human-level AI and preposterous claims about capturing the \"brain in a box\" by 2020. Even emulating insects may be difficult in terms of real progress. Perhaps we can try to focus on worms (Caenorhabditis elegans) which may be better suited for what business needs to quench its thirst for so-called intelligence in AI.\n    ",
        "submission_date": "2016-10-24T00:00:00",
        "last_modified_date": "2016-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07989",
        "title": "Process Discovery using Inductive Miner and Decomposition",
        "authors": [
            "Raji Ghawi"
        ],
        "abstract": "This report presents a submission to the Process Discovery Contest. The contest is dedicated to the assessment of tools and techniques that discover business process models from event logs. The objective is to compare the efficiency of techniques to discover process models that provide a proper balance between \"overfitting\" and \"underfitting\". In the context of the Process Discovery Contest, process discovery is turned into a classification task with a training set and a test set; where a process model needs to decide whether traces are fitting or not. In this report, we first show how we use two discovery techniques, namely: Inductive Miner and Decomposition, to discover process models from the training set using ProM tool. Second, we show how we use replay results to 1) check the rediscoverability of models, and to 2) classify unseen traces (in test logs) as fitting or not. Then, we discuss the classification results of validation logs, the complexity of discovered models, and their impact on the selection of models for submission. The report ends with the pictures of the submitted process models.\n    ",
        "submission_date": "2016-10-25T00:00:00",
        "last_modified_date": "2016-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07997",
        "title": "Artificial Intelligence Safety and Cybersecurity: a Timeline of AI Failures",
        "authors": [
            "Roman V. Yampolskiy",
            "M. S. Spellchecker"
        ],
        "abstract": "In this work, we present and analyze reported failures of artificially intelligent systems and extrapolate our analysis to future AIs. We suggest that both the frequency and the seriousness of future AI failures will steadily increase. AI Safety can be improved based on ideas developed by cybersecurity experts. For narrow AIs safety failures are at the same, moderate, level of criticality as in cybersecurity, however for general AI, failures have a fundamentally different impact. A single failure of a superintelligent system may cause a catastrophic event without a chance for recovery. The goal of cybersecurity is to reduce the number of successful attacks on the system; the goal of AI Safety is to make sure zero attacks succeed in bypassing the safety mechanisms. Unfortunately, such a level of performance is unachievable. Every security system will eventually fail; there is no such thing as a 100% secure system.\n    ",
        "submission_date": "2016-10-25T00:00:00",
        "last_modified_date": "2016-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08115",
        "title": "A Physician Advisory System for Chronic Heart Failure Management Based on Knowledge Patterns",
        "authors": [
            "Zhuo Chen",
            "Kyle Marple",
            "Elmer Salazar",
            "Gopal Gupta",
            "Lakshman Tamil"
        ],
        "abstract": "Management of chronic diseases such as heart failure, diabetes, and chronic obstructive pulmonary disease (COPD) is a major problem in health care. A standard approach that the medical community has devised to manage widely prevalent chronic diseases such as chronic heart failure (CHF) is to have a committee of experts develop guidelines that all physicians should follow. These guidelines typically consist of a series of complex rules that make recommendations based on a patient's information. Due to their complexity, often the guidelines are either ignored or not complied with at all, which can result in poor medical practices. It is not even clear whether it is humanly possible to follow these guidelines due to their length and complexity. In the case of CHF management, the guidelines run nearly 80 pages. In this paper we describe a physician-advisory system for CHF management that codes the entire set of clinical practice guidelines for CHF using answer set programming. Our approach is based on developing reasoning templates (that we call knowledge patterns) and using these patterns to systemically code the clinical guidelines for CHF as ASP rules. Use of the knowledge patterns greatly facilitates the development of our system. Given a patient's medical information, our system generates a recommendation for treatment just as a human physician would, using the guidelines. Our system will work even in the presence of incomplete information. Our work makes two contributions: (i) it shows that highly complex guidelines can be successfully coded as ASP rules, and (ii) it develops a series of knowledge patterns that facilitate the coding of knowledge expressed in a natural language and that can be used for other application domains. This paper is under consideration for acceptance in TPLP.\n    ",
        "submission_date": "2016-10-25T00:00:00",
        "last_modified_date": "2016-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08222",
        "title": "A self-tuning Firefly algorithm to tune the parameters of Ant Colony System (ACSFA)",
        "authors": [
            "M. K. A. Ariyaratne",
            "T. G. I. Fernando",
            "S. Weerakoon"
        ],
        "abstract": "Ant colony system (ACS) is a promising approach which has been widely used in problems such as Travelling Salesman Problems (TSP), Job shop scheduling problems (JSP) and Quadratic Assignment problems (QAP). In its original implementation, parameters of the algorithm were selected by trial and error approach. Over the last few years, novel approaches have been proposed on adapting the parameters of ACS in improving its performance. The aim of this paper is to use a framework introduced for self-tuning optimization algorithms combined with the firefly algorithm (FA) to tune the parameters of the ACS solving symmetric TSP problems. The FA optimizes the problem specific parameters of ACS while the parameters of the FA are tuned by the selected framework itself. With this approach, the user neither has to work with the parameters of ACS nor the parameters of FA. Using common symmetric TSP problems we demonstrate that the framework fits well for the ACS. A detailed statistical analysis further verifies the goodness of the new ACS over the existing ACS and also of the other techniques used to tune the parameters of ACS.\n    ",
        "submission_date": "2016-10-26T00:00:00",
        "last_modified_date": "2016-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08445",
        "title": "New Liftable Classes for First-Order Probabilistic Inference",
        "authors": [
            "Seyed Mehran Kazemi",
            "Angelika Kimmig",
            "Guy Van den Broeck",
            "David Poole"
        ],
        "abstract": "Statistical relational models provide compact encodings of probabilistic dependencies in relational domains, but result in highly intractable graphical models. The goal of lifted inference is to carry out probabilistic inference without needing to reason about each individual separately, by instead treating exchangeable, undistinguished objects as a whole. In this paper, we study the domain recursion inference rule, which, despite its central role in early theoretical results on domain-lifted inference, has later been believed redundant. We show that this rule is more powerful than expected, and in fact significantly extends the range of models for which lifted inference runs in time polynomial in the number of individuals in the domain. This includes an open problem called S4, the symmetric transitivity model, and a first-order logic encoding of the birthday paradox. We further identify new classes S2FO2 and S2RU of domain-liftable theories, which respectively subsume FO2 and recursively unary theories, the largest classes of domain-liftable theories known so far, and show that using domain recursion can achieve exponential speedup even in theories that cannot fully be lifted with the existing set of inference rules.\n    ",
        "submission_date": "2016-10-26T00:00:00",
        "last_modified_date": "2016-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08602",
        "title": "A Review of 40 Years of Cognitive Architecture Research: Core Cognitive Abilities and Practical Applications",
        "authors": [
            "Iuliia Kotseruba",
            "John K. Tsotsos"
        ],
        "abstract": "In this paper we present a broad overview of the last 40 years of research on cognitive architectures. Although the number of existing architectures is nearing several hundred, most of the existing surveys do not reflect this growth and focus on a handful of well-established architectures. Thus, in this survey we wanted to shift the focus towards a more inclusive and high-level overview of the research on cognitive architectures. Our final set of 84 architectures includes 49 that are still actively developed, and borrow from a diverse set of disciplines, spanning areas from psychoanalysis to neuroscience. To keep the length of this paper within reasonable limits we discuss only the core cognitive abilities, such as perception, attention mechanisms, action selection, memory, learning and reasoning. In order to assess the breadth of practical applications of cognitive architectures we gathered information on over 900 practical projects implemented using the cognitive architectures in our list. We use various visualization techniques to highlight overall trends in the development of the field. In addition to summarizing the current state-of-the-art in the cognitive architecture research, this survey describes a variety of methods and ideas that have been tried and their relative success in modeling human cognitive abilities, as well as which aspects of cognitive behavior need more research with respect to their mechanistic counterparts and thus can further inform how cognitive science might progress.\n    ",
        "submission_date": "2016-10-27T00:00:00",
        "last_modified_date": "2018-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08640",
        "title": "Anomaly Detection with the Voronoi Diagram Evolutionary Algorithm",
        "authors": [
            "Marti Luis",
            "Fansi-Tchango Arsene",
            "Navarro Laurent",
            "Marc Schoenauer"
        ],
        "abstract": "This paper presents the Voronoi diagram-based evolutionary algorithm (VorEAl). VorEAl partitions input space in abnormal/normal subsets using Voronoi diagrams. Diagrams are evolved using a multi-objective bio-inspired approach in order to conjointly optimize classification metrics while also being able to represent areas of the data space that are not present in the training dataset. As part of the paper VorEAl is experimentally validated and contrasted with similar approaches.\n    ",
        "submission_date": "2016-10-27T00:00:00",
        "last_modified_date": "2016-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08853",
        "title": "Personalized Risk Scoring for Critical Care Prognosis using Mixtures of Gaussian Processes",
        "authors": [
            "Ahmed M. Alaa",
            "Jinsung Yoon",
            "Scott Hu",
            "Mihaela van der Schaar"
        ],
        "abstract": "Objective: In this paper, we develop a personalized real-time risk scoring algorithm that provides timely and granular assessments for the clinical acuity of ward patients based on their (temporal) lab tests and vital signs; the proposed risk scoring system ensures timely intensive care unit (ICU) admissions for clinically deteriorating patients. Methods: The risk scoring system learns a set of latent patient subtypes from the offline electronic health record data, and trains a mixture of Gaussian Process (GP) experts, where each expert models the physiological data streams associated with a specific patient subtype. Transfer learning techniques are used to learn the relationship between a patient's latent subtype and her static admission information (e.g. age, gender, transfer status, ICD-9 codes, etc). Results: Experiments conducted on data from a heterogeneous cohort of 6,321 patients admitted to Ronald Reagan UCLA medical center show that our risk score significantly and consistently outperforms the currently deployed risk scores, such as the Rothman index, MEWS, APACHE and SOFA scores, in terms of timeliness, true positive rate (TPR), and positive predictive value (PPV). Conclusion: Our results reflect the importance of adopting the concepts of personalized medicine in critical care settings; significant accuracy and timeliness gains can be achieved by accounting for the patients' heterogeneity. Significance: The proposed risk scoring methodology can confer huge clinical and social benefits on more than 200,000 critically ill inpatient who exhibit cardiac arrests in the US every year.\n    ",
        "submission_date": "2016-10-27T00:00:00",
        "last_modified_date": "2016-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09064",
        "title": "Identifying Unknown Unknowns in the Open World: Representations and Policies for Guided Exploration",
        "authors": [
            "Himabindu Lakkaraju",
            "Ece Kamar",
            "Rich Caruana",
            "Eric Horvitz"
        ],
        "abstract": "Predictive models deployed in the real world may assign incorrect labels to instances with high confidence. Such errors or unknown unknowns are rooted in model incompleteness, and typically arise because of the mismatch between training data and the cases encountered at test time. As the models are blind to such errors, input from an oracle is needed to identify these failures. In this paper, we formulate and address the problem of informed discovery of unknown unknowns of any given predictive model where unknown unknowns occur due to systematic biases in the training data. We propose a model-agnostic methodology which uses feedback from an oracle to both identify unknown unknowns and to intelligently guide the discovery. We employ a two-phase approach which first organizes the data into multiple partitions based on the feature similarity of instances and the confidence scores assigned by the predictive model, and then utilizes an explore-exploit strategy for discovering unknown unknowns across these partitions. We demonstrate the efficacy of our framework by varying the underlying causes of unknown unknowns across various applications. To the best of our knowledge, this paper presents the first algorithmic approach to the problem of discovering unknown unknowns of predictive models.\n    ",
        "submission_date": "2016-10-28T00:00:00",
        "last_modified_date": "2016-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09263",
        "title": "Flexible constrained sampling with guarantees for pattern mining",
        "authors": [
            "Vladimir Dzyuba",
            "Matthijs van Leeuwen",
            "Luc De Raedt"
        ],
        "abstract": "Pattern sampling has been proposed as a potential solution to the infamous pattern explosion. Instead of enumerating all patterns that satisfy the constraints, individual patterns are sampled proportional to a given quality measure. Several sampling algorithms have been proposed, but each of them has its limitations when it comes to 1) flexibility in terms of quality measures and constraints that can be used, and/or 2) guarantees with respect to sampling accuracy. We therefore present Flexics, the first flexible pattern sampler that supports a broad class of quality measures and constraints, while providing strong guarantees regarding sampling accuracy. To achieve this, we leverage the perspective on pattern mining as a constraint satisfaction problem and build upon the latest advances in sampling solutions in SAT as well as existing pattern mining algorithms. Furthermore, the proposed algorithm is applicable to a variety of pattern languages, which allows us to introduce and tackle the novel task of sampling sets of patterns. We introduce and empirically evaluate two variants of Flexics: 1) a generic variant that addresses the well-known itemset sampling task and the novel pattern set sampling task as well as a wide range of expressive constraints within these tasks, and 2) a specialized variant that exploits existing frequent itemset techniques to achieve substantial speed-ups. Experiments show that Flexics is both accurate and efficient, making it a useful tool for pattern-based data exploration.\n    ",
        "submission_date": "2016-10-28T00:00:00",
        "last_modified_date": "2017-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09372",
        "title": "A Projective Simulation Scheme for Partially-Observable Multi-Agent Systems",
        "authors": [
            "Rasoul Kheiri"
        ],
        "abstract": "We introduce a kind of partial observability to the projective simulation (PS) learning method. It is done by adding a belief projection operator and an observability parameter to the original framework of the efficiency of the PS model. I provide theoretical formulations, network representations, and situated scenarios derived from the invasion toy problem as a starting point for some multi-agent PS models.\n    ",
        "submission_date": "2016-10-29T00:00:00",
        "last_modified_date": "2019-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09409",
        "title": "Probabilistic Model Checking for Complex Cognitive Tasks -- A case study in human-robot interaction",
        "authors": [
            "Sebastian Junges",
            "Nils Jansen",
            "Joost-Pieter Katoen",
            "Ufuk Topcu"
        ],
        "abstract": "This paper proposes to use probabilistic model checking to synthesize optimal robot policies in multi-tasking autonomous systems that are subject to human-robot interaction. Given the convincing empirical evidence that human behavior can be related to reinforcement models, we take as input a well-studied Q-table model of the human behavior for flexible scenarios. We first describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary but fixed scenario. The distinctive issue is that -- in contrast to existing models -- under-specification of the human behavior is included. Probabilistic model checking is used to predict the human's behavior. Finally, the MDP model is extended with a robot model. Optimal robot policies are synthesized by analyzing the resulting two-player stochastic game. Experimental results with a prototypical implementation using PRISM show promising results.\n    ",
        "submission_date": "2016-10-28T00:00:00",
        "last_modified_date": "2016-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09882",
        "title": "A Survey of Brain Inspired Technologies for Engineering",
        "authors": [
            "Jarryd Son",
            "Amit Kumar Mishra"
        ],
        "abstract": "Cognitive engineering is a multi-disciplinary field and hence it is difficult to find a review article consolidating the leading developments in the field. The in-credible pace at which technology is advancing pushes the boundaries of what is achievable in cognitive engineering. There are also differing approaches to cognitive engineering brought about from the multi-disciplinary nature of the field and the vastness of possible applications. Thus research communities require more frequent reviews to keep up to date with the latest trends. In this paper we shall dis-cuss some of the approaches to cognitive engineering holistically to clarify the reasoning behind the different approaches and to highlight their strengths and weaknesses. We shall then show how developments from seemingly disjointed views could be integrated to achieve the same goal of creating cognitive machines. By reviewing the major contributions in the different fields and showing the potential for a combined approach, this work intends to assist the research community in devising more unified methods and techniques for developing cognitive machines.\n    ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2016-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09900",
        "title": "Inference Compilation and Universal Probabilistic Programming",
        "authors": [
            "Tuan Anh Le",
            "Atilim Gunes Baydin",
            "Frank Wood"
        ],
        "abstract": "We introduce a method for using deep neural networks to amortize the cost of inference in models from the family induced by universal probabilistic programming languages, establishing a framework that combines the strengths of probabilistic programming and deep learning methods. We call what we do \"compilation of inference\" because our method transforms a denotational specification of an inference problem in the form of a probabilistic program written in a universal programming language into a trained neural network denoted in a neural network specification language. When at test time this neural network is fed observational data and executed, it performs approximate inference in the original model specified by the probabilistic program. Our training objective and learning procedure are designed to allow the trained neural network to be used as a proposal distribution in a sequential importance sampling inference engine. We illustrate our method on mixture models and Captcha solving and show significant speedups in the efficiency of inference.\n    ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2017-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09964",
        "title": "Ontology Verbalization using Semantic-Refinement",
        "authors": [
            "Vinu E.V",
            "P Sreenivasa Kumar"
        ],
        "abstract": "We propose a rule-based technique to generate redundancy-free NL descriptions of OWL ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2016-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00094",
        "title": "Learning recurrent representations for hierarchical behavior modeling",
        "authors": [
            "Eyrun Eyjolfsdottir",
            "Kristin Branson",
            "Yisong Yue",
            "Pietro Perona"
        ],
        "abstract": "We propose a framework for detecting action patterns from motion sequences and modeling the sensory-motor relationship of animals, using a generative recurrent neural network. The network has a discriminative part (classifying actions) and a generative part (predicting motion), whose recurrent cells are laterally connected, allowing higher levels of the network to represent high level phenomena. We test our framework on two types of data, fruit fly behavior and online handwriting. Our results show that 1) taking advantage of unlabeled sequences, by predicting future motion, significantly improves action detection performance when training labels are scarce, 2) the network learns to represent high level phenomena such as writer identity and fly gender, without supervision, and 3) simulated motion trajectories, generated by treating motion prediction as input to the network, look realistic and may be used to qualitatively evaluate whether the model has learnt generative control rules.\n    ",
        "submission_date": "2016-11-01T00:00:00",
        "last_modified_date": "2016-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00183",
        "title": "Local Subspace-Based Outlier Detection using Global Neighbourhoods",
        "authors": [
            "Bas van Stein",
            "Matthijs van Leeuwen",
            "Thomas B\u00e4ck"
        ],
        "abstract": "Outlier detection in high-dimensional data is a challenging yet important task, as it has applications in, e.g., fraud detection and quality control. State-of-the-art density-based algorithms perform well because they 1) take the local neighbourhoods of data points into account and 2) consider feature subspaces. In highly complex and high-dimensional data, however, existing methods are likely to overlook important outliers because they do not explicitly take into account that the data is often a mixture distribution of multiple components.\n",
        "submission_date": "2016-11-01T00:00:00",
        "last_modified_date": "2016-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00274",
        "title": "Detecting Affordances by Visuomotor Simulation",
        "authors": [
            "Wolfram Schenck",
            "Hendrik Hasenbein",
            "Ralf M\u00f6ller"
        ],
        "abstract": "The term \"affordance\" denotes the behavioral meaning of objects. We propose a cognitive architecture for the detection of affordances in the visual modality. This model is based on the internal simulation of movement sequences. For each movement step, the resulting sensory state is predicted by a forward model, which in turn triggers the generation of a new (simulated) motor command by an inverse model. Thus, a series of mental images in the sensory and in the motor domain is evoked. Starting from a real sensory state, a large number of such sequences is simulated in parallel. Final affordance detection is based on the generated motor commands. We apply this model to a real-world mobile robot which is faced with obstacle arrangements some of which are passable (corridor) and some of which are not (dead ends). The robot's task is to detect the right affordance (\"pass-through-able\" or \"non-pass-through-able\"). The required internal models are acquired in a hierarchical training process. Afterwards, the robotic agent is able to distinguish reliably between corridors and dead ends. This real-world result enhances the validity of the proposed mental simulation approach. In addition, we compare several key factors in the simulation process regarding performance and efficiency.\n    ",
        "submission_date": "2016-11-01T00:00:00",
        "last_modified_date": "2016-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00538",
        "title": "An application of incomplete pairwise comparison matrices for ranking top tennis players",
        "authors": [
            "S\u00e1ndor Boz\u00f3ki",
            "L\u00e1szl\u00f3 Csat\u00f3",
            "J\u00f3zsef Temesi"
        ],
        "abstract": "Pairwise comparison is an important tool in multi-attribute decision making. Pairwise comparison matrices (PCM) have been applied for ranking criteria and for scoring alternatives according to a given criterion. Our paper presents a special application of incomplete PCMs: ranking of professional tennis players based on their results against each other. The selected 25 players have been on the top of the ATP rankings for a shorter or longer period in the last 40 years. Some of them have never met on the court. One of the aims of the paper is to provide ranking of the selected players, however, the analysis of incomplete pairwise comparison matrices is also in the focus. The eigenvector method and the logarithmic least squares method were used to calculate weights from incomplete PCMs. In our results the top three players of four decades were Nadal, Federer and Sampras. Some questions have been raised on the properties of incomplete PCMs and remains open for further investigation.\n    ",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00549",
        "title": "Inferring Coupling of Distributed Dynamical Systems via Transfer Entropy",
        "authors": [
            "Oliver M. Cliff",
            "Mikhail Prokopenko",
            "Robert Fitch"
        ],
        "abstract": "In this work, we are interested in structure learning for a set of spatially distributed dynamical systems, where individual subsystems are coupled via latent variables and observed through a filter. We represent this model as a directed acyclic graph (DAG) that characterises the unidirectional coupling between subsystems. Standard approaches to structure learning are not applicable in this framework due to the hidden variables, however we can exploit the properties of certain dynamical systems to formulate exact methods based on state space reconstruction. We approach the problem by using reconstruction theorems to analytically derive a tractable expression for the KL-divergence of a candidate DAG from the observed dataset. We show this measure can be decomposed as a function of two information-theoretic measures, transfer entropy and stochastic interaction. We then present two mathematically robust scoring functions based on transfer entropy and statistical independence tests. These results support the previously held conjecture that transfer entropy can be used to infer effective connectivity in complex networks.\n    ",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00576",
        "title": "Strong Neutrosophic Graphs and Subgraph Topological Subspaces",
        "authors": [
            "W. B. Vasantha Kandasamy",
            "Ilanthenral K",
            "Florentin Smarandache"
        ],
        "abstract": "In this book authors for the first time introduce the notion of strong neutrosophic graphs. They are very different from the usual graphs and neutrosophic graphs. Using these new structures special subgraph topological spaces are defined. Further special lattice graph of subgraphs of these graphs are defined and described. Several interesting properties using subgraphs of a strong neutrosophic graph are obtained. Several open conjectures are proposed. These new class of strong neutrosophic graphs will certainly find applications in Neutrosophic Cognitive Maps (NCM), Neutrosophic Relational Maps (NRM) and Neutrosophic Relational Equations (NRE) with appropriate modifications.\n    ",
        "submission_date": "2016-10-30T00:00:00",
        "last_modified_date": "2016-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00685",
        "title": "A Framework for Searching for General Artificial Intelligence",
        "authors": [
            "Marek Rosa",
            "Jan Feyereisl",
            "GoodAI Collective"
        ],
        "abstract": "There is a significant lack of unified approaches to building generally intelligent machines. The majority of current artificial intelligence research operates within a very narrow field of focus, frequently without considering the importance of the 'big picture'. In this document, we seek to describe and unify principles that guide the basis of our development of general artificial intelligence. These principles revolve around the idea that intelligence is a tool for searching for general solutions to problems. We define intelligence as the ability to acquire skills that narrow this search, diversify it and help steer it to more promising areas. We also provide suggestions for studying, measuring, and testing the various skills and abilities that a human-level intelligent machine needs to acquire. The document aims to be both implementation agnostic, and to provide an analytic, systematic, and scalable way to generate hypotheses that we believe are needed to meet the necessary conditions in the search for general artificial intelligence. We believe that such a framework is an important stepping stone for bringing together definitions, highlighting open problems, connecting researchers willing to collaborate, and for unifying the arguably most significant search of this century.\n    ",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00873",
        "title": "Extracting Actionability from Machine Learning Models by Sub-optimal Deterministic Planning",
        "authors": [
            "Qiang Lyu",
            "Yixin Chen",
            "Zhaorong Li",
            "Zhicheng Cui",
            "Ling Chen",
            "Xing Zhang",
            "Haihua Shen"
        ],
        "abstract": "A main focus of machine learning research has been improving the generalization accuracy and efficiency of prediction models. Many models such as SVM, random forest, and deep neural nets have been proposed and achieved great success. However, what emerges as missing in many applications is actionability, i.e., the ability to turn prediction results into actions. For example, in applications such as customer relationship management, clinical prediction, and advertisement, the users need not only accurate prediction, but also actionable instructions which can transfer an input to a desirable goal (e.g., higher profit repays, lower morbidity rates, higher ads hit rates). Existing effort in deriving such actionable knowledge is few and limited to simple action models which restricted to only change one attribute for each action. The dilemma is that in many real applications those action models are often more complex and harder to extract an optimal solution.\n",
        "submission_date": "2016-11-03T00:00:00",
        "last_modified_date": "2016-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01080",
        "title": "Probabilistic Modeling of Progressive Filtering",
        "authors": [
            "Giuliano Armano"
        ],
        "abstract": "Progressive filtering is a simple way to perform hierarchical classification, inspired by the behavior that most humans put into practice while attempting to categorize an item according to an underlying taxonomy. Each node of the taxonomy being associated with a different category, one may visualize the categorization process by looking at the item going downwards through all the nodes that accept it as belonging to the corresponding category. This paper is aimed at modeling the progressive filtering technique from a probabilistic perspective, in a hierarchical text categorization setting. As a result, the designer of a system based on progressive filtering should be facilitated in the task of devising, training, and testing it.\n    ",
        "submission_date": "2016-11-03T00:00:00",
        "last_modified_date": "2016-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01802",
        "title": "Self-Wiring Question Answering Systems",
        "authors": [
            "Ricardo Usbeck",
            "Jonathan Huthmann",
            "Nico Duldhardt",
            "Axel-Cyrille Ngonga Ngomo"
        ],
        "abstract": "Question answering (QA) has been the subject of a resurgence over the past years. The said resurgence has led to a multitude of question answering (QA) systems being developed both by companies and research facilities. While a few components of QA systems get reused across implementations, most systems do not leverage the full potential of component reuse. Hence, the development of QA systems is currently still a tedious and time-consuming process. We address the challenge of accelerating the creation of novel or tailored QA systems by presenting a concept for a self-wiring approach to composing QA systems. Our approach will allow the reuse of existing, web-based QA systems or modules while developing new QA platforms. To this end, it will rely on QA modules being described using the Web Ontology Language. Based on these descriptions, our approach will be able to automatically compose QA systems using a data-driven approach automatically.\n    ",
        "submission_date": "2016-11-06T00:00:00",
        "last_modified_date": "2016-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01855",
        "title": "Neuro-Symbolic Program Synthesis",
        "authors": [
            "Emilio Parisotto",
            "Abdel-rahman Mohamed",
            "Rishabh Singh",
            "Lihong Li",
            "Dengyong Zhou",
            "Pushmeet Kohli"
        ],
        "abstract": "Recent years have seen the proposal of a number of neural architectures for the problem of Program Induction. Given a set of input-output examples, these architectures are able to learn mappings that generalize to new test inputs. While achieving impressive results, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). In this paper, we propose a novel technique, Neuro-Symbolic Program Synthesis, to overcome the above-mentioned problems. Once trained, our approach can automatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time. Our method is based on two novel neural modules. The first module, called the cross correlation I/O network, given a set of input-output examples, produces a continuous representation of the set of I/O examples. The second module, the Recursive-Reverse-Recursive Neural Network (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs. We demonstrate the effectiveness of our approach by applying it to the rich and complex domain of regular expression based string transformations. Experiments show that the R3NN model is not only able to construct programs from new input-output examples, but it is also able to construct new programs for tasks that it had never observed before during training.\n    ",
        "submission_date": "2016-11-06T00:00:00",
        "last_modified_date": "2016-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01929",
        "title": "Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning",
        "authors": [
            "Oron Anschel",
            "Nir Baram",
            "Nahum Shimkin"
        ],
        "abstract": "Instability and variability of Deep Reinforcement Learning (DRL) algorithms tend to adversely affect their performance. Averaged-DQN is a simple extension to the DQN algorithm, based on averaging previously learned Q-values estimates, which leads to a more stable training procedure and improved performance by reducing approximation error variance in the target values. To understand the effect of the algorithm, we examine the source of value function estimation errors and provide an analytical comparison within a simplified model. We further present experiments on the Arcade Learning Environment benchmark that demonstrate significantly improved stability and performance due to the proposed extension.\n    ",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2017-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02126",
        "title": "Dependence and Relevance: A probabilistic view",
        "authors": [
            "Dan Geiger",
            "David Heckerman"
        ],
        "abstract": "We examine three probabilistic concepts related to the sentence \"two variables have no bearing on each other\". We explore the relationships between these three concepts and establish their relevance to the process of constructing similarity networks---a tool for acquiring probabilistic knowledge from human experts. We also establish a precise relationship between connectedness in Bayesian networks and relevance in probability.\n    ",
        "submission_date": "2016-10-27T00:00:00",
        "last_modified_date": "2016-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02154",
        "title": "Bayesian Non-parametric model to Target Gamification Notifications Using Big Data",
        "authors": [
            "Meisam Hejazi Nia",
            "Brian Ratchford"
        ],
        "abstract": "I suggest an approach that helps the online marketers to target their Gamification elements to users by modifying the order of the list of tasks that they send to users. It is more realistic and flexible as it allows the model to learn more parameters when the online marketers collect more data. The targeting approach is scalable and quick, and it can be used over streaming data.\n    ",
        "submission_date": "2016-11-04T00:00:00",
        "last_modified_date": "2016-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02385",
        "title": "Combining observational and experimental data to find heterogeneous treatment effects",
        "authors": [
            "Alexander Peysakhovich",
            "Akos Lada"
        ],
        "abstract": "Every design choice will have different effects on different units. However traditional A/B tests are often underpowered to identify these heterogeneous effects. This is especially true when the set of unit-level attributes is high-dimensional and our priors are weak about which particular covariates are important. However, there are often observational data sets available that are orders of magnitude larger. We propose a method to combine these two data sources to estimate heterogeneous treatment effects. First, we use observational time series data to estimate a mapping from covariates to unit-level effects. These estimates are likely biased but under some conditions the bias preserves unit-level relative rank orderings. If these conditions hold, we only need sufficient experimental data to identify a monotonic, one-dimensional transformation from observationally predicted treatment effects to real treatment effects. This reduces power demands greatly and makes the detection of heterogeneous effects much easier. As an application, we show how our method can be used to improve Facebook page recommendations.\n    ",
        "submission_date": "2016-11-08T00:00:00",
        "last_modified_date": "2016-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02439",
        "title": "Proceedings of the First International Workshop on Argumentation in Logic Programming and Non-Monotonic Reasoning (Arg-LPNMR 2016)",
        "authors": [
            "Sarah Alice Gaggl",
            "Juan Carlos Nieves",
            "Hannes Strass"
        ],
        "abstract": "This volume contains the papers presented at Arg-LPNMR 2016: First International Workshop on Argumentation in Logic Programming and Nonmonotonic Reasoning held on July 8-10, 2016 in New York City, NY.\n    ",
        "submission_date": "2016-11-08T00:00:00",
        "last_modified_date": "2016-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02453",
        "title": "The Data Complexity of Description Logic Ontologies",
        "authors": [
            "Carsten Lutz",
            "Frank Wolter"
        ],
        "abstract": "We analyze the data complexity of ontology-mediated querying where the ontologies are formulated in a description logic (DL) of the ALC family and queries are conjunctive queries, positive existential queries, or acyclic conjunctive queries. Our approach is non-uniform in the sense that we aim to understand the complexity of each single ontology instead of for all ontologies formulated in a certain language. While doing so, we quantify over the queries and are interested, for example, in the question whether all queries can be evaluated in polynomial time w.r.t. a given ontology. Our results include a PTime/coNP-dichotomy for ontologies of depth one in the description logic ALCFI, the same dichotomy for ALC- and ALCI-ontologies of unrestricted depth, and the non-existence of such a dichotomy for ALCF-ontologies. For the latter DL, we additionally show that it is undecidable whether a given ontology admits PTime query evaluation. We also consider the connection between PTime query evaluation and rewritability into (monadic) Datalog.\n    ",
        "submission_date": "2016-11-08T00:00:00",
        "last_modified_date": "2017-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02512",
        "title": "Cognitive Discriminative Mappings for Rapid Learning",
        "authors": [
            "Wen-Chieh Fang",
            "Yi-ting Chiang"
        ],
        "abstract": "Humans can learn concepts or recognize items from just a handful of examples, while machines require many more samples to perform the same task. In this paper, we build a computational model to investigate the possibility of this kind of rapid learning. The proposed method aims to improve the learning task of input from sensory memory by leveraging the information retrieved from long-term memory. We present a simple and intuitive technique called cognitive discriminative mappings (CDM) to explore the cognitive problem. First, CDM separates and clusters the data instances retrieved from long-term memory into distinct classes with a discrimination method in working memory when a sensory input triggers the algorithm. CDM then maps each sensory data instance to be as close as possible to the median point of the data group with the same class. The experimental results demonstrate that the CDM approach is effective for learning the discriminative features of supervised classifications with few training sensory input instances.\n    ",
        "submission_date": "2016-11-08T00:00:00",
        "last_modified_date": "2016-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02646",
        "title": "On interestingness measures of formal concepts",
        "authors": [
            "Sergei O. Kuznetsov",
            "Tatiana Makhalova"
        ],
        "abstract": "Formal concepts and closed itemsets proved to be of big importance for knowledge discovery, both as a tool for concise representation of association rules and a tool for clustering and constructing domain taxonomies and ontologies. Exponential explosion makes it difficult to consider the whole concept lattice arising from data, one needs to select most useful and interesting concepts. In this paper interestingness measures of concepts are considered and compared with respect to various aspects, such as efficiency of computation and applicability to noisy data and performing ranking correlation.\n    ",
        "submission_date": "2016-11-08T00:00:00",
        "last_modified_date": "2017-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02755",
        "title": "Recursive Decomposition for Nonconvex Optimization",
        "authors": [
            "Abram L. Friesen",
            "Pedro Domingos"
        ],
        "abstract": "Continuous optimization is an important problem in many areas of AI, including vision, robotics, probabilistic inference, and machine learning. Unfortunately, most real-world optimization problems are nonconvex, causing standard convex techniques to find only local optima, even with extensions like random restarts and simulated annealing. We observe that, in many cases, the local modes of the objective function have combinatorial structure, and thus ideas from combinatorial optimization can be brought to bear. Based on this, we propose a problem-decomposition approach to nonconvex optimization. Similarly to DPLL-style SAT solvers and recursive conditioning in probabilistic inference, our algorithm, RDIS, recursively sets variables so as to simplify and decompose the objective function into approximately independent sub-functions, until the remaining functions are simple enough to be optimized by standard techniques like gradient descent. The variables to set are chosen by graph partitioning, ensuring decomposition whenever possible. We show analytically that RDIS can solve a broad class of nonconvex optimization problems exponentially faster than gradient descent with random restarts. Experimentally, RDIS outperforms standard techniques on problems like structure from motion and protein folding.\n    ",
        "submission_date": "2016-11-08T00:00:00",
        "last_modified_date": "2016-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02779",
        "title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning",
        "authors": [
            "Yan Duan",
            "John Schulman",
            "Xi Chen",
            "Peter L. Bartlett",
            "Ilya Sutskever",
            "Pieter Abbeel"
        ],
        "abstract": "Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a \"fast\" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL$^2$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose (\"slow\") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the \"fast\" RL algorithm on the current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL$^2$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL$^2$ on a vision-based navigation task and show that it scales up to high-dimensional problems.\n    ",
        "submission_date": "2016-11-09T00:00:00",
        "last_modified_date": "2016-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02885",
        "title": "Encoding monotonic multi-set preferences using CI-nets: preliminary report",
        "authors": [
            "Martin Diller",
            "Anthony Hunter"
        ],
        "abstract": "CP-nets and their variants constitute one of the main AI approaches for specifying and reasoning about preferences. CI-nets, in particular, are a CP-inspired formalism for representing ordinal preferences over sets of goods, which are typically required to be monotonic.\n",
        "submission_date": "2016-11-09T00:00:00",
        "last_modified_date": "2016-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03218",
        "title": "Learning to Play Guess Who? and Inventing a Grounded Language as a Consequence",
        "authors": [
            "Emilio Jorge",
            "Mikael K\u00e5geb\u00e4ck",
            "Fredrik D. Johansson",
            "Emil Gustavsson"
        ],
        "abstract": "Acquiring your first language is an incredible feat and not easily duplicated. Learning to communicate using nothing but a few pictureless books, a corpus, would likely be impossible even for humans. Nevertheless, this is the dominating approach in most natural language processing today. As an alternative, we propose the use of situated interactions between agents as a driving force for communication, and the framework of Deep Recurrent Q-Networks for evolving a shared language grounded in the provided environment. We task the agents with interactive image search in the form of the game Guess Who?. The images from the game provide a non trivial environment for the agents to discuss and a natural grounding for the concepts they decide to encode in their communication. Our experiments show that the agents learn not only to encode physical concepts in their words, i.e. grounding, but also that the agents learn to hold a multi-step dialogue remembering the state of the dialogue from step to step.\n    ",
        "submission_date": "2016-11-10T00:00:00",
        "last_modified_date": "2017-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03398",
        "title": "XCSP3: An Integrated Format for Benchmarking Combinatorial Constrained Problems",
        "authors": [
            "Frederic Boussemart",
            "Christophe Lecoutre",
            "Gilles Audemard",
            "C\u00e9dric Piette"
        ],
        "abstract": "We propose a major revision of the format XCSP 2.1, called XCSP3, to build integrated representations of combinatorial constrained problems. This new format is able to deal with mono/multi optimization, many types of variables, cost functions, reification, views, annotations, variable quantification, distributed, probabilistic and qualitative reasoning. The new format is made compact, highly readable, and rather easy to parse. Interestingly, it captures the structure of the problem models, through the possibilities of declaring arrays of variables, and identifying syntactic and semantic groups of constraints. The number of constraints is kept under control by introducing a limited set of basic constraint forms, and producing almost automatically some of their variations through lifting, restriction, sliding, logical combination and relaxation mechanisms. As a result, XCSP3 encompasses practically all constraints that can be found in major constraint solvers developed by the CP community. A website, which is developed conjointly with the format, contains many models and series of instances. The user can make sophisticated queries for selecting instances from very precise criteria. The objective of XCSP3 is to ease the effort required to test and compare different algorithms by providing a common test-bed of combinatorial constrained instances.\n    ",
        "submission_date": "2016-11-10T00:00:00",
        "last_modified_date": "2024-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03652",
        "title": "Show me the material evidence: Initial experiments on evaluating hypotheses from user-generated multimedia data",
        "authors": [
            "Bernardo Gon\u00e7alves"
        ],
        "abstract": "Subjective questions such as `does neymar dive', or `is clinton lying', or `is trump a fascist', are popular queries to web search engines, as can be seen by autocompletion suggestions on Google, Yahoo and Bing. In the era of cognitive computing, beyond search, they could be handled as hypotheses issued for evaluation. Our vision is to leverage on unstructured data and metadata of the rich user-generated multimedia that is often shared as material evidence in favor or against hypotheses in social media platforms. In this paper we present two preliminary experiments along those lines and discuss challenges for a cognitive computing system that collects material evidence from user-generated multimedia towards aggregating it into some form of collective decision on the hypothesis.\n    ",
        "submission_date": "2016-11-11T00:00:00",
        "last_modified_date": "2016-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03673",
        "title": "Learning to Navigate in Complex Environments",
        "authors": [
            "Piotr Mirowski",
            "Razvan Pascanu",
            "Fabio Viola",
            "Hubert Soyer",
            "Andrew J. Ballard",
            "Andrea Banino",
            "Misha Denil",
            "Ross Goroshin",
            "Laurent Sifre",
            "Koray Kavukcuoglu",
            "Dharshan Kumaran",
            "Raia Hadsell"
        ],
        "abstract": "Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour, its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.\n    ",
        "submission_date": "2016-11-11T00:00:00",
        "last_modified_date": "2017-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03799",
        "title": "Applying Chatbots to the Internet of Things: Opportunities and Architectural Elements",
        "authors": [
            "Rohan Kar",
            "Rishin Haldar"
        ],
        "abstract": "Internet of Things (IoT) is emerging as a significant technology in shaping the future by connecting physical devices or things with internet. It also presents various opportunities for intersection of other technological trends which can allow it to become even more intelligent and efficient. In this paper we focus our attention on the integration of Intelligent Conversational Software Agents or Chatbots with IoT. Literature surveys have looked into various applications, features, underlying technologies and known challenges of IoT. On the other hand, Chatbots are being adopted in greater numbers due to major strides in development of platforms and frameworks. The novelty of this paper lies in the specific integration of Chatbots in the IoT scenario. We analyzed the shortcomings of existing IoT systems and put forward ways to tackle them by incorporating chatbots. A general architecture is proposed for implementing such a system, as well as platforms and frameworks, both commercial and open source, which allow for implementation of such systems. Identification of the newer challenges and possible future directions with this new integration, have also been addressed.\n    ",
        "submission_date": "2016-11-11T00:00:00",
        "last_modified_date": "2016-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03907",
        "title": "Reinforcement Learning in Rich-Observation MDPs using Spectral Methods",
        "authors": [
            "Kamyar Azizzadenesheli",
            "Alessandro Lazaric",
            "Animashree Anandkumar"
        ],
        "abstract": "Reinforcement learning (RL) in Markov decision processes (MDPs) with large state spaces is a challenging problem. The performance of standard RL algorithms degrades drastically with the dimensionality of state space. However, in practice, these large MDPs typically incorporate a latent or hidden low-dimensional structure. In this paper, we study the setting of rich-observation Markov decision processes (ROMDP), where there are a small number of hidden states which possess an injective mapping to the observation states. In other words, every observation state is generated through a single hidden state, and this mapping is unknown a priori. We introduce a spectral decomposition method that consistently learns this mapping, and more importantly, achieves it with low regret. The estimated mapping is integrated into an optimistic RL algorithm (UCRL), which operates on the estimated hidden space. We derive finite-time regret bounds for our algorithm with a weak dependence on the dimensionality of the observed space. In fact, our algorithm asymptotically achieves the same average regret as the oracle UCRL algorithm, which has the knowledge of the mapping from hidden to observed spaces. Thus, we derive an efficient spectral RL algorithm for ROMDPs.\n    ",
        "submission_date": "2016-11-11T00:00:00",
        "last_modified_date": "2018-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03954",
        "title": "Multilingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment",
        "authors": [
            "Muhao Chen",
            "Yingtao Tian",
            "Mohan Yang",
            "Carlo Zaniolo"
        ],
        "abstract": "Many recent works have demonstrated the benefits of knowledge graph embeddings in completing monolingual knowledge graphs. Inasmuch as related knowledge bases are built in several different languages, achieving cross-lingual knowledge alignment will help people in constructing a coherent knowledge base, and assist machines in dealing with different expressions of entity relationships across diverse human languages. Unfortunately, achieving this highly desirable crosslingual alignment by human labor is very costly and errorprone. Thus, we propose MTransE, a translation-based model for multilingual knowledge graph embeddings, to provide a simple and automated solution. By encoding entities and relations of each language in a separated embedding space, MTransE provides transitions for each embedding vector to its cross-lingual counterparts in other spaces, while preserving the functionalities of monolingual embeddings. We deploy three different techniques to represent cross-lingual transitions, namely axis calibration, translation vectors, and linear transformations, and derive five variants for MTransE using different loss functions. Our models can be trained on partially aligned graphs, where just a small portion of triples are aligned with their cross-lingual counterparts. The experiments on cross-lingual entity matching and triple-wise alignment verification show promising results, with some variants consistently outperforming others on different tasks. We also explore how MTransE preserves the key properties of its monolingual counterpart TransE.\n    ",
        "submission_date": "2016-11-12T00:00:00",
        "last_modified_date": "2017-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03977",
        "title": "A Review on Algorithms for Constraint-based Causal Discovery",
        "authors": [
            "Kui Yu",
            "Jiuyong Li",
            "Lin Liu"
        ],
        "abstract": "Causal discovery studies the problem of mining causal relationships between variables from data, which is of primary interest in science. During the past decades, significant amount of progresses have been made toward this fundamental data mining paradigm. Recent years, as the availability of abundant large-sized and complex observational data, the constrain-based approaches have gradually attracted a lot of interest and have been widely applied to many diverse real-world problems due to the fast running speed and easy generalizing to the problem of causal insufficiency. In this paper, we aim to review the constraint-based causal discovery algorithms. Firstly, we discuss the learning paradigm of the constraint-based approaches. Secondly and primarily, the state-of-the-art constraint-based casual inference algorithms are surveyed with the detailed analysis. Thirdly, several related open-source software packages and benchmark data repositories are briefly summarized. As a conclusion, some open problems in constraint-based causal discovery are outlined for future research.\n    ",
        "submission_date": "2016-11-12T00:00:00",
        "last_modified_date": "2016-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04035",
        "title": "Entropic Causal Inference",
        "authors": [
            "Murat Kocaoglu",
            "Alexandros G. Dimakis",
            "Sriram Vishwanath",
            "Babak Hassibi"
        ],
        "abstract": "We consider the problem of identifying the causal direction between two discrete random variables using observational data. Unlike previous work, we keep the most general functional model but make an assumption on the unobserved exogenous variable: Inspired by Occam's razor, we assume that the exogenous variable is simple in the true causal direction. We quantify simplicity using R\u00e9nyi entropy. Our main result is that, under natural assumptions, if the exogenous variable has low $H_0$ entropy (cardinality) in the true direction, it must have high $H_0$ entropy in the wrong direction. We establish several algorithmic hardness results about estimating the minimum entropy exogenous variable. We show that the problem of finding the exogenous variable with minimum entropy is equivalent to the problem of finding minimum joint entropy given $n$ marginal distributions, also known as minimum entropy coupling problem. We propose an efficient greedy algorithm for the minimum entropy coupling problem, that for $n=2$ provably finds a local optimum. This gives a greedy algorithm for finding the exogenous variable with minimum $H_1$ (Shannon Entropy). Our greedy entropy-based causal inference algorithm has similar performance to the state of the art additive noise models in real datasets. One advantage of our approach is that we make no use of the values of random variables but only their distributions. Our method can therefore be used for causal inference for both ordinal and also categorical data, unlike additive noise models.\n    ",
        "submission_date": "2016-11-12T00:00:00",
        "last_modified_date": "2016-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04146",
        "title": "Commonsense Knowledge Enhanced Embeddings for Solving Pronoun Disambiguation Problems in Winograd Schema Challenge",
        "authors": [
            "Quan Liu",
            "Hui Jiang",
            "Zhen-Hua Ling",
            "Xiaodan Zhu",
            "Si Wei",
            "Yu Hu"
        ],
        "abstract": "In this paper, we propose commonsense knowledge enhanced embeddings (KEE) for solving the Pronoun Disambiguation Problems (PDP). The PDP task we investigate in this paper is a complex coreference resolution task which requires the utilization of commonsense knowledge. This task is a standard first round test set in the 2016 Winograd Schema Challenge. In this task, traditional linguistic features that are useful for coreference resolution, e.g. context and gender information, are no longer effective anymore. Therefore, the KEE models are proposed to provide a general framework to make use of commonsense knowledge for solving the PDP problems. Since the PDP task doesn't have training data, the KEE models would be used during the unsupervised feature extraction process. To evaluate the effectiveness of the KEE models, we propose to incorporate various commonsense knowledge bases, including ConceptNet, WordNet, and CauseCom, into the KEE training process. We achieved the best performance by applying the proposed methods to the 2016 Winograd Schema Challenge. In addition, experiments conducted on the standard PDP task indicate that, the proposed KEE models could solve the PDP problems by achieving 66.7% accuracy, which is a new state-of-the-art performance.\n    ",
        "submission_date": "2016-11-13T00:00:00",
        "last_modified_date": "2016-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04363",
        "title": "Weakly Learning to Match Experts in Online Community",
        "authors": [
            "Yujie Qian",
            "Jie Tang",
            "Kan Wu"
        ],
        "abstract": "In online question-and-answer (QA) websites like Quora, one central issue is to find (invite) users who are able to provide answers to a given question and at the same time would be unlikely to say \"no\" to the invitation. The challenge is how to trade off the matching degree between users' expertise and the question topic, and the likelihood of positive response from the invited users. In this paper, we formally formulate the problem and develop a weakly supervised factor graph (WeakFG) model to address the problem. The model explicitly captures expertise matching degree between questions and users. To model the likelihood that an invited user is willing to answer a specific question, we incorporate a set of correlations based on social identity theory into the WeakFG model. We use two different genres of datasets: QA-Expert and Paper-Reviewer, to validate the proposed model. Our experimental results show that the proposed model can significantly outperform (+1.5-10.7% by MAP) the state-of-the-art algorithms for matching users (experts) with community questions. We have also developed an online system to further demonstrate the advantages of the proposed method.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2018-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04369",
        "title": "Feature Engineering and Ensemble Modeling for Paper Acceptance Rank Prediction",
        "authors": [
            "Yujie Qian",
            "Yinpeng Dong",
            "Ye Ma",
            "Hailong Jin",
            "Juanzi Li"
        ],
        "abstract": "Measuring research impact and ranking academic achievement are important and challenging problems. Having an objective picture of research institution is particularly valuable for students, parents and funding agencies, and also attracts attention from government and industry. KDD Cup 2016 proposes the paper acceptance rank prediction task, in which the participants are asked to rank the importance of institutions based on predicting how many of their papers will be accepted at the 8 top conferences in computer science. In our work, we adopt a three-step feature engineering method, including basic features definition, finding similar conferences to enhance the feature set, and dimension reduction using PCA. We propose three ranking models and the ensemble methods for combining such models. Our experiment verifies the effectiveness of our approach. In KDD Cup 2016, we achieved the overall rank of the 2nd place.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2016-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04636",
        "title": "When Saliency Meets Sentiment: Understanding How Image Content Invokes Emotion and Sentiment",
        "authors": [
            "Honglin Zheng",
            "Tianlang Chen",
            "Jiebo Luo"
        ],
        "abstract": "Sentiment analysis is crucial for extracting social signals from social media content. Due to the prevalence of images in social media, image sentiment analysis is receiving increasing attention in recent years. However, most existing systems are black-boxes that do not provide insight on how image content invokes sentiment and emotion in the viewers. Psychological studies have confirmed that salient objects in an image often invoke emotions. In this work, we investigate more fine-grained and more comprehensive interaction between visual saliency and visual sentiment. In particular, we partition images in several primary scene-type dimensions, including: open-closed, natural-manmade, indoor-outdoor, and face-noface. Using state of the art saliency detection algorithm and sentiment classification algorithm, we examine how the sentiment of the salient region(s) in an image relates to the overall sentiment of the image. The experiments on a representative image emotion dataset have shown interesting correlation between saliency and sentiment in different scene types and in turn shed light on the mechanism of visual sentiment evocation.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2016-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04642",
        "title": "Link Prediction using Embedded Knowledge Graphs",
        "authors": [
            "Yelong Shen",
            "Po-Sen Huang",
            "Ming-Wei Chang",
            "Jianfeng Gao"
        ],
        "abstract": "Since large knowledge bases are typically incomplete, missing facts need to be inferred from observed facts in a task called knowledge base completion. The most successful approaches to this task have typically explored explicit paths through sequences of triples. These approaches have usually resorted to human-designed sampling procedures, since large knowledge graphs produce prohibitively large numbers of possible paths, most of which are uninformative. As an alternative approach, we propose performing a single, short sequence of interactive lookup operations on an embedded knowledge graph which has been trained through end-to-end backpropagation to be an optimized and compressed version of the initial knowledge base. Our proposed model, called Embedded Knowledge Graph Network (EKGN), achieves new state-of-the-art results on popular knowledge base completion benchmarks.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2018-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04660",
        "title": "Causal Inference in Observational Data",
        "authors": [
            "Pranjul Yadav",
            "Lisiane Prunelli",
            "Alexander Hoff",
            "Michael Steinbach",
            "Bonnie Westra",
            "Vipin Kumar",
            "Gyorgy Simon"
        ],
        "abstract": "Our aging population increasingly suffers from multiple chronic diseases simultaneously, necessitating the comprehensive treatment of these conditions. Finding the optimal set of drugs for a combinatorial set of diseases is a combinatorial pattern exploration problem. Association rule mining is a popular tool for such problems, but the requirement of health care for finding causal, rather than associative, patterns renders association rule mining unsuitable. To address this issue, we propose a novel framework based on the Rubin-Neyman causal model for extracting causal rules from observational data, correcting for a number of common biases. Specifically, given a set of interventions and a set of items that define subpopulations (e.g., diseases), we wish to find all subpopulations in which effective intervention combinations exist and in each such subpopulation, we wish to find all intervention combinations such that dropping any intervention from this combination will reduce the efficacy of the treatment. A key aspect of our framework is the concept of closed intervention sets which extend the concept of quantifying the effect of a single intervention to a set of concurrent interventions. We also evaluated our causal rule mining framework on the Electronic Health Records (EHR) data of a large cohort of patients from Mayo Clinic and showed that the patterns we extracted are sufficiently rich to explain the controversial findings in the medical literature regarding the effect of a class of cholesterol drugs on Type-II Diabetes Mellitus (T2DM).\n    ",
        "submission_date": "2016-11-15T00:00:00",
        "last_modified_date": "2016-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04717",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning",
        "authors": [
            "Haoran Tang",
            "Rein Houthooft",
            "Davis Foote",
            "Adam Stooke",
            "Xi Chen",
            "Yan Duan",
            "John Schulman",
            "Filip De Turck",
            "Pieter Abbeel"
        ],
        "abstract": "Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.\n    ",
        "submission_date": "2016-11-15T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04845",
        "title": "An Evaluation of Information Sharing Parking Guidance Policies Using a Bayesian Approach",
        "authors": [
            "Xinyi Wu",
            "Kartik Balkumar",
            "Qi Luo",
            "Robert Hampshire",
            "Romesh Saigal"
        ],
        "abstract": "Real-time parking occupancy information is critical for a parking management system to facilitate drivers to park more efficiently. Recent advances in connected and automated vehicle technologies enable sensor-equipped cars (probe cars) to detect and broadcast available parking spaces when driving through parking lots. In this paper, we evaluate the impact of market penetration of probe cars on the system performance, and investigate different parking guidance policies to improve the data acquisition process. We adopt a simulation-based approach to impose four policies on an off- street parking lot influencing the behavior of probe cars to park in assigned parking spaces. This in turn effects the scanning route and the parking space occupancy estimations. The last policy we propose is a near-optimal guidance strategy that maximizes the information gain of posteriors. The results suggest that an efficient information gathering policy can compensate for low penetration of connected and automated vehicles. We also highlight the policy trade-off that occur while attempting to maximize information gain through explorations and improve assignment accuracy through exploitations. Our results can assist urban policy makers in designing and managing smart parking systems.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2016-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04969",
        "title": "An integrated Graphical User Interface for Debugging Answer Set Programs",
        "authors": [
            "Philip Gasteiger",
            "Carmine Dodaro",
            "Benjamin Musitsch",
            "Kristian Reale",
            "Francesco Ricca",
            "Konstantin Schekotihin"
        ],
        "abstract": "Answer Set Programming (ASP) is an expressive knowledge representation and reasoning framework. Due to its rather simple syntax paired with high-performance solvers, ASP is interesting for industrial applications. However, to err is human and thus debugging is an important activity during the development process. Therefore, tools for debugging non-ground answer set programs are needed. In this paper, we present a new graphical debugging interface for non-ground answer set programs. The tool is based on the recently-introduced DWASP approach for debugging and it simplifies the interaction with the debugger. Furthermore, the debugging interface is integrated in ASPIDE, a rich IDE for answer set programs. With our extension ASPIDE turns into a full-fledged IDE by offering debugging support.\n    ",
        "submission_date": "2016-11-15T00:00:00",
        "last_modified_date": "2016-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05170",
        "title": "The Effects of Relative Importance of User Constraints in Cloud of Things Resource Discovery: A Case Study",
        "authors": [
            "Luiz H. Nunes",
            "Julio C. Estrella",
            "Alexandre C. B. Delbem",
            "Charith Perera",
            "Stephan Reiff-Marganiec"
        ],
        "abstract": "Over the last few years, the number of smart objects connected to the Internet has grown exponentially in comparison to the number of services and applications. The integration between Cloud Computing and Internet of Things, named as Cloud of Things, plays a key role in managing the connected things, their data and services. One of the main challenges in Cloud of Things is the resource discovery of the smart objects and their reuse in different contexts. Most of the existent work uses some kind of multi-criteria decision analysis algorithm to perform the resource discovery, but do not evaluate the impact that the user constraints has in the final solution. In this paper, we analyse the behaviour of the SAW, TOPSIS and VIKOR multi-objective decision analyses algorithms and the impact of user constraints on them. We evaluated the quality of the proposed solutions using the Pareto-optimality concept.\n    ",
        "submission_date": "2016-11-16T00:00:00",
        "last_modified_date": "2016-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05190",
        "title": "Driving CDCL Search",
        "authors": [
            "Carmine Dodaro",
            "Philip Gasteiger",
            "Nicola Leone",
            "Benjamin Musitsch",
            "Francesco Ricca",
            "Konstantin Schekotihin"
        ],
        "abstract": "The CDCL algorithm is the leading solution adopted by state-of-the-art solvers for SAT, SMT, ASP, and others. Experiments show that the performance of CDCL solvers can be significantly boosted by embedding domain-specific heuristics, especially on large real-world problems. However, a proper integration of such criteria in off-the-shelf CDCL implementations is not obvious. In this paper, we distill the key ingredients that drive the search of CDCL solvers, and propose a general framework for designing and implementing new heuristics. We implemented our strategy in an ASP solver, and we experimented on two industrial domains. On hard problem instances, state-of-the-art implementations fail to find any solution in acceptable time, whereas our implementation is very successful and finds all solutions.\n    ",
        "submission_date": "2016-11-16T00:00:00",
        "last_modified_date": "2016-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05379",
        "title": "PCT and Beyond: Towards a Computational Framework for `Intelligent' Communicative Systems",
        "authors": [
            "Prof. Roger K. Moore"
        ],
        "abstract": "Recent years have witnessed increasing interest in the potential benefits of `intelligent' autonomous machines such as robots. Honda's Asimo humanoid robot, iRobot's Roomba robot vacuum cleaner and Google's driverless cars have fired the imagination of the general public, and social media buzz with speculation about a utopian world of helpful robot assistants or the coming robot apocalypse! However, there is a long way to go before autonomous systems reach the level of capabilities required for even the simplest of tasks involving human-robot interaction - especially if it involves communicative behaviour such as speech and language. Of course the field of Artificial Intelligence (AI) has made great strides in these areas, and has moved on from abstract high-level rule-based paradigms to embodied architectures whose operations are grounded in real physical environments. What is still missing, however, is an overarching theory of intelligent communicative behaviour that informs system-level design decisions in order to provide a more coherent approach to system integration. This chapter introduces the beginnings of such a framework inspired by the principles of Perceptual Control Theory (PCT). In particular, it is observed that PCT has hitherto tended to view perceptual processes as a relatively straightforward series of transformations from sensation to perception, and has overlooked the potential of powerful generative model-based solutions that have emerged in practical fields such as visual or auditory scene analysis. Starting from first principles, a sequence of arguments is presented which not only shows how these ideas might be integrated into PCT, but which also extend PCT towards a remarkably symmetric architecture for a needs-driven communicative agent. It is concluded that, if behaviour is the control of perception, then perception is the simulation of behaviour.\n    ",
        "submission_date": "2016-11-16T00:00:00",
        "last_modified_date": "2016-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05425",
        "title": "ProjE: Embedding Projection for Knowledge Graph Completion",
        "authors": [
            "Baoxu Shi",
            "Tim Weninger"
        ],
        "abstract": "With the large volume of new information created every day, determining the validity of information in a knowledge graph and filling in its missing parts are crucial tasks for many researchers and practitioners. To address this challenge, a number of knowledge graph completion methods have been developed using low-dimensional graph embeddings. Although researchers continue to improve these models using an increasingly complex feature space, we show that simple changes in the architecture of the underlying model can outperform state-of-the-art models without the need for complex feature engineering. In this work, we present a shared variable neural network model called ProjE that fills-in missing information in a knowledge graph by learning joint embeddings of the knowledge graph's entities and edges, and through subtle, but important, changes to the standard loss function. In doing so, ProjE has a parameter size that is smaller than 11 out of 15 existing methods while performing $37\\%$ better than the current-best method on standard datasets. We also show, via a new fact checking task, that ProjE is capable of accurately determining the veracity of many declarative statements.\n    ",
        "submission_date": "2016-11-16T00:00:00",
        "last_modified_date": "2016-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05497",
        "title": "Explicablility as Minimizing Distance from Expected Behavior",
        "authors": [
            "Anagha Kulkarni",
            "Yantian Zha",
            "Tathagata Chakraborti",
            "Satya Gautam Vadlamudi",
            "Yu Zhang",
            "Subbarao Kambhampati"
        ],
        "abstract": "In order to have effective human-AI collaboration, it is necessary to address how the AI agent's behavior is being perceived by the humans-in-the-loop. When the agent's task plans are generated without such considerations, they may often demonstrate inexplicable behavior from the human's point of view. This problem may arise due to the human's partial or inaccurate understanding of the agent's planning model. This may have serious implications from increased cognitive load to more serious concerns of safety around a physical agent. In this paper, we address this issue by modeling plan explicability as a function of the distance between a plan that agent makes and the plan that human expects it to make. We learn a regression model for mapping the plan distances to explicability scores of plans and develop an anytime search algorithm that can use this model as a heuristic to come up with progressively explicable plans. We evaluate the effectiveness of our approach in a simulated autonomous car domain and a physical robot domain.\n    ",
        "submission_date": "2016-11-16T00:00:00",
        "last_modified_date": "2019-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05735",
        "title": "Optimal Dynamic Coverage Infrastructure for Large-Scale Fleets of Reconnaissance UAVs",
        "authors": [
            "Yaniv Altshuler",
            "Alex Pentland",
            "Shlomo Bekhor",
            "Yoram Shiftan",
            "Alfred Bruckstein"
        ],
        "abstract": "Current state of the art in the field of UAV activation relies solely on human operators for the design and adaptation of the drones' flying routes. Furthermore, this is being done today on an individual level (one vehicle per operators), with some exceptions of a handful of new systems, that are comprised of a small number of self-organizing swarms, manually guided by a human operator.\n",
        "submission_date": "2016-11-17T00:00:00",
        "last_modified_date": "2016-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05740",
        "title": "Fast Non-Parametric Tests of Relative Dependency and Similarity",
        "authors": [
            "Wacha Bounliphone",
            "Eugene Belilovsky",
            "Arthur Tenenhaus",
            "Ioannis Antonoglou",
            "Arthur Gretton",
            "Matthew B. Blashcko"
        ],
        "abstract": "We introduce two novel non-parametric statistical hypothesis tests. The first test, called the relative test of dependency, enables us to determine whether one source variable is significantly more dependent on a first target variable or a second. Dependence is measured via the Hilbert-Schmidt Independence Criterion (HSIC). The second test, called the relative test of similarity, is use to determine which of the two samples from arbitrary distributions is significantly closer to a reference sample of interest and the relative measure of similarity is based on the Maximum Mean Discrepancy (MMD). To construct these tests, we have used as our test statistics the difference of HSIC statistics and of MMD statistics, respectively. The resulting tests are consistent and unbiased, and have favorable convergence properties. The effectiveness of the relative dependency test is demonstrated on several real-world problems: we identify languages groups from a multilingual parallel corpus, and we show that tumor location is more dependent on gene expression than chromosome imbalance. We also demonstrate the performance of the relative test of similarity over a broad selection of model comparisons problems in deep generative models.\n    ",
        "submission_date": "2016-11-17T00:00:00",
        "last_modified_date": "2016-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05950",
        "title": "Analysis of a Design Pattern for Teaching with Features and Labels",
        "authors": [
            "Christopher Meek",
            "Patrice Simard",
            "Xiaojin Zhu"
        ],
        "abstract": "We study the task of teaching a machine to classify objects using features and labels. We introduce the Error-Driven-Featuring design pattern for teaching using features and labels in which a teacher prefers to introduce features only if they are needed. We analyze the potential risks and benefits of this teaching pattern through the use of teaching protocols, illustrative examples, and by providing bounds on the effort required for an optimal machine teacher using a linear learning algorithm, the most commonly used type of learners in interactive machine learning systems. Our analysis provides a deeper understanding of potential trade-offs of using different learning algorithms and between the effort required for featuring (creating new features) and labeling (providing labels for objects).\n    ",
        "submission_date": "2016-11-18T00:00:00",
        "last_modified_date": "2016-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05973",
        "title": "NCBO Ontology Recommender 2.0: An Enhanced Approach for Biomedical Ontology Recommendation",
        "authors": [
            "Marcos Martinez-Romero",
            "Clement Jonquet",
            "Martin J. O'Connor",
            "John Graybeal",
            "Alejandro Pazos",
            "Mark A. Musen"
        ],
        "abstract": "Biomedical researchers use ontologies to annotate their data with ontology terms, enabling better data integration and interoperability. However, the number, variety and complexity of current biomedical ontologies make it cumbersome for researchers to determine which ones to reuse for their specific needs. To overcome this problem, in 2010 the National Center for Biomedical Ontology (NCBO) released the Ontology Recommender, which is a service that receives a biomedical text corpus or a list of keywords and suggests ontologies appropriate for referencing the indicated terms. We developed a new version of the NCBO Ontology Recommender. Called Ontology Recommender 2.0, it uses a new recommendation approach that evaluates the relevance of an ontology to biomedical text data according to four criteria: (1) the extent to which the ontology covers the input data; (2) the acceptance of the ontology in the biomedical community; (3) the level of detail of the ontology classes that cover the input data; and (4) the specialization of the ontology to the domain of the input data. Our evaluation shows that the enhanced recommender provides higher quality suggestions than the original approach, providing better coverage of the input data, more detailed information about their concepts, increased specialization for the domain of the input data, and greater acceptance and use in the community. In addition, it provides users with more explanatory information, along with suggestions of not only individual ontologies but also groups of ontologies. It also can be customized to fit the needs of different scenarios. Ontology Recommender 2.0 combines the strengths of its predecessor with a range of adjustments and new features that improve its reliability and usefulness. Ontology Recommender 2.0 recommends over 500 biomedical ontologies from the NCBO BioPortal platform, where it is openly available.\n    ",
        "submission_date": "2016-11-18T00:00:00",
        "last_modified_date": "2017-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06108",
        "title": "Navigational Rule Derivation: An algorithm to determine the effect of traffic signs on road networks",
        "authors": [
            "Daniil Galaktionov",
            "Miguel R. Luaces",
            "\u00c1ngeles S. Places"
        ],
        "abstract": "In this paper we present an algorithm to build a road network map enriched with traffic rules such as one-way streets and forbidden turns, based on the interpretation of already detected and classified traffic signs. Such algorithm helps to automatize the elaboration of maps for commercial navigation systems. Our solution is based on simulating navigation along the road network, determining at each point of interest the visibility of the signs and their effect on the roads. We test our approach in a small urban network and discuss various ways to generalize it to support more complex environments.\n    ",
        "submission_date": "2016-11-17T00:00:00",
        "last_modified_date": "2016-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06134",
        "title": "Team-maxmin equilibrium: efficiency bounds and algorithms",
        "authors": [
            "Nicola Basilico",
            "Andrea Celli",
            "Giuseppe De Nittis",
            "Nicola Gatti"
        ],
        "abstract": "The Team-maxmin equilibrium prescribes the optimal strategies for a team of rational players sharing the same goal and without the capability of correlating their strategies in strategic games against an adversary. This solution concept can capture situations in which an agent controls multiple resources-corresponding to the team members-that cannot communicate. It is known that such equilibrium always exists and it is unique (unless degeneracy) and these properties make it a credible solution concept to be used in real-world applications, especially in security scenarios. Nevertheless, to the best of our knowledge, the Team-maxmin equilibrium is almost completely unexplored in the literature. In this paper, we investigate bounds of (in)efficiency of the Team-maxmin equilibrium w.r.t. the Nash equilibria and w.r.t. the Maxmin equilibrium when the team members can play correlated strategies. Furthermore, we study a number of algorithms to find and/or approximate an equilibrium, discussing their theoretical guarantees and evaluating their performance by using a standard testbed of game instances.\n    ",
        "submission_date": "2016-11-18T00:00:00",
        "last_modified_date": "2016-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06174",
        "title": "Stratified Knowledge Bases as Interpretable Probabilistic Models (Extended Abstract)",
        "authors": [
            "Ondrej Kuzelka",
            "Jesse Davis",
            "Steven Schockaert"
        ],
        "abstract": "In this paper, we advocate the use of stratified logical theories for representing probabilistic models. We argue that such encodings can be more interpretable than those obtained in existing frameworks such as Markov logic networks. Among others, this allows for the use of domain experts to improve learned models by directly removing, adding, or modifying logical formulas.\n    ",
        "submission_date": "2016-11-18T00:00:00",
        "last_modified_date": "2016-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06468",
        "title": "Generating machine-executable plans from end-user's natural-language instructions",
        "authors": [
            "Rui Liu",
            "Xiaoli Zhang"
        ],
        "abstract": "It is critical for advanced manufacturing machines to autonomously execute a task by following an end-user's natural language (NL) instructions. However, NL instructions are usually ambiguous and abstract so that the machines may misunderstand and incorrectly execute the task. To address this NL-based human-machine communication problem and enable the machines to appropriately execute tasks by following the end-user's NL instructions, we developed a Machine-Executable-Plan-Generation (exePlan) method. The exePlan method conducts task-centered semantic analysis to extract task-related information from ambiguous NL instructions. In addition, the method specifies machine execution parameters to generate a machine-executable plan by interpreting abstract NL instructions. To evaluate the exePlan method, an industrial robot Baxter was instructed by NL to perform three types of industrial tasks {'drill a hole', 'clean a spot', 'install a screw'}. The experiment results proved that the exePlan method was effective in generating machine-executable plans from the end-user's NL instructions. Such a method has the promise to endow a machine with the ability of NL-instructed task execution.\n    ",
        "submission_date": "2016-11-20T00:00:00",
        "last_modified_date": "2016-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06928",
        "title": "Memory Lens: How Much Memory Does an Agent Use?",
        "authors": [
            "Christoph Dann",
            "Katja Hofmann",
            "Sebastian Nowozin"
        ],
        "abstract": "We propose a new method to study the internal memory used by reinforcement learning policies. We estimate the amount of relevant past information by estimating mutual information between behavior histories and the current action of an agent. We perform this estimation in the passive setting, that is, we do not intervene but merely observe the natural behavior of the agent. Moreover, we provide a theoretical justification for our approach by showing that it yields an implementation-independent lower bound on the minimal memory capacity of any agent that implement the observed policy. We demonstrate our approach by estimating the use of memory of DQN policies on concatenated Atari frames, demonstrating sharply different use of memory across 49 games. The study of memory as information that flows from the past to the current action opens avenues to understand and improve successful reinforcement learning algorithms.\n    ",
        "submission_date": "2016-11-21T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07078",
        "title": "A Deep Learning Approach for Joint Video Frame and Reward Prediction in Atari Games",
        "authors": [
            "Felix Leibfried",
            "Nate Kushman",
            "Katja Hofmann"
        ],
        "abstract": "Reinforcement learning is concerned with identifying reward-maximizing behaviour policies in environments that are initially unknown. State-of-the-art reinforcement learning approaches, such as deep Q-networks, are model-free and learn to act effectively across a wide range of environments such as Atari games, but require huge amounts of data. Model-based techniques are more data-efficient, but need to acquire explicit knowledge about the environment.\n",
        "submission_date": "2016-11-21T00:00:00",
        "last_modified_date": "2017-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07478",
        "title": "An unexpected unity among methods for interpreting model predictions",
        "authors": [
            "Scott Lundberg",
            "Su-In Lee"
        ],
        "abstract": "Understanding why a model made a certain prediction is crucial in many data science fields. Interpretable predictions engender appropriate trust and provide insight into how the model may be improved. However, with large modern datasets the best accuracy is often achieved by complex models even experts struggle to interpret, which creates a tension between accuracy and interpretability. Recently, several methods have been proposed for interpreting predictions from complex models by estimating the importance of input features. Here, we present how a model-agnostic additive representation of the importance of input features unifies current methods. This representation is optimal, in the sense that it is the only set of additive values that satisfies important properties. We show how we can leverage these properties to create novel visual explanations of model predictions. The thread of unity that this representation weaves through the literature indicates that there are common principles to be learned about the interpretation of model predictions that apply in many scenarios.\n    ",
        "submission_date": "2016-11-22T00:00:00",
        "last_modified_date": "2016-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07567",
        "title": "Feature Importance Measure for Non-linear Learning Algorithms",
        "authors": [
            "Marina M.-C. Vidovic",
            "Nico G\u00f6rnitz",
            "Klaus-Robert M\u00fcller",
            "Marius Kloft"
        ],
        "abstract": "Complex problems may require sophisticated, non-linear learning methods such as kernel machines or deep neural networks to achieve state of the art prediction accuracies. However, high prediction accuracies are not the only objective to consider when solving problems using machine learning. Instead, particular scientific applications require some explanation of the learned prediction function. Unfortunately, most methods do not come with out of the box straight forward interpretation. Even linear prediction functions are not straight forward to explain if features exhibit complex correlation structure.\n",
        "submission_date": "2016-11-22T00:00:00",
        "last_modified_date": "2016-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08037",
        "title": "A Spatio-Temporal Representation for the Orienteering Problem with Time-Varying Profits",
        "authors": [
            "Zhibei Ma",
            "Kai Yin",
            "Lantao Liu",
            "Gaurav S. Sukhatme"
        ],
        "abstract": "We consider an orienteering problem (OP) where an agent needs to visit a series (possibly a subset) of depots, from which the maximal accumulated profits are desired within given limited time budget. Different from most existing works where the profits are assumed to be static, in this work we investigate a variant that has arbitrary time-dependent profits. Specifically, the profits to be collected change over time and they follow different (e.g., independent) time-varying functions. The problem is of inherent nonlinearity and difficult to solve by existing methods. To tackle the challenge, we present a simple and effective framework that incorporates time-variations into the fundamental planning process. Specifically, we propose a deterministic spatio-temporal representation where both spatial description and temporal logic are unified into one routing topology. By employing existing basic sorting and searching algorithms, the routing solutions can be computed in an extremely efficient way. The proposed method is easy to implement and extensive numerical results show that our approach is time efficient and generates near-optimal solutions.\n    ",
        "submission_date": "2016-11-24T00:00:00",
        "last_modified_date": "2017-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08103",
        "title": "Double-quantitative $\u03b3^{\\ast}-$fuzzy coverings approximation operators",
        "authors": [
            "Guangming Lang"
        ],
        "abstract": "In digital-based information boom, the fuzzy covering rough set model is an important mathematical tool for artificial intelligence, and how to build the bridge between the fuzzy covering rough set theory and Pawlak's model is becoming a hot research topic. In this paper, we first present the $\\gamma-$fuzzy covering based probabilistic and grade approximation operators and double-quantitative approximation operators. We also study the relationships among the three types of $\\gamma-$fuzzy covering based approximation operators. Second, we propose the $\\gamma^{\\ast}-$fuzzy coverings based multi-granulation probabilistic and grade lower and upper approximation operators and multi-granulation double-quantitative lower and upper approximation operators. We also investigate the relationships among these types of $\\gamma-$fuzzy coverings based approximation operators. Finally, we employ several examples to illustrate how to construct the lower and upper approximations of fuzzy sets with the absolute and relative quantitative information.\n    ",
        "submission_date": "2016-11-24T00:00:00",
        "last_modified_date": "2016-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08108",
        "title": "Dynamic Key-Value Memory Networks for Knowledge Tracing",
        "authors": [
            "Jiani Zhang",
            "Xingjian Shi",
            "Irwin King",
            "Dit-Yan Yeung"
        ],
        "abstract": "Knowledge Tracing (KT) is a task of tracing evolving knowledge state of students with respect to one or more concepts as they engage in a sequence of learning activities. One important purpose of KT is to personalize the practice sequence to help students learn knowledge concepts efficiently. However, existing methods such as Bayesian Knowledge Tracing and Deep Knowledge Tracing either model knowledge state for each predefined concept separately or fail to pinpoint exactly which concepts a student is good at or unfamiliar with. To solve these problems, this work introduces a new model called Dynamic Key-Value Memory Networks (DKVMN) that can exploit the relationships between underlying concepts and directly output a student's mastery level of each concept. Unlike standard memory-augmented neural networks that facilitate a single memory matrix or two static memory matrices, our model has one static matrix called key, which stores the knowledge concepts and the other dynamic matrix called value, which stores and updates the mastery levels of corresponding concepts. Experiments show that our model consistently outperforms the state-of-the-art model in a range of KT datasets. Moreover, the DKVMN model can automatically discover underlying concepts of exercises typically performed by human annotations and depict the changing knowledge state of a student.\n    ",
        "submission_date": "2016-11-24T00:00:00",
        "last_modified_date": "2017-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08219",
        "title": "The Off-Switch Game",
        "authors": [
            "Dylan Hadfield-Menell",
            "Anca Dragan",
            "Pieter Abbeel",
            "Stuart Russell"
        ],
        "abstract": "It is clear that one of the primary tools we can use to mitigate the potential risk from a misbehaving AI system is the ability to turn the system off. As the capabilities of AI systems improve, it is important to ensure that such systems do not adopt subgoals that prevent a human from switching them off. This is a challenge because many formulations of rational agents create strong incentives for self-preservation. This is not caused by a built-in instinct, but because a rational agent will maximize expected utility and cannot achieve whatever objective it has been given if it is dead. Our goal is to study the incentives an agent has to allow itself to be switched off. We analyze a simple game between a human H and a robot R, where H can press R's off switch but R can disable the off switch. A traditional agent takes its reward function for granted: we show that such agents have an incentive to disable the off switch, except in the special case where H is perfectly rational. Our key insight is that for R to want to preserve its off switch, it needs to be uncertain about the utility associated with the outcome, and to treat H's actions as important observations about that utility. (R also has no incentive to switch itself off in this setting.) We conclude that giving machines an appropriate level of uncertainty about their objectives leads to safer designs, and we argue that this setting is a useful generalization of the classical AI paradigm of rational agents.\n    ",
        "submission_date": "2016-11-24T00:00:00",
        "last_modified_date": "2017-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08374",
        "title": "Decision Support Systems in Fisheries and Aquaculture: A systematic review",
        "authors": [
            "Bj\u00f8rn Magnus Mathisen",
            "Peter Haro",
            "B\u00e5rd Hanssen",
            "Sara Bj\u00f6rk",
            "St\u00e5le Walderhaug"
        ],
        "abstract": "Decision support systems help decision makers make better decisions in the face of complex decision problems (e.g. investment or policy decisions). Fisheries and Aquaculture is a domain where decision makers face such decisions since they involve factors from many different scientific fields. No systematic overview of literature describing decision support systems and their application in fisheries and aquaculture has been conducted. This paper summarizes scientific literature that describes decision support systems applied to the domain of Fisheries and Aquaculture. We use an established systematic mapping survey method to conduct our literature mapping. Our research questions are: What decision support systems for fisheries and aquaculture exists? What are the most investigated fishery and aquaculture decision support systems topics and how have these changed over time? Do any current DSS for fisheries provide real- time analytics? Do DSSes in Fisheries and Aquaculture build their models using machine learning done on captured and grounded data? The paper then detail how we employ the systematic mapping method in answering these questions. This results in 27 papers being identified as relevant and gives an exposition on the primary methods concluded in the study for designing a decision support system. We provide an analysis of the research done in the studies collected. We discovered that most literature does not consider multiple aspects for multiple stakeholders in their work. In addition we observed that little or no work has been done with real-time analysis in these decision support systems.\n    ",
        "submission_date": "2016-11-25T00:00:00",
        "last_modified_date": "2016-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08481",
        "title": "GuessWhat?! Visual object discovery through multi-modal dialogue",
        "authors": [
            "Harm de Vries",
            "Florian Strub",
            "Sarath Chandar",
            "Olivier Pietquin",
            "Hugo Larochelle",
            "Aaron Courville"
        ],
        "abstract": "We introduce GuessWhat?!, a two-player guessing game as a testbed for research on the interplay of computer vision and dialogue systems. The goal of the game is to locate an unknown object in a rich image scene by asking a sequence of questions. Higher-level image understanding, like spatial reasoning and language grounding, is required to solve the proposed task. Our key contribution is the collection of a large-scale dataset consisting of 150K human-played games with a total of 800K visual question-answer pairs on 66K images. We explain our design decisions in collecting the dataset and introduce the oracle and questioner tasks that are associated with the two players of the game. We prototyped deep learning models to establish initial baselines of the introduced tasks.\n    ",
        "submission_date": "2016-11-23T00:00:00",
        "last_modified_date": "2017-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08499",
        "title": "An Analysis of Tournament Structure",
        "authors": [
            "Nhien Pham Hoang Bao",
            "Hiroyuki Iida"
        ],
        "abstract": "This paper explores a novel way for analyzing the tournament structures to find a best suitable one for the tournament under consideration. It concerns about three aspects such as tournament conducting cost, competitiveness development and ranking precision. It then proposes a new method using progress tree to detect potential throwaway matches. The analysis performed using the proposed method reveals the strengths and weaknesses of tournament structures. As a conclusion, single elimination is best if we want to qualify one winner only, all matches conducted are exciting in term of competitiveness. Double elimination with proper seeding system is a better choice if we want to qualify more winners. A reasonable number of extra matches need to be conducted in exchange of being able to qualify top four winners. Round-robin gives reliable ranking precision for all participants. However, its conduction cost is very high, and it fails to maintain competitiveness development.\n    ",
        "submission_date": "2016-11-16T00:00:00",
        "last_modified_date": "2016-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08555",
        "title": "New Trends in Neutrosophic Theory and Applications",
        "authors": [
            "Florentin Smarandache",
            "Surapati Pramanik"
        ],
        "abstract": "Neutrosophic theory and applications have been expanding in all directions at an astonishing rate especially after the introduction the journal entitled Neutrosophic Sets and Systems. New theories, techniques, algorithms have been rapidly developed. One of the most striking trends in the neutrosophic theory is the hybridization of neutrosophic set with other potential sets such as rough set, bipolar set, soft set, hesitant fuzzy set, etc. The different hybrid structure such as rough neutrosophic set, single valued neutrosophic rough set, bipolar neutrosophic set, single valued neutrosophic hesitant fuzzy set, etc. are proposed in the literature in a short period of time. Neutrosophic set has been a very important tool in all various areas of data mining, decision making, e-learning, engineering, medicine, social science, and some more. The book New Trends in Neutrosophic Theories and Applications focuses on theories, methods, algorithms for decision making and also applications involving neutrosophic information. Some topics deal with data mining, decision making, e-learning, graph theory, medical diagnosis, probability theory, topology, and some more.\n    ",
        "submission_date": "2016-11-23T00:00:00",
        "last_modified_date": "2016-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08572",
        "title": "Bipolar Weighted Argumentation Graphs",
        "authors": [
            "Till Mossakowski",
            "Fabian Neuhaus"
        ],
        "abstract": "This paper discusses the semantics of weighted argumentation graphs that are biplor, i.e. contain both attacks and support graphs. The work builds on previous work by Amgoud, Ben-Naim et. al., which presents and compares several semantics for argumentation graphs that contain only supports or only attacks relationships, respectively.\n    ",
        "submission_date": "2016-11-25T00:00:00",
        "last_modified_date": "2016-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08675",
        "title": "Deep Reinforcement Learning for Multi-Domain Dialogue Systems",
        "authors": [
            "Heriberto Cuay\u00e1huitl",
            "Seunghak Yu",
            "Ashley Williamson",
            "Jacob Carse"
        ],
        "abstract": "Standard deep reinforcement learning methods such as Deep Q-Networks (DQN) for multiple tasks (domains) face scalability problems. We propose a method for multi-domain dialogue policy learning---termed NDQN, and apply it to an information-seeking spoken dialogue system in the domains of restaurants and hotels. Experimental results comparing DQN (baseline) versus NDQN (proposed) using simulations report that our proposed method exhibits better scalability and is promising for optimising the behaviour of multi-domain dialogue systems.\n    ",
        "submission_date": "2016-11-26T00:00:00",
        "last_modified_date": "2016-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08696",
        "title": "Optimizing Expectation with Guarantees in POMDPs (Technical Report)",
        "authors": [
            "Krishnendu Chatterjee",
            "Petr Novotn\u00fd",
            "Guillermo A. P\u00e9rez",
            "Jean-Fran\u00e7ois Raskin",
            "\u0110or\u0111e \u017dikeli\u0107"
        ],
        "abstract": "A standard objective in partially-observable Markov decision processes (POMDPs) is to find a policy that maximizes the expected discounted-sum payoff. However, such policies may still permit unlikely but highly undesirable outcomes, which is problematic especially in safety-critical applications. Recently, there has been a surge of interest in POMDPs where the goal is to maximize the probability to ensure that the payoff is at least a given threshold, but these approaches do not consider any optimization beyond satisfying this threshold constraint. In this work we go beyond both the \"expectation\" and \"threshold\" approaches and consider a \"guaranteed payoff optimization (GPO)\" problem for POMDPs, where we are given a threshold $t$ and the objective is to find a policy $\\sigma$ such that a) each possible outcome of $\\sigma$ yields a discounted-sum payoff of at least $t$, and b) the expected discounted-sum payoff of $\\sigma$ is optimal (or near-optimal) among all policies satisfying a). We present a practical approach to tackle the GPO problem and evaluate it on standard POMDP benchmarks.\n    ",
        "submission_date": "2016-11-26T00:00:00",
        "last_modified_date": "2017-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08773",
        "title": "Embedded Bandits for Large-Scale Black-Box Optimization",
        "authors": [
            "Abdullah Al-Dujaili",
            "S. Suresh"
        ],
        "abstract": "Random embedding has been applied with empirical success to large-scale black-box optimization problems with low effective dimensions. This paper proposes the EmbeddedHunter algorithm, which incorporates the technique in a hierarchical stochastic bandit setting, following the optimism in the face of uncertainty principle and breaking away from the multiple-run framework in which random embedding has been conventionally applied similar to stochastic black-box optimization solvers. Our proposition is motivated by the bounded mean variation in the objective value for a low-dimensional point projected randomly into the decision space of Lipschitz-continuous problems. In essence, the EmbeddedHunter algorithm expands optimistically a partitioning tree over a low-dimensional---equal to the effective dimension of the problem---search space based on a bounded number of random embeddings of sampled points from the low-dimensional space. In contrast to the probabilistic theoretical guarantees of multiple-run random-embedding algorithms, the finite-time analysis of the proposed algorithm presents a theoretical upper bound on the regret as a function of the algorithm's number of iterations. Furthermore, numerical experiments were conducted to validate its performance. The results show a clear performance gain over recently proposed random embedding methods for large-scale problems, provided the intrinsic dimensionality is low.\n    ",
        "submission_date": "2016-11-27T00:00:00",
        "last_modified_date": "2016-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08908",
        "title": "\"Model and Run\" Constraint Networks with a MILP Engine",
        "authors": [
            "Thierry Petit"
        ],
        "abstract": "Constraint Programming (CP) users need significant expertise in order to model their problems appropriately, notably to select propagators and search strategies. This puts the brakes on a broader uptake of CP. In this paper, we introduce MICE, a complete Java CP modeler that can use any Mixed Integer Linear Programming (MILP) solver as a solution technique. Our aim is to provide an alternative tool for democratizing the \"CP-style\" modeling thanks to its simplicity of use, with reasonable solving capabilities. Our contributions include new decompositions of (reified) constraints and constraints on numerical variables.\n    ",
        "submission_date": "2016-11-27T00:00:00",
        "last_modified_date": "2016-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08942",
        "title": "The BIN_COUNTS Constraint: Filtering and Applications",
        "authors": [
            "Roberto Rossi",
            "\u00d6zg\u00fcr Akg\u00fcn",
            "Steven Prestwich",
            "Armagan Tarim"
        ],
        "abstract": "We introduce the BIN_COUNTS constraint, which deals with the problem of counting the number of decision variables in a set which are assigned values that lie in given bins. We illustrate a decomposition and a filtering algorithm that achieves generalised arc consistency. We contrast the filtering power of these two approaches and we discuss a number of applications. We show that BIN_COUNTS can be employed to develop a decomposition for the $\\chi^2$ test constraint, a new statistical constraint that we introduce in this work. We also show how this new constraint can be employed in the context of the Balanced Academic Curriculum Problem and of the Balanced Nursing Workload Problem. For both these problems we carry out numerical studies involving our reformulations. Finally, we present a further application of the $\\chi^2$ test constraint in the context of confidence interval analysis.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2016-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08944",
        "title": "Nonparametric General Reinforcement Learning",
        "authors": [
            "Jan Leike"
        ],
        "abstract": "Reinforcement learning (RL) problems are often phrased in terms of Markov decision processes (MDPs). In this thesis we go beyond MDPs and consider RL in environments that are non-Markovian, non-ergodic and only partially observable. Our focus is not on practical algorithms, but rather on the fundamental underlying problems: How do we balance exploration and exploitation? How do we explore optimally? When is an agent optimal? We follow the nonparametric realizable paradigm.\n",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2016-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09014",
        "title": "Blocking and Other Enhancements for Bottom-Up Model Generation Methods",
        "authors": [
            "Peter Baumgartner",
            "Renate A. Schmidt"
        ],
        "abstract": "Model generation is a problem complementary to theorem proving and is important for fault analysis and debugging of formal specifications of security protocols, programs and terminological definitions. This paper discusses several ways of enhancing the paradigm of bottom-up model generation. The two main contributions are new, generalized blocking techniques and a new range-restriction transformation. The blocking techniques are based on simple transformations of the input set together with standard equality reasoning and redundancy elimination techniques. These provide general methods for finding small, finite models. The range-restriction transformation refines existing transformations to range-restricted clauses by carefully limiting the creation of domain terms. All possible combinations of the introduced techniques and classical range-restriction were tested on the clausal problems of the TPTP Version 6.0.0 with an implementation based on the SPASS theorem prover using a hyperresolution-like refinement. Unrestricted domain blocking gave best results for satisfiable problems showing it is a powerful technique indispensable for bottom-up model generation methods. Both in combination with the new range-restricting transformation, and the classical range-restricting transformation, good results have been obtained. Limiting the creation of terms during the inference process by using the new range restricting transformation has paid off, especially when using it together with a shifting transformation. The experimental results also show that classical range restriction with unrestricted blocking provides a useful complementary method. Overall, the results showed bottom-up model generation methods were good for disproving theorems and generating models for satisfiable problems, but less efficient than SPASS in auto mode for unsatisfiable problems.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09328",
        "title": "Accelerated Gradient Temporal Difference Learning",
        "authors": [
            "Yangchen Pan",
            "Adam White",
            "Martha White"
        ],
        "abstract": "The family of temporal difference (TD) methods span a spectrum from computationally frugal linear methods like TD({\\lambda}) to data efficient least squares methods. Least square methods make the best use of available data directly computing the TD solution and thus do not require tuning a typically highly sensitive learning rate parameter, but require quadratic computation and storage. Recent algorithmic developments have yielded several sub-quadratic methods that use an approximation to the least squares TD solution, but incur bias. In this paper, we propose a new family of accelerated gradient TD (ATD) methods that (1) provide similar data efficiency benefits to least-squares methods, at a fraction of the computation and storage (2) significantly reduce parameter sensitivity compared to linear TD methods, and (3) are asymptotically unbiased. We illustrate these claims with a proof of convergence in expectation and experiments on several benchmark domains and a large-scale industrial energy allocation domain.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2017-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09351",
        "title": "Adams Conditioning and Likelihood Ratio Transfer Mediated Inference",
        "authors": [
            "Jan A. Bergstra"
        ],
        "abstract": "Bayesian inference as applied in a legal setting is about belief transfer and involves a plurality of agents and communication protocols.\n",
        "submission_date": "2016-11-26T00:00:00",
        "last_modified_date": "2019-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09434",
        "title": "Input Switched Affine Networks: An RNN Architecture Designed for Interpretability",
        "authors": [
            "Jakob N. Foerster",
            "Justin Gilmer",
            "Jan Chorowski",
            "Jascha Sohl-Dickstein",
            "David Sussillo"
        ],
        "abstract": "There exist many problem domains where the interpretability of neural network models is essential for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations - in other words an RNN without any explicit nonlinearities, but with input-dependent recurrent weights. This simple form allows the RNN to be analyzed via straightforward linear methods: we can exactly characterize the linear contribution of each input to the model predictions; we can use a change-of-basis to disentangle input, output, and computational hidden unit subspaces; we can fully reverse-engineer the architecture's solution to a simple task. Despite this ease of interpretation, the input switched affine network achieves reasonable performance on a text modeling tasks, and allows greater computational efficiency than networks with standard nonlinearities.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2017-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09573",
        "title": "Learning Concept Hierarchies through Probabilistic Topic Modeling",
        "authors": [
            "V. S. Anoop",
            "S. Asharaf",
            "P. Deepak"
        ],
        "abstract": "With the advent of semantic web, various tools and techniques have been introduced for presenting and organizing knowledge. Concept hierarchies are one such technique which gained significant attention due to its usefulness in creating domain ontologies that are considered as an integral part of semantic web. Automated concept hierarchy learning algorithms focus on extracting relevant concepts from unstructured text corpus and connect them together by identifying some potential relations exist between them. In this paper, we propose a novel approach for identifying relevant concepts from plain text and then learns hierarchy of concepts by exploiting subsumption relation between them. To start with, we model topics using a probabilistic topic model and then make use of some lightweight linguistic process to extract semantically rich concepts. Then we connect concepts by identifying an \"is-a\" relationship between pair of concepts. The proposed method is completely unsupervised and there is no need for a domain specific training corpus for concept extraction and learning. Experiments on large and real-world text corpora such as BBC News dataset and Reuters News corpus shows that the proposed method outperforms some of the existing methods for concept extraction and efficient concept hierarchy learning is possible if the overall task is guided by a probabilistic topic modeling algorithm.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09823",
        "title": "Dialogue Learning With Human-In-The-Loop",
        "authors": [
            "Jiwei Li",
            "Alexander H. Miller",
            "Sumit Chopra",
            "Marc'Aurelio Ranzato",
            "Jason Weston"
        ],
        "abstract": "An important aspect of developing conversational agents is to give a bot the ability to improve through communicating with humans and to learn from the mistakes that it makes. Most research has focused on learning from fixed training sets of labeled data rather than interacting with a dialogue partner in an online fashion. In this paper we explore this direction in a reinforcement learning setting where the bot improves its question-answering ability from feedback a teacher gives following its generated responses. We build a simulator that tests various aspects of such learning in a synthetic environment, and introduce models that work in this regime. Finally, real experiments with Mechanical Turk validate the approach.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2017-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09894",
        "title": "Exploration for Multi-task Reinforcement Learning with Deep Generative Models",
        "authors": [
            "Sai Praveen Bangaru",
            "JS Suhas",
            "Balaraman Ravindran"
        ],
        "abstract": "Exploration in multi-task reinforcement learning is critical in training agents to deduce the underlying MDP. Many of the existing exploration frameworks such as $E^3$, $R_{max}$, Thompson sampling assume a single stationary MDP and are not suitable for system identification in the multi-task setting. We present a novel method to facilitate exploration in multi-task reinforcement learning using deep generative models. We supplement our method with a low dimensional energy model to learn the underlying MDP distribution and provide a resilient and adaptive exploration signal to the agent. We evaluate our method on a new set of environments and provide intuitive interpretation of our results.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09904",
        "title": "C-RNN-GAN: Continuous recurrent neural networks with adversarial training",
        "authors": [
            "Olof Mogren"
        ],
        "abstract": "Generative adversarial networks have been proposed as a way of efficiently training deep generative neural networks. We propose a generative adversarial model that works on continuous sequential data, and apply it by training it on a collection of classical music. We conclude that it generates music that sounds better and better as the model is trained, report statistics on generated music, and let the reader judge the quality by downloading the generated songs.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09940",
        "title": "Neural Combinatorial Optimization with Reinforcement Learning",
        "authors": [
            "Irwan Bello",
            "Hieu Pham",
            "Quoc V. Le",
            "Mohammad Norouzi",
            "Samy Bengio"
        ],
        "abstract": "This paper presents a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning. We focus on the traveling salesman problem (TSP) and train a recurrent network that, given a set of city coordinates, predicts a distribution over different city permutations. Using negative tour length as the reward signal, we optimize the parameters of the recurrent network using a policy gradient method. We compare learning the network parameters on a set of training graphs against learning them on individual test graphs. Despite the computational expense, without much engineering and heuristic designing, Neural Combinatorial Optimization achieves close to optimal results on 2D Euclidean graphs with up to 100 nodes. Applied to the KnapSack, another NP-hard problem, the same method obtains optimal solutions for instances with up to 200 items.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2017-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09948",
        "title": "Contextualizing Geometric Data Analysis and Related Data Analytics: A Virtual Microscope for Big Data Analytics",
        "authors": [
            "Fionn Murtagh",
            "Mohsen Farid"
        ],
        "abstract": "The relevance and importance of contextualizing data analytics is described. Qualitative characteristics might form the context of quantitative analysis. Topics that are at issue include: contrast, baselining, secondary data sources, supplementary data sources, dynamic and heterogeneous data. In geometric data analysis, especially with the Correspondence Analysis platform, various case studies are both experimented with, and are reviewed. In such aspects as paradigms followed, and technical implementation, implicitly and explicitly, an important point made is the major relevance of such work for both burgeoning analytical needs and for new analytical areas including Big Data analytics, and so on. For the general reader, it is aimed to display and describe, first of all, the analytical outcomes that are subject to analysis here, and then proceed to detail the more quantitative outcomes that fully support the analytics carried out.\n    ",
        "submission_date": "2016-11-30T00:00:00",
        "last_modified_date": "2017-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09957",
        "title": "Low-dimensional Data Embedding via Robust Ranking",
        "authors": [
            "Ehsan Amid",
            "Nikos Vlassis",
            "Manfred K. Warmuth"
        ],
        "abstract": "We describe a new method called t-ETE for finding a low-dimensional embedding of a set of objects in Euclidean space. We formulate the embedding problem as a joint ranking problem over a set of triplets, where each triplet captures the relative similarities between three objects in the set. By exploiting recent advances in robust ranking, t-ETE produces high-quality embeddings even in the presence of a significant amount of noise and better preserves local scale than known methods, such as t-STE and t-SNE. In particular, our method produces significantly better results than t-SNE on signature datasets while also being faster to compute.\n    ",
        "submission_date": "2016-11-30T00:00:00",
        "last_modified_date": "2017-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.10095",
        "title": "System-Generated Requests for Rewriting Proposals",
        "authors": [
            "Pietro Speroni di Fenizio",
            "Cyril Velikanov"
        ],
        "abstract": "We present an online deliberation system using mutual evaluation in order to collaboratively develop solutions. Participants submit their proposals and evaluate each other's proposals; some of them may then be invited by the system to rewrite 'problematic' proposals. Two cases are discussed: a proposal supported by many, but not by a given person, who is then invited to rewrite it for making yet more acceptable; and a poorly presented but presumably interesting proposal. The first of these cases has been successfully implemented. Proposals are evaluated along two axes-understandability (or clarity, or, more generally, quality), and agreement. The latter is used by the system to cluster proposals according to their ideas, while the former is used both to present the best proposals on top of their clusters, and to find poorly written proposals candidates for rewriting. These functionalities may be considered as important components of a large scale online deliberation system.\n    ",
        "submission_date": "2016-11-30T00:00:00",
        "last_modified_date": "2016-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.10120",
        "title": "Fusion of EEG and Musical Features in Continuous Music-emotion Recognition",
        "authors": [
            "Nattapong Thammasan",
            "Ken-ichi Fukui",
            "Masayuki Numao"
        ],
        "abstract": "Emotion estimation in music listening is confronting challenges to capture the emotion variation of listeners. Recent years have witnessed attempts to exploit multimodality fusing information from musical contents and physiological signals captured from listeners to improve the performance of emotion recognition. In this paper, we present a study of fusion of signals of electroencephalogram (EEG), a tool to capture brainwaves at a high-temporal resolution, and musical features at decision level in recognizing the time-varying binary classes of arousal and valence. Our empirical results showed that the fusion could outperform the performance of emotion recognition using only EEG modality that was suffered from inter-subject variability, and this suggested the promise of multimodal fusion in improving the accuracy of music-emotion recognition.\n    ",
        "submission_date": "2016-11-30T00:00:00",
        "last_modified_date": "2016-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00092",
        "title": "Computer Assisted Composition with Recurrent Neural Networks",
        "authors": [
            "Christian Walder",
            "Dongwoo Kim"
        ],
        "abstract": "Sequence modeling with neural networks has lead to powerful models of symbolic music data. We address the problem of exploiting these models to reach creative musical goals, by combining with human input. To this end we generalise previous work, which sampled Markovian sequence models under the constraint that the sequence belong to the language of a given finite state machine provided by the human. We consider more expressive non-Markov models, thereby requiring approximate sampling which we provide in the form of an efficient sequential Monte Carlo method. In addition we provide and compare with a beam search strategy for conditional probability maximisation.\n",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2017-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00094",
        "title": "Optimizing Quantiles in Preference-based Markov Decision Processes",
        "authors": [
            "Hugo Gilbert",
            "Paul Weng",
            "Yan Xu"
        ],
        "abstract": "In the Markov decision process model, policies are usually evaluated by expected cumulative rewards. As this decision criterion is not always suitable, we propose in this paper an algorithm for computing a policy optimal for the quantile criterion. Both finite and infinite horizons are considered. Finally we experimentally evaluate our approach on random MDPs and on a data center control problem.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2016-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00104",
        "title": "Robust Optimization for Tree-Structured Stochastic Network Design",
        "authors": [
            "Xiaojian Wu",
            "Akshat Kumar",
            "Daniel Sheldon",
            "Shlomo Zilberstein"
        ],
        "abstract": "Stochastic network design is a general framework for optimizing network connectivity. It has several applications in computational sustainability including spatial conservation planning, pre-disaster network preparation, and river network optimization. A common assumption in previous work has been made that network parameters (e.g., probability of species colonization) are precisely known, which is unrealistic in real- world settings. We therefore address the robust river network design problem where the goal is to optimize river connectivity for fish movement by removing barriers. We assume that fish passability probabilities are known only imprecisely, but are within some interval bounds. We then develop a planning approach that computes the policies with either high robust ratio or low regret. Empirically, our approach scales well to large river networks. We also provide insights into the solutions generated by our robust approach, which has significantly higher robust ratio than the baseline solution with mean parameter estimates.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2016-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00222",
        "title": "Interaction Networks for Learning about Objects, Relations and Physics",
        "authors": [
            "Peter W. Battaglia",
            "Razvan Pascanu",
            "Matthew Lai",
            "Danilo Rezende",
            "Koray Kavukcuoglu"
        ],
        "abstract": "Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2016-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00227",
        "title": "On Coreferring Text-extracted Event Descriptions with the aid of Ontological Reasoning",
        "authors": [
            "Stefano Borgo",
            "Loris Bozzato",
            "Alessio Palmero Aprosio",
            "Marco Rospocher",
            "Luciano Serafini"
        ],
        "abstract": "Systems for automatic extraction of semantic information about events from large textual resources are now available: these tools are capable to generate RDF datasets about text extracted events and this knowledge can be used to reason over the recognized events. On the other hand, text based tasks for event recognition, as for example event coreference (i.e. recognizing whether two textual descriptions refer to the same event), do not take into account ontological information of the extracted events in their process. In this paper, we propose a method to derive event coreference on text extracted event data using semantic based rule reasoning. We demonstrate our method considering a limited (yet representative) set of event types: we introduce a formal analysis on their ontological properties and, on the base of this, we define a set of coreference criteria. We then implement these criteria as RDF-based reasoning rules to be applied on text extracted event data. We evaluate the effectiveness of our approach over a standard coreference benchmark dataset.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2016-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00240",
        "title": "An Evaluation of Models for Runtime Approximation in Link Discovery",
        "authors": [
            "Kleanthi Georgala",
            "Micheal Hoffmann",
            "Axel-Cyrille Ngonga Ngomo"
        ],
        "abstract": "Time-efficient link discovery is of central importance to implement the vision of the Semantic Web. Some of the most rapid Link Discovery approaches rely internally on planning to execute link specifications. In newer works, linear models have been used to estimate the runtime the fastest planners. However, no other category of models has been studied for this purpose so far. In this paper, we study non-linear runtime estimation functions for runtime estimation. In particular, we study exponential and mixed models for the estimation of the runtimes of planners. To this end, we evaluate three different models for runtime on six datasets using 400 link specifications. We show that exponential and mixed models achieve better fits when trained but are only to be preferred in some cases. Our evaluation also shows that the use of better runtime approximation models has a positive impact on the overall execution of link specifications.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2016-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00341",
        "title": "A Compositional Object-Based Approach to Learning Physical Dynamics",
        "authors": [
            "Michael B. Chang",
            "Tomer Ullman",
            "Antonio Torralba",
            "Joshua B. Tenenbaum"
        ],
        "abstract": "We present the Neural Physics Engine (NPE), a framework for learning simulators of intuitive physics that naturally generalize across variable object count and different scene configurations. We propose a factorization of a physical scene into composable object-based representations and a neural network architecture whose compositional structure factorizes object dynamics into pairwise interactions. Like a symbolic physics engine, the NPE is endowed with generic notions of objects and their interactions; realized as a neural network, it can be trained via stochastic gradient descent to adapt to specific object properties and dynamics of different worlds. We evaluate the efficacy of our approach on simple rigid body dynamics in two-dimensional worlds. By comparing to less structured architectures, we show that the NPE's compositional representation of the structure in physical interactions improves its ability to predict movement, generalize across variable object count and different scene configurations, and infer latent properties of objects such as mass.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2017-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00380",
        "title": "Playing Doom with SLAM-Augmented Deep Reinforcement Learning",
        "authors": [
            "Shehroze Bhatti",
            "Alban Desmaison",
            "Ondrej Miksik",
            "Nantas Nardelli",
            "N. Siddharth",
            "Philip H. S. Torr"
        ],
        "abstract": "A number of recent approaches to policy learning in 2D game domains have been successful going directly from raw input images to actions. However when employed in complex 3D environments, they typically suffer from challenges related to partial observability, combinatorial exploration spaces, path planning, and a scarcity of rewarding scenarios. Inspired from prior work in human cognition that indicates how humans employ a variety of semantic concepts and abstractions (object categories, localisation, etc.) to reason about the world, we build an agent-model that incorporates such abstractions into its policy-learning framework. We augment the raw image input to a Deep Q-Learning Network (DQN), by adding details of objects and structural elements encountered, along with the agent's localisation. The different components are automatically extracted and composed into a topological representation using on-the-fly object detection and 3D-scene ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2016-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00742",
        "title": "Comparison of the COG Defuzzification Technique and Its Variations to the GPA Index",
        "authors": [
            "Michael Gr. Voskoglou"
        ],
        "abstract": "The Center of Gravity (COG) method is one of the most popular defuzzification techniques of fuzzy mathematics. In earlier works the COG technique was properly adapted to be used as an assessment model (RFAM)and several variations of it (GRFAM, TFAM and TpFAM)were also constructed for the same purpose. In this paper the outcomes of all these models are compared to the corresponding outcomes of a traditional assessment method of the bi-valued logic, the Grade Point Average (GPA) Index. Examples are also presented illustrating our results.\n    ",
        "submission_date": "2016-11-30T00:00:00",
        "last_modified_date": "2016-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00916",
        "title": "A Matrix Splitting Perspective on Planning with Options",
        "authors": [
            "Pierre-Luc Bacon",
            "Doina Precup"
        ],
        "abstract": "We show that the Bellman operator underlying the options framework leads to a matrix splitting, an approach traditionally used to speed up convergence of iterative solvers for large linear systems of equations. Based on standard comparison theorems for matrix splittings, we then show how the asymptotic rate of convergence varies as a function of the inherent timescales of the options. This new perspective highlights a trade-off between asymptotic performance and the cost of computation associated with building a good set of options.\n    ",
        "submission_date": "2016-12-03T00:00:00",
        "last_modified_date": "2017-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00944",
        "title": "Using Discourse Signals for Robust Instructor Intervention Prediction",
        "authors": [
            "Muthu Kumar Chandrasekaran",
            "Carrie Demmans Epp",
            "Min-Yen Kan",
            "Diane Litman"
        ],
        "abstract": "We tackle the prediction of instructor intervention in student posts from discussion forums in Massive Open Online Courses (MOOCs). Our key finding is that using automatically obtained discourse relations improves the prediction of when instructors intervene in student discussions, when compared with a state-of-the-art, feature-rich baseline. Our supervised classifier makes use of an automatic discourse parser which outputs Penn Discourse Treebank (PDTB) tags that represent in-post discourse features. We show PDTB relation-based features increase the robustness of the classifier and complement baseline features in recalling more diverse instructor intervention patterns. In comprehensive experiments over 14 MOOC offerings from several disciplines, the PDTB discourse features improve performance on average. The resultant models are less dependent on domain-specific vocabulary, allowing them to better generalize to new courses.\n    ",
        "submission_date": "2016-12-03T00:00:00",
        "last_modified_date": "2016-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00959",
        "title": "RecSys Challenge 2016: job recommendations based on preselection of offers and gradient boosting",
        "authors": [
            "Andrzej Pacuk",
            "Piotr Sankowski",
            "Karol W\u0119grzycki",
            "Adam Witkowski",
            "Piotr Wygocki"
        ],
        "abstract": "We present the Mim-Solution's approach to the RecSys Challenge 2016, which ranked 2nd. The goal of the competition was to prepare job recommendations for the users of the website ",
        "submission_date": "2016-12-03T00:00:00",
        "last_modified_date": "2016-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01010",
        "title": "DeepBach: a Steerable Model for Bach Chorales Generation",
        "authors": [
            "Ga\u00ebtan Hadjeres",
            "Fran\u00e7ois Pachet",
            "Frank Nielsen"
        ],
        "abstract": "This paper introduces DeepBach, a graphical model aimed at modeling polyphonic music and specifically hymn-like pieces. We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. DeepBach's strength comes from the use of pseudo-Gibbs sampling coupled with an adapted representation of musical data. This is in contrast with many automatic music composition approaches which tend to compose music sequentially. Our model is also steerable in the sense that a user can constrain the generation by imposing positional constraints such as notes, rhythms or cadences in the generated score. We also provide a plugin on top of the MuseScore music editor making the interaction with DeepBach easy to use.\n    ",
        "submission_date": "2016-12-03T00:00:00",
        "last_modified_date": "2017-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01058",
        "title": "Algorithmic Songwriting with ALYSIA",
        "authors": [
            "Margareta Ackerman",
            "David Loker"
        ],
        "abstract": "This paper introduces ALYSIA: Automated LYrical SongwrIting Application. ALYSIA is based on a machine learning model using Random Forests, and we discuss its success at pitch and rhythm prediction. Next, we show how ALYSIA was used to create original pop songs that were subsequently recorded and produced. Finally, we discuss our vision for the future of Automated Songwriting for both co-creative and autonomous systems.\n    ",
        "submission_date": "2016-12-04T00:00:00",
        "last_modified_date": "2016-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01086",
        "title": "Deep Learning of Robotic Tasks without a Simulator using Strong and Weak Human Supervision",
        "authors": [
            "Bar Hilleli",
            "Ran El-Yaniv"
        ],
        "abstract": "We propose a scheme for training a computerized agent to perform complex human tasks such as highway steering. The scheme is designed to follow a natural learning process whereby a human instructor teaches a computerized trainee. The learning process consists of five elements: (i) unsupervised feature learning; (ii) supervised imitation learning; (iii) supervised reward induction; (iv) supervised safety module construction; and (v) reinforcement learning. We implemented the last four elements of the scheme using deep convolutional networks and applied it to successfully create a computerized agent capable of autonomous highway steering over the well-known racing game Assetto Corsa. We demonstrate that the use of the last four elements is essential to effectively carry out the steering task using vision alone, without access to a driving simulator internals, and operating in wall-clock time. This is made possible also through the introduction of a safety network, a novel way for preventing the agent from performing catastrophic mistakes during the reinforcement learning stage.\n    ",
        "submission_date": "2016-12-04T00:00:00",
        "last_modified_date": "2017-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01120",
        "title": "The Complexity of Bayesian Networks Specified by Propositional and Relational Languages",
        "authors": [
            "Fabio Gagliardi Cozman",
            "Denis Deratani Mau\u00e1"
        ],
        "abstract": "We examine the complexity of inference in Bayesian networks specified by logical languages. We consider representations that range from fragments of propositional logic to function-free first-order logic with equality; in doing so we cover a variety of plate models and of probabilistic relational models. We study the complexity of inferences when network, query and domain are the input (the inferential and the combined complexity), when the network is fixed and query and domain are the input (the query/data complexity), and when the network and query are fixed and the domain is the input (the domain complexity). We draw connections with probabilistic databases and liftability results, and obtain complexity classes that range from polynomial to exponential levels.\n    ",
        "submission_date": "2016-12-04T00:00:00",
        "last_modified_date": "2017-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01608",
        "title": "AI Researchers, Video Games Are Your Friends!",
        "authors": [
            "Julian Togelius"
        ],
        "abstract": "If you are an artificial intelligence researcher, you should look to video games as ideal testbeds for the work you do. If you are a video game developer, you should look to AI for the technology that makes completely new types of games possible. This chapter lays out the case for both of these propositions. It asks the question \"what can video games do for AI\", and discusses how in particular general video game playing is the ideal testbed for artificial general intelligence research. It then asks the question \"what can AI do for video games\", and lays out a vision for what video games might look like if we had significantly more advanced AI at our disposal. The chapter is based on my keynote at IJCCI 2015, and is written in an attempt to be accessible to a broad audience.\n    ",
        "submission_date": "2016-12-06T00:00:00",
        "last_modified_date": "2016-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01691",
        "title": "Fleet Size and Mix Split-Delivery Vehicle Routing",
        "authors": [
            "Arthur Mah\u00e9o",
            "Tommaso Urli",
            "Philip Kilby"
        ],
        "abstract": "In the classic Vehicle Routing Problem (VRP) a fleet of of vehicles has to visit a set of customers while minimising the operations' costs. We study a rich variant of the VRP featuring split deliveries, an heterogeneous fleet, and vehicle-commodity incompatibility constraints. Our goal is twofold: define the cheapest routing and the most adequate fleet.\n",
        "submission_date": "2016-12-06T00:00:00",
        "last_modified_date": "2016-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01857",
        "title": "On a Well-behaved Relational Generalisation of Rough Set Approximations",
        "authors": [
            "Alexa Gopaulsingh"
        ],
        "abstract": "We examine non-dual relational extensions of rough set approximations and find an extension which satisfies surprisingly many of the usual rough set properties. We then use this definition to give an explanation for an observation made by Samanta and Chakraborty in their recent paper [P. Samanta and M.K. Chakraborty. Interface of rough set systems and modal logics: A survey. Transactions on Rough Sets XIX, pages 114-137, 2015].\n    ",
        "submission_date": "2016-12-05T00:00:00",
        "last_modified_date": "2016-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01892",
        "title": "Cross-Lingual Predicate Mapping Between Linked Data Ontologies",
        "authors": [
            "Gautam Singh",
            "Saemi Jang",
            "Mun Y. Yi"
        ],
        "abstract": "Ontologies in different natural languages often differ in quality in terms of richness of schema or richness of internal links. This difference is markedly visible when comparing a rich English language ontology with a non-English language counterpart. Discovering alignment between them is a useful endeavor as it serves as a starting point in bridging the disparity. In particular, our work is motivated by the absence of inter-language links for predicates in the localised versions of DBpedia. In this paper, we propose and demonstrate an ad-hoc system to find possible owl:equivalentProperty links between predicates in ontologies of different natural languages. We seek to achieve this mapping by using pre-existing inter-language links of the resources connected by the given predicate. Thus, our methodology stresses on semantic similarity rather than lexical. Moreover, through an evaluation, we show that our system is capable of outperforming a baseline system that is similar to the one used in recent OAEI campaigns.\n    ",
        "submission_date": "2016-12-06T00:00:00",
        "last_modified_date": "2016-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01941",
        "title": "Coactive Critiquing: Elicitation of Preferences and Features",
        "authors": [
            "Stefano Teso",
            "Paolo Dragone",
            "Andrea Passerini"
        ],
        "abstract": "When faced with complex choices, users refine their own preference criteria as they explore the catalogue of options. In this paper we propose an approach to preference elicitation suited for this scenario. We extend Coactive Learning, which iteratively collects manipulative feedback, to optionally query example critiques. User critiques are integrated into the learning model by dynamically extending the feature space. Our formulation natively supports constructive learning tasks, where the option catalogue is generated on-the-fly. We present an upper bound on the average regret suffered by the learner. Our empirical analysis highlights the promise of our approach.\n    ",
        "submission_date": "2016-12-06T00:00:00",
        "last_modified_date": "2016-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02088",
        "title": "Transition-based versus State-based Reward Functions for MDPs with Value-at-Risk",
        "authors": [
            "Shuai Ma",
            "Jia Yuan Yu"
        ],
        "abstract": "In reinforcement learning, the reward function on current state and action is widely used. When the objective is about the expectation of the (discounted) total reward only, it works perfectly. However, if the objective involves the total reward distribution, the result will be wrong. This paper studies Value-at-Risk (VaR) problems in short- and long-horizon Markov decision processes (MDPs) with two reward functions, which share the same expectations. Firstly we show that with VaR objective, when the real reward function is transition-based (with respect to action and both current and next states), the simplified (state-based, with respect to action and current state only) reward function will change the VaR. Secondly, for long-horizon MDPs, we estimate the VaR function with the aid of spectral theory and the central limit theorem. Thirdly, since the estimation method is for a Markov reward process with the reward function on current state only, we present a transformation algorithm for the Markov reward process with the reward function on current and next states, in order to estimate the VaR function with an intact total reward distribution.\n    ",
        "submission_date": "2016-12-07T00:00:00",
        "last_modified_date": "2018-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02161",
        "title": "Measuring the non-asymptotic convergence of sequential Monte Carlo samplers using probabilistic programming",
        "authors": [
            "Marco F. Cusumano-Towner",
            "Vikash K. Mansinghka"
        ],
        "abstract": "A key limitation of sampling algorithms for approximate inference is that it is difficult to quantify their approximation error. Widely used sampling schemes, such as sequential importance sampling with resampling and Metropolis-Hastings, produce output samples drawn from a distribution that may be far from the target posterior distribution. This paper shows how to upper-bound the symmetric KL divergence between the output distribution of a broad class of sequential Monte Carlo (SMC) samplers and their target posterior distributions, subject to assumptions about the accuracy of a separate gold-standard sampler. The proposed method applies to samplers that combine multiple particles, multinomial resampling, and rejuvenation kernels. The experiments show the technique being used to estimate bounds on the divergence of SMC samplers for posterior inference in a Bayesian linear regression model and a Dirichlet process mixture model.\n    ",
        "submission_date": "2016-12-07T00:00:00",
        "last_modified_date": "2017-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02255",
        "title": "Knowledge Representation in Graphs using Convolutional Neural Networks",
        "authors": [
            "Armando Vieira"
        ],
        "abstract": "Knowledge Graphs (KG) constitute a flexible representation of complex relationships between entities particularly useful for biomedical data. These KG, however, are very sparse with many missing edges (facts) and the visualisation of the mesh of interactions nontrivial. Here we apply a compositional model to embed nodes and relationships into a vectorised semantic space to perform graph completion. A visualisation tool based on Convolutional Neural Networks and Self-Organised Maps (SOM) is proposed to extract high-level insights from the KG. We apply this technique to a subset of CTD, containing interactions of compounds with human genes / proteins and show that the performance is comparable to the one obtained by structural models.\n    ",
        "submission_date": "2016-12-07T00:00:00",
        "last_modified_date": "2016-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02310",
        "title": "Extend natural neighbor: a novel classification method with self-adaptive neighborhood parameters in different stages",
        "authors": [
            "Ji Feng",
            "Qingsheng Zhu",
            "Jinlong Huang",
            "Lijun Yang"
        ],
        "abstract": "Various kinds of k-nearest neighbor (KNN) based classification methods are the bases of many well-established and high-performance pattern-recognition techniques, but both of them are vulnerable to their parameter choice. Essentially, the challenge is to detect the neighborhood of various data sets, while utterly ignorant of the data characteristic. This article introduces a new supervised classification method: the extend natural neighbor (ENaN) method, and shows that it provides a better classification result without choosing the neighborhood parameter artificially. Unlike the original KNN based method which needs a prior k, the ENaNE method predicts different k in different stages. Therefore, the ENaNE method is able to learn more from flexible neighbor information both in training stage and testing stage, and provide a better classification result.\n    ",
        "submission_date": "2016-12-07T00:00:00",
        "last_modified_date": "2016-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02487",
        "title": "Interactive Elicitation of Knowledge on Feature Relevance Improves Predictions in Small Data Sets",
        "authors": [
            "Luana Micallef",
            "Iiris Sundin",
            "Pekka Marttinen",
            "Muhammad Ammad-ud-din",
            "Tomi Peltola",
            "Marta Soare",
            "Giulio Jacucci",
            "Samuel Kaski"
        ],
        "abstract": "Providing accurate predictions is challenging for machine learning algorithms when the number of features is larger than the number of samples in the data. Prior knowledge can improve machine learning models by indicating relevant variables and parameter values. Yet, this prior knowledge is often tacit and only available from domain experts. We present a novel approach that uses interactive visualization to elicit the tacit prior knowledge and uses it to improve the accuracy of prediction models. The main component of our approach is a user model that models the domain expert's knowledge of the relevance of different features for a prediction task. In particular, based on the expert's earlier input, the user model guides the selection of the features on which to elicit user's knowledge next. The results of a controlled user study show that the user model significantly improves prior knowledge elicitation and prediction accuracy, when predicting the relative citation counts of scientific documents in a specific domain.\n    ",
        "submission_date": "2016-12-07T00:00:00",
        "last_modified_date": "2017-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02587",
        "title": "Inverses, Conditionals and Compositional Operators in Separative Valuation Algebra",
        "authors": [
            "Juerg Kohlas"
        ],
        "abstract": "Compositional models were introduce by Jirousek and Shenoy in the general framework of valuation-based systems. They based their theory on an axiomatic system of valuations involving not only the operations of combination and marginalisation, but also of removal. They claimed that this systems covers besides the classical case of discrete probability distributions, also the cases of Gaussian densities and belief functions, and many other systems.\n",
        "submission_date": "2016-12-08T00:00:00",
        "last_modified_date": "2016-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02660",
        "title": "Decision Theory in an Algebraic Setting",
        "authors": [
            "Maurizio Negri"
        ],
        "abstract": "In decision theory an act is a function from a set of conditions to the set of real numbers. The set of conditions is a partition in some algebra of events. The expected value of an act can be calculated when a probability measure is given. We adopt an algebraic point of view by substituting the algebra of events with a finite distributive lattice and the probability measure with a lattice valuation. We introduce a partial order on acts that generalizes the dominance relation and show that the set of acts is a lattice with respect to this order. Finally we analyze some different kinds of comparison between acts, without supposing a common set of conditions for the acts to be compared.\n    ",
        "submission_date": "2016-12-02T00:00:00",
        "last_modified_date": "2016-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02757",
        "title": "Hierarchy through Composition with Linearly Solvable Markov Decision Processes",
        "authors": [
            "Andrew M. Saxe",
            "Adam Earle",
            "Benjamin Rosman"
        ],
        "abstract": "Hierarchical architectures are critical to the scalability of reinforcement learning methods. Current hierarchical frameworks execute actions serially, with macro-actions comprising sequences of primitive actions. We propose a novel alternative to these control hierarchies based on concurrent execution of many actions in parallel. Our scheme uses the concurrent compositionality provided by the linearly solvable Markov decision process (LMDP) framework, which naturally enables a learning agent to draw on several macro-actions simultaneously to solve new tasks. We introduce the Multitask LMDP module, which maintains a parallel distributed representation of tasks and may be stacked to form deep hierarchies abstracted in space and time.\n    ",
        "submission_date": "2016-12-08T00:00:00",
        "last_modified_date": "2016-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02904",
        "title": "GOTM: a Goal-Oriented Framework for Capturing Uncertainty of Medical Treatments",
        "authors": [
            "Davoud Mougouei",
            "David Powers"
        ],
        "abstract": "It has been widely recognized that uncertainty is an inevitable aspect of diagnosis and treatment of medical disorders. Such uncertainties hence, need to be considered in computerized medical models. The existing medical modeling techniques however, have mainly focused on capturing uncertainty associated with diagnosis of medical disorders while ignoring uncertainty of treatments. To tackle this issue, we have proposed using a fuzzy-based modeling and description technique for capturing uncertainties in treatment plans. We have further contributed a formal framework which allows for goal-oriented modeling and analysis of medical treatments.\n    ",
        "submission_date": "2016-12-09T00:00:00",
        "last_modified_date": "2020-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03055",
        "title": "Measuring Adverse Drug Effects on Multimorbity using Tractable Bayesian Networks",
        "authors": [
            "Jessa Bekker",
            "Arjen Hommersom",
            "Martijn Lappenschaar",
            "Jesse Davis"
        ],
        "abstract": "Managing patients with multimorbidity often results in polypharmacy: the prescription of multiple drugs. However, the long-term effects of specific combinations of drugs and diseases are typically unknown. In particular, drugs prescribed for one condition may result in adverse effects for the other. To investigate which types of drugs may affect the further progression of multimorbidity, we query models of diseases and prescriptions that are learned from primary care data. State-of-the-art tractable Bayesian network representations, on which such complex queries can be computed efficiently, are employed for these large medical networks. Our results confirm that prescriptions may lead to unintended negative consequences in further development of multimorbidity in cardiovascular diseases. Moreover, a drug treatment for one disease group may affect diseases of another group.\n    ",
        "submission_date": "2016-12-09T00:00:00",
        "last_modified_date": "2016-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03211",
        "title": "DeepCancer: Detecting Cancer through Gene Expressions via Deep Generative Learning",
        "authors": [
            "Rajendra Rana Bhat",
            "Vivek Viswanath",
            "Xiaolin Li"
        ],
        "abstract": "Transcriptional profiling on microarrays to obtain gene expressions has been used to facilitate cancer diagnosis. We propose a deep generative machine learning architecture (called DeepCancer) that learn features from unlabeled microarray data. These models have been used in conjunction with conventional classifiers that perform classification of the tissue samples as either being cancerous or non-cancerous. The proposed model has been tested on two different clinical datasets. The evaluation demonstrates that DeepCancer model achieves a very high precision score, while significantly controlling the false positive and false negative scores.\n    ",
        "submission_date": "2016-12-09T00:00:00",
        "last_modified_date": "2016-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03328",
        "title": "Knowledge Elicitation via Sequential Probabilistic Inference for High-Dimensional Prediction",
        "authors": [
            "Pedram Daee",
            "Tomi Peltola",
            "Marta Soare",
            "Samuel Kaski"
        ],
        "abstract": "Prediction in a small-sized sample with a large number of covariates, the \"small n, large p\" problem, is challenging. This setting is encountered in multiple applications, such as precision medicine, where obtaining additional samples can be extremely costly or even impossible, and extensive research effort has recently been dedicated to finding principled solutions for accurate prediction. However, a valuable source of additional information, domain experts, has not yet been efficiently exploited. We formulate knowledge elicitation generally as a probabilistic inference process, where expert knowledge is sequentially queried to improve predictions. In the specific case of sparse linear regression, where we assume the expert has knowledge about the values of the regression coefficients or about the relevance of the features, we propose an algorithm and computational approximation for fast and efficient interaction, which sequentially identifies the most informative features on which to query expert knowledge. Evaluations of our method in experiments with simulated and real users show improved prediction accuracy already with a small effort from the expert.\n    ",
        "submission_date": "2016-12-10T00:00:00",
        "last_modified_date": "2017-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03353",
        "title": "FOCA: A Methodology for Ontology Evaluation",
        "authors": [
            "Judson Bandeira",
            "Ig Ibert Bittencourt",
            "Patricia Espinheira",
            "Seiji Isotani"
        ],
        "abstract": "Modeling an ontology is a hard and time-consuming task. Although methodologies are useful for ontologists to create good ontologies, they do not help with the task of evaluating the quality of the ontology to be reused. For these reasons, it is imperative to evaluate the quality of the ontology after constructing it or before reusing it. Few studies usually present only a set of criteria and questions, but no guidelines to evaluate the ontology. The effort to evaluate an ontology is very high as there is a huge dependence on the evaluator's expertise to understand the criteria and questions in depth. Moreover, the evaluation is still very subjective. This study presents a novel methodology for ontology evaluation, taking into account three fundamental principles: i) it is based on the Goal, Question, Metric approach for empirical evaluation; ii) the goals of the methodologies are based on the roles of knowledge representations combined with specific evaluation criteria; iii) each ontology is evaluated according to the type of ontology. The methodology was empirically evaluated using different ontologists and ontologies of the same domain. The main contributions of this study are: i) defining a step-by-step approach to evaluate the quality of an ontology; ii) proposing an evaluation based on the roles of knowledge representations; iii) the explicit difference of the evaluation according to the type of the ontology iii) a questionnaire to evaluate the ontologies; iv) a statistical model that automatically calculates the quality of the ontologies.\n    ",
        "submission_date": "2016-12-10T00:00:00",
        "last_modified_date": "2017-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03471",
        "title": "Reinforcement Learning With Temporal Logic Rewards",
        "authors": [
            "Xiao Li",
            "Cristian-Ioan Vasile",
            "Calin Belta"
        ],
        "abstract": "Reinforcement learning (RL) depends critically on the choice of reward functions used to capture the de- sired behavior and constraints of a robot. Usually, these are handcrafted by a expert designer and represent heuristics for relatively simple tasks. Real world applications typically involve more complex tasks with rich temporal and logical structure. In this paper we take advantage of the expressive power of temporal logic (TL) to specify complex rules the robot should follow, and incorporate domain knowledge into learning. We propose Truncated Linear Temporal Logic (TLTL) as specifications language, that is arguably well suited for the robotics applications, together with quantitative semantics, i.e., robustness degree. We propose a RL approach to learn tasks expressed as TLTL formulae that uses their associated robustness degree as reward functions, instead of the manually crafted heuristics trying to capture the same specifications. We show in simulated trials that learning is faster and policies obtained using the proposed approach outperform the ones learned using heuristic rewards in terms of the robustness degree, i.e., how well the tasks are satisfied. Furthermore, we demonstrate the proposed RL approach in a toast-placing task learned by a Baxter robot.\n    ",
        "submission_date": "2016-12-11T00:00:00",
        "last_modified_date": "2017-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03494",
        "title": "Flu Detector: Estimating influenza-like illness rates from online user-generated content",
        "authors": [
            "Vasileios Lampos"
        ],
        "abstract": "We provide a brief technical description of an online platform for disease monitoring, titled as the Flu Detector (",
        "submission_date": "2016-12-11T00:00:00",
        "last_modified_date": "2016-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03653",
        "title": "Learning to Drive using Inverse Reinforcement Learning and Deep Q-Networks",
        "authors": [
            "Sahand Sharifzadeh",
            "Ioannis Chiotellis",
            "Rudolph Triebel",
            "Daniel Cremers"
        ],
        "abstract": "We propose an inverse reinforcement learning (IRL) approach using Deep Q-Networks to extract the rewards in problems with large state spaces. We evaluate the performance of this approach in a simulation-based autonomous driving scenario. Our results resemble the intuitive relation between the reward function and readings of distance sensors mounted at different poses on the car. We also show that, after a few learning rounds, our simulated agent generates collision-free motions and performs human-like lane change behaviour.\n    ",
        "submission_date": "2016-12-12T00:00:00",
        "last_modified_date": "2017-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03780",
        "title": "Online Reinforcement Learning for Real-Time Exploration in Continuous State and Action Markov Decision Processes",
        "authors": [
            "Ludovic Hofer",
            "Hugo Gimbert"
        ],
        "abstract": "This paper presents a new method to learn online policies in continuous state, continuous action, model-free Markov decision processes, with two properties that are crucial for practical applications. First, the policies are implementable with a very low computational cost: once the policy is computed, the action corresponding to a given state is obtained in logarithmic time with respect to the number of samples used. Second, our method is versatile: it does not rely on any a priori knowledge of the structure of optimal policies. We build upon the Fitted Q-iteration algorithm which represents the $Q$-value as the average of several regression trees. Our algorithm, the Fitted Policy Forest algorithm (FPF), computes a regression forest representing the Q-value and transforms it into a single tree representing the policy, while keeping control on the size of the policy using resampling and leaf merging. We introduce an adaptation of Multi-Resolution Exploration (MRE) which is particularly suited to FPF. We assess the performance of FPF on three classical benchmarks for reinforcement learning: the \"Inverted Pendulum\", the \"Double Integrator\" and \"Car on the Hill\" and show that FPF equals or outperforms other algorithms, although these algorithms rely on the use of particular representations of the policies, especially chosen in order to fit each of the three problems. Finally, we exhibit that the combination of FPF and MRE allows to find nearly optimal solutions in problems where $\\epsilon$-greedy approaches would fail.\n    ",
        "submission_date": "2016-12-12T00:00:00",
        "last_modified_date": "2016-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03801",
        "title": "DeepMind Lab",
        "authors": [
            "Charles Beattie",
            "Joel Z. Leibo",
            "Denis Teplyashin",
            "Tom Ward",
            "Marcus Wainwright",
            "Heinrich K\u00fcttler",
            "Andrew Lefrancq",
            "Simon Green",
            "V\u00edctor Vald\u00e9s",
            "Amir Sadik",
            "Julian Schrittwieser",
            "Keith Anderson",
            "Sarah York",
            "Max Cant",
            "Adam Cain",
            "Adrian Bolton",
            "Stephen Gaffney",
            "Helen King",
            "Demis Hassabis",
            "Shane Legg",
            "Stig Petersen"
        ],
        "abstract": "DeepMind Lab is a first-person 3D game platform designed for research and development of general artificial intelligence and machine learning systems. DeepMind Lab can be used to study how autonomous artificial agents may learn complex tasks in large, partially observed, and visually diverse worlds. DeepMind Lab has a simple and flexible API enabling creative task-designs and novel AI-designs to be explored and quickly iterated upon. It is powered by a fast and widely recognised game engine, and tailored for effective use by the research community.\n    ",
        "submission_date": "2016-12-12T00:00:00",
        "last_modified_date": "2016-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03871",
        "title": "Knowledge Completion for Generics using Guided Tensor Factorization",
        "authors": [
            "Hanie Sedghi",
            "Ashish Sabharwal"
        ],
        "abstract": "Given a knowledge base or KB containing (noisy) facts about common nouns or generics, such as \"all trees produce oxygen\" or \"some animals live in forests\", we consider the problem of inferring additional such facts at a precision similar to that of the starting KB. Such KBs capture general knowledge about the world, and are crucial for various applications such as question answering. Different from commonly studied named entity KBs such as Freebase, generics KBs involve quantification, have more complex underlying regularities, tend to be more incomplete, and violate the commonly used locally closed world assumption (LCWA). We show that existing KB completion methods struggle with this new task, and present the first approach that is successful. Our results demonstrate that external information, such as relation schemas and entity taxonomies, if used appropriately, can be a surprisingly powerful tool in this setting. First, our simple yet effective knowledge guided tensor factorization approach achieves state-of-the-art results on two generics KBs (80% precise) for science, doubling their size at 74%-86% precision. Second, our novel taxonomy guided, submodular, active learning method for collecting annotations about rare entities (e.g., oriole, a bird) is 6x more effective at inferring further new facts about them than multiple active learning baselines.\n    ",
        "submission_date": "2016-12-12T00:00:00",
        "last_modified_date": "2018-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04469",
        "title": "Web-based Argumentation",
        "authors": [
            "Kenrick"
        ],
        "abstract": "Assumption-Based Argumentation (ABA) is an argumentation framework that has been proposed in the late 20th century. Since then, there was still no solver implemented in a programming language which is easy to setup and no solver have been interfaced to the web, which impedes the interests of the public. This project aims to implement an ABA solver in a modern programming language that performs reasonably well and interface it to the web for easier access by the public. This project has demonstrated the novelty of development of an ABA solver, that computes conflict-free, stable, admissible, grounded, ideal, and complete semantics, in Python programming language which can be used via an easy-to-use web interface for visualization of the argument and dispute trees. Experiments were conducted to determine the project's best configurations and to compare this project with proxdd, a state-of-the-art ABA solver, which has no web interface and computes less number of semantics. From the results of the experiments, this project's best configuration is achieved by utilizing \"pickle\" technique and tree caching technique. Using this project's best configuration, this project achieved a lower average runtime compared to proxdd. On other aspect, this project encountered more cases with exceptions compared to proxdd, which might be caused by this project computing more semantics and hence requires more resources to do so. Hence, it can be said that this project run comparably well to the state-of-the-art ABA solver proxdd. Future works of this project include computational complexity analysis and efficiency analysis of algorithms implemented, implementation of more semantics in argumentation framework, and usability testing of the web interface.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2016-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04687",
        "title": "Real-time interactive sequence generation and control with Recurrent Neural Network ensembles",
        "authors": [
            "Memo Akten",
            "Mick Grierson"
        ],
        "abstract": "Recurrent Neural Networks (RNN), particularly Long Short Term Memory (LSTM) RNNs, are a popular and very successful method for learning and generating sequences. However, current generative RNN techniques do not allow real-time interactive control of the sequence generation process, thus aren't well suited for live creative expression. We propose a method of real-time continuous control and 'steering' of sequence generation using an ensemble of RNNs and dynamically altering the mixture weights of the models. We demonstrate the method using character based LSTM networks and a gestural interface allowing users to 'conduct' the generation of text.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2017-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04759",
        "title": "Encapsulating models and approximate inference programs in probabilistic modules",
        "authors": [
            "Marco F. Cusumano-Towner",
            "Vikash K. Mansinghka"
        ],
        "abstract": "This paper introduces the probabilistic module interface, which allows encapsulation of complex probabilistic models with latent variables alongside custom stochastic approximate inference machinery, and provides a platform-agnostic abstraction barrier separating the model internals from the host probabilistic inference system. The interface can be seen as a stochastic generalization of a standard simulation and density interface for probabilistic primitives. We show that sound approximate inference algorithms can be constructed for networks of probabilistic modules, and we demonstrate that the interface can be implemented using learned stochastic inference networks and MCMC and SMC approximate inference programs.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2017-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04791",
        "title": "Scalable Computation of Optimized Queries for Sequential Diagnosis",
        "authors": [
            "Patrick Rodler",
            "Wolfgang Schmid",
            "Kostyantyn Shchekotykhin"
        ],
        "abstract": "In many model-based diagnosis applications it is impossible to provide such a set of observations and/or measurements that allow to identify the real cause of a fault. Therefore, diagnosis systems often return many possible candidates, leaving the burden of selecting the correct diagnosis to a user. Sequential diagnosis techniques solve this problem by automatically generating a sequence of queries to some oracle. The answers to these queries provide additional information necessary to gradually restrict the search space by removing diagnosis candidates inconsistent with the answers.\n",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2016-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04876",
        "title": "Collaborative creativity with Monte-Carlo Tree Search and Convolutional Neural Networks",
        "authors": [
            "Memo Akten",
            "Mick Grierson"
        ],
        "abstract": "We investigate a human-machine collaborative drawing environment in which an autonomous agent sketches images while optionally allowing a user to directly influence the agent's trajectory. We combine Monte Carlo Tree Search with image classifiers and test both shallow models (e.g. multinomial logistic regression) and deep Convolutional Neural Networks (e.g. LeNet, Inception v3). We found that using the shallow model, the agent produces a limited variety of images, which are noticably recogonisable by humans. However, using the deeper models, the agent produces a more diverse range of images, and while the agent remains very confident (99.99%) in having achieved its objective, to humans they mostly resemble unrecognisable 'random' noise. We relate this to recent research which also discovered that 'deep neural networks are easily fooled' \\cite{Nguyen2015} and we discuss possible solutions and future directions for the research.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2016-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04885",
        "title": "Crowdsourced Outcome Determination in Prediction Markets",
        "authors": [
            "Rupert Freeman",
            "Sebastien Lahaie",
            "David M. Pennock"
        ],
        "abstract": "A prediction market is a useful means of aggregating information about a future event. To function, the market needs a trusted entity who will verify the true outcome in the end. Motivated by the recent introduction of decentralized prediction markets, we introduce a mechanism that allows for the outcome to be determined by the votes of a group of arbiters who may themselves hold stakes in the market. Despite the potential conflict of interest, we derive conditions under which we can incentivize arbiters to vote truthfully by using funds raised from market fees to implement a peer prediction mechanism. Finally, we investigate what parameter values could be used in a real-world implementation of our mechanism.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2016-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05028",
        "title": "Ontohub: A semantic repository for heterogeneous ontologies",
        "authors": [
            "Mihai Codescu",
            "Eugen Kuksa",
            "Oliver Kutz",
            "Till Mossakowski",
            "Fabian Neuhaus"
        ],
        "abstract": "Ontohub is a repository engine for managing distributed heterogeneous ontologies. The distributed nature enables communities to share and exchange their contributions easily. The heterogeneous nature makes it possible to integrate ontologies written in various ontology languages. Ontohub supports a wide range of formal logical and ontology languages, as well as various structuring and modularity constructs and inter-theory (concept) mappings, building on the OMG-standardized DOL language. Ontohub repositories are organised as Git repositories, thus inheriting all features of this popular version control system. Moreover, Ontohub is the first repository engine meeting a substantial amount of the requirements formulated in the context of the Open Ontology Repository (OOR) initiative, including an API for federation as well as support for logical inference and axiom selection.\n    ",
        "submission_date": "2016-12-15T00:00:00",
        "last_modified_date": "2016-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05309",
        "title": "Multi-Agent Path Finding with Delay Probabilities",
        "authors": [
            "Hang Ma",
            "T. K. Satish Kumar",
            "Sven Koenig"
        ],
        "abstract": "Several recently developed Multi-Agent Path Finding (MAPF) solvers scale to large MAPF instances by searching for MAPF plans on 2 levels: The high-level search resolves collisions between agents, and the low-level search plans paths for single agents under the constraints imposed by the high-level search. We make the following contributions to solve the MAPF problem with imperfect plan execution with small average makespans: First, we formalize the MAPF Problem with Delay Probabilities (MAPF-DP), define valid MAPF-DP plans and propose the use of robust plan-execution policies for valid MAPF-DP plans to control how each agent proceeds along its path. Second, we discuss 2 classes of decentralized robust plan-execution policies (called Fully Synchronized Policies and Minimal Communication Policies) that prevent collisions during plan execution for valid MAPF-DP plans. Third, we present a 2-level MAPF-DP solver (called Approximate Minimization in Expectation) that generates valid MAPF-DP plans.\n    ",
        "submission_date": "2016-12-15T00:00:00",
        "last_modified_date": "2016-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05348",
        "title": "Machine Reading with Background Knowledge",
        "authors": [
            "Ndapandula Nakashole",
            "Tom M. Mitchell"
        ],
        "abstract": "Intelligent systems capable of automatically understanding natural language text are important for many artificial intelligence applications including mobile phone voice assistants, computer vision, and robotics. Understanding language often constitutes fitting new information into a previously acquired view of the world. However, many machine reading systems rely on the text alone to infer its meaning. In this paper, we pursue a different approach; machine reading methods that make use of background knowledge to facilitate language understanding. To this end, we have developed two methods: The first method addresses prepositional phrase attachment ambiguity. It uses background knowledge within a semi-supervised machine learning algorithm that learns from both labeled and unlabeled data. This approach yields state-of-the-art results on two datasets against strong baselines; The second method extracts relationships from compound nouns. Our knowledge-aware method for compound noun analysis accurately extracts relationships and significantly outperforms a baseline that does not make use of background knowledge.\n    ",
        "submission_date": "2016-12-16T00:00:00",
        "last_modified_date": "2016-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05497",
        "title": "A correlation coefficient of belief functions",
        "authors": [
            "Wen Jiang"
        ],
        "abstract": "How to manage conflict is still an open issue in Dempster-Shafer evidence theory. The correlation coefficient can be used to measure the similarity of evidence in Dempster-Shafer evidence theory. However, existing correlation coefficients of belief functions have some shortcomings. In this paper, a new correlation coefficient is proposed with many desirable properties. One of its applications is to measure the conflict degree among belief functions. Some numerical examples and comparisons demonstrate the effectiveness of the correlation coefficient.\n    ",
        "submission_date": "2016-12-16T00:00:00",
        "last_modified_date": "2017-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05628",
        "title": "An Alternative Softmax Operator for Reinforcement Learning",
        "authors": [
            "Kavosh Asadi",
            "Michael L. Littman"
        ],
        "abstract": "A softmax operator applied to a set of values acts somewhat like the maximization function and somewhat like an average. In sequential decision making, softmax is often used in settings where it is necessary to maximize utility but also to hedge against problems that arise from putting all of one's weight behind a single maximum utility decision. The Boltzmann softmax operator is the most commonly used softmax operator in this setting, but we show that this operator is prone to misbehavior. In this work, we study a differentiable softmax operator that, among other properties, is a non-expansion ensuring a convergent behavior in learning and planning. We introduce a variant of SARSA algorithm that, by utilizing the new operator, computes a Boltzmann policy with a state-dependent temperature parameter. We show that the algorithm is convergent and that it performs favorably in practice.\n    ",
        "submission_date": "2016-12-16T00:00:00",
        "last_modified_date": "2017-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05693",
        "title": "Optimal Target Assignment and Path Finding for Teams of Agents",
        "authors": [
            "Hang Ma",
            "Sven Koenig"
        ],
        "abstract": "We study the TAPF (combined target-assignment and path-finding) problem for teams of agents in known terrain, which generalizes both the anonymous and non-anonymous multi-agent path-finding problems. Each of the teams is given the same number of targets as there are agents in the team. Each agent has to move to exactly one target given to its team such that all targets are visited. The TAPF problem is to first assign agents to targets and then plan collision-free paths for the agents to their targets in a way such that the makespan is minimized. We present the CBM (Conflict-Based Min-Cost-Flow) algorithm, a hierarchical algorithm that solves TAPF instances optimally by combining ideas from anonymous and non-anonymous multi-agent path-finding algorithms. On the low level, CBM uses a min-cost max-flow algorithm on a time-expanded network to assign all agents in a single team to targets and plan their paths. On the high level, CBM uses conflict-based search to resolve collisions among agents in different teams. Theoretically, we prove that CBM is correct, complete and optimal. Experimentally, we show the scalability of CBM to TAPF instances with dozens of teams and hundreds of agents and adapt it to a simulated warehouse system.\n    ",
        "submission_date": "2016-12-17T00:00:00",
        "last_modified_date": "2016-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06000",
        "title": "Sample-efficient Deep Reinforcement Learning for Dialog Control",
        "authors": [
            "Kavosh Asadi",
            "Jason D. Williams"
        ],
        "abstract": "Representing a dialog policy as a recurrent neural network (RNN) is attractive because it handles partial observability, infers a latent representation of state, and can be optimized with supervised learning (SL) or reinforcement learning (RL). For RL, a policy gradient approach is natural, but is sample inefficient. In this paper, we present 3 methods for reducing the number of dialogs required to optimize an RNN-based dialog policy with RL. The key idea is to maintain a second RNN which predicts the value of the current policy, and to apply experience replay to both networks. On two tasks, these methods reduce the number of dialogs/episodes required by about a third, vs. standard policy gradient methods.\n    ",
        "submission_date": "2016-12-18T00:00:00",
        "last_modified_date": "2016-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06007",
        "title": "A Hidden Absorbing Semi-Markov Model for Informatively Censored Temporal Data: Learning and Inference",
        "authors": [
            "Ahmed M. Alaa",
            "Mihaela van der Schaar"
        ],
        "abstract": "Modeling continuous-time physiological processes that manifest a patient's evolving clinical states is a key step in approaching many problems in healthcare. In this paper, we develop the Hidden Absorbing Semi-Markov Model (HASMM): a versatile probabilistic model that is capable of capturing the modern electronic health record (EHR) data. Unlike exist- ing models, an HASMM accommodates irregularly sampled, temporally correlated, and informatively censored physiological data, and can describe non-stationary clinical state transitions. Learning an HASMM from the EHR data is achieved via a novel forward- filtering backward-sampling Monte-Carlo EM algorithm that exploits the knowledge of the end-point clinical outcomes (informative censoring) in the EHR data, and implements the E-step by sequentially sampling the patients' clinical states in the reverse-time direction while conditioning on the future states. Real-time inferences are drawn via a forward- filtering algorithm that operates on a virtually constructed discrete-time embedded Markov chain that mirrors the patient's continuous-time state trajectory. We demonstrate the di- agnostic and prognostic utility of the HASMM in a critical care prognosis setting using a real-world dataset for patients admitted to the Ronald Reagan UCLA Medical Center.\n    ",
        "submission_date": "2016-12-18T00:00:00",
        "last_modified_date": "2016-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06038",
        "title": "Context and Interference Effects in the Combinations of Natural Concepts",
        "authors": [
            "Diederik Aerts",
            "Jonito Aerts Argu\u00eblles",
            "Lester Beltran",
            "Lyneth Beltran",
            "Massimiliano Sassoli de Bianchi",
            "Sandro Sozzo",
            "Tomas Veloz"
        ],
        "abstract": "The mathematical formalism of quantum theory exhibits significant effectiveness when applied to cognitive phenomena that have resisted traditional (set theoretical) modeling. Relying on a decade of research on the operational foundations of micro-physical and conceptual entities, we present a theoretical framework for the representation of concepts and their conjunctions and disjunctions that uses the quantum formalism. This framework provides a unified solution to the 'conceptual combinations problem' of cognitive psychology, explaining the observed deviations from classical (Boolean, fuzzy set and Kolmogorovian) structures in terms of genuine quantum effects. In particular, natural concepts 'interfere' when they combine to form more complex conceptual entities, and they also exhibit a 'quantum-type context-dependence', which are responsible of the 'over- and under-extension' that are systematically observed in experiments on membership judgments.\n    ",
        "submission_date": "2016-12-19T00:00:00",
        "last_modified_date": "2016-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06528",
        "title": "Neuro-symbolic EDA-based Optimisation using ILP-enhanced DBNs",
        "authors": [
            "Sarmimala Saikia",
            "Lovekesh Vig",
            "Ashwin Srinivasan",
            "Gautam Shroff",
            "Puneet Agarwal",
            "Richa Rawat"
        ],
        "abstract": "We investigate solving discrete optimisation problems using the estimation of distribution (EDA) approach via a novel combination of deep belief networks(DBN) and inductive logic programming (ILP).While DBNs are used to learn the structure of successively better feasible solutions,ILP enables the incorporation of domain-based background knowledge related to the goodness of ",
        "submission_date": "2016-12-20T00:00:00",
        "last_modified_date": "2016-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06589",
        "title": "A Latent-class Model for Estimating Product-choice Probabilities from Clickstream Data",
        "authors": [
            "Naoki Nishimura",
            "Noriyoshi Sukegawa",
            "Yuichi Takano",
            "Jiro Iwanaga"
        ],
        "abstract": "This paper analyzes customer product-choice behavior based on the recency and frequency of each customer's page views on e-commerce sites. Recently, we devised an optimization model for estimating product-choice probabilities that satisfy monotonicity, convexity, and concavity constraints with respect to recency and frequency. This shape-restricted model delivered high predictive performance even when there were few training samples. However, typical e-commerce sites deal in many different varieties of products, so the predictive performance of the model can be further improved by integration of such product heterogeneity. For this purpose, we develop a novel latent-class shape-restricted model for estimating product-choice probabilities for each latent class of products. We also give a tailored expectation-maximization algorithm for parameter estimation. Computational results demonstrate that higher predictive performance is achieved with our latent-class model than with the previous shape-restricted model and common latent-class logistic regression.\n    ",
        "submission_date": "2016-12-20T00:00:00",
        "last_modified_date": "2016-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06915",
        "title": "AIVAT: A New Variance Reduction Technique for Agent Evaluation in Imperfect Information Games",
        "authors": [
            "Neil Burch",
            "Martin Schmid",
            "Matej Morav\u010d\u00edk",
            "Michael Bowling"
        ],
        "abstract": "Evaluating agent performance when outcomes are stochastic and agents use randomized strategies can be challenging when there is limited data available. The variance of sampled outcomes may make the simple approach of Monte Carlo sampling inadequate. This is the case for agents playing heads-up no-limit Texas hold'em poker, where man-machine competitions have involved multiple days of consistent play and still not resulted in statistically significant conclusions even when the winner's margin is substantial. In this paper, we introduce AIVAT, a low variance, provably unbiased value assessment tool that uses an arbitrary heuristic estimate of state value, as well as the explicit strategy of a subset of the agents. Unlike existing techniques which reduce the variance from chance events, or only consider game ending actions, AIVAT reduces the variance both from choices by nature and by players with a known strategy. The resulting estimator in no-limit poker can reduce the number of hands needed to draw statistical conclusions by more than a factor of 10.\n    ",
        "submission_date": "2016-12-20T00:00:00",
        "last_modified_date": "2017-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07059",
        "title": "ARES: Adaptive Receding-Horizon Synthesis of Optimal Plans",
        "authors": [
            "Anna Lukina",
            "Lukas Esterle",
            "Christian Hirsch",
            "Ezio Bartocci",
            "Junxing Yang",
            "Ashish Tiwari",
            "Scott A. Smolka",
            "Radu Grosu"
        ],
        "abstract": "We introduce ARES, an efficient approximation algorithm for generating optimal plans (action sequences) that take an initial state of a Markov Decision Process (MDP) to a state whose cost is below a specified (convergence) threshold. ARES uses Particle Swarm Optimization, with adaptive sizing for both the receding horizon and the particle swarm. Inspired by Importance Splitting, the length of the horizon and the number of particles are chosen such that at least one particle reaches a next-level state, that is, a state where the cost decreases by a required delta from the previous-level state. The level relation on states and the plans constructed by ARES implicitly define a Lyapunov function and an optimal policy, respectively, both of which could be explicitly generated by applying ARES to all states of the MDP, up to some topological equivalence relation. We also assess the effectiveness of ARES by statistically evaluating its rate of success in generating optimal plans. The ARES algorithm resulted from our desire to clarify if flying in V-formation is a flocking policy that optimizes energy conservation, clear view, and velocity alignment. That is, we were interested to see if one could find optimal plans that bring a flock from an arbitrary initial state to a state exhibiting a single connected V-formation. For flocks with 7 birds, ARES is able to generate a plan that leads to a V-formation in 95% of the 8,000 random initial configurations within 63 seconds, on average. ARES can also be easily customized into a model-predictive controller (MPC) with an adaptive receding horizon and statistical guarantees of convergence. To the best of our knowledge, our adaptive-sizing approach is the first to provide convergence guarantees in receding-horizon techniques.\n    ",
        "submission_date": "2016-12-21T00:00:00",
        "last_modified_date": "2016-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07548",
        "title": "Non-Deterministic Policy Improvement Stabilizes Approximated Reinforcement Learning",
        "authors": [
            "Wendelin B\u00f6hmer",
            "Rong Guo",
            "Klaus Obermayer"
        ],
        "abstract": "This paper investigates a type of instability that is linked to the greedy policy improvement in approximated reinforcement learning. We show empirically that non-deterministic policy improvement can stabilize methods like LSPI by controlling the improvements' stochasticity. Additionally we show that a suitable representation of the value function also stabilizes the solution to some degree. The presented approach is simple and should also be easily transferable to more sophisticated algorithms like deep reinforcement learning.\n    ",
        "submission_date": "2016-12-22T00:00:00",
        "last_modified_date": "2016-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07555",
        "title": "The SP Theory of Intelligence as a Foundation for the Development of a General, Human-Level Thinking Machine",
        "authors": [
            "J Gerard Wolff"
        ],
        "abstract": "This paper summarises how the \"SP theory of intelligence\" and its realisation in the \"SP computer model\" simplifies and integrates concepts across artificial intelligence and related areas, and thus provides a promising foundation for the development of a general, human-level thinking machine, in accordance with the main goal of research in artificial general intelligence.\n",
        "submission_date": "2016-12-22T00:00:00",
        "last_modified_date": "2016-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07589",
        "title": "Solving Set Optimization Problems by Cardinality Optimization via Weak Constraints with an Application to Argumentation",
        "authors": [
            "Wolfgang Faber",
            "Mauro Vallati",
            "Federico Cerutti",
            "Massimiliano Giacomin"
        ],
        "abstract": "Optimization - minimization or maximization - in the lattice of subsets is a frequent operation in Artificial Intelligence tasks. Examples are subset-minimal model-based diagnosis, nonmonotonic reasoning by means of circumscription, or preferred extensions in abstract argumentation. Finding the optimum among many admissible solutions is often harder than finding admissible solutions with respect to both computational complexity and methodology. This paper addresses the former issue by means of an effective method for finding subset-optimal solutions. It is based on the relationship between cardinality-optimal and subset-optimal solutions, and the fact that many logic-based declarative programming systems provide constructs for finding cardinality-optimal solutions, for example maximum satisfiability (MaxSAT) or weak constraints in Answer Set Programming (ASP). Clearly each cardinality-optimal solution is also a subset-optimal one, and if the language also allows for the addition of particular restricting constructs (both MaxSAT and ASP do) then all subset-optimal solutions can be found by an iterative computation of cardinality-optimal solutions. As a showcase, the computation of preferred extensions of abstract argumentation frameworks using the proposed method is studied.\n    ",
        "submission_date": "2016-12-22T00:00:00",
        "last_modified_date": "2016-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07602",
        "title": "Jointly Extracting Relations with Class Ties via Effective Deep Ranking",
        "authors": [
            "Hai Ye",
            "Wenhan Chao",
            "Zhunchen Luo",
            "Zhoujun Li"
        ],
        "abstract": "Connections between relations in relation extraction, which we call class ties, are common. In distantly supervised scenario, one entity tuple may have multiple relation facts. Exploiting class ties between relations of one entity tuple will be promising for distantly supervised relation extraction. However, previous models are not effective or ignore to model this property. In this work, to effectively leverage class ties, we propose to make joint relation extraction with a unified model that integrates convolutional neural network (CNN) with a general pairwise ranking framework, in which three novel ranking loss functions are introduced. Additionally, an effective method is presented to relieve the severe class imbalance problem from NR (not relation) for model training. Experiments on a widely used dataset show that leveraging class ties will enhance extraction and demonstrate the effectiveness of our model to learn class ties. Our model outperforms the baselines significantly, achieving state-of-the-art performance.\n    ",
        "submission_date": "2016-12-22T00:00:00",
        "last_modified_date": "2017-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07896",
        "title": "A Base Camp for Scaling AI",
        "authors": [
            "C.J.C. Burges",
            "T. Hart",
            "Z. Yang",
            "S. Cucerzan",
            "R.W. White",
            "A. Pastusiak",
            "J. Lewis"
        ],
        "abstract": "Modern statistical machine learning (SML) methods share a major limitation with the early approaches to AI: there is no scalable way to adapt them to new domains. Human learning solves this in part by leveraging a rich, shared, updateable world model. Such scalability requires modularity: updating part of the world model should not impact unrelated parts. We have argued that such modularity will require both \"correctability\" (so that errors can be corrected without introducing new errors) and \"interpretability\" (so that we can understand what components need correcting).\n",
        "submission_date": "2016-12-23T00:00:00",
        "last_modified_date": "2016-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08109",
        "title": "Solving Combinatorial Optimization problems with Quantum inspired Evolutionary Algorithm Tuned using a Novel Heuristic Method",
        "authors": [
            "Nija Mani",
            "Gursaran",
            "Ashish Mani"
        ],
        "abstract": "Quantum inspired Evolutionary Algorithms were proposed more than a decade ago and have been employed for solving a wide range of difficult search and optimization problems. A number of changes have been proposed to improve performance of canonical QEA. However, canonical QEA is one of the few evolutionary algorithms, which uses a search operator with relatively large number of parameters. It is well known that performance of evolutionary algorithms is dependent on specific value of parameters for a given problem. The advantage of having large number of parameters in an operator is that the search process can be made more powerful even with a single operator without requiring a combination of other operators for exploration and exploitation. However, the tuning of operators with large number of parameters is complex and computationally expensive. This paper proposes a novel heuristic method for tuning parameters of canonical QEA. The tuned QEA outperforms canonical QEA on a class of discrete combinatorial optimization problems which, validates the design of the proposed parameter tuning framework. The proposed framework can be used for tuning other algorithms with both large and small number of tunable parameters.\n    ",
        "submission_date": "2016-12-23T00:00:00",
        "last_modified_date": "2019-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08555",
        "title": "Monte Carlo Sort for unreliable human comparisons",
        "authors": [
            "Samuel L Smith"
        ],
        "abstract": "Algorithms which sort lists of real numbers into ascending order have been studied for decades. They are typically based on a series of pairwise comparisons and run entirely on chip. However people routinely sort lists which depend on subjective or complex judgements that cannot be automated. Examples include marketing research; where surveys are used to learn about customer preferences for products, the recruiting process; where interviewers attempt to rank potential employees, and sporting tournaments; where we infer team rankings from a series of one on one matches. We develop a novel sorting algorithm, where each pairwise comparison reflects a subjective human judgement about which element is bigger or better. We introduce a finite and large error rate to each judgement, and we take the cost of each comparison to significantly exceed the cost of other computational steps. The algorithm must request the most informative sequence of comparisons from the user; in order to identify the correct sorted list with minimum human input. Our Discrete Adiabatic Monte Carlo approach exploits the gradual acquisition of information by tracking a set of plausible hypotheses which are updated after each additional comparison.\n    ",
        "submission_date": "2016-12-27T00:00:00",
        "last_modified_date": "2016-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08633",
        "title": "A Sparse Nonlinear Classifier Design Using AUC Optimization",
        "authors": [
            "Vishal Kakkar",
            "Shirish K. Shevade",
            "S Sundararajan",
            "Dinesh Garg"
        ],
        "abstract": "AUC (Area under the ROC curve) is an important performance measure for applications where the data is highly imbalanced. Learning to maximize AUC performance is thus an important research problem. Using a max-margin based surrogate loss function, AUC optimization problem can be approximated as a pairwise rankSVM learning problem. Batch learning methods for solving the kernelized version of this problem suffer from scalability and may not result in sparse classifiers. Recent years have witnessed an increased interest in the development of online or single-pass online learning algorithms that design a classifier by maximizing the AUC performance. The AUC performance of nonlinear classifiers, designed using online methods, is not comparable with that of nonlinear classifiers designed using batch learning algorithms on many real-world datasets. Motivated by these observations, we design a scalable algorithm for maximizing AUC performance by greedily adding the required number of basis functions into the classifier model. The resulting sparse classifiers perform faster inference. Our experimental results show that the level of sparsity achievable can be order of magnitude smaller than the Kernel RankSVM model without affecting the AUC performance much.\n    ",
        "submission_date": "2016-12-27T00:00:00",
        "last_modified_date": "2016-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08657",
        "title": "Role of Simplicity in Creative Behaviour: The Case of the Poietic Generator",
        "authors": [
            "Antoine Saillenfest",
            "Jean-Louis Dessalles",
            "Olivier Auber"
        ],
        "abstract": "We propose to apply Simplicity Theory (ST) to model interest in creative situations. ST has been designed to describe and predict interest in communication. Here we use ST to derive a decision rule that we apply to a simplified version of a creative game, the Poietic Generator. The decision rule produces what can be regarded as an elementary form of creativity. This study is meant as a proof of principle. It suggests that some creative actions may be motivated by the search for unexpected simplicity.\n    ",
        "submission_date": "2016-12-22T00:00:00",
        "last_modified_date": "2016-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08777",
        "title": "Automated timetabling for small colleges and high schools using huge integer programs",
        "authors": [
            "Joshua S. Friedman"
        ],
        "abstract": "We formulate an integer program to solve a highly constrained academic timetabling problem at the United States Merchant Marine Academy. The IP instance that results from our real case study has approximately both 170,000 rows and columns and solves to optimality in 4--24 hours using a commercial solver on a portable computer (near optimal feasible solutions were often found in 4--12 hours). Our model is applicable to both high schools and small colleges who wish to deviate from group scheduling. We also solve a necessary preprocessing student subgrouping problem, which breaks up big groups of students into small groups so they can optimally fit into small capacity classes.\n    ",
        "submission_date": "2016-12-28T00:00:00",
        "last_modified_date": "2017-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08967",
        "title": "Efficient iterative policy optimization",
        "authors": [
            "Nicolas Le Roux"
        ],
        "abstract": "We tackle the issue of finding a good policy when the number of policy updates is limited. This is done by approximating the expected policy reward as a sequence of concave lower bounds which can be efficiently maximized, drastically reducing the number of policy updates required to achieve good performance. We also extend existing methods to negative rewards, enabling the use of control variates.\n    ",
        "submission_date": "2016-12-28T00:00:00",
        "last_modified_date": "2016-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.09212",
        "title": "A hybrid approach to supervised machine learning for algorithmic melody composition",
        "authors": [
            "Rouven Bauer"
        ],
        "abstract": "In this work we present an algorithm for composing monophonic melodies similar in style to those of a given, phrase annotated, sample of melodies. For implementation, a hybrid approach incorporating parametric Markov models of higher order and a contour concept of phrases is used. This work is based on the master thesis of Thayabaran Kathiresan (2015). An online listening test conducted shows that enhancing a pure Markov model with musically relevant context, like count and planed melody contour, improves the result significantly.\n    ",
        "submission_date": "2016-12-29T00:00:00",
        "last_modified_date": "2016-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.09433",
        "title": "Curiosity-Aware Bargaining",
        "authors": [
            "C\u00e9dric Buron",
            "Sylvain Ductor",
            "Zahia Guessoum"
        ],
        "abstract": "Opponent modeling consists in modeling the strategy or preferences of an agent thanks to the data it provides. In the context of automated negotiation and with machine learning, it can result in an advantage so overwhelming that it may restrain some casual agents to be part of the bargaining process. We qualify as \"curious\" an agent driven by the desire of negotiating in order to collect information and improve its opponent model. However, neither curiosity-based rational-ity nor curiosity-robust protocol have been studied in automatic negotiation. In this paper, we rely on mechanism design to propose three extensions of the standard bargaining protocol that limit information leak. Those extensions are supported by an enhanced rationality model, that considers the exchanged information. Also, they are theoretically analyzed and experimentally evaluated.\n    ",
        "submission_date": "2016-12-30T00:00:00",
        "last_modified_date": "2016-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.09591",
        "title": "PrASP Report",
        "authors": [
            "Matthias Nickles"
        ],
        "abstract": "This technical report describes the usage, syntax, semantics and core algorithms of the probabilistic inductive logic programming framework PrASP. PrASP is a research software which integrates non-monotonic reasoning based on Answer Set Programming (ASP), probabilistic inference and parameter learning. In contrast to traditional approaches to Probabilistic (Inductive) Logic Programming, our framework imposes only little restrictions on probabilistic logic programs. In particular, PrASP allows for ASP as well as First-Order Logic syntax, and for the annotation of formulas with point probabilities as well as interval probabilities. A range of widely configurable inference algorithms can be combined in a pipeline-like fashion, in order to cover a variety of use cases.\n    ",
        "submission_date": "2016-12-30T00:00:00",
        "last_modified_date": "2016-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.09593",
        "title": "Fuzzy Constraints Linear Discriminant Analysis",
        "authors": [
            "Hamid Reza Hassanzadeh",
            "Hadi Sadoghi Yazdi",
            "Abedin Vahedian"
        ],
        "abstract": "In this paper we introduce a fuzzy constraint linear discriminant analysis (FC-LDA). The FC-LDA tries to minimize misclassification error based on modified perceptron criterion that benefits handling the uncertainty near the decision boundary by means of a fuzzy linear programming approach with fuzzy resources. The method proposed has low computational complexity because of its linear characteristics and the ability to deal with noisy data with different degrees of tolerance. Obtained results verify the success of the algorithm when dealing with different problems. Comparing FC-LDA and LDA shows superiority in classification task.\n    ",
        "submission_date": "2016-12-30T00:00:00",
        "last_modified_date": "2016-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00027",
        "title": "Computational Pathology: Challenges and Promises for Tissue Analysis",
        "authors": [
            "Thomas J. Fuchs",
            "Joachim M. Buhmann"
        ],
        "abstract": "The histological assessment of human tissue has emerged as the key challenge for detection and treatment of cancer. A plethora of different data sources ranging from tissue microarray data to gene expression, proteomics or metabolomics data provide a detailed overview of the health status of a patient. Medical doctors need to assess these information sources and they rely on data driven automatic analysis tools. Methods for classification, grouping and segmentation of heterogeneous data sources as well as regression of noisy dependencies and estimation of survival probabilities enter the processing workflow of a pathology diagnosis system at various stages. This paper reports on state-of-the-art of the design and effectiveness of computational pathology workflows and it discusses future research directions in this emergent field of medical informatics and diagnostic machine learning.\n    ",
        "submission_date": "2015-12-31T00:00:00",
        "last_modified_date": "2015-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00287",
        "title": "Wavelet Scattering on the Pitch Spiral",
        "authors": [
            "Vincent Lostanlen",
            "St\u00e9phane Mallat"
        ],
        "abstract": "We present a new representation of harmonic sounds that linearizes the dynamics of pitch and spectral envelope, while remaining stable to deformations in the time-frequency plane. It is an instance of the scattering transform, a generic operator which cascades wavelet convolutions and modulus nonlinearities. It is derived from the pitch spiral, in that convolutions are successively performed in time, log-frequency, and octave index. We give a closed-form approximation of spiral scattering coefficients for a nonstationary generalization of the harmonic source-filter model.\n    ",
        "submission_date": "2016-01-03T00:00:00",
        "last_modified_date": "2016-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00318",
        "title": "A Unified Approach for Learning the Parameters of Sum-Product Networks",
        "authors": [
            "Han Zhao",
            "Pascal Poupart",
            "Geoff Gordon"
        ],
        "abstract": "We present a unified approach for learning the parameters of Sum-Product networks (SPNs). We prove that any complete and decomposable SPN is equivalent to a mixture of trees where each tree corresponds to a product of univariate distributions. Based on the mixture model perspective, we characterize the objective function when learning SPNs based on the maximum likelihood estimation (MLE) principle and show that the optimization problem can be formulated as a signomial program. We construct two parameter learning algorithms for SPNs by using sequential monomial approximations (SMA) and the concave-convex procedure (CCCP), respectively. The two proposed methods naturally admit multiplicative updates, hence effectively avoiding the projection operation. With the help of the unified framework, we also show that, in the case of SPNs, CCCP leads to the same algorithm as Expectation Maximization (EM) despite the fact that they are different in general.\n    ",
        "submission_date": "2016-01-03T00:00:00",
        "last_modified_date": "2016-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00372",
        "title": "Mutual Information and Diverse Decoding Improve Neural Machine Translation",
        "authors": [
            "Jiwei Li",
            "Dan Jurafsky"
        ],
        "abstract": "Sequence-to-sequence neural translation models learn semantic and syntactic relations between sentence pairs by optimizing the likelihood of the target given the source, i.e., $p(y|x)$, an objective that ignores other potentially useful sources of information. We introduce an alternative objective function for neural MT that maximizes the mutual information between the source and target sentences, modeling the bi-directional dependency of sources and targets. We implement the model with a simple re-ranking method, and also introduce a decoding algorithm that increases diversity in the N-best list produced by the first pass. Applied to the WMT German/English and French/English tasks, the proposed models offers a consistent performance boost on both standard LSTM and attention-based neural MT architectures.\n    ",
        "submission_date": "2016-01-04T00:00:00",
        "last_modified_date": "2016-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00706",
        "title": "Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis",
        "authors": [
            "Jimei Yang",
            "Scott Reed",
            "Ming-Hsuan Yang",
            "Honglak Lee"
        ],
        "abstract": "An important problem for both graphics and vision is to synthesize novel views of a 3D object from a single image. This is particularly challenging due to the partial observability inherent in projecting a 3D object onto the image space, and the ill-posedness of inferring object shape and pose. However, we can train a neural network to address the problem if we restrict our attention to specific object categories (in our case faces and chairs) for which we can gather ample training data. In this paper, we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image. The recurrent structure allows our model to capture long-term dependencies along a sequence of transformations. We demonstrate the quality of its predictions for human faces on the Multi-PIE dataset and for a dataset of 3D chair models, and also show its ability to disentangle latent factors of variation (e.g., identity and pose) without using full supervision.\n    ",
        "submission_date": "2016-01-05T00:00:00",
        "last_modified_date": "2016-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00720",
        "title": "How do neurons operate on sparse distributed representations? A mathematical theory of sparsity, neurons and active dendrites",
        "authors": [
            "Subutai Ahmad",
            "Jeff Hawkins"
        ],
        "abstract": "We propose a formal mathematical model for sparse representations and active dendrites in neocortex. Our model is inspired by recent experimental findings on active dendritic processing and NMDA spikes in pyramidal neurons. These experimental and modeling studies suggest that the basic unit of pattern memory in the neocortex is instantiated by small clusters of synapses operated on by localized non-linear dendritic processes. We derive a number of scaling laws that characterize the accuracy of such dendrites in detecting activation patterns in a neuronal population under adverse conditions. We introduce the union property which shows that synapses for multiple patterns can be randomly mixed together within a segment and still lead to highly accurate recognition. We describe simulation results that provide further insight into sparse representations as well as two primary results. First we show that pattern recognition by a neuron with active dendrites can be extremely accurate and robust with high dimensional sparse inputs even when using a tiny number of synapses to recognize large patterns. Second, equations representing recognition accuracy of a dendrite predict optimal NMDA spiking thresholds under a generous set of assumptions. The prediction tightly matches NMDA spiking thresholds measured in the literature. Our model matches many of the known properties of pyramidal neurons. As such the theory provides a mathematical framework for understanding the benefits and limits of sparse representations in cortical networks.\n    ",
        "submission_date": "2016-01-05T00:00:00",
        "last_modified_date": "2016-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00738",
        "title": "Resource Sharing for Multi-Tenant NoSQL Data Store in Cloud",
        "authors": [
            "Jiaan Zeng"
        ],
        "abstract": "Multi-tenancy hosting of users in cloud NoSQL data stores is favored by cloud providers because it enables resource sharing at low operating cost. Multi-tenancy takes several forms depending on whether the back-end file system is a local file system (LFS) or a parallel file system (PFS), and on whether tenants are independent or share data across tenants. In this thesis I focus on and propose solutions to two cases: independent data-local file system, and shared data-parallel file system.\n",
        "submission_date": "2016-01-05T00:00:00",
        "last_modified_date": "2016-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00741",
        "title": "Learning Preferences for Manipulation Tasks from Online Coactive Feedback",
        "authors": [
            "Ashesh Jain",
            "Shikhar Sharma",
            "Thorsten Joachims",
            "Ashutosh Saxena"
        ],
        "abstract": "We consider the problem of learning preferences over trajectories for mobile manipulators such as personal robots and assembly line robots. The preferences we learn are more intricate than simple geometric constraints on trajectories; they are rather governed by the surrounding context of various objects and human interactions in the environment. We propose a coactive online learning framework for teaching preferences in contextually rich environments. The key novelty of our approach lies in the type of feedback expected from the user: the human user does not need to demonstrate optimal trajectories as training data, but merely needs to iteratively provide trajectories that slightly improve over the trajectory currently proposed by the system. We argue that this coactive preference feedback can be more easily elicited than demonstrations of optimal trajectories. Nevertheless, theoretical regret bounds of our algorithm match the asymptotic rates of optimal trajectory algorithms.\n",
        "submission_date": "2016-01-05T00:00:00",
        "last_modified_date": "2016-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01058",
        "title": "Wikiometrics: A Wikipedia Based Ranking System",
        "authors": [
            "Gilad Katz",
            "Lior Rokach"
        ],
        "abstract": "We present a new concept - Wikiometrics - the derivation of metrics and indicators from Wikipedia. Wikipedia provides an accurate representation of the real world due to its size, structure, editing policy and popularity. We demonstrate an innovative mining methodology, where different elements of Wikipedia - content, structure, editorial actions and reader reviews - are used to rank items in a manner which is by no means inferior to rankings produced by experts or other methods. We test our proposed method by applying it to two real-world ranking problems: top world universities and academic journals. Our proposed ranking methods were compared to leading and widely accepted benchmarks, and were found to be extremely correlative but with the advantage of the data being publically available.\n    ",
        "submission_date": "2016-01-06T00:00:00",
        "last_modified_date": "2016-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01228",
        "title": "Some Experimental Issues in Financial Fraud Detection: An Investigation",
        "authors": [
            "J. West",
            "Maumita Bhattacharya"
        ],
        "abstract": "Financial fraud detection is an important problem with a number of design aspects to consider. Issues such as algorithm selection and performance analysis will affect the perceived ability of proposed solutions, so for auditors and re-searchers to be able to sufficiently detect financial fraud it is necessary that these issues be thoroughly explored. In this paper we will revisit the key performance metrics used for financial fraud detection with a focus on credit card fraud, critiquing the prevailing ideas and offering our own understandings. There are many different performance metrics that have been employed in prior financial fraud detection research. We will analyse several of the popular metrics and compare their effectiveness at measuring the ability of detection mechanisms. We further investigated the performance of a range of computational intelligence techniques when applied to this problem domain, and explored the efficacy of several binary classification methods.\n    ",
        "submission_date": "2016-01-06T00:00:00",
        "last_modified_date": "2016-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01653",
        "title": "Large Collection of Diverse Gene Set Search Queries Recapitulate Known Protein-Protein Interactions and Gene-Gene Functional Associations",
        "authors": [
            "Avi Ma'ayan",
            "Neil R. Clark"
        ],
        "abstract": "Popular online enrichment analysis tools from the field of molecular systems biology provide users with the ability to submit their experimental results as gene sets for individual analysis. Such queries are kept private, and have never before been considered as a resource for integrative analysis. By harnessing gene set query submissions from thousands of users, we aim to discover biological knowledge beyond the scope of an individual study. In this work, we investigated a large collection of gene sets submitted to the tool Enrichr by thousands of users. Based on co-occurrence, we constructed a global gene-gene association network. We interpret this inferred network as providing a summary of the structure present in this crowdsourced gene set library, and show that this network recapitulates known protein-protein interactions and functional associations between genes. This finding implies that this network also offers predictive value. Furthermore, we visualize this gene-gene association network using a new edge-pruning algorithm that retains both the local and global structures of large-scale networks. Our ability to make predictions for currently unknown gene associations, that may not be captured by individual researchers and data sources, is a demonstration of the potential of harnessing collective knowledge from users of popular tools in the field of molecular systems biology.\n    ",
        "submission_date": "2016-01-07T00:00:00",
        "last_modified_date": "2016-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01700",
        "title": "A Predictive Model using the Markov Property",
        "authors": [
            "Robert A. Murphy"
        ],
        "abstract": "Given a data set of numerical values which are sampled from some unknown probability distribution, we will show how to check if the data set exhibits the Markov property and we will show how to use the Markov property to predict future values from the same distribution, with probability 1.\n    ",
        "submission_date": "2016-01-08T00:00:00",
        "last_modified_date": "2016-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01917",
        "title": "Toward a Robust Diversity-Based Model to Detect Changes of Context",
        "authors": [
            "Sylvain Castagnos",
            "Amaury L 'Huillier",
            "Anne Boyer"
        ],
        "abstract": "Being able to automatically and quickly understand the user context during a session is a main issue for recommender systems. As a first step toward achieving that goal, we propose a model that observes in real time the diversity brought by each item relatively to a short sequence of consultations, corresponding to the recent user history. Our model has a complexity in constant time, and is generic since it can apply to any type of items within an online service (e.g. profiles, products, music tracks) and any application domain (e-commerce, social network, music streaming), as long as we have partial item descriptions. The observation of the diversity level over time allows us to detect implicit changes. In the long term, we plan to characterize the context, i.e. to find common features among a contiguous sub-sequence of items between two changes of context determined by our model. This will allow us to make context-aware and privacy-preserving recommendations, to explain them to users. As this is an ongoing research, the first step consists here in studying the robustness of our model while detecting changes of context. In order to do so, we use a music corpus of 100 users and more than 210,000 consultations (number of songs played in the global history). We validate the relevancy of our detections by finding connections between changes of context and events, such as ends of session. Of course, these events are a subset of the possible changes of context, since there might be several contexts within a session. We altered the quality of our corpus in several manners, so as to test the performances of our model when confronted with sparsity and different types of items. The results show that our model is robust and constitutes a promising approach.\n    ",
        "submission_date": "2016-01-08T00:00:00",
        "last_modified_date": "2016-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.02197",
        "title": "Identifying Stable Patterns over Time for Emotion Recognition from EEG",
        "authors": [
            "Wei-Long Zheng",
            "Jia-Yi Zhu",
            "Bao-Liang Lu"
        ],
        "abstract": "In this paper, we investigate stable patterns of electroencephalogram (EEG) over time for emotion recognition using a machine learning approach. Up to now, various findings of activated patterns associated with different emotions have been reported. However, their stability over time has not been fully investigated yet. In this paper, we focus on identifying EEG stability in emotion recognition. To validate the efficiency of the machine learning algorithms used in this study, we systematically evaluate the performance of various popular feature extraction, feature selection, feature smoothing and pattern classification methods with the DEAP dataset and a newly developed dataset for this study. The experimental results indicate that stable patterns exhibit consistency across sessions; the lateral temporal areas activate more for positive emotion than negative one in beta and gamma bands; the neural patterns of neutral emotion have higher alpha responses at parietal and occipital sites; and for negative emotion, the neural patterns have significant higher delta responses at parietal and occipital sites and higher gamma responses at prefrontal sites. The performance of our emotion recognition system shows that the neural patterns are relatively stable within and between sessions.\n    ",
        "submission_date": "2016-01-10T00:00:00",
        "last_modified_date": "2016-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.02213",
        "title": "On Clustering Time Series Using Euclidean Distance and Pearson Correlation",
        "authors": [
            "Michael R. Berthold",
            "Frank H\u00f6ppner"
        ],
        "abstract": "For time series comparisons, it has often been observed that z-score normalized Euclidean distances far outperform the unnormalized variant. In this paper we show that a z-score normalized, squared Euclidean Distance is, in fact, equal to a distance based on Pearson Correlation. This has profound impact on many distance-based classification or clustering methods. In addition to this theoretically sound result we also show that the often used k-Means algorithm formally needs a mod ification to keep the interpretation as Pearson correlation strictly valid. Experimental results demonstrate that in many cases the standard k-Means algorithm generally produces the same results.\n    ",
        "submission_date": "2016-01-10T00:00:00",
        "last_modified_date": "2016-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.02327",
        "title": "A Synthetic Approach for Recommendation: Combining Ratings, Social Relations, and Reviews",
        "authors": [
            "Guang-Neng Hu",
            "Xin-Yu Dai",
            "Yunya Song",
            "Shu-Jian Huang",
            "Jia-Jun Chen"
        ],
        "abstract": "Recommender systems (RSs) provide an effective way of alleviating the information overload problem by selecting personalized choices. Online social networks and user-generated content provide diverse sources for recommendation beyond ratings, which present opportunities as well as challenges for traditional RSs. Although social matrix factorization (Social MF) can integrate ratings with social relations and topic matrix factorization can integrate ratings with item reviews, both of them ignore some useful information. In this paper, we investigate the effective data fusion by combining the two approaches, in two steps. First, we extend Social MF to exploit the graph structure of neighbors. Second, we propose a novel framework MR3 to jointly model these three types of information effectively for rating prediction by aligning latent factors and hidden topics. We achieve more accurate rating prediction on two real-life datasets. Furthermore, we measure the contribution of each data source to the proposed framework.\n    ",
        "submission_date": "2016-01-11T00:00:00",
        "last_modified_date": "2016-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.02543",
        "title": "Evaluating the Performance of a Speech Recognition based System",
        "authors": [
            "Vinod Kumar Pandey",
            "Sunil Kumar Kopparapu"
        ],
        "abstract": "Speech based solutions have taken center stage with growth in the services industry where there is a need to cater to a very large number of people from all strata of the society. While natural language speech interfaces are the talk in the research community, yet in practice, menu based speech solutions thrive. Typically in a menu based speech solution the user is required to respond by speaking from a closed set of words when prompted by the system. A sequence of human speech response to the IVR prompts results in the completion of a transaction. A transaction is deemed successful if the speech solution can correctly recognize all the spoken utterances of the user whenever prompted by the system. The usual mechanism to evaluate the performance of a speech solution is to do an extensive test of the system by putting it to actual people use and then evaluating the performance by analyzing the logs for successful transactions. This kind of evaluation could lead to dissatisfied test users especially if the performance of the system were to result in a poor transaction completion rate. To negate this the Wizard of Oz approach is adopted during evaluation of a speech system. Overall this kind of evaluations is an expensive proposition both in terms of time and cost. In this paper, we propose a method to evaluate the performance of a speech solution without actually putting it to people use. We first describe the methodology and then show experimentally that this can be used to identify the performance bottlenecks of the speech solution even before the system is actually used thus saving evaluation time and expenses.\n    ",
        "submission_date": "2016-01-11T00:00:00",
        "last_modified_date": "2016-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.02650",
        "title": "Inference rules for RDF(S) and OWL in N3Logic",
        "authors": [
            "Dominik Tomaszuk"
        ],
        "abstract": "This paper presents inference rules for Resource Description Framework (RDF), RDF Schema (RDFS) and Web Ontology Language (OWL). Our formalization is based on Notation 3 Logic, which extended RDF by logical symbols and created Semantic Web logic for deductive RDF graph stores. We also propose OWL-P that is a lightweight formalism of OWL and supports soft inferences by omitting complex language constructs.\n    ",
        "submission_date": "2016-01-11T00:00:00",
        "last_modified_date": "2016-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.02705",
        "title": "Robobarista: Learning to Manipulate Novel Objects via Deep Multimodal Embedding",
        "authors": [
            "Jaeyong Sung",
            "Seok Hyun Jin",
            "Ian Lenz",
            "Ashutosh Saxena"
        ],
        "abstract": "There is a large variety of objects and appliances in human environments, such as stoves, coffee dispensers, juice extractors, and so on. It is challenging for a roboticist to program a robot for each of these object types and for each of their instantiations. In this work, we present a novel approach to manipulation planning based on the idea that many household objects share similarly-operated object parts. We formulate the manipulation planning as a structured prediction problem and learn to transfer manipulation strategy across different objects by embedding point-cloud, natural language, and manipulation trajectory data into a shared embedding space using a deep neural network. In order to learn semantically meaningful spaces throughout our network, we introduce a method for pre-training its lower layers for multimodal feature embedding and a method for fine-tuning this embedding space using a loss-based margin. In order to collect a large number of manipulation demonstrations for different objects, we develop a new crowd-sourcing platform called Robobarista. We test our model on our dataset consisting of 116 objects and appliances with 249 parts along with 250 language instructions, for which there are 1225 crowd-sourced manipulation demonstrations. We further show that our robot with our model can even prepare a cup of a latte with appliances it has never seen before.\n    ",
        "submission_date": "2016-01-12T00:00:00",
        "last_modified_date": "2016-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.02939",
        "title": "The minimal hitting set generation problem: algorithms and computation",
        "authors": [
            "Andrew Gainer-Dewar",
            "Paola Vera-Licona"
        ],
        "abstract": "Finding inclusion-minimal \"hitting sets\" for a given collection of sets is a fundamental combinatorial problem with applications in domains as diverse as Boolean algebra, computational biology, and data mining. Much of the algorithmic literature focuses on the problem of *recognizing* the collection of minimal hitting sets; however, in many of the applications, it is more important to *generate* these hitting sets. We survey twenty algorithms from across a variety of domains, considering their history, classification, useful features, and computational performance on a variety of synthetic and real-world inputs. We also provide a suite of implementations of these algorithms with a ready-to-use, platform-agnostic interface based on Docker containers and the AlgoRun framework, so that interested computational scientists can easily perform similar tests with inputs from their own research areas on their own computers or through a convenient Web interface.\n    ",
        "submission_date": "2016-01-05T00:00:00",
        "last_modified_date": "2016-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.02975",
        "title": "Indicators of Good Student Performance in Moodle Activity Data",
        "authors": [
            "Ewa M\u0142ynarska",
            "Derek Greene",
            "P\u00e1draig Cunningham"
        ],
        "abstract": "In this paper we conduct an analysis of Moodle activity data focused on identifying early predictors of good student performance. The analysis shows that three relevant hypotheses are largely supported by the data. These hypotheses are: early submission is a good sign, a high level of activity is predictive of good results and evening activity is even better than daytime activity. We highlight some pathological examples where high levels of activity correlates with bad results.\n    ",
        "submission_date": "2016-01-12T00:00:00",
        "last_modified_date": "2016-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.03095",
        "title": "Submodular Optimization under Noise",
        "authors": [
            "Avinatan Hassidim",
            "Yaron Singer"
        ],
        "abstract": "We consider the problem of maximizing a monotone submodular function under noise. There has been a great deal of work on optimization of submodular functions under various constraints, resulting in algorithms that provide desirable approximation guarantees. In many applications, however, we do not have access to the submodular function we aim to optimize, but rather to some erroneous or noisy version of it. This raises the question of whether provable guarantees are obtainable in presence of error and noise. We provide initial answers, by focusing on the question of maximizing a monotone submodular function under a cardinality constraint when given access to a noisy oracle of the function. We show that:\n",
        "submission_date": "2016-01-12T00:00:00",
        "last_modified_date": "2016-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.03778",
        "title": "Trust from the past: Bayesian Personalized Ranking based Link Prediction in Knowledge Graphs",
        "authors": [
            "Baichuan Zhang",
            "Sutanay Choudhury",
            "Mohammad Al Hasan",
            "Xia Ning",
            "Khushbu Agarwal",
            "Sumit Purohit",
            "Paola Pesntez Cabrera"
        ],
        "abstract": "Link prediction, or predicting the likelihood of a link in a knowledge graph based on its existing state is a key research task. It differs from a traditional link prediction task in that the links in a knowledge graph are categorized into different predicates and the link prediction performance of different predicates in a knowledge graph generally varies widely. In this work, we propose a latent feature embedding based link prediction model which considers the prediction task for each predicate disjointly. To learn the model parameters it utilizes a Bayesian personalized ranking based optimization technique. Experimental results on large-scale knowledge bases such as YAGO2 show that our link prediction approach achieves substantially higher performance than several state-of-art approaches. We also show that for a given predicate the topological properties of the knowledge graph induced by the given predicate edges are key indicators of the link prediction performance of that predicate in the knowledge graph.\n    ",
        "submission_date": "2016-01-14T00:00:00",
        "last_modified_date": "2016-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.04037",
        "title": "Funnel Libraries for Real-Time Robust Feedback Motion Planning",
        "authors": [
            "Anirudha Majumdar",
            "Russ Tedrake"
        ],
        "abstract": "We consider the problem of generating motion plans for a robot that are guaranteed to succeed despite uncertainty in the environment, parametric model uncertainty, and disturbances. Furthermore, we consider scenarios where these plans must be generated in real-time, because constraints such as obstacles in the environment may not be known until they are perceived (with a noisy sensor) at runtime. Our approach is to pre-compute a library of \"funnels\" along different maneuvers of the system that the state is guaranteed to remain within (despite bounded disturbances) when the feedback controller corresponding to the maneuver is executed. We leverage powerful computational machinery from convex optimization (sums-of-squares programming in particular) to compute these funnels. The resulting funnel library is then used to sequentially compose motion plans at runtime while ensuring the safety of the robot. A major advantage of the work presented here is that by explicitly taking into account the effect of uncertainty, the robot can evaluate motion plans based on how vulnerable they are to disturbances.\n",
        "submission_date": "2016-01-15T00:00:00",
        "last_modified_date": "2017-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.04038",
        "title": "It's about time: Online Macrotask Sequencing in Expert Crowdsourcing",
        "authors": [
            "Heinz Schmitz",
            "Ioanna Lykourentzou"
        ],
        "abstract": "We introduce the problem of Task Assignment and Sequencing (TAS), which adds the timeline perspective to expert crowdsourcing optimization. Expert crowdsourcing involves macrotasks, like document writing, product design, or web development, which take more time than typical binary microtasks, require expert skills, assume varying degrees of knowledge over a topic, and require crowd workers to build on each other's contributions. Current works usually assume offline optimization models, which consider worker and task arrivals known and do not take into account the element of time. Realistically however, time is critical: tasks have deadlines, expert workers are available only at specific time slots, and worker/task arrivals are not known a-priori. Our work is the first to address the problem of optimal task sequencing for online, heterogeneous, time-constrained macrotasks. We propose tas-online, an online algorithm that aims to complete as many tasks as possible within budget, required quality and a given timeline, without future input information regarding job release dates or worker availabilities. Results, comparing tas-online to four typical benchmarks, show that it achieves more completed jobs, lower flow times and higher job quality. This work has practical implications for improving the Quality of Service of current crowdsourcing platforms, allowing them to offer cost, quality and time improvements for expert tasks.\n    ",
        "submission_date": "2016-01-15T00:00:00",
        "last_modified_date": "2016-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.04126",
        "title": "Engineering Safety in Machine Learning",
        "authors": [
            "Kush R. Varshney"
        ],
        "abstract": "Machine learning algorithms are increasingly influencing our decisions and interacting with us in all parts of our daily lives. Therefore, just like for power plants, highways, and myriad other engineered sociotechnical systems, we must consider the safety of systems involving machine learning. In this paper, we first discuss the definition of safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. Then we examine dimensions, such as the choice of cost function and the appropriateness of minimizing the empirical average training cost, along which certain real-world applications may not be completely amenable to the foundational principle of modern statistical machine learning: empirical risk minimization. In particular, we note an emerging dichotomy of applications: ones in which safety is important and risk minimization is not the complete story (we name these Type A applications), and ones in which safety is not so critical and risk minimization is sufficient (we name these Type B applications). Finally, we discuss how four different strategies for achieving safety in engineering (inherently safe design, safety reserves, safe fail, and procedural safeguards) can be mapped to the machine learning context through interpretability and causality of predictive models, objectives beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software.\n    ",
        "submission_date": "2016-01-16T00:00:00",
        "last_modified_date": "2016-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.04149",
        "title": "$\\mathbf{D^3}$: Deep Dual-Domain Based Fast Restoration of JPEG-Compressed Images",
        "authors": [
            "Zhangyang Wang",
            "Ding Liu",
            "Shiyu Chang",
            "Qing Ling",
            "Yingzhen Yang",
            "Thomas S. Huang"
        ],
        "abstract": "In this paper, we design a Deep Dual-Domain ($\\mathbf{D^3}$) based fast restoration model to remove artifacts of JPEG compressed images. It leverages the large learning capacity of deep networks, as well as the problem-specific expertise that was hardly incorporated in the past design of deep architectures. For the latter, we take into consideration both the prior knowledge of the JPEG compression scheme, and the successful practice of the sparsity-based dual-domain approach. We further design the One-Step Sparse Inference (1-SI) module, as an efficient and light-weighted feed-forward approximation of sparse coding. Extensive experiments verify the superiority of the proposed $D^3$ model over several state-of-the-art methods. Specifically, our best model is capable of outperforming the latest deep model for around 1 dB in PSNR, and is 30 times faster.\n    ",
        "submission_date": "2016-01-16T00:00:00",
        "last_modified_date": "2016-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.04153",
        "title": "Studying Very Low Resolution Recognition Using Deep Networks",
        "authors": [
            "Zhangyang Wang",
            "Shiyu Chang",
            "Yingzhen Yang",
            "Ding Liu",
            "Thomas S. Huang"
        ],
        "abstract": "Visual recognition research often assumes a sufficient resolution of the region of interest (ROI). That is usually violated in practice, inspiring us to explore the Very Low Resolution Recognition (VLRR) problem. Typically, the ROI in a VLRR problem can be smaller than $16 \\times 16$ pixels, and is challenging to be recognized even by human experts. We attempt to solve the VLRR problem using deep learning methods. Taking advantage of techniques primarily in super resolution, domain adaptation and robust regression, we formulate a dedicated deep learning method and demonstrate how these techniques are incorporated step by step. Any extra complexity, when introduced, is fully justified by both analysis and simulation results. The resulting \\textit{Robust Partially Coupled Networks} achieves feature enhancement and recognition simultaneously. It allows for both the flexibility to combat the LR-HR domain mismatch, and the robustness to outliers. Finally, the effectiveness of the proposed models is evaluated on three different VLRR tasks, including face identification, digit recognition and font recognition, all of which obtain very impressive performances.\n    ",
        "submission_date": "2016-01-16T00:00:00",
        "last_modified_date": "2016-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.04800",
        "title": "Top-N Recommender System via Matrix Completion",
        "authors": [
            "Zhao Kang",
            "Chong Peng",
            "Qiang Cheng"
        ],
        "abstract": "Top-N recommender systems have been investigated widely both in industry and academia. However, the recommendation quality is far from satisfactory. In this paper, we propose a simple yet promising algorithm. We fill the user-item matrix based on a low-rank assumption and simultaneously keep the original information. To do that, a nonconvex rank relaxation rather than the nuclear norm is adopted to provide a better rank approximation and an efficient optimization strategy is designed. A comprehensive set of experiments on real datasets demonstrates that our method pushes the accuracy of Top-N recommendation to a new level.\n    ",
        "submission_date": "2016-01-19T00:00:00",
        "last_modified_date": "2016-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.04908",
        "title": "Graded Entailment for Compositional Distributional Semantics",
        "authors": [
            "Desislava Bankova",
            "Bob Coecke",
            "Martha Lewis",
            "Daniel Marsden"
        ],
        "abstract": "The categorical compositional distributional model of natural language provides a conceptually motivated procedure to compute the meaning of sentences, given grammatical structure and the meanings of its words. This approach has outperformed other models in mainstream empirical language processing tasks. However, until recently it has lacked the crucial feature of lexical entailment -- as do other distributional models of meaning.\n",
        "submission_date": "2016-01-19T00:00:00",
        "last_modified_date": "2016-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.04943",
        "title": "Semantics for probabilistic programming: higher-order functions, continuous distributions, and soft constraints",
        "authors": [
            "Sam Staton",
            "Hongseok Yang",
            "Chris Heunen",
            "Ohad Kammar",
            "Frank Wood"
        ],
        "abstract": "We study the semantic foundation of expressive probabilistic programming languages, that support higher-order functions, continuous distributions, and soft constraints (such as Anglican, Church, and Venture). We define a metalanguage (an idealised version of Anglican) for probabilistic computation with the above features, develop both operational and denotational semantics, and prove soundness, adequacy, and termination. They involve measure theory, stochastic labelled transition systems, and functor categories, but admit intuitive computational readings, one of which views sampled random variables as dynamically allocated read-only variables. We apply our semantics to validate nontrivial equations underlying the correctness of certain compiler optimisations and inference algorithms such as sequential Monte Carlo simulation. The language enables defining probability distributions on higher-order functions, and we study their properties.\n    ",
        "submission_date": "2016-01-19T00:00:00",
        "last_modified_date": "2016-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.05140",
        "title": "The DARPA Twitter Bot Challenge",
        "authors": [
            "V.S. Subrahmanian",
            "Amos Azaria",
            "Skylar Durst",
            "Vadim Kagan",
            "Aram Galstyan",
            "Kristina Lerman",
            "Linhong Zhu",
            "Emilio Ferrara",
            "Alessandro Flammini",
            "Filippo Menczer",
            "Andrew Stevens",
            "Alexander Dekhtyar",
            "Shuyang Gao",
            "Tad Hogg",
            "Farshad Kooti",
            "Yan Liu",
            "Onur Varol",
            "Prashant Shiralkar",
            "Vinod Vydiswaran",
            "Qiaozhu Mei",
            "Tim Hwang"
        ],
        "abstract": "A number of organizations ranging from terrorist groups such as ISIS to politicians and nation states reportedly conduct explicit campaigns to influence opinion on social media, posing a risk to democratic processes. There is thus a growing need to identify and eliminate \"influence bots\" - realistic, automated identities that illicitly shape discussion on sites like Twitter and Facebook - before they get too influential. Spurred by such events, DARPA held a 4-week competition in February/March 2015 in which multiple teams supported by the DARPA Social Media in Strategic Communications program competed to identify a set of previously identified \"influence bots\" serving as ground truth on a specific topic within Twitter. Past work regarding influence bots often has difficulty supporting claims about accuracy, since there is limited ground truth (though some exceptions do exist [3,7]). However, with the exception of [3], no past work has looked specifically at identifying influence bots on a specific topic. This paper describes the DARPA Challenge and describes the methods used by the three top-ranked teams.\n    ",
        "submission_date": "2016-01-20T00:00:00",
        "last_modified_date": "2016-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.05403",
        "title": "Semantic Word Clusters Using Signed Normalized Graph Cuts",
        "authors": [
            "Jo\u00e3o Sedoc",
            "Jean Gallier",
            "Lyle Ungar",
            "Dean Foster"
        ],
        "abstract": "Vector space representations of words capture many aspects of word similarity, but such methods tend to make vector spaces in which antonyms (as well as synonyms) are close to each other. We present a new signed spectral normalized graph cut algorithm, signed clustering, that overlays existing thesauri upon distributionally derived vector representations of words, so that antonym relationships between word pairs are represented by negative weights. Our signed clustering algorithm produces clusters of words which simultaneously capture distributional and synonym relations. We evaluate these clusters against the SimLex-999 dataset (Hill et al.,2014) of human judgments of word pair similarities, and also show the benefit of using our clusters to predict the sentiment of a given text.\n    ",
        "submission_date": "2016-01-20T00:00:00",
        "last_modified_date": "2016-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.05744",
        "title": "Sub-Optimal Multi-Phase Path Planning: A Method for Solving Rubik's Revenge",
        "authors": [
            "Jared Weed"
        ],
        "abstract": "Rubik's Revenge, a 4x4x4 variant of the Rubik's puzzles, remains to date as an unsolved puzzle. That is to say, we do not have a method or successful categorization to optimally solve every one of its approximately $7.401 \\times 10^{45}$ possible configurations. Rubik's Cube, Rubik's Revenge's predecessor (3x3x3), with its approximately $4.33 \\times 10^{19}$ possible configurations, has only recently been completely solved by Rokicki et. al, further finding that any configuration requires no more than 20 moves. With the sheer dimension of Rubik's Revenge and its total configuration space, a brute-force method of finding all optimal solutions would be in vain. Similar to the methods used by Rokicki et. al on Rubik's Cube, in this paper we develop a method for solving arbitrary configurations of Rubik's Revenge in phases, using a combination of a powerful algorithm known as IDA* and a useful definition of distance in the cube space. While time-series results were not successfully gathered, it will be shown that this method far outweighs current human-solving methods and can be used to determine loose upper bounds for the cube space. Discussion will suggest that this method can also be applied to other puzzles with the proper transformations.\n    ",
        "submission_date": "2016-01-20T00:00:00",
        "last_modified_date": "2016-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06041",
        "title": "Online Event Recognition from Moving Vessel Trajectories",
        "authors": [
            "Kostas Patroumpas",
            "Elias Alevizos",
            "Alexander Artikis",
            "Marios Vodas",
            "Nikos Pelekis",
            "Yannis Theodoridis"
        ],
        "abstract": "We present a system for online monitoring of maritime activity over streaming positions from numerous vessels sailing at sea. It employs an online tracking module for detecting important changes in the evolving trajectory of each vessel across time, and thus can incrementally retain concise, yet reliable summaries of its recent movement. In addition, thanks to its complex event recognition module, this system can also offer instant notification to marine authorities regarding emergency situations, such as risk of collisions, suspicious moves in protected zones, or package picking at open sea. Not only did our extensive tests validate the performance, efficiency, and robustness of the system against scalable volumes of real-world and synthetically enlarged datasets, but its deployment against online feeds from vessels has also confirmed its capabilities for effective, real-time maritime surveillance.\n    ",
        "submission_date": "2016-01-22T00:00:00",
        "last_modified_date": "2016-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06071",
        "title": "Bitwise Neural Networks",
        "authors": [
            "Minje Kim",
            "Paris Smaragdis"
        ],
        "abstract": "Based on the assumption that there exists a neural network that efficiently represents a set of Boolean functions between all binary inputs and outputs, we propose a process for developing and deploying neural networks whose weight parameters, bias terms, input, and intermediate hidden layer output signals, are all binary-valued, and require only basic bit logic for the feedforward pass. The proposed Bitwise Neural Network (BNN) is especially suitable for resource-constrained environments, since it replaces either floating or fixed-point arithmetic with significantly more efficient bitwise operations. Hence, the BNN requires for less spatial complexity, less memory bandwidth, and less power consumption in hardware. In order to design such networks, we propose to add a few training schemes, such as weight compression and noisy backpropagation, which result in a bitwise network that performs almost as well as its corresponding real-valued network. We test the proposed network on the MNIST dataset, represented using binary features, and show that BNNs result in competitive performance while offering dramatic computational savings.\n    ",
        "submission_date": "2016-01-22T00:00:00",
        "last_modified_date": "2016-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06580",
        "title": "Is swarm intelligence able to create mazes?",
        "authors": [
            "Dawid Polap",
            "Marcin Wozniak",
            "Christian Napoli",
            "Emiliano Tramontana"
        ],
        "abstract": "In this paper, the idea of applying Computational Intelligence in the process of creation board games, in particular mazes, is presented. For two different algorithms the proposed idea has been examined. The results of the experiments are shown and discussed to present advantages and disadvantages.\n    ",
        "submission_date": "2016-01-25T00:00:00",
        "last_modified_date": "2016-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06602",
        "title": "Expected Similarity Estimation for Large-Scale Batch and Streaming Anomaly Detection",
        "authors": [
            "Markus Schneider",
            "Wolfgang Ertel",
            "Fabio Ramos"
        ],
        "abstract": "We present a novel algorithm for anomaly detection on very large datasets and data streams. The method, named EXPected Similarity Estimation (EXPoSE), is kernel-based and able to efficiently compute the similarity between new data points and the distribution of regular data. The estimator is formulated as an inner product with a reproducing kernel Hilbert space embedding and makes no assumption about the type or shape of the underlying data distribution. We show that offline (batch) learning with EXPoSE can be done in linear time and online (incremental) learning takes constant time per instance and model update. Furthermore, EXPoSE can make predictions in constant time, while it requires only constant memory. In addition, we propose different methodologies for concept drift adaptation on evolving data streams. On several real datasets we demonstrate that our approach can compete with state of the art algorithms for anomaly detection while being an order of magnitude faster than most other approaches.\n    ",
        "submission_date": "2016-01-25T00:00:00",
        "last_modified_date": "2016-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06672",
        "title": "Pricing Vehicle Sharing with Proximity Information",
        "authors": [
            "Jakub Marecek",
            "Robert Shorten",
            "Jia Yuan Yu"
        ],
        "abstract": "For vehicle sharing schemes, where drop-off positions are not fixed, we propose a pricing scheme, where the price depends in part on the distance between where a vehicle is being dropped off and where the closest shared vehicle is parked. Under certain restrictive assumptions, we show that this pricing leads to a socially optimal spread of the vehicles within a region.\n    ",
        "submission_date": "2016-01-25T00:00:00",
        "last_modified_date": "2016-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06931",
        "title": "Fisher Motion Descriptor for Multiview Gait Recognition",
        "authors": [
            "F.M. Castro",
            "M.J. Mar\u00edn-Jim\u00e9nez",
            "N. Guil",
            "R. Mu\u00f1oz-Salinas"
        ],
        "abstract": "The goal of this paper is to identify individuals by analyzing their gait. Instead of using binary silhouettes as input data (as done in many previous works) we propose and evaluate the use of motion descriptors based on densely sampled short-term trajectories. We take advantage of state-of-the-art people detectors to define custom spatial configurations of the descriptors around the target person, obtaining a rich representation of the gait motion. The local motion features (described by the Divergence-Curl-Shear descriptor) extracted on the different spatial areas of the person are combined into a single high-level gait descriptor by using the Fisher Vector encoding. The proposed approach, coined Pyramidal Fisher Motion, is experimentally validated on `CASIA' dataset (parts B and C), `TUM GAID' dataset, `CMU MoBo' dataset and the recent `AVA Multiview Gait' dataset. The results show that this new approach achieves state-of-the-art results in the problem of gait recognition, allowing to recognize walking people from diverse viewpoints on single and multiple camera setups, wearing different clothes, carrying bags, walking at diverse speeds and not limited to straight walking paths.\n    ",
        "submission_date": "2016-01-26T00:00:00",
        "last_modified_date": "2016-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.07252",
        "title": "Font Identification in Historical Documents Using Active Learning",
        "authors": [
            "Anshul Gupta",
            "Ricardo Gutierrez-Osuna",
            "Matthew Christy",
            "Richard Furuta",
            "Laura Mandell"
        ],
        "abstract": "Identifying the type of font (e.g., Roman, Blackletter) used in historical documents can help optical character recognition (OCR) systems produce more accurate text transcriptions. Towards this end, we present an active-learning strategy that can significantly reduce the number of labeled samples needed to train a font classifier. Our approach extracts image-based features that exploit geometric differences between fonts at the word level, and combines them into a bag-of-word representation for each page in a document. We evaluate six sampling strategies based on uncertainty, dissimilarity and diversity criteria, and test them on a database containing over 3,000 historical documents with Blackletter, Roman and Mixed fonts. Our results show that a combination of uncertainty and diversity achieves the highest predictive accuracy (89% of test cases correctly classified) while requiring only a small fraction of the data (17%) to be labeled. We discuss the implications of this result for mass digitization projects of historical documents.\n    ",
        "submission_date": "2016-01-27T00:00:00",
        "last_modified_date": "2016-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.07358",
        "title": "Quantum machine learning with glow for episodic tasks and decision games",
        "authors": [
            "Jens Clausen",
            "Hans J. Briegel"
        ],
        "abstract": "We consider a general class of models, where a reinforcement learning (RL) agent learns from cyclic interactions with an external environment via classical signals. Perceptual inputs are encoded as quantum states, which are subsequently transformed by a quantum channel representing the agent's memory, while the outcomes of measurements performed at the channel's output determine the agent's actions. The learning takes place via stepwise modifications of the channel properties. They are described by an update rule that is inspired by the projective simulation (PS) model and equipped with a glow mechanism that allows for a backpropagation of policy changes, analogous to the eligibility traces in RL and edge glow in PS. In this way, the model combines features of PS with the ability for generalization, offered by its physical embodiment as a quantum system. We apply the agent to various setups of an invasion game and a grid world, which serve as elementary model tasks allowing a direct comparison with a basic classical PS agent.\n    ",
        "submission_date": "2016-01-27T00:00:00",
        "last_modified_date": "2016-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.07446",
        "title": "A First Attempt to Cloud-Based User Verification in Distributed System",
        "authors": [
            "Marcin Wozniak",
            "Dawid Polap",
            "Grzegorz Borowik",
            "Christian Napoli"
        ],
        "abstract": "In this paper, the idea of client verification in distributed systems is presented. The proposed solution presents a sample system where client verification through cloud resources using input signature is discussed. For different signatures the proposed method has been examined. Research results are presented and discussed to show potential advantages.\n    ",
        "submission_date": "2016-01-27T00:00:00",
        "last_modified_date": "2016-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.00203",
        "title": "Greedy Deep Dictionary Learning",
        "authors": [
            "Snigdha Tariyal",
            "Angshul Majumdar",
            "Richa Singh",
            "Mayank Vatsa"
        ],
        "abstract": "In this work we propose a new deep learning tool called deep dictionary learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at a time. This requires solving a simple (shallow) dictionary learning problem, the solution to this is well known. We apply the proposed technique on some benchmark deep learning datasets. We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tools like discriminative KSVD and label consistent KSVD. Our method yields better results than all.\n    ",
        "submission_date": "2016-01-31T00:00:00",
        "last_modified_date": "2016-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.00487",
        "title": "Towards a Cognitive Routing Engine for Software Defined Networks",
        "authors": [
            "Frederic Francois",
            "Erol Gelenbe"
        ],
        "abstract": "Most Software Defined Networks (SDN) traffic engineering applications use excessive and frequent global monitoring in order to find the optimal Quality-of-Service (QoS) paths for the current state of the network. In this work, we present the motivations, architecture and initial evaluation of a SDN application called Cognitive Routing Engine (CRE) which is able to find near-optimal paths for a user-specified QoS while using a very small monitoring overhead compared to global monitoring which is required to guarantee that optimal paths are found. Smaller monitoring overheads bring the advantage of smaller response time for the SDN controllers and switches. The initial evaluation of CRE on a SDN representation of the GEANT academic network shows that it is possible to find near-optimal paths with a small optimality gap of 1.65% while using 9.5 times less monitoring.\n    ",
        "submission_date": "2016-02-01T00:00:00",
        "last_modified_date": "2016-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.00991",
        "title": "Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks",
        "authors": [
            "Peter Ondruska",
            "Ingmar Posner"
        ],
        "abstract": "This paper presents to the best of our knowledge the first end-to-end object tracking approach which directly maps from raw sensor input to object tracks in sensor space without requiring any feature engineering or system identification in the form of plant or sensor models. Specifically, our system accepts a stream of raw sensor data at one end and, in real-time, produces an estimate of the entire environment state at the output including even occluded objects. We achieve this by framing the problem as a deep learning task and exploit sequence models in the form of recurrent neural networks to learn a mapping from sensor measurements to object tracks. In particular, we propose a learning method based on a form of input dropout which allows learning in an unsupervised manner, only based on raw, occluded sensor data without access to ground-truth annotations. We demonstrate our approach using a synthetic dataset designed to mimic the task of tracking objects in 2D laser data -- as commonly encountered in robotics applications -- and show that it learns to track many dynamic objects despite occlusions and the presence of sensor noise.\n    ",
        "submission_date": "2016-02-02T00:00:00",
        "last_modified_date": "2016-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.01576",
        "title": "A Factorized Recurrent Neural Network based architecture for medium to large vocabulary Language Modelling",
        "authors": [
            "Anantharaman Palacode Narayana Iyer"
        ],
        "abstract": "Statistical language models are central to many applications that use semantics. Recurrent Neural Networks (RNN) are known to produce state of the art results for language modelling, outperforming their traditional n-gram counterparts in many cases. To generate a probability distribution across a vocabulary, these models require a softmax output layer that linearly increases in size with the size of the vocabulary. Large vocabularies need a commensurately large softmax layer and training them on typical laptops/PCs requires significant time and machine resources. In this paper we present a new technique for implementing RNN based large vocabulary language models that substantially speeds up computation while optimally using the limited memory resources. Our technique, while building on the notion of factorizing the output layer by having multiple output layers, improves on the earlier work by substantially optimizing on the individual output layer size and also eliminating the need for a multistep prediction process.\n    ",
        "submission_date": "2016-02-04T00:00:00",
        "last_modified_date": "2016-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.01635",
        "title": "A Generalised Quantifier Theory of Natural Language in Categorical Compositional Distributional Semantics with Bialgebras",
        "authors": [
            "Jules Hedges",
            "Mehrnoosh Sadrzadeh"
        ],
        "abstract": "Categorical compositional distributional semantics is a model of natural language; it combines the statistical vector space models of words with the compositional models of grammar. We formalise in this model the generalised quantifier theory of natural language, due to Barwise and Cooper. The underlying setting is a compact closed category with bialgebras. We start from a generative grammar formalisation and develop an abstract categorical compositional semantics for it, then instantiate the abstract setting to sets and relations and to finite dimensional vector spaces and linear maps. We prove the equivalence of the relational instantiation to the truth theoretic semantics of generalised quantifiers. The vector space instantiation formalises the statistical usages of words and enables us to, for the first time, reason about quantified phrases and sentences compositionally in distributional semantics.\n    ",
        "submission_date": "2016-02-04T00:00:00",
        "last_modified_date": "2017-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.01921",
        "title": "Recognition of Visually Perceived Compositional Human Actions by Multiple Spatio-Temporal Scales Recurrent Neural Networks",
        "authors": [
            "Haanvid Lee",
            "Minju Jung",
            "Jun Tani"
        ],
        "abstract": "The current paper proposes a novel neural network model for recognizing visually perceived human actions. The proposed multiple spatio-temporal scales recurrent neural network (MSTRNN) model is derived by introducing multiple timescale recurrent dynamics to the conventional convolutional neural network model. One of the essential characteristics of the MSTRNN is that its architecture imposes both spatial and temporal constraints simultaneously on the neural activity which vary in multiple scales among different layers. As suggested by the principle of the upward and downward causation, it is assumed that the network can develop meaningful structures such as functional hierarchy by taking advantage of such constraints during the course of learning. To evaluate the characteristics of the model, the current study uses three types of human action video dataset consisting of different types of primitive actions and different levels of compositionality on them. The performance of the MSTRNN in testing with these dataset is compared with the ones by other representative deep learning models used in the field. The analysis of the internal representation obtained through the learning with the dataset clarifies what sorts of functional hierarchy can be developed by extracting the essential compositionality underlying the dataset.\n    ",
        "submission_date": "2016-02-05T00:00:00",
        "last_modified_date": "2017-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02210",
        "title": "Classification accuracy as a proxy for two sample testing",
        "authors": [
            "Ilmun Kim",
            "Aaditya Ramdas",
            "Aarti Singh",
            "Larry Wasserman"
        ],
        "abstract": "When data analysts train a classifier and check if its accuracy is significantly different from chance, they are implicitly performing a two-sample test. We investigate the statistical properties of this flexible approach in the high-dimensional setting. We prove two results that hold for all classifiers in any dimensions: if its true error remains $\\epsilon$-better than chance for some $\\epsilon>0$ as $d,n \\to \\infty$, then (a) the permutation-based test is consistent (has power approaching to one), (b) a computationally efficient test based on a Gaussian approximation of the null distribution is also consistent. To get a finer understanding of the rates of consistency, we study a specialized setting of distinguishing Gaussians with mean-difference $\\delta$ and common (known or unknown) covariance $\\Sigma$, when $d/n \\to c \\in (0,\\infty)$. We study variants of Fisher's linear discriminant analysis (LDA) such as \"naive Bayes\" in a nontrivial regime when $\\epsilon \\to 0$ (the Bayes classifier has true accuracy approaching 1/2), and contrast their power with corresponding variants of Hotelling's test. Surprisingly, the expressions for their power match exactly in terms of $n,d,\\delta,\\Sigma$, and the LDA approach is only worse by a constant factor, achieving an asymptotic relative efficiency (ARE) of $1/\\sqrt{\\pi}$ for balanced samples. We also extend our results to high-dimensional elliptical distributions with finite kurtosis. Other results of independent interest include minimax lower bounds, and the optimality of Hotelling's test when $d=o(n)$. Simulation results validate our theory, and we present practical takeaway messages along with natural open problems.\n    ",
        "submission_date": "2016-02-06T00:00:00",
        "last_modified_date": "2020-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02211",
        "title": "Fuzzy Maximum Satisfiability",
        "authors": [
            "Mohamed El Halaby",
            "Areeg Abdalla"
        ],
        "abstract": "In this paper, we extend the Maximum Satisfiability (MaxSAT) problem to \u0141ukasiewicz logic. The MaxSAT problem for a set of formulae {\\Phi} is the problem of finding an assignment to the variables in {\\Phi} that satisfies the maximum number of formulae. Three possible solutions (encodings) are proposed to the new problem: (1) Disjunctive Linear Relations (DLRs), (2) Mixed Integer Linear Programming (MILP) and (3) Weighted Constraint Satisfaction Problem (WCSP). Like its Boolean counterpart, the extended fuzzy MaxSAT will have numerous applications in optimization problems that involve vagueness.\n    ",
        "submission_date": "2016-02-06T00:00:00",
        "last_modified_date": "2016-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02332",
        "title": "Scalable Text Mining with Sparse Generative Models",
        "authors": [
            "Antti Puurula"
        ],
        "abstract": "The information age has brought a deluge of data. Much of this is in text form, insurmountable in scope for humans and incomprehensible in structure for computers. Text mining is an expanding field of research that seeks to utilize the information contained in vast document collections. General data mining methods based on machine learning face challenges with the scale of text data, posing a need for scalable text mining methods.\n",
        "submission_date": "2016-02-07T00:00:00",
        "last_modified_date": "2016-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02334",
        "title": "ERBlox: Combining Matching Dependencies with Machine Learning for Entity Resolution",
        "authors": [
            "Zeinab Bahmani",
            "Leopoldo Bertossi",
            "Nikolaos Vasiloglou"
        ],
        "abstract": "Entity resolution (ER), an important and common data cleaning problem, is about detecting data duplicate representations for the same external entities, and merging them into single representations. Relatively recently, declarative rules called \"matching dependencies\" (MDs) have been proposed for specifying similarity conditions under which attribute values in database records are merged. In this work we show the process and the benefits of integrating four components of ER: (a) Building a classifier for duplicate/non-duplicate record pairs built using machine learning (ML) techniques; (b) Use of MDs for supporting the blocking phase of ML; (c) Record merging on the basis of the classifier results; and (d) The use of the declarative language \"LogiQL\" -an extended form of Datalog supported by the \"LogicBlox\" platform- for all activities related to data processing, and the specification and enforcement of MDs.\n    ",
        "submission_date": "2016-02-07T00:00:00",
        "last_modified_date": "2017-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02377",
        "title": "Find an Optimal Path in Static System and Dynamical System within Polynomial Runtime",
        "authors": [
            "Yong Tan"
        ],
        "abstract": "We study an ancient problem that in a static or dynamical system, sought an optimal path, which the context always means within an extremal condition. In fact, through those discussions about this theme, we established a universal essential calculated model to serve for these complex systems. Meanwhile we utilize the sample space to character the system. These contents in this paper would involve in several major areas including the geometry, probability, graph algorithms and some prior approaches, which stands the ultimately subtle linear algorithm to solve this class problem. Along with our progress, our discussion would demonstrate more general meaning and robust character, which provides clear ideas or notion to support our concrete applications, who work in a more popular complex system.\n    ",
        "submission_date": "2016-02-07T00:00:00",
        "last_modified_date": "2016-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02473",
        "title": "Particle Swarm Optimized Power Consumption of Trilateration",
        "authors": [
            "Hussein S. Al-Olimat",
            "Robert C. Green II",
            "Mansoor Alam",
            "Vijay Devabhaktuni",
            "Wei Cheng"
        ],
        "abstract": "Trilateration-based localization (TBL) has become a corner stone of modern technology. This study formulates the concern on how wireless sensor networks can take advantage of the computational intelligent techniques using both single- and multi-objective particle swarm optimization (PSO) with an overall aim of concurrently minimizing the required time for localization, minimizing energy consumed during localization, and maximizing the number of nodes fully localized through the adjustment of wireless sensor transmission ranges while using TBL process. A parameter-study of the applied PSO variants is performed, leading to results that show algorithmic improvements of up to 32% in the evaluated objectives.\n    ",
        "submission_date": "2016-02-08T00:00:00",
        "last_modified_date": "2016-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02658",
        "title": "Graying the black box: Understanding DQNs",
        "authors": [
            "Tom Zahavy",
            "Nir Ben Zrihem",
            "Shie Mannor"
        ],
        "abstract": "In recent years there is a growing interest in using deep representations for reinforcement learning. In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter. Moreover, we propose a new model, the Semi Aggregated Markov Decision Process (SAMDP), and an algorithm that learns it automatically. The SAMDP model allows us to identify spatio-temporal abstractions directly from features and may be used as a sub-goal detector in future work. Using our tools we reveal that the features learned by DQNs aggregate the state space in a hierarchical fashion, explaining its success. Moreover, we are able to understand and describe the policies learned by DQNs for three different Atari2600 games and suggest ways to interpret, debug and optimize deep neural networks in reinforcement learning.\n    ",
        "submission_date": "2016-02-08T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02685",
        "title": "Predicting Clinical Events by Combining Static and Dynamic Information Using Recurrent Neural Networks",
        "authors": [
            "Crist\u00f3bal Esteban",
            "Oliver Staeck",
            "Yinchong Yang",
            "Volker Tresp"
        ],
        "abstract": "In clinical data sets we often find static information (e.g. patient gender, blood type, etc.) combined with sequences of data that are recorded during multiple hospital visits (e.g. medications prescribed, tests performed, etc.). Recurrent Neural Networks (RNNs) have proven to be very successful for modelling sequences of data in many areas of Machine Learning. In this work we present an approach based on RNNs, specifically designed for the clinical domain, that combines static and dynamic information in order to predict future events. We work with a database collected in the Charit\u00e9 Hospital in Berlin that contains complete information concerning patients that underwent a kidney transplantation. After the transplantation three main endpoints can occur: rejection of the kidney, loss of the kidney and death of the patient. Our goal is to predict, based on information recorded in the Electronic Health Record of each patient, whether any of those endpoints will occur within the next six or twelve months after each visit to the clinic. We compared different types of RNNs that we developed for this work, with a model based on a Feedforward Neural Network and a Logistic Regression model. We found that the RNN that we developed based on Gated Recurrent Units provides the best performance for this task. We also used the same models for a second task, i.e., next event prediction, and found that here the model based on a Feedforward Neural Network outperformed the other models. Our hypothesis is that long-term dependencies are not as relevant in this task.\n    ",
        "submission_date": "2016-02-08T00:00:00",
        "last_modified_date": "2016-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02706",
        "title": "Decoy Bandits Dueling on a Poset",
        "authors": [
            "Julien Audiffren",
            "Ralaivola Liva"
        ],
        "abstract": "We adress the problem of dueling bandits defined on partially ordered sets, or posets. In this setting, arms may not be comparable, and there may be several (incomparable) optimal arms. We propose an algorithm, UnchainedBandits, that efficiently finds the set of optimal arms of any poset even when pairs of comparable arms cannot be distinguished from pairs of incomparable arms, with a set of minimal assumptions. This algorithm relies on the concept of decoys, which stems from social psychology. For the easier case where the incomparability information may be accessible, we propose a second algorithm, SlicingBandits, which takes advantage of this information and achieves a very significant gain of performance compared to UnchainedBandits. We provide theoretical guarantees and experimental evaluation for both algorithms.\n    ",
        "submission_date": "2016-02-08T00:00:00",
        "last_modified_date": "2016-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02710",
        "title": "Strategic disclosure of opinions on a social network",
        "authors": [
            "Umberto Grandi",
            "Emiliano Lorini",
            "Laurent Perrussel"
        ],
        "abstract": "We study the strategic aspects of social influence in a society of agents linked by a trust network, introducing a new class of games called games of influence. A game of influence is an infinite repeated game with incomplete information in which, at each stage of interaction, an agent can make her opinions visible (public) or invisible (private) in order to influence other agents' opinions. The influence process is mediated by a trust network, as we assume that the opinion of a given agent is only affected by the opinions of those agents that she considers trustworthy (i.e., the agents in the trust network that are directly linked to her). Each agent is endowed with a goal, expressed in a suitable temporal language inspired from linear temporal logic (LTL). We show that games of influence provide a simple abstraction to explore the effects of the trust network structure on the agents' behaviour, by considering solution concepts from game-theory such as Nash equilibrium, weak dominance and winning strategies.\n    ",
        "submission_date": "2016-02-05T00:00:00",
        "last_modified_date": "2016-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02743",
        "title": "The IMP game: Learnability, approximability and adversarial learning beyond $\u03a3^0_1$",
        "authors": [
            "Michael Brand",
            "David L. Dowe"
        ],
        "abstract": "We introduce a problem set-up we call the Iterated Matching Pennies (IMP) game and show that it is a powerful framework for the study of three problems: adversarial learnability, conventional (i.e., non-adversarial) learnability and approximability. Using it, we are able to derive the following theorems. (1) It is possible to learn by example all of $\\Sigma^0_1 \\cup \\Pi^0_1$ as well as some supersets; (2) in adversarial learning (which we describe as a pursuit-evasion game), the pursuer has a winning strategy (in other words, $\\Sigma^0_1$ can be learned adversarially, but $\\Pi^0_1$ not); (3) some languages in $\\Pi^0_1$ cannot be approximated by any language in $\\Sigma^0_1$.\n",
        "submission_date": "2016-02-07T00:00:00",
        "last_modified_date": "2016-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.03348",
        "title": "Iterative Hierarchical Optimization for Misspecified Problems (IHOMP)",
        "authors": [
            "Daniel J. Mankowitz",
            "Timothy A. Mann",
            "Shie Mannor"
        ],
        "abstract": "For complex, high-dimensional Markov Decision Processes (MDPs), it may be necessary to represent the policy with function approximation. A problem is misspecified whenever, the representation cannot express any policy with acceptable performance. We introduce IHOMP : an approach for solving misspecified problems. IHOMP iteratively learns a set of context specialized options and combines these options to solve an otherwise misspecified problem. Our main contribution is proving that IHOMP enjoys theoretical convergence guarantees. In addition, we extend IHOMP to exploit Option Interruption (OI) enabling it to decide where the learned options can be reused. Our experiments demonstrate that IHOMP can find near-optimal solutions to otherwise misspecified problems and that OI can further improve the solutions.\n    ",
        "submission_date": "2016-02-10T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.03351",
        "title": "Adaptive Skills, Adaptive Partitions (ASAP)",
        "authors": [
            "Daniel J. Mankowitz",
            "Timothy A. Mann",
            "Shie Mannor"
        ],
        "abstract": "We introduce the Adaptive Skills, Adaptive Partitions (ASAP) framework that (1) learns skills (i.e., temporally extended actions or options) as well as (2) where to apply them. We believe that both (1) and (2) are necessary for a truly general skill learning framework, which is a key building block needed to scale up to lifelong learning agents. The ASAP framework can also solve related new tasks simply by adapting where it applies its existing learned skills. We prove that ASAP converges to a local optimum under natural conditions. Finally, our experimental results, which include a RoboCup domain, demonstrate the ability of ASAP to learn where to reuse skills as well as solve multiple tasks with considerably less experience than solving each task from scratch.\n    ",
        "submission_date": "2016-02-10T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.03814",
        "title": "Enabling Basic Normative HRI in a Cognitive Robotic Architecture",
        "authors": [
            "Vasanth Sarathy",
            "Jason R. Wilson",
            "Thomas Arnold",
            "Matthias Scheutz"
        ],
        "abstract": "Collaborative human activities are grounded in social and moral norms, which humans consciously and subconsciously use to guide and constrain their decision-making and behavior, thereby strengthening their interactions and preventing emotional and physical harm. This type of norm-based processing is also critical for robots in many human-robot interaction scenarios (e.g., when helping elderly and disabled persons in assisted living facilities, or assisting humans in assembly tasks in factories or even the space station). In this position paper, we will briefly describe how several components in an integrated cognitive architecture can be used to implement processes that are required for normative human-robot interactions, especially in collaborative tasks where actions and situations could potentially be perceived as threatening and thus need a change in course of action to mitigate the perceived threats.\n    ",
        "submission_date": "2016-02-11T00:00:00",
        "last_modified_date": "2016-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04375",
        "title": "Science Question Answering using Instructional Materials",
        "authors": [
            "Mrinmaya Sachan",
            "Avinava Dubey",
            "Eric P. Xing"
        ],
        "abstract": "We provide a solution for elementary science test using instructional materials. We posit that there is a hidden structure that explains the correctness of an answer given the question and instructional materials and present a unified max-margin framework that learns to find these hidden structures (given a corpus of question-answer pairs and instructional materials), and uses what it learns to answer novel elementary science questions. Our evaluation shows that our framework outperforms several strong baselines.\n    ",
        "submission_date": "2016-02-13T00:00:00",
        "last_modified_date": "2016-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04484",
        "title": "Surprising properties of dropout in deep networks",
        "authors": [
            "David P. Helmbold",
            "Philip M. Long"
        ],
        "abstract": "We analyze dropout in deep networks with rectified linear units and the quadratic loss. Our results expose surprising differences between the behavior of dropout and more traditional regularizers like weight decay. For example, on some simple data sets dropout training produces negative weights even though the output is the sum of the inputs. This provides a counterpoint to the suggestion that dropout discourages co-adaptation of weights. We also show that the dropout penalty can grow exponentially in the depth of the network while the weight-decay penalty remains essentially linear, and that dropout is insensitive to various re-scalings of the input features, outputs, and network weights. This last insensitivity implies that there are no isolated local minima of the dropout training criterion. Our work uncovers new properties of dropout, extends our understanding of why dropout succeeds, and lays the foundation for further progress.\n    ",
        "submission_date": "2016-02-14T00:00:00",
        "last_modified_date": "2017-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04621",
        "title": "Deep Exploration via Bootstrapped DQN",
        "authors": [
            "Ian Osband",
            "Charles Blundell",
            "Alexander Pritzel",
            "Benjamin Van Roy"
        ],
        "abstract": "Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games.\n    ",
        "submission_date": "2016-02-15T00:00:00",
        "last_modified_date": "2016-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04709",
        "title": "Identifying Structures in Social Conversations in NSCLC Patients through the Semi-Automatic extraction of Topical Taxonomies",
        "authors": [
            "Giancarlo Crocetti",
            "Amir A. Delay",
            "Fatemeh Seyedmendhi"
        ],
        "abstract": "The exploration of social conversations for addressing patient's needs is an important analytical task in which many scholarly publications are contributing to fill the knowledge gap in this area. The main difficulty remains the inability to turn such contributions into pragmatic processes the pharmaceutical industry can leverage in order to generate insight from social media data, which can be considered as one of the most challenging source of information available today due to its sheer volume and noise. This study is based on the work by Scott Spangler and Jeffrey Kreulen and applies it to identify structure in social media through the extraction of a topical taxonomy able to capture the latent knowledge in social conversations in health-related sites. The mechanism for automatically identifying and generating a taxonomy from social conversations is developed and pressured tested using public data from media sites focused on the needs of cancer patients and their families. Moreover, a novel method for generating the category's label and the determination of an optimal number of categories is presented which extends Scott and Jeffrey's research in a meaningful way. We assume the reader is familiar with taxonomies, what they are and how they are used.\n    ",
        "submission_date": "2016-02-12T00:00:00",
        "last_modified_date": "2016-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04889",
        "title": "Unsupervised Domain Adaptation Using Approximate Label Matching",
        "authors": [
            "Jordan T. Ash",
            "Robert E. Schapire",
            "Barbara E. Engelhardt"
        ],
        "abstract": "Domain adaptation addresses the problem created when training data is generated by a so-called source distribution, but test data is generated by a significantly different target distribution. In this work, we present approximate label matching (ALM), a new unsupervised domain adaptation technique that creates and leverages a rough labeling on the test samples, then uses these noisy labels to learn a transformation that aligns the source and target samples. We show that the transformation estimated by ALM has favorable properties compared to transformations estimated by other methods, which do not use any kind of target labeling. Our model is regularized by requiring that a classifier trained to discriminate source from transformed target samples cannot distinguish between the two. We experiment with ALM on simulated and real data, and show that it outperforms techniques commonly used in the field.\n    ",
        "submission_date": "2016-02-16T00:00:00",
        "last_modified_date": "2017-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04921",
        "title": "A diffusion and clustering-based approach for finding coherent motions and understanding crowd scenes",
        "authors": [
            "Weiyao Lin",
            "Yang Mi",
            "Weiyue Wang",
            "Jianxin Wu",
            "Jingdong Wang",
            "Tao Mei"
        ],
        "abstract": "This paper addresses the problem of detecting coherent motions in crowd scenes and presents its two applications in crowd scene understanding: semantic region detection and recurrent activity mining. It processes input motion fields (e.g., optical flow fields) and produces a coherent motion filed, named as thermal energy field. The thermal energy field is able to capture both motion correlation among particles and the motion trends of individual particles which are helpful to discover coherency among them. We further introduce a two-step clustering process to construct stable semantic regions from the extracted time-varying coherent motions. These semantic regions can be used to recognize pre-defined activities in crowd scenes. Finally, we introduce a cluster-and-merge process which automatically discovers recurrent activities in crowd scenes by clustering and merging the extracted coherent motions. Experiments on various videos demonstrate the effectiveness of our approach.\n    ",
        "submission_date": "2016-02-16T00:00:00",
        "last_modified_date": "2016-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04938",
        "title": "\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
        "authors": [
            "Marco Tulio Ribeiro",
            "Sameer Singh",
            "Carlos Guestrin"
        ],
        "abstract": "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.\n    ",
        "submission_date": "2016-02-16T00:00:00",
        "last_modified_date": "2016-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04983",
        "title": "Contextual Media Retrieval Using Natural Language Queries",
        "authors": [
            "Sreyasi Nag Chowdhury",
            "Mateusz Malinowski",
            "Andreas Bulling",
            "Mario Fritz"
        ],
        "abstract": "The widespread integration of cameras in hand-held and head-worn devices as well as the ability to share content online enables a large and diverse visual capture of the world that millions of users build up collectively every day. We envision these images as well as associated meta information, such as GPS coordinates and timestamps, to form a collective visual memory that can be queried while automatically taking the ever-changing context of mobile users into account. As a first step towards this vision, in this work we present Xplore-M-Ego: a novel media retrieval system that allows users to query a dynamic database of images and videos using spatio-temporal natural language queries. We evaluate our system using a new dataset of real user queries as well as through a usability study. One key finding is that there is a considerable amount of inter-user variability, for example in the resolution of spatial relations in natural language utterances. We show that our retrieval system can cope with this variability using personalisation through an online learning-based retrieval formulation.\n    ",
        "submission_date": "2016-02-16T00:00:00",
        "last_modified_date": "2016-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05012",
        "title": "A Subsequence Interleaving Model for Sequential Pattern Mining",
        "authors": [
            "Jaroslav Fowkes",
            "Charles Sutton"
        ],
        "abstract": "Recent sequential pattern mining methods have used the minimum description length (MDL) principle to define an encoding scheme which describes an algorithm for mining the most compressing patterns in a database. We present a novel subsequence interleaving model based on a probabilistic model of the sequence database, which allows us to search for the most compressing set of patterns without designing a specific encoding scheme. Our proposed algorithm is able to efficiently mine the most relevant sequential patterns and rank them using an associated measure of interestingness. The efficient inference in our model is a direct result of our use of a structural expectation-maximization framework, in which the expectation-step takes the form of a submodular optimization problem subject to a coverage constraint. We show on both synthetic and real world datasets that our model mines a set of sequential patterns with low spuriousness and redundancy, high interpretability and usefulness in real-world applications. Furthermore, we demonstrate that the quality of the patterns from our approach is comparable to, if not better than, existing state of the art sequential pattern mining algorithms.\n    ",
        "submission_date": "2016-02-16T00:00:00",
        "last_modified_date": "2016-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05028",
        "title": "Symmetry Breaking Predicates for SAT-based DFA Identification",
        "authors": [
            "Vladimir Ulyantsev",
            "Ilya Zakirzyanov",
            "Anatoly Shalyto"
        ],
        "abstract": "It was shown before that the NP-hard problem of deterministic finite automata (DFA) identification can be effectively translated to Boolean satisfiability (SAT). Modern SAT-solvers can tackle hard DFA identification instances efficiently. We present a technique to reduce the problem search space by enforcing an enumeration of DFA states in depth-first search (DFS) or breadth-first search (BFS) order. We propose symmetry breaking predicates, which can be added to Boolean formulae representing various DFA identification problems. We show how to apply this technique to DFA identification from both noiseless and noisy data. Also we propose a method to identify all automata of the desired size. The proposed approach outperforms the current state-of-the-art DFASAT method for DFA identification from noiseless data. A big advantage of the proposed approach is that it allows to determine exactly the existence or non-existence of a solution of the noisy DFA identification problem unlike metaheuristic approaches such as genetic algorithms.\n    ",
        "submission_date": "2016-02-16T00:00:00",
        "last_modified_date": "2016-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05220",
        "title": "BioSpaun: A large-scale behaving brain model with complex neurons",
        "authors": [
            "Chris Eliasmith",
            "Jan Gosmann",
            "Xuan Choo"
        ],
        "abstract": "We describe a large-scale functional brain model that includes detailed, conductance-based, compartmental models of individual neurons. We call the model BioSpaun, to indicate the increased biological plausibility of these neurons, and because it is a direct extension of the Spaun model \\cite{Eliasmith2012b}. We demonstrate that including these detailed compartmental models does not adversely affect performance across a variety of tasks, including digit recognition, serial working memory, and counting. We then explore the effects of applying TTX, a sodium channel blocking drug, to the model. We characterize the behavioral changes that result from this molecular level intervention. We believe this is the first demonstration of a large-scale brain model that clearly links low-level molecular interventions and high-level behavior.\n    ",
        "submission_date": "2016-02-16T00:00:00",
        "last_modified_date": "2016-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05292",
        "title": "Authorship Attribution Using a Neural Network Language Model",
        "authors": [
            "Zhenhao Ge",
            "Yufang Sun",
            "Mark J. T. Smith"
        ],
        "abstract": "In practice, training language models for individual authors is often expensive because of limited data resources. In such cases, Neural Network Language Models (NNLMs), generally outperform the traditional non-parametric N-gram models. Here we investigate the performance of a feed-forward NNLM on an authorship attribution problem, with moderate author set size and relatively limited data. We also consider how the text topics impact performance. Compared with a well-constructed N-gram baseline method with Kneser-Ney smoothing, the proposed method achieves nearly 2:5% reduction in perplexity and increases author classification accuracy by 3:43% on average, given as few as 5 test sentences. The performance is very competitive with the state of the art in terms of accuracy and demand on test data. The source code, preprocessed datasets, a detailed description of the methodology and results are available at ",
        "submission_date": "2016-02-17T00:00:00",
        "last_modified_date": "2016-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05352",
        "title": "Recommendations as Treatments: Debiasing Learning and Evaluation",
        "authors": [
            "Tobias Schnabel",
            "Adith Swaminathan",
            "Ashudeep Singh",
            "Navin Chandak",
            "Thorsten Joachims"
        ],
        "abstract": "Most data for evaluating and training recommender systems is subject to selection biases, either through self-selection by the users or through the actions of the recommendation system itself. In this paper, we provide a principled approach to handling selection biases, adapting models and estimation techniques from causal inference. The approach leads to unbiased performance estimators despite biased data, and to a matrix factorization method that provides substantially improved prediction performance on real-world data. We theoretically and empirically characterize the robustness of the approach, finding that it is highly practical and scalable.\n    ",
        "submission_date": "2016-02-17T00:00:00",
        "last_modified_date": "2016-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05450",
        "title": "Inverse Reinforcement Learning in Swarm Systems",
        "authors": [
            "Adrian \u0160o\u0161i\u0107",
            "Wasiur R. KhudaBukhsh",
            "Abdelhak M. Zoubir",
            "Heinz Koeppl"
        ],
        "abstract": "Inverse reinforcement learning (IRL) has become a useful tool for learning behavioral models from demonstration data. However, IRL remains mostly unexplored for multi-agent systems. In this paper, we show how the principle of IRL can be extended to homogeneous large-scale problems, inspired by the collective swarming behavior of natural systems. In particular, we make the following contributions to the field: 1) We introduce the swarMDP framework, a sub-class of decentralized partially observable Markov decision processes endowed with a swarm characterization. 2) Exploiting the inherent homogeneity of this framework, we reduce the resulting multi-agent IRL problem to a single-agent one by proving that the agent-specific value functions in this model coincide. 3) To solve the corresponding control problem, we propose a novel heterogeneous learning scheme that is particularly tailored to the swarm setting. Results on two example systems demonstrate that our framework is able to produce meaningful local reward models from which we can replicate the observed global system dynamics.\n    ",
        "submission_date": "2016-02-17T00:00:00",
        "last_modified_date": "2017-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05473",
        "title": "Auxiliary Deep Generative Models",
        "authors": [
            "Lars Maal\u00f8e",
            "Casper Kaae S\u00f8nderby",
            "S\u00f8ren Kaae S\u00f8nderby",
            "Ole Winther"
        ],
        "abstract": "Deep generative models parameterized by neural networks have recently achieved state-of-the-art performance in unsupervised and semi-supervised learning. We extend deep generative models with auxiliary variables which improves the variational approximation. The auxiliary variables leave the generative model unchanged but make the variational distribution more expressive. Inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections. Our findings suggest that more expressive and properly specified deep generative models converge faster with better results. We show state-of-the-art performance within semi-supervised learning on MNIST, SVHN and NORB datasets.\n    ",
        "submission_date": "2016-02-17T00:00:00",
        "last_modified_date": "2016-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05897",
        "title": "Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity",
        "authors": [
            "Amit Daniely",
            "Roy Frostig",
            "Yoram Singer"
        ],
        "abstract": "We develop a general duality between neural networks and compositional kernels, striving towards a better understanding of deep learning. We show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space. Hence, though the training objective is hard to optimize in the worst case, the initial weights form a good starting point for optimization. Our dual view also reveals a pragmatic and aesthetic perspective of neural networks and underscores their expressive power.\n    ",
        "submission_date": "2016-02-18T00:00:00",
        "last_modified_date": "2017-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06052",
        "title": "Strong Backdoors for Default Logic",
        "authors": [
            "Johannes K. Fichte",
            "Arne Meier",
            "Irina Schindler"
        ],
        "abstract": "In this paper, we introduce a notion of backdoors to Reiter's propositional default logic and study structural properties of it. Also we consider the problems of backdoor detection (parameterised by the solution size) as well as backdoor evaluation (parameterised by the size of the given backdoor), for various kinds of target classes (cnf, horn, krom, monotone, identity). We show that backdoor detection is fixed-parameter tractable for the considered target classes, and backdoor evaluation is either fixed-parameter tractable, in para-DP2 , or in para-NP, depending on the target class.\n    ",
        "submission_date": "2016-02-19T00:00:00",
        "last_modified_date": "2016-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06136",
        "title": "Ordonnancement d'entit\u00e9s pour la rencontre du web des documents et du web des donn\u00e9es",
        "authors": [
            "Mazen Alsarem",
            "Pierre-Edouard Portier",
            "Sylvie Calabretto",
            "Harald Kosch"
        ],
        "abstract": "The advances of the Linked Open Data (LOD) initiative are giving rise to a more structured web of data. Indeed, a few datasets act as hubs (e.g., DBpedia) connecting many other datasets. They also made possible new web services for entity detection inside plain text (e.g., DBpedia Spotlight), thus allowing for new applications that will benefit from a combination of the web of documents and the web of data. To ease the emergence of these new use-cases, we propose a query-biased algorithm for the ranking of entities detected inside a web page. Our algorithm combine link analysis with dimensionality reduction. We use crowdsourcing for building a publicly available and reusable dataset on which we compare our algorithm to the state of the art. Finally, we use this algorithm for the construction of semantic snippets for which we evaluate the usability and the usefulness with a crowdsourcing-based approach.\n    ",
        "submission_date": "2016-02-19T00:00:00",
        "last_modified_date": "2016-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06359",
        "title": "Text Matching as Image Recognition",
        "authors": [
            "Liang Pang",
            "Yanyan Lan",
            "Jiafeng Guo",
            "Jun Xu",
            "Shengxian Wan",
            "Xueqi Cheng"
        ],
        "abstract": "Matching two texts is a fundamental problem in many natural language processing tasks. An effective way is to extract meaningful matching patterns from words, phrases, and sentences to produce the matching score. Inspired by the success of convolutional neural network in image recognition, where neurons can capture many complicated patterns based on the extracted elementary visual patterns such as oriented edges and corners, we propose to model text matching as the problem of image recognition. Firstly, a matching matrix whose entries represent the similarities between words is constructed and viewed as an image. Then a convolutional neural network is utilized to capture rich matching patterns in a layer-by-layer way. We show that by resembling the compositional hierarchies of patterns in image recognition, our model can successfully identify salient signals such as n-gram and n-term matchings. Experimental results demonstrate its superiority against the baselines.\n    ",
        "submission_date": "2016-02-20T00:00:00",
        "last_modified_date": "2016-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06458",
        "title": "Causes for Query Answers from Databases, Datalog Abduction and View-Updates: The Presence of Integrity Constraints",
        "authors": [
            "Babak Salimi",
            "Leopoldo Bertossi"
        ],
        "abstract": "Causality has been recently introduced in databases, to model, characterize and possibly compute causes for query results (answers). Connections between queryanswer causality, consistency-based diagnosis, database repairs (wrt. integrity constraint violations), abductive diagnosis and the view-update problem have been established. In this work we further investigate connections between query-answer causality and abductive diagnosis and the view-update problem. In this context, we also define and investigate the notion of query-answer causality in the presence of integrity constraints.\n    ",
        "submission_date": "2016-02-20T00:00:00",
        "last_modified_date": "2016-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06483",
        "title": "Social planning for social HRI",
        "authors": [
            "Liz Sonenberg",
            "Tim Miller",
            "Adrian Pearce",
            "Paolo Felli",
            "Christian Muise",
            "Frank Dignum"
        ],
        "abstract": "Making a computational agent 'social' has implications for how it perceives itself and the environment in which it is situated, including the ability to recognise the behaviours of others. We point to recent work on social planning, i.e. planning in settings where the social context is relevant in the assessment of the beliefs and capabilities of others, and in making appropriate choices of what to do next.\n    ",
        "submission_date": "2016-02-21T00:00:00",
        "last_modified_date": "2016-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06522",
        "title": "Machine learning meets network science: dimensionality reduction for fast and efficient embedding of networks in the hyperbolic space",
        "authors": [
            "Josephine Maria Thomas",
            "Alessandro Muscoloni",
            "Sara Ciucci",
            "Ginestra Bianconi",
            "Carlo Vittorio Cannistraci"
        ],
        "abstract": "Complex network topologies and hyperbolic geometry seem specularly connected, and one of the most fascinating and challenging problems of recent complex network theory is to map a given network to its hyperbolic space. The Popularity Similarity Optimization (PSO) model represents - at the moment - the climax of this theory. It suggests that the trade-off between node popularity and similarity is a mechanism to explain how complex network topologies emerge - as discrete samples - from the continuous world of hyperbolic geometry. The hyperbolic space seems appropriate to represent real complex networks. In fact, it preserves many of their fundamental topological properties, and can be exploited for real applications such as, among others, link prediction and community detection. Here, we observe for the first time that a topological-based machine learning class of algorithms - for nonlinear unsupervised dimensionality reduction - can directly approximate the network's node angular coordinates of the hyperbolic model into a two-dimensional space, according to a similar topological organization that we named angular coalescence. On the basis of this phenomenon, we propose a new class of algorithms that offers fast and accurate coalescent embedding of networks in the hyperbolic space even for graphs with thousands of nodes.\n    ",
        "submission_date": "2016-02-21T00:00:00",
        "last_modified_date": "2016-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06539",
        "title": "Determining the best attributes for surveillance video keywords generation",
        "authors": [
            "Liangchen Liu",
            "Arnold Wiliem",
            "Shaokang Chen",
            "Kun Zhao",
            "Brian C. Lovell"
        ],
        "abstract": "Automatic video keyword generation is one of the key ingredients in reducing the burden of security officers in analyzing surveillance videos. Keywords or attributes are generally chosen manually based on expert knowledge of surveillance. Most existing works primarily aim at either supervised learning approaches relying on extensive manual labelling or hierarchical probabilistic models that assume the features are extracted using the bag-of-words approach; thus limiting the utilization of the other features. To address this, we turn our attention to automatic attribute discovery approaches. However, it is not clear which automatic discovery approach can discover the most meaningful attributes. Furthermore, little research has been done on how to compare and choose the best automatic attribute discovery methods. In this paper, we propose a novel approach, based on the shared structure exhibited amongst meaningful attributes, that enables us to compare between different automatic attribute discovery ",
        "submission_date": "2016-02-21T00:00:00",
        "last_modified_date": "2016-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06662",
        "title": "Recurrent Orthogonal Networks and Long-Memory Tasks",
        "authors": [
            "Mikael Henaff",
            "Arthur Szlam",
            "Yann LeCun"
        ],
        "abstract": "Although RNNs have been shown to be powerful tools for processing sequential data, finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets originally outlined in (Hochreiter and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illuminate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions furthermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices.\n    ",
        "submission_date": "2016-02-22T00:00:00",
        "last_modified_date": "2017-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06667",
        "title": "A Motion Planning Strategy for the Active Vision-Based Mapping of Ground-Level Structures",
        "authors": [
            "Manikandasriram Srinivasan Ramanagopal",
            "Andr\u00e9 Phu-Van Nguyen",
            "Jerome Le Ny"
        ],
        "abstract": "This paper presents a strategy to guide a mobile ground robot equipped with a camera or depth sensor, in order to autonomously map the visible part of a bounded three-dimensional structure. We describe motion planning algorithms that determine appropriate successive viewpoints and attempt to fill holes automatically in a point cloud produced by the sensing and perception layer. The emphasis is on accurately reconstructing a 3D model of a structure of moderate size rather than mapping large open environments, with applications for example in architecture, construction and inspection. The proposed algorithms do not require any initialization in the form of a mesh model or a bounding box, and the paths generated are well adapted to situations where the vision sensor is used simultaneously for mapping and for localizing the robot, in the absence of additional absolute positioning system. We analyze the coverage properties of our policy, and compare its performance to the classic frontier based exploration algorithm. We illustrate its efficacy for different structure sizes, levels of localization accuracy and range of the depth sensor, and validate our design on a real-world experiment.\n    ",
        "submission_date": "2016-02-22T00:00:00",
        "last_modified_date": "2017-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06897",
        "title": "Enablers and Inhibitors in Causal Justifications of Logic Programs",
        "authors": [
            "Pedro Cabalar",
            "Jorge Fandinno"
        ],
        "abstract": "To appear in Theory and Practice of Logic Programming (TPLP). In this paper we propose an extension of logic programming (LP) where each default literal derived from the well-founded model is associated to a justification represented as an algebraic expression. This expression contains both causal explanations (in the form of proof graphs built with rule labels) and terms under the scope of negation that stand for conditions that enable or disable the application of causal rules. Using some examples, we discuss how these new conditions, we respectively call \"enablers\" and \"inhibitors\", are intimately related to default negation and have an essentially different nature from regular cause-effect relations. The most important result is a formal comparison to the recent algebraic approaches for justifications in LP: \"Why-not Provenance\" (WnP) and \"Causal Graphs\" (CG). We show that the current approach extends both WnP and CG justifications under the Well-Founded Semantics and, as a byproduct, we also establish a formal relation between these two approaches.\n    ",
        "submission_date": "2016-02-22T00:00:00",
        "last_modified_date": "2016-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06977",
        "title": "Augur: Mining Human Behaviors from Fiction to Power Interactive Systems",
        "authors": [
            "Ethan Fast",
            "William McGrath",
            "Pranav Rajpurkar",
            "Michael Bernstein"
        ],
        "abstract": "From smart homes that prepare coffee when we wake, to phones that know not to interrupt us during important conversations, our collective visions of HCI imagine a future in which computers understand a broad range of human behaviors. Today our systems fall short of these visions, however, because this range of behaviors is too large for designers or programmers to capture manually. In this paper, we instead demonstrate it is possible to mine a broad knowledge base of human behavior by analyzing more than one billion words of modern fiction. Our resulting knowledge base, Augur, trains vector models that can predict many thousands of user activities from surrounding objects in modern contexts: for example, whether a user may be eating food, meeting with a friend, or taking a selfie. Augur uses these predictions to identify actions that people commonly take on objects in the world and estimate a user's future activities given their current situation. We demonstrate Augur-powered, activity-based systems such as a phone that silences itself when the odds of you answering it are low, and a dynamic music player that adjusts to your present activity. A field deployment of an Augur-powered wearable camera resulted in 96% recall and 71% precision on its unsupervised predictions of common daily activities. A second evaluation where human judges rated the system's predictions over a broad set of input images found that 94% were rated sensible.\n    ",
        "submission_date": "2016-02-22T00:00:00",
        "last_modified_date": "2016-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06979",
        "title": "Empath: Understanding Topic Signals in Large-Scale Text",
        "authors": [
            "Ethan Fast",
            "Binbin Chen",
            "Michael Bernstein"
        ],
        "abstract": "Human language is colored by a broad range of topics, but existing text analysis tools only focus on a small number of them. We present Empath, a tool that can generate and validate new lexical categories on demand from a small set of seed terms (like \"bleed\" and \"punch\" to generate the category violence). Empath draws connotations between words and phrases by deep learning a neural embedding across more than 1.8 billion words of modern fiction. Given a small set of seed words that characterize a category, Empath uses its neural embedding to discover new related terms, then validates the category with a crowd-powered filter. Empath also analyzes text across 200 built-in, pre-validated categories we have generated from common topics in our web dataset, like neglect, government, and social media. We show that Empath's data-driven, human validated categories are highly correlated (r=0.906) with similar categories in LIWC.\n    ",
        "submission_date": "2016-02-22T00:00:00",
        "last_modified_date": "2016-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07024",
        "title": "Moving Target Defense for Web Applications using Bayesian Stackelberg Games",
        "authors": [
            "Sailik Sengupta",
            "Satya Gautam Vadlamudi",
            "Subbarao Kambhampati",
            "Marthony Taguinod",
            "Adam Doup\u00e9",
            "Ziming Zhao",
            "Gail-Joon Ahn"
        ],
        "abstract": "The present complexity in designing web applications makes software security a difficult goal to achieve. An attacker can explore a deployed service on the web and attack at his/her own leisure. Moving Target Defense (MTD) in web applications is an effective mechanism to nullify this advantage of their reconnaissance but the framework demands a good switching strategy when switching between multiple configurations for its web-stack. To address this issue, we propose modeling of a real-world MTD web application as a repeated Bayesian game. We then formulate an optimization problem that generates an effective switching strategy while considering the cost of switching between different web-stack configurations. To incorporate this model into a developed MTD system, we develop an automated system for generating attack sets of Common Vulnerabilities and Exposures (CVEs) for input attacker types with predefined capabilities. Our framework obtains realistic reward values for the players (defenders and attackers) in this game by using security domain expertise on CVEs obtained from the National Vulnerability Database (NVD). We also address the issue of prioritizing vulnerabilities that when fixed, improves the security of the MTD system. Lastly, we demonstrate the robustness of our proposed model by evaluating its performance when there is uncertainty about input attacker information.\n    ",
        "submission_date": "2016-02-23T00:00:00",
        "last_modified_date": "2016-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07029",
        "title": "Latent Skill Embedding for Personalized Lesson Sequence Recommendation",
        "authors": [
            "Siddharth Reddy",
            "Igor Labutov",
            "Thorsten Joachims"
        ],
        "abstract": "Students in online courses generate large amounts of data that can be used to personalize the learning process and improve quality of education. In this paper, we present the Latent Skill Embedding (LSE), a probabilistic model of students and educational content that can be used to recommend personalized sequences of lessons with the goal of helping students prepare for specific assessments. Akin to collaborative filtering for recommender systems, the algorithm does not require students or content to be described by features, but it learns a representation using access traces. We formulate this problem as a regularized maximum-likelihood embedding of students, lessons, and assessments from historical student-content interactions. An empirical evaluation on large-scale data from Knewton, an adaptive learning technology company, shows that this approach predicts assessment results competitively with benchmark models and is able to discriminate between lesson sequences that lead to mastery and failure.\n    ",
        "submission_date": "2016-02-23T00:00:00",
        "last_modified_date": "2016-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07064",
        "title": "SIFT: An Algorithm for Extracting Structural Information From Taxonomies",
        "authors": [
            "Jorge Martinez-Gil"
        ],
        "abstract": "In this work we present SIFT, a 3-step algorithm for the analysis of the structural information represented by means of a taxonomy. The major advantage of this algorithm is the capability to leverage the information inherent to the hierarchical structures of taxonomies to infer correspondences which can allow to merge them in a later step. This method is particular relevant in scenarios where taxonomy alignment techniques exploiting textual information from taxonomy nodes cannot operate successfully.\n    ",
        "submission_date": "2016-02-23T00:00:00",
        "last_modified_date": "2016-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07332",
        "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
        "authors": [
            "Ranjay Krishna",
            "Yuke Zhu",
            "Oliver Groth",
            "Justin Johnson",
            "Kenji Hata",
            "Joshua Kravitz",
            "Stephanie Chen",
            "Yannis Kalantidis",
            "Li-Jia Li",
            "David A. Shamma",
            "Michael S. Bernstein",
            "Fei-Fei Li"
        ],
        "abstract": "Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked \"What vehicle is the person riding?\", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that \"the person is riding a horse-drawn carriage\".\n",
        "submission_date": "2016-02-23T00:00:00",
        "last_modified_date": "2016-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07360",
        "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size",
        "authors": [
            "Forrest N. Iandola",
            "Song Han",
            "Matthew W. Moskewicz",
            "Khalid Ashraf",
            "William J. Dally",
            "Kurt Keutzer"
        ],
        "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).\n",
        "submission_date": "2016-02-24T00:00:00",
        "last_modified_date": "2016-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07362",
        "title": "The Possibilities and Limitations of Private Prediction Markets",
        "authors": [
            "Rachel Cummings",
            "David M. Pennock",
            "Jennifer Wortman Vaughan"
        ],
        "abstract": "We consider the design of private prediction markets, financial markets designed to elicit predictions about uncertain events without revealing too much information about market participants' actions or beliefs. Our goal is to design market mechanisms in which participants' trades or wagers influence the market's behavior in a way that leads to accurate predictions, yet no single participant has too much influence over what others are able to observe. We study the possibilities and limitations of such mechanisms using tools from differential privacy. We begin by designing a private one-shot wagering mechanism in which bettors specify a belief about the likelihood of a future event and a corresponding monetary wager. Wagers are redistributed among bettors in a way that more highly rewards those with accurate predictions. We provide a class of wagering mechanisms that are guaranteed to satisfy truthfulness, budget balance in expectation, and other desirable properties while additionally guaranteeing epsilon-joint differential privacy in the bettors' reported beliefs, and analyze the trade-off between the achievable level of privacy and the sensitivity of a bettor's payment to her own report. We then ask whether it is possible to obtain privacy in dynamic prediction markets, focusing our attention on the popular cost-function framework in which securities with payments linked to future events are bought and sold by an automated market maker. We show that under general conditions, it is impossible for such a market maker to simultaneously achieve bounded worst-case loss and epsilon-differential privacy without allowing the privacy guarantee to degrade extremely quickly as the number of trades grows, making such markets impractical in settings in which privacy is valued. We conclude by suggesting several avenues for potentially circumventing this lower bound.\n    ",
        "submission_date": "2016-02-24T00:00:00",
        "last_modified_date": "2016-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07435",
        "title": "Parametric Prediction from Parametric Agents",
        "authors": [
            "Yuan Luo",
            "Nihar B. Shah",
            "Jianwei Huang",
            "Jean Walrand"
        ],
        "abstract": "We consider a problem of prediction based on opinions elicited from heterogeneous rational agents with private information. Making an accurate prediction with a minimal cost requires a joint design of the incentive mechanism and the prediction algorithm. Such a problem lies at the nexus of statistical learning theory and game theory, and arises in many domains such as consumer surveys and mobile crowdsourcing. In order to elicit heterogeneous agents' private information and incentivize agents with different capabilities to act in the principal's best interest, we design an optimal joint incentive mechanism and prediction algorithm called COPE (COst and Prediction Elicitation), the analysis of which offers several valuable engineering insights. First, when the costs incurred by the agents are linear in the exerted effort, COPE corresponds to a \"crowd contending\" mechanism, where the principal only employs the agent with the highest capability. Second, when the costs are quadratic, COPE corresponds to a \"crowd-sourcing\" mechanism that employs multiple agents with different capabilities at the same time. Numerical simulations show that COPE improves the principal's profit and the network profit significantly (larger than 30% in our simulations), comparing to those mechanisms that assume all agents have equal capabilities.\n    ",
        "submission_date": "2016-02-24T00:00:00",
        "last_modified_date": "2016-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07563",
        "title": "Multilingual Twitter Sentiment Classification: The Role of Human Annotators",
        "authors": [
            "Igor Mozetic",
            "Miha Grcar",
            "Jasmina Smailovic"
        ],
        "abstract": "What are the limits of automated Twitter sentiment classification? We analyze a large set of manually labeled tweets in different languages, use them as training data, and construct automated classification models. It turns out that the quality of classification models depends much more on the quality and size of training data than on the type of the model trained. Experimental results indicate that there is no statistically significant difference between the performance of the top classification models. We quantify the quality of training data by applying various annotator agreement measures, and identify the weakest points of different datasets. We show that the model performance approaches the inter-annotator agreement when the size of the training set is sufficiently large. However, it is crucial to regularly monitor the self- and inter-annotator agreements since this improves the training datasets and consequently the model performance. Finally, we show that there is strong evidence that humans perceive the sentiment classes (negative, neutral, and positive) as ordered.\n    ",
        "submission_date": "2016-02-24T00:00:00",
        "last_modified_date": "2016-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07637",
        "title": "A Survey on Domain-Specific Languages for Machine Learning in Big Data",
        "authors": [
            "Ivens Portugal",
            "Paulo Alencar",
            "Donald Cowan"
        ],
        "abstract": "The amount of data generated in the modern society is increasing rapidly. New problems and novel approaches of data capture, storage, analysis and visualization are responsible for the emergence of the Big Data research field. Machine Learning algorithms can be used in Big Data to make better and more accurate inferences. However, because of the challenges Big Data imposes, these algorithms need to be adapted and optimized to specific applications. One important decision made by software engineers is the choice of the language that is used in the implementation of these algorithms. Therefore, this literature survey identifies and describes domain-specific languages and frameworks used for Machine Learning in Big Data. By doing this, software engineers can then make more informed choices and beginners have an overview of the main languages used in this domain.\n    ",
        "submission_date": "2016-02-24T00:00:00",
        "last_modified_date": "2016-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07714",
        "title": "Learning values across many orders of magnitude",
        "authors": [
            "Hado van Hasselt",
            "Arthur Guez",
            "Matteo Hessel",
            "Volodymyr Mnih",
            "David Silver"
        ],
        "abstract": "Most learning algorithms are not invariant to the scale of the function that is being approximated. We propose to adaptively normalize the targets used in learning. This is useful in value-based reinforcement learning, where the magnitude of appropriate value approximations can change over time when we update the policy of behavior. Our main motivation is prior work on learning to play Atari games, where the rewards were all clipped to a predetermined range. This clipping facilitates learning across many different games with a single learning algorithm, but a clipped reward function can result in qualitatively different behavior. Using the adaptive normalization we can remove this domain-specific heuristic without diminishing overall performance.\n    ",
        "submission_date": "2016-02-24T00:00:00",
        "last_modified_date": "2016-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07783",
        "title": "Top-N Recommendation with Novel Rank Approximation",
        "authors": [
            "Zhao Kang",
            "Qiang Cheng"
        ],
        "abstract": "The importance of accurate recommender systems has been widely recognized by academia and industry. However, the recommendation quality is still rather low. Recently, a linear sparse and low-rank representation of the user-item matrix has been applied to produce Top-N recommendations. This approach uses the nuclear norm as a convex relaxation for the rank function and has achieved better recommendation accuracy than the state-of-the-art methods. In the past several years, solving rank minimization problems by leveraging nonconvex relaxations has received increasing attention. Some empirical results demonstrate that it can provide a better approximation to original problems than convex relaxation. In this paper, we propose a novel rank approximation to enhance the performance of Top-N recommendation systems, where the approximation error is controllable. Experimental results on real data show that the proposed rank approximation improves the Top-$N$ recommendation accuracy substantially.\n    ",
        "submission_date": "2016-02-25T00:00:00",
        "last_modified_date": "2016-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07868",
        "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks",
        "authors": [
            "Tim Salimans",
            "Diederik P. Kingma"
        ],
        "abstract": "We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.\n    ",
        "submission_date": "2016-02-25T00:00:00",
        "last_modified_date": "2016-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07905",
        "title": "Thompson Sampling is Asymptotically Optimal in General Environments",
        "authors": [
            "Jan Leike",
            "Tor Lattimore",
            "Laurent Orseau",
            "Marcus Hutter"
        ],
        "abstract": "We discuss a variant of Thompson sampling for nonparametric reinforcement learning in a countable classes of general stochastic environments. These environments can be non-Markov, non-ergodic, and partially observable. We show that Thompson sampling learns the environment class in the sense that (1) asymptotically its value converges to the optimal value in mean and (2) given a recoverability assumption regret is sublinear.\n    ",
        "submission_date": "2016-02-25T00:00:00",
        "last_modified_date": "2016-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.08159",
        "title": "Harnessing disordered quantum dynamics for machine learning",
        "authors": [
            "Keisuke Fujii",
            "Kohei Nakajima"
        ],
        "abstract": "Quantum computer has an amazing potential of fast information processing. However, realisation of a digital quantum computer is still a challenging problem requiring highly accurate controls and key application strategies. Here we propose a novel platform, quantum reservoir computing, to solve these issues successfully by exploiting natural quantum dynamics, which is ubiquitous in laboratories nowadays, for machine learning. In this framework, nonlinear dynamics including classical chaos can be universally emulated in quantum systems. A number of numerical experiments show that quantum systems consisting of at most seven qubits possess computational capabilities comparable to conventional recurrent neural networks of 500 nodes. This discovery opens up a new paradigm for information processing with artificial intelligence powered by quantum physics.\n    ",
        "submission_date": "2016-02-26T00:00:00",
        "last_modified_date": "2016-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.08199",
        "title": "Category Theoretic Analysis of Photon-based Decision Making",
        "authors": [
            "Makoto Naruse",
            "Song-Ju Kim",
            "Masashi Aono",
            "Martin Berthel",
            "Aur\u00e9lien Drezet",
            "Serge Huant",
            "Hirokazu Hori"
        ],
        "abstract": "Decision making is a vital function in this age of machine learning and artificial intelligence, yet its physical realization and theoretical fundamentals are still not completely understood. In our former study, we demonstrated that single-photons can be used to make decisions in uncertain, dynamically changing environments. The two-armed bandit problem was successfully solved using the dual probabilistic and particle attributes of single photons. In this study, we present a category theoretic modeling and analysis of single-photon-based decision making, including a quantitative analysis that is in agreement with the experimental results. A category theoretic model reveals the complex interdependencies of subject matter entities in a simplified manner, even in dynamically changing environments. In particular, the octahedral and braid structures in triangulated categories provide a better understanding and quantitative metrics of the underlying mechanisms of a single-photon decision maker. This study provides both insight and a foundation for analyzing more complex and uncertain problems, to further machine learning and artificial intelligence.\n    ",
        "submission_date": "2016-02-26T00:00:00",
        "last_modified_date": "2018-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.08350",
        "title": "Large-Scale Detection of Non-Technical Losses in Imbalanced Data Sets",
        "authors": [
            "Patrick O. Glauner",
            "Andre Boechat",
            "Lautaro Dolberg",
            "Radu State",
            "Franck Bettinger",
            "Yves Rangoni",
            "Diogo Duarte"
        ],
        "abstract": "Non-technical losses (NTL) such as electricity theft cause significant harm to our economies, as in some countries they may range up to 40% of the total electricity distributed. Detecting NTLs requires costly on-site inspections. Accurate prediction of NTLs for customers using machine learning is therefore crucial. To date, related research largely ignore that the two classes of regular and non-regular customers are highly imbalanced, that NTL proportions may change and mostly consider small data sets, often not allowing to deploy the results in production. In this paper, we present a comprehensive approach to assess three NTL detection models for different NTL proportions in large real world data sets of 100Ks of customers: Boolean rules, fuzzy logic and Support Vector Machine. This work has resulted in appreciable results that are about to be deployed in a leading industry solution. We believe that the considerations and observations made in this contribution are necessary for future smart meter research in order to report their effectiveness on imbalanced and large real world data sets.\n    ",
        "submission_date": "2016-02-26T00:00:00",
        "last_modified_date": "2017-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.08671",
        "title": "Lie Access Neural Turing Machine",
        "authors": [
            "Greg Yang"
        ],
        "abstract": "Following the recent trend in explicit neural memory structures, we present a new design of an external memory, wherein memories are stored in an Euclidean key space $\\mathbb R^n$. An LSTM controller performs read and write via specialized read and write heads. It can move a head by either providing a new address in the key space (aka random access) or moving from its previous position via a Lie group action (aka Lie access). In this way, the \"L\" and \"R\" instructions of a traditional Turing Machine are generalized to arbitrary elements of a fixed Lie group action. For this reason, we name this new model the Lie Access Neural Turing Machine, or LANTM.\n",
        "submission_date": "2016-02-28T00:00:00",
        "last_modified_date": "2016-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.08771",
        "title": "Investigating practical linear temporal difference learning",
        "authors": [
            "Adam White",
            "Martha White"
        ],
        "abstract": "Off-policy reinforcement learning has many applications including: learning from demonstration, learning multiple goal seeking policies in parallel, and representing predictive knowledge. Recently there has been an proliferation of new policy-evaluation algorithms that fill a longstanding algorithmic void in reinforcement learning: combining robustness to off-policy sampling, function approximation, linear complexity, and temporal difference (TD) updates. This paper contains two main contributions. First, we derive two new hybrid TD policy-evaluation algorithms, which fill a gap in this collection of algorithms. Second, we perform an empirical comparison to elicit which of these new linear TD methods should be preferred in different situations, and make concrete suggestions about practical use.\n    ",
        "submission_date": "2016-02-28T00:00:00",
        "last_modified_date": "2016-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.08903",
        "title": "Range-based argumentation semantics as 2-valued models",
        "authors": [
            "Mauricio Osorio",
            "Juan Carlos Nieves"
        ],
        "abstract": "Characterizations of semi-stable and stage extensions in terms of 2-valued logical models are presented. To this end, the so-called GL-supported and GL-stage models are defined. These two classes of logical models are logic programming counterparts of the notion of range which is an established concept in argumentation semantics.\n    ",
        "submission_date": "2016-02-29T00:00:00",
        "last_modified_date": "2016-02-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.09118",
        "title": "Easy Monotonic Policy Iteration",
        "authors": [
            "Joshua Achiam"
        ],
        "abstract": "A key problem in reinforcement learning for control with general function approximators (such as deep neural networks and other nonlinear functions) is that, for many algorithms employed in practice, updates to the policy or $Q$-function may fail to improve performance---or worse, actually cause the policy performance to degrade. Prior work has addressed this for policy iteration by deriving tight policy improvement bounds; by optimizing the lower bound on policy improvement, a better policy is guaranteed. However, existing approaches suffer from bounds that are hard to optimize in practice because they include sup norm terms which cannot be efficiently estimated or differentiated. In this work, we derive a better policy improvement bound where the sup norm of the policy divergence has been replaced with an average divergence; this leads to an algorithm, Easy Monotonic Policy Iteration, that generates sequences of policies with guaranteed non-decreasing returns and is easy to implement in a sample-based framework.\n    ",
        "submission_date": "2016-02-29T00:00:00",
        "last_modified_date": "2016-02-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.00448",
        "title": "Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization",
        "authors": [
            "Chelsea Finn",
            "Sergey Levine",
            "Pieter Abbeel"
        ],
        "abstract": "Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.\n    ",
        "submission_date": "2016-03-01T00:00:00",
        "last_modified_date": "2016-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.00709",
        "title": "Probabilistic Relational Model Benchmark Generation",
        "authors": [
            "Mouna Ben Ishak",
            "Rajani Chulyadyo",
            "Philippe Leray"
        ],
        "abstract": "The validation of any database mining methodology goes through an evaluation process where benchmarks availability is essential. In this paper, we aim to randomly generate relational database benchmarks that allow to check probabilistic dependencies among the attributes. We are particularly interested in Probabilistic Relational Models (PRMs), which extend Bayesian Networks (BNs) to a relational data mining context and enable effective and robust reasoning over relational data. Even though a panoply of works have focused, separately , on the generation of random Bayesian networks and relational databases, no work has been identified for PRMs on that track. This paper provides an algorithmic approach for generating random PRMs from scratch to fill this gap. The proposed method allows to generate PRMs as well as synthetic relational data from a randomly generated relational schema and a random set of probabilistic dependencies. This can be of interest not only for machine learning researchers to evaluate their proposals in a common framework, but also for databases designers to evaluate the effectiveness of the components of a database management system.\n    ",
        "submission_date": "2016-03-02T00:00:00",
        "last_modified_date": "2016-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.00748",
        "title": "Continuous Deep Q-Learning with Model-based Acceleration",
        "authors": [
            "Shixiang Gu",
            "Timothy Lillicrap",
            "Ilya Sutskever",
            "Sergey Levine"
        ],
        "abstract": "Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions. However, the sample complexity of model-free algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks. We propose two complementary techniques for improving the efficiency of such algorithms. First, we derive a continuous variant of the Q-learning algorithm, which we call normalized adantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning. We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable.\n    ",
        "submission_date": "2016-03-02T00:00:00",
        "last_modified_date": "2016-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.00788",
        "title": "Automatic Differentiation Variational Inference",
        "authors": [
            "Alp Kucukelbir",
            "Dustin Tran",
            "Rajesh Ranganath",
            "Andrew Gelman",
            "David M. Blei"
        ],
        "abstract": "Probabilistic modeling is iterative. A scientist posits a simple model, fits it to her data, refines it according to her analysis, and repeats. However, fitting complex models to large data is a bottleneck in this process. Deriving algorithms for new models can be both mathematically and computationally challenging, which makes it difficult to efficiently cycle through the steps. To this end, we develop automatic differentiation variational inference (ADVI). Using our method, the scientist only provides a probabilistic model and a dataset, nothing else. ADVI automatically derives an efficient variational inference algorithm, freeing the scientist to refine and explore many models. ADVI supports a broad class of models-no conjugacy assumptions are required. We study ADVI across ten different models and apply it to a dataset with millions of observations. ADVI is integrated into Stan, a probabilistic programming system; it is available for immediate use.\n    ",
        "submission_date": "2016-03-02T00:00:00",
        "last_modified_date": "2016-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.00806",
        "title": "Hybrid Collaborative Filtering with Autoencoders",
        "authors": [
            "Florian Strub",
            "Jeremie Mary",
            "Romaric Gaudel"
        ],
        "abstract": "Collaborative Filtering aims at exploiting the feedback of users to provide personalised recommendations. Such algorithms look for latent variables in a large sparse matrix of ratings. They can be enhanced by adding side information to tackle the well-known cold start problem. While Neu-ral Networks have tremendous success in image and speech recognition, they have received less attention in Collaborative Filtering. This is all the more surprising that Neural Networks are able to discover latent variables in large and heterogeneous datasets. In this paper, we introduce a Collaborative Filtering Neural network architecture aka CFN which computes a non-linear Matrix Factorization from sparse rating inputs and side information. We show experimentally on the MovieLens and Douban dataset that CFN outper-forms the state of the art and benefits from side information. We provide an implementation of the algorithm as a reusable plugin for Torch, a popular Neural Network framework.\n    ",
        "submission_date": "2016-03-02T00:00:00",
        "last_modified_date": "2016-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.00964",
        "title": "Object Manipulation Learning by Imitation",
        "authors": [
            "Zhen Zeng",
            "Benjamin Kuipers"
        ],
        "abstract": "We aim to enable robot to learn object manipulation by imitation. Given external observations of demonstrations on object manipulations, we believe that two underlying problems to address in learning by imitation is 1) segment a given demonstration into skills that can be individually learned and reused, and 2) formulate the correct RL (Reinforcement Learning) problem that only considers the relevant aspects of each skill so that the policy for each skill can be effectively learned. Previous works made certain progress in this direction, but none has taken private information into account. The public information is the information that is available in the external observations of demonstration, and the private information is the information that are only available to the agent that executes the actions, such as tactile sensations. Our contribution is that we provide a method for the robot to automatically segment the demonstration of object manipulations into multiple skills, and formulate the correct RL problem for each skill, and automatically decide whether the private information is an important aspect of each skill based on interaction with the world. Our experiment shows that our robot learns to pick up a block, and stack it onto another block by imitating an observed demonstration. The evaluation is based on 1) whether the demonstration is reasonably segmented, 2) whether the correct RL problems are formulated, 3) and whether a good policy is learned.\n    ",
        "submission_date": "2016-03-03T00:00:00",
        "last_modified_date": "2017-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01006",
        "title": "Automatic learning of gait signatures for people identification",
        "authors": [
            "F.M. Castro",
            "M.J. Marin-Jimenez",
            "N. Guil",
            "N. Perez de la Blanca"
        ],
        "abstract": "This work targets people identification in video based on the way they walk (i.e. gait). While classical methods typically derive gait signatures from sequences of binary silhouettes, in this work we explore the use of convolutional neural networks (CNN) for learning high-level descriptors from low-level motion features (i.e. optical flow components). We carry out a thorough experimental evaluation of the proposed CNN architecture on the challenging TUM-GAID dataset. The experimental results indicate that using spatio-temporal cuboids of optical flow as input data for CNN allows to obtain state-of-the-art results on the gait task with an image resolution eight times lower than the previously reported results (i.e. 80x60 pixels).\n    ",
        "submission_date": "2016-03-03T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01067",
        "title": "Modeling the Sequence of Brain Volumes by Local Mesh Models for Brain Decoding",
        "authors": [
            "Itir Onal",
            "Mete Ozay",
            "Eda Mizrak",
            "Ilke Oztekin",
            "Fatos T. Yarman Vural"
        ],
        "abstract": "We represent the sequence of fMRI (Functional Magnetic Resonance Imaging) brain volumes recorded during a cognitive stimulus by a graph which consists of a set of local meshes. The corresponding cognitive process, encoded in the brain, is then represented by these meshes each of which is estimated assuming a linear relationship among the voxel time series in a predefined locality. First, we define the concept of locality in two neighborhood systems, namely, the spatial and functional neighborhoods. Then, we construct spatially and functionally local meshes around each voxel, called seed voxel, by connecting it either to its spatial or functional p-nearest neighbors. The mesh formed around a voxel is a directed sub-graph with a star topology, where the direction of the edges is taken towards the seed voxel at the center of the mesh. We represent the time series recorded at each seed voxel in terms of linear combination of the time series of its p-nearest neighbors in the mesh. The relationships between a seed voxel and its neighbors are represented by the edge weights of each mesh, and are estimated by solving a linear regression equation. The estimated mesh edge weights lead to a better representation of information in the brain for encoding and decoding of the cognitive tasks. We test our model on a visual object recognition and emotional memory retrieval experiments using Support Vector Machines that are trained using the mesh edge weights as features. In the experimental analysis, we observe that the edge weights of the spatial and functional meshes perform better than the state-of-the-art brain decoding models.\n    ",
        "submission_date": "2016-03-03T00:00:00",
        "last_modified_date": "2016-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01121",
        "title": "Deep Reinforcement Learning from Self-Play in Imperfect-Information Games",
        "authors": [
            "Johannes Heinrich",
            "David Silver"
        ],
        "abstract": "Many real-world applications can be described as large-scale games of imperfect information. To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain. In this paper we introduce the first scalable end-to-end approach to learning approximate Nash equilibria without prior domain knowledge. Our method combines fictitious self-play with deep reinforcement learning. When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged. In Limit Texas Holdem, a poker game of real-world scale, NFSP learnt a strategy that approached the performance of state-of-the-art, superhuman algorithms based on significant domain expertise.\n    ",
        "submission_date": "2016-03-03T00:00:00",
        "last_modified_date": "2016-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01250",
        "title": "Decision Forests, Convolutional Networks and the Models in-Between",
        "authors": [
            "Yani Ioannou",
            "Duncan Robertson",
            "Darko Zikic",
            "Peter Kontschieder",
            "Jamie Shotton",
            "Matthew Brown",
            "Antonio Criminisi"
        ],
        "abstract": "This paper investigates the connections between two state of the art classifiers: decision forests (DFs, including decision jungles) and convolutional neural networks (CNNs). Decision forests are computationally efficient thanks to their conditional computation property (computation is confined to only a small region of the tree, the nodes along a single branch). CNNs achieve state of the art accuracy, thanks to their representation learning capabilities. We present a systematic analysis of how to fuse conditional computation with representation learning and achieve a continuum of hybrid models with different ratios of accuracy vs. efficiency. We call this new family of hybrid models conditional networks. Conditional networks can be thought of as: i) decision trees augmented with data transformation operators, or ii) CNNs, with block-diagonal sparse weight matrices, and explicit data routing functions. Experimental validation is performed on the common task of image classification on both the CIFAR and Imagenet datasets. Compared to state of the art CNNs, our hybrid models yield the same accuracy with a fraction of the compute cost and much smaller number of parameters.\n    ",
        "submission_date": "2016-03-03T00:00:00",
        "last_modified_date": "2016-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01524",
        "title": "Analyzing Games with Ambiguous Player Types using the ${\\rm MINthenMAX}$ Decision Model",
        "authors": [
            "Ilan Nehama"
        ],
        "abstract": "In many common interactive scenarios, participants lack information about other participants, and specifically about the preferences of other participants. In this work, we model an extreme case of incomplete information, which we term games with type ambiguity, where a participant lacks even information enabling him to form a belief on the preferences of others. Under type ambiguity, one cannot analyze the scenario using the commonly used Bayesian framework, and therefore he needs to model the participants using a different decision model.\n",
        "submission_date": "2016-03-04T00:00:00",
        "last_modified_date": "2016-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01570",
        "title": "Coordination Event Detection and Initiator Identification in Time Series Data",
        "authors": [
            "Chainarong Amornbunchornvej",
            "Ivan Brugere",
            "Ariana Strandburg-Peshkin",
            "Damien Farine",
            "Margaret C. Crofoot",
            "Tanya Y. Berger-Wolf"
        ],
        "abstract": "Behavior initiation is a form of leadership and is an important aspect of social organization that affects the processes of group formation, dynamics, and decision-making in human societies and other social animal species. In this work, we formalize the \"Coordination Initiator Inference Problem\" and propose a simple yet powerful framework for extracting periods of coordinated activity and determining individuals who initiated this coordination, based solely on the activity of individuals within a group during those periods. The proposed approach, given arbitrary individual time series, automatically (1) identifies times of coordinated group activity, (2) determines the identities of initiators of those activities, and (3) classifies the likely mechanism by which the group coordination occurred, all of which are novel computational tasks. We demonstrate our framework on both simulated and real-world data: trajectories tracking of animals as well as stock market data. Our method is competitive with existing global leadership inference methods but provides the first approaches for local leadership and coordination mechanism classification. Our results are consistent with ground-truthed biological data and the framework finds many known events in financial data which are not otherwise reflected in the aggregate NASDAQ index. Our method is easily generalizable to any coordinated time-series data from interacting entities.\n    ",
        "submission_date": "2016-03-04T00:00:00",
        "last_modified_date": "2019-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01595",
        "title": "Sentiment Analysis in Scholarly Book Reviews",
        "authors": [
            "Hussam Hamdan",
            "Patrice Bellot",
            "Frederic Bechet"
        ],
        "abstract": "So far different studies have tackled the sentiment analysis in several domains such as restaurant and movie reviews. But, this problem has not been studied in scholarly book reviews which is different in terms of review style and size. In this paper, we propose to combine different features in order to be presented to a supervised classifiers which extract the opinion target expressions and detect their polarities in scholarly book reviews. We construct a labeled corpus for training and evaluating our methods in French book reviews. We also evaluate them on English restaurant reviews in order to measure their robustness across the domains and languages. The evaluation shows that our methods are enough robust for English restaurant reviews and French book reviews.\n    ",
        "submission_date": "2016-03-04T00:00:00",
        "last_modified_date": "2016-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01770",
        "title": "An Argument-based Creative Assistant for Harmonic Blending",
        "authors": [
            "Maximos Kaliakatsos-Papakostas",
            "Roberto Confalonieri",
            "Joseph Corneli",
            "Asterios Zacharakis",
            "Emilios Cambouropoulos"
        ],
        "abstract": "Conceptual blending is a powerful tool for computational creativity where, for example, the properties of two harmonic spaces may be combined in a consistent manner to produce a novel harmonic space. However, deciding about the importance of property features in the input spaces and evaluating the results of conceptual blending is a nontrivial task. In the specific case of musical harmony, defining the salient features of chord transitions and evaluating invented harmonic spaces requires deep musicological background knowledge. In this paper, we propose a creative tool that helps musicologists to evaluate and to enhance harmonic innovation. This tool allows a music expert to specify arguments over given transition properties. These arguments are then considered by the system when defining combinations of features in an idiom-blending process. A music expert can assess whether the new harmonic idiom makes musicological sense and re-adjust the arguments (selection of features) to explore alternative blends that can potentially produce better harmonic spaces. We conclude with a discussion of future work that would further automate the harmonisation process.\n    ",
        "submission_date": "2016-03-06T00:00:00",
        "last_modified_date": "2016-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01882",
        "title": "Composing inference algorithms as program transformations",
        "authors": [
            "Robert Zinkov",
            "Chung-chieh Shan"
        ],
        "abstract": "Probabilistic inference procedures are usually coded painstakingly from scratch, for each target model and each inference algorithm. We reduce this effort by generating inference procedures from models automatically. We make this code generation modular by decomposing inference algorithms into reusable program-to-program transformations. These transformations perform exact inference as well as generate probabilistic programs that compute expectations, densities, and MCMC samples. The resulting inference procedures are about as accurate and fast as other probabilistic programming systems on real-world problems.\n    ",
        "submission_date": "2016-03-06T00:00:00",
        "last_modified_date": "2017-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.02028",
        "title": "Adaptive Visualisation System for Construction Building Information Models Using Saliency",
        "authors": [
            "Hugo Martin",
            "Sylvain Chevallier",
            "Eric Monacelli"
        ],
        "abstract": "Building Information Modeling (BIM) is a recent construction process based on a 3D model, containing every component related to the building achievement. Architects, structure engineers, method engineers, and others participant to the building process work on this model through the design-to-construction cycle. The high complexity and the large amount of information included in these models raise several issues, delaying its wide adoption in the industrial world. One of the most important is the visualization: professionals have difficulties to find out the relevant information for their job. Actual solutions suffer from two limitations: the BIM models information are processed manually and insignificant information are simply hidden, leading to inconsistencies in the building model. This paper describes a system relying on an ontological representation of the building information to label automatically the building elements. Depending on the user's department, the visualization is modified according to these labels by automatically adjusting the colors and image properties based on a saliency model. The proposed saliency model incorporates several adaptations to fit the specificities of architectural images.\n    ",
        "submission_date": "2016-03-07T00:00:00",
        "last_modified_date": "2016-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.02038",
        "title": "Unscented Bayesian Optimization for Safe Robot Grasping",
        "authors": [
            "Jos\u00e9 Nogueira",
            "Ruben Martinez-Cantin",
            "Alexandre Bernardino",
            "Lorenzo Jamone"
        ],
        "abstract": "We address the robot grasp optimization problem of unknown objects considering uncertainty in the input space. Grasping unknown objects can be achieved by using a trial and error exploration strategy. Bayesian optimization is a sample efficient optimization algorithm that is especially suitable for this setups as it actively reduces the number of trials for learning about the function to optimize. In fact, this active object exploration is the same strategy that infants do to learn optimal grasps. One problem that arises while learning grasping policies is that some configurations of grasp parameters may be very sensitive to error in the relative pose between the object and robot end-effector. We call these configurations unsafe because small errors during grasp execution may turn good grasps into bad grasps. Therefore, to reduce the risk of grasp failure, grasps should be planned in safe areas. We propose a new algorithm, Unscented Bayesian optimization that is able to perform sample efficient optimization while taking into consideration input noise to find safe optima. The contribution of Unscented Bayesian optimization is twofold as if provides a new decision process that drives exploration to safe regions and a new selection procedure that chooses the optimal in terms of its safety without extra analysis or computational cost. Both contributions are rooted on the strong theory behind the unscented transformation, a popular nonlinear approximation method. We show its advantages with respect to the classical Bayesian optimization both in synthetic problems and in realistic robot grasp simulations. The results highlights that our method achieves optimal and robust grasping policies after few trials while the selected grasps remain in safe regions.\n    ",
        "submission_date": "2016-03-07T00:00:00",
        "last_modified_date": "2016-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.02199",
        "title": "Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection",
        "authors": [
            "Sergey Levine",
            "Peter Pastor",
            "Alex Krizhevsky",
            "Deirdre Quillen"
        ],
        "abstract": "We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.\n    ",
        "submission_date": "2016-03-07T00:00:00",
        "last_modified_date": "2016-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.02626",
        "title": "UTA-poly and UTA-splines: additive value functions with polynomial marginals",
        "authors": [
            "Olivier Sobrie",
            "Nicolas Gillis",
            "Vincent Mousseau",
            "Marc Pirlot"
        ],
        "abstract": "Additive utility function models are widely used in multiple criteria decision analysis. In such models, a numerical value is associated to each alternative involved in the decision problem. It is computed by aggregating the scores of the alternative on the different criteria of the decision problem. The score of an alternative is determined by a marginal value function that evolves monotonically as a function of the performance of the alternative on this criterion. Determining the shape of the marginals is not easy for a decision maker. It is easier for him/her to make statements such as \"alternative $a$ is preferred to $b$\". In order to help the decision maker, UTA disaggregation procedures use linear programming to approximate the marginals by piecewise linear functions based only on such statements. In this paper, we propose to infer polynomials and splines instead of piecewise linear functions for the marginals. In this aim, we use semidefinite programming instead of linear programming. We illustrate this new elicitation method and present some experimental results.\n    ",
        "submission_date": "2016-03-05T00:00:00",
        "last_modified_date": "2016-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.02740",
        "title": "Pairwise Choice Markov Chains",
        "authors": [
            "Stephen Ragain",
            "Johan Ugander"
        ],
        "abstract": "As datasets capturing human choices grow in richness and scale -- particularly in online domains -- there is an increasing need for choice models that escape traditional choice-theoretic axioms such as regularity, stochastic transitivity, and Luce's choice axiom. In this work we introduce the Pairwise Choice Markov Chain (PCMC) model of discrete choice, an inferentially tractable model that does not assume any of the above axioms while still satisfying the foundational axiom of uniform expansion, a considerably weaker assumption than Luce's choice axiom. We show that the PCMC model significantly outperforms the Multinomial Logit (MNL) model in prediction tasks on both synthetic and empirical datasets known to exhibit violations of Luce's axiom. Our analysis also synthesizes several recent observations connecting the Multinomial Logit model and Markov chains; the PCMC model retains the Multinomial Logit model as a special case.\n    ",
        "submission_date": "2016-03-08T00:00:00",
        "last_modified_date": "2021-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.02776",
        "title": "Implicit Discourse Relation Classification via Multi-Task Neural Networks",
        "authors": [
            "Yang Liu",
            "Sujian Li",
            "Xiaodong Zhang",
            "Zhifang Sui"
        ],
        "abstract": "Without discourse connectives, classifying implicit discourse relations is a challenging task and a bottleneck for building a practical discourse parser. Previous research usually makes use of one kind of discourse framework such as PDTB or RST to improve the classification performance on discourse relations. Actually, under different discourse annotation frameworks, there exist multiple corpora which have internal connections. To exploit the combination of different discourse corpora, we design related discourse classification tasks specific to a corpus, and propose a novel Convolutional Neural Network embedded multi-task learning system to synthesize these tasks by learning both unique and shared representations for each task. The experimental results on the PDTB implicit discourse relation classification task demonstrate that our model achieves significant gains over baseline systems.\n    ",
        "submission_date": "2016-03-09T00:00:00",
        "last_modified_date": "2016-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03007",
        "title": "Robot Dream",
        "authors": [
            "Alexander Tchitchigin",
            "Max Talanov",
            "Larisa Safina",
            "Manuel Mazzara"
        ],
        "abstract": "In this position paper we present a novel approach to neurobiologically plausible implementation of emotional reactions and behaviors for real-time autonomous robotic systems. The working metaphor we use is the \"day\" and \"night\" phases of mammalian life. During the \"day\" phase a robotic system stores the inbound information and is controlled by a light-weight rule-based system in real time. In contrast to that, during the \"night\" phase the stored information is been transferred to the supercomputing system to update the realistic neural network: emotional and behavioral strategies.\n    ",
        "submission_date": "2016-03-09T00:00:00",
        "last_modified_date": "2016-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03112",
        "title": "Building a Fine-Grained Entity Typing System Overnight for a New X (X = Language, Domain, Genre)",
        "authors": [
            "Lifu Huang",
            "Jonathan May",
            "Xiaoman Pan",
            "Heng Ji"
        ],
        "abstract": "Recent research has shown great progress on fine-grained entity typing. Most existing methods require pre-defining a set of types and training a multi-class classifier from a large labeled data set based on multi-level linguistic features. They are thus limited to certain domains, genres and languages. In this paper, we propose a novel unsupervised entity typing framework by combining symbolic and distributional semantics. We start from learning general embeddings for each entity mention, compose the embeddings of specific contexts using linguistic structures, link the mention to knowledge bases and learn its related knowledge representations. Then we develop a novel joint hierarchical clustering and linking algorithm to type all mentions using these representations. This framework doesn't rely on any annotated data, predefined typing schema, or hand-crafted features, therefore it can be quickly adapted to a new domain, genre and language. Furthermore, it has great flexibility at incorporating linguistic structures (e.g., Abstract Meaning Representation (AMR), dependency relations) to improve specific context representation. Experiments on genres (news and discussion forum) show comparable performance with state-of-the-art supervised typing systems trained from a large amount of labeled data. Results on various languages (English, Chinese, Japanese, Hausa, and Yoruba) and domains (general and biomedical) demonstrate the portability of our framework.\n    ",
        "submission_date": "2016-03-10T00:00:00",
        "last_modified_date": "2016-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03251",
        "title": "A Markovian-based Approach for Daily Living Activities Recognition",
        "authors": [
            "Zaineb Liouane",
            "Tayeb Lemlouma",
            "Philippe Roose",
            "Fr\u00e9d\u00e9ric Weis",
            "Messaoud Hassani"
        ],
        "abstract": "Recognizing the activities of daily living plays an important role in healthcare. It is necessary to use an adapted model to simulate the human behavior in a domestic space to monitor the patient harmonically and to intervene in the necessary time. In this paper, we tackle this problem using the hierarchical hidden Markov model for representing and recognizing complex indoor activities. We propose a new grammar, called \"Home By Room Activities Language\", to facilitate the complexity of human scenarios and consider the abnormal activities.\n    ",
        "submission_date": "2016-03-10T00:00:00",
        "last_modified_date": "2016-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03491",
        "title": "Bayesian Opponent Exploitation in Imperfect-Information Games",
        "authors": [
            "Sam Ganzfried",
            "Qingyun Sun"
        ],
        "abstract": "Two fundamental problems in computational game theory are computing a Nash equilibrium and learning to exploit opponents given observations of their play (opponent exploitation). The latter is perhaps even more important than the former: Nash equilibrium does not have a compelling theoretical justification in game classes other than two-player zero-sum, and for all games one can potentially do better by exploiting perceived weaknesses of the opponent than by following a static equilibrium strategy throughout the match. The natural setting for opponent exploitation is the Bayesian setting where we have a prior model that is integrated with observations to create a posterior opponent model that we respond to. The most natural, and a well-studied prior distribution is the Dirichlet distribution. An exact polynomial-time algorithm is known for best-responding to the posterior distribution for an opponent assuming a Dirichlet prior with multinomial sampling in normal-form games; however, for imperfect-information games the best known algorithm is based on approximating an infinite integral without theoretical guarantees. We present the first exact algorithm for a natural class of imperfect-information games. We demonstrate that our algorithm runs quickly in practice and outperforms the best prior approaches. We also present an algorithm for the uniform prior setting.\n    ",
        "submission_date": "2016-03-10T00:00:00",
        "last_modified_date": "2018-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03795",
        "title": "Demonstrating the Feasibility of Automatic Game Balancing",
        "authors": [
            "Vanessa Volz",
            "G\u00fcnter Rudolph",
            "Boris Naujoks"
        ],
        "abstract": "Game balancing is an important part of the (computer) game design process, in which designers adapt a game prototype so that the resulting gameplay is as entertaining as possible. In industry, the evaluation of a game is often based on costly playtests with human players. It suggests itself to automate this process using surrogate models for the prediction of gameplay and outcome. In this paper, the feasibility of automatic balancing using simulation- and deck-based objectives is investigated for the card game top trumps. Additionally, the necessity of a multi-objective approach is asserted by a comparison with the only known (single-objective) method. We apply a multi-objective evolutionary algorithm to obtain decks that optimise objectives, e.g. win rate and average number of tricks, developed to express the fairness and the excitement of a game of top trumps. The results are compared with decks from published top trumps decks using simulation-based objectives. The possibility to generate decks better or at least as good as decks from published top trumps decks in terms of these objectives is demonstrated. Our results indicate that automatic balancing with the presented approach is feasible even for more complex games such as real-time strategy games.\n    ",
        "submission_date": "2016-03-11T00:00:00",
        "last_modified_date": "2016-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03827",
        "title": "Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks",
        "authors": [
            "Ji Young Lee",
            "Franck Dernoncourt"
        ],
        "abstract": "Recent approaches based on artificial neural networks (ANNs) have shown promising results for short-text classification. However, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one. In this work, we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts. Our model achieves state-of-the-art results on three different datasets for dialog act prediction.\n    ",
        "submission_date": "2016-03-12T00:00:00",
        "last_modified_date": "2016-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03833",
        "title": "From virtual demonstration to real-world manipulation using LSTM and MDN",
        "authors": [
            "Rouhollah Rahmatizadeh",
            "Pooya Abolghasemi",
            "Aman Behal",
            "Ladislau B\u00f6l\u00f6ni"
        ],
        "abstract": "Robots assisting the disabled or elderly must perform complex manipulation tasks and must adapt to the home environment and preferences of their user. Learning from demonstration is a promising choice, that would allow the non-technical user to teach the robot different tasks. However, collecting demonstrations in the home environment of a disabled user is time consuming, disruptive to the comfort of the user, and presents safety challenges. It would be desirable to perform the demonstrations in a virtual environment. In this paper we describe a solution to the challenging problem of behavior transfer from virtual demonstration to a physical robot. The virtual demonstrations are used to train a deep neural network based controller, which is using a Long Short Term Memory (LSTM) recurrent neural network to generate trajectories. The training process uses a Mixture Density Network (MDN) to calculate an error signal suitable for the multimodal nature of demonstrations. The controller learned in the virtual environment is transferred to a physical robot (a Rethink Robotics Baxter). An off-the-shelf vision component is used to substitute for geometric knowledge available in the simulation and an inverse kinematics module is used to allow the Baxter to enact the trajectory. Our experimental studies validate the three contributions of the paper: (1) the controller learned from virtual demonstrations can be used to successfully perform the manipulation tasks on a physical robot, (2) the LSTM+MDN architectural choice outperforms other choices, such as the use of feedforward networks and mean-squared error based training signals and (3) allowing imperfect demonstrations in the training set also allows the controller to learn how to correct its manipulation mistakes.\n    ",
        "submission_date": "2016-03-12T00:00:00",
        "last_modified_date": "2017-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03980",
        "title": "On Learning High Dimensional Structured Single Index Models",
        "authors": [
            "Nikhil Rao",
            "Ravi Ganti",
            "Laura Balzano",
            "Rebecca Willett",
            "Robert Nowak"
        ],
        "abstract": "Single Index Models (SIMs) are simple yet flexible semi-parametric models for machine learning, where the response variable is modeled as a monotonic function of a linear combination of features. Estimation in this context requires learning both the feature weights and the nonlinear function that relates features to observations. While methods have been described to learn SIMs in the low dimensional regime, a method that can efficiently learn SIMs in high dimensions, and under general structural assumptions, has not been forthcoming. In this paper, we propose computationally efficient algorithms for SIM inference in high dimensions with structural constraints. Our general approach specializes to sparsity, group sparsity, and low-rank assumptions among others. Experiments show that the proposed method enjoys superior predictive performance when compared to generalized linear models, and achieves results comparable to or better than single layer feedforward neural networks with significantly less computational cost.\n    ",
        "submission_date": "2016-03-13T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.04068",
        "title": "A Signaling Game Approach to Databases Querying and Interaction",
        "authors": [
            "Ben McCamish",
            "Vahid Ghadakchi",
            "Arash Termehchy",
            "Behrouz Touri"
        ],
        "abstract": "As most users do not precisely know the structure and/or the content of databases, their queries do not exactly reflect their information needs. The database management systems (DBMS) may interact with users and use their feedback on the returned results to learn the information needs behind their queries. Current query interfaces assume that users do not learn and modify the way way they express their information needs in form of queries during their interaction with the DBMS. Using a real-world interaction workload, we show that users learn and modify how to express their information needs during their interactions with the DBMS and their learning is accurately modeled by a well-known reinforcement learning mechanism. As current data interaction systems assume that users do not modify their strategies, they cannot discover the information needs behind users' queries effectively. We model the interaction between users and DBMS as a game with identical interest between two rational agents whose goal is to establish a common language for representing information needs in form of queries. We propose a reinforcement learning method that learns and answers the information needs behind queries and adapts to the changes in users' strategies and prove that it improves the effectiveness of answering queries stochastically speaking. We propose two efficient implementation of this method over large relational databases. Our extensive empirical studies over real-world query workloads indicate that our algorithms are efficient and effective.\n    ",
        "submission_date": "2016-03-13T00:00:00",
        "last_modified_date": "2018-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.04118",
        "title": "Active Algorithms For Preference Learning Problems with Multiple Populations",
        "authors": [
            "Aniruddha Bhargava",
            "Ravi Ganti",
            "Robert Nowak"
        ],
        "abstract": "In this paper we model the problem of learning preferences of a population as an active learning problem. We propose an algorithm can adaptively choose pairs of items to show to users coming from a heterogeneous population, and use the obtained reward to decide which pair of items to show next. We provide computationally efficient algorithms with provable sample complexity guarantees for this problem in both the noiseless and noisy cases. In the process of establishing sample complexity guarantees for our algorithms, we establish new results using a Nystr{\u00f6}m-like method which can be of independent interest. We supplement our theoretical results with experimental comparisons.\n    ",
        "submission_date": "2016-03-14T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.04259",
        "title": "Item2Vec: Neural Item Embedding for Collaborative Filtering",
        "authors": [
            "Oren Barkan",
            "Noam Koenigstein"
        ],
        "abstract": "Many Collaborative Filtering (CF) algorithms are item-based in the sense that they analyze item-item relations in order to produce item similarities. Recently, several works in the field of Natural Language Processing (NLP) suggested to learn a latent representation of words using neural embedding algorithms. Among them, the Skip-gram with Negative Sampling (SGNS), also known as word2vec, was shown to provide state-of-the-art results on various linguistics tasks. In this paper, we show that item-based CF can be cast in the same framework of neural word embedding. Inspired by SGNS, we describe a method we name item2vec for item-based CF that produces embedding for items in a latent space. The method is capable of inferring item-item relations even when user information is not available. We present experimental results that demonstrate the effectiveness of the item2vec method and show it is competitive with SVD.\n    ",
        "submission_date": "2016-03-14T00:00:00",
        "last_modified_date": "2017-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.04319",
        "title": "Learning Network of Multivariate Hawkes Processes: A Time Series Approach",
        "authors": [
            "Jalal Etesami",
            "Negar Kiyavash",
            "Kun Zhang",
            "Kushagra Singhal"
        ],
        "abstract": "Learning the influence structure of multiple time series data is of great interest to many disciplines. This paper studies the problem of recovering the causal structure in network of multivariate linear Hawkes processes. In such processes, the occurrence of an event in one process affects the probability of occurrence of new events in some other processes. Thus, a natural notion of causality exists between such processes captured by the support of the excitation matrix. We show that the resulting causal influence network is equivalent to the Directed Information graph (DIG) of the processes, which encodes the causal factorization of the joint distribution of the processes. Furthermore, we present an algorithm for learning the support of excitation matrix (or equivalently the DIG). The performance of the algorithm is evaluated on synthesized multivariate Hawkes networks as well as a stock market and MemeTracker real-world dataset.\n    ",
        "submission_date": "2016-03-14T00:00:00",
        "last_modified_date": "2016-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.04466",
        "title": "Sequential Voting Promotes Collective Discovery in Social Recommendation Systems",
        "authors": [
            "L. Elisa Celis",
            "Peter M. Krafft",
            "Nathan Kobe"
        ],
        "abstract": "One goal of online social recommendation systems is to harness the wisdom of crowds in order to identify high quality content. Yet the sequential voting mechanisms that are commonly used by these systems are at odds with existing theoretical and empirical literature on optimal aggregation. This literature suggests that sequential voting will promote herding---the tendency for individuals to copy the decisions of others around them---and hence lead to suboptimal content recommendation. Is there a problem with our practice, or a problem with our theory? Previous attempts at answering this question have been limited by a lack of objective measurements of content quality. Quality is typically defined endogenously as the popularity of content in absence of social influence. The flaw of this metric is its presupposition that the preferences of the crowd are aligned with underlying quality. Domains in which content quality can be defined exogenously and measured objectively are thus needed in order to better assess the design choices of social recommendation systems. In this work, we look to the domain of education, where content quality can be measured via how well students are able to learn from the material presented to them. Through a behavioral experiment involving a simulated massive open online course (MOOC) run on Amazon Mechanical Turk, we show that sequential voting systems can surface better content than systems that elicit independent votes.\n    ",
        "submission_date": "2016-03-14T00:00:00",
        "last_modified_date": "2016-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.04535",
        "title": "Learning Domain-Invariant Subspace using Domain Features and Independence Maximization",
        "authors": [
            "Ke Yan",
            "Lu Kou",
            "David Zhang"
        ],
        "abstract": "Domain adaptation algorithms are useful when the distributions of the training and the test data are different. In this paper, we focus on the problem of instrumental variation and time-varying drift in the field of sensors and measurement, which can be viewed as discrete and continuous distributional change in the feature space. We propose maximum independence domain adaptation (MIDA) and semi-supervised MIDA (SMIDA) to address this problem. Domain features are first defined to describe the background information of a sample, such as the device label and acquisition time. Then, MIDA learns a subspace which has maximum independence with the domain features, so as to reduce the inter-domain discrepancy in distributions. A feature augmentation strategy is also designed to project samples according to their backgrounds so as to improve the adaptation. The proposed algorithms are flexible and fast. Their effectiveness is verified by experiments on synthetic datasets and four real-world ones on sensors, measurement, and computer vision. They can greatly enhance the practicability of sensor systems, as well as extend the application scope of existing domain adaptation algorithms by uniformly handling different kinds of distributional change.\n    ",
        "submission_date": "2016-03-15T00:00:00",
        "last_modified_date": "2017-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.05106",
        "title": "One-Shot Generalization in Deep Generative Models",
        "authors": [
            "Danilo Jimenez Rezende",
            "Shakir Mohamed",
            "Ivo Danihelka",
            "Karol Gregor",
            "Daan Wierstra"
        ],
        "abstract": "Humans have an impressive ability to reason about new concepts and experiences from just a single example. In particular, humans have an ability for one-shot generalization: an ability to encounter a new concept, understand its structure, and then be able to generate compelling alternative variations of the concept. We develop machine learning systems with this important capacity by developing new deep generative models, models that combine the representational power of deep learning with the inferential power of Bayesian reasoning. We develop a class of sequential generative models that are built on the principles of feedback and attention. These two characteristics lead to generative models that are among the state-of-the art in density estimation and image generation. We demonstrate the one-shot generalization ability of our models using three tasks: unconditional sampling, generating new exemplars of a given concept, and generating new exemplars of a family of concepts. In all cases our models are able to generate compelling and diverse samples---having seen new examples just once---providing an important class of general-purpose models for one-shot machine learning.\n    ",
        "submission_date": "2016-03-16T00:00:00",
        "last_modified_date": "2016-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.05145",
        "title": "Suppressing the Unusual: towards Robust CNNs using Symmetric Activation Functions",
        "authors": [
            "Qiyang Zhao",
            "Lewis D Griffin"
        ],
        "abstract": "Many deep Convolutional Neural Networks (CNN) make incorrect predictions on adversarial samples obtained by imperceptible perturbations of clean samples. We hypothesize that this is caused by a failure to suppress unusual signals within network layers. As remedy we propose the use of Symmetric Activation Functions (SAF) in non-linear signal transducer units. These units suppress signals of exceptional magnitude. We prove that SAF networks can perform classification tasks to arbitrary precision in a simplified situation. In practice, rather than use SAFs alone, we add them into CNNs to improve their robustness. The modified CNNs can be easily trained using popular strategies with the moderate training load. Our experiments on MNIST and CIFAR-10 show that the modified CNNs perform similarly to plain ones on clean samples, and are remarkably more robust against adversarial and nonsense samples.\n    ",
        "submission_date": "2016-03-16T00:00:00",
        "last_modified_date": "2016-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.05474",
        "title": "Neural Aggregation Network for Video Face Recognition",
        "authors": [
            "Jiaolong Yang",
            "Peiran Ren",
            "Dongqing Zhang",
            "Dong Chen",
            "Fang Wen",
            "Hongdong Li",
            "Gang Hua"
        ],
        "abstract": "This paper presents a Neural Aggregation Network (NAN) for video face recognition. The network takes a face video or face image set of a person with a variable number of face images as its input, and produces a compact, fixed-dimension feature representation for recognition. The whole network is composed of two modules. The feature embedding module is a deep Convolutional Neural Network (CNN) which maps each face image to a feature vector. The aggregation module consists of two attention blocks which adaptively aggregate the feature vectors to form a single feature inside the convex hull spanned by them. Due to the attention mechanism, the aggregation is invariant to the image order. Our NAN is trained with a standard classification or verification loss without any extra supervision signal, and we found that it automatically learns to advocate high-quality face images while repelling low-quality ones such as blurred, occluded and improperly exposed faces. The experiments on IJB-A, YouTube Face, Celebrity-1000 video face recognition benchmarks show that it consistently outperforms naive aggregation methods and achieves the state-of-the-art accuracy.\n    ",
        "submission_date": "2016-03-17T00:00:00",
        "last_modified_date": "2017-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.05594",
        "title": "Mapping Temporal Variables into the NeuCube for Improved Pattern Recognition, Predictive Modelling and Understanding of Stream Data",
        "authors": [
            "Enmei Tu",
            "Nikola Kasabov",
            "Jie Yang"
        ],
        "abstract": "This paper proposes a new method for an optimized mapping of temporal variables, describing a temporal stream data, into the recently proposed NeuCube spiking neural network architecture. This optimized mapping extends the use of the NeuCube, which was initially designed for spatiotemporal brain data, to work on arbitrary stream data and to achieve a better accuracy of temporal pattern recognition, a better and earlier event prediction and a better understanding of complex temporal stream data through visualization of the NeuCube connectivity. The effect of the new mapping is demonstrated on three bench mark problems. The first one is early prediction of patient sleep stage event from temporal physiological data. The second one is pattern recognition of dynamic temporal patterns of traffic in the Bay Area of California and the last one is the Challenge 2012 contest data set. In all cases the use of the proposed mapping leads to an improved accuracy of pattern recognition and event prediction and a better understanding of the data when compared to traditional machine learning techniques or spiking neural network reservoirs with arbitrary mapping of the variables.\n    ",
        "submission_date": "2016-03-17T00:00:00",
        "last_modified_date": "2016-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.05670",
        "title": "Bank distress in the news: Describing events through deep learning",
        "authors": [
            "Samuel R\u00f6nnqvist",
            "Peter Sarlin"
        ],
        "abstract": "While many models are purposed for detecting the occurrence of significant events in financial systems, the task of providing qualitative detail on the developments is not usually as well automated. We present a deep learning approach for detecting relevant discussion in text and extracting natural language descriptions of events. Supervised by only a small set of event information, comprising entity names and dates, the model is leveraged by unsupervised learning of semantic vector representations on extensive text data. We demonstrate applicability to the study of financial risk based on news (6.6M articles), particularly bank distress and government interventions (243 events), where indices can signal the level of bank-stress-related reporting at the entity level, or aggregated at national or European level, while being coupled with explanations. Thus, we exemplify how text, as timely, widely available and descriptive data, can serve as a useful complementary source of information for financial and systemic risk analytics.\n    ",
        "submission_date": "2016-03-17T00:00:00",
        "last_modified_date": "2016-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.05959",
        "title": "Efficient Multi-Scale 3D CNN with Fully Connected CRF for Accurate Brain Lesion Segmentation",
        "authors": [
            "Konstantinos Kamnitsas",
            "Christian Ledig",
            "Virginia F.J. Newcombe",
            "Joanna P. Simpson",
            "Andrew D. Kane",
            "David K. Menon",
            "Daniel Rueckert",
            "Ben Glocker"
        ],
        "abstract": "We propose a dual pathway, 11-layers deep, three-dimensional Convolutional Neural Network for the challenging task of brain lesion segmentation. The devised architecture is the result of an in-depth analysis of the limitations of current networks proposed for similar applications. To overcome the computational burden of processing 3D medical scans, we have devised an efficient and effective dense training scheme which joins the processing of adjacent image patches into one pass through the network while automatically adapting to the inherent class imbalance present in the data. Further, we analyze the development of deeper, thus more discriminative 3D CNNs. In order to incorporate both local and larger contextual information, we employ a dual pathway architecture that processes the input images at multiple scales simultaneously. For post-processing of the network's soft segmentation, we use a 3D fully connected Conditional Random Field which effectively removes false positives. Our pipeline is extensively evaluated on three challenging tasks of lesion segmentation in multi-channel MRI patient data with traumatic brain injuries, brain tumors, and ischemic stroke. We improve on the state-of-the-art for all three applications, with top ranking performance on the public benchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient, which allows its adoption in a variety of research and clinical settings. The source code of our implementation is made publicly available.\n    ",
        "submission_date": "2016-03-18T00:00:00",
        "last_modified_date": "2017-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06015",
        "title": "A Comprehensive Performance Evaluation of Deformable Face Tracking \"In-the-Wild\"",
        "authors": [
            "Grigorios G. Chrysos",
            "Epameinondas Antonakos",
            "Patrick Snape",
            "Akshay Asthana",
            "Stefanos Zafeiriou"
        ],
        "abstract": "Recently, technologies such as face detection, facial landmark localisation and face recognition and verification have matured enough to provide effective and efficient solutions for imagery captured under arbitrary conditions (referred to as \"in-the-wild\"). This is partially attributed to the fact that comprehensive \"in-the-wild\" benchmarks have been developed for face detection, landmark localisation and recognition/verification. A very important technology that has not been thoroughly evaluated yet is deformable face tracking \"in-the-wild\". Until now, the performance has mainly been assessed qualitatively by visually assessing the result of a deformable face tracking technology on short videos. In this paper, we perform the first, to the best of our knowledge, thorough evaluation of state-of-the-art deformable face tracking pipelines using the recently introduced 300VW benchmark. We evaluate many different architectures focusing mainly on the task of on-line deformable face tracking. In particular, we compare the following general strategies: (a) generic face detection plus generic facial landmark localisation, (b) generic model free tracking plus generic facial landmark localisation, as well as (c) hybrid approaches using state-of-the-art face detection, model free tracking and facial landmark localisation technologies. Our evaluation reveals future avenues for further research on the topic.\n    ",
        "submission_date": "2016-03-18T00:00:00",
        "last_modified_date": "2017-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06059",
        "title": "Generating Natural Questions About an Image",
        "authors": [
            "Nasrin Mostafazadeh",
            "Ishan Misra",
            "Jacob Devlin",
            "Margaret Mitchell",
            "Xiaodong He",
            "Lucy Vanderwende"
        ],
        "abstract": "There has been an explosion of work in the vision & language community during the past few years from image captioning to video transcription, and answering questions about images. These tasks have focused on literal descriptions of the image. To move beyond the literal, we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image. In this paper, we introduce the novel task of Visual Question Generation (VQG), where the system is tasked with asking a natural and engaging question when shown an image. We provide three datasets which cover a variety of images from object-centric to event-centric, with considerably more abstract training data than provided to state-of-the-art captioning systems thus far. We train and test several generative and retrieval models to tackle the task of VQG. Evaluation results show that while such models ask reasonable questions for a variety of images, there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics. Our proposed task offers a new challenge to the community which we hope furthers interest in exploring deeper connections between vision & language.\n    ",
        "submission_date": "2016-03-19T00:00:00",
        "last_modified_date": "2016-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06127",
        "title": "Sentence Pair Scoring: Towards Unified Framework for Text Comprehension",
        "authors": [
            "Petr Baudi\u0161",
            "Jan Pichl",
            "Tom\u00e1\u0161 Vysko\u010dil",
            "Jan \u0160ediv\u00fd"
        ],
        "abstract": "We review the task of Sentence Pair Scoring, popular in the literature in various forms - viewed as Answer Sentence Selection, Semantic Text Scoring, Next Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a component of Memory Networks.\n",
        "submission_date": "2016-03-19T00:00:00",
        "last_modified_date": "2016-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06129",
        "title": "Automated Correction for Syntax Errors in Programming Assignments using Recurrent Neural Networks",
        "authors": [
            "Sahil Bhatia",
            "Rishabh Singh"
        ],
        "abstract": "We present a method for automatically generating repair feedback for syntax errors for introductory programming problems. Syntax errors constitute one of the largest classes of errors (34%) in our dataset of student submissions obtained from a MOOC course on edX. The previous techniques for generating automated feed- back on programming assignments have focused on functional correctness and style considerations of student programs. These techniques analyze the program AST of the program and then perform some dynamic and symbolic analyses to compute repair feedback. Unfortunately, it is not possible to generate ASTs for student pro- grams with syntax errors and therefore the previous feedback techniques are not applicable in repairing syntax errors.\n",
        "submission_date": "2016-03-19T00:00:00",
        "last_modified_date": "2016-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06143",
        "title": "Neurally-Guided Procedural Models: Amortized Inference for Procedural Graphics Programs using Neural Networks",
        "authors": [
            "Daniel Ritchie",
            "Anna Thomas",
            "Pat Hanrahan",
            "Noah D. Goodman"
        ],
        "abstract": "Probabilistic inference algorithms such as Sequential Monte Carlo (SMC) provide powerful tools for constraining procedural models in computer graphics, but they require many samples to produce desirable results. In this paper, we show how to create procedural models which learn how to satisfy constraints. We augment procedural models with neural networks which control how the model makes random choices based on the output it has generated thus far. We call such models neurally-guided procedural models. As a pre-computation, we train these models to maximize the likelihood of example outputs generated via SMC. They are then used as efficient SMC importance samplers, generating high-quality results with very few samples. We evaluate our method on L-system-like models with image-based constraints. Given a desired quality threshold, neurally-guided models can generate satisfactory results up to 10x faster than unguided models.\n    ",
        "submission_date": "2016-03-19T00:00:00",
        "last_modified_date": "2016-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06212",
        "title": "Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science",
        "authors": [
            "Randal S. Olson",
            "Nathan Bartley",
            "Ryan J. Urbanowicz",
            "Jason H. Moore"
        ],
        "abstract": "As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning---pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a series of simulated and real-world benchmark data sets. In particular, we show that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. We also address the tendency for TPOT to design overly complex pipelines by integrating Pareto optimization, which produces compact pipelines without sacrificing classification accuracy. As such, this work represents an important step toward fully automating machine learning pipeline design.\n    ",
        "submission_date": "2016-03-20T00:00:00",
        "last_modified_date": "2016-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06217",
        "title": "An Approximation Approach for Solving the Subpath Planning Problem",
        "authors": [
            "Masoud Safilian",
            "S. Mehdi Tashakkori",
            "Sepehr Eghbali",
            "Aliakbar Safilian"
        ],
        "abstract": "The subpath planning problem is a branch of the path planning problem, which has widespread applications in automated manufacturing process as well as vehicle and robot navigation. This problem is to find the shortest path or tour subject for travelling a set of given subpaths. The current approaches for dealing with the subpath planning problem are all based on meta-heuristic approaches. It is well-known that meta-heuristic based approaches have several deficiencies. To address them, we propose a novel approximation algorithm in the O(n^3) time complexity class, which guarantees to solve any subpath planning problem instance with the fixed ratio bound of 2. Also, the formal proofs of the claims, our empirical evaluation shows that our approximation method acts much better than a state-of-the-art method, both in result and execution time.\n    ",
        "submission_date": "2016-03-20T00:00:00",
        "last_modified_date": "2016-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06288",
        "title": "Multi-fidelity Gaussian Process Bandit Optimisation",
        "authors": [
            "Kirthevasan Kandasamy",
            "Gautam Dasarathy",
            "Junier B. Oliva",
            "Jeff Schneider",
            "Barnabas Poczos"
        ],
        "abstract": "In many scientific and engineering applications, we are tasked with the maximisation of an expensive to evaluate black box function $f$. Traditional settings for this problem assume just the availability of this single function. However, in many cases, cheap approximations to $f$ may be obtainable. For example, the expensive real world behaviour of a robot can be approximated by a cheap computer simulation. We can use these approximations to eliminate low function value regions cheaply and use the expensive evaluations of $f$ in a small but promising region and speedily identify the optimum. We formalise this task as a \\emph{multi-fidelity} bandit problem where the target function and its approximations are sampled from a Gaussian process. We develop MF-GP-UCB, a novel method based on upper confidence bound techniques. In our theoretical analysis we demonstrate that it exhibits precisely the above behaviour, and achieves better regret than strategies which ignore multi-fidelity information. Empirically, MF-GP-UCB outperforms such naive strategies and other multi-fidelity methods on several synthetic and real experiments.\n    ",
        "submission_date": "2016-03-20T00:00:00",
        "last_modified_date": "2019-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06318",
        "title": "Harnessing Deep Neural Networks with Logic Rules",
        "authors": [
            "Zhiting Hu",
            "Xuezhe Ma",
            "Zhengzhong Liu",
            "Eduard Hovy",
            "Eric Xing"
        ],
        "abstract": "Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.\n    ",
        "submission_date": "2016-03-21T00:00:00",
        "last_modified_date": "2020-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06393",
        "title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning",
        "authors": [
            "Jiatao Gu",
            "Zhengdong Lu",
            "Hang Li",
            "Victor O.K. Li"
        ],
        "abstract": "We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose a new model called CopyNet with encoder-decoder structure. CopyNet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of CopyNet. For example, CopyNet can outperform regular RNN-based model with remarkable margins on text summarization tasks.\n    ",
        "submission_date": "2016-03-21T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06554",
        "title": "Action-Affect Classification and Morphing using Multi-Task Representation Learning",
        "authors": [
            "Timothy J. Shields",
            "Mohamed R. Amer",
            "Max Ehrlich",
            "Amir Tamrakar"
        ],
        "abstract": "Most recent work focused on affect from facial expressions, and not as much on body. This work focuses on body affect analysis. Affect does not occur in isolation. Humans usually couple affect with an action in natural interactions; for example, a person could be talking and smiling. Recognizing body affect in sequences requires efficient algorithms to capture both the micro movements that differentiate between happy and sad and the macro variations between different actions. We depart from traditional approaches for time-series data analytics by proposing a multi-task learning model that learns a shared representation that is well-suited for action-affect classification as well as generation. For this paper we choose Conditional Restricted Boltzmann Machines to be our building block. We propose a new model that enhances the CRBM model with a factored multi-task component to become Multi-Task Conditional Restricted Boltzmann Machines (MTCRBMs). We evaluate our approach on two publicly available datasets, the Body Affect dataset and the Tower Game dataset, and show superior classification performance improvement over the state-of-the-art, as well as the generative abilities of our model.\n    ",
        "submission_date": "2016-03-21T00:00:00",
        "last_modified_date": "2016-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06677",
        "title": "Learning Executable Semantic Parsers for Natural Language Understanding",
        "authors": [
            "Percy Liang"
        ],
        "abstract": "For building question answering systems and natural language interfaces, semantic parsing has emerged as an important and powerful paradigm. Semantic parsers map natural language into logical forms, the classic representation for many important linguistic phenomena. The modern twist is that we are interested in learning semantic parsers from data, which introduces a new layer of statistical and computational issues. This article lays out the components of a statistical semantic parser, highlighting the key challenges. We will see that semantic parsing is a rich fusion of the logical and the statistical world, and that this fusion will play an integral role in the future of natural language understanding systems.\n    ",
        "submission_date": "2016-03-22T00:00:00",
        "last_modified_date": "2016-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06807",
        "title": "Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus",
        "authors": [
            "Iulian Vlad Serban",
            "Alberto Garc\u00eda-Dur\u00e1n",
            "Caglar Gulcehre",
            "Sungjin Ahn",
            "Sarath Chandar",
            "Aaron Courville",
            "Yoshua Bengio"
        ],
        "abstract": "Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. However, to this date, there are no large-scale question-answer corpora available. In this paper we present the 30M Factoid Question-Answer Corpus, an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions. The produced question answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics. Across all evaluation criteria the question-generation model outperforms the competing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear comparable in quality to real human-generated questions.\n    ",
        "submission_date": "2016-03-22T00:00:00",
        "last_modified_date": "2016-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06881",
        "title": "Feeling the Bern: Adaptive Estimators for Bernoulli Probabilities of Pairwise Comparisons",
        "authors": [
            "Nihar B. Shah",
            "Sivaraman Balakrishnan",
            "Martin J. Wainwright"
        ],
        "abstract": "We study methods for aggregating pairwise comparison data in order to estimate outcome probabilities for future comparisons among a collection of n items. Working within a flexible framework that imposes only a form of strong stochastic transitivity (SST), we introduce an adaptivity index defined by the indifference sets of the pairwise comparison probabilities. In addition to measuring the usual worst-case risk of an estimator, this adaptivity index also captures the extent to which the estimator adapts to instance-specific difficulty relative to an oracle estimator. We prove three main results that involve this adaptivity index and different algorithms. First, we propose a three-step estimator termed Count-Randomize-Least squares (CRL), and show that it has adaptivity index upper bounded as $\\sqrt{n}$ up to logarithmic factors. We then show that that conditional on the hardness of planted clique, no computationally efficient estimator can achieve an adaptivity index smaller than $\\sqrt{n}$. Second, we show that a regularized least squares estimator can achieve a poly-logarithmic adaptivity index, thereby demonstrating a $\\sqrt{n}$-gap between optimal and computationally achievable adaptivity. Finally, we prove that the standard least squares estimator, which is known to be optimally adaptive in several closely related problems, fails to adapt in the context of estimating pairwise probabilities.\n    ",
        "submission_date": "2016-03-22T00:00:00",
        "last_modified_date": "2016-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07292",
        "title": "Debugging Machine Learning Tasks",
        "authors": [
            "Aleksandar Chakarov",
            "Aditya Nori",
            "Sriram Rajamani",
            "Shayak Sen",
            "Deepak Vijaykeerthy"
        ],
        "abstract": "Unlike traditional programs (such as operating systems or word processors) which have large amounts of code, machine learning tasks use programs with relatively small amounts of code (written in machine learning libraries), but voluminous amounts of data. Just like developers of traditional programs debug errors in their code, developers of machine learning tasks debug and fix errors in their data. However, algorithms and tools for debugging and fixing errors in data are less common, when compared to their counterparts for detecting and fixing errors in code. In this paper, we consider classification tasks where errors in training data lead to misclassifications in test points, and propose an automated method to find the root causes of such misclassifications. Our root cause analysis is based on Pearl's theory of causation, and uses Pearl's PS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi, encodes the computation of PS as a probabilistic program, and uses recent work on probabilistic programs and transformations on probabilistic programs (along with gray-box models of machine learning algorithms) to efficiently compute PS. Psi is able to identify root causes of data errors in interesting data sets.\n    ",
        "submission_date": "2016-03-23T00:00:00",
        "last_modified_date": "2016-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07294",
        "title": "On the Theory and Practice of Privacy-Preserving Bayesian Data Analysis",
        "authors": [
            "James Foulds",
            "Joseph Geumlek",
            "Max Welling",
            "Kamalika Chaudhuri"
        ],
        "abstract": "Bayesian inference has great promise for the privacy-preserving analysis of sensitive data, as posterior sampling automatically preserves differential privacy, an algorithmic notion of data privacy, under certain conditions (Dimitrakakis et al., 2014; Wang et al., 2015). While this one posterior sample (OPS) approach elegantly provides privacy \"for free,\" it is data inefficient in the sense of asymptotic relative efficiency (ARE). We show that a simple alternative based on the Laplace mechanism, the workhorse of differential privacy, is as asymptotically efficient as non-private posterior inference, under general assumptions. This technique also has practical advantages including efficient use of the privacy budget for MCMC. We demonstrate the practicality of our approach on a time-series analysis of sensitive military records from the Afghanistan and Iraq wars disclosed by the Wikileaks organization.\n    ",
        "submission_date": "2016-03-23T00:00:00",
        "last_modified_date": "2016-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07396",
        "title": "A Diagram Is Worth A Dozen Images",
        "authors": [
            "Aniruddha Kembhavi",
            "Mike Salvato",
            "Eric Kolve",
            "Minjoon Seo",
            "Hannaneh Hajishirzi",
            "Ali Farhadi"
        ],
        "abstract": "Diagrams are common tools for representing complex concepts, relationships and events, often when it would be difficult to portray the same information with natural images. Understanding natural images has been extensively studied in computer vision, while diagram understanding has received little attention. In this paper, we study the problem of diagram interpretation and reasoning, the challenging task of identifying the structure of a diagram and the semantics of its constituents and their relationships. We introduce Diagram Parse Graphs (DPG) as our representation to model the structure of diagrams. We define syntactic parsing of diagrams as learning to infer DPGs for diagrams and study semantic interpretation and reasoning of diagrams in the context of diagram question answering. We devise an LSTM-based method for syntactic parsing of diagrams and introduce a DPG-based attention model for diagram question answering. We compile a new dataset of diagrams with exhaustive annotations of constituents and relationships for over 5,000 diagrams and 15,000 questions and answers. Our results show the significance of our models for syntactic parsing and question answering in diagrams using DPGs.\n    ",
        "submission_date": "2016-03-24T00:00:00",
        "last_modified_date": "2016-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07442",
        "title": "Pixel-Level Domain Transfer",
        "authors": [
            "Donggeun Yoo",
            "Namil Kim",
            "Sunggyun Park",
            "Anthony S. Paek",
            "In So Kweon"
        ],
        "abstract": "We present an image-conditional image generation model. The model transfers an input domain to a target domain in semantic level, and generates the target image in pixel level. To generate realistic target images, we employ the real/fake-discriminator as in Generative Adversarial Nets, but also introduce a novel domain-discriminator to make the generated image relevant to the input image. We verify our model through a challenging task of generating a piece of clothing from an input image of a dressed person. We present a high quality clothing dataset containing the two domains, and succeed in demonstrating decent results.\n    ",
        "submission_date": "2016-03-24T00:00:00",
        "last_modified_date": "2016-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07453",
        "title": "An Expressive Probabilistic Temporal Logic",
        "authors": [
            "Bruno Woltzenlogel Paleo"
        ],
        "abstract": "This paper argues that a combined treatment of probabilities, time and actions is essential for an appropriate logical account of the notion of probability; and, based on this intuition, describes an expressive probabilistic temporal logic for reasoning about actions with uncertain outcomes. The logic is modal and higher-order: modalities annotated by actions are used to express possibility and necessity of propositions in the next states resulting from the actions, and a higher-order function is needed to express the probability operator. The proposed logic is shown to be an adequate extension of classical mathematical probability theory, and its expressiveness is illustrated through the formalization of the Monty Hall problem.\n    ",
        "submission_date": "2016-03-24T00:00:00",
        "last_modified_date": "2017-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07810",
        "title": "Conditional Similarity Networks",
        "authors": [
            "Andreas Veit",
            "Serge Belongie",
            "Theofanis Karaletsos"
        ],
        "abstract": "What makes images similar? To measure the similarity between images, they are typically embedded in a feature-vector space, in which their distance preserve the relative dissimilarity. However, when learning such similarity embeddings the simplifying assumption is commonly made that images are only compared to one unique measure of similarity. A main reason for this is that contradicting notions of similarities cannot be captured in a single space. To address this shortcoming, we propose Conditional Similarity Networks (CSNs) that learn embeddings differentiated into semantically distinct subspaces that capture the different notions of similarities. CSNs jointly learn a disentangled embedding where features for different similarities are encoded in separate dimensions as well as masks that select and reweight relevant dimensions to induce a subspace that encodes a specific similarity notion. We show that our approach learns interpretable image representations with visually relevant semantic subspaces. Further, when evaluating on triplet questions from multiple similarity notions our model even outperforms the accuracy obtained by training individual specialized networks for each notion separately.\n    ",
        "submission_date": "2016-03-25T00:00:00",
        "last_modified_date": "2017-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07886",
        "title": "A Novel Biologically Mechanism-Based Visual Cognition Model--Automatic Extraction of Semantics, Formation of Integrated Concepts and Re-selection Features for Ambiguity",
        "authors": [
            "Peijie Yin",
            "Hong Qiao",
            "Wei Wu",
            "Lu Qi",
            "YinLin Li",
            "Shanlin Zhong",
            "Bo Zhang"
        ],
        "abstract": "Integration between biology and information science benefits both fields. Many related models have been proposed, such as computational visual cognition models, computational motor control models, integrations of both and so on. In general, the robustness and precision of recognition is one of the key problems for object recognition models.\n",
        "submission_date": "2016-03-25T00:00:00",
        "last_modified_date": "2016-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08023",
        "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
        "authors": [
            "Chia-Wei Liu",
            "Ryan Lowe",
            "Iulian V. Serban",
            "Michael Noseworthy",
            "Laurent Charlin",
            "Joelle Pineau"
        ],
        "abstract": "We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.\n    ",
        "submission_date": "2016-03-25T00:00:00",
        "last_modified_date": "2017-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08079",
        "title": "Do You See What I Mean? Visual Resolution of Linguistic Ambiguities",
        "authors": [
            "Yevgeni Berzak",
            "Andrei Barbu",
            "Daniel Harari",
            "Boris Katz",
            "Shimon Ullman"
        ],
        "abstract": "Understanding language goes hand in hand with the ability to integrate complex contextual information obtained via perception. In this work, we present a novel task for grounded language understanding: disambiguating a sentence given a visual scene which depicts one of the possible interpretations of that sentence. To this end, we introduce a new multimodal corpus containing ambiguous sentences, representing a wide range of syntactic, semantic and discourse ambiguities, coupled with videos that visualize the different interpretations for each sentence. We address this task by extending a vision model which determines if a sentence is depicted by a video. We demonstrate how such a model can be adjusted to recognize different interpretations of the same underlying sentence, allowing to disambiguate sentences in a unified fashion across the different ambiguity types.\n    ",
        "submission_date": "2016-03-26T00:00:00",
        "last_modified_date": "2016-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08507",
        "title": "Generating Visual Explanations",
        "authors": [
            "Lisa Anne Hendricks",
            "Zeynep Akata",
            "Marcus Rohrbach",
            "Jeff Donahue",
            "Bernt Schiele",
            "Trevor Darrell"
        ],
        "abstract": "Clearly explaining a rationale for a classification decision to an end-user can be as important as the decision itself. Existing approaches for deep visual recognition are generally opaque and do not output any justification text; contemporary vision-language models can describe image content but fail to take into account class-discriminative image aspects which justify visual predictions. We propose a new model that focuses on the discriminating properties of the visible object, jointly predicts a class label, and explains why the predicted label is appropriate for the image. We propose a novel loss function based on sampling and reinforcement learning that learns to generate sentences that realize a global sentence property, such as class specificity. Our results on a fine-grained bird species classification dataset show that our model is able to generate explanations which are not only consistent with an image but also more discriminative than descriptions produced by existing captioning methods.\n    ",
        "submission_date": "2016-03-28T00:00:00",
        "last_modified_date": "2016-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08561",
        "title": "Shuffle and Learn: Unsupervised Learning using Temporal Order Verification",
        "authors": [
            "Ishan Misra",
            "C. Lawrence Zitnick",
            "Martial Hebert"
        ],
        "abstract": "In this paper, we present an approach for learning a visual representation from the raw spatiotemporal signals in videos. Our representation is learned without supervision from semantic labels. We formulate our method as an unsupervised sequential verification task, i.e., we determine whether a sequence of frames from a video is in the correct temporal order. With this simple task and no semantic labels, we learn a powerful visual representation using a Convolutional Neural Network (CNN). The representation contains complementary information to that learned from supervised image datasets like ImageNet. Qualitative results show that our method captures information that is temporally varying, such as human pose. When used as pre-training for action recognition, our method gives significant gains over learning without external data on benchmark datasets like UCF101 and HMDB51. To demonstrate its sensitivity to human pose, we show results for pose estimation on the FLIC and MPII datasets that are competitive, or better than approaches using significantly more supervision. Our method can be combined with supervised representations to provide an additional boost in accuracy.\n    ",
        "submission_date": "2016-03-28T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08976",
        "title": "Local Search Yields a PTAS for k-Means in Doubling Metrics",
        "authors": [
            "Zachary Friggstad",
            "Mohsen Rezapour",
            "Mohammad R. Salavatipour"
        ],
        "abstract": "The most well known and ubiquitous clustering problem encountered in nearly every branch of science is undoubtedly $k$-means: given a set of data points and a parameter $k$, select $k$ centres and partition the data points into $k$ clusters around these centres so that the sum of squares of distances of the points to their cluster centre is minimized. Typically these data points lie $\\mathbb{R}^d$ for some $d\\geq 2$.\n",
        "submission_date": "2016-03-29T00:00:00",
        "last_modified_date": "2017-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09405",
        "title": "Enhancing Sentence Relation Modeling with Auxiliary Character-level Embedding",
        "authors": [
            "Peng Li",
            "Heng Huang"
        ],
        "abstract": "Neural network based approaches for sentence relation modeling automatically generate hidden matching features from raw sentence pairs. However, the quality of matching feature representation may not be satisfied due to complex semantic relations such as entailment or contradiction. To address this challenge, we propose a new deep neural network architecture that jointly leverage pre-trained word embedding and auxiliary character embedding to learn sentence meanings. The two kinds of word sequence representations as inputs into multi-layer bidirectional LSTM to learn enhanced sentence representation. After that, we construct matching features followed by another temporal CNN to learn high-level hidden matching feature representations. Experimental results demonstrate that our approach consistently outperforms the existing methods on standard evaluation datasets.\n    ",
        "submission_date": "2016-03-30T00:00:00",
        "last_modified_date": "2016-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09488",
        "title": "Building the Signature of Set Theory Using the MathSem Program",
        "authors": [
            "Andrey Luxemburg"
        ],
        "abstract": "Knowledge representation is a popular research field in IT. As mathematical knowledge is most formalized, its representation is important and interesting. Mathematical knowledge consists of various mathematical theories. In this paper we consider a deductive system that derives mathematical notions, axioms and theorems. All these notions, axioms and theorems can be considered as the part of elementary set theory. This theory will be represented as a semantic net.\n    ",
        "submission_date": "2016-03-31T00:00:00",
        "last_modified_date": "2016-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09727",
        "title": "Neural Language Correction with Character-Based Attention",
        "authors": [
            "Ziang Xie",
            "Anand Avati",
            "Naveen Arivazhagan",
            "Dan Jurafsky",
            "Andrew Y. Ng"
        ],
        "abstract": "Natural language correction has the potential to help language learners improve their writing skills. While approaches with separate classifiers for different error types have high precision, they do not flexibly handle errors such as redundancy or non-idiomatic phrasing. On the other hand, word and phrase-based machine translation methods are not designed to cope with orthographic errors, and have recently been outpaced by neural models. Motivated by these issues, we present a neural network-based approach to language correction. The core component of our method is an encoder-decoder recurrent neural network with an attention mechanism. By operating at the character level, the network avoids the problem of out-of-vocabulary words. We illustrate the flexibility of our approach on dataset of noisy, user-generated text collected from an English learner forum. When combined with a language model, our method achieves a state-of-the-art $F_{0.5}$-score on the CoNLL 2014 Shared Task. We further demonstrate that training the network on additional data with synthesized errors can improve performance.\n    ",
        "submission_date": "2016-03-31T00:00:00",
        "last_modified_date": "2016-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00066",
        "title": "To Fall Or Not To Fall: A Visual Approach to Physical Stability Prediction",
        "authors": [
            "Wenbin Li",
            "Seyedmajid Azimi",
            "Ale\u0161 Leonardis",
            "Mario Fritz"
        ],
        "abstract": "Understanding physical phenomena is a key competence that enables humans and animals to act and interact under uncertain perception in previously unseen environments containing novel object and their configurations. Developmental psychology has shown that such skills are acquired by infants from observations at a very early stage.\n",
        "submission_date": "2016-03-31T00:00:00",
        "last_modified_date": "2016-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00449",
        "title": "3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction",
        "authors": [
            "Christopher B. Choy",
            "Danfei Xu",
            "JunYoung Gwak",
            "Kevin Chen",
            "Silvio Savarese"
        ],
        "abstract": "Inspired by the recent success of methods that employ shape priors to achieve robust 3D reconstructions, we propose a novel recurrent neural network architecture that we call the 3D Recurrent Reconstruction Neural Network (3D-R2N2). The network learns a mapping from images of objects to their underlying 3D shapes from a large collection of synthetic data. Our network takes in one or more images of an object instance from arbitrary viewpoints and outputs a reconstruction of the object in the form of a 3D occupancy grid. Unlike most of the previous works, our network does not require any image annotations or object class labels for training or testing. Our extensive experimental analysis shows that our reconstruction framework i) outperforms the state-of-the-art methods for single view reconstruction, and ii) enables the 3D reconstruction of objects in situations when traditional SFM/SLAM methods fail (because of lack of texture and/or wide baseline).\n    ",
        "submission_date": "2016-04-02T00:00:00",
        "last_modified_date": "2016-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00461",
        "title": "Embedding Lexical Features via Low-Rank Tensors",
        "authors": [
            "Mo Yu",
            "Mark Dredze",
            "Raman Arora",
            "Matthew Gormley"
        ],
        "abstract": "Modern NLP models rely heavily on engineered features, which often combine word and contextual information into complex lexical features. Such combination results in large numbers of features, which can lead to over-fitting. We present a new model that represents complex lexical features---comprised of parts for words, contextual information and labels---in a tensor that captures conjunction information among these parts. We apply low-rank tensor approximations to the corresponding parameter tensors to reduce the parameter space and improve prediction speed. Furthermore, we investigate two methods for handling features that include $n$-grams of mixed lengths. Our model achieves state-of-the-art results on tasks in relation extraction, PP-attachment, and preposition disambiguation.\n    ",
        "submission_date": "2016-04-02T00:00:00",
        "last_modified_date": "2016-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00536",
        "title": "Improving SAT Solvers via Blocked Clause Decomposition",
        "authors": [
            "Jingchao Chen"
        ],
        "abstract": "The decision variable selection policy used by the most competitive CDCL (Conflict-Driven Clause Learning) SAT solvers is either VSIDS (Variable State Independent Decaying Sum) or its variants such as exponential version EVSIDS. The common characteristic of VSIDS and its variants is to make use of statistical information in the solving process, but ignore structure information of the problem. For this reason, this paper modifies the decision variable selection policy, and presents a SAT solving technique based on BCD (Blocked Clause Decomposition). Its basic idea is that a part of decision variables are selected by VSIDS heuristic, while another part of decision variables are selected by blocked sets that are obtained by BCD. Compared with the existing BCD-based technique, our technique is simple, and need not to reencode CNF formulas. SAT solvers for certified UNSAT track can apply also our BCD-based technique. Our experiments on application benchmarks demonstrate that the new variables selection policy based on BCD can increase the performance of SAT solvers such as abcdSAT. The solver with BCD solved an instance from the SAT Race 2015 that was not solved by any solver so far. This shows that in some cases, the heuristic based on structure information is more efficient than that based on statistical information.\n    ",
        "submission_date": "2016-04-02T00:00:00",
        "last_modified_date": "2016-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00644",
        "title": "An electronic-game framework for evaluating coevolutionary algorithms",
        "authors": [
            "Karine da Silva Miras de Ara\u00fajo",
            "Fabr\u00edcio Olivetti de Fran\u00e7a"
        ],
        "abstract": "One of the common artificial intelligence applications in electronic games consists of making an artificial agent learn how to execute some determined task successfully in a game environment. One way to perform this task is through machine learning algorithms capable of learning the sequence of actions required to win in a given game environment. There are several supervised learning techniques able to learn the correct answer for a problem through examples. However, when learning how to play electronic games, the correct answer might only be known by the end of the game, after all the actions were already taken. Thus, not being possible to measure the accuracy of each individual action to be taken at each time step. A way for dealing with this problem is through Neuroevolution, a method which trains Artificial Neural Networks using evolutionary algorithms. In this article, we introduce a framework for testing optimization algorithms with artificial agent controllers in electronic games, called EvoMan, which is inspired in the action-platformer game Mega Man II. The environment can be configured to run in different experiment modes, as single evolution, coevolution and others. To demonstrate some challenges regarding the proposed platform, as initial experiments we applied Neuroevolution using Genetic Algorithms and the NEAT algorithm, in the context of competitively coevolving two distinct agents in this game.\n    ",
        "submission_date": "2016-04-03T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00647",
        "title": "Multi-Relational Learning at Scale with ADMM",
        "authors": [
            "Lucas Drumond",
            "Ernesto Diaz-Aviles",
            "Lars Schmidt-Thieme"
        ],
        "abstract": "Learning from multiple-relational data which contains noise, ambiguities, or duplicate entities is essential to a wide range of applications such as statistical inference based on Web Linked Data, recommender systems, computational biology, and natural language processing. These tasks usually require working with very large and complex datasets - e.g., the Web graph - however, current approaches to multi-relational learning are not practical for such scenarios due to their high computational complexity and poor scalability on large data.\n",
        "submission_date": "2016-04-03T00:00:00",
        "last_modified_date": "2016-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00664",
        "title": "Bicycle-Sharing System Analysis and Trip Prediction",
        "authors": [
            "Jiawei Zhang",
            "Xiao Pan",
            "Moyin Li",
            "Philip S. Yu"
        ],
        "abstract": "Bicycle-sharing systems, which can provide shared bike usage services for the public, have been launched in many big cities. In bicycle-sharing systems, people can borrow and return bikes at any stations in the service region very conveniently. Therefore, bicycle-sharing systems are normally used as a short-distance trip supplement for private vehicles as well as regular public transportation. Meanwhile, for stations located at different places in the service region, the bike usages can be quite skewed and imbalanced. Some stations have too many incoming bikes and get jammed without enough docks for upcoming bikes, while some other stations get empty quickly and lack enough bikes for people to check out. Therefore, inferring the potential destinations and arriving time of each individual trip beforehand can effectively help the service providers schedule manual bike re-dispatch in advance. In this paper, we will study the individual trip prediction problem for bicycle-sharing systems. To address the problem, we study a real-world bicycle-sharing system and analyze individuals' bike usage behaviors first. Based on the analysis results, a new trip destination prediction and trip duration inference model will be introduced. Experiments conducted on a real-world bicycle-sharing system demonstrate the effectiveness of the proposed model.\n    ",
        "submission_date": "2016-04-03T00:00:00",
        "last_modified_date": "2016-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00697",
        "title": "A New Learning Method for Inference Accuracy, Core Occupation, and Performance Co-optimization on TrueNorth Chip",
        "authors": [
            "Wei Wen",
            "Chunpeng Wu",
            "Yandan Wang",
            "Kent Nixon",
            "Qing Wu",
            "Mark Barnell",
            "Hai Li",
            "Yiran Chen"
        ],
        "abstract": "IBM TrueNorth chip uses digital spikes to perform neuromorphic computing and achieves ultrahigh execution parallelism and power efficiency. However, in TrueNorth chip, low quantization resolution of the synaptic weights and spikes significantly limits the inference (e.g., classification) accuracy of the deployed neural network model. Existing workaround, i.e., averaging the results over multiple copies instantiated in spatial and temporal domains, rapidly exhausts the hardware resources and slows down the computation. In this work, we propose a novel learning method on TrueNorth platform that constrains the random variance of each computation copy and reduces the number of needed copies. Compared to the existing learning method, our method can achieve up to 68.8% reduction of the required neuro-synaptic cores or 6.5X speedup, with even slightly improved inference accuracy.\n    ",
        "submission_date": "2016-04-03T00:00:00",
        "last_modified_date": "2016-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00727",
        "title": "Character-Level Question Answering with Attention",
        "authors": [
            "David Golub",
            "Xiaodong He"
        ],
        "abstract": "We show that a character-level encoder-decoder framework can be successfully applied to question answering with a structured knowledge base. We use our model for single-relation question answering and demonstrate the effectiveness of our approach on the SimpleQuestions dataset (Bordes et al., 2015), where we improve state-of-the-art accuracy from 63.9% to 70.9%, without use of ensembles. Importantly, our character-level model has 16x fewer parameters than an equivalent word-level model, can be learned with significantly less data compared to previous work, which relies on data augmentation, and is robust to new entities in testing.\n    ",
        "submission_date": "2016-04-04T00:00:00",
        "last_modified_date": "2016-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00921",
        "title": "A Review of Theoretical and Practical Challenges of Trusted Autonomy in Big Data",
        "authors": [
            "Hussein A. Abbass",
            "George Leu",
            "Kathryn Merrick"
        ],
        "abstract": "Despite the advances made in artificial intelligence, software agents, and robotics, there is little we see today that we can truly call a fully autonomous system. We conjecture that the main inhibitor for advancing autonomy is lack of trust. Trusted autonomy is the scientific and engineering field to establish the foundations and ground work for developing trusted autonomous systems (robotics and software agents) that can be used in our daily life, and can be integrated with humans seamlessly, naturally and efficiently.\n",
        "submission_date": "2016-03-16T00:00:00",
        "last_modified_date": "2016-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00923",
        "title": "Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning",
        "authors": [
            "Philip S. Thomas",
            "Emma Brunskill"
        ],
        "abstract": "In this paper we present a new way of predicting the performance of a reinforcement learning policy given historical data that may have been generated by a different policy. The ability to evaluate a policy from historical data is important for applications where the deployment of a bad policy can be dangerous or costly. We show empirically that our algorithm produces estimates that often have orders of magnitude lower mean squared error than existing methods---it makes more efficient use of the available data. Our new estimator is based on two advances: an extension of the doubly robust estimator (Jiang and Li, 2015), and a new way to mix between model based estimates and importance sampling based estimates.\n    ",
        "submission_date": "2016-04-04T00:00:00",
        "last_modified_date": "2016-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00932",
        "title": "Asking the metaquestions in constraint tractability",
        "authors": [
            "Hubie Chen",
            "Benoit Larose"
        ],
        "abstract": "The constraint satisfaction problem (CSP) involves deciding, given a set of variables and a set of constraints on the variables, whether or not there is an assignment to the variables satisfying all of the constraints. One formulation of the CSP is as the problem of deciding, given a pair (G,H) of relational structures, whether or not there is a homomorphism from the first structure to the second structure. The CSP is in general NP-hard; a common way to restrict this problem is to fix the second structure H, so that each structure H gives rise to a problem CSP(H). The problem family CSP(H) has been studied using an algebraic approach, which links the algorithmic and complexity properties of each problem CSP(H) to a set of operations, the so-called polymorphisms of H. Certain types of polymorphisms are known to imply the polynomial-time tractability of $CSP(H)$, and others are conjectured to do so. This article systematically studies---for various classes of polymorphisms---the computational complexity of deciding whether or not a given structure H admits a polymorphism from the class. Among other results, we prove the NP-completeness of deciding a condition conjectured to characterize the tractable problems CSP(H), as well as the NP-completeness of deciding if CSP(H) has bounded width.\n    ",
        "submission_date": "2016-04-04T00:00:00",
        "last_modified_date": "2017-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00980",
        "title": "A Mathematical Trust Algebra for International Nation Relations Computation and Evaluation",
        "authors": [
            "Mohd Anuar Mat Isa",
            "Ramlan Mahmod",
            "Nur Izura Udzir",
            "Jamalul-lail Ab Manan",
            "Ali Dehghan Tanha"
        ],
        "abstract": "This paper presents a trust computation for international relations and its calculus, which related to Bayesian inference, Dempster Shafer theory and subjective logic. We proposed a method that allows a trust computation which is previously subjective and incomputable. An example of case study for the trust computation is the United States of America Great Britain relations. The method supports decision makers in a government such as foreign ministry, defense ministry, presidential or prime minister office. The Department of Defense (DoD) may use our method to determine a nation that can be known as a friendly, neutral or hostile nation.\n    ",
        "submission_date": "2016-02-13T00:00:00",
        "last_modified_date": "2016-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01166",
        "title": "An Efficient Algorithm for Mining Frequent Sequence with Constraint Programming",
        "authors": [
            "John O.R. Aoga",
            "Tias Guns",
            "Pierre Schaus"
        ],
        "abstract": "The main advantage of Constraint Programming (CP) approaches for sequential pattern mining (SPM) is their modularity, which includes the ability to add new constraints (regular expressions, length restrictions, etc). The current best CP approach for SPM uses a global constraint (module) that computes the projected database and enforces the minimum frequency; it does this with a filtering algorithm similar to the PrefixSpan method. However, the resulting system is not as scalable as some of the most advanced mining systems like Zaki's cSPADE. We show how, using techniques from both data mining and CP, one can use a generic constraint solver and yet outperform existing specialized systems. This is mainly due to two improvements in the module that computes the projected frequencies: first, computing the projected database can be sped up by pre-computing the positions at which an symbol can become unsupported by a sequence, thereby avoiding to scan the full sequence each time; and second by taking inspiration from the trailing used in CP solvers to devise a backtracking-aware data structure that allows fast incremental storing and restoring of the projected database. Detailed experiments show how this approach outperforms existing CP as well as specialized systems for SPM, and that the gain in efficiency translates directly into increased efficiency for other settings such as mining with regular expressions.\n    ",
        "submission_date": "2016-04-05T00:00:00",
        "last_modified_date": "2016-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01272",
        "title": "Feature extraction using Latent Dirichlet Allocation and Neural Networks: A case study on movie synopses",
        "authors": [
            "Despoina Christou"
        ],
        "abstract": "Feature extraction has gained increasing attention in the field of machine learning, as in order to detect patterns, extract information, or predict future observations from big data, the urge of informative features is crucial. The process of extracting features is highly linked to dimensionality reduction as it implies the transformation of the data from a sparse high-dimensional space, to higher level meaningful abstractions. This dissertation employs Neural Networks for distributed paragraph representations, and Latent Dirichlet Allocation to capture higher level features of paragraph vectors. Although Neural Networks for distributed paragraph representations are considered the state of the art for extracting paragraph vectors, we show that a quick topic analysis model such as Latent Dirichlet Allocation can provide meaningful features too. We evaluate the two methods on the CMU Movie Summary Corpus, a collection of 25,203 movie plot summaries extracted from Wikipedia. Finally, for both approaches, we use K-Nearest Neighbors to discover similar movies, and plot the projected representations using T-Distributed Stochastic Neighbor Embedding to depict the context similarities. These similarities, expressed as movie distances, can be used for movies recommendation. The recommended movies of this approach are compared with the recommended movies from IMDB, which use a collaborative filtering recommendation approach, to show that our two models could constitute either an alternative or a supplementary recommendation approach.\n    ",
        "submission_date": "2016-04-05T00:00:00",
        "last_modified_date": "2016-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01335",
        "title": "Deep Cross Residual Learning for Multitask Visual Recognition",
        "authors": [
            "Brendan Jou",
            "Shih-Fu Chang"
        ],
        "abstract": "Residual learning has recently surfaced as an effective means of constructing very deep neural networks for object recognition. However, current incarnations of residual networks do not allow for the modeling and integration of complex relations between closely coupled recognition tasks or across domains. Such problems are often encountered in multimedia applications involving large-scale content recognition. We propose a novel extension of residual learning for deep networks that enables intuitive learning across multiple related tasks using cross-connections called cross-residuals. These cross-residuals connections can be viewed as a form of in-network regularization and enables greater network generalization. We show how cross-residual learning (CRL) can be integrated in multitask networks to jointly train and detect visual concepts across several tasks. We present a single multitask cross-residual network with >40% less parameters that is able to achieve competitive, or even better, detection performance on a visual sentiment concept detection problem normally requiring multiple specialized single-task networks. The resulting multitask cross-residual network also achieves better detection performance by about 10.4% over a standard multitask residual network without cross-residuals with even a small amount of cross-task weighting.\n    ",
        "submission_date": "2016-04-05T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01360",
        "title": "The Curious Robot: Learning Visual Representations via Physical Interactions",
        "authors": [
            "Lerrel Pinto",
            "Dhiraj Gandhi",
            "Yuanfeng Han",
            "Yong-Lae Park",
            "Abhinav Gupta"
        ],
        "abstract": "What is the right supervisory signal to train visual representations? Current approaches in computer vision use category labels from datasets such as ImageNet to train ConvNets. However, in case of biological agents, visual representation learning does not require millions of semantic labels. We argue that biological agents use physical interactions with the world to learn visual representations unlike current vision systems which just use passive observations (images and videos downloaded from web). For example, babies push objects, poke them, put them in their mouth and throw them to learn representations. Towards this goal, we build one of the first systems on a Baxter platform that pushes, pokes, grasps and observes objects in a tabletop environment. It uses four different types of physical interactions to collect more than 130K datapoints, with each datapoint providing supervision to a shared ConvNet architecture allowing us to learn visual representations. We show the quality of learned representations by observing neuron activations and performing nearest neighbor retrieval on this learned representation. Quantitatively, we evaluate our learned ConvNet on image classification tasks and show improvements compared to learning without external data. Finally, on the task of instance retrieval, our network outperforms the ImageNet network on recall@1 by 3%\n    ",
        "submission_date": "2016-04-05T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01662",
        "title": "A Survey on Bayesian Deep Learning",
        "authors": [
            "Hao Wang",
            "Dit-Yan Yeung"
        ],
        "abstract": "A comprehensive artificial intelligence system needs to not only perceive the environment with different `senses' (e.g., seeing and hearing) but also infer the world's conditional (or even causal) relations and corresponding uncertainty. The past decade has seen major advances in many perception tasks such as visual object recognition and speech recognition using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. In recent years, Bayesian deep learning has emerged as a unified probabilistic framework to tightly integrate deep learning and Bayesian models. In this general framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in turn, the feedback from the inference process is able to enhance the perception of text or images. This survey provides a comprehensive introduction to Bayesian deep learning and reviews its recent applications on recommender systems, topic models, control, etc. Besides, we also discuss the relationship and differences between Bayesian deep learning and other related topics such as Bayesian treatment of neural networks. For a constantly updating project page, please refer to ",
        "submission_date": "2016-04-06T00:00:00",
        "last_modified_date": "2021-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01673",
        "title": "On the uniform one-dimensional fragment",
        "authors": [
            "Antti Kuusisto"
        ],
        "abstract": "The uniform one-dimensional fragment of first-order logic, U1, is a formalism that extends two-variable logic in a natural way to contexts with relations of all arities. We survey properties of U1 and investigate its relationship to description logics designed to accommodate higher arity relations, with particular attention given to DLR_reg. We also define a description logic version of a variant of U1 and prove a range of new results concerning the expressivity of U1 and related logics.\n    ",
        "submission_date": "2016-04-06T00:00:00",
        "last_modified_date": "2023-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01696",
        "title": "A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories",
        "authors": [
            "Nasrin Mostafazadeh",
            "Nathanael Chambers",
            "Xiaodong He",
            "Devi Parikh",
            "Dhruv Batra",
            "Lucy Vanderwende",
            "Pushmeet Kohli",
            "James Allen"
        ],
        "abstract": "Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the 'Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of ~50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.\n    ",
        "submission_date": "2016-04-06T00:00:00",
        "last_modified_date": "2016-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01734",
        "title": "Efficiency and Sequenceability in Fair Division of Indivisible Goods with Additive Preferences",
        "authors": [
            "Sylvain Bouveret",
            "Michel Lema\u00eetre"
        ],
        "abstract": "In fair division of indivisible goods, using sequences of sincere choices (or picking sequences) is a natural way to allocate the objects. The idea is the following: at each stage, a designated agent picks one object among those that remain. This paper, restricted to the case where the agents have numerical additive preferences over objects, revisits to some extent the seminal paper by Brams and King [9] which was specific to ordinal and linear order preferences over items. We point out similarities and differences with this latter context. In particular, we show that any Pareto-optimal allocation (under additive preferences) is sequenceable, but that the converse is not true anymore. This asymmetry leads naturally to the definition of a \"scale of efficiency\" having three steps: Pareto-optimality, sequenceability without Pareto-optimality, and non-sequenceability. Finally, we investigate the links between these efficiency properties and the \"scale of fairness\" we have described in an earlier work [7]: we first show that an allocation can be envy-free and non-sequenceable, but that every competitive equilibrium with equal incomes is sequenceable. Then we experimentally explore the links between the scales of efficiency and fairness.\n    ",
        "submission_date": "2016-04-06T00:00:00",
        "last_modified_date": "2016-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01785",
        "title": "Safe Probability",
        "authors": [
            "Peter Gr\u00fcnwald"
        ],
        "abstract": "We formalize the idea of probability distributions that lead to reliable predictions about some, but not all aspects of a domain. The resulting notion of `safety' provides a fresh perspective on foundational issues in statistics, providing a middle ground between imprecise probability and multiple-prior models on the one hand and strictly Bayesian approaches on the other. It also allows us to formalize fiducial distributions in terms of the set of random variables that they can safely predict, thus taking some of the sting out of the fiducial idea. By restricting probabilistic inference to safe uses, one also automatically avoids paradoxes such as the Monty Hall problem. Safety comes in a variety of degrees, such as \"validity\" (the strongest notion), \"calibration\", \"confidence safety\" and \"unbiasedness\" (almost the weakest notion).\n    ",
        "submission_date": "2016-04-06T00:00:00",
        "last_modified_date": "2016-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01802",
        "title": "Learning to Track at 100 FPS with Deep Regression Networks",
        "authors": [
            "David Held",
            "Sebastian Thrun",
            "Silvio Savarese"
        ],
        "abstract": "Machine learning techniques are often used in computer vision due to their ability to leverage large amounts of training data to improve performance. Unfortunately, most generic object trackers are still trained from scratch online and do not benefit from the large number of videos that are readily available for offline training. We propose a method for offline training of neural networks that can track novel objects at test-time at 100 fps. Our tracker is significantly faster than previous methods that use neural networks for tracking, which are typically very slow to run and not practical for real-time applications. Our tracker uses a simple feed-forward network with no online training required. The tracker learns a generic relationship between object motion and appearance and can be used to track novel objects that do not appear in the training set. We test our network on a standard tracking benchmark to demonstrate our tracker's state-of-the-art performance. Further, our performance improves as we add more videos to our offline training set. To the best of our knowledge, our tracker is the first neural-network tracker that learns to track generic objects at 100 fps.\n    ",
        "submission_date": "2016-04-06T00:00:00",
        "last_modified_date": "2016-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02006",
        "title": "Accelerating Science: A Computing Research Agenda",
        "authors": [
            "Vasant G. Honavar",
            "Mark D. Hill",
            "Katherine Yelick"
        ],
        "abstract": "The emergence of \"big data\" offers unprecedented opportunities for not only accelerating scientific advances but also enabling new modes of discovery. Scientific progress in many disciplines is increasingly enabled by our ability to examine natural phenomena through the computational lens, i.e., using algorithmic or information processing abstractions of the underlying processes; and our ability to acquire, share, integrate and analyze disparate types of data. However, there is a huge gap between our ability to acquire, store, and process data and our ability to make effective use of the data to advance discovery. Despite successful automation of routine aspects of data management and analytics, most elements of the scientific process currently require considerable human expertise and effort. Accelerating science to keep pace with the rate of data acquisition and data processing calls for the development of algorithmic or information processing abstractions, coupled with formal methods and tools for modeling and simulation of natural processes as well as major innovations in cognitive tools for scientists, i.e., computational tools that leverage and extend the reach of human intellect, and partner with humans on a broad range of tasks in scientific discovery (e.g., identifying, prioritizing formulating questions, designing, prioritizing and executing experiments designed to answer a chosen question, drawing inferences and evaluating the results, and formulating new questions, in a closed-loop fashion). This calls for concerted research agenda aimed at: Development, analysis, integration, sharing, and simulation of algorithmic or information processing abstractions of natural processes, coupled with formal methods and tools for their analyses and simulation; Innovations in cognitive tools that augment and extend human intellect and partner with humans in all aspects of science.\n    ",
        "submission_date": "2016-04-06T00:00:00",
        "last_modified_date": "2016-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02523",
        "title": "Differential Evolution for Efficient AUV Path Planning in Time Variant Uncertain Underwater Environment",
        "authors": [
            "S. Mahmoud Zadeh",
            "D. M.W. Powers",
            "A. Yazdani",
            "K. Sammut",
            "A Atyabi"
        ],
        "abstract": "The AUV three-dimension path planning in complex turbulent underwater environment is investigated in this research, in which static current map data and uncertain static-moving time variant obstacles are taken into account. Robustness of AUVs path planning to this strong variability is known as a complex NP-hard problem and is considered a critical issue to ensure vehicles safe deployment. Efficient evolutionary techniques have substantial potential of handling NP hard complexity of path planning problem as more powerful and fast algorithms among other approaches for mentioned problem. For the purpose of this research Differential Evolution (DE) technique is conducted to solve the AUV path planning problem in a realistic underwater environment. The path planners designed in this paper are capable of extracting feasible areas of a real map to determine the allowed spaces for deployment, where coastal area, islands, static/dynamic obstacles and ocean current is taken into account and provides the efficient path with a small computation time. The results obtained from analyze of experimental demonstrate the inherent robustness and drastic efficiency of the proposed scheme in enhancement of the vehicles path planning capability in coping undesired current, using useful current flow, and avoid colliding collision boundaries in a real-time manner. The proposed approach is also flexible and strictly respects to vehicle's kinematic constraints resisting current instabilities.\n    ",
        "submission_date": "2016-04-09T00:00:00",
        "last_modified_date": "2016-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03114",
        "title": "Conversational flow in Oxford-style debates",
        "authors": [
            "Justine Zhang",
            "Ravi Kumar",
            "Sujith Ravi",
            "Cristian Danescu-Niculescu-Mizil"
        ],
        "abstract": "Public debates are a common platform for presenting and juxtaposing diverging views on important issues. In this work we propose a methodology for tracking how ideas flow between participants throughout a debate. We use this approach in a case study of Oxford-style debates---a competitive format where the winner is determined by audience votes---and show how the outcome of a debate depends on aspects of conversational flow. In particular, we find that winners tend to make better use of a debate's interactive component than losers, by actively pursuing their opponents' points rather than promoting their own ideas over the course of the conversation.\n    ",
        "submission_date": "2016-04-11T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03243",
        "title": "Separating Sets of Strings by Finding Matching Patterns is Almost Always Hard",
        "authors": [
            "Giuseppe Lancia",
            "Luke Mathieson",
            "Pablo Moscato"
        ],
        "abstract": "We study the complexity of the problem of searching for a set of patterns that separate two given sets of strings. This problem has applications in a wide variety of areas, most notably in data mining, computational biology, and in understanding the complexity of genetic algorithms. We show that the basic problem of finding a small set of patterns that match one set of strings but do not match any string in a second set is difficult (NP-complete, W[2]-hard when parameterized by the size of the pattern set, and APX-hard). We then perform a detailed parameterized analysis of the problem, separating tractable and intractable variants. In particular we show that parameterizing by the size of pattern set and the number of strings, and the size of the alphabet and the number of strings give FPT results, amongst others.\n    ",
        "submission_date": "2016-04-12T00:00:00",
        "last_modified_date": "2016-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03265",
        "title": "Volumetric and Multi-View CNNs for Object Classification on 3D Data",
        "authors": [
            "Charles R. Qi",
            "Hao Su",
            "Matthias Niessner",
            "Angela Dai",
            "Mengyuan Yan",
            "Leonidas J. Guibas"
        ],
        "abstract": "3D shape models are becoming widely available and easier to capture, making available 3D information crucial for progress in object classification. Current state-of-the-art methods rely on CNNs to address this problem. Recently, we witness two types of CNNs being developed: CNNs based upon volumetric representations versus CNNs based upon multi-view representations. Empirical results from these two types of CNNs exhibit a large gap, indicating that existing volumetric CNN architectures and approaches are unable to fully exploit the power of 3D representations. In this paper, we aim to improve both volumetric CNNs and multi-view CNNs according to extensive analysis of existing approaches. To this end, we introduce two distinct network architectures of volumetric CNNs. In addition, we examine multi-view CNNs, where we introduce multi-resolution filtering in 3D. Overall, we are able to outperform current state-of-the-art methods for both volumetric CNNs and multi-view CNNs. We provide extensive experiments designed to evaluate underlying design choices, thus providing a better understanding of the space of methods available for object classification on 3D data.\n    ",
        "submission_date": "2016-04-12T00:00:00",
        "last_modified_date": "2016-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03303",
        "title": "Optimal Route Planning with Prioritized Task Scheduling for AUV Missions",
        "authors": [
            "S. Mahmoud Zadeh",
            "D. Powers",
            "K. Sammut",
            "A. Lammas",
            "A.M. Yazdani"
        ],
        "abstract": "This paper presents a solution to Autonomous Underwater Vehicles (AUVs) large scale route planning and task assignment joint problem. Given a set of constraints (e.g., time) and a set of task priority values, the goal is to find the optimal route for underwater mission that maximizes the sum of the priorities and minimizes the total risk percentage while meeting the given constraints. Making use of the heuristic nature of genetic and swarm intelligence algorithms in solving NP-hard graph problems, Particle Swarm Optimization (PSO) and Genetic Algorithm (GA) are employed to find the optimum solution, where each individual in the population is a candidate solution (route). To evaluate the robustness of the proposed methods, the performance of the all PS and GA algorithms are examined and compared for a number of Monte Carlo runs. Simulation results suggest that the routes generated by both algorithms are feasible and reliable enough, and applicable for underwater motion planning. However, the GA-based route planner produces superior results comparing to the results obtained from the PSO based route planner.\n    ",
        "submission_date": "2016-04-12T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03458",
        "title": "Resource Allocation with Population Dynamics",
        "authors": [
            "Jonathan Epperlein",
            "Jakub Marecek"
        ],
        "abstract": "Many analyses of resource-allocation problems employ simplistic models of the population. Using the example of a resource-allocation problem of Marecek et al. [",
        "submission_date": "2016-04-12T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03468",
        "title": "Backward-Forward Search for Manipulation Planning",
        "authors": [
            "Caelan Reed Garrett",
            "Tomas Lozano-Perez",
            "Leslie Pack Kaelbling"
        ],
        "abstract": "In this paper we address planning problems in high-dimensional hybrid configuration spaces, with a particular focus on manipulation planning problems involving many objects. We present the hybrid backward-forward (HBF) planning algorithm that uses a backward identification of constraints to direct the sampling of the infinite action space in a forward search from the initial state towards a goal configuration. The resulting planner is probabilistically complete and can effectively construct long manipulation plans requiring both prehensile and nonprehensile actions in cluttered environments.\n    ",
        "submission_date": "2016-04-12T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03526",
        "title": "Spatiotemporal Articulated Models for Dynamic SLAM",
        "authors": [
            "Suren Kumar",
            "Vikas Dhiman",
            "Madan Ravi Ganesh",
            "Jason J. Corso"
        ],
        "abstract": "We propose an online spatiotemporal articulation model estimation framework that estimates both articulated structure as well as a temporal prediction model solely using passive observations. The resulting model can predict future mo- tions of an articulated object with high confidence because of the spatial and temporal structure. We demonstrate the effectiveness of the predictive model by incorporating it within a standard simultaneous localization and mapping (SLAM) pipeline for mapping and robot localization in previously unexplored dynamic environments. Our method is able to localize the robot and map a dynamic scene by explaining the observed motion in the world. We demonstrate the effectiveness of the proposed framework for both simulated and real-world dynamic environments.\n    ",
        "submission_date": "2016-04-12T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03632",
        "title": "Strategyproof Peer Selection using Randomization, Partitioning, and Apportionment",
        "authors": [
            "Haris Aziz",
            "Omer Lev",
            "Nicholas Mattei",
            "Jeffrey S. Rosenschein",
            "Toby Walsh"
        ],
        "abstract": "Peer reviews, evaluations, and selections are a fundamental aspect of modern science. Funding bodies the world over employ experts to review and select the best proposals from those submitted for funding. The problem of peer selection, however, is much more general: a professional society may want to give a subset of its members awards based on the opinions of all members; an instructor for a Massive Open Online Course (MOOC) or an online course may want to crowdsource grading; or a marketing company may select ideas from group brainstorming sessions based on peer evaluation.\n",
        "submission_date": "2016-04-13T00:00:00",
        "last_modified_date": "2019-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03655",
        "title": "A Discrete and Bounded Envy-Free Cake Cutting Protocol for Any Number of Agents",
        "authors": [
            "Haris Aziz",
            "Simon Mackenzie"
        ],
        "abstract": "We consider the well-studied cake cutting problem in which the goal is to find an envy-free allocation based on queries from $n$ agents. The problem has received attention in computer science, mathematics, and economics. It has been a major open problem whether there exists a discrete and bounded envy-free protocol. We resolve the problem by proposing a discrete and bounded envy-free protocol for any number of agents. The maximum number of queries required by the protocol is $n^{n^{n^{n^{n^n}}}}$. We additionally show that even if we do not run our protocol to completion, it can find in at most $n^3{(n^2)}^n$ queries a partial allocation of the cake that achieves proportionality (each agent gets at least $1/n$ of the value of the whole cake) and envy-freeness. Finally we show that an envy-free partial allocation can be computed in at most $n^3{(n^2)}^n$ queries such that each agent gets a connected piece that gives the agent at least $1/(3n)$ of the value of the whole cake.\n    ",
        "submission_date": "2016-04-13T00:00:00",
        "last_modified_date": "2017-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03692",
        "title": "Learning Social Affordance for Human-Robot Interaction",
        "authors": [
            "Tianmin Shu",
            "M. S. Ryoo",
            "Song-Chun Zhu"
        ],
        "abstract": "In this paper, we present an approach for robot learning of social affordance from human activity videos. We consider the problem in the context of human-robot interaction: Our approach learns structural representations of human-human (and human-object-human) interactions, describing how body-parts of each agent move with respect to each other and what spatial relations they should maintain to complete each sub-event (i.e., sub-goal). This enables the robot to infer its own movement in reaction to the human body motion, allowing it to naturally replicate such interactions.\n",
        "submission_date": "2016-04-13T00:00:00",
        "last_modified_date": "2016-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03793",
        "title": "HordeQBF: A Modular and Massively Parallel QBF Solver",
        "authors": [
            "Tomas Balyo",
            "Florian Lonsing"
        ],
        "abstract": "The recently developed massively parallel satisfiability (SAT) solver HordeSAT was designed in a modular way to allow the integration of any sequential CDCL-based SAT solver in its core. We integrated the QCDCL-based quantified Boolean formula (QBF) solver DepQBF in HordeSAT to obtain a massively parallel QBF solver---HordeQBF. In this paper we describe the details of this integration and report on results of the experimental evaluation of HordeQBF's performance. HordeQBF achieves superlinear average and median speedup on the hard application instances of the 2014 QBF Gallery.\n    ",
        "submission_date": "2016-04-13T00:00:00",
        "last_modified_date": "2016-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03853",
        "title": "Hierarchical Compound Poisson Factorization",
        "authors": [
            "Mehmet E. Basbug",
            "Barbara E. Engelhardt"
        ],
        "abstract": "Non-negative matrix factorization models based on a hierarchical Gamma-Poisson structure capture user and item behavior effectively in extremely sparse data sets, making them the ideal choice for collaborative filtering applications. Hierarchical Poisson factorization (HPF) in particular has proved successful for scalable recommendation systems with extreme sparsity. HPF, however, suffers from a tight coupling of sparsity model (absence of a rating) and response model (the value of the rating), which limits the expressiveness of the latter. Here, we introduce hierarchical compound Poisson factorization (HCPF) that has the favorable Gamma-Poisson structure and scalability of HPF to high-dimensional extremely sparse matrices. More importantly, HCPF decouples the sparsity model from the response model, allowing us to choose the most suitable distribution for the response. HCPF can capture binary, non-negative discrete, non-negative continuous, and zero-inflated continuous responses. We compare HCPF with HPF on nine discrete and three continuous data sets and conclude that HCPF captures the relationship between sparsity and response better than HPF.\n    ",
        "submission_date": "2016-04-13T00:00:00",
        "last_modified_date": "2016-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03901",
        "title": "Single-Image Depth Perception in the Wild",
        "authors": [
            "Weifeng Chen",
            "Zhao Fu",
            "Dawei Yang",
            "Jia Deng"
        ],
        "abstract": "This paper studies single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings. We introduce a new dataset \"Depth in the Wild\" consisting of images in the wild annotated with relative depth between pairs of random points. We also propose a new algorithm that learns to estimate metric depth using annotations of relative depth. Compared to the state of the art, our algorithm is simpler and performs better. Experiments show that our algorithm, combined with existing RGB-D data and our new relative depth annotations, significantly improves single-image depth perception in the wild.\n    ",
        "submission_date": "2016-04-13T00:00:00",
        "last_modified_date": "2017-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03968",
        "title": "Visual Storytelling",
        "authors": [
            "Ting-Hao",
            "Huang",
            "Francis Ferraro",
            "Nasrin Mostafazadeh",
            "Ishan Misra",
            "Aishwarya Agrawal",
            "Jacob Devlin",
            "Ross Girshick",
            "Xiaodong He",
            "Pushmeet Kohli",
            "Dhruv Batra",
            "C. Lawrence Zitnick",
            "Devi Parikh",
            "Lucy Vanderwende",
            "Michel Galley",
            "Margaret Mitchell"
        ],
        "abstract": "We introduce the first dataset for sequential vision-to-language, and explore how this data may be used for the task of visual storytelling. The first release of this dataset, SIND v.1, includes 81,743 unique photos in 20,211 sequences, aligned to both descriptive (caption) and story language. We establish several strong baselines for the storytelling task, and motivate an automatic metric to benchmark progress. Modelling concrete description as well as figurative and social language, as provided in this dataset and the storytelling task, has the potential to move artificial intelligence from basic understandings of typical visual scenes towards more and more human-like understanding of grounded event structure and subjective expression.\n    ",
        "submission_date": "2016-04-13T00:00:00",
        "last_modified_date": "2016-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04138",
        "title": "An Improved Discrete Bat Algorithm for Symmetric and Asymmetric Traveling Salesman Problems",
        "authors": [
            "Eneko Osaba",
            "Xin-She Yang",
            "Fernando Diaz",
            "Pedro Lopez-Garcia",
            "Roberto Carballedo"
        ],
        "abstract": "Bat algorithm is a population metaheuristic proposed in 2010 which is based on the echolocation or bio-sonar characteristics of microbats. Since its first implementation, the bat algorithm has been used in a wide range of fields. In this paper, we present a discrete version of the bat algorithm to solve the well-known symmetric and asymmetric traveling salesman problems. In addition, we propose an improvement in the basic structure of the classic bat algorithm. To prove that our proposal is a promising approximation method, we have compared its performance in 37 instances with the results obtained by five different techniques: evolutionary simulated annealing, genetic algorithm, an island based distributed genetic algorithm, a discrete firefly algorithm and an imperialist competitive algorithm. In order to obtain fair and rigorous comparisons, we have conducted three different statistical tests along the paper: the Student's $t$-test, the Holm's test, and the Friedman test. We have also compared the convergence behaviour shown by our proposal with the ones shown by the evolutionary simulated annealing, and the discrete firefly algorithm. The experimentation carried out in this study has shown that the presented improved bat algorithm outperforms significantly all the other alternatives in most of the cases.\n    ",
        "submission_date": "2016-04-14T00:00:00",
        "last_modified_date": "2016-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04146",
        "title": "A Discrete Firefly Algorithm to Solve a Rich Vehicle Routing Problem Modelling a Newspaper Distribution System with Recycling Policy",
        "authors": [
            "E. Osaba",
            "Xin-She Yang",
            "F. Diaz",
            "E. Onieva",
            "A. D. Masegosa",
            "A. Perallos"
        ],
        "abstract": "A real-world newspaper distribution problem with recycling policy is tackled in this work. In order to meet all the complex restrictions contained in such a problem, it has been modeled as a rich vehicle routing problem, which can be more specifically considered as an asymmetric and clustered vehicle routing problem with simultaneous pickup and deliveries, variable costs and forbidden paths (AC-VRP-SPDVCFP). This is the first study of such a problem in the literature. For this reason, a benchmark composed by 15 instances has been also proposed. In the design of this benchmark, real geographical positions have been used, located in the province of Bizkaia, Spain. For the proper treatment of this AC-VRP-SPDVCFP, a discrete firefly algorithm (DFA) has been developed. This application is the first application of the firefly algorithm to any rich vehicle routing problem. To prove that the proposed DFA is a promising technique, its performance has been compared with two other well-known techniques: an evolutionary algorithm and an evolutionary simulated annealing. Our results have shown that the DFA has outperformed these two classic meta-heuristics.\n    ",
        "submission_date": "2016-04-14T00:00:00",
        "last_modified_date": "2016-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04169",
        "title": "A Deterministic Annealing Approach to the Multiple Traveling Salesmen and Related Problems",
        "authors": [
            "Mayank Baranwal",
            "Brian Roehl",
            "Srinivasa M. Salapaka"
        ],
        "abstract": "This paper presents a novel and efficient heuristic framework for approximating the solutions to the multiple traveling salesmen problem (m-TSP) and other variants on the TSP. The approach adopted in this paper is an extension of the Maximum-Entropy-Principle (MEP) and the Deterministic Annealing (DA) algorithm. The framework is presented as a general tool that can be suitably adapted to a number of variants on the basic TSP. Additionally, unlike most other heuristics for the TSP, the framework presented in this paper is independent of the edges defined between any two pairs of nodes. This makes the algorithm particularly suited for variants such as the close-enough traveling salesman problem (CETSP) which are challenging due to added computational complexity. The examples presented in this paper illustrate the effectiveness of this new framework for use in TSP and many variants thereof.\n    ",
        "submission_date": "2016-04-14T00:00:00",
        "last_modified_date": "2016-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04358",
        "title": "StalemateBreaker: A Proactive Content-Introducing Approach to Automatic Human-Computer Conversation",
        "authors": [
            "Xiang Li",
            "Lili Mou",
            "Rui Yan",
            "Ming Zhang"
        ],
        "abstract": "Existing open-domain human-computer conversation systems are typically passive: they either synthesize or retrieve a reply provided a human-issued utterance. It is generally presumed that humans should take the role to lead the conversation and introduce new content when a stalemate occurs, and that the computer only needs to \"respond.\" In this paper, we propose StalemateBreaker, a conversation system that can proactively introduce new content when appropriate. We design a pipeline to determine when, what, and how to introduce new content during human-computer conversation. We further propose a novel reranking algorithm Bi-PageRank-HITS to enable rich interaction between conversation context and candidate replies. Experiments show that both the content-introducing approach and the reranking algorithm are effective. Our full StalemateBreaker model outperforms a state-of-the-practice conversation system by +14.4% p@1 when a stalemate occurs.\n    ",
        "submission_date": "2016-04-15T00:00:00",
        "last_modified_date": "2016-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04359",
        "title": "Complexity of Manipulation with Partial Information in Voting",
        "authors": [
            "Palash Dey",
            "Neeldhara Misra",
            "Y. Narahari"
        ],
        "abstract": "The Coalitional Manipulation problem has been studied extensively in the literature for many voting rules. However, most studies have focused on the complete information setting, wherein the manipulators know the votes of the non-manipulators. While this assumption is reasonable for purposes of showing intractability, it is unrealistic for algorithmic considerations. In most real-world scenarios, it is impractical for the manipulators to have accurate knowledge of all the other votes. In this paper, we investigate manipulation with incomplete information. In our framework, the manipulators know a partial order for each voter that is consistent with the true preference of that voter. In this setting, we formulate three natural computational notions of manipulation, namely weak, opportunistic, and strong manipulation. We say that an extension of a partial order is if there exists a manipulative vote for that extension.\n",
        "submission_date": "2016-04-15T00:00:00",
        "last_modified_date": "2017-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04378",
        "title": "Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN",
        "authors": [
            "Shengxian Wan",
            "Yanyan Lan",
            "Jun Xu",
            "Jiafeng Guo",
            "Liang Pang",
            "Xueqi Cheng"
        ],
        "abstract": "Semantic matching, which aims to determine the matching degree between two texts, is a fundamental problem for many NLP applications. Recently, deep learning approach has been applied to this problem and significant improvements have been achieved. In this paper, we propose to view the generation of the global interaction between two texts as a recursive process: i.e. the interaction of two texts at each position is a composition of the interactions between their prefixes as well as the word level interaction at the current position. Based on this idea, we propose a novel deep architecture, namely Match-SRNN, to model the recursive matching structure. Firstly, a tensor is constructed to capture the word level interactions. Then a spatial RNN is applied to integrate the local interactions recursively, with importance determined by four types of gates. Finally, the matching score is calculated based on the global interaction. We show that, after degenerated to the exact matching scenario, Match-SRNN can approximate the dynamic programming process of longest common subsequence. Thus, there exists a clear interpretation for Match-SRNN. Our experiments on two semantic matching tasks showed the effectiveness of Match-SRNN, and its ability of visualizing the learned matching structure.\n    ",
        "submission_date": "2016-04-15T00:00:00",
        "last_modified_date": "2016-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04403",
        "title": "Elicitation for Preferences Single Peaked on Trees",
        "authors": [
            "Palash Dey",
            "Neeldhara Misra"
        ],
        "abstract": "In multiagent systems, we often have a set of agents each of which have a preference ordering over a set of items and one would like to know these preference orderings for various tasks, for example, data analysis, preference aggregation, voting etc. However, we often have a large number of items which makes it impractical to ask the agents for their complete preference ordering. In such scenarios, we usually elicit these agents' preferences by asking (a hopefully small number of) comparison queries --- asking an agent to compare two items. Prior works on preference elicitation focus on unrestricted domain and the domain of single peaked preferences and show that the preferences in single peaked domain can be elicited by much less number of queries compared to unrestricted domain. We extend this line of research and study preference elicitation for single peaked preferences on trees which is a strict superset of the domain of single peaked preferences. We show that the query complexity crucially depends on the number of leaves, the path cover number, and the distance from path of the underlying single peaked tree, whereas the other natural parameters like maximum degree, diameter, pathwidth do not play any direct role in determining query complexity. We then investigate the query complexity for finding a weak Condorcet winner for preferences single peaked on a tree and show that this task has much less query complexity than preference elicitation. Here again we observe that the number of leaves in the underlying single peaked tree and the path cover number of the tree influence the query complexity of the problem.\n    ",
        "submission_date": "2016-04-15T00:00:00",
        "last_modified_date": "2016-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04558",
        "title": "Accessing accurate documents by mining auxiliary document information",
        "authors": [
            "Jinju Joby",
            "Jyothi Korra"
        ],
        "abstract": "Earlier techniques of text mining included algorithms like k-means, Naive Bayes, SVM which classify and cluster the text document for mining relevant information about the documents. The need for improving the mining techniques has us searching for techniques using the available algorithms. This paper proposes one technique which uses the auxiliary information that is present inside the text documents to improve the mining. This auxiliary information can be a description to the content. This information can be either useful or completely useless for mining. The user should assess the worth of the auxiliary information before considering this technique for text mining. In this paper, a combination of classical clustering algorithms is used to mine the datasets. The algorithm runs in two stages which carry out mining at different levels of abstraction. The clustered documents would then be classified based on the necessary groups. The proposed technique is aimed at improved results of document clustering.\n    ",
        "submission_date": "2016-04-15T00:00:00",
        "last_modified_date": "2016-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04562",
        "title": "A Network-based End-to-End Trainable Task-oriented Dialogue System",
        "authors": [
            "Tsung-Hsien Wen",
            "David Vandyke",
            "Nikola Mrksic",
            "Milica Gasic",
            "Lina M. Rojas-Barahona",
            "Pei-Hao Su",
            "Stefan Ultes",
            "Steve Young"
        ],
        "abstract": "Teaching machines to accomplish tasks by conversing naturally with humans is challenging. Currently, developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting, or acquiring costly labelled datasets to solve a statistical learning problem for each component. In this work we introduce a neural network-based text-in, text-out end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand. The results show that the model can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain.\n    ",
        "submission_date": "2016-04-15T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04725",
        "title": "Unanimously acceptable agreements for negotiation teams in unpredictable domains",
        "authors": [
            "Victor Sanchez-Anguix",
            "Reyhan Aydogan",
            "Vicente Julian",
            "Catholijn Jonker"
        ],
        "abstract": "A negotiation team is a set of agents with common and possibly also conflicting preferences that forms one of the parties of a negotiation. A negotiation team is involved in two decision making processes simultaneously, a negotiation with the opponents, and an intra-team process to decide on the moves to make in the negotiation. This article focuses on negotiation team decision making for circumstances that require unanimity of team decisions. Existing agent-based approaches only guarantee unanimity in teams negotiating in domains exclusively composed of predictable and compatible issues. This article presents a model for negotiation teams that guarantees unanimous team decisions in domains consisting of predictable and compatible, and also unpredictable issues. Moreover, the article explores the influence of using opponent, and team member models in the proposing strategies that team members use. Experimental results show that the team benefits if team members employ Bayesian learning to model their teammates' preferences.\n    ",
        "submission_date": "2016-04-16T00:00:00",
        "last_modified_date": "2016-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04727",
        "title": "Tasks for agent-based negotiation teams: Analysis, review, and challenges",
        "authors": [
            "Victor Sanchez-Anguix",
            "Vicente Julian",
            "Vicente Botti",
            "Ana Garcia-Fornes"
        ],
        "abstract": "An agent-based negotiation team is a group of interdependent agents that join together as a single negotiation party due to their shared interests in the negotiation at hand. The reasons to employ an agent-based negotiation team may vary: (i) more computation and parallelization capabilities, (ii) unite agents with different expertise and skills whose joint work makes it possible to tackle complex negotiation domains, (iii) the necessity to represent different stakeholders or different preferences in the same party (e.g., organizations, countries, and married couple). The topic of agent-based negotiation teams has been recently introduced in multi-agent research. Therefore, it is necessary to identify good practices, challenges, and related research that may help in advancing the state-of-the-art in agent-based negotiation teams. For that reason, in this article we review the tasks to be carried out by agent-based negotiation teams. Each task is analyzed and related with current advances in different research areas. The analysis aims to identify special challenges that may arise due to the particularities of agent-based negotiation teams.\n    ",
        "submission_date": "2016-04-16T00:00:00",
        "last_modified_date": "2016-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04728",
        "title": "Reaching Unanimous Agreements Within Agent-Based Negotiation Teams With Linear and Monotonic Utility Functions",
        "authors": [
            "Victor Sanchez-Anguix",
            "Vicente Julian",
            "Vicente Botti",
            "Ana Garcia-Fornes"
        ],
        "abstract": "In this article, an agent-based negotiation model for negotiation teams that negotiate a deal with an opponent is presented. Agent-based negotiation teams are groups of agents that join together as a single negotiation party because they share an interest that is related to the negotiation process. The model relies on a trusted mediator that coordinates and helps team members in the decisions that they have to take during the negotiation process: which offer is sent to the opponent, and whether the offers received from the opponent are accepted. The main strength of the proposed negotiation model is the fact that it guarantees unanimity within team decisions since decisions report a utility to team members that is greater than or equal to their aspiration levels at each negotiation round. This work analyzes how unanimous decisions are taken within the team and the robustness of the model against different types of manipulations. An empirical evaluation is also performed to study the impact of the different parameters of the model.\n    ",
        "submission_date": "2016-04-16T00:00:00",
        "last_modified_date": "2016-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04730",
        "title": "Evolutionary-aided negotiation model for bilateral bargaining in Ambient Intelligence domains with complex utility functions",
        "authors": [
            "Victor Sanchez-Anguix",
            "Soledad Valero",
            "Vicente Julian",
            "Vicente Botti",
            "Ana Garcia-Fornes"
        ],
        "abstract": "Ambient Intelligence aims to offer personalized services and easier ways of interaction between people and systems. Since several users and systems may coexist in these environments, it is quite possible that entities with opposing preferences need to cooperate to reach their respective goals. Automated negotiation is pointed as one of the mechanisms that may provide a solution to this kind of problems. In this article, a multi-issue bilateral bargaining model for Ambient Intelligence domains is presented where it is assumed that agents have computational bounded resources and do not know their opponents' preferences. The main goal of this work is to provide negotiation models that obtain efficient agreements while maintaining the computational cost low. A niching genetic algorithm is used before the negotiation process to sample one's own utility function (self-sampling). During the negotiation process, genetic operators are applied over the opponent's and one's own offers in order to sample new offers that are interesting for both parties. Results show that the proposed model is capable of outperforming similarity heuristics which only sample before the negotiation process and of obtaining similar results to similarity heuristics which have access to all of the possible offers.\n    ",
        "submission_date": "2016-04-16T00:00:00",
        "last_modified_date": "2016-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04736",
        "title": "Intra-Team Strategies for Teams Negotiating Against Competitor, Matchers, and Conceders",
        "authors": [
            "Victor Sanchez-Anguix",
            "Reyhan Aydogan",
            "Vicente Julian",
            "Catholijn Jonker"
        ],
        "abstract": "Under some circumstances, a group of individuals may need to negotiate together as a negotiation team against another party. Unlike bilateral negotiation between two individuals, this type of negotiations entails to adopt an intra-team strategy for negotiation teams in order to make team decisions and accordingly negotiate with the opponent. It is crucial to be able to negotiate successfully with heterogeneous opponents since opponents' negotiation strategy and behavior may vary in an open environment. While one opponent might collaborate and concede over time, another may not be inclined to concede. This paper analyzes the performance of recently proposed intra-team strategies for negotiation teams against different categories of opponents: competitors, matchers, and conceders. Furthermore, it provides an extension of the negotiation tool Genius for negotiation teams in bilateral settings. Consequently, this work facilitates research in negotiation teams.\n    ",
        "submission_date": "2016-04-16T00:00:00",
        "last_modified_date": "2016-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04737",
        "title": "Studying the impact of negotiation environments on negotiation teams' performance",
        "authors": [
            "Victor Sanchez-Anguix",
            "Vicente Julian",
            "Vicente Botti",
            "Ana Garcia-Fornes"
        ],
        "abstract": "In this article we study the impact of the negotiation environment on the performance of several intra-team strategies (team dynamics) for agent-based negotiation teams that negotiate with an opponent. An agent-based negotiation team is a group of agents that joins together as a party because they share common interests in the negotiation at hand. It is experimentally shown how negotiation environment conditions like the deadline of both parties, the concession speed of the opponent, similarity among team members, and team size affect performance metrics like the minimum utility of team members, the average utility of team members, and the number of negotiation rounds. Our goal is identifying which intra-team strategies work better in different environmental conditions in order to provide useful knowledge for team members to select appropriate intra-team strategies according to environmental conditions.\n    ",
        "submission_date": "2016-04-16T00:00:00",
        "last_modified_date": "2016-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04854",
        "title": "Toward Efficient Task Assignment and Motion Planning for Large Scale Underwater Mission",
        "authors": [
            "Somaiyeh Mahmoud Zadeh",
            "David MW Powers",
            "Karl Sammut",
            "Amirmehdi Yazdani"
        ],
        "abstract": "An Autonomous Underwater Vehicle (AUV) needs to acquire a certain degree of autonomy for any particular underwater mission to fulfill the mission objectives successfully and ensure its safety in all stages of the mission in a large scale operating filed. In this paper, a novel combinatorial conflict-free-task assignment strategy consisting an interactive engagement of a local path planner and an adaptive global route planner, is introduced. The method is established upon the heuristic search potency of the Particle Swarm Optimisation (PSO) algorithm to address the discrete nature of routing-task assignment approach and the complexity of NP-hard path planning problem. The proposed hybrid method is highly efficient for having a reactive guidance framework that guarantees successful completion of missions specifically in cluttered environments. To examine the performance of the method in a context of mission productivity, mission time management and vehicle safety, a series of simulation studies are undertaken. The results of simulations declare that the proposed method is reliable and robust, particularly in dealing with uncertainties, and it can significantly enhance the level of vehicle's autonomy by relying on its reactive nature and capability of providing fast feasible solutions.\n    ",
        "submission_date": "2016-04-17T00:00:00",
        "last_modified_date": "2016-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04928",
        "title": "Learning to Incentivize: Eliciting Effort via Output Agreement",
        "authors": [
            "Yang Liu",
            "Yiling Chen"
        ],
        "abstract": "In crowdsourcing when there is a lack of verification for contributed answers, output agreement mechanisms are often used to incentivize participants to provide truthful answers when the correct answer is hold by the majority. In this paper, we focus on using output agreement mechanisms to elicit effort, in addition to eliciting truthful answers, from a population of workers. We consider a setting where workers have heterogeneous cost of effort exertion and examine the data requester's problem of deciding the reward level in output agreement for optimal elicitation. In particular, when the requester knows the cost distribution, we derive the optimal reward level for output agreement mechanisms. This is achieved by first characterizing Bayesian Nash equilibria of output agreement mechanisms for a given reward level. When the requester does not know the cost distribution, we develop sequential mechanisms that combine learning the cost distribution with incentivizing effort exertion to approximately determine the optimal reward level.\n    ",
        "submission_date": "2016-04-17T00:00:00",
        "last_modified_date": "2016-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05024",
        "title": "Empirical study of PROXTONE and PROXTONE$^+$ for Fast Learning of Large Scale Sparse Models",
        "authors": [
            "Ziqiang Shi",
            "Rujie Liu"
        ],
        "abstract": "PROXTONE is a novel and fast method for optimization of large scale non-smooth convex problem \\cite{shi2015large}. In this work, we try to use PROXTONE method in solving large scale \\emph{non-smooth non-convex} problems, for example training of sparse deep neural network (sparse DNN) or sparse convolutional neural network (sparse CNN) for embedded or mobile device. PROXTONE converges much faster than first order methods, while first order method is easy in deriving and controlling the sparseness of the solutions. Thus in some applications, in order to train sparse models fast, we propose to combine the merits of both methods, that is we use PROXTONE in the first several epochs to reach the neighborhood of an optimal solution, and then use the first order method to explore the possibility of sparsity in the following training. We call such method PROXTONE plus (PROXTONE$^+$). Both PROXTONE and PROXTONE$^+$ are tested in our experiments, and which demonstrate both methods improved convergence speed twice as fast at least on diverse sparse model learning problems, and at the same time reduce the size to 0.5\\% for DNN models. The source of all the algorithms is available upon request.\n    ",
        "submission_date": "2016-04-18T00:00:00",
        "last_modified_date": "2016-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05091",
        "title": "End-to-End Tracking and Semantic Segmentation Using Recurrent Neural Networks",
        "authors": [
            "Peter Ondruska",
            "Julie Dequaire",
            "Dominic Zeng Wang",
            "Ingmar Posner"
        ],
        "abstract": "In this work we present a novel end-to-end framework for tracking and classifying a robot's surroundings in complex, dynamic and only partially observable real-world environments. The approach deploys a recurrent neural network to filter an input stream of raw laser measurements in order to directly infer object locations, along with their identity in both visible and occluded areas. To achieve this we first train the network using unsupervised Deep Tracking, a recently proposed theoretical framework for end-to-end space occupancy prediction. We show that by learning to track on a large amount of unsupervised data, the network creates a rich internal representation of its environment which we in turn exploit through the principle of inductive transfer of knowledge to perform the task of it's semantic classification. As a result, we show that only a small amount of labelled data suffices to steer the network towards mastering this additional task. Furthermore we propose a novel recurrent neural network architecture specifically tailored to tracking and semantic classification in real-world robotics applications. We demonstrate the tracking and classification performance of the method on real-world data collected at a busy road junction. Our evaluation shows that the proposed end-to-end framework compares favourably to a state-of-the-art, model-free tracking solution and that it outperforms a conventional one-shot training scheme for semantic classification.\n    ",
        "submission_date": "2016-04-18T00:00:00",
        "last_modified_date": "2016-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05129",
        "title": "Memory shapes time perception and intertemporal choices",
        "authors": [
            "Pedro A. Ortega",
            "Naftali Tishby"
        ],
        "abstract": "There is a consensus that human and non-human subjects experience temporal distortions in many stages of their perceptual and decision-making systems. Similarly, intertemporal choice research has shown that decision-makers undervalue future outcomes relative to immediate ones. Here we combine techniques from information theory and artificial intelligence to show how both temporal distortions and intertemporal choice preferences can be explained as a consequence of the coding efficiency of sensorimotor representation. In particular, the model implies that interactions that constrain future behavior are perceived as being both longer in duration and more valuable. Furthermore, using simulations of artificial agents, we investigate how memory constraints enforce a renormalization of the perceived timescales. Our results show that qualitatively different discount functions, such as exponential and hyperbolic discounting, arise as a consequence of an agent's probabilistic model of the world.\n    ",
        "submission_date": "2016-04-18T00:00:00",
        "last_modified_date": "2016-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05170",
        "title": "A Repeated Signal Difference for Recognising Patterns",
        "authors": [
            "Kieran Greer"
        ],
        "abstract": "This paper describes a new mechanism that might help with defining pattern sequences, by the fact that it can produce an upper bound on the ensemble value that can persistently oscillate with the actual values produced from each pattern. With every firing event, a node also receives an on/off feedback switch. If the node fires, then it sends a feedback result depending on the input signal strength. If the input signal is positive or larger, it can store an 'on' switch feedback for the next iteration. If the signal is negative or smaller, it can store an 'off' switch feedback for the next iteration. If the node does not fire, then it does not affect the current feedback situation and receives the switch command produced by the last active pattern event for the same neuron. The upper bound therefore also represents the largest or most enclosing pattern set and the lower value is for the actual set of firing patterns. If the pattern sequence repeats, it will oscillate between the two values, allowing them to be recognised and measured more easily, over time. Tests show that changing the sequence ordering produces different value sets, which can also be measured.\n    ",
        "submission_date": "2016-04-18T00:00:00",
        "last_modified_date": "2016-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05194",
        "title": "Preference Elicitation For Single Crossing Domain",
        "authors": [
            "Palash Dey",
            "Neeldhara Misra"
        ],
        "abstract": "Eliciting the preferences of a set of agents over a set of alternatives is a problem of fundamental importance in social choice theory. Prior work on this problem has studied the query complexity of preference elicitation for the unrestricted domain and for the domain of single peaked preferences. In this paper, we consider the domain of single crossing preference profiles and study the query complexity of preference elicitation under various settings. We consider two distinct situations: when an ordering of the voters with respect to which the profile is single crossing is known versus when it is unknown. We also consider different access models: when the votes can be accessed at random, as opposed to when they are coming in a pre-defined sequence. In the sequential access model, we distinguish two cases when the ordering is known: the first is that sequence in which the votes appear is also a single-crossing order, versus when it is not.\n",
        "submission_date": "2016-04-15T00:00:00",
        "last_modified_date": "2016-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05225",
        "title": "Annotation Order Matters: Recurrent Image Annotator for Arbitrary Length Image Tagging",
        "authors": [
            "Jiren Jin",
            "Hideki Nakayama"
        ],
        "abstract": "Automatic image annotation has been an important research topic in facilitating large scale image management and retrieval. Existing methods focus on learning image-tag correlation or correlation between tags to improve annotation accuracy. However, most of these methods evaluate their performance using top-k retrieval performance, where k is fixed. Although such setting gives convenience for comparing different methods, it is not the natural way that humans annotate images. The number of annotated tags should depend on image contents. Inspired by the recent progress in machine translation and image captioning, we propose a novel Recurrent Image Annotator (RIA) model that forms image annotation task as a sequence generation problem so that RIA can natively predict the proper length of tags according to image contents. We evaluate the proposed model on various image annotation datasets. In addition to comparing our model with existing methods using the conventional top-k evaluation measures, we also provide our model as a high quality baseline for the arbitrary length image tagging task. Moreover, the results of our experiments show that the order of tags in training phase has a great impact on the final annotation performance.\n    ",
        "submission_date": "2016-04-18T00:00:00",
        "last_modified_date": "2016-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05280",
        "title": "Asymptotic Convergence in Online Learning with Unbounded Delays",
        "authors": [
            "Scott Garrabrant",
            "Nate Soares",
            "Jessica Taylor"
        ],
        "abstract": "We study the problem of predicting the results of computations that are too expensive to run, via the observation of the results of smaller computations. We model this as an online learning problem with delayed feedback, where the length of the delay is unbounded, which we study mainly in a stochastic setting. We show that in this setting, consistency is not possible in general, and that optimal forecasters might not have average regret going to zero. However, it is still possible to give algorithms that converge asymptotically to Bayes-optimal predictions, by evaluating forecasters on specific sparse independent subsequences of their predictions. We give an algorithm that does this, which converges asymptotically on good behavior, and give very weak bounds on how long it takes to converge. We then relate our results back to the problem of predicting large computations in a deterministic setting.\n    ",
        "submission_date": "2016-04-18T00:00:00",
        "last_modified_date": "2016-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05468",
        "title": "Understanding Rating Behaviour and Predicting Ratings by Identifying Representative Users",
        "authors": [
            "Rahul Kamath",
            "Masanao Ochi",
            "Yutaka Matsuo"
        ],
        "abstract": "Online user reviews describing various products and services are now abundant on the web. While the information conveyed through review texts and ratings is easily comprehensible, there is a wealth of hidden information in them that is not immediately obvious. In this study, we unlock this hidden value behind user reviews to understand the various dimensions along which users rate products. We learn a set of users that represent each of these dimensions and use their ratings to predict product ratings. Specifically, we work with restaurant reviews to identify users whose ratings are influenced by dimensions like 'Service', 'Atmosphere' etc. in order to predict restaurant ratings and understand the variation in rating behaviour across different cuisines. While previous approaches to obtaining product ratings require either a large number of user ratings or a few review texts, we show that it is possible to predict ratings with few user ratings and no review text. Our experiments show that our approach outperforms other conventional methods by 16-27% in terms of RMSE.\n    ",
        "submission_date": "2016-04-19T00:00:00",
        "last_modified_date": "2016-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05577",
        "title": "Contribution to the Formal Specification and Verification of a Multi-Agent Robotic System",
        "authors": [
            "Nadeem Akhtar",
            "Malik M. Saad Missen"
        ],
        "abstract": "It is important to have multi-agent robotic system specifications that ensure correctness properties of safety and liveness. As these systems have concurrency, and often have dynamic environment, the formal specification and verification of these systems along with step-wise refinement from abstract to concrete concepts play a major role in system correctness. Formal verification is used for exhaustive investigation of the system space thus ensuring that undetected failures in the behavior are excluded. We construct the system incrementally from subcomponents, based on software architecture. The challenge is to develop a safe multi-agent robotic system, more specifically to ensure the correctness properties of safety and liveness. Formal specifications based on model-checking are flexible, have a concrete syntax, and play vital role in correctness of a multi-agent robotic system. To formally verify safety and liveness of such systems is important because they have high concurrency and in most of the cases have dynamic environment. We have considered a case-study of a multi-agent robotic system for the transport of stock between storehouses to exemplify our formal approach. Our proposed development approach allows for formal verification during specification definition. The development process has been classified in to four major phases of requirement specifications, verification specifications, architecture specifications and implementation.\n    ",
        "submission_date": "2015-10-02T00:00:00",
        "last_modified_date": "2015-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05692",
        "title": "Proving the Incompatibility of Efficiency and Strategyproofness via SMT Solving",
        "authors": [
            "Florian Brandl",
            "Felix Brandt",
            "Manuel Eberl",
            "Christian Geist"
        ],
        "abstract": "Two important requirements when aggregating the preferences of multiple agents are that the outcome should be economically efficient and the aggregation mechanism should not be manipulable. In this paper, we provide a computer-aided proof of a sweeping impossibility using these two conditions for randomized aggregation mechanisms. More precisely, we show that every efficient aggregation mechanism can be manipulated for all expected utility representations of the agents' preferences. This settles an open problem and strengthens a number of existing theorems, including statements that were shown within the special domain of assignment. Our proof is obtained by formulating the claim as a satisfiability problem over predicates from real-valued arithmetic, which is then checked using an SMT (satisfiability modulo theories) solver. In order to verify the correctness of the result, a minimal unsatisfiable set of constraints returned by the SMT solver was translated back into a proof in higher-order logic, which was automatically verified by an interactive theorem prover. To the best of our knowledge, this is the first application of SMT solvers in computational social choice.\n    ",
        "submission_date": "2016-04-19T00:00:00",
        "last_modified_date": "2017-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05753",
        "title": "Sketching and Neural Networks",
        "authors": [
            "Amit Daniely",
            "Nevena Lazic",
            "Yoram Singer",
            "Kunal Talwar"
        ],
        "abstract": "High-dimensional sparse data present computational and statistical challenges for supervised learning. We propose compact linear sketches for reducing the dimensionality of the input, followed by a single layer neural network. We show that any sparse polynomial function can be computed, on nearly all sparse binary vectors, by a single layer neural network that takes a compact sketch of the vector as input. Consequently, when a set of sparse binary vectors is approximately separable using a sparse polynomial, there exists a single-layer neural network that takes a short sketch as input and correctly classifies nearly all the points. Previous work has proposed using sketches to reduce dimensionality while preserving the hypothesis class. However, the sketch size has an exponential dependence on the degree in the case of polynomial classifiers. In stark contrast, our approach of using improper learning, using a larger hypothesis class allows the sketch size to have a logarithmic dependence on the degree. Even in the linear case, our approach allows us to improve on the pesky $O({1}/{{\\gamma}^2})$ dependence of random projections, on the margin $\\gamma$. We empirically show that our approach leads to more compact neural networks than related methods such as feature hashing at equal or better performance.\n    ",
        "submission_date": "2016-04-19T00:00:00",
        "last_modified_date": "2016-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05865",
        "title": "Estimating 3D Trajectories from 2D Projections via Disjunctive Factored Four-Way Conditional Restricted Boltzmann Machines",
        "authors": [
            "Decebal Constantin Mocanu",
            "Haitham Bou Ammar",
            "Luis Puig",
            "Eric Eaton",
            "Antonio Liotta"
        ],
        "abstract": "Estimation, recognition, and near-future prediction of 3D trajectories based on their two dimensional projections available from one camera source is an exceptionally difficult problem due to uncertainty in the trajectories and environment, high dimensionality of the specific trajectory states, lack of enough labeled data and so on. In this article, we propose a solution to solve this problem based on a novel deep learning model dubbed Disjunctive Factored Four-Way Conditional Restricted Boltzmann Machine (DFFW-CRBM). Our method improves state-of-the-art deep learning techniques for high dimensional time-series modeling by introducing a novel tensor factorization capable of driving forth order Boltzmann machines to considerably lower energy levels, at no computational costs. DFFW-CRBMs are capable of accurately estimating, recognizing, and performing near-future prediction of three-dimensional trajectories from their 2D projections while requiring limited amount of labeled data. We evaluate our method on both simulated and real-world data, showing its effectiveness in predicting and classifying complex ball trajectories and human activities.\n    ",
        "submission_date": "2016-04-20T00:00:00",
        "last_modified_date": "2017-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05878",
        "title": "A Factorization Machine Framework for Testing Bigram Embeddings in Knowledgebase Completion",
        "authors": [
            "Johannes Welbl",
            "Guillaume Bouchard",
            "Sebastian Riedel"
        ],
        "abstract": "Embedding-based Knowledge Base Completion models have so far mostly combined distributed representations of individual entities or relations to compute truth scores of missing links. Facts can however also be represented using pairwise embeddings, i.e. embeddings for pairs of entities and relations. In this paper we explore such bigram embeddings with a flexible Factorization Machine model and several ablations from it. We investigate the relevance of various bigram types on the fb15k237 dataset and find relative improvements compared to a compositional model.\n    ",
        "submission_date": "2016-04-20T00:00:00",
        "last_modified_date": "2016-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05942",
        "title": "Multiplayer Games for Learning Multirobot Coordination Algorithms",
        "authors": [
            "Arash Tavakoli",
            "Haig Nalbandian",
            "Nora Ayanian"
        ],
        "abstract": "Humans have an impressive ability to solve complex coordination problems in a fully distributed manner. This ability, if learned as a set of distributed multirobot coordination strategies, can enable programming large groups of robots to collaborate towards complex coordination objectives in a way similar to humans. Such strategies would offer robustness, adaptability, fault-tolerance, and, importantly, distributed decision-making. To that end, we have designed a networked gaming platform to investigate human group behavior, specifically in solving complex collaborative coordinated tasks. Through this platform, we are able to limit the communication, sensing, and actuation capabilities provided to the players. With the aim of learning coordination algorithms for robots in mind, we define these capabilities to mimic those of a simple ground robot.\n    ",
        "submission_date": "2016-04-20T00:00:00",
        "last_modified_date": "2016-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05978",
        "title": "A topological insight into restricted Boltzmann machines",
        "authors": [
            "Decebal Constantin Mocanu",
            "Elena Mocanu",
            "Phuong H. Nguyen",
            "Madeleine Gibescu",
            "Antonio Liotta"
        ],
        "abstract": "Restricted Boltzmann Machines (RBMs) and models derived from them have been successfully used as basic building blocks in deep artificial neural networks for automatic features extraction, unsupervised weights initialization, but also as density estimators. Thus, their generative and discriminative capabilities, but also their computational time are instrumental to a wide range of applications. Our main contribution is to look at RBMs from a topological perspective, bringing insights from network science. Firstly, here we show that RBMs and Gaussian RBMs (GRBMs) are bipartite graphs which naturally have a small-world topology. Secondly, we demonstrate both on synthetic and real-world datasets that by constraining RBMs and GRBMs to a scale-free topology (while still considering local neighborhoods and data distribution), we reduce the number of weights that need to be computed by a few orders of magnitude, at virtually no loss in generative performance. Thirdly, we show that, for a fixed number of weights, our proposed sparse models (which by design have a higher number of hidden neurons) achieve better generative capabilities than standard fully connected RBMs and GRBMs (which by design have a smaller number of hidden neurons), at no additional computational costs.\n    ",
        "submission_date": "2016-04-20T00:00:00",
        "last_modified_date": "2016-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06020",
        "title": "Constructive Preference Elicitation by Setwise Max-margin Learning",
        "authors": [
            "Stefano Teso",
            "Andrea Passerini",
            "Paolo Viappiani"
        ],
        "abstract": "In this paper we propose an approach to preference elicitation that is suitable to large configuration spaces beyond the reach of existing state-of-the-art approaches. Our setwise max-margin method can be viewed as a generalization of max-margin learning to sets, and can produce a set of \"diverse\" items that can be used to ask informative queries to the user. Moreover, the approach can encourage sparsity in the parameter space, in order to favor the assessment of utility towards combinations of weights that concentrate on just few features. We present a mixed integer linear programming formulation and show how our approach compares favourably with Bayesian preference elicitation alternatives and easily scales to realistic datasets.\n    ",
        "submission_date": "2016-04-20T00:00:00",
        "last_modified_date": "2016-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06057",
        "title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation",
        "authors": [
            "Tejas D. Kulkarni",
            "Karthik R. Narasimhan",
            "Ardavan Saeedi",
            "Joshua B. Tenenbaum"
        ],
        "abstract": "Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'.\n    ",
        "submission_date": "2016-04-20T00:00:00",
        "last_modified_date": "2016-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06635",
        "title": "Bridging LSTM Architecture and the Neural Dynamics during Reading",
        "authors": [
            "Peng Qian",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "abstract": "Recently, the long short-term memory neural network (LSTM) has attracted wide interest due to its success in many tasks. LSTM architecture consists of a memory cell and three gates, which looks similar to the neuronal networks in the brain. However, there still lacks the evidence of the cognitive plausibility of LSTM architecture as well as its working mechanism. In this paper, we study the cognitive plausibility of LSTM by aligning its internal architecture with the brain activity observed via fMRI when the subjects read a story. Experiment results show that the artificial memory vector in LSTM can accurately predict the observed sequential brain activities, indicating the correlation between LSTM architecture and the cognitive process of story reading.\n    ",
        "submission_date": "2016-04-22T00:00:00",
        "last_modified_date": "2016-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06743",
        "title": "Latent Contextual Bandits and their Application to Personalized Recommendations for New Users",
        "authors": [
            "Li Zhou",
            "Emma Brunskill"
        ],
        "abstract": "Personalized recommendations for new users, also known as the cold-start problem, can be formulated as a contextual bandit problem. Existing contextual bandit algorithms generally rely on features alone to capture user variability. Such methods are inefficient in learning new users' interests. In this paper we propose Latent Contextual Bandits. We consider both the benefit of leveraging a set of learned latent user classes for new users, and how we can learn such latent classes from prior users. We show that our approach achieves a better regret bound than existing algorithms. We also demonstrate the benefit of our approach using a large real world dataset and a preliminary user study.\n    ",
        "submission_date": "2016-04-22T00:00:00",
        "last_modified_date": "2016-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06770",
        "title": "A Hybrid Approach to Query Answering under Expressive Datalog+/-",
        "authors": [
            "Mostafa Milani",
            "Andrea Cali",
            "Leopoldo Bertossi"
        ],
        "abstract": "Datalog+/- is a family of ontology languages that combine good computational properties with high expressive power. Datalog+/- languages are provably able to capture the most relevant Semantic Web languages. In this paper we consider the class of weakly-sticky (WS) Datalog+/- programs, which allow for certain useful forms of joins in rule bodies as well as extending the well-known class of weakly-acyclic TGDs. So far, only non-deterministic algorithms were known for answering queries on WS Datalog+/- programs. We present novel deterministic query answering algorithms under WS Datalog+/-. In particular, we propose: (1) a bottom-up grounding algorithm based on a query-driven chase, and (2) a hybrid approach based on transforming a WS program into a so-called sticky one, for which query rewriting techniques are known. We discuss how our algorithms can be optimized and effectively applied for query answering in real-world scenarios.\n    ",
        "submission_date": "2016-04-22T00:00:00",
        "last_modified_date": "2016-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06778",
        "title": "Benchmarking Deep Reinforcement Learning for Continuous Control",
        "authors": [
            "Yan Duan",
            "Xi Chen",
            "Rein Houthooft",
            "John Schulman",
            "Pieter Abbeel"
        ],
        "abstract": "Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at ",
        "submission_date": "2016-04-22T00:00:00",
        "last_modified_date": "2016-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06976",
        "title": "Extracted Social Network Mining",
        "authors": [
            "Mahyuddin K. M. Nasution"
        ],
        "abstract": "In this paper we study the relationship between the resources of social networks by exploring the Web as big data based on a simple search engine. We have used set theory by utilizing the occurrence and co-occurrence for defining the singleton or doubleton spaces of event in a search engine model, and then provided them as representation of social actors and their relationship in clusters. Thus, there are behaviors of social actors and their relation based on Web.\n    ",
        "submission_date": "2016-04-24T00:00:00",
        "last_modified_date": "2016-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07063",
        "title": "The Dichotomy for Conservative Constraint Satisfaction is Polynomially Decidable",
        "authors": [
            "Cl\u00e9ment Carbonnel"
        ],
        "abstract": "Given a fixed constraint language $\\Gamma$, the conservative CSP over $\\Gamma$ (denoted by c-CSP($\\Gamma$)) is a variant of CSP($\\Gamma$) where the domain of each variable can be restricted arbitrarily. A dichotomy is known for conservative CSP: for every fixed language $\\Gamma$, c-CSP($\\Gamma$) is either in P or NP-complete. However, the characterization of conservatively tractable languages is of algebraic nature and the naive recognition algorithm is super-exponential in the domain size. The main contribution of this paper is a polynomial-time algorithm that, given a constraint language $\\Gamma$ as input, decides if c-CSP($\\Gamma$) is tractable. In addition, if $\\Gamma$ is proven tractable the algorithm also outputs its coloured graph, which contains valuable information on the structure of $\\Gamma$.\n    ",
        "submission_date": "2016-04-24T00:00:00",
        "last_modified_date": "2016-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07093",
        "title": "Semi-supervised Vocabulary-informed Learning",
        "authors": [
            "Yanwei Fu",
            "Leonid Sigal"
        ],
        "abstract": "Despite significant progress in object categorization, in recent years, a number of important challenges remain, mainly, ability to learn from limited labeled data and ability to recognize object classes within large, potentially open, set of labels. Zero-shot learning is one way of addressing these challenges, but it has only been shown to work with limited sized class vocabularies and typically requires separation between supervised and unsupervised classes, allowing former to inform the latter but not vice versa. We propose the notion of semi-supervised vocabulary-informed learning to alleviate the above mentioned challenges and address problems of supervised, zero-shot and open set recognition using a unified framework. Specifically, we propose a maximum margin framework for semantic manifold-based recognition that incorporates distance constraints from (both supervised and unsupervised) vocabulary atoms, ensuring that labeled samples are projected closest to their correct prototypes, in the embedding space, than to others. We show that resulting model shows improvements in supervised, zero-shot, and large open set recognition, with up to 310K class vocabulary on AwA and ImageNet datasets.\n    ",
        "submission_date": "2016-04-24T00:00:00",
        "last_modified_date": "2016-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07102",
        "title": "Makeup like a superstar: Deep Localized Makeup Transfer Network",
        "authors": [
            "Si Liu",
            "Xinyu Ou",
            "Ruihe Qian",
            "Wei Wang",
            "Xiaochun Cao"
        ],
        "abstract": "In this paper, we propose a novel Deep Localized Makeup Transfer Network to automatically recommend the most suitable makeup for a female and synthesis the makeup on her face. Given a before-makeup face, her most suitable makeup is determined automatically. Then, both the beforemakeup and the reference faces are fed into the proposed Deep Transfer Network to generate the after-makeup face. Our end-to-end makeup transfer network have several nice properties including: (1) with complete functions: including foundation, lip gloss, and eye shadow transfer; (2) cosmetic specific: different cosmetics are transferred in different manners; (3) localized: different cosmetics are applied on different facial regions; (4) producing naturally looking results without obvious artifacts; (5) controllable makeup lightness: various results from light makeup to heavy makeup can be generated. Qualitative and quantitative experiments show that our network performs much better than the methods of [Guo and Sim, 2009] and two variants of NerualStyle [Gatys et al., 2015a].\n    ",
        "submission_date": "2016-04-25T00:00:00",
        "last_modified_date": "2016-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07176",
        "title": "Protein Secondary Structure Prediction Using Cascaded Convolutional and Recurrent Neural Networks",
        "authors": [
            "Zhen Li",
            "Yizhou Yu"
        ],
        "abstract": "Protein secondary structure prediction is an important problem in bioinformatics. Inspired by the recent successes of deep neural networks, in this paper, we propose an end-to-end deep network that predicts protein secondary structures from integrated local and global contextual features. Our deep architecture leverages convolutional neural networks with different kernel sizes to extract multiscale local contextual features. In addition, considering long-range dependencies existing in amino acid sequences, we set up a bidirectional neural network consisting of gated recurrent unit to capture global contextual features. Furthermore, multi-task learning is utilized to predict secondary structure labels and amino-acid solvent accessibility simultaneously. Our proposed deep network demonstrates its effectiveness by achieving state-of-the-art performance, i.e., 69.7% Q8 accuracy on the public benchmark CB513, 76.9% Q8 accuracy on CASP10 and 73.1% Q8 accuracy on CASP11. Our model and results are publicly available.\n    ",
        "submission_date": "2016-04-25T00:00:00",
        "last_modified_date": "2016-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07178",
        "title": "Weighted Spectral Cluster Ensemble",
        "authors": [
            "Muhammad Yousefnezhad",
            "Daoqiang Zhang"
        ],
        "abstract": "Clustering explores meaningful patterns in the non-labeled data sets. Cluster Ensemble Selection (CES) is a new approach, which can combine individual clustering results for increasing the performance of the final results. Although CES can achieve better final results in comparison with individual clustering algorithms and cluster ensemble methods, its performance can be dramatically affected by its consensus diversity metric and thresholding procedure. There are two problems in CES: 1) most of the diversity metrics is based on heuristic Shannon's entropy and 2) estimating threshold values are really hard in practice. The main goal of this paper is proposing a robust approach for solving the above mentioned problems. Accordingly, this paper develops a novel framework for clustering problems, which is called Weighted Spectral Cluster Ensemble (WSCE), by exploiting some concepts from community detection arena and graph based clustering. Under this framework, a new version of spectral clustering, which is called Two Kernels Spectral Clustering, is used for generating graphs based individual clustering results. Further, by using modularity, which is a famous metric in the community detection, on the transformed graph representation of individual clustering results, our approach provides an effective diversity estimation for individual clustering results. Moreover, this paper introduces a new approach for combining the evaluated individual clustering results without the procedure of thresholding. Experimental study on varied data sets demonstrates that the prosed approach achieves superior performance to state-of-the-art methods.\n    ",
        "submission_date": "2016-04-25T00:00:00",
        "last_modified_date": "2016-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07322",
        "title": "Predictive No-Reference Assessment of Video Quality",
        "authors": [
            "Maria Torres Vega",
            "Decebal Constantin Mocanu",
            "Antonio Liotta"
        ],
        "abstract": "Among the various means to evaluate the quality of video streams, No-Reference (NR) methods have low computation and may be executed on thin clients. Thus, NR algorithms would be perfect candidates in cases of real-time quality assessment, automated quality control and, particularly, in adaptive mobile streaming. Yet, existing NR approaches are often inaccurate, in comparison to Full-Reference (FR) algorithms, especially under lossy network conditions. In this work, we present an NR method that combines machine learning with simple NR metrics to achieve a quality index comparably as accurate as the Video Quality Metric (VQM) Full-Reference algorithm. Our method is tested in an extensive dataset (960 videos), under lossy network conditions and considering nine different machine learning algorithms. Overall, we achieve an over 97% correlation with VQM, while allowing real-time assessment of video quality of experience in realistic streaming scenarios.\n    ",
        "submission_date": "2016-04-25T00:00:00",
        "last_modified_date": "2016-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07379",
        "title": "Context Encoders: Feature Learning by Inpainting",
        "authors": [
            "Deepak Pathak",
            "Philipp Krahenbuhl",
            "Jeff Donahue",
            "Trevor Darrell",
            "Alexei A. Efros"
        ],
        "abstract": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.\n    ",
        "submission_date": "2016-04-25T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07407",
        "title": "Conversational Markers of Constructive Discussions",
        "authors": [
            "Vlad Niculae",
            "Cristian Danescu-Niculescu-Mizil"
        ],
        "abstract": "Group discussions are essential for organizing every aspect of modern life, from faculty meetings to senate debates, from grant review panels to papal conclaves. While costly in terms of time and organization effort, group discussions are commonly seen as a way of reaching better decisions compared to solutions that do not require coordination between the individuals (e.g. voting)---through discussion, the sum becomes greater than the parts. However, this assumption is not irrefutable: anecdotal evidence of wasteful discussions abounds, and in our own experiments we find that over 30% of discussions are unproductive.\n",
        "submission_date": "2016-04-25T00:00:00",
        "last_modified_date": "2016-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07513",
        "title": "Semantic Change Detection with Hypermaps",
        "authors": [
            "Teppei Suzuki",
            "Soma Shirakabe",
            "Yudai Miyashita",
            "Akio Nakamura",
            "Yutaka Satoh",
            "Hirokatsu Kataoka"
        ],
        "abstract": "Change detection is the study of detecting changes between two different images of a scene taken at different times. By the detected change areas, however, a human cannot understand how different the two images. Therefore, a semantic understanding is required in the change detection research such as disaster investigation. The paper proposes the concept of semantic change detection, which involves intuitively inserting semantic meaning into detected change areas. We mainly focus on the novel semantic segmentation in addition to a conventional change detection approach. In order to solve this problem and obtain a high-level of performance, we propose an improvement to the hypercolumns representation, hereafter known as hypermaps, which effectively uses convolutional maps obtained from convolutional neural networks (CNNs). We also employ multi-scale feature representation captured by different image patches. We applied our method to the TSUNAMI Panoramic Change Detection dataset, and re-annotated the changed areas of the dataset via semantic classes. The results show that our multi-scale hypermaps provided outstanding performance on the re-annotated TSUNAMI dataset.\n    ",
        "submission_date": "2016-04-26T00:00:00",
        "last_modified_date": "2017-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07706",
        "title": "Distributed Clustering of Linear Bandits in Peer to Peer Networks",
        "authors": [
            "Nathan Korda",
            "Balazs Szorenyi",
            "Shuai Li"
        ],
        "abstract": "We provide two distributed confidence ball algorithms for solving linear bandit problems in peer to peer networks with limited communication capabilities. For the first, we assume that all the peers are solving the same linear bandit problem, and prove that our algorithm achieves the optimal asymptotic regret rate of any centralised algorithm that can instantly communicate information between the peers. For the second, we assume that there are clusters of peers solving the same bandit problem within each cluster, and we prove that our algorithm discovers these clusters, while achieving the optimal asymptotic regret rate within each one. Through experiments on several real-world datasets, we demonstrate the performance of proposed algorithms compared to the state-of-the-art.\n    ",
        "submission_date": "2016-04-26T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07928",
        "title": "Distributed Flexible Nonlinear Tensor Factorization",
        "authors": [
            "Shandian Zhe",
            "Kai Zhang",
            "Pengyuan Wang",
            "Kuang-chih Lee",
            "Zenglin Xu",
            "Yuan Qi",
            "Zoubin Ghahramani"
        ],
        "abstract": "Tensor factorization is a powerful tool to analyse multi-way data. Compared with traditional multi-linear methods, nonlinear tensor factorization models are capable of capturing more complex relationships in the data. However, they are computationally expensive and may suffer severe learning bias in case of extreme data sparsity. To overcome these limitations, in this paper we propose a distributed, flexible nonlinear tensor factorization model. Our model can effectively avoid the expensive computations and structural restrictions of the Kronecker-product in existing TGP formulations, allowing an arbitrary subset of tensorial entries to be selected to contribute to the training. At the same time, we derive a tractable and tight variational evidence lower bound (ELBO) that enables highly decoupled, parallel computations and high-quality inference. Based on the new bound, we develop a distributed inference algorithm in the MapReduce framework, which is key-value-free and can fully exploit the memory cache mechanism in fast MapReduce systems such as SPARK. Experimental results fully demonstrate the advantages of our method over several state-of-the-art approaches, in terms of both predictive performance and computational efficiency. Moreover, our approach shows a promising potential in the application of Click-Through-Rate (CTR) prediction for online advertising.\n    ",
        "submission_date": "2016-04-27T00:00:00",
        "last_modified_date": "2016-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07981",
        "title": "The Power of Arc Consistency for CSPs Defined by Partially-Ordered Forbidden Patterns",
        "authors": [
            "Martin C. Cooper",
            "Stanislav \u017divn\u00fd"
        ],
        "abstract": "Characterising tractable fragments of the constraint satisfaction problem (CSP) is an important challenge in theoretical computer science and artificial intelligence. Forbidding patterns (generic sub-instances) provides a means of defining CSP fragments which are neither exclusively language-based nor exclusively structure-based. It is known that the class of binary CSP instances in which the broken-triangle pattern (BTP) does not occur, a class which includes all tree-structured instances, are decided by arc consistency (AC), a ubiquitous reduction operation in constraint solvers. We provide a characterisation of simple partially-ordered forbidden patterns which have this AC-solvability property. It turns out that BTP is just one of five such AC-solvable patterns. The four other patterns allow us to exhibit new tractable classes.\n    ",
        "submission_date": "2016-04-27T00:00:00",
        "last_modified_date": "2017-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08153",
        "title": "Classifying Options for Deep Reinforcement Learning",
        "authors": [
            "Kai Arulkumaran",
            "Nat Dilokthanakul",
            "Murray Shanahan",
            "Anil Anthony Bharath"
        ],
        "abstract": "In this paper we combine one method for hierarchical reinforcement learning - the options framework - with deep Q-networks (DQNs) through the use of different \"option heads\" on the policy network, and a supervisory network for choosing between the different options. We utilise our setup to investigate the effects of architectural constraints in subtasks with positive and negative transfer, across a range of network capacities. We empirically show that our augmented DQN has lower sample complexity when simultaneously learning subtasks with negative transfer, without degrading performance when learning subtasks with positive transfer.\n    ",
        "submission_date": "2016-04-27T00:00:00",
        "last_modified_date": "2017-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08448",
        "title": "Exploiting variable associations to configure efficient local search algorithms in large-scale binary integer programs",
        "authors": [
            "Shunji Umetani"
        ],
        "abstract": "We present a data mining approach for reducing the search space of local search algorithms in a class of binary integer programs including the set covering and partitioning problems. The quality of locally optimal solutions typically improves if a larger neighborhood is used, while the computation time of searching the neighborhood increases exponentially. To overcome this, we extract variable associations from the instance to be solved in order to identify promising pairs of flipping variables in the neighborhood search. Based on this, we develop a 4-flip neighborhood local search algorithm that incorporates an efficient incremental evaluation of solutions and an adaptive control of penalty weights. Computational results show that the proposed method improves the performance of the local search algorithm for large-scale set covering and partitioning problems.\n    ",
        "submission_date": "2016-04-28T00:00:00",
        "last_modified_date": "2017-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08612",
        "title": "Mysteries of Visual Experience",
        "authors": [
            "Jerome Feldman"
        ],
        "abstract": "Science is a crowning glory of the human spirit and its applications remain our best hope for social progress. But there are limitations to current science and perhaps to any science. The general mind-body problem is known to be intractable and currently mysterious. This is one of many deep problems that are universally agreed to be beyond the current purview of Science, including quantum phenomena, etc. But all of these famous unsolved problems are either remote from everyday experience (entanglement, dark matter) or are hard to even define sharply (phenomenology, consciousness, etc.).\n",
        "submission_date": "2016-04-28T00:00:00",
        "last_modified_date": "2022-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08642",
        "title": "On the representation and embedding of knowledge bases beyond binary relations",
        "authors": [
            "Jianfeng Wen",
            "Jianxin Li",
            "Yongyi Mao",
            "Shini Chen",
            "Richong Zhang"
        ],
        "abstract": "The models developed to date for knowledge base embedding are all based on the assumption that the relations contained in knowledge bases are binary. For the training and testing of these embedding models, multi-fold (or n-ary) relational data are converted to triples (e.g., in FB15K dataset) and interpreted as instances of binary relations. This paper presents a canonical representation of knowledge bases containing multi-fold relations. We show that the existing embedding models on the popular FB15K datasets correspond to a sub-optimal modelling framework, resulting in a loss of structural information. We advocate a novel modelling framework, which models multi-fold relations directly using this canonical representation. Using this framework, the existing TransH model is generalized to a new model, m-TransH. We demonstrate experimentally that m-TransH outperforms TransH by a large margin, thereby establishing a new state of the art.\n    ",
        "submission_date": "2016-04-28T00:00:00",
        "last_modified_date": "2016-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08781",
        "title": "Teaching natural language to computers",
        "authors": [
            "Joseph Corneli",
            "Miriam Corneli"
        ],
        "abstract": "\"Natural Language,\" whether spoken and attended to by humans, or processed and generated by computers, requires networked structures that reflect creative processes in semantic, syntactic, phonetic, linguistic, social, emotional, and cultural modules. Being able to produce novel and useful behavior following repeated practice gets to the root of both artificial intelligence and human language. This paper investigates the modalities involved in language-like applications that computers -- and programmers -- engage with, and aims to fine tune the questions we ask to better account for context, self-awareness, and embodiment.\n    ",
        "submission_date": "2016-04-29T00:00:00",
        "last_modified_date": "2016-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08859",
        "title": "The Z-loss: a shift and scale invariant classification loss belonging to the Spherical Family",
        "authors": [
            "Alexandre de Br\u00e9bisson",
            "Pascal Vincent"
        ],
        "abstract": "Despite being the standard loss function to train multi-class neural networks, the log-softmax has two potential limitations. First, it involves computations that scale linearly with the number of output classes, which can restrict the size of problems we are able to tackle with current hardware. Second, it remains unclear how close it matches the task loss such as the top-k error rate or other non-differentiable evaluation metrics which we aim to optimize ultimately. In this paper, we introduce an alternative classification loss function, the Z-loss, which is designed to address these two issues. Unlike the log-softmax, it has the desirable property of belonging to the spherical loss family (Vincent et al., 2015), a class of loss functions for which training can be performed very efficiently with a complexity independent of the number of output classes. We show experimentally that it significantly outperforms the other spherical loss functions previously investigated. Furthermore, we show on a word language modeling task that it also outperforms the log-softmax with respect to certain ranking scores, such as top-k scores, suggesting that the Z-loss has the flexibility to better match the task loss. These qualities thus makes the Z-loss an appealing candidate to train very efficiently large output networks such as word-language models or other extreme classification problems. On the One Billion Word (Chelba et al., 2014) dataset, we are able to train a model with the Z-loss 40 times faster than the log-softmax and more than 4 times faster than the hierarchical softmax.\n    ",
        "submission_date": "2016-04-29T00:00:00",
        "last_modified_date": "2016-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08880",
        "title": "Deep, Convolutional, and Recurrent Models for Human Activity Recognition using Wearables",
        "authors": [
            "Nils Y. Hammerla",
            "Shane Halloran",
            "Thomas Ploetz"
        ],
        "abstract": "Human activity recognition (HAR) in ubiquitous computing is beginning to adopt deep learning to substitute for well-established analysis techniques that rely on hand-crafted feature extraction and classification techniques. From these isolated applications of custom deep architectures it is, however, difficult to gain an overview of their suitability for problems ranging from the recognition of manipulative gestures to the segmentation and identification of physical activities like running or ascending stairs. In this paper we rigorously explore deep, convolutional, and recurrent approaches across three representative datasets that contain movement data captured with wearable sensors. We describe how to train recurrent approaches in this setting, introduce a novel regularisation approach, and illustrate how they outperform the state-of-the-art on a large benchmark dataset. Across thousands of recognition experiments with randomly sampled model configurations we investigate the suitability of each model for different tasks in HAR, explore the impact of hyperparameters using the fANOVA framework, and provide guidelines for the practitioner who wants to apply deep learning in their problem setting.\n    ",
        "submission_date": "2016-04-29T00:00:00",
        "last_modified_date": "2016-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08934",
        "title": "An expressive dissimilarity measure for relational clustering using neighbourhood trees",
        "authors": [
            "Sebastijan Dumancic",
            "Hendrik Blockeel"
        ],
        "abstract": "Clustering is an underspecified task: there are no universal criteria for what makes a good clustering. This is especially true for relational data, where similarity can be based on the features of individuals, the relationships between them, or a mix of both. Existing methods for relational clustering have strong and often implicit biases in this respect. In this paper, we introduce a novel similarity measure for relational data. It is the first measure to incorporate a wide variety of types of similarity, including similarity of attributes, similarity of relational context, and proximity in a hypergraph. We experimentally evaluate how using this similarity affects the quality of clustering on very different types of datasets. The experiments demonstrate that (a) using this similarity in standard clustering methods consistently gives good results, whereas other measures work well only on datasets that match their bias; and (b) on most datasets, the novel similarity outperforms even the best among the existing ones.\n    ",
        "submission_date": "2016-04-29T00:00:00",
        "last_modified_date": "2017-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.00064",
        "title": "Higher Order Recurrent Neural Networks",
        "authors": [
            "Rohollah Soltani",
            "Hui Jiang"
        ],
        "abstract": "In this paper, we study novel neural network structures to better model long term dependency in sequential data. We propose to use more memory units to keep track of more preceding states in recurrent neural networks (RNNs), which are all recurrently fed to the hidden layers as feedback through different weighted paths. By extending the popular recurrent structure in RNNs, we provide the models with better short-term memory mechanism to learn long term dependency in sequences. Analogous to digital filters in signal processing, we call these structures as higher order RNNs (HORNNs). Similar to RNNs, HORNNs can also be learned using the back-propagation through time method. HORNNs are generally applicable to a variety of sequence modelling tasks. In this work, we have examined HORNNs for the language modeling task using two popular data sets, namely the Penn Treebank (PTB) and English text8 data sets. Experimental results have shown that the proposed HORNNs yield the state-of-the-art performance on both data sets, significantly outperforming the regular RNNs as well as the popular LSTMs.\n    ",
        "submission_date": "2016-04-30T00:00:00",
        "last_modified_date": "2016-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.00122",
        "title": "An Improved System for Sentence-level Novelty Detection in Textual Streams",
        "authors": [
            "Xinyu Fu",
            "Eugene Ch'ng",
            "Uwe Aickelin",
            "Lanyun Zhang"
        ],
        "abstract": "Novelty detection in news events has long been a difficult problem. A number of models performed well on specific data streams but certain issues are far from being solved, particularly in large data streams from the WWW where unpredictability of new terms requires adaptation in the vector space model. We present a novel event detection system based on the Incremental Term Frequency-Inverse Document Frequency (TF-IDF) weighting incorporated with Locality Sensitive Hashing (LSH). Our system could efficiently and effectively adapt to the changes within the data streams of any new terms with continual updates to the vector space model. Regarding miss probability, our proposed novelty detection framework outperforms a recognised baseline system by approximately 16% when evaluating a benchmark dataset from Google News.\n    ",
        "submission_date": "2016-04-30T00:00:00",
        "last_modified_date": "2016-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.00164",
        "title": "Look-ahead before you leap: end-to-end active recognition by forecasting the effect of motion",
        "authors": [
            "Dinesh Jayaraman",
            "Kristen Grauman"
        ],
        "abstract": "Visual recognition systems mounted on autonomous moving agents face the challenge of unconstrained data, but simultaneously have the opportunity to improve their performance by moving to acquire new views of test data. In this work, we first show how a recurrent neural network-based system may be trained to perform end-to-end learning of motion policies suited for this \"active recognition\" setting. Further, we hypothesize that active vision requires an agent to have the capacity to reason about the effects of its motions on its view of the world. To verify this hypothesis, we attempt to induce this capacity in our active recognition pipeline, by simultaneously learning to forecast the effects of the agent's motions on its internal representation of the environment conditional on all past views. Results across two challenging datasets confirm both that our end-to-end system successfully learns meaningful policies for active category recognition, and that \"learning to look ahead\" further boosts recognition performance.\n    ",
        "submission_date": "2016-04-30T00:00:00",
        "last_modified_date": "2016-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.00303",
        "title": "A Self-Taught Artificial Agent for Multi-Physics Computational Model Personalization",
        "authors": [
            "Dominik Neumann",
            "Tommaso Mansi",
            "Lucian Itu",
            "Bogdan Georgescu",
            "Elham Kayvanpour",
            "Farbod Sedaghat-Hamedani",
            "Ali Amr",
            "Jan Haas",
            "Hugo Katus",
            "Benjamin Meder",
            "Stefan Steidl",
            "Joachim Hornegger",
            "Dorin Comaniciu"
        ],
        "abstract": "Personalization is the process of fitting a model to patient data, a critical step towards application of multi-physics computational models in clinical practice. Designing robust personalization algorithms is often a tedious, time-consuming, model- and data-specific process. We propose to use artificial intelligence concepts to learn this task, inspired by how human experts manually perform it. The problem is reformulated in terms of reinforcement learning. In an off-line phase, Vito, our self-taught artificial agent, learns a representative decision process model through exploration of the computational model: it learns how the model behaves under change of parameters. The agent then automatically learns an optimal strategy for on-line personalization. The algorithm is model-independent; applying it to a new model requires only adjusting few hyper-parameters of the agent and defining the observations to match. The full knowledge of the model itself is not required. Vito was tested in a synthetic scenario, showing that it could learn how to optimize cost functions generically. Then Vito was applied to the inverse problem of cardiac electrophysiology and the personalization of a whole-body circulation model. The obtained results suggested that Vito could achieve equivalent, if not better goodness of fit than standard methods, while being more robust (up to 11% higher success rates) and with faster (up to seven times) convergence rate. Our artificial intelligence approach could thus make personalization algorithms generalizable and self-adaptable to any patient and any model.\n    ",
        "submission_date": "2016-05-01T00:00:00",
        "last_modified_date": "2016-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.00596",
        "title": "Graph Clustering Bandits for Recommendation",
        "authors": [
            "Shuai Li",
            "Claudio Gentile",
            "Alexandros Karatzoglou"
        ],
        "abstract": "We investigate an efficient context-dependent clustering technique for recommender systems based on exploration-exploitation strategies through multi-armed bandits over multiple users. Our algorithm dynamically groups users based on their observed behavioral similarity during a sequence of logged activities. In doing so, the algorithm reacts to the currently served user by shaping clusters around him/her but, at the same time, it explores the generation of clusters over users which are not currently engaged. We motivate the effectiveness of this clustering policy, and provide an extensive empirical analysis on real-world datasets, showing scalability and improved prediction performance over state-of-the-art methods for sequential clustering of users in multi-armed bandit scenarios.\n    ",
        "submission_date": "2016-05-02T00:00:00",
        "last_modified_date": "2016-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.00854",
        "title": "Fast Simulation of Probabilistic Boolean Networks (Technical Report)",
        "authors": [
            "Andrzej Mizera",
            "Jun Pang",
            "Qixia Yuan"
        ],
        "abstract": "Probabilistic Boolean networks (PBNs) is an important mathematical framework widely used for modelling and analysing biological systems. PBNs are suited for modelling large biological systems, which more and more often arise in systems biology. However, the large system size poses a~significant challenge to the analysis of PBNs, in particular, to the crucial analysis of their steady-state behaviour. Numerical methods for performing steady-state analyses suffer from the state-space explosion problem, which makes the utilisation of statistical methods the only viable approach. However, such methods require long simulations of PBNs, rendering the simulation speed a crucial efficiency factor. For large PBNs and high estimation precision requirements, a slow simulation speed becomes an obstacle. In this paper, we propose a structure-based method for fast simulation of PBNs. This method first performs a network reduction operation and then divides nodes into groups for parallel simulation. Experimental results show that our method can lead to an approximately 10 times speedup for computing steady-state probabilities of a real-life biological network.\n    ",
        "submission_date": "2016-04-28T00:00:00",
        "last_modified_date": "2016-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01018",
        "title": "A Solution to Time-Varying Markov Decision Processes",
        "authors": [
            "Lantao Liu",
            "Gaurav S. Sukhatme"
        ],
        "abstract": "We consider a decision-making problem where the environment varies both in space and time. Such problems arise naturally when considering e.g., the navigation of an underwater robot amidst ocean currents or the navigation of an aerial vehicle in wind. To model such spatiotemporal variation, we extend the standard Markov Decision Process (MDP) to a new framework called the Time-Varying Markov Decision Process (TVMDP). The TVMDP has a time-varying state transition model and transforms the standard MDP that considers only immediate and static uncertainty descriptions of state transitions, to a framework that is able to adapt to future time-varying transition dynamics over some horizon. We show how to solve a TVMDP via a redesign of the MDP value propagation mechanisms by incorporating the introduced dynamics along the temporal dimension. We validate our framework in a marine robotics navigation setting using spatiotemporal ocean data and show that it outperforms prior efforts.\n    ",
        "submission_date": "2016-05-03T00:00:00",
        "last_modified_date": "2018-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01207",
        "title": "Ontology-Mediated Queries: Combined Complexity and Succinctness of Rewritings via Circuit Complexity",
        "authors": [
            "Meghyn Bienvenu",
            "Stanislav Kikot",
            "Roman Kontchakov",
            "Vladimir Podolskii",
            "Michael Zakharyaschev"
        ],
        "abstract": "We give solutions to two fundamental computational problems in ontology-based data access with the W3C standard ontology language OWL 2 QL: the succinctness problem for first-order rewritings of ontology-mediated queries (OMQs), and the complexity problem for OMQ answering. We classify OMQs according to the shape of their conjunctive queries (treewidth, the number of leaves) and the existential depth of their ontologies. For each of these classes, we determine the combined complexity of OMQ answering, and whether all OMQs in the class have polynomial-size first-order, positive existential, and nonrecursive datalog rewritings. We obtain the succinctness results using hypergraph programs, a new computational model for Boolean functions, which makes it possible to connect the size of OMQ rewritings and circuit complexity.\n    ",
        "submission_date": "2016-05-04T00:00:00",
        "last_modified_date": "2016-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01335",
        "title": "Learning from the memory of Atari 2600",
        "authors": [
            "Jakub Sygnowski",
            "Henryk Michalewski"
        ],
        "abstract": "We train a number of neural networks to play games Bowling, Breakout and Seaquest using information stored in the memory of a video game console Atari 2600. We consider four models of neural networks which differ in size and architecture: two networks which use only information contained in the RAM and two mixed networks which use both information in the RAM and information from the screen. As the benchmark we used the convolutional model proposed in NIPS and received comparable results in all considered games. Quite surprisingly, in the case of Seaquest we were able to train RAM-only agents which behave better than the benchmark screen-only agent. Mixing screen and RAM did not lead to an improved performance comparing to screen-only and RAM-only agents.\n    ",
        "submission_date": "2016-05-04T00:00:00",
        "last_modified_date": "2016-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01459",
        "title": "Movement Coordination in Human-Robot Teams: A Dynamical Systems Approach",
        "authors": [
            "Tariq Iqbal",
            "Samantha Rack",
            "Laurel D. Riek"
        ],
        "abstract": "In order to be effective teammates, robots need to be able to understand high-level human behavior to recognize, anticipate, and adapt to human motion. We have designed a new approach to enable robots to perceive human group motion in real-time, anticipate future actions, and synthesize their own motion accordingly. We explore this within the context of joint action, where humans and robots move together synchronously. In this paper, we present an anticipation method which takes high-level group behavior into account. We validate the method within a human-robot interaction scenario, where an autonomous mobile robot observes a team of human dancers, and then successfully and contingently coordinates its movements to \"join the dance\". We compared the results of our anticipation method to move the robot with another method which did not rely on high-level group behavior, and found our method performed better both in terms of more closely synchronizing the robot's motion to the team, and also exhibiting more contingent and fluent motion. These findings suggest that the robot performs better when it has an understanding of high-level group behavior than when it does not. This work will help enable others in the robotics community to build more fluent and adaptable robots in the future.\n    ",
        "submission_date": "2016-05-04T00:00:00",
        "last_modified_date": "2016-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01622",
        "title": "Improving abcdSAT by At-Least-One Recently Used Clause Management Strategy",
        "authors": [
            "Jingchao Chen"
        ],
        "abstract": "We improve further the 2015 version of abcdSAT by various heuristics such as at-least-one recently used strategy, learnt clause database approximation reduction etc. Based on the requirement of different tracks at the SAT Competition 2016, we develop three versions of abcdSAT: drup, inc and lim, which participate in the competition of main (agile), incremental library and no-limit track, respectively.\n    ",
        "submission_date": "2016-05-05T00:00:00",
        "last_modified_date": "2016-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01681",
        "title": "Brain Emotional Learning-Based Prediction Model (For Long-Term Chaotic Prediction Applications)",
        "authors": [
            "Mahboobeh Parsapoor"
        ],
        "abstract": "This study suggests a new prediction model for chaotic time series inspired by the brain emotional learning of mammals. We describe the structure and function of this model, which is referred to as BELPM (Brain Emotional Learning-Based Prediction Model). Structurally, the model mimics the connection between the regions of the limbic system, and functionally it uses weighted k nearest neighbors to imitate the roles of those regions. The learning algorithm of BELPM is defined using steepest descent (SD) and the least square estimator (LSE). Two benchmark chaotic time series, Lorenz and Henon, have been used to evaluate the performance of BELPM. The obtained results have been compared with those of other prediction methods. The results show that BELPM has the capability to achieve a reasonable accuracy for long-term prediction of chaotic time series, using a limited amount of training data and a reasonably low computational time.\n    ",
        "submission_date": "2016-05-05T00:00:00",
        "last_modified_date": "2016-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01703",
        "title": "A note on adjusting $R^2$ for using with cross-validation",
        "authors": [
            "Indre Zliobaite",
            "Nikolaj Tatti"
        ],
        "abstract": "We show how to adjust the coefficient of determination ($R^2$) when used for measuring predictive accuracy via leave-one-out cross-validation.\n    ",
        "submission_date": "2016-05-05T00:00:00",
        "last_modified_date": "2016-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01939",
        "title": "Energy Disaggregation for Real-Time Building Flexibility Detection",
        "authors": [
            "Elena Mocanu",
            "Phuong H. Nguyen",
            "Madeleine Gibescu"
        ],
        "abstract": "Energy is a limited resource which has to be managed wisely, taking into account both supply-demand matching and capacity constraints in the distribution grid. One aspect of the smart energy management at the building level is given by the problem of real-time detection of flexible demand available. In this paper we propose the use of energy disaggregation techniques to perform this task. Firstly, we investigate the use of existing classification methods to perform energy disaggregation. A comparison is performed between four classifiers, namely Naive Bayes, k-Nearest Neighbors, Support Vector Machine and AdaBoost. Secondly, we propose the use of Restricted Boltzmann Machine to automatically perform feature extraction. The extracted features are then used as inputs to the four classifiers and consequently shown to improve their accuracy. The efficiency of our approach is demonstrated on a real database consisting of detailed appliance-level measurements with high temporal resolution, which has been used for energy disaggregation in previous studies, namely the REDD. The results show robustness and good generalization capabilities to newly presented buildings with at least 96% accuracy.\n    ",
        "submission_date": "2016-05-06T00:00:00",
        "last_modified_date": "2016-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02038",
        "title": "Markov Chain methods for the bipartite Boolean quadratic programming problem",
        "authors": [
            "Daniel Karapetyan",
            "Abraham P. Punnen",
            "Andrew J. Parkes"
        ],
        "abstract": "We study the Bipartite Boolean Quadratic Programming Problem (BBQP) which is an extension of the well known Boolean Quadratic Programming Problem (BQP). Applications of the BBQP include mining discrete patterns from binary data, approximating matrices by rank-one binary matrices, computing the cut-norm of a matrix, and solving optimisation problems such as maximum weight biclique, bipartite maximum weight cut, maximum weight induced sub-graph of a bipartite graph, etc. For the BBQP, we first present several algorithmic components, specifically, hill climbers and mutations, and then show how to combine them in a high-performance metaheuristic. Instead of hand-tuning a standard metaheuristic to test the efficiency of the hybrid of the components, we chose to use an automated generation of a multi-component metaheuristic to save human time, and also improve objectivity in the analysis and comparisons of components. For this we designed a new metaheuristic schema which we call Conditional Markov Chain Search (CMCS). We show that CMCS is flexible enough to model several standard metaheuristics; this flexibility is controlled by multiple numeric parameters, and so is convenient for automated generation. We study the configurations revealed by our approach and show that the best of them outperforms the previous state-of-the-art BBQP algorithm by several orders of magnitude. In our experiments we use benchmark instances introduced in the preliminary version of this paper and described here, which have already become the de facto standard in the BBQP literature.\n    ",
        "submission_date": "2016-05-06T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02046",
        "title": "Low-Complexity Stochastic Generalized Belief Propagation",
        "authors": [
            "Farzin Haddadpour",
            "Mahdi Jafari Siavoshani",
            "Morteza Noshad"
        ],
        "abstract": "The generalized belief propagation (GBP), introduced by Yedidia et al., is an extension of the belief propagation (BP) algorithm, which is widely used in different problems involved in calculating exact or approximate marginals of probability distributions. In many problems, it has been observed that the accuracy of GBP considerably outperforms that of BP. However, because in general the computational complexity of GBP is higher than BP, its application is limited in practice.\n",
        "submission_date": "2016-05-06T00:00:00",
        "last_modified_date": "2016-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02097",
        "title": "ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning",
        "authors": [
            "Micha\u0142 Kempka",
            "Marek Wydmuch",
            "Grzegorz Runc",
            "Jakub Toczek",
            "Wojciech Ja\u015bkowski"
        ],
        "abstract": "The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real-world tasks since they involve non-realistic 2D environments and the third-person perspective. Here, we propose a novel test-bed platform for reinforcement learning research from raw visual information which employs the first-person perspective in a semi-realistic 3D world. The software, called ViZDoom, is based on the classical first-person shooter video game, Doom. It allows developing bots that play the game using the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a convenient mechanism of user scenarios. In the experimental part, we test the environment by trying to learn bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, for both scenarios, we were able to train competent bots, which exhibit human-like behaviors. The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible.\n    ",
        "submission_date": "2016-05-06T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02129",
        "title": "Adobe-MIT submission to the DSTC 4 Spoken Language Understanding pilot task",
        "authors": [
            "Franck Dernoncourt",
            "Ji Young Lee",
            "Trung H. Bui",
            "Hung H. Bui"
        ],
        "abstract": "The Dialog State Tracking Challenge 4 (DSTC 4) proposes several pilot tasks. In this paper, we focus on the spoken language understanding pilot task, which consists of tagging a given utterance with speech acts and semantic slots. We compare different classifiers: the best system obtains 0.52 and 0.67 F1-scores on the test set for speech act recognition for the tourist and the guide respectively, and 0.52 F1-score for semantic tagging for both the guide and the tourist.\n    ",
        "submission_date": "2016-05-07T00:00:00",
        "last_modified_date": "2016-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02130",
        "title": "Robust Dialog State Tracking for Large Ontologies",
        "authors": [
            "Franck Dernoncourt",
            "Ji Young Lee",
            "Trung H. Bui",
            "Hung H. Bui"
        ],
        "abstract": "The Dialog State Tracking Challenge 4 (DSTC 4) differentiates itself from the previous three editions as follows: the number of slot-value pairs present in the ontology is much larger, no spoken language understanding output is given, and utterances are labeled at the subdialog level. This paper describes a novel dialog state tracking method designed to work robustly under these conditions, using elaborate string matching, coreference resolution tailored for dialogs and a few other improvements. The method can correctly identify many values that are not explicitly present in the utterance. On the final evaluation, our method came in first among 7 competing teams and 24 entries. The F1-score achieved by our method was 9 and 7 percentage points higher than that of the runner-up for the utterance-level evaluation and for the subdialog-level evaluation, respectively.\n    ",
        "submission_date": "2016-05-07T00:00:00",
        "last_modified_date": "2016-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02401",
        "title": "Audio Event Detection using Weakly Labeled Data",
        "authors": [
            "Anurag Kumar",
            "Bhiksha Raj"
        ],
        "abstract": "Acoustic event detection is essential for content analysis and description of multimedia recordings. The majority of current literature on the topic learns the detectors through fully-supervised techniques employing strongly labeled data. However, the labels available for majority of multimedia data are generally weak and do not provide sufficient detail for such methods to be employed. In this paper we propose a framework for learning acoustic event detectors using only weakly labeled data. We first show that audio event detection using weak labels can be formulated as an Multiple Instance Learning problem. We then suggest two frameworks for solving multiple-instance learning, one based on support vector machines, and the other on neural networks. The proposed methods can help in removing the time consuming and expensive process of manually annotating data to facilitate fully supervised learning. Moreover, it can not only detect events in a recording but can also provide temporal locations of events in the recording. This helps in obtaining a complete description of the recording and is notable since temporal information was never known in the first place in weakly labeled data.\n    ",
        "submission_date": "2016-05-09T00:00:00",
        "last_modified_date": "2016-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02669",
        "title": "The GPU-based Parallel Ant Colony System",
        "authors": [
            "Rafa\u0142 Skinderowicz"
        ],
        "abstract": "The Ant Colony System (ACS) is, next to Ant Colony Optimization (ACO) and the MAX-MIN Ant System (MMAS), one of the most efficient metaheuristic algorithms inspired by the behavior of ants. In this article we present three novel parallel versions of the ACS for the graphics processing units (GPUs). To the best of our knowledge, this is the first such work on the ACS which shares many key elements of the ACO and the MMAS, but differences in the process of building solutions and updating the pheromone trails make obtaining an efficient parallel version for the GPUs a difficult task. The proposed parallel versions of the ACS differ mainly in their implementations of the pheromone memory. The first two use the standard pheromone matrix, and the third uses a novel selective pheromone memory. Computational experiments conducted on several Travelling Salesman Problem (TSP) instances of sizes ranging from 198 to 2392 cities showed that the parallel ACS on Nvidia Kepler GK104 GPU (1536 CUDA cores) is able to obtain a speedup up to 24.29x vs the sequential ACS running on a single core of Intel Xeon E5-2670 CPU. The parallel ACS with the selective pheromone memory achieved speedups up to 16.85x, but in most cases the obtained solutions were of significantly better quality than for the sequential ACS.\n    ",
        "submission_date": "2016-05-09T00:00:00",
        "last_modified_date": "2017-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02697",
        "title": "Ask Your Neurons: A Deep Learning Approach to Visual Question Answering",
        "authors": [
            "Mateusz Malinowski",
            "Marcus Rohrbach",
            "Mario Fritz"
        ],
        "abstract": "We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Ask Your Neurons, a scalable, jointly trained, end-to-end formulation to this problem.\n",
        "submission_date": "2016-05-09T00:00:00",
        "last_modified_date": "2016-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03035",
        "title": "Context-Aware Adaptive Framework for e-Health Monitoring",
        "authors": [
            "Haider Mshali",
            "Tayeb Lemlouma",
            "Damien Magoni"
        ],
        "abstract": "For improving e-health services, we propose a context-aware framework to monitor the activities of daily living of dependent persons. We define a strategy for generating long-term realistic scenarios and a framework containing an adaptive monitoring algorithm based on three approaches for optimizing resource usage. The used approaches provide a deep knowledge about the person's context by considering: the person's profile, the activities and the relationships between activities. We evaluate the performances of our framework and show its adaptability and significant reduction in network, energy and processing usage over a traditional monitoring implementation.\n    ",
        "submission_date": "2016-05-10T00:00:00",
        "last_modified_date": "2016-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03269",
        "title": "A Hierarchical Emotion Regulated Sensorimotor Model: Case Studies",
        "authors": [
            "Junpei Zhong",
            "Rony Novianto",
            "Mingjun Dai",
            "Xinzheng Zhang",
            "Angelo Cangelosi"
        ],
        "abstract": "Inspired by the hierarchical cognitive architecture and the perception-action model (PAM), we propose that the internal status acts as a kind of common-coding representation which affects, mediates and even regulates the sensorimotor behaviours. These regulation can be depicted in the Bayesian framework, that is why cognitive agents are able to generate behaviours with subtle differences according to their emotion or recognize the emotion by perception. A novel recurrent neural network called recurrent neural network with parametric bias units (RNNPB) runs in three modes, constructing a two-level emotion regulated learning model, was further applied to testify this theory in two different cases.\n    ",
        "submission_date": "2016-05-11T00:00:00",
        "last_modified_date": "2016-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03468",
        "title": "A constrained L1 minimization approach for estimating multiple Sparse Gaussian or Nonparanormal Graphical Models",
        "authors": [
            "Beilun Wang",
            "Ritambhara Singh",
            "Yanjun Qi"
        ],
        "abstract": "Identifying context-specific entity networks from aggregated data is an important task, arising often in bioinformatics and neuroimaging. Computationally, this task can be formulated as jointly estimating multiple different, but related, sparse Undirected Graphical Models (UGM) from aggregated samples across several contexts. Previous joint-UGM studies have mostly focused on sparse Gaussian Graphical Models (sGGMs) and can't identify context-specific edge patterns directly. We, therefore, propose a novel approach, SIMULE (detecting Shared and Individual parts of MULtiple graphs Explicitly) to learn multi-UGM via a constrained L1 minimization. SIMULE automatically infers both specific edge patterns that are unique to each context and shared interactions preserved among all the contexts. Through the L1 constrained formulation, this problem is cast as multiple independent subtasks of linear programming that can be solved efficiently in parallel. In addition to Gaussian data, SIMULE can also handle multivariate Nonparanormal data that greatly relaxes the normality assumption that many real-world applications do not follow. We provide a novel theoretical proof showing that SIMULE achieves a consistent result at the rate O(log(Kp)/n_{tot}). On multiple synthetic datasets and two biomedical datasets, SIMULE shows significant improvement over state-of-the-art multi-sGGM and single-UGM baselines.\n    ",
        "submission_date": "2016-05-11T00:00:00",
        "last_modified_date": "2017-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03498",
        "title": "Deep Neural Networks Under Stress",
        "authors": [
            "Micael Carvalho",
            "Matthieu Cord",
            "Sandra Avila",
            "Nicolas Thome",
            "Eduardo Valle"
        ],
        "abstract": "In recent years, deep architectures have been used for transfer learning with state-of-the-art performance in many datasets. The properties of their features remain, however, largely unstudied under the transfer perspective. In this work, we present an extensive analysis of the resiliency of feature vectors extracted from deep models, with special focus on the trade-off between performance and compression rate. By introducing perturbations to image descriptions extracted from a deep convolutional neural network, we change their precision and number of dimensions, measuring how it affects the final score. We show that deep features are more robust to these disturbances when compared to classical approaches, achieving a compression rate of 98.4%, while losing only 0.88% of their original score for Pascal VOC 2007.\n    ",
        "submission_date": "2016-05-11T00:00:00",
        "last_modified_date": "2016-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03661",
        "title": "Learning Representations for Counterfactual Inference",
        "authors": [
            "Fredrik D. Johansson",
            "Uri Shalit",
            "David Sontag"
        ],
        "abstract": "Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, \"Would this patient have lower blood sugar had she received a different medication?\". We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art.\n    ",
        "submission_date": "2016-05-12T00:00:00",
        "last_modified_date": "2018-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03915",
        "title": "Optimizing human-interpretable dialog management policy using Genetic Algorithm",
        "authors": [
            "Hang Ren",
            "Weiqun Xu",
            "Yonghong Yan"
        ],
        "abstract": "Automatic optimization of spoken dialog management policies that are robust to environmental noise has long been the goal for both academia and industry. Approaches based on reinforcement learning have been proved to be effective. However, the numerical representation of dialog policy is human-incomprehensible and difficult for dialog system designers to verify or modify, which limits its practical application. In this paper we propose a novel framework for optimizing dialog policies specified in domain language using genetic algorithm. The human-interpretable representation of policy makes the method suitable for practical employment. We present learning algorithms using user simulation and real human-machine dialogs ",
        "submission_date": "2016-05-12T00:00:00",
        "last_modified_date": "2016-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04039",
        "title": "Cross-Domain Visual Matching via Generalized Similarity Measure and Feature Learning",
        "authors": [
            "Liang Lin",
            "Guangrun Wang",
            "Wangmeng Zuo",
            "Xiangchu Feng",
            "Lei Zhang"
        ],
        "abstract": "Cross-domain visual data matching is one of the fundamental problems in many real-world vision tasks, e.g., matching persons across ID photos and surveillance videos. Conventional approaches to this problem usually involves two steps: i) projecting samples from different domains into a common space, and ii) computing (dis-)similarity in this space based on a certain distance. In this paper, we present a novel pairwise similarity measure that advances existing models by i) expanding traditional linear projections into affine transformations and ii) fusing affine Mahalanobis distance and Cosine similarity by a data-driven combination. Moreover, we unify our similarity measure with feature representation learning via deep convolutional neural networks. Specifically, we incorporate the similarity measure matrix into the deep architecture, enabling an end-to-end way of model optimization. We extensively evaluate our generalized similarity model in several challenging cross-domain matching tasks: person re-identification under different views and face verification over different modalities (i.e., faces from still images and videos, older and younger faces, and sketch and photo portraits). The experimental results demonstrate superior performance of our model over other state-of-the-art methods.\n    ",
        "submission_date": "2016-05-13T00:00:00",
        "last_modified_date": "2016-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04056",
        "title": "Causal Discovery for Manufacturing Domains",
        "authors": [
            "Katerina Marazopoulou",
            "Rumi Ghosh",
            "Prasanth Lade",
            "David Jensen"
        ],
        "abstract": "Yield and quality improvement is of paramount importance to any manufacturing company. One of the ways of improving yield is through discovery of the root causal factors affecting yield. We propose the use of data-driven interpretable causal models to identify key factors affecting yield. We focus on factors that are measured in different stages of production and testing in the manufacturing cycle of a product. We apply causal structure learning techniques on real data collected from this line. Specifically, the goal of this work is to learn interpretable causal models from observational data produced by manufacturing lines.\n",
        "submission_date": "2016-05-13T00:00:00",
        "last_modified_date": "2016-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04072",
        "title": "Towards Empathetic Human-Robot Interactions",
        "authors": [
            "Pascale Fung",
            "Dario Bertero",
            "Yan Wan",
            "Anik Dey",
            "Ricky Ho Yin Chan",
            "Farhad Bin Siddique",
            "Yang Yang",
            "Chien-Sheng Wu",
            "Ruixi Lin"
        ],
        "abstract": "Since the late 1990s when speech companies began providing their customer-service software in the market, people have gotten used to speaking to machines. As people interact more often with voice and gesture controlled machines, they expect the machines to recognize different emotions, and understand other high level communication features such as humor, sarcasm and intention. In order to make such communication possible, the machines need an empathy module in them which can extract emotions from human speech and behavior and can decide the correct response of the robot. Although research on empathetic robots is still in the early stage, we described our approach using signal processing techniques, sentiment analysis and machine learning algorithms to make robots that can \"understand\" human emotion. We propose Zara the Supergirl as a prototype system of empathetic robots. It is a software based virtual android, with an animated cartoon character to present itself on the screen. She will get \"smarter\" and more empathetic through its deep learning algorithms, and by gathering more data and learning from it. In this paper, we present our work so far in the areas of deep learning of emotion and sentiment recognition, as well as humor recognition. We hope to explore the future direction of android development and how it can help improve people's lives.\n    ",
        "submission_date": "2016-05-13T00:00:00",
        "last_modified_date": "2016-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04074",
        "title": "Wisdom of Crowds cluster ensemble",
        "authors": [
            "Hosein Alizadeh",
            "Muhammad Yousefnezhad",
            "Behrouz Minaei Bidgoli"
        ],
        "abstract": "The Wisdom of Crowds is a phenomenon described in social science that suggests four criteria applicable to groups of people. It is claimed that, if these criteria are satisfied, then the aggregate decisions made by a group will often be better than those of its individual members. Inspired by this concept, we present a novel feedback framework for the cluster ensemble problem, which we call Wisdom of Crowds Cluster Ensemble (WOCCE). Although many conventional cluster ensemble methods focusing on diversity have recently been proposed, WOCCE analyzes the conditions necessary for a crowd to exhibit this collective wisdom. These include decentralization criteria for generating primary results, independence criteria for the base algorithms, and diversity criteria for the ensemble members. We suggest appropriate procedures for evaluating these measures, and propose a new measure to assess the diversity. We evaluate the performance of WOCCE against some other traditional base algorithms as well as state-of-the-art ensemble methods. The results demonstrate the efficiency of WOCCE's aggregate decision-making compared to other algorithms.\n    ",
        "submission_date": "2016-05-13T00:00:00",
        "last_modified_date": "2016-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04122",
        "title": "Natural Language Semantics and Computability",
        "authors": [
            "Richard Moot",
            "Christian Retor\u00e9"
        ],
        "abstract": "This paper is a reflexion on the computability of natural language semantics. It does not contain a new model or new results in the formal semantics of natural language: it is rather a computational analysis of the logical models and algorithms currently used in natural language semantics, defined as the mapping of a statement to logical formulas - formulas, because a statement can be ambiguous. We argue that as long as possible world semantics is left out, one can compute the semantic representation(s) of a given statement, including aspects of lexical meaning. We also discuss the algorithmic complexity of this process.\n    ",
        "submission_date": "2016-05-13T00:00:00",
        "last_modified_date": "2016-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04135",
        "title": "Online Optimization Methods for the Quantification Problem",
        "authors": [
            "Purushottam Kar",
            "Shuai Li",
            "Harikrishna Narasimhan",
            "Sanjay Chawla",
            "Fabrizio Sebastiani"
        ],
        "abstract": "The estimation of class prevalence, i.e., the fraction of a population that belongs to a certain class, is a very useful tool in data analytics and learning, and finds applications in many domains such as sentiment analysis, epidemiology, etc. For example, in sentiment analysis, the objective is often not to estimate whether a specific text conveys a positive or a negative sentiment, but rather estimate the overall distribution of positive and negative sentiments during an event window. A popular way of performing the above task, often dubbed quantification, is to use supervised learning to train a prevalence estimator from labeled data.\n",
        "submission_date": "2016-05-13T00:00:00",
        "last_modified_date": "2016-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04263",
        "title": "OBDA Constraints for Effective Query Answering (Extended Version)",
        "authors": [
            "Dag Hovland",
            "Davide Lanti",
            "Martin Rezk",
            "Guohui Xiao"
        ],
        "abstract": "In Ontology Based Data Access (OBDA) users pose SPARQL queries over an ontology that lies on top of relational datasources. These queries are translated on-the-fly into SQL queries by OBDA systems. Standard SPARQL-to-SQL translation techniques in OBDA often produce SQL queries containing redundant joins and unions, even after a number of semantic and structural optimizations. These redundancies are detrimental to the performance of query answering, especially in complex industrial OBDA scenarios with large enterprise databases. To address this issue, we introduce two novel notions of OBDA constraints and show how to exploit them for efficient query answering. We conduct an extensive set of experiments on large datasets using real world data and queries, showing that these techniques strongly improve the performance of query answering up to orders of magnitude.\n    ",
        "submission_date": "2016-05-13T00:00:00",
        "last_modified_date": "2016-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04465",
        "title": "Monotone Retargeting for Unsupervised Rank Aggregation with Object Features",
        "authors": [
            "Avradeep Bhowmik",
            "Joydeep Ghosh"
        ],
        "abstract": "Learning the true ordering between objects by aggregating a set of expert opinion rank order lists is an important and ubiquitous problem in many applications ranging from social choice theory to natural language processing and search aggregation. We study the problem of unsupervised rank aggregation where no ground truth ordering information in available, neither about the true preference ordering between any set of objects nor about the quality of individual rank lists. Aggregating the often inconsistent and poor quality rank lists in such an unsupervised manner is a highly challenging problem, and standard consensus-based methods are often ill-defined, and difficult to solve. In this manuscript we propose a novel framework to bypass these issues by using object attributes to augment the standard rank aggregation framework. We design algorithms that learn joint models on both rank lists and object features to obtain an aggregated rank ordering that is more accurate and robust, and also helps weed out rank lists of dubious validity. We validate our techniques on synthetic datasets where our algorithm is able to estimate the true rank ordering even when the rank lists are corrupted. Experiments on three real datasets, MQ2008, MQ2008 and OHSUMED, show that using object features can result in significant improvement in performance over existing rank aggregation methods that do not use object information. Furthermore, when at least some of the rank lists are of high quality, our methods are able to effectively exploit their high expertise to output an aggregated rank ordering of great accuracy.\n    ",
        "submission_date": "2016-05-14T00:00:00",
        "last_modified_date": "2016-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04466",
        "title": "Generalized Linear Models for Aggregated Data",
        "authors": [
            "Avradeep Bhowmik",
            "Joydeep Ghosh",
            "Oluwasanmi Koyejo"
        ],
        "abstract": "Databases in domains such as healthcare are routinely released to the public in aggregated form. Unfortunately, naive modeling with aggregated data may significantly diminish the accuracy of inferences at the individual level. This paper addresses the scenario where features are provided at the individual level, but the target variables are only available as histogram aggregates or order statistics. We consider a limiting case of generalized linear modeling when the target variables are only known up to permutation, and explore how this relates to permutation testing; a standard technique for assessing statistical dependency. Based on this relationship, we propose a simple algorithm to estimate the model parameters and individual level inferences via alternating imputation and standard generalized linear model fitting. Our results suggest the effectiveness of the proposed approach when, in the original data, permutation testing accurately ascertains the veracity of the linear relationship. The framework is extended to general histogram data with larger bins - with order statistics such as the median as a limiting case. Our experimental results on simulated data and aggregated healthcare data suggest a diminishing returns property with respect to the granularity of the histogram - when a linear relationship holds in the original data, the targets can be predicted accurately given relatively coarse histograms.\n    ",
        "submission_date": "2016-05-14T00:00:00",
        "last_modified_date": "2016-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04672",
        "title": "A Critical Examination of RESCAL for Completion of Knowledge Bases with Transitive Relations",
        "authors": [
            "Pushpendre Rastogi",
            "Benjamin Van Durme"
        ],
        "abstract": "Link prediction in large knowledge graphs has received a lot of attention recently because of its importance for inferring missing relations and for completing and improving noisily extracted knowledge graphs. Over the years a number of machine learning researchers have presented various models for predicting the presence of missing relations in a knowledge base. Although all the previous methods are presented with empirical results that show high performance on select datasets, there is almost no previous work on understanding the connection between properties of a knowledge base and the performance of a model. In this paper we analyze the RESCAL method and prove that it can not encode asymmetric transitive relations in knowledge bases.\n    ",
        "submission_date": "2016-05-16T00:00:00",
        "last_modified_date": "2016-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04682",
        "title": "High-Performance Computing for Scheduling Decision Support: A Parallel Depth-First Search Heuristic",
        "authors": [
            "Gerhard Rauchecker",
            "Guido Schryen"
        ],
        "abstract": "Many academic disciplines - including information systems, computer science, and operations management - face scheduling problems as important decision making tasks. Since many scheduling problems are NP-hard in the strong sense, there is a need for developing solution heuristics. For scheduling problems with setup times on unrelated parallel machines, there is limited research on solution methods and to the best of our knowledge, parallel computer architectures have not yet been taken advantage of. We address this gap by proposing and implementing a new solution heuristic and by testing different parallelization strategies. In our computational experiments, we show that our heuristic calculates near-optimal solutions even for large instances and that computing time can be reduced substantially by our parallelization approach.\n    ",
        "submission_date": "2016-05-16T00:00:00",
        "last_modified_date": "2016-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04715",
        "title": "On the Complexity of Connection Games",
        "authors": [
            "\u00c9douard Bonnet",
            "Florian Jamain",
            "Abdallah Saffidine"
        ],
        "abstract": "In this paper, we study three connection games among the most widely played: Havannah, Twixt, and Slither. We show that determining the outcome of an arbitrary input position is PSPACE-complete in all three cases. Our reductions are based on the popular graph problem Generalized Geography and on Hex itself. We also consider the complexity of generalizations of Hex parameterized by the length of the solution and establish that while Short Generalized Hex is W[1]-hard, Short Hex is FPT. Finally, we prove that the ultra-weak solution to the empty starting position in hex cannot be fully adapted to any of these three games.\n    ",
        "submission_date": "2016-05-16T00:00:00",
        "last_modified_date": "2016-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04812",
        "title": "Off-policy evaluation for slate recommendation",
        "authors": [
            "Adith Swaminathan",
            "Akshay Krishnamurthy",
            "Alekh Agarwal",
            "Miroslav Dud\u00edk",
            "John Langford",
            "Damien Jose",
            "Imed Zitouni"
        ],
        "abstract": "This paper studies the evaluation of policies that recommend an ordered set of items (e.g., a ranking) based on some context---a common scenario in web search, ads, and recommendation. We build on techniques from combinatorial bandits to introduce a new practical estimator that uses logged data to estimate a policy's performance. A thorough empirical evaluation on real-world data reveals that our estimator is accurate in a variety of settings, including as a subroutine in a learning-to-rank task, where it achieves competitive performance. We derive conditions under which our estimator is unbiased---these conditions are weaker than prior heuristics for slate evaluation---and experimentally demonstrate a smaller bias than parametric approaches, even when these conditions are violated. Finally, our theory and experiments also show exponential savings in the amount of required data compared with general unbiased estimators.\n    ",
        "submission_date": "2016-05-16T00:00:00",
        "last_modified_date": "2017-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04874",
        "title": "Gearbox Fault Detection through PSO Exact Wavelet Analysis and SVM Classifier",
        "authors": [
            "Amir Hosein Zamanian",
            "Abdolreza Ohadi"
        ],
        "abstract": "Time-frequency methods for vibration-based gearbox faults detection have been considered the most efficient method. Among these methods, continuous wavelet transform (CWT) as one of the best time-frequency method has been used for both stationary and transitory signals. Some deficiencies of CWT are problem of overlapping and distortion ofsignals. In this condition, a large amount of redundant information exists so that it may cause false alarm or misinterpretation of the operator. In this paper a modified method called Exact Wavelet Analysis is used to minimize the effects of overlapping and distortion in case of gearbox faults. To implement exact wavelet analysis, Particle Swarm Optimization (PSO) algorithm has been used for this purpose. This method have been implemented for the acceleration signals from 2D acceleration sensor acquired by Advantech PCI-1710 card from a gearbox test setup in Amirkabir University of Technology. Gearbox has been considered in both healthy and chipped tooth gears conditions. Kernelized Support Vector Machine (SVM) with radial basis functions has used the extracted features from exact wavelet analysis for classification. The efficiency of this classifier is then evaluated with the other signals acquired from the setup test. The results show that in comparison of CWT, PSO Exact Wavelet Transform has better ability in feature extraction in price of more computational effort. In addition, PSO exact wavelet has better speed comparing to Genetic Algorithm (GA) exact wavelet in condition of equal population because of factoring mutation and crossover in PSO algorithm. SVM classifier with the extracted features in gearbox shows very good results and its ability has been proved.\n    ",
        "submission_date": "2016-05-12T00:00:00",
        "last_modified_date": "2016-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04934",
        "title": "Self-Reflective Risk-Aware Artificial Cognitive Modeling for Robot Response to Human Behaviors",
        "authors": [
            "Fei Han",
            "Christopher Reardon",
            "Lynne E. Parker",
            "Hao Zhang"
        ],
        "abstract": "In order for cooperative robots (\"co-robots\") to respond to human behaviors accurately and efficiently in human-robot collaboration, interpretation of human actions, awareness of new situations, and appropriate decision making are all crucial abilities for co-robots. For this purpose, the human behaviors should be interpreted by co-robots in the same manner as human peers. To address this issue, a novel interpretability indicator is introduced so that robot actions are appropriate to the current human behaviors. In addition, the complete consideration of all potential situations of a robot's environment is nearly impossible in real-world applications, making it difficult for the co-robot to act appropriately and safely in new scenarios. This is true even when the pretrained model is highly accurate in a known situation. For effective and safe teaming with humans, we introduce a new generalizability indicator that allows a co-robot to self-reflect and reason about when an observation falls outside the co-robot's learned model. Based on topic modeling and two novel indicators, we propose a new Self-reflective Risk-aware Artificial Cognitive (SRAC) model. The co-robots are able to consider action risks and identify new situations so that better decisions can be made. Experiments both using real-world datasets and on physical robots suggest that our SRAC model significantly outperforms the traditional methodology and enables better decision making in response to human activities.\n    ",
        "submission_date": "2016-05-16T00:00:00",
        "last_modified_date": "2016-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05166",
        "title": "Digital Stylometry: Linking Profiles Across Social Networks",
        "authors": [
            "Soroush Vosoughi",
            "Helen Zhou",
            "Deb Roy"
        ],
        "abstract": "There is an ever growing number of users with accounts on multiple social media and networking sites. Consequently, there is increasing interest in matching user accounts and profiles across different social networks in order to create aggregate profiles of users. In this paper, we present models for Digital Stylometry, which is a method for matching users through stylometry inspired techniques. We experimented with linguistic, temporal, and combined temporal-linguistic models for matching user accounts, using standard and novel techniques. Using publicly available data, our best model, a combined temporal-linguistic one, was able to correctly match the accounts of 31% of 5,612 distinct users across Twitter and Facebook.\n    ",
        "submission_date": "2016-05-17T00:00:00",
        "last_modified_date": "2016-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05195",
        "title": "Enhanced Twitter Sentiment Classification Using Contextual Information",
        "authors": [
            "Soroush Vosoughi",
            "Helen Zhou",
            "Deb Roy"
        ],
        "abstract": "The rise in popularity and ubiquity of Twitter has made sentiment analysis of tweets an important and well-covered area of research. However, the 140 character limit imposed on tweets makes it hard to use standard linguistic methods for sentiment classification. On the other hand, what tweets lack in structure they make up with sheer volume and rich metadata. This metadata includes geolocation, temporal and author information. We hypothesize that sentiment is dependent on all these contextual factors. Different locations, times and authors have different emotional valences. In this paper, we explored this hypothesis by utilizing distant supervision to collect millions of labelled tweets from different locations, times and authors. We used this data to analyse the variation of tweet sentiments across different authors, times and locations. Once we explored and understood the relationship between these variables and sentiment, we used a Bayesian approach to combine these variables with more standard linguistic features such as n-grams to create a Twitter sentiment classifier. This combined classifier outperforms the purely linguistic classifier, showing that integrating the rich contextual information available on Twitter into sentiment classification is a promising direction of research.\n    ",
        "submission_date": "2016-05-17T00:00:00",
        "last_modified_date": "2021-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05247",
        "title": "Heart Rate Variability and Respiration Signal as Diagnostic Tools for Late Onset Sepsis in Neonatal Intensive Care Units",
        "authors": [
            "Yuan Wang",
            "Guy Carrault",
            "Alain Beuchee",
            "Nathalie Costet",
            "Huazhong Shu",
            "Lotfi Senhadji"
        ],
        "abstract": "Apnea-bradycardia is one of the major clinical early indicators of late-onset sepsis occurring in approximately 7% to 10% of all neonates and in more than 25% of very low birth weight infants in NICU. The objective of this paper was to determine if HRV, respiration and their relationships help to diagnose infection in premature infants via non-invasive ways in NICU. Therefore, we implement Mono-Channel (MC) and Bi-Channel (BC) Analysis in two groups: sepsis (S) vs. non-sepsis (NS). Firstly, we studied RR series not only by linear methods: time domain and frequency domain, but also by non-linear methods: chaos theory and information theory. The results show that alpha Slow, alpha Fast and Sample Entropy are significant parameters to distinguish S from NS. Secondly, the question about the functional coupling of HRV and nasal respiration is addressed. Local linear correlation coefficient r2t,f has been explored, while non-linear regression coefficient h2 was calculated in two directions. It is obvious that r2t,f within the third frequency band (0.2<f<0.4 Hz) and h2 in two directions were complementary approaches to diagnose sepsis. Thirdly, feasibility study is carried out on the candidate parameters selected from MC and BC respectively. We discovered that the proposed test based on optimal fusion of 6 features shows good performance with the largest AUC and a reduced probability of false alarm (PFA).\n    ",
        "submission_date": "2016-05-12T00:00:00",
        "last_modified_date": "2016-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05273",
        "title": "Learning Convolutional Neural Networks for Graphs",
        "authors": [
            "Mathias Niepert",
            "Mohamed Ahmed",
            "Konstantin Kutzkov"
        ],
        "abstract": "Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.\n    ",
        "submission_date": "2016-05-17T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05359",
        "title": "Option Discovery in Hierarchical Reinforcement Learning using Spatio-Temporal Clustering",
        "authors": [
            "Aravind Srinivas",
            "Ramnandan Krishnamurthy",
            "Peeyush Kumar",
            "Balaraman Ravindran"
        ],
        "abstract": "This paper introduces an automated skill acquisition framework in reinforcement learning which involves identifying a hierarchical description of the given task in terms of abstract states and extended actions between abstract states. Identifying such structures present in the task provides ways to simplify and speed up reinforcement learning algorithms. These structures also help to generalize such algorithms over multiple tasks without relearning policies from scratch. We use ideas from dynamical systems to find metastable regions in the state space and associate them with abstract states. The spectral clustering algorithm PCCA+ is used to identify suitable abstractions aligned to the underlying structure. Skills are defined in terms of the sequence of actions that lead to transitions between such abstract states. The connectivity information from PCCA+ is used to generate these skills or options. These skills are independent of the learning task and can be efficiently reused across a variety of tasks defined over the same model. This approach works well even without the exact model of the environment by using sample trajectories to construct an approximate estimate. We also present our approach to scaling the skill acquisition framework to complex tasks with large state spaces for which we perform state aggregation using the representation learned from an action conditional video prediction network and use the skill acquisition framework on the aggregated state space.\n    ",
        "submission_date": "2016-05-17T00:00:00",
        "last_modified_date": "2020-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05365",
        "title": "Dynamic Frame skip Deep Q Network",
        "authors": [
            "Aravind Srinivas",
            "Sahil Sharma",
            "Balaraman Ravindran"
        ],
        "abstract": "Deep Reinforcement Learning methods have achieved state of the art performance in learning control policies for the games in the Atari 2600 domain. One of the important parameters in the Arcade Learning Environment (ALE) is the frame skip rate. It decides the granularity at which agents can control game play. A frame skip value of $k$ allows the agent to repeat a selected action $k$ number of times. The current state of the art architectures like Deep Q-Network (DQN) and Dueling Network Architectures (DuDQN) consist of a framework with a static frame skip rate, where the action output from the network is repeated for a fixed number of frames regardless of the current state. In this paper, we propose a new architecture, Dynamic Frame skip Deep Q-Network (DFDQN) which makes the frame skip rate a dynamic learnable parameter. This allows us to choose the number of times an action is to be repeated based on the current state. We show empirically that such a setting improves the performance on relatively harder games like Seaquest.\n    ",
        "submission_date": "2016-05-17T00:00:00",
        "last_modified_date": "2020-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05433",
        "title": "Relations such as Hypernymy: Identifying and Exploiting Hearst Patterns in Distributional Vectors for Lexical Entailment",
        "authors": [
            "Stephen Roller",
            "Katrin Erk"
        ],
        "abstract": "We consider the task of predicting lexical entailment using distributional vectors. We perform a novel qualitative analysis of one existing model which was previously shown to only measure the prototypicality of word pairs. We find that the model strongly learns to identify hypernyms using Hearst patterns, which are well known to be predictive of lexical relations. We present a novel model which exploits this behavior as a method of feature extraction in an iterative procedure similar to Principal Component Analysis. Our model combines the extracted features with the strengths of other proposed models in the literature, and matches or outperforms prior work on multiple data sets.\n    ",
        "submission_date": "2016-05-18T00:00:00",
        "last_modified_date": "2016-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05448",
        "title": "The Bees Algorithm for the Vehicle Routing Problem",
        "authors": [
            "Aish Fenton"
        ],
        "abstract": "In this thesis we present a new algorithm for the Vehicle Routing Problem called the Enhanced Bees Algorithm. It is adapted from a fairly recent algorithm, the Bees Algorithm, which was developed for continuous optimisation problems. We show that the results obtained by the Enhanced Bees Algorithm are competitive with the best meta-heuristics available for the Vehicle Routing Problem (within 0.5% of the optimal solution for common benchmark problems). We show that the algorithm has good runtime performance, producing results within 2% of the optimal solution within 60 seconds, making it suitable for use within real world dispatch scenarios.\n    ",
        "submission_date": "2016-05-18T00:00:00",
        "last_modified_date": "2016-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05711",
        "title": "The Information-Collecting Vehicle Routing Problem: Stochastic Optimization for Emergency Storm Response",
        "authors": [
            "Lina Al-Kanj",
            "Warren B. Powell",
            "Belgacem Bouzaiene-Ayari"
        ],
        "abstract": "Utilities face the challenge of responding to power outages due to storms and ice damage, but most power grids are not equipped with sensors to pinpoint the precise location of the faults causing the outage. Instead, utilities have to depend primarily on phone calls (trouble calls) from customers who have lost power to guide the dispatching of utility trucks. In this paper, we develop a policy that routes a utility truck to restore outages in the power grid as quickly as possible, using phone calls to create beliefs about outages, but also using utility trucks as a mechanism for collecting additional information. This means that routing decisions change not only the physical state of the truck (as it moves from one location to another) and the grid (as the truck performs repairs), but also our belief about the network, creating the first stochastic vehicle routing problem that explicitly models information collection and belief modeling. We address the problem of managing a single utility truck, which we start by formulating as a sequential stochastic optimization model which captures our belief about the state of the grid. We propose a stochastic lookahead policy, and use Monte Carlo tree search (MCTS) to produce a practical policy that is asymptotically optimal. Simulation results show that the developed policy restores the power grid much faster compared to standard industry heuristics.\n    ",
        "submission_date": "2016-05-18T00:00:00",
        "last_modified_date": "2016-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06069",
        "title": "A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues",
        "authors": [
            "Iulian Vlad Serban",
            "Alessandro Sordoni",
            "Ryan Lowe",
            "Laurent Charlin",
            "Joelle Pineau",
            "Aaron Courville",
            "Yoshua Bengio"
        ],
        "abstract": "Sequential data often possesses a hierarchical structure with complex dependencies between subsequences, such as found between the utterances in a dialogue. In an effort to model this kind of generative process, we propose a neural network-based generative architecture, with latent stochastic variables that span a variable number of time steps. We apply the proposed model to the task of dialogue response generation and compare it with recent neural network architectures. We evaluate the model performance through automatic evaluation metrics and by carrying out a human evaluation. The experiments demonstrate that our model improves upon recently proposed models and that the latent variables facilitate the generation of long outputs and maintain the context.\n    ",
        "submission_date": "2016-05-19T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06181",
        "title": "Variational hybridization and transformation for large inaccurate noisy-or networks",
        "authors": [
            "Yusheng Xie",
            "Nan Du",
            "Wei Fan",
            "Jing Zhai",
            "Weicheng Zhu"
        ],
        "abstract": "Variational inference provides approximations to the computationally intractable posterior distribution in Bayesian networks. A prominent medical application of noisy-or Bayesian network is to infer potential diseases given observed symptoms. Previous studies focus on approximating a handful of complicated pathological cases using variational transformation. Our goal is to use variational transformation as part of a novel hybridized inference for serving reliable and real time diagnosis at web scale. We propose a hybridized inference that allows variational parameters to be estimated without disease posteriors or priors, making the inference faster and much of its computation recyclable. In addition, we propose a transformation ranking algorithm that is very stable to large variances in network prior probabilities, a common issue that arises in medical applications of Bayesian networks. In experiments, we perform comparative study on a large real life medical network and scalability study on a much larger (36,000x) synthesized network.\n    ",
        "submission_date": "2016-05-20T00:00:00",
        "last_modified_date": "2016-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06201",
        "title": "Adversarial Delays in Online Strongly-Convex Optimization",
        "authors": [
            "Daniel Khashabi",
            "Kent Quanrud",
            "Amirhossein Taghvaei"
        ],
        "abstract": "We consider the problem of strongly-convex online optimization in presence of adversarial delays; in a T-iteration online game, the feedback of the player's query at time t is arbitrarily delayed by an adversary for d_t rounds and delivered before the game ends, at iteration t+d_t-1. Specifically for \\algo{online-gradient-descent} algorithm we show it has a simple regret bound of \\Oh{\\sum_{t=1}^T \\log (1+ \\frac{d_t}{t})}. This gives a clear and simple bound without resorting any distributional and limiting assumptions on the delays. We further show how this result encompasses and generalizes several of the existing known results in the literature. Specifically it matches the celebrated logarithmic regret \\Oh{\\log T} when there are no delays (i.e. d_t = 1) and regret bound of \\Oh{\\tau \\log T} for constant delays d_t = \\tau.\n    ",
        "submission_date": "2016-05-20T00:00:00",
        "last_modified_date": "2019-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06319",
        "title": "As Cool as a Cucumber: Towards a Corpus of Contemporary Similes in Serbian",
        "authors": [
            "Nikola Milosevic",
            "Goran Nenadic"
        ],
        "abstract": "Similes are natural language expressions used to compare unlikely things, where the comparison is not taken literally. They are often used in everyday communication and are an important part of cultural heritage. Having an up-to-date corpus of similes is challenging, as they are constantly coined and/or adapted to the contemporary times. In this paper we present a methodology for semi-automated collection of similes from the world wide web using text mining techniques. We expanded an existing corpus of traditional similes (containing 333 similes) by collecting 446 additional expressions. We, also, explore how crowdsourcing can be used to extract and curate new similes.\n    ",
        "submission_date": "2016-05-20T00:00:00",
        "last_modified_date": "2016-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06377",
        "title": "Towards Automation of Knowledge Understanding: An Approach for Probabilistic Generative Classifiers",
        "authors": [
            "Dominik Fisch",
            "Christian Gruhl",
            "Edgar Kalkowski",
            "Bernhard Sick",
            "Seppo J. Ovaska"
        ],
        "abstract": "After data selection, pre-processing, transformation, and feature extraction, knowledge extraction is not the final step in a data mining process. It is then necessary to understand this knowledge in order to apply it efficiently and effectively. Up to now, there is a lack of appropriate techniques that support this significant step. This is partly due to the fact that the assessment of knowledge is often highly subjective, e.g., regarding aspects such as novelty or usefulness. These aspects depend on the specific knowledge and requirements of the data miner. There are, however, a number of aspects that are objective and for which it is possible to provide appropriate measures. In this article we focus on classification problems and use probabilistic generative classifiers based on mixture density models that are quite common in data mining applications. We define objective measures to assess the informativeness, uniqueness, importance, discrimination, representativity, uncertainty, and distinguishability of rules contained in these classifiers numerically. These measures not only support a data miner in evaluating results of a data mining process based on such classifiers. As we will see in illustrative case studies, they may also be used to improve the data mining process itself or to support the later application of the extracted knowledge.\n    ",
        "submission_date": "2016-05-20T00:00:00",
        "last_modified_date": "2016-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06431",
        "title": "Residual Networks Behave Like Ensembles of Relatively Shallow Networks",
        "authors": [
            "Andreas Veit",
            "Michael Wilber",
            "Serge Belongie"
        ],
        "abstract": "In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.\n    ",
        "submission_date": "2016-05-20T00:00:00",
        "last_modified_date": "2016-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06450",
        "title": "Query-Efficient Imitation Learning for End-to-End Autonomous Driving",
        "authors": [
            "Jiakai Zhang",
            "Kyunghyun Cho"
        ],
        "abstract": "One way to approach end-to-end autonomous driving is to learn a policy function that maps from a sensory input, such as an image frame from a front-facing camera, to a driving action, by imitating an expert driver, or a reference policy. This can be done by supervised learning, where a policy function is tuned to minimize the difference between the predicted and ground-truth actions. A policy function trained in this way however is known to suffer from unexpected behaviours due to the mismatch between the states reachable by the reference policy and trained policy functions. More advanced algorithms for imitation learning, such as DAgger, addresses this issue by iteratively collecting training examples from both reference and trained policies. These algorithms often requires a large number of queries to a reference policy, which is undesirable as the reference policy is often expensive. In this paper, we propose an extension of the DAgger, called SafeDAgger, that is query-efficient and more suitable for end-to-end autonomous driving. We evaluate the proposed SafeDAgger in a car racing simulator and show that it indeed requires less queries to a reference policy. We observe a significant speed up in convergence, which we conjecture to be due to the effect of automated curriculum learning.\n    ",
        "submission_date": "2016-05-20T00:00:00",
        "last_modified_date": "2016-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06466",
        "title": "Anomaly Detection in XML-Structured SOAP Messages Using Tree-Based Association Rule Mining",
        "authors": [
            "Reyhaneh Ghassem Esfahani",
            "Mohammad Abadollahi Azgomi",
            "Reza Fathi"
        ],
        "abstract": "Web services are software systems designed for supporting interoperable dynamic cross-enterprise interactions. The result of attacks to Web services can be catastrophic and causing the disclosure of enterprises' confidential data. As new approaches of attacking arise every day, anomaly detection systems seem to be invaluable tools in this context. The aim of this work has been to target the attacks that reside in the Web service layer and the extensible markup language (XML)-structured simple object access protocol (SOAP) messages. After studying the shortcomings of the existing solutions, a new approach for detecting anomalies in Web services is outlined. More specifically, the proposed technique illustrates how to identify anomalies by employing mining methods on XML-structured SOAP messages. This technique also takes the advantages of tree-based association rule mining to extract knowledge in the training phase, which is used in the test phase to detect anomalies. In addition, this novel composition of techniques brings nearly low false alarm rate while maintaining the detection rate reasonably high, which is shown by a case study.\n    ",
        "submission_date": "2016-05-20T00:00:00",
        "last_modified_date": "2016-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06593",
        "title": "Online Influence Maximization under Independent Cascade Model with Semi-Bandit Feedback",
        "authors": [
            "Zheng Wen",
            "Branislav Kveton",
            "Michal Valko",
            "Sharan Vaswani"
        ],
        "abstract": "We study the online influence maximization problem in social networks under the independent cascade model. Specifically, we aim to learn the set of \"best influencers\" in a social network online while repeatedly interacting with it. We address the challenges of (i) combinatorial action space, since the number of feasible influencer sets grows exponentially with the maximum number of influencers, and (ii) limited feedback, since only the influenced portion of the network is observed. Under a stochastic semi-bandit feedback, we propose and analyze IMLinUCB, a computationally efficient UCB-based algorithm. Our bounds on the cumulative regret are polynomial in all quantities of interest, achieve near-optimal dependence on the number of interactions and reflect the topology of the network and the activation probabilities of its edges, thereby giving insights on the problem complexity. To the best of our knowledge, these are the first such results. Our experiments show that in several representative graph topologies, the regret of IMLinUCB scales as suggested by our upper bounds. IMLinUCB permits linear generalization and thus is both statistically and computationally suitable for large-scale problems. Our experiments also show that IMLinUCB with linear generalization can lead to low regret in real-world online influence maximization.\n    ",
        "submission_date": "2016-05-21T00:00:00",
        "last_modified_date": "2018-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06640",
        "title": "Programming with a Differentiable Forth Interpreter",
        "authors": [
            "Matko Bo\u0161njak",
            "Tim Rockt\u00e4schel",
            "Jason Naradowsky",
            "Sebastian Riedel"
        ],
        "abstract": "Given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. In this paper, we consider the case of prior procedural knowledge for neural networks, such as knowing how a program should traverse a sequence, but not what local actions should be performed at each step. To this end, we present an end-to-end differentiable interpreter for the programming language Forth which enables programmers to write program sketches with slots that can be filled with behaviour trained from program input-output data. We can optimise this behaviour directly through gradient descent techniques on user-specified objectives, and also integrate the program into any larger neural computation graph. We show empirically that our interpreter is able to effectively leverage different levels of prior program structure and learn complex behaviours such as sequence sorting and addition. When connected to outputs of an LSTM and trained jointly, our interpreter achieves state-of-the-art accuracy for end-to-end reasoning about quantities expressed in natural language stories.\n    ",
        "submission_date": "2016-05-21T00:00:00",
        "last_modified_date": "2017-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06838",
        "title": "Causality on Longitudinal Data: Stable Specification Search in Constrained Structural Equation Modeling",
        "authors": [
            "Ridho Rahmadi",
            "Perry Groot",
            "Marieke HC van Rijn",
            "Jan AJG van den Brand",
            "Marianne Heins",
            "Hans Knoop",
            "Tom Heskes"
        ],
        "abstract": "A typical problem in causal modeling is the instability of model structure learning, i.e., small changes in finite data can result in completely different optimal models. The present work introduces a novel causal modeling algorithm for longitudinal data, that is robust for finite samples based on recent advances in stability selection using subsampling and selection algorithms. Our approach uses exploratory search but allows incorporation of prior knowledge, e.g., the absence of a particular causal relationship between two specific variables. We represent causal relationships using structural equation models. Models are scored along two objectives: the model fit and the model complexity. Since both objectives are often conflicting we apply a multi-objective evolutionary algorithm to search for Pareto optimal models. To handle the instability of small finite data samples, we repeatedly subsample the data and select those substructures (from the optimal models) that are both stable and parsimonious. These substructures can be visualized through a causal graph. Our more exploratory approach achieves at least comparable performance as, but often a significant improvement over state-of-the-art alternative approaches on a simulated data set with a known ground truth. We also present the results of our method on three real-world longitudinal data sets on chronic fatigue syndrome, Alzheimer disease, and chronic kidney disease. The findings obtained with our approach are generally in line with results from more hypothesis-driven analyses in earlier studies and suggest some novel relationships that deserve further research.\n    ",
        "submission_date": "2016-05-22T00:00:00",
        "last_modified_date": "2017-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06995",
        "title": "DP-EM: Differentially Private Expectation Maximization",
        "authors": [
            "Mijung Park",
            "Jimmy Foulds",
            "Kamalika Chaudhuri",
            "Max Welling"
        ],
        "abstract": "The iterative nature of the expectation maximization (EM) algorithm presents a challenge for privacy-preserving estimation, as each iteration increases the amount of noise needed. We propose a practical private EM algorithm that overcomes this challenge using two innovations: (1) a novel moment perturbation formulation for differentially private EM (DP-EM), and (2) the use of two recently developed composition methods to bound the privacy \"cost\" of multiple EM iterations: the moments accountant (MA) and zero-mean concentrated differential privacy (zCDP). Both MA and zCDP bound the moment generating function of the privacy loss random variable and achieve a refined tail bound, which effectively decrease the amount of additive noise. We present empirical results showing the benefits of our approach, as well as similar performance between these two composition methods in the DP-EM setting for Gaussian mixture models. Our approach can be readily extended to many iterative learning algorithms, opening up various exciting future directions.\n    ",
        "submission_date": "2016-05-23T00:00:00",
        "last_modified_date": "2016-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06996",
        "title": "Extracting Higher-Order Goals from the Mizar Mathematical Library",
        "authors": [
            "Chad Brown",
            "Josef Urban"
        ],
        "abstract": "Certain constructs allowed in Mizar articles cannot be represented in first-order logic but can be represented in higher-order logic. We describe a way to obtain higher-order theorem proving problems from Mizar articles that make use of these constructs. In particular, higher-order logic is used to represent schemes, a global choice construct and set level binders. The higher-order automated theorem provers Satallax and LEO-II have been run on collections of these problems and the results are discussed.\n    ",
        "submission_date": "2016-05-23T00:00:00",
        "last_modified_date": "2016-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07026",
        "title": "Spontaneous vs. Posed smiles - can we tell the difference?",
        "authors": [
            "Bappaditya Mandal",
            "Nizar Ouarti"
        ],
        "abstract": "Smile is an irrefutable expression that shows the physical state of the mind in both true and deceptive ways. Generally, it shows happy state of the mind, however, `smiles' can be deceptive, for example people can give a smile when they feel happy and sometimes they might also give a smile (in a different way) when they feel pity for others. This work aims to distinguish spontaneous (felt) smile expressions from posed (deliberate) smiles by extracting and analyzing both global (macro) motion of the face and subtle (micro) changes in the facial expression features through both tracking a series of facial fiducial markers as well as using dense optical flow. Specifically the eyes and lips features are captured and used for analysis. It aims to automatically classify all smiles into either `spontaneous' or `posed' categories, by using support vector machines (SVM). Experimental results on large database show promising results as compared to other relevant methods.\n    ",
        "submission_date": "2016-05-23T00:00:00",
        "last_modified_date": "2016-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07079",
        "title": "Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets",
        "authors": [
            "Aaron Klein",
            "Stefan Falkner",
            "Simon Bartels",
            "Philipp Hennig",
            "Frank Hutter"
        ],
        "abstract": "Bayesian optimization has become a successful tool for hyperparameter optimization of machine learning algorithms, such as support vector machines or deep neural networks. Despite its success, for large datasets, training and validating a single configuration often takes hours, days, or even weeks, which limits the achievable performance. To accelerate hyperparameter optimization, we propose a generative model for the validation error as a function of training set size, which is learned during the optimization process and allows exploration of preliminary configurations on small subsets, by extrapolating to the full dataset. We construct a Bayesian optimization procedure, dubbed Fabolas, which models loss and training time as a function of dataset size and automatically trades off high information gain about the global optimum against computational cost. Experiments optimizing support vector machines and deep neural networks show that Fabolas often finds high-quality solutions 10 to 100 times faster than other state-of-the-art Bayesian optimization methods or the recently proposed bandit strategy Hyperband.\n    ",
        "submission_date": "2016-05-23T00:00:00",
        "last_modified_date": "2017-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07148",
        "title": "Backprop KF: Learning Discriminative Deterministic State Estimators",
        "authors": [
            "Tuomas Haarnoja",
            "Anurag Ajay",
            "Sergey Levine",
            "Pieter Abbeel"
        ],
        "abstract": "Generative state estimators based on probabilistic filters and smoothers are one of the most popular classes of state estimators for robots and autonomous vehicles. However, generative models have limited capacity to handle rich sensory observations, such as camera images, since they must model the entire distribution over sensor readings. Discriminative models do not suffer from this limitation, but are typically more complex to train as latent variable models for state estimation. We present an alternative approach where the parameters of the latent state distribution are directly optimized as a deterministic computation graph, resulting in a simple and effective gradient descent algorithm for training discriminative state estimators. We show that this procedure can be used to train state estimators that use complex input, such as raw camera images, which must be processed using expressive nonlinear function approximators such as convolutional neural networks. Our model can be viewed as a type of recurrent neural network, and the connection to probabilistic filtering allows us to design a network architecture that is particularly well suited for state estimation. We evaluate our approach on synthetic tracking task with raw image inputs and on the visual odometry task in the KITTI dataset. The results show significant improvement over both standard generative approaches and regular recurrent neural networks.\n    ",
        "submission_date": "2016-05-23T00:00:00",
        "last_modified_date": "2017-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07156",
        "title": "Genetic Architect: Discovering Genomic Structure with Learned Neural Architectures",
        "authors": [
            "Laura Deming",
            "Sasha Targ",
            "Nate Sauder",
            "Diogo Almeida",
            "Chun Jimmie Ye"
        ],
        "abstract": "Each human genome is a 3 billion base pair set of encoding instructions. Decoding the genome using deep learning fundamentally differs from most tasks, as we do not know the full structure of the data and therefore cannot design architectures to suit it. As such, architectures that fit the structure of genomics should be learned not prescribed. Here, we develop a novel search algorithm, applicable across domains, that discovers an optimal architecture which simultaneously learns general genomic patterns and identifies the most important sequence motifs in predicting functional genomic outcomes. The architectures we find using this algorithm succeed at using only RNA expression data to predict gene regulatory structure, learn human-interpretable visualizations of key sequence motifs, and surpass state-of-the-art results on benchmark genomics challenges.\n    ",
        "submission_date": "2016-05-23T00:00:00",
        "last_modified_date": "2016-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07157",
        "title": "Unsupervised Learning for Physical Interaction through Video Prediction",
        "authors": [
            "Chelsea Finn",
            "Ian Goodfellow",
            "Sergey Levine"
        ],
        "abstract": "A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 59,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot's future actions amounts to learning a \"visual imagination\" of different futures based on different courses of action. Our experiments show that our proposed method produces more accurate video predictions both quantitatively and qualitatively, when compared to prior methods.\n    ",
        "submission_date": "2016-05-23T00:00:00",
        "last_modified_date": "2016-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07246",
        "title": "Adaptive ADMM with Spectral Penalty Parameter Selection",
        "authors": [
            "Zheng Xu",
            "Mario A. T. Figueiredo",
            "Tom Goldstein"
        ],
        "abstract": "The alternating direction method of multipliers (ADMM) is a versatile tool for solving a wide range of constrained optimization problems, with differentiable or non-differentiable objective functions. Unfortunately, its performance is highly sensitive to a penalty parameter, which makes ADMM often unreliable and hard to automate for a non-expert user. We tackle this weakness of ADMM by proposing a method to adaptively tune the penalty parameters to achieve fast convergence. The resulting adaptive ADMM (AADMM) algorithm, inspired by the successful Barzilai-Borwein spectral method for gradient descent, yields fast convergence and relative insensitivity to the initial stepsize and problem scaling.\n    ",
        "submission_date": "2016-05-24T00:00:00",
        "last_modified_date": "2017-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07334",
        "title": "Near-optimal Bayesian Active Learning with Correlated and Noisy Tests",
        "authors": [
            "Yuxin Chen",
            "S. Hamed Hassani",
            "Andreas Krause"
        ],
        "abstract": "We consider the Bayesian active learning and experimental design problem, where the goal is to learn the value of some unknown target variable through a sequence of informative, noisy tests. In contrast to prior work, we focus on the challenging, yet practically relevant setting where test outcomes can be conditionally dependent given the hidden target variable. Under such assumptions, common heuristics, such as greedily performing tests that maximize the reduction in uncertainty of the target, often perform poorly. In this paper, we propose ECED, a novel, computationally efficient active learning algorithm, and prove strong theoretical guarantees that hold with correlated, noisy tests. Rather than directly optimizing the prediction error, at each step, ECED picks the test that maximizes the gain in a surrogate objective, which takes into account the dependencies between tests. Our analysis relies on an information-theoretic auxiliary function to track the progress of ECED, and utilizes adaptive submodularity to attain the near-optimal bound. We demonstrate strong empirical performance of ECED on two problem instances, including a Bayesian experimental design task intended to distinguish among economic theories of how people make risky decisions, and an active preference learning task via pairwise comparisons.\n    ",
        "submission_date": "2016-05-24T00:00:00",
        "last_modified_date": "2016-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07496",
        "title": "Alternating Optimisation and Quadrature for Robust Control",
        "authors": [
            "Supratik Paul",
            "Konstantinos Chatzilygeroudis",
            "Kamil Ciosek",
            "Jean-Baptiste Mouret",
            "Michael A. Osborne",
            "Shimon Whiteson"
        ],
        "abstract": "Bayesian optimisation has been successfully applied to a variety of reinforcement learning problems. However, the traditional approach for learning optimal policies in simulators does not utilise the opportunity to improve learning by adjusting certain environment variables: state features that are unobservable and randomly determined by the environment in a physical setting but are controllable in a simulator. This paper considers the problem of finding a robust policy while taking into account the impact of environment variables. We present Alternating Optimisation and Quadrature (ALOQ), which uses Bayesian optimisation and Bayesian quadrature to address such settings. ALOQ is robust to the presence of significant rare events, which may not be observable under random sampling, but play a substantial role in determining the optimal policy. Experimental results across different domains show that ALOQ can learn more efficiently and robustly than existing methods.\n    ",
        "submission_date": "2016-05-24T00:00:00",
        "last_modified_date": "2017-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07511",
        "title": "A note on privacy preserving iteratively reweighted least squares",
        "authors": [
            "Mijung Park",
            "Max Welling"
        ],
        "abstract": "Iteratively reweighted least squares (IRLS) is a widely-used method in machine learning to estimate the parameters in the generalised linear models. In particular, IRLS for L1 minimisation under the linear model provides a closed-form solution in each step, which is a simple multiplication between the inverse of the weighted second moment matrix and the weighted first moment vector. When dealing with privacy sensitive data, however, developing a privacy preserving IRLS algorithm faces two challenges. First, due to the inversion of the second moment matrix, the usual sensitivity analysis in differential privacy incorporating a single datapoint perturbation gets complicated and often requires unrealistic assumptions. Second, due to its iterative nature, a significant cumulative privacy loss occurs. However, adding a high level of noise to compensate for the privacy loss hinders from getting accurate estimates. Here, we develop a practical algorithm that overcomes these challenges and outputs privatised and accurate IRLS solutions. In our method, we analyse the sensitivity of each moments separately and treat the matrix inversion and multiplication as a post-processing step, which simplifies the sensitivity analysis. Furthermore, we apply the {\\it{concentrated differential privacy}} formalism, a more relaxed version of differential privacy, which requires adding a significantly less amount of noise for the same level of privacy guarantee, compared to the conventional and advanced compositions of differentially private mechanisms.\n    ",
        "submission_date": "2016-05-24T00:00:00",
        "last_modified_date": "2016-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07604",
        "title": "Posterior Dispersion Indices",
        "authors": [
            "Alp Kucukelbir",
            "David M. Blei"
        ],
        "abstract": "Probabilistic modeling is cyclical: we specify a model, infer its posterior, and evaluate its performance. Evaluation drives the cycle, as we revise our model based on how it performs. This requires a metric. Traditionally, predictive accuracy prevails. Yet, predictive accuracy does not tell the whole story. We propose to evaluate a model through posterior dispersion. The idea is to analyze how each datapoint fares in relation to posterior uncertainty around the hidden structure. We propose a family of posterior dispersion indices (PDI) that capture this idea. A PDI identifies rich patterns of model mismatch in three real data examples: voting preferences, supermarket shopping, and population genetics.\n    ",
        "submission_date": "2016-05-24T00:00:00",
        "last_modified_date": "2016-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07700",
        "title": "Learning Purposeful Behaviour in the Absence of Rewards",
        "authors": [
            "Marlos C. Machado",
            "Michael Bowling"
        ],
        "abstract": "Artificial intelligence is commonly defined as the ability to achieve goals in the world. In the reinforcement learning framework, goals are encoded as reward functions that guide agent behaviour, and the sum of observed rewards provide a notion of progress. However, some domains have no such reward signal, or have a reward signal so sparse as to appear absent. Without reward feedback, agent behaviour is typically random, often dithering aimlessly and lacking intentionality. In this paper we present an algorithm capable of learning purposeful behaviour in the absence of rewards. The algorithm proceeds by constructing temporally extended actions (options), through the identification of purposes that are \"just out of reach\" of the agent's current behaviour. These purposes establish intrinsic goals for the agent to learn, ultimately resulting in a suite of behaviours that encourage the agent to visit different parts of the state space. Moreover, the approach is particularly suited for settings where rewards are very sparse, and such behaviours can help in the exploration of the environment until reward is observed.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07722",
        "title": "Yum-me: A Personalized Nutrient-based Meal Recommender System",
        "authors": [
            "Longqi Yang",
            "Cheng-Kang Hsieh",
            "Hongjian Yang",
            "Nicola Dell",
            "Serge Belongie",
            "Curtis Cole",
            "Deborah Estrin"
        ],
        "abstract": "Nutrient-based meal recommendations have the potential to help individuals prevent or manage conditions such as diabetes and obesity. However, learning people's food preferences and making recommendations that simultaneously appeal to their palate and satisfy nutritional expectations are challenging. Existing approaches either only learn high-level preferences or require a prolonged learning period. We propose Yum-me, a personalized nutrient-based meal recommender system designed to meet individuals' nutritional expectations, dietary restrictions, and fine-grained food preferences. Yum-me enables a simple and accurate food preference profiling procedure via a visual quiz-based user interface, and projects the learned profile into the domain of nutritionally appropriate food options to find ones that will appeal to the user. We present the design and implementation of Yum-me, and further describe and evaluate two innovative contributions. The first contriution is an open source state-of-the-art food image analysis model, named FoodDist. We demonstrate FoodDist's superior performance through careful benchmarking and discuss its applicability across a wide array of dietary applications. The second contribution is a novel online learning framework that learns food preference from item-wise and pairwise image comparisons. We evaluate the framework in a field study of 227 anonymous users and demonstrate that it outperforms other baselines by a significant margin. We further conducted an end-to-end validation of the feasibility and effectiveness of Yum-me through a 60-person user study, in which Yum-me improves the recommendation acceptance rate by 42.63%.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2017-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07723",
        "title": "Data Programming: Creating Large Training Sets, Quickly",
        "authors": [
            "Alexander Ratner",
            "Christopher De Sa",
            "Sen Wu",
            "Daniel Selsam",
            "Christopher R\u00e9"
        ],
        "abstract": "Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of training sets called data programming in which users express weak supervision strategies or domain heuristics as labeling functions, which are programs that label subsets of the data, but that are noisy and may conflict. We show that by explicitly representing this training set labeling process as a generative model, we can \"denoise\" the generated training set, and establish theoretically that we can recover the parameters of these generative models in a handful of settings. We then show how to modify a discriminative loss function to make it noise-aware, and demonstrate our method over a range of discriminative models including logistic regression and LSTMs. Experimentally, on the 2014 TAC-KBP Slot Filling challenge, we show that data programming would have led to a new winning score, and also show that applying data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points over a state-of-the-art LSTM baseline (and into second place in the competition). Additionally, in initial user studies we observed that data programming may be an easier way for non-experts to create machine learning models when training data is limited or unavailable.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2017-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07736",
        "title": "Learning Multiagent Communication with Backpropagation",
        "authors": [
            "Sainbayar Sukhbaatar",
            "Arthur Szlam",
            "Rob Fergus"
        ],
        "abstract": "Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNet, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07844",
        "title": "Dimension Projection among Languages based on Pseudo-relevant Documents for Query Translation",
        "authors": [
            "Javid Dadashkarimi",
            "Mahsa S. Shahshahani",
            "Amirhossein Tebbifakhr",
            "Heshaam Faili",
            "Azadeh Shakery"
        ],
        "abstract": "Using top-ranked documents in response to a query has been shown to be an effective approach to improve the quality of query translation in dictionary-based cross-language information retrieval. In this paper, we propose a new method for dictionary-based query translation based on dimension projection of embedded vectors from the pseudo-relevant documents in the source language to their equivalents in the target language. To this end, first we learn low-dimensional vectors of the words in the pseudo-relevant collections separately and then aim to find a query-dependent transformation matrix between the vectors of translation pairs appeared in the collections. At the next step, representation of each query term is projected to the target language and then, after using a softmax function, a query-dependent translation model is built. Finally, the model is used for query translation. Our experiments on four CLEF collections in French, Spanish, German, and Italian demonstrate that the proposed method outperforms a word embedding baseline based on bilingual shuffling and a further number of competitive baselines. The proposed method reaches up to 87% performance of machine translation (MT) in short queries and considerable improvements in verbose queries.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07918",
        "title": "Automatic Open Knowledge Acquisition via Long Short-Term Memory Networks with Feedback Negative Sampling",
        "authors": [
            "Byungsoo Kim",
            "Hwanjo Yu",
            "Gary Geunbae Lee"
        ],
        "abstract": "Previous studies in Open Information Extraction (Open IE) are mainly based on extraction patterns. They manually define patterns or automatically learn them from a large corpus. However, these approaches are limited when grasping the context of a sentence, and they fail to capture implicit relations. In this paper, we address this problem with the following methods. First, we exploit long short-term memory (LSTM) networks to extract higher-level features along the shortest dependency paths, connecting headwords of relations and arguments. The path-level features from LSTM networks provide useful clues regarding contextual information and the validity of arguments. Second, we constructed samples to train LSTM networks without the need for manual labeling. In particular, feedback negative sampling picks highly negative samples among non-positive samples through a model trained with positive samples. The experimental results show that our approach produces more precise and abundant extractions than state-of-the-art open IE systems. To the best of our knowledge, this is the first work to apply deep learning to Open IE.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07999",
        "title": "Toward a general, scaleable framework for Bayesian teaching with applications to topic models",
        "authors": [
            "Baxter S. Eaves Jr",
            "Patrick Shafto"
        ],
        "abstract": "Machines, not humans, are the world's dominant knowledge accumulators but humans remain the dominant decision makers. Interpreting and disseminating the knowledge accumulated by machines requires expertise, time, and is prone to failure. The problem of how best to convey accumulated knowledge from computers to humans is a critical bottleneck in the broader application of machine learning. We propose an approach based on human teaching where the problem is formalized as selecting a small subset of the data that will, with high probability, lead the human user to the correct inference. This approach, though successful for modeling human learning in simple laboratory experiments, has failed to achieve broader relevance due to challenges in formulating general and scalable algorithms. We propose general-purpose teaching via pseudo-marginal sampling and demonstrate the algorithm by teaching topic models. Simulation results show our sampling-based approach: effectively approximates the probability where ground-truth is possible via enumeration, results in data that are markedly different from those expected by random sampling, and speeds learning especially for small amounts of data. Application to movie synopsis data illustrates differences between teaching and random sampling for teaching distributions and specific topics, and demonstrates gains in scalability and applicability to real-world problems.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.08062",
        "title": "A PAC RL Algorithm for Episodic POMDPs",
        "authors": [
            "Zhaohan Daniel Guo",
            "Shayan Doroudi",
            "Emma Brunskill"
        ],
        "abstract": "Many interesting real world domains involve reinforcement learning (RL) in partially observable environments. Efficient learning in such domains is important, but existing sample complexity bounds for partially observable RL are at least exponential in the episode length. We give, to our knowledge, the first partially observable RL algorithm with a polynomial bound on the number of episodes on which the algorithm may not achieve near-optimal performance. Our algorithm is suitable for an important class of episodic POMDPs. Our approach builds on recent advances in method of moments for latent variable model estimation.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.08104",
        "title": "Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning",
        "authors": [
            "William Lotter",
            "Gabriel Kreiman",
            "David Cox"
        ],
        "abstract": "While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (\"PredNet\") architecture that is inspired by the concept of \"predictive coding\" from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2017-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.08374",
        "title": "Kronecker Determinantal Point Processes",
        "authors": [
            "Zelda Mariet",
            "Suvrit Sra"
        ],
        "abstract": "Determinantal Point Processes (DPPs) are probabilistic models over all subsets a ground set of $N$ items. They have recently gained prominence in several applications that rely on \"diverse\" subsets. However, their applicability to large problems is still limited due to the $\\mathcal O(N^3)$ complexity of core tasks such as sampling and learning. We enable efficient sampling and learning for DPPs by introducing KronDPP, a DPP model whose kernel matrix decomposes as a tensor product of multiple smaller kernel matrices. This decomposition immediately enables fast exact sampling. But contrary to what one may expect, leveraging the Kronecker product structure for speeding up DPP learning turns out to be more difficult. We overcome this challenge, and derive batch and stochastic optimization algorithms for efficiently learning the parameters of a KronDPP.\n    ",
        "submission_date": "2016-05-26T00:00:00",
        "last_modified_date": "2016-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.08412",
        "title": "CITlab ARGUS for historical handwritten documents",
        "authors": [
            "Gundram Leifert",
            "Tobias Strau\u00df",
            "Tobias Gr\u00fcning",
            "Roger Labahn"
        ],
        "abstract": "We describe CITlab's recognition system for the HTRtS competition attached to the 13. International Conference on Document Analysis and Recognition, ICDAR 2015. The task comprises the recognition of historical handwritten documents. The core algorithms of our system are based on multi-dimensional recurrent neural networks (MDRNN) and connectionist temporal classification (CTC). The software modules behind that as well as the basic utility technologies are essentially powered by PLANET's ARGUS framework for intelligent text recognition and image processing.\n    ",
        "submission_date": "2016-05-26T00:00:00",
        "last_modified_date": "2016-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.08464",
        "title": "Low-Cost Scene Modeling using a Density Function Improves Segmentation Performance",
        "authors": [
            "Vivek Sharma",
            "Sule Yildirim-Yayilgan",
            "Luc Van Gool"
        ],
        "abstract": "We propose a low cost and effective way to combine a free simulation software and free CAD models for modeling human-object interaction in order to improve human & object segmentation. It is intended for research scenarios related to safe human-robot collaboration (SHRC) and interaction (SHRI) in the industrial domain. The task of human and object modeling has been used for detecting activity, and for inferring and predicting actions, different from those works, we do human and object modeling in order to learn interactions in RGB-D data for improving segmentation. For this purpose, we define a novel density function to model a three dimensional (3D) scene in a virtual environment (VREP). This density function takes into account various possible configurations of human-object and object-object relationships and interactions governed by their affordances. Using this function, we synthesize a large, realistic and highly varied synthetic RGB-D dataset that we use for training. We train a random forest classifier, and the pixelwise predictions obtained is integrated as a unary term in a pairwise conditional random fields (CRF). Our evaluation shows that modeling these interactions improves segmentation performance by ~7\\% in mean average precision and recall over state-of-the-art methods that ignore these interactions in real-world data. Our approach is computationally efficient, robust and can run real-time on consumer hardware.\n    ",
        "submission_date": "2016-05-26T00:00:00",
        "last_modified_date": "2016-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.08478",
        "title": "Model-Free Imitation Learning with Policy Optimization",
        "authors": [
            "Jonathan Ho",
            "Jayesh K. Gupta",
            "Stefano Ermon"
        ],
        "abstract": "In imitation learning, an agent learns how to behave in an environment with an unknown cost function by mimicking expert demonstrations. Existing imitation learning algorithms typically involve solving a sequence of planning or reinforcement learning problems. Such algorithms are therefore not directly applicable to large, high-dimensional environments, and their performance can significantly degrade if the planning problems are not solved to optimality. Under the apprenticeship learning formalism, we develop alternative model-free algorithms for finding a parameterized stochastic policy that performs at least as well as an expert policy on an unknown cost function, based on sample trajectories from the expert. Our approach, based on policy gradients, scales to large continuous environments with guaranteed convergence to local minima.\n    ",
        "submission_date": "2016-05-26T00:00:00",
        "last_modified_date": "2016-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.08695",
        "title": "TensorFlow: A system for large-scale machine learning",
        "authors": [
            "Mart\u00edn Abadi",
            "Paul Barham",
            "Jianmin Chen",
            "Zhifeng Chen",
            "Andy Davis",
            "Jeffrey Dean",
            "Matthieu Devin",
            "Sanjay Ghemawat",
            "Geoffrey Irving",
            "Michael Isard",
            "Manjunath Kudlur",
            "Josh Levenberg",
            "Rajat Monga",
            "Sherry Moore",
            "Derek G. Murray",
            "Benoit Steiner",
            "Paul Tucker",
            "Vijay Vasudevan",
            "Pete Warden",
            "Martin Wicke",
            "Yuan Yu",
            "Xiaoqiang Zheng"
        ],
        "abstract": "TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous \"parameter server\" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.\n    ",
        "submission_date": "2016-05-27T00:00:00",
        "last_modified_date": "2016-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.08803",
        "title": "Density estimation using Real NVP",
        "authors": [
            "Laurent Dinh",
            "Jascha Sohl-Dickstein",
            "Samy Bengio"
        ],
        "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.\n    ",
        "submission_date": "2016-05-27T00:00:00",
        "last_modified_date": "2017-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.08878",
        "title": "Computational Estimate Visualisation and Evaluation of Agent Classified Rules Learning System",
        "authors": [
            "Kennedy E. Ehimwenma",
            "Martin Beer",
            "Paul Crowther"
        ],
        "abstract": "Student modelling and agent classified rules learning as applied in the development of the intelligent Preassessment System has been presented in [10],[11]. In this paper, we now demystify the theory behind the development of the pre-assessment system followed by some computational experimentation and graph visualisation of the agent classified rules learning algorithm in the estimation and prediction of classified rules. In addition, we present some preliminary results of the pre-assessment system evaluation. From the results, it is gathered that the system has performed according to its design specification.\n    ",
        "submission_date": "2016-05-28T00:00:00",
        "last_modified_date": "2016-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.09042",
        "title": "MCMC assisted by Belief Propagation",
        "authors": [
            "Sungsoo Ahn",
            "Michael Chertkov",
            "Jinwoo Shin"
        ],
        "abstract": "Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most popular algorithms for computational inference in Graphical Models (GM). In principle, MCMC is an exact probabilistic method which, however, often suffers from exponentially slow mixing. In contrast, BP is a deterministic method, which is typically fast, empirically very successful, however in general lacking control of accuracy over loopy graphs. In this paper, we introduce MCMC algorithms correcting the approximation error of BP, i.e., we provide a way to compensate for BP errors via a consecutive BP-aware MCMC. Our framework is based on the Loop Calculus (LC) approach which allows expressing the BP error as a sum of weighted generalized loops. Although the full series is computationally intractable, it is known that a truncated series, summing up all 2-regular loops, is computable in polynomial-time for planar pair-wise binary GMs and it also provides a highly accurate approximation empirically. Motivated by this, we first propose a polynomial-time approximation MCMC scheme for the truncated series of general (non-planar) pair-wise binary models. Our main idea here is to use the Worm algorithm, known to provide fast mixing in other (related) problems, and then design an appropriate rejection scheme to sample 2-regular loops. Furthermore, we also design an efficient rejection-free MCMC scheme for approximating the full series. The main novelty underlying our design is in utilizing the concept of cycle basis, which provides an efficient decomposition of the generalized loops. In essence, the proposed MCMC schemes run on transformed GM built upon the non-trivial BP solution, and our experiments show that this synthesis of BP and MCMC outperforms both direct MCMC and bare BP schemes.\n    ",
        "submission_date": "2016-05-29T00:00:00",
        "last_modified_date": "2020-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.09293",
        "title": "Internal Guidance for Satallax",
        "authors": [
            "Michael F\u00e4rber",
            "Chad Brown"
        ],
        "abstract": "We propose a new internal guidance method for automated theorem provers based on the given-clause algorithm. Our method influences the choice of unprocessed clauses using positive and negative examples from previous proofs. To this end, we present an efficient scheme for Naive Bayesian classification by generalising label occurrences to types with monoid structure. This makes it possible to extend existing fast classifiers, which consider only positive examples, with negative ones. We implement the method in the higher-order logic prover Satallax, where we modify the delay with which propositions are processed. We evaluated our method on a simply-typed higher-order logic version of the Flyspeck project, where it solves 26% more problems than Satallax without internal guidance.\n    ",
        "submission_date": "2016-05-30T00:00:00",
        "last_modified_date": "2016-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.09304",
        "title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks",
        "authors": [
            "Anh Nguyen",
            "Alexey Dosovitskiy",
            "Jason Yosinski",
            "Thomas Brox",
            "Jeff Clune"
        ],
        "abstract": "Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right - similar to why we study the human brain - and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).\n    ",
        "submission_date": "2016-05-30T00:00:00",
        "last_modified_date": "2016-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.09370",
        "title": "Unsupervised Discovery of El Nino Using Causal Feature Learning on Microlevel Climate Data",
        "authors": [
            "Krzysztof Chalupka",
            "Tobias Bischoff",
            "Pietro Perona",
            "Frederick Eberhardt"
        ],
        "abstract": "We show that the climate phenomena of El Nino and La Nina arise naturally as states of macro-variables when our recent causal feature learning framework (Chalupka 2015, Chalupka 2016) is applied to micro-level measures of zonal wind (ZW) and sea surface temperatures (SST) taken over the equatorial band of the Pacific Ocean. The method identifies these unusual climate states on the basis of the relation between ZW and SST patterns without any input about past occurrences of El Nino or La Nina. The simpler alternatives of (i) clustering the SST fields while disregarding their relationship with ZW patterns, or (ii) clustering the joint ZW-SST patterns, do not discover El Nino. We discuss the degree to which our method supports a causal interpretation and use a low-dimensional toy example to explain its success over other clustering approaches. Finally, we propose a new robust and scalable alternative to our original algorithm (Chalupka 2016), which circumvents the need for high-dimensional density learning.\n    ",
        "submission_date": "2016-05-30T00:00:00",
        "last_modified_date": "2016-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.09497",
        "title": "Interdependent Scheduling Games",
        "authors": [
            "Andres Abeliuk",
            "Haris Aziz",
            "Gerardo Berbeglia",
            "Serge Gaspers",
            "Petr Kalina",
            "Nicholas Mattei",
            "Dominik Peters",
            "Paul Stursberg",
            "Pascal Van Hentenryck",
            "Toby Walsh"
        ],
        "abstract": "We propose a model of interdependent scheduling games in which each player controls a set of services that they schedule independently. A player is free to schedule his own services at any time; however, each of these services only begins to accrue reward for the player when all predecessor services, which may or may not be controlled by the same player, have been activated. This model, where players have interdependent services, is motivated by the problems faced in planning and coordinating large-scale infrastructures, e.g., restoring electricity and gas to residents after a natural disaster or providing medical care in a crisis when different agencies are responsible for the delivery of staff, equipment, and medicine. We undertake a game-theoretic analysis of this setting and in particular consider the issues of welfare maximization, computing best responses, Nash dynamics, and existence and computation of Nash equilibria.\n    ",
        "submission_date": "2016-05-31T00:00:00",
        "last_modified_date": "2016-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.09564",
        "title": "Determining the Characteristic Vocabulary for a Specialized Dictionary using Word2vec and a Directed Crawler",
        "authors": [
            "Gregory Grefenstette",
            "Lawrence Muchemi"
        ],
        "abstract": "Specialized dictionaries are used to understand concepts in specific domains, especially where those concepts are not part of the general vocabulary, or having meanings that differ from ordinary languages. The first step in creating a specialized dictionary involves detecting the characteristic vocabulary of the domain in question. Classical methods for detecting this vocabulary involve gathering a domain corpus, calculating statistics on the terms found there, and then comparing these statistics to a background or general language corpus. Terms which are found significantly more often in the specialized corpus than in the background corpus are candidates for the characteristic vocabulary of the domain. Here we present two tools, a directed crawler, and a distributional semantics package, that can be used together, circumventing the need of a background corpus. Both tools are available on the web.\n    ",
        "submission_date": "2016-05-31T00:00:00",
        "last_modified_date": "2016-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.09593",
        "title": "Adaptive Learning Rate via Covariance Matrix Based Preconditioning for Deep Neural Networks",
        "authors": [
            "Yasutoshi Ida",
            "Yasuhiro Fujiwara",
            "Sotetsu Iwamura"
        ],
        "abstract": "Adaptive learning rate algorithms such as RMSProp are widely used for training deep neural networks. RMSProp offers efficient training since it uses first order gradients to approximate Hessian-based preconditioning. However, since the first order gradients include noise caused by stochastic optimization, the approximation may be inaccurate. In this paper, we propose a novel adaptive learning rate algorithm called SDProp. Its key idea is effective handling of the noise by preconditioning based on covariance matrix. For various neural networks, our approach is more efficient and effective than RMSProp and its variant.\n    ",
        "submission_date": "2016-05-31T00:00:00",
        "last_modified_date": "2017-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.09674",
        "title": "VIME: Variational Information Maximizing Exploration",
        "authors": [
            "Rein Houthooft",
            "Xi Chen",
            "Yan Duan",
            "John Schulman",
            "Filip De Turck",
            "Pieter Abbeel"
        ],
        "abstract": "Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.\n    ",
        "submission_date": "2016-05-31T00:00:00",
        "last_modified_date": "2017-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.09757",
        "title": "Towards ontology driven learning of visual concept detectors",
        "authors": [
            "Sanchit Arora",
            "Chuck Cho",
            "Paul Fitzpatrick",
            "Francois Scharffe"
        ],
        "abstract": "The maturity of deep learning techniques has led in recent years to a breakthrough in object recognition in visual media. While for some specific benchmarks, neural techniques seem to match if not outperform human judgement, challenges are still open for detecting arbitrary concepts in arbitrary videos. In this paper, we propose a system that combines neural techniques, a large scale visual concepts ontology, and an active learning loop, to provide on the fly model learning of arbitrary concepts. We give an overview of the system as a whole, and focus on the central role of the ontology for guiding and bootstrapping the learning of new concepts, improving the recall of concept detection, and, on the user end, providing semantic search on a library of annotated videos.\n    ",
        "submission_date": "2016-05-31T00:00:00",
        "last_modified_date": "2016-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.09772",
        "title": "Technical Report: Directed Controller Synthesis of Discrete Event Systems",
        "authors": [
            "Daniel Ciolek",
            "Victor Braberman",
            "Nicol\u00e1s D'Ippolito",
            "Sebasti\u00e1n Uchitel"
        ],
        "abstract": "This paper presents a Directed Controller Synthesis (DCS) technique for discrete event systems. The DCS method explores the solution space for reactive controllers guided by a domain-independent heuristic. The heuristic is derived from an efficient abstraction of the environment based on the componentized way in which complex environments are described. Then by building the composition of the components on-the-fly DCS obtains a solution by exploring a reduced portion of the state space. This work focuses on untimed discrete event systems with safety and co-safety (i.e. reachability) goals. An evaluation for the technique is presented comparing it to other well-known approaches to controller synthesis (based on symbolic representation and compositional analyses).\n    ",
        "submission_date": "2016-05-31T00:00:00",
        "last_modified_date": "2016-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.09782",
        "title": "Adversarial Feature Learning",
        "authors": [
            "Jeff Donahue",
            "Philipp Kr\u00e4henb\u00fchl",
            "Trevor Darrell"
        ],
        "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.\n    ",
        "submission_date": "2016-05-31T00:00:00",
        "last_modified_date": "2017-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00002",
        "title": "Uncertain programming model for multi-item solid transportation problem",
        "authors": [
            "Hasan Dalman"
        ],
        "abstract": "In this paper, an uncertain Multi-objective Multi-item Solid Transportation Problem (MMSTP) based on uncertainty theory is presented. In the model, transportation costs, supplies, demands and conveyances parameters are taken to be uncertain parameters. There are restrictions on some items and conveyances of the model. Therefore, some particular items cannot be transported by some exceptional conveyances. Using the advantage of uncertainty theory, the MMSTP is first converted into an equivalent deterministic MMSTP. By applying convex combination method and minimizing distance function method, the deterministic MMSTP is reduced into single objective programming problems. Thus, both single objective programming problems are solved using Maple 18.02 optimization toolbox. Finally, a numerical example is given to illustrate the performance of the models.\n    ",
        "submission_date": "2016-05-31T00:00:00",
        "last_modified_date": "2016-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00117",
        "title": "Hardness of the Pricing Problem for Chains in Barter Exchanges",
        "authors": [
            "Benjamin Plaut",
            "John P. Dickerson",
            "Tuomas Sandholm"
        ],
        "abstract": "Kidney exchange is a barter market where patients trade willing but medically incompatible donors. These trades occur via cycles, where each patient-donor pair both gives and receives a kidney, and via chains, which begin with an altruistic donor who does not require a kidney in return. For logistical reasons, the maximum length of a cycle is typically limited to a small constant, while chains can be much longer. Given a compatibility graph of patient-donor pairs, altruists, and feasible potential transplants between them, finding even a maximum-cardinality set of vertex-disjoint cycles and chains is NP-hard. There has been much work on developing provably optimal solvers that are efficient in practice. One of the leading techniques has been branch and price, where column generation is used to incrementally bring cycles and chains into the optimization model on an as-needed basis. In particular, only positive-price columns need to be brought into the model. We prove that finding a positive-price chain is NP-complete. This shows incorrectness of two leading branch-and-price solvers that suggested polynomial-time chain pricing algorithms.\n    ",
        "submission_date": "2016-06-01T00:00:00",
        "last_modified_date": "2016-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00401",
        "title": "How to advance general game playing artificial intelligence by player modelling",
        "authors": [
            "Benjamin Ultan Cowley"
        ],
        "abstract": "General game playing artificial intelligence has recently seen important advances due to the various techniques known as 'deep learning'. However the advances conceal equally important limitations in their reliance on: massive data sets; fortuitously constructed problems; and absence of any human-level complexity, including other human opponents. On the other hand, deep learning systems which do beat human champions, such as in Go, do not generalise well. The power of deep learning simultaneously exposes its weakness. Given that deep learning is mostly clever reconfigurations of well-established methods, moving beyond the state of art calls for forward-thinking visionary solutions, not just more of the same. I present the argument that general game playing artificial intelligence will require a generalised player model. This is because games are inherently human artefacts which therefore, as a class of problems, contain cases which require a human-style problem solving approach. I relate this argument to the performance of state of art general game playing agents. I then describe a concept for a formal category theoretic basis to a generalised player model. This formal model approach integrates my existing 'Behavlets' method for psychologically-derived player modelling:\n",
        "submission_date": "2016-06-01T00:00:00",
        "last_modified_date": "2016-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00561",
        "title": "Mining Software Components from Object-Oriented APIs",
        "authors": [
            "Anas Shatnawi",
            "Abdelhak Seriai",
            "Houari Sahraoui",
            "Zakarea Al-Shara"
        ],
        "abstract": "Object-oriented Application Programing Interfaces (APIs) support software reuse by providing pre-implemented functionalities. Due to the huge number of included classes, reusing and understanding large APIs is a complex task. Otherwise, software components are admitted to be more reusable and understandable entities than object-oriented ones. Thus, in this paper, we propose an approach for reengineering object-oriented APIs into component-based ones. We mine components as a group of classes based on the frequency they are used together and their ability to form a quality-centric component. To validate our approach, we experimented on 100 Java applications that used Android APIs.\n    ",
        "submission_date": "2016-06-02T00:00:00",
        "last_modified_date": "2016-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00776",
        "title": "Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation",
        "authors": [
            "Iulian Vlad Serban",
            "Tim Klinger",
            "Gerald Tesauro",
            "Kartik Talamadupula",
            "Bowen Zhou",
            "Yoshua Bengio",
            "Aaron Courville"
        ],
        "abstract": "We introduce the multiresolution recurrent neural network, which extends the sequence-to-sequence framework to model natural language generation as two parallel discrete stochastic processes: a sequence of high-level coarse tokens, and a sequence of natural language tokens. There are many ways to estimate or learn the high-level coarse tokens, but we argue that a simple extraction procedure is sufficient to capture a wealth of high-level discourse semantics. Such procedure allows training the multiresolution recurrent neural network by maximizing the exact joint log-likelihood over both sequences. In contrast to the standard log- likelihood objective w.r.t. natural language tokens (word perplexity), optimizing the joint log-likelihood biases the model towards modeling high-level abstractions. We apply the proposed model to the task of dialogue response generation in two challenging domains: the Ubuntu technical support domain, and Twitter conversations. On Ubuntu, the model outperforms competing approaches by a substantial margin, achieving state-of-the-art results according to both automatic evaluation metrics and a human evaluation study. On Twitter, the model appears to generate more relevant and on-topic responses according to automatic evaluation metrics. Finally, our experiments demonstrate that the proposed model is more adept at overcoming the sparsity of natural language and is better able to capture long-term structure.\n    ",
        "submission_date": "2016-06-02T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00787",
        "title": "Post-Inference Prior Swapping",
        "authors": [
            "Willie Neiswanger",
            "Eric Xing"
        ],
        "abstract": "While Bayesian methods are praised for their ability to incorporate useful prior knowledge, in practice, convenient priors that allow for computationally cheap or tractable inference are commonly used. In this paper, we investigate the following question: for a given model, is it possible to compute an inference result with any convenient false prior, and afterwards, given any target prior of interest, quickly transform this result into the target posterior? A potential solution is to use importance sampling (IS). However, we demonstrate that IS will fail for many choices of the target prior, depending on its parametric form and similarity to the false prior. Instead, we propose prior swapping, a method that leverages the pre-inferred false posterior to efficiently generate accurate posterior samples under arbitrary target priors. Prior swapping lets us apply less-costly inference algorithms to certain models, and incorporate new or updated prior information \"post-inference\". We give theoretical guarantees about our method, and demonstrate it empirically on a number of models and priors.\n    ",
        "submission_date": "2016-06-02T00:00:00",
        "last_modified_date": "2017-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00917",
        "title": "Towards a Job Title Classification System",
        "authors": [
            "Faizan Javed",
            "Matt McNair",
            "Ferosh Jacob",
            "Meng Zhao"
        ],
        "abstract": "Document classification for text, images and other applicable entities has long been a focus of research in academia and also finds application in many industrial settings. Amidst a plethora of approaches to solve such problems, machine-learning techniques have found success in a variety of scenarios. In this paper we discuss the design of a machine learning-based semi-supervised job title classification system for the online job recruitment domain currently in production at ",
        "submission_date": "2016-06-02T00:00:00",
        "last_modified_date": "2016-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00927",
        "title": "An interactive fuzzy goal programming algorithm to solve decentralized bi-level multiobjective fractional programming problem",
        "authors": [
            "Hasan Dalman"
        ],
        "abstract": "This paper proposes a fuzzy goal programming based on Taylor series for solving decentralized bi-level multiobjective fractional programming (DBLMOFP) problem. In the proposed approach, all of the membership functions are associated with the fuzzy goals of each objective at the both levels and also the fractional membership functions are converted to linear functions using the Taylor series approach. Then a fuzzy goal programming is proposed to reach the highest degree of each of the membership goals by taking the most satisfactory solution for all decision makers at the both levels. Finally, a numerical example is presented to illustrate the effectiveness of the proposed approach.\n    ",
        "submission_date": "2016-06-02T00:00:00",
        "last_modified_date": "2016-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00979",
        "title": "Question Answering over Knowledge Base with Neural Attention Combining Global Knowledge Information",
        "authors": [
            "Yuanzhe Zhang",
            "Kang Liu",
            "Shizhu He",
            "Guoliang Ji",
            "Zhanyi Liu",
            "Hua Wu",
            "Jun Zhao"
        ],
        "abstract": "With the rapid growth of knowledge bases (KBs) on the web, how to take full advantage of them becomes increasingly important. Knowledge base-based question answering (KB-QA) is one of the most promising approaches to access the substantial knowledge. Meantime, as the neural network-based (NN-based) methods develop, NN-based KB-QA has already achieved impressive results. However, previous work did not put emphasis on question representation, and the question is converted into a fixed vector regardless of its candidate answers. This simple representation strategy is unable to express the proper information of the question. Hence, we present a neural attention-based model to represent the questions dynamically according to the different focuses of various candidate answer aspects. In addition, we leverage the global knowledge inside the underlying KB, aiming at integrating the rich KB information into the representation of the answers. And it also alleviates the out of vocabulary (OOV) problem, which helps the attention model to represent the question more precisely. The experimental results on WEBQUESTIONS demonstrate the effectiveness of the proposed approach.\n    ",
        "submission_date": "2016-06-03T00:00:00",
        "last_modified_date": "2016-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01245",
        "title": "Scalable Algorithms for Tractable Schatten Quasi-Norm Minimization",
        "authors": [
            "Fanhua Shang",
            "Yuanyuan Liu",
            "James Cheng"
        ],
        "abstract": "The Schatten-p quasi-norm $(0<p<1)$ is usually used to replace the standard nuclear norm in order to approximate the rank function more accurately. However, existing Schatten-p quasi-norm minimization algorithms involve singular value decomposition (SVD) or eigenvalue decomposition (EVD) in each iteration, and thus may become very slow and impractical for large-scale problems. In this paper, we first define two tractable Schatten quasi-norms, i.e., the Frobenius/nuclear hybrid and bi-nuclear quasi-norms, and then prove that they are in essence the Schatten-2/3 and 1/2 quasi-norms, respectively, which lead to the design of very efficient algorithms that only need to update two much smaller factor matrices. We also design two efficient proximal alternating linearized minimization algorithms for solving representative matrix completion problems. Finally, we provide the global convergence and performance guarantees for our algorithms, which have better convergence properties than existing algorithms. Experimental results on synthetic and real-world data show that our algorithms are more accurate than the state-of-the-art methods, and are orders of magnitude faster.\n    ",
        "submission_date": "2016-06-04T00:00:00",
        "last_modified_date": "2016-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01269",
        "title": "End-to-end LSTM-based dialog control optimized with supervised and reinforcement learning",
        "authors": [
            "Jason D. Williams",
            "Geoffrey Zweig"
        ],
        "abstract": "This paper presents a model for end-to-end learning of task-oriented dialog systems. The main component of the model is a recurrent neural network (an LSTM), which maps from raw dialog history directly to a distribution over system actions. The LSTM automatically infers a representation of dialog history, which relieves the system developer of much of the manual feature engineering of dialog state. In addition, the developer can provide software that expresses business rules and provides access to programmatic APIs, enabling the LSTM to take actions in the real world on behalf of the user. The LSTM can be optimized using supervised learning (SL), where a domain expert provides example dialogs which the LSTM should imitate; or using reinforcement learning (RL), where the system improves by interacting directly with end users. Experiments show that SL and RL are complementary: SL alone can derive a reasonable initial policy from a small number of training dialogs; and starting RL optimization with a policy trained with SL substantially accelerates the learning rate of RL.\n    ",
        "submission_date": "2016-06-03T00:00:00",
        "last_modified_date": "2016-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01307",
        "title": "Scene Grammars, Factor Graphs, and Belief Propagation",
        "authors": [
            "Jeroen Chua",
            "Pedro F. Felzenszwalb"
        ],
        "abstract": "We describe a general framework for probabilistic modeling of complex scenes and inference from ambiguous observations. The approach is motivated by applications in image analysis and is based on the use of priors defined by stochastic grammars. We define a class of grammars that capture relationships between the objects in a scene and provide important contextual cues for statistical inference. The distribution over scenes defined by a probabilistic scene grammar can be represented by a graphical model and this construction can be used for efficient inference with loopy belief propagation.\n",
        "submission_date": "2016-06-03T00:00:00",
        "last_modified_date": "2019-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01404",
        "title": "Generating Natural Language Inference Chains",
        "authors": [
            "Vladyslav Kolesnyk",
            "Tim Rockt\u00e4schel",
            "Sebastian Riedel"
        ],
        "abstract": "The ability to reason with natural language is a fundamental prerequisite for many NLP tasks such as information extraction, machine translation and question answering. To quantify this ability, systems are commonly tested whether they can recognize textual entailment, i.e., whether one sentence can be inferred from another one. However, in most NLP applications only single source sentences instead of sentence pairs are available. Hence, we propose a new task that measures how well a model can generate an entailed sentence from a source sentence. We take entailment-pairs of the Stanford Natural Language Inference corpus and train an LSTM with attention. On a manually annotated test set we found that 82% of generated sentences are correct, an improvement of 10.3% over an LSTM baseline. A qualitative analysis shows that this model is not only capable of shortening input sentences, but also inferring new statements via paraphrasing and phrase entailment. We then apply this model recursively to input-output pairs, thereby generating natural language inference chains that can be used to automatically construct an entailment graph from source sentences. Finally, by swapping source and target sentences we can also train a model that given an input sentence invents additional information to generate a new sentence.\n    ",
        "submission_date": "2016-06-04T00:00:00",
        "last_modified_date": "2016-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01515",
        "title": "Coordination in Categorical Compositional Distributional Semantics",
        "authors": [
            "Dimitri Kartsaklis"
        ],
        "abstract": "An open problem with categorical compositional distributional semantics is the representation of words that are considered semantically vacuous from a distributional perspective, such as determiners, prepositions, relative pronouns or coordinators. This paper deals with the topic of coordination between identical syntactic types, which accounts for the majority of coordination cases in language. By exploiting the compact closed structure of the underlying category and Frobenius operators canonically induced over the fixed basis of finite-dimensional vector spaces, we provide a morphism as representation of a coordinator tensor, and we show how it lifts from atomic types to compound types. Linguistic intuitions are provided, and the importance of the Frobenius operators as an addition to the compact closed setting with regard to language is discussed.\n    ",
        "submission_date": "2016-06-05T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01540",
        "title": "OpenAI Gym",
        "authors": [
            "Greg Brockman",
            "Vicki Cheung",
            "Ludwig Pettersson",
            "Jonas Schneider",
            "John Schulman",
            "Jie Tang",
            "Wojciech Zaremba"
        ],
        "abstract": "OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.\n    ",
        "submission_date": "2016-06-05T00:00:00",
        "last_modified_date": "2016-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01623",
        "title": "Position-Indexed Formulations for Kidney Exchange",
        "authors": [
            "John P. Dickerson",
            "David F. Manlove",
            "Benjamin Plaut",
            "Tuomas Sandholm",
            "James Trimble"
        ],
        "abstract": "A kidney exchange is an organized barter market where patients in need of a kidney swap willing but incompatible donors. Determining an optimal set of exchanges is theoretically and empirically hard. Traditionally, exchanges took place in cycles, with each participating patient-donor pair both giving and receiving a kidney. The recent introduction of chains, where a donor without a paired patient triggers a sequence of donations without requiring a kidney in return, increased the efficacy of fielded kidney exchanges---while also dramatically raising the empirical computational hardness of clearing the market in practice. While chains can be quite long, unbounded-length chains are not desirable: planned donations can fail before transplant for a variety of reasons, and the failure of a single donation causes the rest of that chain to fail, so parallel shorter chains are better in practice.\n",
        "submission_date": "2016-06-06T00:00:00",
        "last_modified_date": "2016-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01847",
        "title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding",
        "authors": [
            "Akira Fukui",
            "Dong Huk Park",
            "Daylen Yang",
            "Anna Rohrbach",
            "Trevor Darrell",
            "Marcus Rohrbach"
        ],
        "abstract": "Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.\n    ",
        "submission_date": "2016-06-06T00:00:00",
        "last_modified_date": "2016-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01855",
        "title": "Bayesian Poisson Tucker Decomposition for Learning the Structure of International Relations",
        "authors": [
            "Aaron Schein",
            "Mingyuan Zhou",
            "David M. Blei",
            "Hanna Wallach"
        ],
        "abstract": "We introduce Bayesian Poisson Tucker decomposition (BPTD) for modeling country--country interaction event data. These data consist of interaction events of the form \"country $i$ took action $a$ toward country $j$ at time $t$.\" BPTD discovers overlapping country--community memberships, including the number of latent communities. In addition, it discovers directed community--community interaction networks that are specific to \"topics\" of action types and temporal \"regimes.\" We show that BPTD yields an efficient MCMC inference algorithm and achieves better predictive performance than related models. We also demonstrate that it discovers interpretable latent structure that agrees with our knowledge of international relations.\n    ",
        "submission_date": "2016-06-06T00:00:00",
        "last_modified_date": "2016-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01885",
        "title": "Learning to Optimize",
        "authors": [
            "Ke Li",
            "Jitendra Malik"
        ],
        "abstract": "Algorithm design is a laborious process and often requires many iterations of ideation and validation. In this paper, we explore automating algorithm design and present a method to learn an optimization algorithm, which we believe to be the first method that can automatically discover a better algorithm. We approach this problem from a reinforcement learning perspective and represent any particular optimization algorithm as a policy. We learn an optimization algorithm using guided policy search and demonstrate that the resulting algorithm outperforms existing hand-engineered algorithms in terms of convergence speed and/or the final objective value.\n    ",
        "submission_date": "2016-06-06T00:00:00",
        "last_modified_date": "2016-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01930",
        "title": "Consistency and Trust in Peer Data Exchange Systems",
        "authors": [
            "Leopoldo Bertossi",
            "Loreto Bravo"
        ],
        "abstract": "We propose and investigate a semantics for \"peer data exchange systems\" where different peers are related by data exchange constraints and trust relationships. These two elements plus the data at the peers' sites and their local integrity constraints are made compatible via a semantics that characterizes sets of \"solution instances\" for the peers. They are the intended -possibly virtual- instances for a peer that are obtained through a data repair semantics that we introduce and investigate. The semantically correct answers from a peer to a query, the so-called \"peer consistent answers\", are defined as those answers that are invariant under all its different solution instances. We show that solution instances can be specified as the models of logic programs with a stable model semantics. The repair semantics is based on null values as used in SQL databases, and is also of independent interest for repairs of single databases with respect to integrity constraints.\n    ",
        "submission_date": "2016-06-06T00:00:00",
        "last_modified_date": "2016-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02239",
        "title": "A Formal Calculus for International Relations Computation and Evaluation",
        "authors": [
            "Mohd Anuar Mat Isa",
            "Ramlan Mahmod",
            "Nur Izura Udzir",
            "Jamalul-lail Ab Manan",
            "Audun J\u00f8sang",
            "Ali Dehghan Tanha"
        ],
        "abstract": "This publication presents a relation computation or calculus for international relations using a mathematical modeling. It examined trust for international relations and its calculus, which related to Bayesian inference, Dempster-Shafer theory and subjective logic. Based on an observation in the literature, we found no literature discussing the calculus method for the international relations. To bridge this research gap, we propose a relation algebra method for international relations computation. The proposed method will allow a relation computation which is previously subjective and incomputable. We also present three international relations as case studies to demonstrate the proposed method is a real-world scenario. The method will deliver the relation computation for the international relations that to support decision makers in a government such as foreign ministry, defense ministry, presidential or prime minister office. The Department of Defense (DoD) may use our method to determine a nation that can be identified as a friendly, neutral or hostile nation.\n    ",
        "submission_date": "2016-04-02T00:00:00",
        "last_modified_date": "2016-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02355",
        "title": "Active Long Term Memory Networks",
        "authors": [
            "Tommaso Furlanello",
            "Jiaping Zhao",
            "Andrew M. Saxe",
            "Laurent Itti",
            "Bosco S. Tjan"
        ],
        "abstract": "Continual Learning in artificial neural networks suffers from interference and forgetting when different tasks are learned sequentially. This paper introduces the Active Long Term Memory Networks (A-LTM), a model of sequential multi-task deep learning that is able to maintain previously learned association between sensory input and behavioral output while acquiring knew knowledge. A-LTM exploits the non-convex nature of deep neural networks and actively maintains knowledge of previously learned, inactive tasks using a distillation loss. Distortions of the learned input-output map are penalized but hidden layers are free to transverse towards new local optima that are more favorable for the multi-task objective. We re-frame the McClelland's seminal Hippocampal theory with respect to Catastrophic Inference (CI) behavior exhibited by modern deep architectures trained with back-propagation and inhomogeneous sampling of latent factors across epochs. We present empirical results of non-trivial CI during continual learning in Deep Linear Networks trained on the same task, in Convolutional Neural Networks when the task shifts from predicting semantic to graphical factors and during domain adaptation from simple to complex environments. We present results of the A-LTM model's ability to maintain viewpoint recognition learned in the highly controlled iLab-20M dataset with 10 object categories and 88 camera viewpoints, while adapting to the unstructured domain of Imagenet with 1,000 object categories.\n    ",
        "submission_date": "2016-06-07T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02378",
        "title": "SE3-Nets: Learning Rigid Body Motion using Deep Neural Networks",
        "authors": [
            "Arunkumar Byravan",
            "Dieter Fox"
        ],
        "abstract": "We introduce SE3-Nets, which are deep neural networks designed to model and learn rigid body motion from raw point cloud data. Based only on sequences of depth images along with action vectors and point wise data associations, SE3-Nets learn to segment effected object parts and predict their motion resulting from the applied force. Rather than learning point wise flow vectors, SE3-Nets predict SE3 transformations for different parts of the scene. Using simulated depth data of a table top scene and a robot manipulator, we show that the structure underlying SE3-Nets enables them to generate a far more consistent prediction of object motion than traditional flow based networks. Additional experiments with a depth camera observing a Baxter robot pushing objects on a table show that SE3-Nets also work well on real data.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2017-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02382",
        "title": "Deep Learning Convolutional Networks for Multiphoton Microscopy Vasculature Segmentation",
        "authors": [
            "Petteri Teikari",
            "Marc Santos",
            "Charissa Poon",
            "Kullervo Hynynen"
        ],
        "abstract": "Recently there has been an increasing trend to use deep learning frameworks for both 2D consumer images and for 3D medical images. However, there has been little effort to use deep frameworks for volumetric vascular segmentation. We wanted to address this by providing a freely available dataset of 12 annotated two-photon vasculature microscopy stacks. We demonstrated the use of deep learning framework consisting both 2D and 3D convolutional filters (ConvNet). Our hybrid 2D-3D architecture produced promising segmentation result. We derived the architectures from Lee et al. who used the ZNN framework initially designed for electron microscope image segmentation. We hope that by sharing our volumetric vasculature datasets, we will inspire other researchers to experiment with vasculature dataset and improve the used network architectures.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02396",
        "title": "Deep Successor Reinforcement Learning",
        "authors": [
            "Tejas D. Kulkarni",
            "Ardavan Saeedi",
            "Simanta Gautam",
            "Samuel J. Gershman"
        ],
        "abstract": "Learning robust value functions given raw observations and rewards is now possible with model-free and model-based deep reinforcement learning algorithms. There is a third alternative, called Successor Representations (SR), which decomposes the value function into two components -- a reward predictor and a successor map. The successor map represents the expected future state occupancy from any given state and the reward predictor maps states to scalar rewards. The value function of a state can be computed as the inner product between the successor map and the reward weights. In this paper, we present DSR, which generalizes SR within an end-to-end deep reinforcement learning framework. DSR has several appealing properties including: increased sensitivity to distal reward changes due to factorization of reward and world dynamics, and the ability to extract bottleneck states (subgoals) given successor maps trained under a random policy. We show the efficacy of our approach on two diverse environments given raw pixel observations -- simple grid-world domains (MazeBase) and the Doom game engine.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02407",
        "title": "Structured Convolution Matrices for Energy-efficient Deep learning",
        "authors": [
            "Rathinakumar Appuswamy",
            "Tapan Nayak",
            "John Arthur",
            "Steven Esser",
            "Paul Merolla",
            "Jeffrey Mckinstry",
            "Timothy Melano",
            "Myron Flickner",
            "Dharmendra Modha"
        ],
        "abstract": "We derive a relationship between network representation in energy-efficient neuromorphic architectures and block Toplitz convolutional matrices. Inspired by this connection, we develop deep convolutional networks using a family of structured convolutional matrices and achieve state-of-the-art trade-off between energy efficiency and classification accuracy for well-known image recognition tasks. We also put forward a novel method to train binary convolutional networks by utilising an existing connection between noisy-rectified linear units and binary activations.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02421",
        "title": "Gossip Dual Averaging for Decentralized Optimization of Pairwise Functions",
        "authors": [
            "Igor Colin",
            "Aur\u00e9lien Bellet",
            "Joseph Salmon",
            "St\u00e9phan Cl\u00e9men\u00e7on"
        ],
        "abstract": "In decentralized networks (of sensors, connected objects, etc.), there is an important need for efficient algorithms to optimize a global cost function, for instance to learn a global model from the local data collected by each computing unit. In this paper, we address the problem of decentralized minimization of pairwise functions of the data points, where these points are distributed over the nodes of a graph defining the communication topology of the network. This general problem finds applications in ranking, distance metric learning and graph inference, among others. We propose new gossip algorithms based on dual averaging which aims at solving such problems both in synchronous and asynchronous settings. The proposed framework is flexible enough to deal with constrained and regularized variants of the optimization problem. Our theoretical analysis reveals that the proposed algorithms preserve the convergence rate of centralized dual averaging up to an additive bias term. We present numerical simulations on Area Under the ROC Curve (AUC) maximization and metric learning problems which illustrate the practical interest of our approach.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02447",
        "title": "Learning Language Games through Interaction",
        "authors": [
            "Sida I. Wang",
            "Percy Liang",
            "Christopher D. Manning"
        ],
        "abstract": "We introduce a new language learning setting relevant to building adaptive natural language interfaces. It is inspired by Wittgenstein's language games: a human wishes to accomplish some task (e.g., achieving a certain configuration of blocks), but can only communicate with a computer, who performs the actual actions (e.g., removing all red blocks). The computer initially knows nothing about language and therefore must learn it from scratch through interaction, while the human adapts to the computer's capabilities. We created a game in a blocks world and collected interactions from 100 people playing it. First, we analyze the humans' strategies, showing that using compositionality and avoiding synonyms correlates positively with task performance. Second, we compare computer strategies, showing how to quickly learn a semantic parsing model from scratch, and that modeling pragmatics further accelerates learning for successful players.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02485",
        "title": "Exploring Implicit Human Responses to Robot Mistakes in a Learning from Demonstration Task",
        "authors": [
            "Cory J. Hayes",
            "Maryam Moosaei",
            "Laurel D. Riek"
        ],
        "abstract": "As robots enter human environments, they will be expected to accomplish a tremendous range of tasks. It is not feasible for robot designers to pre-program these behaviors or know them in advance, so one way to address this is through end-user programming, such as via learning from demonstration (LfD). While significant work has been done on the mechanics of enabling robot learning from human teachers, one unexplored aspect is enabling mutual feedback between both the human teacher and robot during the learning process, i.e., implicit learning. In this paper, we explore one aspect of this mutual understanding, grounding sequences, where both a human and robot provide non-verbal feedback to signify their mutual understanding during interaction. We conducted a study where people taught an autonomous humanoid robot a dance, and performed gesture analysis to measure people's responses to the robot during correct and incorrect demonstrations.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02542",
        "title": "Symbolic Music Data Version 1.0",
        "authors": [
            "Christian Walder"
        ],
        "abstract": "In this document, we introduce a new dataset designed for training machine learning models of symbolic music data. Five datasets are provided, one of which is from a newly collected corpus of 20K midi files. We describe our preprocessing and cleaning pipeline, which includes the exclusion of a number of files based on scores from a previously developed probabilistic machine learning model. We also define training, testing and validation splits for the new dataset, based on a clustering scheme which we also describe. Some simple histograms are included.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02556",
        "title": "DISCO Nets: DISsimilarity COefficient Networks",
        "authors": [
            "Diane Bouchacourt",
            "M. Pawan Kumar",
            "Sebastian Nowozin"
        ],
        "abstract": "We present a new type of probabilistic model which we call DISsimilarity COefficient Networks (DISCO Nets). DISCO Nets allow us to efficiently sample from a posterior distribution parametrised by a neural network. During training, DISCO Nets are learned by minimising the dissimilarity coefficient between the true distribution and the estimated distribution. This allows us to tailor the training to the loss related to the task at hand. We empirically show that (i) by modeling uncertainty on the output value, DISCO Nets outperform equivalent non-probabilistic predictive networks and (ii) DISCO Nets accurately model the uncertainty of the output, outperforming existing probabilistic models based on deep neural networks.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02583",
        "title": "The Dark Side of Ethical Robots",
        "authors": [
            "Dieter Vanderelst",
            "Alan Winfield"
        ],
        "abstract": "Concerns over the risks associated with advances in Artificial Intelligence have prompted calls for greater efforts toward robust and beneficial AI, including machine ethics. Recently, roboticists have responded by initiating the development of so-called ethical robots. These robots would, ideally, evaluate the consequences of their actions and morally justify their choices. This emerging field promises to develop extensively over the next years. However, in this paper, we point out an inherent limitation of the emerging field of ethical robots. We show that building ethical robots also necessarily facilitates the construction of unethical robots. In three experiments, we show that it is remarkably easy to modify an ethical robot so that it behaves competitively, or even aggressively. The reason for this is that the specific AI, required to make an ethical robot, can always be exploited to make unethical robots. Hence, the development of ethical robots will not guarantee the responsible deployment of AI. While advocating for ethical robots, we conclude that preventing the misuse of robots is beyond the scope of engineering, and requires instead governance frameworks underpinned by legislation. Without this, the development of ethical robots will serve to increase the risks of robotic malpractice instead of diminishing it.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02647",
        "title": "Safe and Efficient Off-Policy Reinforcement Learning",
        "authors": [
            "R\u00e9mi Munos",
            "Tom Stepleton",
            "Anna Harutyunyan",
            "Marc G. Bellemare"
        ],
        "abstract": "In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace($\\lambda$), with three desired properties: (1) it has low variance; (2) it safely uses samples collected from any behaviour policy, whatever its degree of \"off-policyness\"; and (3) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies. We analyze the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. We believe this is the first return-based off-policy control algorithm converging a.s. to $Q^*$ without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins' Q($\\lambda$), which was an open problem since 1989. We illustrate the benefits of Retrace($\\lambda$) on a standard suite of Atari 2600 games.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02807",
        "title": "Face valuing: Training user interfaces with facial expressions and reinforcement learning",
        "authors": [
            "Vivek Veeriah",
            "Patrick M. Pilarski",
            "Richard S. Sutton"
        ],
        "abstract": "An important application of interactive machine learning is extending or amplifying the cognitive and physical capabilities of a human. To accomplish this, machines need to learn about their human users' intentions and adapt to their preferences. In most current research, a user has conveyed preferences to a machine using explicit corrective or instructive feedback; explicit feedback imposes a cognitive load on the user and is expensive in terms of human effort. The primary objective of the current work is to demonstrate that a learning agent can reduce the amount of explicit feedback required for adapting to the user's preferences pertaining to a task by learning to perceive a value of its behavior from the human user, particularly from the user's facial expressions---we call this face valuing. We empirically evaluate face valuing on a grip selection task. Our preliminary results suggest that an agent can quickly adapt to a user's changing preferences with minimal explicit feedback by learning a value function that maps facial features extracted from a camera image to expected future reward. We believe that an agent learning to perceive a value from the body language of its human user is complementary to existing interactive machine learning approaches and will help in creating successful human-machine interactive applications.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02825",
        "title": "Arbitrage-Free Combinatorial Market Making via Integer Programming",
        "authors": [
            "Christian Kroer",
            "Miroslav Dud\u00edk",
            "S\u00e9bastien Lahaie",
            "Sivaraman Balakrishnan"
        ],
        "abstract": "We present a new combinatorial market maker that operates arbitrage-free combinatorial prediction markets specified by integer programs. Although the problem of arbitrage-free pricing, while maintaining a bound on the subsidy provided by the market maker, is #P-hard in the worst case, we posit that the typical case might be amenable to modern integer programming (IP) solvers. At the crux of our method is the Frank-Wolfe (conditional gradient) algorithm which is used to implement a Bregman projection aligned with the market maker's cost function, using an IP solver as an oracle. We demonstrate the tractability and improved accuracy of our approach on real-world prediction market data from combinatorial bets placed on the 2010 NCAA Men's Division I Basketball Tournament, where the outcome space is of size 2^63. To our knowledge, this is the first implementation and empirical evaluation of an arbitrage-free combinatorial prediction market on this scale.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02854",
        "title": "e-Commerce product classification: our participation at cDiscount 2015 challenge",
        "authors": [
            "Ioannis Partalas",
            "Georgios Balikas"
        ],
        "abstract": "This report describes our participation in the cDiscount 2015 challenge where the goal was to classify product items in a predefined taxonomy of products. Our best submission yielded an accuracy score of 64.20\\% in the private part of the leaderboard and we were ranked 10th out of 175 participating teams. We followed a text classification approach employing mainly linear models. The final solution was a weighted voting system which combined a variety of trained models.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02858",
        "title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task",
        "authors": [
            "Danqi Chen",
            "Jason Bolton",
            "Christopher D. Manning"
        ],
        "abstract": "Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 73.6% and 76.6% on these two datasets, exceeding current state-of-the-art results by 7-10% and approaching what we believe is the ceiling for performance on this task.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02877",
        "title": "Understanding User Instructions by Utilizing Open Knowledge for Service Robots",
        "authors": [
            "Dongcai Lu",
            "Feng Wu",
            "Xiaoping Chen"
        ],
        "abstract": "Understanding user instructions in natural language is an active research topic in AI and robotics. Typically, natural user instructions are high-level and can be reduced into low-level tasks expressed in common verbs (e.g., `take', `get', `put'). For robots understanding such instructions, one of the key challenges is to process high-level user instructions and achieve the specified tasks with robots' primitive actions. To address this, we propose novel algorithms by utilizing semantic roles of common verbs defined in semantic dictionaries and integrating multiple open knowledge to generate task plans. Specifically, we present a new method for matching and recovering semantics of user instructions and a novel task planner that exploits functional knowledge of robot's action model. To verify and evaluate our approach, we implemented a prototype system using knowledge from several open resources. Experiments on our system confirmed the correctness and efficiency of our algorithms. Notably, our system has been deployed in the KeJia robot, which participated the annual RoboCup@Home competitions in the past three years and achieved encouragingly high scores in the benchmark tests.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02979",
        "title": "Generative Topic Embedding: a Continuous Representation of Documents (Extended Version with Proofs)",
        "authors": [
            "Shaohua Li",
            "Tat-Seng Chua",
            "Jun Zhu",
            "Chunyan Miao"
        ],
        "abstract": "Word embedding maps words into a low-dimensional continuous embedding space by exploiting the local word collocation patterns in a small context window. On the other hand, topic modeling maps documents onto a low-dimensional topic space, by utilizing the global word collocation patterns in the same document. These two types of patterns are complementary. In this paper, we propose a generative topic embedding model to combine the two types of patterns. In our model, topics are represented by embedding vectors, and are shared across documents. The probability of each word is influenced by both its local context and its topic. A variational inference method yields the topic embeddings as well as the topic mixing proportions for each document. Jointly they represent the document in a low-dimensional continuous space. In two document classification tasks, our method performs better than eight existing methods, with fewer features. In addition, we illustrate with an example that our method can generate coherent topics even based on only one document.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03002",
        "title": "MuFuRU: The Multi-Function Recurrent Unit",
        "authors": [
            "Dirk Weissenborn",
            "Tim Rockt\u00e4schel"
        ],
        "abstract": "Recurrent neural networks such as the GRU and LSTM found wide adoption in natural language processing and achieve state-of-the-art results for many tasks. These models are characterized by a memory state that can be written to and read from by applying gated composition operations to the current input and the previous state. However, they only cover a small subset of potentially useful compositions. We propose Multi-Function Recurrent Units (MuFuRUs) that allow for arbitrary differentiable functions as composition operations. Furthermore, MuFuRUs allow for an input- and state-dependent choice of these composition operations that is learned. Our experiments demonstrate that the additional functionality helps in different sequence modeling tasks, including the evaluation of propositional logic formulae, language modeling and sentiment analysis.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03044",
        "title": "The \"Horse'' Inside: Seeking Causes Behind the Behaviours of Music Content Analysis Systems",
        "authors": [
            "Bob L. Sturm"
        ],
        "abstract": "Building systems that possess the sensitivity and intelligence to identify and describe high-level attributes in music audio signals continues to be an elusive goal, but one that surely has broad and deep implications for a wide variety of applications. Hundreds of papers have so far been published toward this goal, and great progress appears to have been made. Some systems produce remarkable accuracies at recognising high-level semantic concepts, such as music style, genre and mood. However, it might be that these numbers do not mean what they seem. In this paper, we take a state-of-the-art music content analysis system and investigate what causes it to achieve exceptionally high performance in a benchmark music audio dataset. We dissect the system to understand its operation, determine its sensitivities and limitations, and predict the kinds of knowledge it could and could not possess about music. We perform a series of experiments to illuminate what the system has actually learned to do, and to what extent it is performing the intended music listening task. Our results demonstrate how the initial manifestation of music intelligence in this state-of-the-art can be deceptive. Our work provides constructive directions toward developing music content analysis systems that can address the music information and creation needs of real-world users.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03152",
        "title": "Policy Networks with Two-Stage Training for Dialogue Systems",
        "authors": [
            "Mehdi Fatemi",
            "Layla El Asri",
            "Hannes Schulz",
            "Jing He",
            "Kaheer Suleman"
        ],
        "abstract": "In this paper, we propose to use deep policy networks which are trained with an advantage actor-critic method for statistically optimised dialogue systems. First, we show that, on summary state and action spaces, deep Reinforcement Learning (RL) outperforms Gaussian Processes methods. Summary state and action spaces lead to good performance but require pre-engineering effort, RL knowledge, and domain expertise. In order to remove the need to define such summary spaces, we show that deep RL can also be trained efficiently on the original state and action spaces. Dialogue systems based on partially observable Markov decision processes are known to require many dialogues to train, which makes them unappealing for practical deployment. We show that a deep RL method based on an actor-critic architecture can exploit a small amount of data very efficiently. Indeed, with only a few hundred dialogues collected with a handcrafted policy, the actor-critic deep learner is considerably bootstrapped from a combination of supervised and batch RL. In addition, convergence to an optimal policy is significantly sped up compared to other deep RL methods initialized on the data with batch RL. All experiments are performed on a restaurant domain derived from the Dialogue State Tracking Challenge 2 (DSTC2) dataset.\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03254",
        "title": "Natural Language Generation enhances human decision-making with uncertain information",
        "authors": [
            "Dimitra Gkatzia",
            "Oliver Lemon",
            "Verena Rieser"
        ],
        "abstract": "Decision-making is often dependent on uncertain data, e.g. data associated with confidence scores or probabilities. We present a comparison of different information presentations for uncertain data and, for the first time, measure their effects on human decision-making. We show that the use of Natural Language Generation (NLG) improves decision-making under uncertainty, compared to state-of-the-art graphical-based representation methods. In a task-based study with 442 adults, we found that presentations using NLG lead to 24% better decision-making on average than the graphical presentations, and to 44% better decision-making when NLG is combined with graphics. We also show that women achieve significantly better results when presented with NLG output (an 87% increase on average compared to graphical presentations).\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03335",
        "title": "WordNet2Vec: Corpora Agnostic Word Vectorization Method",
        "authors": [
            "Roman Bartusiak",
            "\u0141ukasz Augustyniak",
            "Tomasz Kajdanowicz",
            "Przemys\u0142aw Kazienko",
            "Maciej Piasecki"
        ],
        "abstract": "A complex nature of big data resources demands new methods for structuring especially for textual content. WordNet is a good knowledge source for comprehensive abstraction of natural language as its good implementations exist for many languages. Since WordNet embeds natural language in the form of a complex network, a transformation mechanism WordNet2Vec is proposed in the paper. It creates vectors for each word from WordNet. These vectors encapsulate general position - role of a given word towards all other words in the natural language. Any list or set of such vectors contains knowledge about the context of its component within the whole language. Such word representation can be easily applied to many analytic tasks like classification or clustering. The usefulness of the WordNet2Vec method was demonstrated in sentiment analysis, i.e. classification with transfer learning for the real Amazon opinion textual dataset.\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03432",
        "title": "Scan Order in Gibbs Sampling: Models in Which it Matters and Bounds on How Much",
        "authors": [
            "Bryan He",
            "Christopher De Sa",
            "Ioannis Mitliagkas",
            "Christopher R\u00e9"
        ],
        "abstract": "Gibbs sampling is a Markov Chain Monte Carlo sampling technique that iteratively samples variables from their conditional distributions. There are two common scan orders for the variables: random scan and systematic scan. Due to the benefits of locality in hardware, systematic scan is commonly used, even though most statistical guarantees are only for random scan. While it has been conjectured that the mixing times of random scan and systematic scan do not differ by more than a logarithmic factor, we show by counterexample that this is not the case, and we prove that that the mixing times do not differ by more than a polynomial factor under mild conditions. To prove these relative bounds, we introduce a method of augmenting the state space to study systematic scan using conductance.\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03475",
        "title": "De-identification of Patient Notes with Recurrent Neural Networks",
        "authors": [
            "Franck Dernoncourt",
            "Ji Young Lee",
            "Ozlem Uzuner",
            "Peter Szolovits"
        ],
        "abstract": "Objective: Patient notes in electronic health records (EHRs) may contain critical information for medical investigations. However, the vast majority of medical investigators can only access de-identified notes, in order to protect the confidentiality of patients. In the United States, the Health Insurance Portability and Accountability Act (HIPAA) defines 18 types of protected health information (PHI) that needs to be removed to de-identify patient notes. Manual de-identification is impractical given the size of EHR databases, the limited number of researchers with access to the non-de-identified notes, and the frequent mistakes of human annotators. A reliable automated de-identification system would consequently be of high value.\n",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03476",
        "title": "Generative Adversarial Imitation Learning",
        "authors": [
            "Jonathan Ho",
            "Stefano Ermon"
        ],
        "abstract": "Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03490",
        "title": "The Mythos of Model Interpretability",
        "authors": [
            "Zachary C. Lipton"
        ],
        "abstract": "Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2017-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03568",
        "title": "Word Sense Disambiguation using a Bidirectional LSTM",
        "authors": [
            "Mikael K\u00e5geb\u00e4ck",
            "Hans Salomonsson"
        ],
        "abstract": "In this paper we present a clean, yet effective, model for word sense disambiguation. Our approach leverage a bidirectional long short-term memory network which is shared between all words. This enables the model to share statistical strength and to scale well with vocabulary size. The model is trained end-to-end, directly from the raw text to sense labels, and makes effective use of word order. We evaluate our approach on two standard datasets, using identical hyperparameter settings, which are in turn tuned on a third set of held out data. We employ no external resources (e.g. knowledge graphs, part-of-speech tagging, etc), language specific features, or hand crafted rules, but still achieve statistically equivalent results to the best state-of-the-art systems, that employ no such limitations.\n    ",
        "submission_date": "2016-06-11T00:00:00",
        "last_modified_date": "2016-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03667",
        "title": "Deep Reinforcement Learning with a Combinatorial Action Space for Predicting Popular Reddit Threads",
        "authors": [
            "Ji He",
            "Mari Ostendorf",
            "Xiaodong He",
            "Jianshu Chen",
            "Jianfeng Gao",
            "Lihong Li",
            "Li Deng"
        ],
        "abstract": "We introduce an online popularity prediction and tracking task as a benchmark task for reinforcement learning with a combinatorial, natural language action space. A specified number of discussion threads predicted to be popular are recommended, chosen from a fixed window of recent comments to track. Novel deep reinforcement learning architectures are studied for effective modeling of the value function associated with actions comprised of interdependent sub-actions. The proposed model, which represents dependence between sub-actions through a bi-directional LSTM, gives the best performance across different experimental configurations and domains, and it also generalizes well with varying numbers of recommendation requests.\n    ",
        "submission_date": "2016-06-12T00:00:00",
        "last_modified_date": "2016-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03777",
        "title": "Neural Belief Tracker: Data-Driven Dialogue State Tracking",
        "authors": [
            "Nikola Mrk\u0161i\u0107",
            "Diarmuid \u00d3 S\u00e9aghdha",
            "Tsung-Hsien Wen",
            "Blaise Thomson",
            "Steve Young"
        ],
        "abstract": "One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user's goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users' language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.\n    ",
        "submission_date": "2016-06-12T00:00:00",
        "last_modified_date": "2017-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03860",
        "title": "Robust Probabilistic Modeling with Bayesian Data Reweighting",
        "authors": [
            "Yixin Wang",
            "Alp Kucukelbir",
            "David M. Blei"
        ],
        "abstract": "Probabilistic models analyze data by relying on a set of assumptions. Data that exhibit deviations from these assumptions can undermine inference and prediction quality. Robust models offer protection against mismatch between a model's assumptions and reality. We propose a way to systematically detect and mitigate mismatch of a large class of probabilistic models. The idea is to raise the likelihood of each observation to a weight and then to infer both the latent variables and the weights from data. Inferring the weights allows a model to identify observations that match its assumptions and down-weight others. This enables robust inference and improves predictive accuracy. We study four different forms of mismatch with reality, ranging from missing latent groups to structure misspecification. A Poisson factorization analysis of the Movielens 1M dataset shows the benefits of this approach in a practical scenario.\n    ",
        "submission_date": "2016-06-13T00:00:00",
        "last_modified_date": "2018-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03864",
        "title": "Neural Associative Memory for Dual-Sequence Modeling",
        "authors": [
            "Dirk Weissenborn"
        ],
        "abstract": "Many important NLP problems can be posed as dual-sequence or sequence-to-sequence modeling tasks. Recent advances in building end-to-end neural architectures have been highly successful in solving such tasks. In this work we propose a new architecture for dual-sequence modeling that is based on associative memory. We derive AM-RNNs, a recurrent associative memory (AM) which augments generic recurrent neural networks (RNN). This architecture is extended to the Dual AM-RNN which operates on two AMs at once. Our models achieve very competitive results on textual entailment. A qualitative analysis demonstrates that long range dependencies between source and target-sequence can be bridged effectively using Dual AM-RNNs. However, an initial experiment on auto-encoding reveals that these benefits are not exploited by the system when learning to solve sequence-to-sequence tasks which indicates that additional supervision or regularization is needed.\n    ",
        "submission_date": "2016-06-13T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03968",
        "title": "Visual-Inertial-Semantic Scene Representation for 3-D Object Detection",
        "authors": [
            "Jingming Dong",
            "Xiaohan Fei",
            "Stefano Soatto"
        ],
        "abstract": "We describe a system to detect objects in three-dimensional space using video and inertial sensors (accelerometer and gyrometer), ubiquitous in modern mobile platforms from phones to drones. Inertials afford the ability to impose class-specific scale priors for objects, and provide a global orientation reference. A minimal sufficient representation, the posterior of semantic (identity) and syntactic (pose) attributes of objects in space, can be decomposed into a geometric term, which can be maintained by a localization-and-mapping filter, and a likelihood function, which can be approximated by a discriminatively-trained convolutional neural network. The resulting system can process the video stream causally in real time, and provides a representation of objects in the scene that is persistent: Confidence in the presence of objects grows with evidence, and objects previously seen are kept in memory even when temporarily occluded, with their return into view automatically predicted to prime re-detection.\n    ",
        "submission_date": "2016-06-13T00:00:00",
        "last_modified_date": "2017-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03976",
        "title": "Estimating individual treatment effect: generalization bounds and algorithms",
        "authors": [
            "Uri Shalit",
            "Fredrik D. Johansson",
            "David Sontag"
        ],
        "abstract": "There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a \"balanced\" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.\n    ",
        "submission_date": "2016-06-13T00:00:00",
        "last_modified_date": "2017-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04055",
        "title": "A modified single and multi-objective bacteria foraging optimization for the solution of quadratic assignment problem",
        "authors": [
            "Saeid Parvandeh",
            "Parya Soltani",
            "Mohammadreza Boroumand",
            "Fahimeh Boroumand"
        ],
        "abstract": "Non-polynomial hard (NP-hard) problems are challenging because no polynomial-time algorithm has yet been discovered to solve them in polynomial time. The Bacteria Foraging Optimization (BFO) algorithm is one of the metaheuristics algorithms that is mostly used for NP-hard problems. BFO is inspired by the behavior of the bacteria foraging such as Escherichia coli (E-coli). The aim of BFO is to eliminate those bacteria that have weak foraging properties and maintain those bacteria that have breakthrough foraging properties toward the optimum. Despite the strength of this algorithm, most of the problems reaching optimal solutions are time-demanding or impossible. In this paper, we modified single objective BFO by adding a mutation operator and multi-objective BFO (MOBFO) by adding mutation and crossover from genetic algorithm operators to update the solutions in each generation, and local tabu search algorithm to reach the local optimum solution. Additionally, we used a fast nondominated sort algorithm in MOBFO to find the best-nondominated solutions in each generation. We evaluated the performance of the proposed algorithms through a number of single and multi-objective Quadratic Assignment Problem (QAP) instances. The experimental results show that our approaches outperform some previous optimization algorithms in both convergent and divergent solutions.\n    ",
        "submission_date": "2016-06-13T00:00:00",
        "last_modified_date": "2020-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04165",
        "title": "Using Virtual Humans to Understand Real Ones",
        "authors": [
            "Katie Hoemann",
            "Behnaz Rezaei",
            "Stacy C. Marsella",
            "Sarah Ostadabbas"
        ],
        "abstract": "Human interactions are characterized by explicit as well as implicit channels of communication. While the explicit channel transmits overt messages, the implicit ones transmit hidden messages about the communicator (e.g., his/her intentions and attitudes). There is a growing consensus that providing a computer with the ability to manipulate implicit affective cues should allow for a more meaningful and natural way of studying particular non-verbal signals of human-human communications by human-computer interactions. In this pilot study, we created a non-dynamic human-computer interaction while manipulating three specific non-verbal channels of communication: gaze pattern, facial expression, and gesture. Participants rated the virtual agent on affective dimensional scales (pleasure, arousal, and dominance) while their physiological signal (electrodermal activity, EDA) was captured during the interaction. Assessment of the behavioral data revealed a significant and complex three-way interaction between gaze, gesture, and facial configuration on the dimension of pleasure, as well as a main effect of gesture on the dimension of dominance. These results suggest a complex relationship between different non-verbal cues and the social context in which they are interpreted. Qualifying considerations as well as possible next steps are further discussed in light of these exploratory findings.\n    ",
        "submission_date": "2016-06-13T00:00:00",
        "last_modified_date": "2016-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04327",
        "title": "Entropy/IP: Uncovering Structure in IPv6 Addresses",
        "authors": [
            "Pawel Foremski",
            "David Plonka",
            "Arthur Berger"
        ],
        "abstract": "In this paper, we introduce Entropy/IP: a system that discovers Internet address structure based on analyses of a subset of IPv6 addresses known to be active, i.e., training data, gleaned by readily available passive and active means. The system is completely automated and employs a combination of information-theoretic and machine learning techniques to probabilistically model IPv6 addresses. We present results showing that our system is effective in exposing structural characteristics of portions of the IPv6 Internet address space populated by active client, service, and router addresses.\n",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04414",
        "title": "The Parallel Knowledge Gradient Method for Batch Bayesian Optimization",
        "authors": [
            "Jian Wu",
            "Peter I. Frazier"
        ],
        "abstract": "In many applications of black-box optimization, one can evaluate multiple points simultaneously, e.g. when evaluating the performances of several different neural network architectures in a parallel computing environment. In this paper, we develop a novel batch Bayesian optimization algorithm --- the parallel knowledge gradient method. By construction, this method provides the one-step Bayes-optimal batch of points to sample. We provide an efficient strategy for computing this Bayes-optimal batch of points, and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch Bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms, especially when function evaluations are noisy.\n    ",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2018-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04615",
        "title": "Deep Reinforcement Learning With Macro-Actions",
        "authors": [
            "Ishan P. Durugkar",
            "Clemens Rosenbaum",
            "Stefan Dernbach",
            "Sridhar Mahadevan"
        ],
        "abstract": "Deep reinforcement learning has been shown to be a powerful framework for learning policies from complex high-dimensional sensory inputs to actions in complex tasks, such as the Atari domain. In this paper, we explore output representation modeling in the form of temporal abstraction to improve convergence and reliability of deep reinforcement learning approaches. We concentrate on macro-actions, and evaluate these on different Atari 2600 games, where we show that they yield significant improvements in learning speed. Additionally, we show that they can even achieve better scores than DQN. We offer analysis and explanation for both convergence and final results, revealing a problem deep RL approaches have with sparse reward signals.\n    ",
        "submission_date": "2016-06-15T00:00:00",
        "last_modified_date": "2016-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04686",
        "title": "Natural Language Generation as Planning under Uncertainty Using Reinforcement Learning",
        "authors": [
            "Verena Rieser",
            "Oliver Lemon"
        ],
        "abstract": "We present and evaluate a new model for Natural Language Generation (NLG) in Spoken Dialogue Systems, based on statistical planning, given noisy feedback from the current generation context (e.g. a user and a surface realiser). We study its use in a standard NLG problem: how to present information (in this case a set of search results) to users, given the complex trade- offs between utterance length, amount of information conveyed, and cognitive load. We set these trade-offs by analysing existing MATCH data. We then train a NLG pol- icy using Reinforcement Learning (RL), which adapts its behaviour to noisy feed- back from the current generation context. This policy is compared to several base- lines derived from previous work in this area. The learned policy significantly out- performs all the prior approaches.\n    ",
        "submission_date": "2016-06-15T00:00:00",
        "last_modified_date": "2016-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04753",
        "title": "Safe Exploration in Finite Markov Decision Processes with Gaussian Processes",
        "authors": [
            "Matteo Turchetta",
            "Felix Berkenkamp",
            "Andreas Krause"
        ],
        "abstract": "In classical reinforcement learning, when exploring an environment, agents accept arbitrary short term loss for long term gain. This is infeasible for safety critical applications, such as robotics, where even a single unsafe action may cause system failure. In this paper, we address the problem of safely exploring finite Markov decision processes (MDP). We define safety in terms of an, a priori unknown, safety constraint that depends on states and actions. We aim to explore the MDP under this constraint, assuming that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop a novel algorithm for this task and prove that it is able to completely explore the safely reachable part of the MDP without violating the safety constraint. To achieve this, it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment. Moreover, the algorithm explicitly considers reachability when exploring the MDP, ensuring that it does not get stuck in any state with no safe way out. We demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover.\n    ",
        "submission_date": "2016-06-15T00:00:00",
        "last_modified_date": "2016-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05124",
        "title": "Robust Active Perception via Data-association aware Belief Space planning",
        "authors": [
            "Shashank Pathak",
            "Antony Thomas",
            "Asaf Feniger",
            "Vadim Indelman"
        ],
        "abstract": "We develop a belief space planning (BSP) approach that advances the state of the art by incorporating reasoning about data association (DA) within planning, while considering additional sources of uncertainty. Existing BSP approaches typically assume data association is given and perfect, an assumption that can be harder to justify while operating, in the presence of localization uncertainty, in ambiguous and perceptually aliased environments. In contrast, our data association aware belief space planning (DA-BSP) approach explicitly reasons about DA within belief evolution, and as such can better accommodate these challenging real world scenarios. In particular, we show that due to perceptual aliasing, the posterior belief becomes a mixture of probability distribution functions, and design cost functions that measure the expected level of ambiguity and posterior uncertainty. Using these and standard costs (e.g.~control penalty, distance to goal) within the objective function, yields a general framework that reliably represents action impact, and in particular, capable of active disambiguation. Our approach is thus applicable to robust active perception and autonomous navigation in perceptually aliased environments. We demonstrate key aspects in basic and realistic simulations.\n    ",
        "submission_date": "2016-06-16T00:00:00",
        "last_modified_date": "2016-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05313",
        "title": "Unsupervised Risk Estimation Using Only Conditional Independence Structure",
        "authors": [
            "Jacob Steinhardt",
            "Percy Liang"
        ],
        "abstract": "We show how to estimate a model's test error from unlabeled data, on distributions very different from the training distribution, while assuming only that certain conditional independencies are preserved between train and test. We do not need to assume that the optimal predictor is the same between train and test, or that the true distribution lies in any parametric family. We can also efficiently differentiate the error estimate to perform unsupervised discriminative learning. Our technical tool is the method of moments, which allows us to exploit conditional independencies in the absence of a fully-specified model. Our framework encompasses a large family of losses including the log and exponential loss, and extends to structured output settings such as hidden Markov models.\n    ",
        "submission_date": "2016-06-16T00:00:00",
        "last_modified_date": "2016-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05336",
        "title": "On the Expressive Power of Deep Neural Networks",
        "authors": [
            "Maithra Raghu",
            "Ben Poole",
            "Jon Kleinberg",
            "Surya Ganguli",
            "Jascha Sohl-Dickstein"
        ],
        "abstract": "We propose a new approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute. Our approach is based on an interrelated set of measures of expressivity, unified by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path. Our findings can be summarized as follows:\n",
        "submission_date": "2016-06-16T00:00:00",
        "last_modified_date": "2017-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05427",
        "title": "Proceedings First International Workshop on Hammers for Type Theories",
        "authors": [
            "Jasmin Christian Blanchette",
            "Cezary Kaliszyk"
        ],
        "abstract": "This volume of EPTCS contains the proceedings of the First Workshop on Hammers for Type Theories (HaTT 2016), held on 1 July 2016 as part of the International Joint Conference on Automated Reasoning (IJCAR 2016) in Coimbra, Portugal. The proceedings contain four regular papers, as well as abstracts of the two invited talks by Pierre Corbineau (Verimag, France) and Aleksy Schubert (University of Warsaw, Poland).\n\n    ",
        "submission_date": "2016-06-17T00:00:00",
        "last_modified_date": "2016-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05468",
        "title": "Most central or least central? How much modeling decisions influence a node's centrality ranking in multiplex networks",
        "authors": [
            "Sude Tavassoli",
            "Katharina Anna Zweig"
        ],
        "abstract": "To understand a node's centrality in a multiplex network, its centrality values in all the layers of the network can be aggregated. This requires a normalization of the values, to allow their meaningful comparison and aggregation over networks with different sizes and orders. The concrete choices of such preprocessing steps like normalization and aggregation are almost never discussed in network analytic papers. In this paper, we show that even sticking to the most simple centrality index (the degree) but using different, classic choices of normalization and aggregation strategies, can turn a node from being among the most central to being among the least central. We present our results by using an aggregation operator which scales between different, classic aggregation strategies based on three multiplex networks. We also introduce a new visualization and characterization of a node's sensitivity to the choice of a normalization and aggregation strategy in multiplex networks. The observed high sensitivity of single nodes to the specific choice of aggregation and normalization strategies is of strong importance, especially for all kinds of intelligence-analytic software as it questions the interpretations of the findings.\n    ",
        "submission_date": "2016-06-17T00:00:00",
        "last_modified_date": "2016-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05506",
        "title": "Learning Abstract Classes using Deep Learning",
        "authors": [
            "Sebastian Stabinger",
            "Antonio Rodriguez-Sanchez",
            "Justus Piater"
        ],
        "abstract": "Humans are generally good at learning abstract concepts about objects and scenes (e.g.\\ spatial orientation, relative sizes, etc.). Over the last years convolutional neural networks have achieved almost human performance in recognizing concrete classes (i.e.\\ specific object categories). This paper tests the performance of a current CNN (GoogLeNet) on the task of differentiating between abstract classes which are trivially differentiable for humans. We trained and tested the CNN on the two abstract classes of horizontal and vertical orientation and determined how well the network is able to transfer the learned classes to other, previously unseen objects.\n    ",
        "submission_date": "2016-06-17T00:00:00",
        "last_modified_date": "2016-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05611",
        "title": "Data-driven HR - R\u00e9sum\u00e9 Analysis Based on Natural Language Processing and Machine Learning",
        "authors": [
            "Tim Zimmermann",
            "Leo Kotschenreuther",
            "Karsten Schmidt"
        ],
        "abstract": "Recruiters usually spend less than a minute looking at each r\u00e9sum\u00e9 when deciding whether it's worth continuing the recruitment process with the candidate. Recruiters focus on keywords, and it's almost impossible to guarantee a fair process of candidate selection. The main scope of this paper is to tackle this issue by introducing a data-driven approach that shows how to process r\u00e9sum\u00e9s automatically and give recruiters more time to only examine promising candidates. Furthermore, we show how to leverage Machine Learning and Natural Language Processing in order to extract all required information from the r\u00e9sum\u00e9s. Once the information is extracted, a ranking score is calculated. The score describes how well the candidates fit based on their education, work experience and skills. Later this paper illustrates a prototype application that shows how this novel approach can increase the productivity of recruiters. The application enables them to filter and rank candidates based on predefined job descriptions. Guided by the ranking, recruiters can get deeper insights from candidate profiles and validate why and how the application ranked them. This application shows how to improve the hiring process by giving an unbiased hiring decision support.\n    ",
        "submission_date": "2016-06-17T00:00:00",
        "last_modified_date": "2016-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05725",
        "title": "An Efficient Large-scale Semi-supervised Multi-label Classifier Capable of Handling Missing labels",
        "authors": [
            "Amirhossein Akbarnejad",
            "Mahdieh Soleymani Baghshah"
        ],
        "abstract": "Multi-label classification has received considerable interest in recent years. Multi-label classifiers have to address many problems including: handling large-scale datasets with many instances and a large set of labels, compensating missing label assignments in the training set, considering correlations between labels, as well as exploiting unlabeled data to improve prediction performance. To tackle datasets with a large set of labels, embedding-based methods have been proposed which seek to represent the label assignments in a low-dimensional space. Many state-of-the-art embedding-based methods use a linear dimensionality reduction to represent the label assignments in a low-dimensional space. However, by doing so, these methods actually neglect the tail labels - labels that are infrequently assigned to instances. We propose an embedding-based method that non-linearly embeds the label vectors using an stochastic approach, thereby predicting the tail labels more accurately. Moreover, the proposed method have excellent mechanisms for handling missing labels, dealing with large-scale datasets, as well as exploiting unlabeled data. With the best of our knowledge, our proposed method is the first multi-label classifier that simultaneously addresses all of the mentioned challenges. Experiments on real-world datasets show that our method outperforms stateof-the-art multi-label classifiers by a large margin, in terms of prediction performance, as well as training time.\n    ",
        "submission_date": "2016-06-18T00:00:00",
        "last_modified_date": "2016-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05735",
        "title": "A Comparative Analysis of classification data mining techniques : Deriving key factors useful for predicting students performance",
        "authors": [
            "Muhammed Salman Shamsi",
            "Jhansi Lakshmi"
        ],
        "abstract": "Students opting for Engineering as their discipline is increasing rapidly. But due to various factors and inappropriate primary education in India, failure rates are high. Students are unable to excel in core engineering because of complex and mathematical subjects. Hence, they fail in such subjects. With the help of data mining techniques, we can predict the performance of students in terms of grades and failure in subjects. This paper performs a comparative analysis of various classification techniques, such as Na\u00efve Bayes, LibSVM, J48, Random Forest, and JRip and tries to choose best among these. Based on the results obtained, we found that Na\u00efve Bayes is the most accurate method in terms of students failure prediction and JRip is most accurate in terms of students grade prediction. We also found that JRip marginally differs from Na\u00efve Bayes in terms of accuracy for students failure prediction and gives us a set of rules from which we derive the key factors influencing students performance. Finally, we suggest various ways to mitigate these factors. This study is limited to Indian Education system scenarios. However, the factors found can be helpful in other scenarios as well.\n    ",
        "submission_date": "2016-06-18T00:00:00",
        "last_modified_date": "2016-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06031",
        "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context",
        "authors": [
            "Denis Paperno",
            "Germ\u00e1n Kruszewski",
            "Angeliki Lazaridou",
            "Quan Ngoc Pham",
            "Raffaella Bernardi",
            "Sandro Pezzelle",
            "Marco Baroni",
            "Gemma Boleda",
            "Raquel Fern\u00e1ndez"
        ],
        "abstract": "We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06197",
        "title": "Polymetric Rhythmic Feel for a Cognitive Drum Computer",
        "authors": [
            "Oliver Weede"
        ],
        "abstract": "This paper addresses a question about music cognition: how do we derive polymetric structures. A preference rule system is presented which is implemented into a drum computer. The preference rule system allows inferring local polymetric structures, like two-over-three and three-over-two. By analyzing the micro-timing of West African percussion music a timing pattern consisting of six pulses was discovered. It integrates binary and ternary rhythmic feels. The presented drum computer integrates the discovered superimposed polymetric swing (timing and velocity) appropriate to the rhythmic sequence the user inputs. For binary sequences, the amount of binary swing is increased and for ternary sequences, the ternary swing is increased.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06269",
        "title": "Founded Semantics and Constraint Semantics of Logic Rules",
        "authors": [
            "Yanhong A. Liu",
            "Scott D. Stoller"
        ],
        "abstract": "Logic rules and inference are fundamental in computer science and have been studied extensively. However, prior semantics of logic languages can have subtle implications and can disagree significantly, on even very simple programs, including in attempting to solve the well-known Russell's paradox. These semantics are often non-intuitive and hard-to-understand when unrestricted negation is used in recursion.\n",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2020-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06368",
        "title": "Unanimous Prediction for 100% Precision with Application to Learning Semantic Mappings",
        "authors": [
            "Fereshte Khani",
            "Martin Rinard",
            "Percy Liang"
        ],
        "abstract": "Can we train a system that, on any new input, either says \"don't know\" or makes a prediction that is guaranteed to be correct? We answer the question in the affirmative provided our model family is well-specified. Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operationalize this principle for semantic parsing, the task of mapping utterances to logical forms. We develop a simple, efficient method that reasons over the infinite set of all consistent models by only checking two of the models. We prove that our method obtains 100% precision even with a modest amount of training data from a possibly adversarial distribution. Empirically, we demonstrate the effectiveness of our approach on the standard GeoQuery dataset.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06461",
        "title": "Neighborhood Mixture Model for Knowledge Base Completion",
        "authors": [
            "Dat Quoc Nguyen",
            "Kairit Sirts",
            "Lizhen Qu",
            "Mark Johnson"
        ],
        "abstract": "Knowledge bases are useful resources for many natural language processing tasks, however, they are far from complete. In this paper, we define a novel entity representation as a mixture of its neighborhood in the knowledge base and apply this technique on TransE-a well-known embedding model for knowledge base completion. Experimental results show that the neighborhood information significantly helps to improve the results of the TransE model, leading to better performance than obtained by other state-of-the-art embedding models on three benchmark datasets for triple classification, entity prediction and relation prediction tasks.\n    ",
        "submission_date": "2016-06-21T00:00:00",
        "last_modified_date": "2017-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06512",
        "title": "Graphical Models for Optimal Power Flow",
        "authors": [
            "Krishnamurthy Dvijotham",
            "Pascal Van Hentenryck",
            "Michael Chertkov",
            "Sidhant Misra",
            "Marc Vuffray"
        ],
        "abstract": "Optimal power flow (OPF) is the central optimization problem in electric power grids. Although solved routinely in the course of power grid operations, it is known to be strongly NP-hard in general, and weakly NP-hard over tree networks. In this paper, we formulate the optimal power flow problem over tree networks as an inference problem over a tree-structured graphical model where the nodal variables are low-dimensional vectors. We adapt the standard dynamic programming algorithm for inference over a tree-structured graphical model to the OPF problem. Combining this with an interval discretization of the nodal variables, we develop an approximation algorithm for the OPF problem. Further, we use techniques from constraint programming (CP) to perform interval computations and adaptive bound propagation to obtain practically efficient algorithms. Compared to previous algorithms that solve OPF with optimality guarantees using convex relaxations, our approach is able to work for arbitrary distribution networks and handle mixed-integer optimization problems. Further, it can be implemented in a distributed message-passing fashion that is scalable and is suitable for \"smart grid\" applications like control of distributed energy resources. We evaluate our technique numerically on several benchmark networks and show that practical OPF problems can be solved effectively using this approach.\n    ",
        "submission_date": "2016-06-21T00:00:00",
        "last_modified_date": "2016-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06897",
        "title": "An effective approach for classification of advanced malware with high accuracy",
        "authors": [
            "Ashu Sharma",
            "Sanjay K. Sahay"
        ],
        "abstract": "Combating malware is very important for software/systems security, but to prevent the software/systems from the advanced malware, viz. metamorphic malware is a challenging task, as it changes the structure/code after each infection. Therefore in this paper, we present a novel approach to detect the advanced malware with high accuracy by analyzing the occurrence of opcodes (features) by grouping the executables. These groups are made on the basis of our earlier studies [1] that the difference between the sizes of any two malware generated by popular advanced malware kits viz. PS-MPC, G2 and NGVCK are within 5 KB. On the basis of obtained promising features, we studied the performance of thirteen classifiers using N-fold cross-validation available in machine learning tool WEKA. Among these thirteen classifiers we studied in-depth top five classifiers (Random forest, LMT, NBT, J48 and FT) and obtain more than 96.28% accuracy for the detection of unknown malware, which is better than the maximum detection accuracy (95.9%) reported by Santos et al (2013). In these top five classifiers, our approach obtained a detection accuracy of 97.95% by the Random forest.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06900",
        "title": "Inferring Logical Forms From Denotations",
        "authors": [
            "Panupong Pasupat",
            "Percy Liang"
        ],
        "abstract": "A core problem in learning semantic parsers from denotations is picking out consistent logical forms--those that yield the correct denotation--from a combinatorially large space. To control the search space, previous work relied on restricted set of rules, which limits expressivity. In this paper, we consider a much more expressive class of logical forms, and show how to use dynamic programming to efficiently represent the complete set of consistent logical forms. Expressivity also introduces many more spurious logical forms which are consistent with the correct denotation but do not represent the meaning of the utterance. To address this, we generate fictitious worlds and use crowdsourced denotations on these worlds to filter out spurious logical forms. On the WikiTableQuestions dataset, we increase the coverage of answerable questions from 53.5% to 76%, and the additional crowdsourced supervision lets us rule out 92.1% of spurious logical forms.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2016-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06908",
        "title": "Grouping the executables to detect malware with high accuracy",
        "authors": [
            "Sanjay K. Sahay",
            "Ashu Sharma"
        ],
        "abstract": "The metamorphic malware variants with the same malicious behavior (family), can obfuscate themselves to look different from each other. This variation in structure leads to a huge signature database for traditional signature matching techniques to detect them. In order to effective and efficient detection of malware in large amounts of executables, we need to partition these files into groups which can identify their respective families. In addition, the grouping criteria should be chosen such a way that, it can also be applied to unknown files encounter on computers for classification. This paper discusses the study of malware and benign executables in groups to detect unknown malware with high accuracy. We studied sizes of malware generated by three popular second generation malware (metamorphic malware) creator kits viz. G2, PS-MPC and NGVCK, and observed that the size variation in any two generated malware from same kit is not much. Hence, we grouped the executables on the basis of malware sizes by using Optimal k-Means Clustering algorithm and used these obtained groups to select promising features for training (Random forest, J48, LMT, FT and NBT) classifiers to detect variants of malware or unknown malware. We find that detection of malware on the basis of their respected file sizes gives accuracy up to 99.11% from the classifiers.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06979",
        "title": "Simultaneous Control and Human Feedback in the Training of a Robotic Agent with Actor-Critic Reinforcement Learning",
        "authors": [
            "Kory W. Mathewson",
            "Patrick M. Pilarski"
        ],
        "abstract": "This paper contributes a preliminary report on the advantages and disadvantages of incorporating simultaneous human control and feedback signals in the training of a reinforcement learning robotic agent. While robotic human-machine interfaces have become increasingly complex in both form and function, control remains challenging for users. This has resulted in an increasing gap between user control approaches and the number of robotic motors which can be controlled. One way to address this gap is to shift some autonomy to the robot. Semi-autonomous actions of the robotic agent can then be shaped by human feedback, simplifying user control. Most prior work on agent shaping by humans has incorporated training with feedback, or has included indirect control signals. By contrast, in this paper we explore how a human can provide concurrent feedback signals and real-time myoelectric control signals to train a robot's actor-critic reinforcement learning control system. Using both a physical and a simulated robotic system, we compare training performance on a simple movement task when reward is derived from the environment, when reward is provided by the human, and combinations of these two approaches. Our results indicate that some benefit can be gained with the inclusion of human generated feedback.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07025",
        "title": "Efficient Attack Graph Analysis through Approximate Inference",
        "authors": [
            "Luis Mu\u00f1oz-Gonz\u00e1lez",
            "Daniele Sgandurra",
            "Andrea Paudice",
            "Emil C. Lupu"
        ],
        "abstract": "Attack graphs provide compact representations of the attack paths that an attacker can follow to compromise network resources by analysing network vulnerabilities and topology. These representations are a powerful tool for security risk assessment. Bayesian inference on attack graphs enables the estimation of the risk of compromise to the system's components given their vulnerabilities and interconnections, and accounts for multi-step attacks spreading through the system. Whilst static analysis considers the risk posture at rest, dynamic analysis also accounts for evidence of compromise, e.g. from SIEM software or forensic investigation. However, in this context, exact Bayesian inference techniques do not scale well. In this paper we show how Loopy Belief Propagation - an approximate inference technique - can be applied to attack graphs, and that it scales linearly in the number of nodes for both static and dynamic analysis, making such analyses viable for larger networks. We experiment with different topologies and network clustering on synthetic Bayesian attack graphs with thousands of nodes to show that the algorithm's accuracy is acceptable and converge to a stable solution. We compare sequential and parallel versions of Loopy Belief Propagation with exact inference techniques for both static and dynamic analysis, showing the advantages of approximate inference techniques to scale to larger attack graphs.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07035",
        "title": "Ancestral Causal Inference",
        "authors": [
            "Sara Magliacane",
            "Tom Claassen",
            "Joris M. Mooij"
        ],
        "abstract": "Constraint-based causal discovery from limited data is a notoriously difficult challenge due to the many borderline independence test decisions. Several approaches to improve the reliability of the predictions by exploiting redundancy in the independence information have been proposed recently. Though promising, existing approaches can still be greatly improved in terms of accuracy and scalability. We present a novel method that reduces the combinatorial explosion of the search space by using a more coarse-grained representation of causal information, drastically reducing computation time. Additionally, we propose a method to score causal predictions based on their confidence. Crucially, our implementation also allows one to easily combine observational and interventional data and to incorporate various types of available background knowledge. We prove soundness and asymptotic consistency of our method and demonstrate that it can outperform the state-of-the-art on synthetic data, achieving a speedup of several orders of magnitude. We illustrate its practical feasibility by applying it on a challenging protein data set.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2017-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07149",
        "title": "An Approach to Stable Gradient Descent Adaptation of Higher-Order Neural Units",
        "authors": [
            "Ivo Bukovsky",
            "Noriyasu Homma"
        ],
        "abstract": "Stability evaluation of a weight-update system of higher-order neural units (HONUs) with polynomial aggregation of neural inputs (also known as classes of polynomial neural networks) for adaptation of both feedforward and recurrent HONUs by a gradient descent method is introduced. An essential core of the approach is based on spectral radius of a weight-update system, and it allows stability monitoring and its maintenance at every adaptation step individually. Assuring stability of the weight-update system (at every single adaptation step) naturally results in adaptation stability of the whole neural architecture that adapts to target data. As an aside, the used approach highlights the fact that the weight optimization of HONU is a linear problem, so the proposed approach can be generally extended to any neural architecture that is linear in its adaptable parameters.\n    ",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07259",
        "title": "Log-based Evaluation of Label Splits for Process Models",
        "authors": [
            "Niek Tax",
            "Natalia Sidorova",
            "Reinder Haakma",
            "Wil M. P. van der Aalst"
        ],
        "abstract": "Process mining techniques aim to extract insights in processes from event logs. One of the challenges in process mining is identifying interesting and meaningful event labels that contribute to a better understanding of the process. Our application area is mining data from smart homes for elderly, where the ultimate goal is to signal deviations from usual behavior and provide timely recommendations in order to extend the period of independent living. Extracting individual process models showing user behavior is an important instrument in achieving this goal. However, the interpretation of sensor data at an appropriate abstraction level is not straightforward. For example, a motion sensor in a bedroom can be triggered by tossing and turning in bed or by getting up. We try to derive the actual activity depending on the context (time, previous events, etc.). In this paper we introduce the notion of label refinements, which links more abstract event descriptions with their more refined counterparts. We present a statistical evaluation method to determine the usefulness of a label refinement for a given event log from a process perspective. Based on data from smart homes, we show how our statistical evaluation method for label refinements can be used in practice. Our method was able to select two label refinements out of a set of candidate label refinements that both had a positive effect on model precision.\n    ",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07282",
        "title": "A review of Gaussian Markov models for conditional independence",
        "authors": [
            "Irene C\u00f3rdoba",
            "Concha Bielza",
            "Pedro Larra\u00f1aga"
        ],
        "abstract": "Markov models lie at the interface between statistical independence in a probability distribution and graph separation properties. We review model selection and estimation in directed and undirected Markov models with Gaussian parametrization, emphasizing the main similarities and differences. These two model classes are similar but not equivalent, although they share a common intersection. We present the existing results from a historical perspective, taking into account the amount of literature existing from both the artificial intelligence and statistics research communities, where these models were originated. We cover classical topics such as maximum likelihood estimation and model selection via hypothesis testing, but also more modern approaches like regularization and Bayesian methods. We also discuss how the Markov models reviewed fit in the rich hierarchy of other, higher level Markov model classes. Finally, we close the paper overviewing relaxations of the Gaussian assumption and pointing out the main areas of application where these Markov models are nowadays used.\n    ",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2019-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07295",
        "title": "Proceedings Fifteenth Conference on Theoretical Aspects of Rationality and Knowledge",
        "authors": [
            "R Ramanujam"
        ],
        "abstract": "The 15th Conference on Theoretical Aspects of Rationality and Knowledge (TARK) took place in Carnegie Mellon University, Pittsburgh, USA from June 4 to 6, 2015.\n",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07356",
        "title": "Analyzing the Behavior of Visual Question Answering Models",
        "authors": [
            "Aishwarya Agrawal",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "abstract": "Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze two models, one each from two major classes of VQA models -- with-attention and without-attention and show the similarities and differences in the behavior of these models. We also analyze the winning entry of the VQA Challenge 2016.\n",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07384",
        "title": "Robust Learning of Fixed-Structure Bayesian Networks",
        "authors": [
            "Yu Cheng",
            "Ilias Diakonikolas",
            "Daniel Kane",
            "Alistair Stewart"
        ],
        "abstract": "We investigate the problem of learning Bayesian networks in a robust model where an $\\epsilon$-fraction of the samples are adversarially corrupted. In this work, we study the fully observable discrete case where the structure of the network is given. Even in this basic setting, previous learning algorithms either run in exponential time or lose dimension-dependent factors in their error guarantees. We provide the first computationally efficient robust learning algorithm for this problem with dimension-independent error guarantees. Our algorithm has near-optimal sample complexity, runs in polynomial time, and achieves error that scales nearly-linearly with the fraction of adversarially corrupted samples. Finally, we show on both synthetic and semi-synthetic data that our algorithm performs well in practice.\n    ",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2018-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07419",
        "title": "Learning to Poke by Poking: Experiential Learning of Intuitive Physics",
        "authors": [
            "Pulkit Agrawal",
            "Ashvin Nair",
            "Pieter Abbeel",
            "Jitendra Malik",
            "Sergey Levine"
        ],
        "abstract": "We investigate an experiential learning paradigm for acquiring an internal model of intuitive physics. Our model is evaluated on a real-world robotic manipulation task that requires displacing objects to target locations by poking. The robot gathered over 400 hours of experience by executing more than 100K pokes on different objects. We propose a novel approach based on deep neural networks for modeling the dynamics of robot's interactions directly from images, by jointly estimating forward and inverse models of dynamics. The inverse model objective provides supervision to construct informative visual features, which the forward model can then predict and in turn regularize the feature space for the inverse model. The interplay between these two objectives creates useful, accurate models that can then be used for multi-step decision making. This formulation has the additional benefit that it is possible to learn forward models in an abstract feature space and thus alleviate the need of predicting pixels. Our experiments show that this joint modeling approach outperforms alternative methods.\n    ",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2017-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07461",
        "title": "LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks",
        "authors": [
            "Hendrik Strobelt",
            "Sebastian Gehrmann",
            "Hanspeter Pfister",
            "Alexander M. Rush"
        ],
        "abstract": "Recurrent neural networks, and in particular long short-term memory (LSTM) networks, are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In this work, we present LSTMVIS, a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. The tool allows users to select a hypothesis input range to focus on local state changes, to match these states changes to similar patterns in a large data set, and to align these results with structural annotations from their domain. We show several use cases of the tool for analyzing specific hidden state properties on dataset containing nesting, phrase structure, and chord progressions, and demonstrate how the tool can be used to isolate patterns for further statistical analysis. We characterize the domain, the different stakeholders, and their goals and tasks.\n    ",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2017-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07487",
        "title": "The VGLC: The Video Game Level Corpus",
        "authors": [
            "Adam James Summerville",
            "Sam Snodgrass",
            "Michael Mateas",
            "Santiago Onta\u00f1\u00f3n"
        ],
        "abstract": "Levels are a key component of many different video games, and a large body of work has been produced on how to procedurally generate game levels. Recently, Machine Learning techniques have been applied to video game level generation towards the purpose of automatically generating levels that have the properties of the training corpus. Towards that end we have made available a corpora of video game levels in an easy to parse format ideal for different machine learning and other game AI research purposes.\n    ",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2016-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07493",
        "title": "Sort Story: Sorting Jumbled Images and Captions into Stories",
        "authors": [
            "Harsh Agrawal",
            "Arjun Chandrasekaran",
            "Dhruv Batra",
            "Devi Parikh",
            "Mohit Bansal"
        ],
        "abstract": "Temporal common sense has applications in AI tasks such as QA, multi-document summarization, and human-AI communication. We propose the task of sequencing -- given a jumbled set of aligned image-caption pairs that belong to a story, the task is to sort them such that the output sequence forms a coherent story. We present multiple approaches, via unary (position) and pairwise (order) predictions, and their ensemble-based combinations, achieving strong results on this task. We use both text-based and image-based features, which depict complementary improvements. Using qualitative examples, we demonstrate that our models have learnt interesting aspects of temporal common sense.\n    ",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2016-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07515",
        "title": "Resolving Distributed Knowledge",
        "authors": [
            "Thomas \u00c5gotnes",
            "Y\u00ec N. W\u00e1ng"
        ],
        "abstract": "Distributed knowledge is the sum of the knowledge in a group; what someone who is able to discern between two possible worlds whenever any member of the group can discern between them, would know. Sometimes distributed knowledge is referred to as the potential knowledge of a group, or the joint knowledge they could obtain if they had unlimited means of communication. In epistemic logic, the formula D_G{\\phi} is intended to express the fact that group G has distributed knowledge of {\\phi}, that there is enough information in the group to infer {\\phi}. But this is not the same as reasoning about what happens if the members of the group share their information. In this paper we introduce an operator R_G, such that R_G{\\phi} means that {\\phi} is true after G have shared all their information with each other - after G's distributed knowledge has been resolved. The R_G operators are called resolution operators. Semantically, we say that an expression R_G{\\phi} is true iff {\\phi} is true in what van Benthem [11, p. 249] calls (G's) communication core; the model update obtained by removing links to states for members of G that are not linked by all members of G. We study logics with different combinations of resolution operators and operators for common and distributed knowledge. Of particular interest is the relationship between distributed and common knowledge. The main results are sound and complete axiomatizations.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07522",
        "title": "Ceteris paribus logic in counterfactual reasoning",
        "authors": [
            "Patrick Girard",
            "Marcus Anthony Triplett"
        ],
        "abstract": "The semantics for counterfactuals due to David Lewis has been challenged on the basis of unlikely, or impossible, events. Such events may skew a given similarity order in favour of those possible worlds which exhibit them. By updating the relational structure of a model according to a ceteris paribus clause one forces out, in a natural manner, those possible worlds which do not satisfy the requirements of the clause. We develop a ceteris paribus logic for counterfactual reasoning capable of performing such actions, and offer several alternative (relaxed) interpretations of ceteris paribus. We apply this framework in a way which allows us to reason counterfactually without having our similarity order skewed by unlikely events. This continues the investigation of formal ceteris paribus reasoning, which has previously been applied to preferences, logics of game forms, and questions in decision-making, among other areas.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07523",
        "title": "An Axiomatic Approach to Routing",
        "authors": [
            "Omer Lev",
            "Moshe Tennenholtz",
            "Aviv Zohar"
        ],
        "abstract": "Information delivery in a network of agents is a key issue for large, complex systems that need to do so in a predictable, efficient manner. The delivery of information in such multi-agent systems is typically implemented through routing protocols that determine how information flows through the network. Different routing protocols exist each with its own benefits, but it is generally unclear which properties can be successfully combined within a given algorithm. We approach this problem from the axiomatic point of view, i.e., we try to establish what are the properties we would seek to see in such a system, and examine the different properties which uniquely define common routing algorithms used today.\n",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07524",
        "title": "Preference at First Sight",
        "authors": [
            "Chanjuan Liu"
        ],
        "abstract": "We consider decision-making and game scenarios in which an agent is limited by his/her computational ability to foresee all the available moves towards the future - that is, we study scenarios with short sight. We focus on how short sight affects the logical properties of decision making in multi-agent settings. We start with  single-agent sequential decision making (SSDM) processes, modeling them by a new structure of \"preference-sight trees\".  Using this model, we first explore the relation between a new natural solution concept of Sight-Compatible Backward Induction  (SCBI) and the histories produced by classical Backward Induction (BI). In particular, we find necessary and sufficient conditions for the two analyses to be equivalent. Next, we study whether larger sight always contributes to better outcomes. Then we develop a simple logical special-purpose language to formally express some key properties of our preference-sight models. Lastly, we show how short-sight SSDM scenarios call for substantial enrichments of existing fixed-point logics that have been developed for the classical BI solution concept. We also discuss changes in earlier modal logics expressing \"surface reasoning\" about best actions in the presence of short sight. Our analysis may point the way to logical and computational analysis of more realistic game models.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07525",
        "title": "Relating Knowledge and Coordinated Action: The Knowledge of Preconditions Principle",
        "authors": [
            "Yoram Moses"
        ],
        "abstract": "The Knowledge of Preconditions principle (KoP) is proposed as a widely applicable connection between knowledge and action in multi-agent systems. Roughly speaking, it asserts that if some condition is a necessary condition for performing a given action A, then knowing that this condition holds is also a necessary condition for performing A.  Since the specifications of tasks often involve necessary conditions for actions, the KoP principle shows that such specifications induce knowledge preconditions for the actions. Distributed protocols or multi-agent plans that satisfy the specifications  must ensure that this knowledge be attained, and that it is  detected by the agents as a condition for action. The knowledge of preconditions principle is formalised in the runs and systems framework, and is proven to hold in a wide class of settings. Well-known connections between knowledge and coordinated action are extended and shown to derive directly from the KoP principle: a  \"common knowledge of preconditions\" principle is established showing that common knowledge is a necessary condition for performing simultaneous actions, and a  \"nested knowledge of preconditions\" principle is proven, showing that coordinating actions to be  performed in linear temporal order requires a corresponding form of nested knowledge. \n\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07526",
        "title": "Parameterized Complexity Results for a Model of Theory of Mind Based on Dynamic Epistemic Logic",
        "authors": [
            "Iris van de Pol",
            "Iris van Rooij",
            "Jakub Szymanik"
        ],
        "abstract": "In this paper we introduce a computational-level model of theory of mind (ToM) based on dynamic epistemic logic (DEL), and we analyze its computational complexity. The model is a special case of DEL model checking. We provide a parameterized complexity analysis, considering several aspects of DEL (e.g., number of agents, size of preconditions, etc.) as parameters. We show that model checking for DEL is PSPACE-hard, also when restricted to single-pointed models and S5 relations, thereby solving an open problem in the literature. Our approach is aimed at formalizing current intractability claims in the cognitive science literature regarding computational models of ToM.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07533",
        "title": "Translucent Players: Explaining Cooperative Behavior in Social Dilemmas",
        "authors": [
            "Valerio Capraro",
            "Joseph Y. Halpern"
        ],
        "abstract": "In the last few decades, numerous experiments have shown that humans do not always behave so as to maximize their material payoff. Cooperative behavior when non-cooperation is a dominant strategy (with respect to the material payoffs) is particularly puzzling. Here we propose a novel approach to explain cooperation, assuming what Halpern and Pass call translucent players. Typically, players are assumed to be opaque, in the sense that a deviation by one player in a normal-form game does not affect the strategies used by other players. But a player may believe that if he switches from one strategy to another, the fact that he chooses to switch may be visible to the other players. For example, if he chooses to defect in Prisoner's Dilemma, the other player may sense his guilt. We show that by assuming translucent players, we can recover many of the regularities observed in human behavior in well-studied games such as Prisoner's Dilemma, Traveler's Dilemma, Bertrand Competition, and the Public Goods game.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07572",
        "title": "Enriching Linked Datasets with New Object Properties",
        "authors": [
            "Subhashree S",
            "P Sreenivasa Kumar"
        ],
        "abstract": "Although several RDF knowledge bases are available through the LOD initiative, the ontology schema of such linked datasets is not very rich. In particular, they lack object properties. The problem of finding new object properties (and their instances) between any two given classes has not been investigated in detail in the context of Linked Data. In this paper, we present DART (Detecting Arbitrary Relations for enriching T-Boxes of Linked Data) - an unsupervised solution to enrich the LOD cloud with new object properties between two given classes. DART exploits contextual similarity to identify text patterns from the web corpus that can potentially represent relations between individuals. These text patterns are then clustered by means of paraphrase detection to capture the object properties between the two given LOD classes. DART also performs fully automated mapping of the discovered relations to the properties in the linked dataset. This serves many purposes such as identification of completely new relations, elimination of irrelevant relations, and generation of prospective property axioms. We have empirically evaluated our approach on several pairs of classes and found that the system can indeed be used for enriching the linked datasets with new object properties and their instances. We compared DART with newOntExt system which is an offshoot of the NELL (Never-Ending Language Learning) effort. Our experiments reveal that DART gives better results than newOntExt with respect to both the correctness, as well as the number of relations.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2017-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07695",
        "title": "Fully DNN-based Multi-label regression for audio tagging",
        "authors": [
            "Yong Xu",
            "Qiang Huang",
            "Wenwu Wang",
            "Philip J. B. Jackson",
            "Mark D. Plumbley"
        ],
        "abstract": "Acoustic event detection for content analysis in most cases relies on lots of labeled data. However, manually annotating data is a time-consuming task, which thus makes few annotated resources available so far. Unlike audio event detection, automatic audio tagging, a multi-label acoustic event classification task, only relies on weakly labeled data. This is highly desirable to some practical applications using audio analysis. In this paper we propose to use a fully deep neural network (DNN) framework to handle the multi-label classification task in a regression way. Considering that only chunk-level rather than frame-level labels are available, the whole or almost whole frames of the chunk were fed into the DNN to perform a multi-label regression for the expected tags. The fully DNN, which is regarded as an encoding function, can well map the audio features sequence to a multi-tag vector. A deep pyramid structure was also designed to extract more robust high-level features related to the target tags. Further improved methods were adopted, such as the Dropout and background noise aware training, to enhance its generalization capability for new audio recordings in mismatched environments. Compared with the conventional Gaussian Mixture Model (GMM) and support vector machine (SVM) methods, the proposed fully DNN-based method could well utilize the long-term temporal information with the whole chunk as the input. The results show that our approach obtained a 15% relative improvement compared with the official GMM-based method of DCASE 2016 challenge.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07722",
        "title": "Neural Network Based Next-Song Recommendation",
        "authors": [
            "Kai-Chun Hsu",
            "Szu-Yu Chou",
            "Yi-Hsuan Yang",
            "Tai-Shih Chi"
        ],
        "abstract": "Recently, the next-item/basket recommendation system, which considers the sequential relation between bought items, has drawn attention of researchers. The utilization of sequential patterns has boosted performance on several kinds of recommendation tasks. Inspired by natural language processing (NLP) techniques, we propose a novel neural network (NN) based next-song recommender, CNN-rec, in this paper. Then, we compare the proposed system with several NN based and classic recommendation systems on the next-song recommendation task. Verification results indicate the proposed system outperforms classic systems and has comparable performance with the state-of-the-art system.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07786",
        "title": "Precise neural network computation with imprecise analog devices",
        "authors": [
            "Jonathan Binas",
            "Daniel Neil",
            "Giacomo Indiveri",
            "Shih-Chii Liu",
            "Michael Pfeiffer"
        ],
        "abstract": "The operations used for neural network computation map favorably onto simple analog circuits, which outshine their digital counterparts in terms of compactness and efficiency. Nevertheless, such implementations have been largely supplanted by digital designs, partly because of device mismatch effects due to material and fabrication imperfections. We propose a framework that exploits the power of deep learning to compensate for this mismatch by incorporating the measured device variations as constraints in the neural network training process. This eliminates the need for mismatch minimization strategies and allows circuit complexity and power-consumption to be reduced to a minimum. Our results, based on large-scale simulations as well as a prototype VLSI chip implementation indicate a processing efficiency comparable to current state-of-art digital implementations. This method is suitable for future technology based on nanodevices with large variability, such as memristive arrays.\n    ",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2020-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07841",
        "title": "Proactive Decision Support using Automated Planning",
        "authors": [
            "Satya Gautam Vadlamudi",
            "Tathagata Chakraborti",
            "Yu Zhang",
            "Subbarao Kambhampati"
        ],
        "abstract": "Proactive decision support (PDS) helps in improving the decision making experience of human decision makers in human-in-the-loop planning environments. Here both the quality of the decisions and the ease of making them are enhanced. In this regard, we propose a PDS framework, named RADAR, based on the research in Automated Planning in AI, that aids the human decision maker with her plan to achieve her goals by providing alerts on: whether such a plan can succeed at all, whether there exist any resource constraints that may foil her plan, etc. This is achieved by generating and analyzing the landmarks that must be accomplished by any successful plan on the way to achieving the goals. Note that, this approach also supports naturalistic decision making which is being acknowledged as a necessary element in proactive decision support, since it only aids the human decision maker through suggestions and alerts rather than enforcing fixed plans or decisions. We demonstrate the utility of the proposed framework through search-and-rescue examples in a fire-fighting domain.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07908",
        "title": "Label Tree Embeddings for Acoustic Scene Classification",
        "authors": [
            "Huy Phan",
            "Lars Hertel",
            "Marco Maass",
            "Philipp Koch",
            "Alfred Mertins"
        ],
        "abstract": "We present in this paper an efficient approach for acoustic scene classification by exploring the structure of class labels. Given a set of class labels, a category taxonomy is automatically learned by collectively optimizing a clustering of the labels into multiple meta-classes in a tree structure. An acoustic scene instance is then embedded into a low-dimensional feature representation which consists of the likelihoods that it belongs to the meta-classes. We demonstrate state-of-the-art results on two different datasets for the acoustic scene classification task, including the DCASE 2013 and LITIS Rouen datasets.\n    ",
        "submission_date": "2016-06-25T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08104",
        "title": "Content-Based Top-N Recommendation using Heterogeneous Relations",
        "authors": [
            "Yifan Chen",
            "Xiang Zhao",
            "Junjiao Gan",
            "Junkai Ren",
            "Yang Fang"
        ],
        "abstract": "Top-$N$ recommender systems have been extensively studied. However, the sparsity of user-item activities has not been well resolved. While many hybrid systems were proposed to address the cold-start problem, the profile information has not been sufficiently leveraged. Furthermore, the heterogeneity of profiles between users and items intensifies the challenge. In this paper, we propose a content-based top-$N$ recommender system by learning the global term weights in profiles. To achieve this, we bring in PathSim, which could well measures the node similarity with heterogeneous relations (between users and items). Starting from the original TF-IDF value, the global term weights gradually converge, and eventually reflect both profile and activity information. To facilitate training, the derivative is reformulated into matrix form, which could easily be paralleled. We conduct extensive experiments, which demonstrate the superiority of the proposed method.\n    ",
        "submission_date": "2016-06-27T00:00:00",
        "last_modified_date": "2016-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08140",
        "title": "STransE: a novel embedding model of entities and relationships in knowledge bases",
        "authors": [
            "Dat Quoc Nguyen",
            "Kairit Sirts",
            "Lizhen Qu",
            "Mark Johnson"
        ],
        "abstract": "Knowledge bases of real-world facts about entities and their relationships are useful resources for a variety of natural language processing tasks. However, because knowledge bases are typically incomplete, it is useful to be able to perform link prediction or knowledge base completion, i.e., predict whether a relationship not in the knowledge base is likely to be true. This paper combines insights from several previous link prediction models into a new embedding model STransE that represents each entity as a low-dimensional vector, and each relation by two matrices and a translation vector. STransE is a simple combination of the SE and TransE models, but it obtains better link prediction performance on two benchmark datasets than previous embedding models. Thus, STransE can serve as a new baseline for the more complex models in the link prediction task.\n    ",
        "submission_date": "2016-06-27T00:00:00",
        "last_modified_date": "2017-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08359",
        "title": "Lifted Rule Injection for Relation Embeddings",
        "authors": [
            "Thomas Demeester",
            "Tim Rockt\u00e4schel",
            "Sebastian Riedel"
        ],
        "abstract": "Methods based on representation learning currently hold the state-of-the-art in many natural language processing and knowledge base inference tasks. Yet, a major challenge is how to efficiently incorporate commonsense knowledge into such models. A recent approach regularizes relation and entity representations by propositionalization of first-order logic rules. However, propositionalization does not scale beyond domains with only few entities and rules. In this paper we present a highly efficient method for incorporating implication rules into distributed representations for automated knowledge base construction. We map entity-tuple embeddings into an approximately Boolean space and encourage a partial ordering over relation embeddings based on implication rules mined from WordNet. Surprisingly, we find that the strong restriction of the entity-tuple embedding space does not hurt the expressiveness of the model and even acts as a regularizer that improves generalization. By incorporating few commonsense rules, we achieve an increase of 2 percentage points mean average precision over a matrix factorization baseline, while observing a negligible increase in runtime.\n    ",
        "submission_date": "2016-06-27T00:00:00",
        "last_modified_date": "2016-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08362",
        "title": "A Reduction for Optimizing Lattice Submodular Functions with Diminishing Returns",
        "authors": [
            "Alina Ene",
            "Huy L. Nguyen"
        ],
        "abstract": "A function $f: \\mathbb{Z}_+^E \\rightarrow \\mathbb{R}_+$ is DR-submodular if it satisfies $f({\\bf x} + \\chi_i) -f ({\\bf x}) \\ge f({\\bf y} + \\chi_i) - f({\\bf y})$ for all ${\\bf x}\\le {\\bf y}, i\\in E$. Recently, the problem of maximizing a DR-submodular function $f: \\mathbb{Z}_+^E \\rightarrow \\mathbb{R}_+$ subject to a budget constraint $\\|{\\bf x}\\|_1 \\leq B$ as well as additional constraints has received significant attention \\cite{SKIK14,SY15,MYK15,SY16}.\n",
        "submission_date": "2016-06-27T00:00:00",
        "last_modified_date": "2018-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08777",
        "title": "\"Show me the cup\": Reference with Continuous Representations",
        "authors": [
            "Gemma Boleda",
            "Sebastian Pad\u00f3",
            "Marco Baroni"
        ],
        "abstract": "One of the most basic functions of language is to refer to objects in a shared scene. Modeling reference with continuous representations is challenging because it requires individuation, i.e., tracking and distinguishing an arbitrary number of referents. We introduce a neural network model that, given a definite description and a set of objects represented by natural images, points to the intended object if the expression has a unique referent, or indicates a failure, if it does not. The model, directly trained on reference acts, is competitive with a pipeline manually engineered to perform the same task, both when referents are purely visual, and when they are characterized by a combination of visual and linguistic properties.\n    ",
        "submission_date": "2016-06-28T00:00:00",
        "last_modified_date": "2016-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08808",
        "title": "Adaptive Training of Random Mapping for Data Quantization",
        "authors": [
            "Miao Cheng",
            "Ah Chung Tsoi"
        ],
        "abstract": "Data quantization learns encoding results of data with certain requirements, and provides a broad perspective of many real-world applications to data handling. Nevertheless, the results of encoder is usually limited to multivariate inputs with the random mapping, and side information of binary codes are hardly to mostly depict the original data patterns as possible. In the literature, cosine based random quantization has attracted much attentions due to its intrinsic bounded results. Nevertheless, it usually suffers from the uncertain outputs, and information of original data fails to be fully preserved in the reduced codes. In this work, a novel binary embedding method, termed adaptive training quantization (ATQ), is proposed to learn the ideal transform of random encoder, where the limitation of cosine random mapping is tackled. As an adaptive learning idea, the reduced mapping is adaptively calculated with idea of data group, while the bias of random transform is to be improved to hold most matching information. Experimental results show that the proposed method is able to obtain outstanding performance compared with other random quantization methods.\n    ",
        "submission_date": "2016-06-28T00:00:00",
        "last_modified_date": "2017-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08842",
        "title": "Active Ranking from Pairwise Comparisons and when Parametric Assumptions Don't Help",
        "authors": [
            "Reinhard Heckel",
            "Nihar B. Shah",
            "Kannan Ramchandran",
            "Martin J. Wainwright"
        ],
        "abstract": "We consider sequential or active ranking of a set of n items based on noisy pairwise comparisons. Items are ranked according to the probability that a given item beats a randomly chosen item, and ranking refers to partitioning the items into sets of pre-specified sizes according to their scores. This notion of ranking includes as special cases the identification of the top-k items and the total ordering of the items. We first analyze a sequential ranking algorithm that counts the number of comparisons won, and uses these counts to decide whether to stop, or to compare another pair of items, chosen based on confidence intervals specified by the data collected up to that point. We prove that this algorithm succeeds in recovering the ranking using a number of comparisons that is optimal up to logarithmic factors. This guarantee does not require any structural properties of the underlying pairwise probability matrix, unlike a significant body of past work on pairwise ranking based on parametric models such as the Thurstone or Bradley-Terry-Luce models. It has been a long-standing open question as to whether or not imposing these parametric assumptions allows for improved ranking algorithms. For stochastic comparison models, in which the pairwise probabilities are bounded away from zero, our second contribution is to resolve this issue by proving a lower bound for parametric models. This shows, perhaps surprisingly, that these popular parametric modeling choices offer at most logarithmic gains for stochastic comparisons.\n    ",
        "submission_date": "2016-06-28T00:00:00",
        "last_modified_date": "2016-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08866",
        "title": "Technical Report: Towards a Universal Code Formatter through Machine Learning",
        "authors": [
            "Terence Parr",
            "Jurgin Vinju"
        ],
        "abstract": "There are many declarative frameworks that allow us to implement code formatters relatively easily for any specific language, but constructing them is cumbersome. The first problem is that \"everybody\" wants to format their code differently, leading to either many formatter variants or a ridiculous number of configuration options. Second, the size of each implementation scales with a language's grammar size, leading to hundreds of rules.\n",
        "submission_date": "2016-06-28T00:00:00",
        "last_modified_date": "2016-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08928",
        "title": "subgraph2vec: Learning Distributed Representations of Rooted Sub-graphs from Large Graphs",
        "authors": [
            "Annamalai Narayanan",
            "Mahinthan Chandramohan",
            "Lihui Chen",
            "Yang Liu",
            "Santhoshkumar Saminathan"
        ],
        "abstract": "In this paper, we present subgraph2vec, a novel approach for learning latent representations of rooted subgraphs from large graphs inspired by recent advancements in Deep Learning and Graph Kernels. These latent representations encode semantic substructure dependencies in a continuous vector space, which is easily exploited by statistical models for tasks such as graph classification, clustering, link prediction and community detection. subgraph2vec leverages on local information obtained from neighbourhoods of nodes to learn their latent representations in an unsupervised fashion. We demonstrate that subgraph vectors learnt by our approach could be used in conjunction with classifiers such as CNNs, SVMs and relational data clustering algorithms to achieve significantly superior accuracies. Also, we show that the subgraph vectors could be used for building a deep learning variant of Weisfeiler-Lehman graph kernel. Our experiments on several benchmark and large-scale real-world datasets reveal that subgraph2vec achieves significant improvements in accuracies over existing graph kernels on both supervised and unsupervised learning tasks. Specifically, on two realworld program analysis tasks, namely, code clone and malware detection, subgraph2vec outperforms state-of-the-art kernels by more than 17% and 4%, respectively.\n    ",
        "submission_date": "2016-06-29T00:00:00",
        "last_modified_date": "2016-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08954",
        "title": "Greedy, Joint Syntactic-Semantic Parsing with Stack LSTMs",
        "authors": [
            "Swabha Swayamdipta",
            "Miguel Ballesteros",
            "Chris Dyer",
            "Noah A. Smith"
        ],
        "abstract": "We present a transition-based parser that jointly produces syntactic and semantic dependencies. It learns a representation of the entire algorithm state, using stack long short-term memories. Our greedy inference algorithm has linear time, including feature extraction. On the CoNLL 2008--9 English shared tasks, we obtain the best published parsing performance among models that jointly learn syntax and semantics.\n    ",
        "submission_date": "2016-06-29T00:00:00",
        "last_modified_date": "2018-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09403",
        "title": "Learning Crosslingual Word Embeddings without Bilingual Corpora",
        "authors": [
            "Long Duong",
            "Hiroshi Kanayama",
            "Tengfei Ma",
            "Steven Bird",
            "Trevor Cohn"
        ],
        "abstract": "Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools. However, previous attempts had expensive resource requirements, difficulty incorporating monolingual data or were unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-the-art performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task.\n    ",
        "submission_date": "2016-06-30T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09581",
        "title": "Performance Based Evaluation of Various Machine Learning Classification Techniques for Chronic Kidney Disease Diagnosis",
        "authors": [
            "Sahil Sharma",
            "Vinod Sharma",
            "Atul Sharma"
        ],
        "abstract": "Areas where Artificial Intelligence (AI) & related fields are finding their applications are increasing day by day, moving from core areas of computer science they are finding their applications in various other ",
        "submission_date": "2016-06-28T00:00:00",
        "last_modified_date": "2016-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09632",
        "title": "A Permutation-based Model for Crowd Labeling: Optimal Estimation and Robustness",
        "authors": [
            "Nihar B. Shah",
            "Sivaraman Balakrishnan",
            "Martin J. Wainwright"
        ],
        "abstract": "The task of aggregating and denoising crowd-labeled data has gained increased significance with the advent of crowdsourcing platforms and massive datasets. We propose a permutation-based model for crowd labeled data that is a significant generalization of the classical Dawid-Skene model, and introduce a new error metric by which to compare different estimators. We derive global minimax rates for the permutation-based model that are sharp up to logarithmic factors, and match the minimax lower bounds derived under the simpler Dawid-Skene model. We then design two computationally-efficient estimators: the WAN estimator for the setting where the ordering of workers in terms of their abilities is approximately known, and the OBI-WAN estimator where that is not known. For each of these estimators, we provide non-asymptotic bounds on their performance. We conduct synthetic simulations and experiments on real-world crowdsourcing data, and the experimental results corroborate our theoretical findings.\n    ",
        "submission_date": "2016-06-30T00:00:00",
        "last_modified_date": "2021-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00186",
        "title": "Throwing fuel on the embers: Probability or Dichotomy, Cognitive or Linguistic?",
        "authors": [
            "David M. W. Powers"
        ],
        "abstract": "Prof. Robert Berwick's abstract for his forthcoming invited talk at the ACL2016 workshop on Cognitive Aspects of Computational Language Learning revives an ancient debate. Entitled \"Why take a chance?\", Berwick seems to refer implicitly to Chomsky's critique of the statistical approach of Harris as well as the currently dominant paradigms in CoNLL.\n",
        "submission_date": "2016-07-01T00:00:00",
        "last_modified_date": "2016-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00215",
        "title": "Why is Posterior Sampling Better than Optimism for Reinforcement Learning?",
        "authors": [
            "Ian Osband",
            "Benjamin Van Roy"
        ],
        "abstract": "Computational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an $\\tilde{O}(H\\sqrt{SAT})$ Bayesian expected regret bound for PSRL in finite-horizon episodic Markov decision processes, where $H$ is the horizon, $S$ is the number of states, $A$ is the number of actions and $T$ is the time elapsed. This improves upon the best previous bound of $\\tilde{O}(H S \\sqrt{AT})$ for any reinforcement learning algorithm.\n    ",
        "submission_date": "2016-07-01T00:00:00",
        "last_modified_date": "2017-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00279",
        "title": "Meaningful Models: Utilizing Conceptual Structure to Improve Machine Learning Interpretability",
        "authors": [
            "Nick Condry"
        ],
        "abstract": "The last decade has seen huge progress in the development of advanced machine learning models; however, those models are powerless unless human users can interpret them. Here we show how the mind's construction of concepts and meaning can be used to create more interpretable machine learning models. By proposing a novel method of classifying concepts, in terms of 'form' and 'function', we elucidate the nature of meaning and offer proposals to improve model understandability. As machine learning begins to permeate daily life, interpretable models may serve as a bridge between domain-expert authors and non-expert users.\n    ",
        "submission_date": "2016-07-01T00:00:00",
        "last_modified_date": "2016-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00410",
        "title": "Domain Adaptation for Neural Networks by Parameter Augmentation",
        "authors": [
            "Yusuke Watanabe",
            "Kazuma Hashimoto",
            "Yoshimasa Tsuruoka"
        ],
        "abstract": "We propose a simple domain adaptation method for neural networks in a supervised setting. Supervised domain adaptation is a way of improving the generalization performance on the target domain by using the source domain dataset, assuming that both of the datasets are labeled. Recently, recurrent neural networks have been shown to be successful on a variety of NLP tasks such as caption generation; however, the existing domain adaptation techniques are limited to (1) tune the model parameters by the target dataset after the training by the source dataset, or (2) design the network to have dual output, one for the source domain and the other for the target domain. Reformulating the idea of the domain adaptation technique proposed by Daume (2007), we propose a simple domain adaptation method, which can be applied to neural networks trained with a cross-entropy loss. On captioning datasets, we show performance improvements over other domain adaptation methods.\n    ",
        "submission_date": "2016-07-01T00:00:00",
        "last_modified_date": "2016-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00474",
        "title": "Adaptive Neighborhood Graph Construction for Inference in Multi-Relational Networks",
        "authors": [
            "Shobeir Fakhraei",
            "Dhanya Sridhar",
            "Jay Pujara",
            "Lise Getoor"
        ],
        "abstract": "A neighborhood graph, which represents the instances as vertices and their relations as weighted edges, is the basis of many semi-supervised and relational models for node labeling and link prediction. Most methods employ a sequential process to construct the neighborhood graph. This process often consists of generating a candidate graph, pruning the candidate graph to make a neighborhood graph, and then performing inference on the variables (i.e., nodes) in the neighborhood graph. In this paper, we propose a framework that can dynamically adapt the neighborhood graph based on the states of variables from intermediate inference results, as well as structural properties of the relations connecting them. A key strength of our framework is its ability to handle multi-relational data and employ varying amounts of relations for each instance based on the intermediate inference results. We formulate the link prediction task as inference on neighborhood graphs, and include preliminary results illustrating the effects of different strategies in our proposed framework.\n    ",
        "submission_date": "2016-07-02T00:00:00",
        "last_modified_date": "2016-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00623",
        "title": "Visualizing Natural Language Descriptions: A Survey",
        "authors": [
            "Kaveh Hassani",
            "Won-Sook Lee"
        ],
        "abstract": "A natural language interface exploits the conceptual simplicity and naturalness of the language to create a high-level user-friendly communication channel between humans and machines. One of the promising applications of such interfaces is generating visual interpretations of semantic content of a given natural language that can be then visualized either as a static scene or a dynamic animation. This survey discusses requirements and challenges of developing such systems and reports 26 graphical systems that exploit natural language interfaces and addresses both artificial intelligence and visualization aspects. This work serves as a frame of reference to researchers and to enable further advances in the field.\n    ",
        "submission_date": "2016-07-03T00:00:00",
        "last_modified_date": "2016-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00695",
        "title": "Can we reach Pareto optimal outcomes using bottom-up approaches?",
        "authors": [
            "Victor Sanchez-Anguix",
            "Reyhan Aydogan",
            "Tim Baarslag",
            "Catholijn M. Jonker"
        ],
        "abstract": "Traditionally, researchers in decision making have focused on attempting to reach Pareto Optimality using horizontal approaches, where optimality is calculated taking into account every participant at the same time. Sometimes, this may prove to be a difficult task (e.g., conflict, mistrust, no information sharing, etc.). In this paper, we explore the possibility of achieving Pareto Optimal outcomes in a group by using a bottom-up approach: discovering Pareto optimal outcomes by interacting in subgroups. We analytically show that Pareto optimal outcomes in a subgroup are also Pareto optimal in a supergroup of those agents in the case of strict, transitive, and complete preferences. Then, we empirically analyze the prospective usability and practicality of bottom-up approaches in a variety of decision making domains.\n    ",
        "submission_date": "2016-07-03T00:00:00",
        "last_modified_date": "2016-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00872",
        "title": "Neighborhood Features Help Detecting Non-Technical Losses in Big Data Sets",
        "authors": [
            "Patrick Glauner",
            "Jorge Meira",
            "Lautaro Dolberg",
            "Radu State",
            "Franck Bettinger",
            "Yves Rangoni",
            "Diogo Duarte"
        ],
        "abstract": "Electricity theft is a major problem around the world in both developed and developing countries and may range up to 40% of the total electricity distributed. More generally, electricity theft belongs to non-technical losses (NTL), which are losses that occur during the distribution of electricity in power grids. In this paper, we build features from the neighborhood of customers. We first split the area in which the customers are located into grids of different sizes. For each grid cell we then compute the proportion of inspected customers and the proportion of NTL found among the inspected customers. We then analyze the distributions of features generated and show why they are useful to predict NTL. In addition, we compute features from the consumption time series of customers. We also use master data features of customers, such as their customer class and voltage of their connection. We compute these features for a Big Data base of 31M meter readings, 700K customers and 400K inspection results. We then use these features to train four machine learning algorithms that are particularly suitable for Big Data sets because of their parallelizable structure: logistic regression, k-nearest neighbors, linear support vector machine and random forest. Using the neighborhood features instead of only analyzing the time series has resulted in appreciable results for Big Data sets for varying NTL proportions of 1%-90%. This work can therefore be deployed to a wide range of different regions around the world.\n    ",
        "submission_date": "2016-07-04T00:00:00",
        "last_modified_date": "2017-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00913",
        "title": "Superintelligence cannot be contained: Lessons from Computability Theory",
        "authors": [
            "Manuel Alfonseca",
            "Manuel Cebrian",
            "Antonio Fernandez Anta",
            "Lorenzo Coviello",
            "Andres Abeliuk",
            "Iyad Rahwan"
        ],
        "abstract": "Superintelligence is a hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human minds. In light of recent advances in machine intelligence, a number of scientists, philosophers and technologists have revived the discussion about the potential catastrophic risks entailed by such an entity. In this article, we trace the origins and development of the neo-fear of superintelligence, and some of the major proposals for its containment. We argue that such containment is, in principle, impossible, due to fundamental limits inherent to computing itself. Assuming that a superintelligence will contain a program that includes all the programs that can be executed by a universal Turing machine on input potentially as complex as the state of the world, strict containment requires simulations of such a program, something theoretically (and practically) infeasible.\n    ",
        "submission_date": "2016-07-04T00:00:00",
        "last_modified_date": "2016-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00976",
        "title": "Modelling Context with User Embeddings for Sarcasm Detection in Social Media",
        "authors": [
            "Silvio Amir",
            "Byron C. Wallace",
            "Hao Lyu",
            "Paula Carvalho M\u00e1rio J. Silva"
        ],
        "abstract": "We introduce a deep neural network for automated sarcasm detection. Recent work has emphasized the need for models to capitalize on contextual features, beyond lexical and syntactic cues present in utterances. For example, different speakers will tend to employ sarcasm regarding different subjects and, thus, sarcasm detection models ought to encode such speaker information. Current methods have achieved this by way of laborious feature engineering. By contrast, we propose to automatically learn and then exploit user embeddings, to be used in concert with lexical signals to recognize sarcasm. Our approach does not require elaborate feature engineering (and concomitant data scraping); fitting user embeddings requires only the text from their previous posts. The experimental results show that our model outperforms a state-of-the-art approach leveraging an extensive set of carefully crafted features.\n    ",
        "submission_date": "2016-07-04T00:00:00",
        "last_modified_date": "2016-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01036",
        "title": "Bootstrap Model Aggregation for Distributed Statistical Learning",
        "authors": [
            "Jun Han",
            "Qiang Liu"
        ],
        "abstract": "In distributed, or privacy-preserving learning, we are often given a set of probabilistic models estimated from different local repositories, and asked to combine them into a single model that gives efficient statistical estimation. A simple method is to linearly average the parameters of the local models, which, however, tends to be degenerate or not applicable on non-convex models, or models with different parameter dimensions. One more practical strategy is to generate bootstrap samples from the local models, and then learn a joint model based on the combined bootstrap set. Unfortunately, the bootstrap procedure introduces additional noise and can significantly deteriorate the performance. In this work, we propose two variance reduction methods to correct the bootstrap noise, including a weighted M-estimator that is both statistically efficient and practically powerful. Both theoretical and empirical analysis is provided to demonstrate our methods.\n    ",
        "submission_date": "2016-07-04T00:00:00",
        "last_modified_date": "2017-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01115",
        "title": "Click Carving: Segmenting Objects in Video with Point Clicks",
        "authors": [
            "Suyog Dutt Jain",
            "Kristen Grauman"
        ],
        "abstract": "We present a novel form of interactive video object segmentation where a few clicks by the user helps the system produce a full spatio-temporal segmentation of the object of interest. Whereas conventional interactive pipelines take the user's initialization as a starting point, we show the value in the system taking the lead even in initialization. In particular, for a given video frame, the system precomputes a ranked list of thousands of possible segmentation hypotheses (also referred to as object region proposals) using image and motion cues. Then, the user looks at the top ranked proposals, and clicks on the object boundary to carve away erroneous ones. This process iterates (typically 2-3 times), and each time the system revises the top ranked proposal set, until the user is satisfied with a resulting segmentation mask. Finally, the mask is propagated across the video to produce a spatio-temporal object tube. On three challenging datasets, we provide extensive comparisons with both existing work and simpler alternative methods. In all, the proposed Click Carving approach strikes an excellent balance of accuracy and human effort. It outperforms all similarly fast methods, and is competitive or better than those requiring 2 to 12 times the effort.\n    ",
        "submission_date": "2016-07-05T00:00:00",
        "last_modified_date": "2016-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01202",
        "title": "Optimal control for a robotic exploration, pick-up and delivery problem",
        "authors": [
            "Vladislav Nenchev",
            "Christos G. Cassandras",
            "J\u00f6rg Raisch"
        ],
        "abstract": "This paper addresses an optimal control problem for a robot that has to find and collect a finite number of objects and move them to a depot in minimum time. The robot has fourth-order dynamics that change instantaneously at any pick-up or drop-off of an object. The objects are modeled by point masses with a-priori unknown locations in a bounded two-dimensional space that may contain unknown obstacles. For this hybrid system, an Optimal Control Problem (OCP) is approximately solved by a receding horizon scheme, where the derived lower bound for the cost-to-go is evaluated for the worst and for a probabilistic case, assuming a uniform distribution of the objects. First, a time-driven approximate solution based on time and position space discretization and mixed integer programming is presented. Due to the high computational cost of this solution, an alternative event-driven approximate approach based on a suitable motion parameterization and gradient-based optimization is proposed. The solutions are compared in a numerical example, suggesting that the latter approach offers a significant computational advantage while yielding similar qualitative results compared to the former. The methods are particularly relevant for various robotic applications like automated cleaning, search and rescue, harvesting or manufacturing.\n    ",
        "submission_date": "2016-07-05T00:00:00",
        "last_modified_date": "2016-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01381",
        "title": "One-Shot Session Recommendation Systems with Combinatorial Items",
        "authors": [
            "Yahel David",
            "Dotan Di Castro",
            "Zohar Karnin"
        ],
        "abstract": "In recent years, content recommendation systems in large websites (or \\emph{content providers}) capture an increased focus. While the type of content varies, e.g.\\ movies, articles, music, advertisements, etc., the high level problem remains the same. Based on knowledge obtained so far on the user, recommend the most desired content. In this paper we present a method to handle the well known user-cold-start problem in recommendation systems. In this scenario, a recommendation system encounters a new user and the objective is to present items as relevant as possible with the hope of keeping the user's session as long as possible. We formulate an optimization problem aimed to maximize the length of this initial session, as this is believed to be the key to have the user come back and perhaps register to the system. In particular, our model captures the fact that a single round with low quality recommendation is likely to terminate the session. In such a case, we do not proceed to the next round as the user leaves the system, possibly never to seen again. We denote this phenomenon a \\emph{One-Shot Session}. Our optimization problem is formulated as an MDP where the action space is of a combinatorial nature as we recommend in each round, multiple items. This huge action space presents a computational challenge making the straightforward solution intractable. We analyze the structure of the MDP to prove monotone and submodular like properties that allow a computationally efficient solution via a method denoted by \\emph{Greedy Value Iteration} (G-VI).\n    ",
        "submission_date": "2016-07-05T00:00:00",
        "last_modified_date": "2016-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01478",
        "title": "Mixed Strategy for Constrained Stochastic Optimal Control",
        "authors": [
            "Masahiro Ono",
            "Mahmoud El Chamie",
            "Marco Pavone",
            "Behcet Acikmese"
        ],
        "abstract": "Choosing control inputs randomly can result in a reduced expected cost in optimal control problems with stochastic constraints, such as stochastic model predictive control (SMPC). We consider a controller with initial randomization, meaning that the controller randomly chooses from K+1 control sequences at the beginning (called K-randimization).It is known that, for a finite-state, finite-action Markov Decision Process (MDP) with K constraints, K-randimization is sufficient to achieve the minimum cost. We found that the same result holds for stochastic optimal control problems with continuous state and action ",
        "submission_date": "2016-07-06T00:00:00",
        "last_modified_date": "2016-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01690",
        "title": "A New Hierarchical Redundancy Eliminated Tree Augmented Naive Bayes Classifier for Coping with Gene Ontology-based Features",
        "authors": [
            "Cen Wan",
            "Alex A. Freitas"
        ],
        "abstract": "The Tree Augmented Naive Bayes classifier is a type of probabilistic graphical model that can represent some feature dependencies. In this work, we propose a Hierarchical Redundancy Eliminated Tree Augmented Naive Bayes (HRE-TAN) algorithm, which considers removing the hierarchical redundancy during the classifier learning process, when coping with data containing hierarchically structured features. The experiments showed that HRE-TAN obtains significantly better predictive performance than the conventional Tree Augmented Naive Bayes classifier, and enhanced the robustness against imbalanced class distributions, in aging-related gene datasets with Gene Ontology terms used as features.\n    ",
        "submission_date": "2016-07-06T00:00:00",
        "last_modified_date": "2016-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01719",
        "title": "Deep CORAL: Correlation Alignment for Deep Domain Adaptation",
        "authors": [
            "Baochen Sun",
            "Kate Saenko"
        ],
        "abstract": "Deep neural networks are able to learn powerful representations from large quantities of labeled input data, however they cannot always generalize well across changes in input distributions. Domain adaptation algorithms have been proposed to compensate for the degradation in performance due to domain shift. In this paper, we address the case when the target domain is unlabeled, requiring unsupervised adaptation. CORAL is a \"frustratingly easy\" unsupervised domain adaptation method that aligns the second-order statistics of the source and target distributions with a linear transformation. Here, we extend CORAL to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks (Deep CORAL). Experiments on standard benchmark datasets show state-of-the-art performance.\n    ",
        "submission_date": "2016-07-06T00:00:00",
        "last_modified_date": "2016-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01869",
        "title": "Scalable Semantic Matching of Queries to Ads in Sponsored Search Advertising",
        "authors": [
            "Mihajlo Grbovic",
            "Nemanja Djuric",
            "Vladan Radosavljevic",
            "Fabrizio Silvestri",
            "Ricardo Baeza-Yates",
            "Andrew Feng",
            "Erik Ordentlich",
            "Lee Yang",
            "Gavin Owens"
        ],
        "abstract": "Sponsored search represents a major source of revenue for web search engines. This popular advertising model brings a unique possibility for advertisers to target users' immediate intent communicated through a search query, usually by displaying their ads alongside organic search results for queries deemed relevant to their products or services. However, due to a large number of unique queries it is challenging for advertisers to identify all such relevant queries. For this reason search engines often provide a service of advanced matching, which automatically finds additional relevant queries for advertisers to bid on. We present a novel advanced matching approach based on the idea of semantic embeddings of queries and ads. The embeddings were learned using a large data set of user search sessions, consisting of search queries, clicked ads and search links, while utilizing contextual information such as dwell time and skipped ads. To address the large-scale nature of our problem, both in terms of data and vocabulary size, we propose a novel distributed algorithm for training of the embeddings. Finally, we present an approach for overcoming a cold-start problem associated with new ads and queries. We report results of editorial evaluation and online tests on actual search traffic. The results show that our approach significantly outperforms baselines in terms of relevance, coverage, and incremental revenue. Lastly, we open-source learned query embeddings to be used by researchers in computational advertising and related fields.\n    ",
        "submission_date": "2016-07-07T00:00:00",
        "last_modified_date": "2016-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02061",
        "title": "Representing Verbs with Rich Contexts: an Evaluation on Verb Similarity",
        "authors": [
            "Emmanuele Chersoni",
            "Enrico Santus",
            "Alessandro Lenci",
            "Philippe Blache",
            "Chu-Ren Huang"
        ],
        "abstract": "Several studies on sentence processing suggest that the mental lexicon keeps track of the mutual expectations between words. Current DSMs, however, represent context words as separate features, thereby loosing important information for word expectations, such as word interrelations. In this paper, we present a DSM that addresses this issue by defining verb contexts as joint syntactic dependencies. We test our representation in a verb similarity task on two datasets, showing that joint contexts achieve performances comparable to single dependencies or even better. Moreover, they are able to overcome the data sparsity problem of joint feature spaces, in spite of the limited size of our training corpus.\n    ",
        "submission_date": "2016-07-07T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02137",
        "title": "Fundamental Parameters of Main-Sequence Stars in an Instant with Machine Learning",
        "authors": [
            "Earl P. Bellinger",
            "George C. Angelou",
            "Saskia Hekker",
            "Sarbani Basu",
            "Warrick Ball",
            "Elisabeth Guggenberger"
        ],
        "abstract": "Owing to the remarkable photometric precision of space observatories like Kepler, stellar and planetary systems beyond our own are now being characterized en masse for the first time. These characterizations are pivotal for endeavors such as searching for Earth-like planets and solar twins, understanding the mechanisms that govern stellar evolution, and tracing the dynamics of our Galaxy. The volume of data that is becoming available, however, brings with it the need to process this information accurately and rapidly. While existing methods can constrain fundamental stellar parameters such as ages, masses, and radii from these observations, they require substantial computational efforts to do so.\n",
        "submission_date": "2016-07-06T00:00:00",
        "last_modified_date": "2016-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02306",
        "title": "CaR-FOREST: Joint Classification-Regression Decision Forests for Overlapping Audio Event Detection",
        "authors": [
            "Huy Phan",
            "Lars Hertel",
            "Marco Maass",
            "Philipp Koch",
            "Alfred Mertins"
        ],
        "abstract": "This report describes our submissions to Task2 and Task3 of the DCASE 2016 challenge. The systems aim at dealing with the detection of overlapping audio events in continuous streams, where the detectors are based on random decision forests. The proposed forests are jointly trained for classification and regression simultaneously. Initially, the training is classification-oriented to encourage the trees to select discriminative features from overlapping mixtures to separate positive audio segments from the negative ones. The regression phase is then carried out to let the positive audio segments vote for the event onsets and offsets, and therefore model the temporal structure of audio events. One random decision forest is specifically trained for each event category of interest. Experimental results on the development data show that our systems significantly outperform the baseline on the Task2 evaluation while they are inferior to the baseline in the Task3 evaluation.\n    ",
        "submission_date": "2016-07-08T00:00:00",
        "last_modified_date": "2016-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02399",
        "title": "Translating Bayesian Networks into Entity Relationship Models, Extended Version",
        "authors": [
            "Frank Rosner",
            "Alexander Hinneburg"
        ],
        "abstract": "Big data analytics applications drive the convergence of data management and machine learning. But there is no conceptual language available that is spoken in both worlds. The main contribution of the paper is a method to translate Bayesian networks, a main conceptual language for probabilistic graphical models, into usable entity relationship models. The transformed representation of a Bayesian network leaves out mathematical details about probabilistic relationships but unfolds all information relevant for data management tasks. As a real world example, we present the TopicExplorer system that uses Bayesian topic models as a core component in an interactive, database-supported web application. Last, we sketch a conceptual framework that eases machine learning specific development tasks while building big data analytics applications.\n    ",
        "submission_date": "2016-07-08T00:00:00",
        "last_modified_date": "2016-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02419",
        "title": "Divisive-agglomerative algorithm and complexity of automatic classification problems",
        "authors": [
            "Alexander Rubchinsky"
        ],
        "abstract": "An algorithm of solution of the Automatic Classification (AC for brevity) problem is set forth in the paper. In the AC problem, it is required to find one or several artitions, starting with the given pattern matrix or dissimilarity, similarity matrix.\n    ",
        "submission_date": "2016-07-05T00:00:00",
        "last_modified_date": "2016-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02444",
        "title": "Explaining Deep Convolutional Neural Networks on Music Classification",
        "authors": [
            "Keunwoo Choi",
            "George Fazekas",
            "Mark Sandler"
        ],
        "abstract": "Deep convolutional neural networks (CNNs) have been actively adopted in the field of music information retrieval, e.g. genre classification, mood detection, and chord recognition. However, the process of learning and prediction is little understood, particularly when it is applied to spectrograms. We introduce auralisation of a CNN to understand its underlying mechanism, which is based on a deconvolution procedure introduced in [2]. Auralisation of a CNN is converting the learned convolutional features that are obtained from deconvolution into audio signals. In the experiments and discussions, we explain trained features of a 5-layer CNN based on the deconvolved spectrograms and auralised signals. The pairwise correlations per layers with varying different musical attributes are also investigated to understand the evolution of the learnt features. It is shown that in the deep layers, the features are learnt to capture textures, the patterns of continuous distributions, rather than shapes of lines.\n    ",
        "submission_date": "2016-07-08T00:00:00",
        "last_modified_date": "2016-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02466",
        "title": "Solving finite-domain linear constraints in presence of the $\\texttt{alldifferent}$",
        "authors": [
            "Milan Bankovi\u0107"
        ],
        "abstract": "In this paper, we investigate the possibility of improvement of the widely-used filtering algorithm for the linear constraints in constraint satisfaction problems in the presence of the alldifferent constraints. In many cases, the fact that the variables in a linear constraint are also constrained by some alldifferent constraints may help us to calculate stronger bounds of the variables, leading to a stronger constraint propagation. We propose an improved filtering algorithm that targets such cases. We provide a detailed description of the proposed algorithm and prove its correctness. We evaluate the approach on five different problems that involve combinations of the linear and the alldifferent constraints. We also compare our algorithm to other relevant approaches. The experimental results show a great potential of the proposed improvement.\n    ",
        "submission_date": "2016-07-08T00:00:00",
        "last_modified_date": "2016-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02576",
        "title": "Analysis of opinionated text for opinion mining",
        "authors": [
            "K Paramesha",
            "K C Ravishankar"
        ],
        "abstract": "In sentiment analysis, the polarities of the opinions expressed on an object/feature are determined to assess the sentiment of a sentence or document whether it is positive/negative/neutral. Naturally, the object/feature is a noun representation which refers to a product or a component of a product, let us say, the \"lens\" in a camera and opinions emanating on it are captured in adjectives, verbs, adverbs and noun words themselves. Apart from such words, other meta-information and diverse effective features are also going to play an important role in influencing the sentiment polarity and contribute significantly to the performance of the system. In this paper, some of the associated information/meta-data are explored and investigated in the sentiment text. Based on the analysis results presented here, there is scope for further assessment and utilization of the meta-information as features in text categorization, ranking text document, identification of spam documents and polarity classification problems.\n    ",
        "submission_date": "2016-07-09T00:00:00",
        "last_modified_date": "2016-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02660",
        "title": "Augmenting Supervised Emotion Recognition with Rule-Based Decision Model",
        "authors": [
            "Amol Patwardhan",
            "Gerald Knapp"
        ],
        "abstract": "The aim of this research is development of rule based decision model for emotion recognition. This research also proposes using the rules for augmenting inter-corporal recognition accuracy in multimodal systems that use supervised learning techniques. The classifiers for such learning based recognition systems are susceptible to over fitting and only perform well on intra-corporal data. To overcome the limitation this research proposes using rule based model as an additional modality. The rules were developed using raw feature data from visual channel, based on human annotator agreement and existing studies that have attributed movement and postures to emotions. The outcome of the rule evaluations was combined during the decision phase of emotion recognition system. The results indicate rule based emotion recognition augment recognition accuracy of learning based systems and also provide better recognition rate across inter corpus emotion test data.\n    ",
        "submission_date": "2016-07-09T00:00:00",
        "last_modified_date": "2016-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02682",
        "title": "Extending Weakly-Sticky Datalog+/-: Query-Answering Tractability and Optimizations",
        "authors": [
            "Mostafa Milani",
            "Leopoldo Bertossi"
        ],
        "abstract": "Weakly-sticky (WS) Datalog+/- is an expressive member of the family of Datalog+/- programs that is based on the syntactic notions of stickiness and weak-acyclicity. Query answering over the WS programs has been investigated, but there is still much work to do on the design and implementation of practical query answering (QA) algorithms and their optimizations. Here, we study sticky and WS programs from the point of view of the behavior of the chase procedure, extending the stickiness property of the chase to that of generalized stickiness of the chase (gsch-property). With this property we specify the semantic class of GSCh programs, which includes sticky and WS programs, and other syntactic subclasses that we identify. In particular, we introduce joint-weakly-sticky (JWS) programs, that include WS programs. We also propose a bottom-up QA algorithm for a range of subclasses of GSCh. The algorithm runs in polynomial time (in data) for JWS programs. Unlike the WS class, JWS is closed under a general magic-sets rewriting procedure for the optimization of programs with existential rules. We apply the magic-sets rewriting in combination with the proposed QA algorithm for the optimization of QA over JWS programs.\n    ",
        "submission_date": "2016-07-10T00:00:00",
        "last_modified_date": "2016-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02784",
        "title": "Open Information Extraction",
        "authors": [
            "Duc-Thuan Vo",
            "Ebrahim Bagheri"
        ],
        "abstract": "Open Information Extraction (Open IE) systems aim to obtain relation tuples with highly scalable extraction in portable across domain by identifying a variety of relation phrases and their arguments in arbitrary sentences. The first generation of Open IE learns linear chain models based on unlexicalized features such as Part-of-Speech (POS) or shallow tags to label the intermediate words between pair of potential arguments for identifying extractable relations. Open IE currently is developed in the second generation that is able to extract instances of the most frequently observed relation types such as Verb, Noun and Prep, Verb and Prep, and Infinitive with deep linguistic analysis. They expose simple yet principled ways in which verbs express relationships in linguistics such as verb phrase-based extraction or clause-based extraction. They obtain a significantly higher performance over previous systems in the first generation. In this paper, we describe an overview of two Open IE generations including strengths, weaknesses and application areas.\n    ",
        "submission_date": "2016-07-10T00:00:00",
        "last_modified_date": "2016-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02802",
        "title": "Mapping distributional to model-theoretic semantic spaces: a baseline",
        "authors": [
            "Franck Dernoncourt"
        ],
        "abstract": "Word embeddings have been shown to be useful across state-of-the-art systems in many natural language processing tasks, ranging from question answering systems to dependency parsing. (Herbelot and Vecchi, 2015) explored word embeddings and their utility for modeling language semantics. In particular, they presented an approach to automatically map a standard distributional semantic space onto a set-theoretic model using partial least squares regression. We show in this paper that a simple baseline achieves a +51% relative improvement compared to their model on one of the two datasets they used, and yields competitive results on the second dataset.\n    ",
        "submission_date": "2016-07-11T00:00:00",
        "last_modified_date": "2016-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02902",
        "title": "sk_p: a neural program corrector for MOOCs",
        "authors": [
            "Yewen Pu",
            "Karthik Narasimhan",
            "Armando Solar-Lezama",
            "Regina Barzilay"
        ],
        "abstract": "We present a novel technique for automatic program correction in MOOCs, capable of fixing both syntactic and semantic errors without manual, problem specific correction strategies. Given an incorrect student program, it generates candidate programs from a distribution of likely corrections, and checks each candidate for correctness against a test suite.\n",
        "submission_date": "2016-07-11T00:00:00",
        "last_modified_date": "2016-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.03189",
        "title": "A Framework for Estimating Long Term Driver Behavior",
        "authors": [
            "Vijay Gadepally",
            "Ashok Krishnamurthy"
        ],
        "abstract": "The authors present a cyber-physical systems study on the estimation of driver behavior in autonomous vehicles and vehicle safety systems. Extending upon previous work, the approach described is suitable for the long term estimation and tracking of autonomous vehicle behavior. The proposed system makes use of a previously defined Hybrid State System and Hidden Markov Model (HSS+HMM) system which has provided good results for driver behavior estimation. The HSS+HMM system utilizes the hybrid characteristics of decision-behavior coupling of many systems such as the driver and the vehicle, uses Kalman Filter estimates of observable parameters to track the instantaneous continuous state, and estimates the most likely driver state. The HSS+HMM system is encompassed in a HSS structure and inter-system connectivity is determined by using Signal Processing and Pattern Recognition techniques. The proposed method is suitable for scenarios that involve unknown decisions of other individuals, such as lane changes or intersection precedence/access. The long term driver behavior estimation system involves an extended HSS+HMM structure that is capable of including external information in the estimation process. Through the grafting and pruning of metastates, the HSS+HMM system can be dynamically updated to best represent driver choices given external information. Three application examples are also provided to elucidate the theoretical system.\n    ",
        "submission_date": "2016-07-11T00:00:00",
        "last_modified_date": "2016-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.03317",
        "title": "Populations can be essential in tracking dynamic optima",
        "authors": [
            "Duc-Cuong Dang",
            "Thomas Jansen",
            "Per Kristian Lehre"
        ],
        "abstract": "Real-world optimisation problems are often dynamic. Previously good solutions must be updated or replaced due to changes in objectives and constraints. It is often claimed that evolutionary algorithms are particularly suitable for dynamic optimisation because a large population can contain different solutions that may be useful in the future. However, rigorous theoretical demonstrations for how populations in dynamic optimisation can be essential are sparse and restricted to special cases.\n",
        "submission_date": "2016-07-12T00:00:00",
        "last_modified_date": "2016-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.03354",
        "title": "Extended Graded Modalities in Strategy Logic",
        "authors": [
            "Benjamin Aminof",
            "Vadim Malvone",
            "Aniello Murano",
            "Sasha Rubin"
        ],
        "abstract": "Strategy Logic (SL) is a logical formalism for strategic reasoning in multi-agent systems. Its main feature is that it has variables for strategies that are associated to specific agents with a binding operator. We introduce Graded Strategy Logic (GradedSL), an extension of SL by graded quantifiers over tuples of strategy variables, i.e., \"there exist at least g different tuples (x_1,...,x_n) of strategies\" where g is a cardinal from the set N union {aleph_0, aleph_1, 2^aleph_0}. We prove that the model-checking problem of GradedSL is decidable. We then turn to the complexity of fragments of GradedSL. When the g's are restricted to finite cardinals, written GradedNSL, the complexity of model-checking is no harder than for SL, i.e., it is non-elementary in the quantifier rank. We illustrate our formalism by showing how to count the number of different strategy profiles that are Nash equilibria (NE), or subgame-perfect equilibria (SPE). By analyzing the structure of the specific formulas involved, we conclude that the important problems of checking for the existence of a unique NE or SPE can both be solved in 2ExpTime, which is not harder than merely checking for the existence of such equilibria.\n    ",
        "submission_date": "2016-07-12T00:00:00",
        "last_modified_date": "2016-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.03516",
        "title": "Deep Reconstruction-Classification Networks for Unsupervised Domain Adaptation",
        "authors": [
            "Muhammad Ghifary",
            "W. Bastiaan Kleijn",
            "Mengjie Zhang",
            "David Balduzzi",
            "Wen Li"
        ],
        "abstract": "In this paper, we propose a novel unsupervised domain adaptation algorithm based on deep learning for visual object recognition. Specifically, we design a new model called Deep Reconstruction-Classification Network (DRCN), which jointly learns a shared encoding representation for two tasks: i) supervised classification of labeled source data, and ii) unsupervised reconstruction of unlabeled target ",
        "submission_date": "2016-07-12T00:00:00",
        "last_modified_date": "2016-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.04110",
        "title": "Using Recurrent Neural Network for Learning Expressive Ontologies",
        "authors": [
            "Giulio Petrucci",
            "Chiara Ghidini",
            "Marco Rospocher"
        ],
        "abstract": "Recently, Neural Networks have been proven extremely effective in many natural language processing tasks such as sentiment analysis, question answering, or machine translation. Aiming to exploit such advantages in the Ontology Learning process, in this technical report we present a detailed description of a Recurrent Neural Network based system to be used to pursue such goal.\n    ",
        "submission_date": "2016-07-14T00:00:00",
        "last_modified_date": "2016-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.04324",
        "title": "Random-Key Cuckoo Search for the Travelling Salesman Problem",
        "authors": [
            "Aziz Ouaarab",
            "B. Ahiod",
            "Xin-She Yang"
        ],
        "abstract": "Combinatorial optimization problems are typically NP-hard, and thus very challenging to solve. In this paper, we present the random key cuckoo search (RKCS) algorithm for solving the famous Travelling Salesman Problem (TSP). We used a simplified random-key encoding scheme to pass from a continuous space (real numbers) to a combinatorial space. We also consider the displacement of a solution in both spaces using Levy flights. The performance of the proposed RKCS is tested against a set of benchmarks of symmetric TSP from the well-known TSPLIB library. The results of the tests show that RKCS is superior to some other metaheuristic algorithms.\n    ",
        "submission_date": "2016-04-14T00:00:00",
        "last_modified_date": "2016-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.04373",
        "title": "Vista: A Visually, Socially, and Temporally-aware Model for Artistic Recommendation",
        "authors": [
            "Ruining He",
            "Chen Fang",
            "Zhaowen Wang",
            "Julian McAuley"
        ],
        "abstract": "Understanding users' interactions with highly subjective content---like artistic images---is challenging due to the complex semantics that guide our preferences. On the one hand one has to overcome `standard' recommender systems challenges, such as dealing with large, sparse, and long-tailed datasets. On the other, several new challenges present themselves, such as the need to model content in terms of its visual appearance, or even social dynamics, such as a preference toward a particular artist that is independent of the art they create.\n",
        "submission_date": "2016-07-15T00:00:00",
        "last_modified_date": "2016-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.04376",
        "title": "Intrinsically Motivated Multimodal Structure Learning",
        "authors": [
            "Jay Ming Wong",
            "Roderic A. Grupen"
        ],
        "abstract": "We present a long-term intrinsically motivated structure learning method for modeling transition dynamics during controlled interactions between a robot and semi-permanent structures in the world. In particular, we discuss how partially-observable state is represented using distributions over a Markovian state and build models of objects that predict how state distributions change in response to interactions with such objects. These structures serve as the basis for a number of possible future tasks defined as Markov Decision Processes (MDPs). The approach is an example of a structure learning technique applied to a multimodal affordance representation that yields a population of forward models for use in planning. We evaluate the approach using experiments on a bimanual mobile manipulator (uBot-6) that show the performance of model acquisition as the number of transition actions increases.\n    ",
        "submission_date": "2016-07-15T00:00:00",
        "last_modified_date": "2016-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.04817",
        "title": "Global Continuous Optimization with Error Bound and Fast Convergence",
        "authors": [
            "Kenji Kawaguchi",
            "Yu Maruyama",
            "Xiaoyu Zheng"
        ],
        "abstract": "This paper considers global optimization with a black-box unknown objective function that can be non-convex and non-differentiable. Such a difficult optimization problem arises in many real-world applications, such as parameter tuning in machine learning, engineering design problem, and planning with a complex physics simulator. This paper proposes a new global optimization algorithm, called Locally Oriented Global Optimization (LOGO), to aim for both fast convergence in practice and finite-time error bound in theory. The advantage and usage of the new algorithm are illustrated via theoretical analysis and an experiment conducted with 11 benchmark test functions. Further, we modify the LOGO algorithm to specifically solve a planning problem via policy search with continuous state/action space and long time horizon while maintaining its finite-time error bound. We apply the proposed planning method to accident management of a nuclear power plant. The result of the application study demonstrates the practical utility of our method.\n    ",
        "submission_date": "2016-07-17T00:00:00",
        "last_modified_date": "2016-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.04917",
        "title": "Piecewise convexity of artificial neural networks",
        "authors": [
            "Blaine Rister",
            "Daniel L Rubin"
        ],
        "abstract": "Although artificial neural networks have shown great promise in applications including computer vision and speech recognition, there remains considerable practical and theoretical difficulty in optimizing their parameters. The seemingly unreasonable success of gradient descent methods in minimizing these non-convex functions remains poorly understood. In this work we offer some theoretical guarantees for networks with piecewise affine activation functions, which have in recent years become the norm. We prove three main results. Firstly, that the network is piecewise convex as a function of the input data. Secondly, that the network, considered as a function of the parameters in a single layer, all others held constant, is again piecewise convex. Finally, that the network as a function of all its parameters is piecewise multi-convex, a generalization of biconvexity. From here we characterize the local minima and stationary points of the training objective, showing that they minimize certain subsets of the parameter space. We then analyze the performance of two optimization algorithms on multi-convex problems: gradient descent, and a method which repeatedly solves a number of convex sub-problems. We prove necessary convergence conditions for the first algorithm and both necessary and sufficient conditions for the second, after introducing regularization to the objective. Finally, we remark on the remaining difficulty of the global optimization problem. Under the squared error objective, we show that by varying the training data, a single rectifier neuron admits local minima arbitrarily far apart, both in objective value and parameter space.\n    ",
        "submission_date": "2016-07-17T00:00:00",
        "last_modified_date": "2016-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05174",
        "title": "Is spoken language all-or-nothing? Implications for future speech-based human-machine interaction",
        "authors": [
            "Roger K. Moore"
        ],
        "abstract": "Recent years have seen significant market penetration for voice-based personal assistants such as Apple's Siri. However, despite this success, user take-up is frustratingly low. This position paper argues that there is a habitability gap caused by the inevitable mismatch between the capabilities and expectations of human users and the features and benefits provided by contemporary technology. Suggestions are made as to how such problems might be mitigated, but a more worrisome question emerges: \"is spoken language all-or-nothing\"? The answer, based on contemporary views on the special nature of (spoken) language, is that there may indeed be a fundamental limit to the interaction that can take place between mismatched interlocutors (such as humans and machines). However, it is concluded that interactions between native and non-native speakers, or between adults and children, or even between humans and dogs, might provide critical inspiration for the design of future speech-based human-machine interaction.\n    ",
        "submission_date": "2016-07-18T00:00:00",
        "last_modified_date": "2016-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05540",
        "title": "Exploiting Vagueness for Multi-Agent Consensus",
        "authors": [
            "Michael Crosscombe",
            "Jonathan Lawry"
        ],
        "abstract": "A framework for consensus modelling is introduced using Kleene's three valued logic as a means to express vagueness in agents' beliefs. Explicitly borderline cases are inherent to propositions involving vague concepts where sentences of a propositional language may be absolutely true, absolutely false or borderline. By exploiting these intermediate truth values, we can allow agents to adopt a more vague interpretation of underlying concepts in order to weaken their beliefs and reduce the levels of inconsistency, so as to achieve consensus. We consider a consensus combination operation which results in agents adopting the borderline truth value as a shared viewpoint if they are in direct conflict. Simulation experiments are presented which show that applying this operator to agents chosen at random (subject to a consistency threshold) from a population, with initially diverse opinions, results in convergence to a smaller set of more precise shared beliefs. Furthermore, if the choice of agents for combination is dependent on the payoff of their beliefs, this acting as a proxy for performance or usefulness, then the system converges to beliefs which, on average, have higher payoff.\n    ",
        "submission_date": "2016-07-19T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05809",
        "title": "Neural Contextual Conversation Learning with Labeled Question-Answering Pairs",
        "authors": [
            "Kun Xiong",
            "Anqi Cui",
            "Zefeng Zhang",
            "Ming Li"
        ],
        "abstract": "Neural conversational models tend to produce generic or safe responses in different contexts, e.g., reply \\textit{\"Of course\"} to narrative statements or \\textit{\"I don't know\"} to questions. In this paper, we propose an end-to-end approach to avoid such problem in neural generative models. Additional memory mechanisms have been introduced to standard sequence-to-sequence (seq2seq) models, so that context can be considered while generating sentences. Three seq2seq models, which memorize a fix-sized contextual vector from hidden input, hidden input/output and a gated contextual attention structure respectively, have been trained and tested on a dataset of labeled question-answering pairs in Chinese. The model with contextual attention outperforms others including the state-of-the-art seq2seq models on perplexity test. The novel contextual model generates diverse and robust responses, and is able to carry out conversations on a wide range of topics appropriately.\n    ",
        "submission_date": "2016-07-20T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05954",
        "title": "On the estimation of stellar parameters with uncertainty prediction from Generative Artificial Neural Networks: application to Gaia RVS simulated spectra",
        "authors": [
            "C. Dafonte",
            "D. Fustes",
            "M. Manteiga",
            "D. Garabato",
            "M. A. Alvarez",
            "A. Ulla",
            "C. Allende Prieto"
        ],
        "abstract": "Aims. We present an innovative artificial neural network (ANN) architecture, called Generative ANN (GANN), that computes the forward model, that is it learns the function that relates the unknown outputs (stellar atmospheric parameters, in this case) to the given inputs (spectra). Such a model can be integrated in a Bayesian framework to estimate the posterior distribution of the outputs. Methods. The architecture of the GANN follows the same scheme as a normal ANN, but with the inputs and outputs inverted. We train the network with the set of atmospheric parameters (Teff, logg, [Fe/H] and [alpha/Fe]), obtaining the stellar spectra for such inputs. The residuals between the spectra in the grid and the estimated spectra are minimized using a validation dataset to keep solutions as general as possible. Results. The performance of both conventional ANNs and GANNs to estimate the stellar parameters as a function of the star brightness is presented and compared for different Galactic populations. GANNs provide significantly improved parameterizations for early and intermediate spectral types with rich and intermediate metallicities. The behaviour of both algorithms is very similar for our sample of late-type stars, obtaining residuals in the derivation of [Fe/H] and [alpha/Fe] below 0.1dex for stars with Gaia magnitude Grvs<12, which accounts for a number in the order of four million stars to be observed by the Radial Velocity Spectrograph of the Gaia satellite. Conclusions. Uncertainty estimation of computed astrophysical parameters is crucial for the validation of the parameterization itself and for the subsequent exploitation by the astronomical community. GANNs produce not only the parameters for a given spectrum, but a goodness-of-fit between the observed spectrum and the predicted one for a given set of parameters. Moreover, they allow us to obtain the full posterior distribution...\n    ",
        "submission_date": "2016-07-19T00:00:00",
        "last_modified_date": "2016-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06264",
        "title": "Left/Right Hand Segmentation in Egocentric Videos",
        "authors": [
            "Alejandro Betancourt",
            "Pietro Morerio",
            "Emilia Barakova",
            "Lucio Marcenaro",
            "Matthias Rauterberg",
            "Carlo Regazzoni"
        ],
        "abstract": "Wearable cameras allow people to record their daily activities from a user-centered (First Person Vision) perspective. Due to their favorable location, wearable cameras frequently capture the hands of the user, and may thus represent a promising user-machine interaction tool for different applications. Existent First Person Vision methods handle hand segmentation as a background-foreground problem, ignoring two important facts: i) hands are not a single \"skin-like\" moving element, but a pair of interacting cooperative entities, ii) close hand interactions may lead to hand-to-hand occlusions and, as a consequence, create a single hand-like segment. These facts complicate a proper understanding of hand movements and interactions. Our approach extends traditional background-foreground strategies, by including a hand-identification step (left-right) based on a Maxwell distribution of angle and position. Hand-to-hand occlusions are addressed by exploiting temporal superpixels. The experimental results show that, in addition to a reliable left/right hand-segmentation, our approach considerably improves the traditional background-foreground hand-segmentation.\n    ",
        "submission_date": "2016-07-21T00:00:00",
        "last_modified_date": "2016-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06275",
        "title": "Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering",
        "authors": [
            "Peng Li",
            "Wei Li",
            "Zhengyan He",
            "Xuguang Wang",
            "Ying Cao",
            "Jie Zhou",
            "Wei Xu"
        ],
        "abstract": "While question answering (QA) with neural network, i.e. neural QA, has achieved promising results in recent years, lacking of large scale real-word QA dataset is still a challenge for developing and evaluating neural QA system. To alleviate this problem, we propose a large scale human annotated real-world QA dataset WebQA with more than 42k questions and 556k evidences. As existing neural QA methods resolve QA either as sequence generation or classification/ranking problem, they face challenges of expensive softmax computation, unseen answers handling or separate candidate answer generation component. In this work, we cast neural QA as a sequence labeling problem and propose an end-to-end sequence labeling model, which overcomes all the above challenges. Experimental results on WebQA show that our model outperforms the baselines significantly with an F1 score of 74.69% with word-based input, and the performance drops only 3.72 F1 points with more challenging character-based input.\n    ",
        "submission_date": "2016-07-21T00:00:00",
        "last_modified_date": "2016-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06520",
        "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings",
        "authors": [
            "Tolga Bolukbasi",
            "Kai-Wei Chang",
            "James Zou",
            "Venkatesh Saligrama",
            "Adam Kalai"
        ],
        "abstract": "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to \"debias\" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.\n    ",
        "submission_date": "2016-07-21T00:00:00",
        "last_modified_date": "2016-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06532",
        "title": "Novel Word Embedding and Translation-based Language Modeling for Extractive Speech Summarization",
        "authors": [
            "Kuan-Yu Chen",
            "Shih-Hung Liu",
            "Berlin Chen",
            "Hsin-Min Wang",
            "Hsin-Hsi Chen"
        ],
        "abstract": "Word embedding methods revolve around learning continuous distributed vector representations of words with neural networks, which can capture semantic and/or syntactic cues, and in turn be used to induce similarity measures among words, sentences and documents in context. Celebrated methods can be categorized as prediction-based and count-based methods according to the training objectives and model architectures. Their pros and cons have been extensively analyzed and evaluated in recent studies, but there is relatively less work continuing the line of research to develop an enhanced learning method that brings together the advantages of the two model families. In addition, the interpretation of the learned word representations still remains somewhat opaque. Motivated by the observations and considering the pressing need, this paper presents a novel method for learning the word representations, which not only inherits the advantages of classic word embedding methods but also offers a clearer and more rigorous interpretation of the learned word representations. Built upon the proposed word embedding method, we further formulate a translation-based language modeling framework for the extractive speech summarization task. A series of empirical evaluations demonstrate the effectiveness of the proposed word representation learning and language modeling techniques in extractive speech summarization.\n    ",
        "submission_date": "2016-07-22T00:00:00",
        "last_modified_date": "2016-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06560",
        "title": "Automated Prediction of Temporal Relations",
        "authors": [
            "Amol S Patwardhan",
            "Jacob Badeaux",
            "Siavash",
            "Gerald M Knapp"
        ],
        "abstract": "Background: There has been growing research interest in automated answering of questions or generation of summary of free form text such as news article. In order to implement this task, the computer should be able to identify the sequence of events, duration of events, time at which event occurred and the relationship type between event pairs, time pairs or event-time pairs. Specific Problem: It is important to accurately identify the relationship type between combinations of event and time before the temporal ordering of events can be defined. The machine learning approach taken in Mani et. al (2006) provides an accuracy of only 62.5 on the baseline data from TimeBank. The researchers used maximum entropy classifier in their methodology. TimeML uses the TLINK annotation to tag a relationship type between events and time. The time complexity is quadratic when it comes to tagging documents with TLINK using human annotation. This research proposes using decision tree and parsing to improve the relationship type tagging. This research attempts to solve the gaps in human annotation by automating the task of relationship type tagging in an attempt to improve the accuracy of event and time relationship in annotated documents. Scope information: The documents from the domain of news will be used. The tagging will be performed within the same document and not across documents. The relationship types will be identified only for a pair of event and time and not a chain of events. The research focuses on documents tagged using the TimeML specification which contains tags such as EVENT, TLINK, and TIMEX. Each tag has attributes such as identifier, relation, POS, time etc.\n    ",
        "submission_date": "2016-07-22T00:00:00",
        "last_modified_date": "2016-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06583",
        "title": "Classification of Alzheimer's Disease Structural MRI Data by Deep Learning Convolutional Neural Networks",
        "authors": [
            "Saman Sarraf",
            "Ghassem Tofighi"
        ],
        "abstract": "Recently, machine learning techniques especially predictive modeling and pattern recognition in biomedical sciences from drug delivery system to medical imaging has become one of the important methods which are assisting researchers to have deeper understanding of entire issue and to solve complex medical problems. Deep learning is a powerful machine learning algorithm in classification while extracting low to high-level features. In this paper, we used convolutional neural network to classify Alzheimer's brain from normal healthy brain. The importance of classifying this kind of medical data is to potentially develop a predict model or system in order to recognize the type disease from normal subjects or to estimate the stage of the disease. Classification of clinical data such as Alzheimer's disease has been always challenging and most problematic part has been always selecting the most discriminative features. Using Convolutional Neural Network (CNN) and the famous architecture LeNet-5, we successfully classified structural MRI data of Alzheimer's subjects from normal controls where the accuracy of test data on trained data reached 98.84%. This experiment suggests us the shift and scale invariant features extracted by CNN followed by deep learning classification is most powerful method to distinguish clinical data from healthy data in fMRI. This approach also enables us to expand our methodology to predict more complicated systems.\n    ",
        "submission_date": "2016-07-22T00:00:00",
        "last_modified_date": "2017-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06641",
        "title": "Optimal resampling for the noisy OneMax problem",
        "authors": [
            "Jialin Liu",
            "Michael Fairbank",
            "Diego P\u00e9rez-Li\u00e9bana",
            "Simon M. Lucas"
        ],
        "abstract": "The OneMax problem is a standard benchmark optimisation problem for a binary search space. Recent work on applying a Bandit-Based Random Mutation Hill-Climbing algorithm to the noisy OneMax Problem showed that it is important to choose a good value for the resampling number to make a careful trade off between taking more samples in order to reduce noise, and taking fewer samples to reduce the total computational cost. This paper extends that observation, by deriving an analytical expression for the running time of the RMHC algorithm with resampling applied to the noisy OneMax problem, and showing both theoretically and empirically that the optimal resampling number increases with the number of dimensions in the search space.\n    ",
        "submission_date": "2016-07-22T00:00:00",
        "last_modified_date": "2017-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06667",
        "title": "Inpainting of long audio segments with similarity graphs",
        "authors": [
            "Nathanael Perraudin",
            "Nicki Holighaus",
            "Piotr Majdak",
            "Peter Balazs"
        ],
        "abstract": "We present a novel method for the compensation of long duration data loss in audio signals, in particular music. The concealment of such signal defects is based on a graph that encodes signal structure in terms of time-persistent spectral similarity. A suitable candidate segment for the substitution of the lost content is proposed by an intuitive optimization scheme and smoothly inserted into the gap, i.e. the lost or distorted signal region. Extensive listening tests show that the proposed algorithm provides highly promising results when applied to a variety of real-world music signals.\n    ",
        "submission_date": "2016-07-22T00:00:00",
        "last_modified_date": "2018-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07043",
        "title": "Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition",
        "authors": [
            "Jun Liu",
            "Amir Shahroudy",
            "Dong Xu",
            "Gang Wang"
        ],
        "abstract": "3D action recognition - analysis of human actions based on 3D skeleton data - becomes popular recently due to its succinctness, robustness, and view-invariant representation. Recent attempts on this problem suggested to develop RNN-based learning methods to model the contextual dependency in the temporal domain. In this paper, we extend this idea to spatio-temporal domains to analyze the hidden sources of action-related information within the input data over both domains concurrently. Inspired by the graphical structure of the human skeleton, we further propose a more powerful tree-structure based traversal method. To handle the noise and occlusion in 3D skeleton data, we introduce new gating mechanism within LSTM to learn the reliability of the sequential input data and accordingly adjust its effect on updating the long-term context information stored in the memory cell. Our method achieves state-of-the-art performance on 4 challenging benchmark datasets for 3D human action analysis.\n    ",
        "submission_date": "2016-07-24T00:00:00",
        "last_modified_date": "2016-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07326",
        "title": "Meta-Prod2Vec - Product Embeddings Using Side-Information for Recommendation",
        "authors": [
            "Flavian Vasile",
            "Elena Smirnova",
            "Alexis Conneau"
        ],
        "abstract": "We propose Meta-Prod2vec, a novel method to compute item similarities for recommendation that leverages existing item metadata. Such scenarios are frequently encountered in applications such as content recommendation, ad targeting and web search. Our method leverages past user interactions with items and their attributes to compute low-dimensional embeddings of items. Specifically, the item metadata is in- jected into the model as side information to regularize the item embeddings. We show that the new item representa- tions lead to better performance on recommendation tasks on an open music dataset.\n    ",
        "submission_date": "2016-07-25T00:00:00",
        "last_modified_date": "2016-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07437",
        "title": "Symbols of a cosmic order",
        "authors": [
            "F. Hadi Madjid",
            "John M. Myers"
        ],
        "abstract": "The world runs on communicated sequences of symbols, e.g. numerals. Examining both engineered and natural communications networks reveals an unsuspected order that depends on contact with an unpredictable entity. This order has three roots. The first is a proof within quantum theory that no evidence can ever determine its explanation, so that an agent choosing an explanation must do so unpredictably. The second root is the showing that clocks that step computers do not \"tell time\" but serve as self-adjusting symbol-handling agents that regulate \"logically synchronized\" motion in response to unpredictable disturbances. Such a clock-agent has a certain independence as well as the capacity to communicate via unpredictable symbols with other clock-agents and to adjust its own tick rate in response to that communication. The third root is the noticing of unpredictable symbol exchange in natural systems, including the transmission of symbols found in molecular biology. We introduce a symbol-handling agent as a role played in some cases by a person, for example a physicist who chooses an explanation of given experimental outcomes, and in other cases by some other biological entity, and in still other cases by an inanimate device, such as a computer-based detector used in physical measurements. While we forbear to try to explain the propensity of agents at all levels from cells to civilizations to form and operate networks of logically synchronized symbol-handling agents, we point to this propensity as an overlooked cosmic order, an order structured by the unpredictability ensuing from the proof. Appreciating the cosmic order leads to a conception of agency that replaces volition by unpredictability and re-conceives the notion of objectivity in a way that makes a place for agency in the world as described by physics. Some specific implications for physics are outlined.\n    ",
        "submission_date": "2016-07-26T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07514",
        "title": "Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder",
        "authors": [
            "Soroush Vosoughi",
            "Prashanth Vijayaraghavan",
            "Deb Roy"
        ],
        "abstract": "We present Tweet2Vec, a novel method for generating general-purpose vector representation of tweets. The model learns tweet embeddings using character-level CNN-LSTM encoder-decoder. We trained our model on 3 million, randomly selected English-language tweets. The model was evaluated using two methods: tweet semantic similarity and tweet sentiment categorization, outperforming the previous state-of-the-art in both tasks. The evaluations demonstrate the power of the tweet embeddings generated by our model for various tweet categorization tasks. The vector representations generated by our model are generic, and hence can be applied to a variety of tasks. Though the model presented in this paper is trained on English-language tweets, the method presented can be used to learn tweet embeddings for different languages.\n    ",
        "submission_date": "2016-07-26T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07602",
        "title": "OntoCat: Automatically categorizing knowledge in API Documentation",
        "authors": [
            "Niraj Kumar",
            "Premkumar Devanbu"
        ],
        "abstract": "Most application development happens in the context of complex APIs; reference documentation for APIs has grown tremendously in variety, complexity, and volume, and can be difficult to navigate. There is a growing need to develop well-organized ways to access the knowledge latent in the documentation; several research efforts deal with the organization (ontology) of API-related knowledge. Extensive knowledge-engineering work, supported by a rigorous qualitative analysis, by Maalej & Robillard [3] has identified a useful taxonomy of API knowledge. Based on this taxonomy, we introduce a domain independent technique to extract the knowledge types from the given API reference documentation. Our system, OntoCat, introduces total nine different features and their semantic and statistical combinations to classify the different knowledge types. We tested OntoCat on python API reference documentation. Our experimental results show the effectiveness of the system and opens the scope of probably related research areas (i.e., user behavior, documentation quality, etc.).\n    ",
        "submission_date": "2016-07-26T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07684",
        "title": "The Price of Anarchy in Auctions",
        "authors": [
            "Tim Roughgarden",
            "Vasilis Syrgkanis",
            "Eva Tardos"
        ],
        "abstract": "This survey outlines a general and modular theory for proving approximation guarantees for equilibria of auctions in complex settings. This theory complements traditional economic techniques, which generally focus on exact and optimal solutions and are accordingly limited to relatively stylized settings.\n",
        "submission_date": "2016-07-26T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07896",
        "title": "Polling-systems-based Autonomous Vehicle Coordination in Traffic Intersections with No Traffic Signals",
        "authors": [
            "David Miculescu",
            "Sertac Karaman"
        ],
        "abstract": "The rapid development of autonomous vehicles spurred a careful investigation of the potential benefits of all-autonomous transportation networks. Most studies conclude that autonomous systems can enable drastic improvements in performance. A widely studied concept is all-autonomous, collision-free intersections, where vehicles arriving in a traffic intersection with no traffic light adjust their speeds to cross safely through the intersection as quickly as possible. In this paper, we propose a coordination control algorithm for this problem, assuming stochastic models for the arrival times of the vehicles. The proposed algorithm provides provable guarantees on safety and performance. More precisely, it is shown that no collisions occur surely, and moreover a rigorous upper bound is provided for the expected wait time. The algorithm is also demonstrated in simulations. The proposed algorithms are inspired by polling systems. In fact, the problem studied in this paper leads to a new polling system where customers are subject to differential constraints, which may be interesting in its own right.\n    ",
        "submission_date": "2016-07-26T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07906",
        "title": "Approximation and Parameterized Complexity of Minimax Approval Voting",
        "authors": [
            "Marek Cygan",
            "\u0141ukasz Kowalik",
            "Arkadiusz Soca\u0142a",
            "Krzysztof Sornat"
        ],
        "abstract": "We present three results on the complexity of Minimax Approval Voting. First, we study Minimax Approval Voting parameterized by the Hamming distance $d$ from the solution to the votes. We show Minimax Approval Voting admits no algorithm running in time $\\mathcal{O}^\\star(2^{o(d\\log d)})$, unless the Exponential Time Hypothesis (ETH) fails. This means that the $\\mathcal{O}^\\star(d^{2d})$ algorithm of Misra et al. [AAMAS 2015] is essentially optimal. Motivated by this, we then show a parameterized approximation scheme, running in time $\\mathcal{O}^\\star(\\left({3}/{\\epsilon}\\right)^{2d})$, which is essentially tight assuming ETH. Finally, we get a new polynomial-time randomized approximation scheme for Minimax Approval Voting, which runs in time $n^{\\mathcal{O}(1/\\epsilon^2 \\cdot \\log(1/\\epsilon))} \\cdot \\mathrm{poly}(m)$, almost matching the running time of the fastest known PTAS for Closest String due to Ma and Sun [SIAM J. Comp. 2009].\n    ",
        "submission_date": "2016-07-26T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07956",
        "title": "Joint Embedding of Hierarchical Categories and Entities for Concept Categorization and Dataless Classification",
        "authors": [
            "Yuezhang Li",
            "Ronghuo Zheng",
            "Tian Tian",
            "Zhiting Hu",
            "Rahul Iyer",
            "Katia Sycara"
        ],
        "abstract": "Due to the lack of structured knowledge applied in learning distributed representation of cate- gories, existing work cannot incorporate category hierarchies into entity information. We propose a framework that embeds entities and categories into a semantic space by integrating structured knowledge and taxonomy hierarchy from large knowledge bases. The framework allows to com- pute meaningful semantic relatedness between entities and categories. Our framework can han- dle both single-word concepts and multiple-word concepts with superior performance on concept categorization and yield state of the art results on dataless hierarchical classification.\n    ",
        "submission_date": "2016-07-27T00:00:00",
        "last_modified_date": "2016-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08085",
        "title": "Improving Semantic Embedding Consistency by Metric Learning for Zero-Shot Classification",
        "authors": [
            "Maxime Bucher",
            "St\u00e9phane Herbin",
            "Fr\u00e9d\u00e9ric Jurie"
        ],
        "abstract": "This paper addresses the task of zero-shot image classification. The key contribution of the proposed approach is to control the semantic embedding of images -- one of the main ingredients of zero-shot learning -- by formulating it as a metric learning problem. The optimized empirical criterion associates two types of sub-task constraints: metric discriminating capacity and accurate attribute prediction. This results in a novel expression of zero-shot learning not requiring the notion of class in the training phase: only pairs of image/attributes, augmented with a consistency indicator, are given as ground truth. At test time, the learned model can predict the consistency of a test image with a given set of attributes , allowing flexible ways to produce recognition inferences. Despite its simplicity, the proposed approach gives state-of-the-art results on four challenging datasets used for zero-shot recognition evaluation.\n    ",
        "submission_date": "2016-07-27T00:00:00",
        "last_modified_date": "2016-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08098",
        "title": "The Actias system: supervised multi-strategy learning paradigm using categorical logic",
        "authors": [
            "Carlos Leandro",
            "Helder Pita",
            "Lu\u00eds Monteiro"
        ],
        "abstract": "One of the most difficult problems in the development of intelligent systems is the construction of the underlying knowledge base. As a consequence, the rate of progress in the development of this type of system is directly related to the speed with which knowledge bases can be assembled, and on its quality. We attempt to solve the knowledge acquisition problem, for a Business Information System, developing a supervised multistrategy learning paradigm. This paradigm is centred on a collaborative data mining strategy, where groups of experts collaborate using data-mining process on the supervised acquisition of new knowledge extracted from heterogeneous machine learning data models.\n",
        "submission_date": "2016-05-06T00:00:00",
        "last_modified_date": "2016-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08116",
        "title": "A DEMATEL-Based Completion Method for Incomplete Pairwise Comparison Matrix in AHP",
        "authors": [
            "Xinyi Zhou",
            "Yong Hu",
            "Yong Deng",
            "Felix T.S. Chan",
            "Alessio Ishizak"
        ],
        "abstract": "Pairwise comparison matrix as a crucial component of AHP, presents the prefer- ence relations among alternatives. However, in many cases, the pairwise comparison matrix is difficult to complete, which obstructs the subsequent operations of the clas- sical AHP. In this paper, based on DEMATEL which has ability to derive the total relation matrix from direct relation matrix, a new completion method for incomplete pairwise comparison matrix is proposed. The proposed method provides a new per- spective to estimate the missing values with explicit physical meaning. Besides, the proposed method has low computational cost. This promising method has a wide application in multi-criteria decision-making.\n    ",
        "submission_date": "2016-07-23T00:00:00",
        "last_modified_date": "2016-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08149",
        "title": "N-opcode Analysis for Android Malware Classification and Categorization",
        "authors": [
            "BooJoong Kang",
            "Suleiman Y. Yerima",
            "Kieran McLaughlin",
            "Sakir Sezer"
        ],
        "abstract": "Malware detection is a growing problem particularly on the Android mobile platform due to its increasing popularity and accessibility to numerous third party app markets. This has also been made worse by the increasingly sophisticated detection avoidance techniques employed by emerging malware families. This calls for more effective techniques for detection and classification of Android malware. Hence, in this paper we present an n-opcode analysis based approach that utilizes machine learning to classify and categorize Android malware. This approach enables automated feature discovery that eliminates the need for applying expert or domain knowledge to define the needed features. Our experiments on 2520 samples that were performed using up to 10-gram opcode features showed that an f-measure of 98% is achievable using this approach.\n    ",
        "submission_date": "2016-07-27T00:00:00",
        "last_modified_date": "2016-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08186",
        "title": "Android Malware Detection Using Parallel Machine Learning Classifiers",
        "authors": [
            "Suleiman Y. Yerima",
            "Sakir Sezer",
            "Igor Muttik"
        ],
        "abstract": "Mobile malware has continued to grow at an alarming rate despite on-going efforts towards mitigating the problem. This has been particularly noticeable on Android due to its being an open platform that has subsequently overtaken other platforms in the share of the mobile smart devices market. Hence, incentivizing a new wave of emerging Android malware sophisticated enough to evade most common detection methods. This paper proposes and investigates a parallel machine learning based classification approach for early detection of Android malware. Using real malware samples and benign applications, a composite classification model is developed from parallel combination of heterogeneous classifiers. The empirical evaluation of the model under different combination schemes demonstrates its efficacy and potential to improve detection accuracy. More importantly, by utilizing several classifiers with diverse characteristics, their strengths can be harnessed not only for enhanced Android malware detection but also quicker white box analysis by means of the more interpretable constituent classifiers.\n    ",
        "submission_date": "2016-07-27T00:00:00",
        "last_modified_date": "2016-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08325",
        "title": "VHT: Vertical Hoeffding Tree",
        "authors": [
            "Nicolas Kourtellis",
            "Gianmarco De Francisci Morales",
            "Albert Bifet",
            "Arinto Murdopo"
        ],
        "abstract": "IoT Big Data requires new machine learning methods able to scale to large size of data arriving at high speed. Decision trees are popular machine learning models since they are very effective, yet easy to interpret and visualize. In the literature, we can find distributed algorithms for learning decision trees, and also streaming algorithms, but not algorithms that combine both features. In this paper we present the Vertical Hoeffding Tree (VHT), the first distributed streaming algorithm for learning decision trees. It features a novel way of distributing decision trees via vertical parallelism. The algorithm is implemented on top of Apache SAMOA, a platform for mining distributed data streams, and thus able to run on real-world clusters. We run several experiments to study the accuracy and throughput performance of our new VHT algorithm, as well as its ability to scale while keeping its superior performance with respect to non-distributed decision trees.\n    ",
        "submission_date": "2016-07-28T00:00:00",
        "last_modified_date": "2016-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08329",
        "title": "Robust Contextual Outlier Detection: Where Context Meets Sparsity",
        "authors": [
            "Jiongqian Liang",
            "Srinivasan Parthasarathy"
        ],
        "abstract": "Outlier detection is a fundamental data science task with applications ranging from data cleaning to network security. Given the fundamental nature of the task, this has been the subject of much research. Recently, a new class of outlier detection algorithms has emerged, called {\\it contextual outlier detection}, and has shown improved performance when studying anomalous behavior in a specific context. However, as we point out in this article, such approaches have limited applicability in situations where the context is sparse (i.e. lacking a suitable frame of reference). Moreover, approaches developed to date do not scale to large datasets. To address these problems, here we propose a novel and robust approach alternative to the state-of-the-art called RObust Contextual Outlier Detection (ROCOD). We utilize a local and global behavioral model based on the relevant contexts, which is then integrated in a natural and robust fashion. We also present several optimizations to improve the scalability of the approach. We run ROCOD on both synthetic and real-world datasets and demonstrate that it outperforms other competitive baselines on the axes of efficacy and efficiency (40X speedup compared to modern contextual outlier detection methods). We also drill down and perform a fine-grained analysis to shed light on the rationale for the performance gains of ROCOD and reveal its effectiveness when handling objects with sparse contexts.\n    ",
        "submission_date": "2016-07-28T00:00:00",
        "last_modified_date": "2016-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08438",
        "title": "Faceless Person Recognition; Privacy Implications in Social Media",
        "authors": [
            "Seong Joon Oh",
            "Rodrigo Benenson",
            "Mario Fritz",
            "Bernt Schiele"
        ],
        "abstract": "As we shift more of our lives into the virtual domain, the volume of data shared on the web keeps increasing and presents a threat to our privacy. This works contributes to the understanding of privacy implications of such data sharing by analysing how well people are recognisable in social media data. To facilitate a systematic study we define a number of scenarios considering factors such as how many heads of a person are tagged and if those heads are obfuscated or not. We propose a robust person recognition system that can handle large variations in pose and clothing, and can be trained with few training samples. Our results indicate that a handful of images is enough to threaten users' privacy, even in the presence of obfuscation. We show detailed experimental results, and discuss their implications.\n    ",
        "submission_date": "2016-07-28T00:00:00",
        "last_modified_date": "2016-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08583",
        "title": "Darknet and Deepnet Mining for Proactive Cybersecurity Threat Intelligence",
        "authors": [
            "Eric Nunes",
            "Ahmad Diab",
            "Andrew Gunn",
            "Ericsson Marin",
            "Vineet Mishra",
            "Vivin Paliath",
            "John Robertson",
            "Jana Shakarian",
            "Amanda Thart",
            "Paulo Shakarian"
        ],
        "abstract": "In this paper, we present an operational system for cyber threat intelligence gathering from various social platforms on the Internet particularly sites on the darknet and deepnet. We focus our attention to collecting information from hacker forum discussions and marketplaces offering products and services focusing on malicious hacking. We have developed an operational system for obtaining information from these sites for the purposes of identifying emerging cyber threats. Currently, this system collects on average 305 high-quality cyber threat warnings each week. These threat warnings include information on newly developed malware and exploits that have not yet been deployed in a cyber-attack. This provides a significant service to cyber-defenders. The system is significantly augmented through the use of various data mining and machine learning techniques. With the use of machine learning models, we are able to recall 92% of products in marketplaces and 80% of discussions on forums relating to malicious hacking with high precision. We perform preliminary analysis on the data collected, demonstrating its application to aid a security expert for better threat analysis.\n    ",
        "submission_date": "2016-07-28T00:00:00",
        "last_modified_date": "2016-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08592",
        "title": "Modeling selectional restrictions in a relational type system",
        "authors": [
            "Erkki Luuk"
        ],
        "abstract": "Selectional restrictions are semantic constraints on forming certain complex types in natural language. The paper gives an overview of modeling selectional restrictions in a relational type system with morphological and syntactic types. We discuss some foundations of the system and ways of formalizing selectional restrictions.\n",
        "submission_date": "2016-07-28T00:00:00",
        "last_modified_date": "2016-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08665",
        "title": "Introspective Perception: Learning to Predict Failures in Vision Systems",
        "authors": [
            "Shreyansh Daftry",
            "Sam Zeng",
            "J. Andrew Bagnell",
            "Martial Hebert"
        ],
        "abstract": "As robots aspire for long-term autonomous operations in complex dynamic environments, the ability to reliably take mission-critical decisions in ambiguous situations becomes critical. This motivates the need to build systems that have situational awareness to assess how qualified they are at that moment to make a decision. We call this self-evaluating capability as introspection. In this paper, we take a small step in this direction and propose a generic framework for introspective behavior in perception systems. Our goal is to learn a model to reliably predict failures in a given system, with respect to a task, directly from input sensor data. We present this in the context of vision-based autonomous MAV flight in outdoor natural environments, and show that it effectively handles uncertain situations.\n    ",
        "submission_date": "2016-07-28T00:00:00",
        "last_modified_date": "2016-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08695",
        "title": "Semi-supervised evidential label propagation algorithm for graph data",
        "authors": [
            "Kuang Zhou",
            "Arnaud Martin",
            "Quan Pan"
        ],
        "abstract": "In the task of community detection, there often exists some useful prior information. In this paper, a Semi-supervised clustering approach using a new Evidential Label Propagation strategy (SELP) is proposed to incorporate the domain knowledge into the community detection model. The main advantage of SELP is that it can take limited supervised knowledge to guide the detection process. The prior information of community labels is expressed in the form of mass functions initially. Then a new evidential label propagation rule is adopted to propagate the labels from labeled data to unlabeled ones. The outliers can be identified to be in a special class. The experimental results demonstrate the effectiveness of SELP.\n    ",
        "submission_date": "2016-07-29T00:00:00",
        "last_modified_date": "2016-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08723",
        "title": "Cognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner",
        "authors": [
            "Emmanuel Dupoux"
        ],
        "abstract": "During their first years of life, infants learn the language(s) of their environment at an amazing speed despite large cross cultural variations in amount and complexity of the available language input. Understanding this simple fact still escapes current cognitive and linguistic theories. Recently, spectacular progress in the engineering science, notably, machine learning and wearable technology, offer the promise of revolutionizing the study of cognitive development. Machine learning offers powerful learning algorithms that can achieve human-like performance on many linguistic tasks. Wearable sensors can capture vast amounts of data, which enable the reconstruction of the sensory experience of infants in their natural environment. The project of 'reverse engineering' language development, i.e., of building an effective system that mimics infant's achievements appears therefore to be within reach. Here, we analyze the conditions under which such a project can contribute to our scientific understanding of early language development. We argue that instead of defining a sub-problem or simplifying the data, computational models should address the full complexity of the learning situation, and take as input the raw sensory signals available to infants. This implies that (1) accessible but privacy-preserving repositories of home data be setup and widely shared, and (2) models be evaluated at different linguistic levels through a benchmark of psycholinguist tests that can be passed by machines and humans alike, (3) linguistically and psychologically plausible learning architectures be scaled up to real data using probabilistic/optimization principles from machine learning. We discuss the feasibility of this approach and present preliminary results.\n    ",
        "submission_date": "2016-07-29T00:00:00",
        "last_modified_date": "2018-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08864",
        "title": "The DLVHEX System for Knowledge Representation: Recent Advances (System Description)",
        "authors": [
            "Christoph Redl"
        ],
        "abstract": "The DLVHEX system implements the HEX-semantics, which integrates answer set programming (ASP) with arbitrary external sources. Since its first release ten years ago, significant advancements were achieved. Most importantly, the exploitation of properties of external sources led to efficiency improvements and flexibility enhancements of the language, and technical improvements on the system side increased user's convenience. In this paper, we present the current status of the system and point out the most important recent enhancements over early versions. While existing literature focuses on theoretical aspects and specific components, a bird's eye view of the overall system is missing. In order to promote the system for real-world applications, we further present applications which were already successfully realized on top of DLVHEX. This paper is under consideration for acceptance in Theory and Practice of Logic Programming.\n    ",
        "submission_date": "2016-07-29T00:00:00",
        "last_modified_date": "2016-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08878",
        "title": "Identifying and Harnessing the Building Blocks of Machine Learning Pipelines for Sensible Initialization of a Data Science Automation Tool",
        "authors": [
            "Randal S. Olson",
            "Jason H. Moore"
        ],
        "abstract": "As data science continues to grow in popularity, there will be an increasing need to make data science tools more scalable, flexible, and accessible. In particular, automated machine learning (AutoML) systems seek to automate the process of designing and optimizing machine learning pipelines. In this chapter, we present a genetic programming-based AutoML system called TPOT that optimizes a series of feature preprocessors and machine learning models with the goal of maximizing classification accuracy on a supervised classification problem. Further, we analyze a large database of pipelines that were previously used to solve various supervised classification problems and identify 100 short series of machine learning operations that appear the most frequently, which we call the building blocks of machine learning pipelines. We harness these building blocks to initialize TPOT with promising solutions, and find that this sensible initialization method significantly improves TPOT's performance on one benchmark at no cost of significantly degrading performance on the others. Thus, sensible initialization with machine learning pipeline building blocks shows promise for GP-based AutoML systems, and should be further refined in future work.\n    ",
        "submission_date": "2016-07-29T00:00:00",
        "last_modified_date": "2016-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00100",
        "title": "Online Learning of Event Definitions",
        "authors": [
            "Nikos Katzouris",
            "Alexander Artikis",
            "Georgios Paliouras"
        ],
        "abstract": "Systems for symbolic event recognition infer occurrences of events in time using a set of event definitions in the form of first-order rules. The Event Calculus is a temporal logic that has been used as a basis in event recognition applications, providing among others, direct connections to machine learning, via Inductive Logic Programming (ILP). We present an ILP system for online learning of Event Calculus theories. To allow for a single-pass learning strategy, we use the Hoeffding bound for evaluating clauses on a subset of the input stream. We employ a decoupling scheme of the Event Calculus axioms during the learning process, that allows to learn each clause in isolation. Moreover, we use abductive-inductive logic programming techniques to handle unobserved target predicates. We evaluate our approach on an activity recognition application and compare it to a number of batch learning techniques. We obtain results of comparable predicative accuracy with significant speed-ups in training time. We also outperform hand-crafted rules and match the performance of a sound incremental learner that can only operate on noise-free datasets. This paper is under consideration for acceptance in TPLP.\n    ",
        "submission_date": "2016-07-30T00:00:00",
        "last_modified_date": "2016-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00329",
        "title": "Keyphrase Extraction using Sequential Labeling",
        "authors": [
            "Sujatha Das Gollapalli",
            "Xiao-li Li"
        ],
        "abstract": "Keyphrases efficiently summarize a document's content and are used in various document processing and retrieval tasks. Several unsupervised techniques and classifiers exist for extracting keyphrases from text documents. Most of these methods operate at a phrase-level and rely on part-of-speech (POS) filters for candidate phrase generation. In addition, they do not directly handle keyphrases of varying lengths. We overcome these modeling shortcomings by addressing keyphrase extraction as a sequential labeling task in this paper. We explore a basic set of features commonly used in NLP tasks as well as predictions from various unsupervised methods to train our taggers. In addition to a more natural modeling for the keyphrase extraction problem, we show that tagging models yield significant performance benefits over existing state-of-the-art extraction methods.\n    ",
        "submission_date": "2016-08-01T00:00:00",
        "last_modified_date": "2016-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00359",
        "title": "Discovering Latent States for Model Learning: Applying Sensorimotor Contingencies Theory and Predictive Processing to Model Context",
        "authors": [
            "Nikolas J. Hemion"
        ],
        "abstract": "Autonomous robots need to be able to adapt to unforeseen situations and to acquire new skills through trial and error. Reinforcement learning in principle offers a suitable methodological framework for this kind of autonomous learning. However current computational reinforcement learning agents mostly learn each individual skill entirely from scratch. How can we enable artificial agents, such as robots, to acquire some form of generic knowledge, which they could leverage for the learning of new skills? This paper argues that, like the brain, the cognitive system of artificial agents has to develop a world model to support adaptive behavior and learning. Inspiration is taken from two recent developments in the cognitive science literature: predictive processing theories of cognition, and the sensorimotor contingencies theory of perception. Based on these, a hypothesis is formulated about what the content of information might be that is encoded in an internal world model, and how an agent could autonomously acquire it. A computational model is described to formalize this hypothesis, and is evaluated in a series of simulation experiments.\n    ",
        "submission_date": "2016-08-01T00:00:00",
        "last_modified_date": "2016-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00627",
        "title": "Learning Transferable Policies for Monocular Reactive MAV Control",
        "authors": [
            "Shreyansh Daftry",
            "J. Andrew Bagnell",
            "Martial Hebert"
        ],
        "abstract": "The ability to transfer knowledge gained in previous tasks into new contexts is one of the most important mechanisms of human learning. Despite this, adapting autonomous behavior to be reused in partially similar settings is still an open problem in current robotics research. In this paper, we take a small step in this direction and propose a generic framework for learning transferable motion policies. Our goal is to solve a learning problem in a target domain by utilizing the training data in a different but related source domain. We present this in the context of an autonomous MAV flight using monocular reactive control, and demonstrate the efficacy of our proposed approach through extensive real-world flight experiments in outdoor cluttered environments.\n    ",
        "submission_date": "2016-08-01T00:00:00",
        "last_modified_date": "2016-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00655",
        "title": "A Web-based Tool for Identifying Strategic Intervention Points in Complex Systems",
        "authors": [
            "Sotiris Moschoyiannis",
            "Nicholas Elia",
            "Alexandra S. Penn",
            "David J.B. Lloyd",
            "Chris Knight"
        ],
        "abstract": "Steering a complex system towards a desired outcome is a challenging task. The lack of clarity on the system's exact architecture and the often scarce scientific data upon which to base the operationalisation of the dynamic rules that underpin the interactions between participant entities are two contributing factors. We describe an analytical approach that builds on Fuzzy Cognitive Mapping (FCM) to address the latter and represent the system as a complex network. We apply results from network controllability to address the former and determine minimal control configurations - subsets of factors, or system levers, which comprise points for strategic intervention in steering the system. We have implemented the combination of these techniques in an analytical tool that runs in the browser, and generates all minimal control configurations of a complex network.  We demonstrate our approach by reporting on our experience of working alongside industrial, local-government, and NGO stakeholders in the Humber region, UK. Our results are applied to the decision-making process involved in the transition of the region to a bio-based economy.\n    ",
        "submission_date": "2016-08-02T00:00:00",
        "last_modified_date": "2016-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00667",
        "title": "Can Active Learning Experience Be Transferred?",
        "authors": [
            "Hong-Min Chu",
            "Hsuan-Tien Lin"
        ],
        "abstract": "Active learning is an important machine learning problem in reducing the human labeling effort. Current active learning strategies are designed from human knowledge, and are applied on each dataset in an immutable manner. In other words, experience about the usefulness of strategies cannot be updated and transferred to improve active learning on other datasets. This paper initiates a pioneering study on whether active learning experience can be transferred. We first propose a novel active learning model that linearly aggregates existing strategies. The linear weights can then be used to represent the active learning experience. We equip the model with the popular linear upper- confidence-bound (LinUCB) algorithm for contextual bandit to update the weights. Finally, we extend our model to transfer the experience across datasets with the technique of biased regularization. Empirical studies demonstrate that the learned experience not only is competitive with existing strategies on most single datasets, but also can be transferred across datasets to improve the performance on future learning tasks.\n    ",
        "submission_date": "2016-08-02T00:00:00",
        "last_modified_date": "2016-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00700",
        "title": "A Survey of Visual Analysis of Human Motion and Its Applications",
        "authors": [
            "Qifei Wang"
        ],
        "abstract": "This paper summarizes the recent progress in human motion analysis and its applications. In the beginning, we reviewed the motion capture systems and the representation model of human's motion data. Next, we sketched the advanced human motion data processing technologies, including motion data filtering, temporal alignment, and segmentation. The following parts overview the state-of-the-art approaches of action recognition and dynamics measuring since these two are the most active research areas in human motion analysis. The last part discusses some emerging applications of the human motion analysis in healthcare, human robot interaction, security surveillance, virtual reality and animation. The promising research topics of human motion analysis in the future is also summarized in the last part.\n    ",
        "submission_date": "2016-08-02T00:00:00",
        "last_modified_date": "2016-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00737",
        "title": "Context Discovery for Model Learning in Partially Observable Environments",
        "authors": [
            "Nikolas J. Hemion"
        ],
        "abstract": "The ability to learn a model is essential for the success of autonomous agents. Unfortunately, learning a model is difficult in partially observable environments, where latent environmental factors influence what the agent observes. In the absence of a supervisory training signal, autonomous agents therefore require a mechanism to autonomously discover these environmental factors, or sensorimotor contexts.\n",
        "submission_date": "2016-08-02T00:00:00",
        "last_modified_date": "2016-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00876",
        "title": "Relational Similarity Machines",
        "authors": [
            "Ryan A. Rossi",
            "Rong Zhou",
            "Nesreen K. Ahmed"
        ],
        "abstract": "This paper proposes Relational Similarity Machines (RSM): a fast, accurate, and flexible relational learning framework for supervised and semi-supervised learning tasks. Despite the importance of relational learning, most existing methods are hard to adapt to different settings, due to issues with efficiency, scalability, accuracy, and flexibility for handling a wide variety of classification problems, data, constraints, and tasks. For instance, many existing methods perform poorly for multi-class classification problems, graphs that are sparsely labeled or network data with low relational autocorrelation. In contrast, the proposed relational learning framework is designed to be (i) fast for learning and inference at real-time interactive rates, and (ii) flexible for a variety of learning settings (multi-class problems), constraints (few labeled instances), and application domains. The experiments demonstrate the effectiveness of RSM for a variety of tasks and data.\n    ",
        "submission_date": "2016-08-02T00:00:00",
        "last_modified_date": "2016-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01018",
        "title": "Proceedings of the 2016 Workshop on Semantic Spaces at the Intersection of NLP, Physics and Cognitive Science",
        "authors": [
            "Dimitrios Kartsaklis",
            "Martha Lewis",
            "Laura Rimell"
        ],
        "abstract": "This volume contains the Proceedings of the 2016 Workshop on Semantic Spaces at the Intersection of NLP, Physics and Cognitive Science (SLPCS 2016), which was held on the 11th of June at the University of Strathclyde, Glasgow, and was co-located with Quantum Physics and Logic (QPL 2016). Exploiting the common ground provided by the concept of a vector space, the workshop brought together researchers working at the intersection of Natural Language Processing (NLP), cognitive science, and physics, offering them an appropriate forum for presenting their uniquely motivated work and ideas. The interplay between these three disciplines inspired theoretically motivated approaches to the understanding of how word meanings interact with each other in sentences and discourse, how diagrammatic reasoning depicts and simplifies this interaction, how language models are determined by input from the world, and how word and sentence meanings interact logically. This first edition of the workshop consisted of three invited talks from distinguished speakers (Hans Briegel, Peter G\u00e4rdenfors, Dominic Widdows) and eight presentations of selected contributed papers. Each submission was refereed by at least three members of the Programme Committee, who delivered detailed and insightful comments and suggestions.\n\n    ",
        "submission_date": "2016-08-02T00:00:00",
        "last_modified_date": "2016-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01039",
        "title": "Empirical Evaluation of Real World Tournaments",
        "authors": [
            "Nicholas Mattei",
            "Toby Walsh"
        ],
        "abstract": "Computational Social Choice (ComSoc) is a rapidly developing field at the intersection of computer science, economics, social choice, and political science. The study of tournaments is fundamental to ComSoc and many results have been published about tournament solution sets and reasoning in tournaments. Theoretical results in ComSoc tend to be worst case and tell us little about performance in practice. To this end we detail some experiments on tournaments using real wold data from soccer and tennis. We make three main contributions to the understanding of tournaments using real world data from English Premier League, the German Bundesliga, and the ATP World Tour: (1) we find that the NP-hard question of finding a seeding for which a given team can win a tournament is easily solvable in real world instances, (2) using detailed and principled methodology from statistical physics we show that our real world data obeys a log-normal distribution; and (3) leveraging our log-normal distribution result and using robust statistical methods, we show that the popular Condorcet Random (CR) tournament model does not generate realistic tournament data.\n    ",
        "submission_date": "2016-08-03T00:00:00",
        "last_modified_date": "2016-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01127",
        "title": "Autonomous Grounding of Visual Field Experience through Sensorimotor Prediction",
        "authors": [
            "Alban Laflaqui\u00e8re"
        ],
        "abstract": "In a developmental framework, autonomous robots need to explore the world and learn how to interact with it. Without an a priori model of the system, this opens the challenging problem of having robots master their interface with the world: how to perceive their environment using their sensors, and how to act in it using their motors. The sensorimotor approach of perception claims that a naive agent can learn to master this interface by capturing regularities in the way its actions transform its sensory inputs. In this paper, we apply such an approach to the discovery and mastery of the visual field associated with a visual sensor. A computational model is formalized and applied to a simulated system to illustrate the approach.\n    ",
        "submission_date": "2016-08-03T00:00:00",
        "last_modified_date": "2016-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01404",
        "title": "Quantifier Scope in Categorical Compositional Distributional Semantics",
        "authors": [
            "Mehrnoosh Sadrzadeh"
        ],
        "abstract": "In  previous  work with J. Hedges, we formalised a generalised quantifiers theory of natural language in  categorical compositional distributional semantics  with the help of bialgebras. In this paper, we show how  quantifier scope ambiguity can be represented in that setting and how this representation can be generalised to branching  quantifiers. \n    ",
        "submission_date": "2016-08-04T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01603",
        "title": "Stable Models for Infinitary Formulas with Extensional Atoms",
        "authors": [
            "Amelia Harrison",
            "Vladimir Lifschitz"
        ],
        "abstract": "The definition of stable models for propositional formulas with infinite conjunctions and disjunctions can be used to describe the semantics of answer set programming languages. In this note, we enhance that definition by introducing a distinction between intensional and extensional atoms. The symmetric splitting theorem for first-order formulas is then extended to infinitary formulas and used to reason about infinitary definitions. This note is under consideration for publication in Theory and Practice of Logic Programming.\n    ",
        "submission_date": "2016-08-04T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01716",
        "title": "A Polynomial-Time Deterministic Approach to the Traveling Salesperson Problem",
        "authors": [
            "Ali Jazayeri",
            "Hiroki Sayama"
        ],
        "abstract": "We propose a new polynomial-time deterministic algorithm that produces an approximated solution for the traveling salesperson problem. The proposed algorithm ranks cities based on their priorities calculated using a power function of means and standard deviations of their distances from other cities and then connects the cities to their neighbors in the order of their priorities. When connecting a city, a neighbor is selected based on their neighbors' priorities calculated as another power function that additionally includes their distance from the focal city to be connected. This repeats until all the cities are connected into a single loop. The time complexity of the proposed algorithm is $O(n^2)$, where $n$ is the number of cities. Numerical evaluation shows that, despite its simplicity, the proposed algorithm produces shorter tours with less time complexity than other conventional tour construction heuristics. The proposed algorithm can be used by itself or as an initial tour generator for other more complex heuristic optimization algorithms.\n    ",
        "submission_date": "2016-08-04T00:00:00",
        "last_modified_date": "2018-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01961",
        "title": "De-Conflated Semantic Representations",
        "authors": [
            "Mohammad Taher Pilehvar",
            "Nigel Collier"
        ],
        "abstract": "One major deficiency of most semantic representation techniques is that they usually model a word type as a single point in the semantic space, hence conflating all the meanings that the word can have. Addressing this issue by learning distinct representations for individual meanings of words has been the subject of several research studies in the past few years. However, the generated sense representations are either not linked to any sense inventory or are unreliable for infrequent word senses. We propose a technique that tackles these problems by de-conflating the representations of words based on the deep knowledge it derives from a semantic network. Our approach provides multiple advantages in comparison to the past work, including its high coverage and the ability to generate accurate representations even for infrequent word senses. We carry out evaluations on six datasets across two semantic similarity tasks and report state-of-the-art results on most of them.\n    ",
        "submission_date": "2016-08-05T00:00:00",
        "last_modified_date": "2016-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01987",
        "title": "Human collective intelligence as distributed Bayesian inference",
        "authors": [
            "Peter M. Krafft",
            "Julia Zheng",
            "Wei Pan",
            "Nicol\u00e1s Della Penna",
            "Yaniv Altshuler",
            "Erez Shmueli",
            "Joshua B. Tenenbaum",
            "Alex Pentland"
        ],
        "abstract": "Collective intelligence is believed to underly the remarkable success of human society. The formation of accurate shared beliefs is one of the key components of human collective intelligence. How are accurate shared beliefs formed in groups of fallible individuals? Answering this question requires a multiscale analysis. We must understand both the individual decision mechanisms people use, and the properties and dynamics of those mechanisms in the aggregate. As of yet, mathematical tools for such an approach have been lacking. To address this gap, we introduce a new analytical framework: We propose that groups arrive at accurate shared beliefs via distributed Bayesian inference. Distributed inference occurs through information processing at the individual level, and yields rational belief formation at the group level. We instantiate this framework in a new model of human social decision-making, which we validate using a dataset we collected of over 50,000 users of an online social trading platform where investors mimic each others' trades using real money in foreign exchange and other asset markets. We find that in this setting people use a decision mechanism in which popularity is treated as a prior distribution for which decisions are best to make. This mechanism is boundedly rational at the individual level, but we prove that in the aggregate implements a type of approximate \"Thompson sampling\"---a well-known and highly effective single-agent Bayesian machine learning algorithm for sequential decision-making. The perspective of distributed Bayesian inference therefore reveals how collective rationality emerges from the boundedly rational decision mechanisms people use.\n    ",
        "submission_date": "2016-08-05T00:00:00",
        "last_modified_date": "2016-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02076",
        "title": "Bi-directional Attention with Agreement for Dependency Parsing",
        "authors": [
            "Hao Cheng",
            "Hao Fang",
            "Xiaodong He",
            "Jianfeng Gao",
            "Li Deng"
        ],
        "abstract": "We develop a novel bi-directional attention model for dependency parsing, which learns to agree on headword predictions from the forward and backward parsing directions. The parsing procedure for each direction is formulated as sequentially querying the memory component that stores continuous headword embeddings. The proposed parser makes use of {\\it soft} headword embeddings, allowing the model to implicitly capture high-order parsing history without dramatically increasing the computational complexity. We conduct experiments on English, Chinese, and 12 other languages from the CoNLL 2006 shared task, showing that the proposed model achieves state-of-the-art unlabeled attachment scores on 6 languages.\n    ",
        "submission_date": "2016-08-06T00:00:00",
        "last_modified_date": "2016-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02158",
        "title": "Deep Survival Analysis",
        "authors": [
            "Rajesh Ranganath",
            "Adler Perotte",
            "No\u00e9mie Elhadad",
            "David Blei"
        ],
        "abstract": "The electronic health record (EHR) provides an unprecedented opportunity to build actionable tools to support physicians at the point of care. In this paper, we investigate survival analysis in the context of EHR data. We introduce deep survival analysis, a hierarchical generative approach to survival analysis. It departs from previous approaches in two primary ways: (1) all observations, including covariates, are modeled jointly conditioned on a rich latent structure; and (2) the observations are aligned by their failure time, rather than by an arbitrary time zero as in traditional survival analysis. Further, it (3) scalably handles heterogeneous (continuous and discrete) data types that occur in the EHR. We validate deep survival analysis model by stratifying patients according to risk of developing coronary heart disease (CHD). Specifically, we study a dataset of 313,000 patients corresponding to 5.5 million months of observations. When compared to the clinically validated Framingham CHD risk score, deep survival analysis is significantly superior in stratifying patients according to their risk.\n    ",
        "submission_date": "2016-08-06T00:00:00",
        "last_modified_date": "2016-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02164",
        "title": "Adapting Deep Network Features to Capture Psychological Representations",
        "authors": [
            "Joshua C. Peterson",
            "Joshua T. Abbott",
            "Thomas L. Griffiths"
        ],
        "abstract": "Deep neural networks have become increasingly successful at solving classic perception problems such as object recognition, semantic segmentation, and scene understanding, often reaching or surpassing human-level accuracy. This success is due in part to the ability of DNNs to learn useful representations of high-dimensional inputs, a problem that humans must also solve. We examine the relationship between the representations learned by these networks and human psychological representations recovered from similarity judgments. We find that deep features learned in service of object classification account for a significant amount of the variance in human similarity judgments for a set of animal images. However, these features do not capture some qualitative distinctions that are a key part of human representations. To remedy this, we develop a method for adapting deep features to align with human similarity judgments, resulting in image representations that can potentially be used to extend the scope of psychological experiments.\n    ",
        "submission_date": "2016-08-06T00:00:00",
        "last_modified_date": "2016-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02165",
        "title": "ShapeFit and ShapeKick for Robust, Scalable Structure from Motion",
        "authors": [
            "Thomas Goldstein",
            "Paul Hand",
            "Choongbum Lee",
            "Vladislav Voroninski",
            "Stefano Soatto"
        ],
        "abstract": "We introduce a new method for location recovery from pair-wise directions that leverages an efficient convex program that comes with exact recovery guarantees, even in the presence of adversarial outliers. When pairwise directions represent scaled relative positions between pairs of views (estimated for instance with epipolar geometry) our method can be used for location recovery, that is the determination of relative pose up to a single unknown scale. For this task, our method yields performance comparable to the state-of-the-art with an order of magnitude speed-up. Our proposed numerical framework is flexible in that it accommodates other approaches to location recovery and can be used to speed up other methods. These properties are demonstrated by extensively testing against state-of-the-art methods for location recovery on 13 large, irregular collections of images of real scenes in addition to simulated data with ground truth.\n    ",
        "submission_date": "2016-08-07T00:00:00",
        "last_modified_date": "2016-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02229",
        "title": "Towards the Self-constructive Brain: emergence of adaptive behavior",
        "authors": [
            "Fernando Corbacho"
        ],
        "abstract": "Adaptive behavior is mainly the result of adaptive brains. We go a step beyond and claim that the brain does not only adapt to its surrounding reality but rather, it builds itself up to constructs its own reality. That is, rather than just trying to passively understand its environment, the brain is the architect of its own reality in an active process where its internal models of the external world frame how its new interactions with the environment are assimilated. These internal models represent relevant predictive patterns of interaction all over the different brain structures: perceptual, sensorimotor, motor, etc. The emergence of adaptive behavior arises from this self-constructive nature of the brain, based on the following principles of organization: self-experimental, self- growing, and self-repairing. Self-experimental, since to ensure survival, the self-constructive brain (SCB) is an active machine capable of performing experiments of its own interactions with the environment by mental simulation. Self-growing, since it dynamically and incrementally constructs internal structures in order to build a model of the world as it gathers statistics from its interactions with the environment. Self-repairing, since to survive the SCB must also be robust and capable of finding ways to repair parts of previously working structures and hence re-construct a previous relevant pattern of activity.\n    ",
        "submission_date": "2016-08-07T00:00:00",
        "last_modified_date": "2016-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02341",
        "title": "Towards Representation Learning with Tractable Probabilistic Models",
        "authors": [
            "Antonio Vergari",
            "Nicola Di Mauro",
            "Floriana Esposito"
        ],
        "abstract": "Probabilistic models learned as density estimators can be exploited in representation learning beside being toolboxes used to answer inference queries only. However, how to extract useful representations highly depends on the particular model involved. We argue that tractable inference, i.e. inference that can be computed in polynomial time, can enable general schemes to extract features from black box models. We plan to investigate how Tractable Probabilistic Models (TPMs) can be exploited to generate embeddings by random query evaluations. We devise two experimental designs to assess and compare different TPMs as feature extractors in an unsupervised representation learning framework. We show some experimental results on standard image datasets by applying such a method to Sum-Product Networks and Mixture of Trees as tractable models generating embeddings.\n    ",
        "submission_date": "2016-08-08T00:00:00",
        "last_modified_date": "2016-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02658",
        "title": "Revisiting Causality Inference in Memory-less Transition Networks",
        "authors": [
            "Abbas Shojaee",
            "Isuru Ranasinghe",
            "Alireza Ani"
        ],
        "abstract": "Several methods exist to infer causal networks from massive volumes of observational data. However, almost all existing methods require a considerable length of time series data to capture cause and effect relationships. In contrast, memory-less transition networks or Markov Chain data, which refers to one-step transitions to and from an event, have not been explored for causality inference even though such data is widely available. We find that causal network can be inferred from characteristics of four unique distribution zones around each event. We call this Composition of Transitions and show that cause, effect, and random events exhibit different behavior in their compositions. We applied machine learning models to learn these different behaviors and to infer causality. We name this new method Causality Inference using Composition of Transitions (CICT). To evaluate CICT, we used an administrative inpatient healthcare dataset to set up a network of patients transitions between different diagnoses. We show that CICT is highly accurate in inferring whether the transition between a pair of events is causal or random and performs well in identifying the direction of causality in a bi-directional association.\n    ",
        "submission_date": "2016-08-08T00:00:00",
        "last_modified_date": "2016-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02717",
        "title": "Mean Box Pooling: A Rich Image Representation and Output Embedding for the Visual Madlibs Task",
        "authors": [
            "Ashkan Mokarian",
            "Mateusz Malinowski",
            "Mario Fritz"
        ],
        "abstract": "We present Mean Box Pooling, a novel visual representation that pools over CNN representations of a large number, highly overlapping object proposals. We show that such representation together with nCCA, a successful multimodal embedding technique, achieves state-of-the-art performance on the Visual Madlibs task. Moreover, inspired by the nCCA's objective function, we extend classical CNN+LSTM approach to train the network by directly maximizing the similarity between the internal representation of the deep learning architecture and candidate answers. Again, such approach achieves a significant improvement over the prior work that also uses CNN+LSTM approach on Visual Madlibs.\n    ",
        "submission_date": "2016-08-09T00:00:00",
        "last_modified_date": "2016-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02833",
        "title": "Facial Expression Recognition Using a Hybrid CNN-SIFT Aggregator",
        "authors": [
            "Mundher Al-Shabi",
            "Wooi Ping Cheah",
            "Tee Connie"
        ],
        "abstract": "Deriving an effective facial expression recognition component is important for a successful human-computer interaction system. Nonetheless, recognizing facial expression remains a challenging task. This paper describes a novel approach towards facial expression recognition task. The proposed method is motivated by the success of Convolutional Neural Networks (CNN) on the face recognition problem. Unlike other works, we focus on achieving good accuracy while requiring only a small sample data for training. Scale Invariant Feature Transform (SIFT) features are used to increase the performance on small data as SIFT does not require extensive training data to generate useful features. In this paper, both Dense SIFT and regular SIFT are studied and compared when merged with CNN features. Moreover, an aggregator of the models is developed. The proposed approach is tested on the FER-2013 and CK+ datasets. Results demonstrate the superiority of CNN with Dense SIFT over conventional CNN and CNN with SIFT. The accuracy even increased when all the models are aggregated which generates state-of-art results on FER-2013 and CK+ datasets, where it achieved 73.4% on FER-2013 and 99.1% on CK+.\n    ",
        "submission_date": "2016-08-09T00:00:00",
        "last_modified_date": "2017-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02971",
        "title": "Neuroevolution-Based Inverse Reinforcement Learning",
        "authors": [
            "Karan K. Budhraja",
            "Tim Oates"
        ],
        "abstract": "The problem of Learning from Demonstration is targeted at learning to perform tasks based on observed examples. One approach to Learning from Demonstration is Inverse Reinforcement Learning, in which actions are observed to infer rewards. This work combines a feature based state evaluation approach to Inverse Reinforcement Learning with neuroevolution, a paradigm for modifying neural networks based on their performance on a given task. Neural networks are used to learn from a demonstrated expert policy and are evolved to generate a policy similar to the demonstration. The algorithm is discussed and evaluated against competitive feature-based Inverse Reinforcement Learning approaches. At the cost of execution time, neural networks allow for non-linear combinations of features in state evaluations. These valuations may correspond to state value or state reward. This results in better correspondence to observed examples as opposed to using linear combinations. This work also extends existing work on Bayesian Non-Parametric Feature Construction for Inverse Reinforcement Learning by using non-linear combinations of intermediate data to improve performance. The algorithm is observed to be specifically suitable for a linearly solvable non-deterministic Markov Decision Processes in which multiple rewards are sparsely scattered in state space. A conclusive performance hierarchy between evaluated algorithms is presented.\n    ",
        "submission_date": "2016-08-09T00:00:00",
        "last_modified_date": "2016-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03000",
        "title": "Neural Generation of Regular Expressions from Natural Language with Minimal Domain Knowledge",
        "authors": [
            "Nicholas Locascio",
            "Karthik Narasimhan",
            "Eduardo DeLeon",
            "Nate Kushman",
            "Regina Barzilay"
        ],
        "abstract": "This paper explores the task of translating natural language queries into regular expressions which embody their meaning. In contrast to prior work, the proposed neural model does not utilize domain-specific crafting, learning to translate directly from a parallel corpus. To fully explore the potential of neural models, we propose a methodology for collecting a large corpus of regular expression, natural language pairs. Our resulting model achieves a performance gain of 19.6% over previous state-of-the-art models.\n    ",
        "submission_date": "2016-08-09T00:00:00",
        "last_modified_date": "2016-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03235",
        "title": "Gaze2Segment: A Pilot Study for Integrating Eye-Tracking Technology into Medical Image Segmentation",
        "authors": [
            "Naji Khosravan",
            "Haydar Celik",
            "Baris Turkbey",
            "Ruida Cheng",
            "Evan McCreedy",
            "Matthew McAuliffe",
            "Sandra Bednarova",
            "Elizabeth Jones",
            "Xinjian Chen",
            "Peter L. Choyke",
            "Bradford J. Wood",
            "Ulas Bagci"
        ],
        "abstract": "This study introduced a novel system, called Gaze2Segment, integrating biological and computer vision techniques to support radiologists' reading experience with an automatic image segmentation task. During diagnostic assessment of lung CT scans, the radiologists' gaze information were used to create a visual attention map. This map was then combined with a computer-derived saliency map, extracted from the gray-scale CT images. The visual attention map was used as an input for indicating roughly the location of a object of interest. With computer-derived saliency information, on the other hand, we aimed at finding foreground and background cues for the object of interest. At the final step, these cues were used to initiate a seed-based delineation process. Segmentation accuracy of the proposed Gaze2Segment was found to be 86% with dice similarity coefficient and 1.45 mm with Hausdorff distance. To the best of our knowledge, Gaze2Segment is the first true integration of eye-tracking technology into a medical image segmentation task without the need for any further user-interaction.\n    ",
        "submission_date": "2016-08-10T00:00:00",
        "last_modified_date": "2016-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03544",
        "title": "On Context-Dependent Clustering of Bandits",
        "authors": [
            "Claudio Gentile",
            "Shuai Li",
            "Purushottam Kar",
            "Alexandros Karatzoglou",
            "Evans Etrue",
            "Giovanni Zappella"
        ],
        "abstract": "We investigate a novel cluster-of-bandit algorithm CAB for collaborative recommendation tasks that implements the underlying feedback sharing mechanism by estimating the neighborhood of users in a context-dependent manner. CAB makes sharp departures from the state of the art by incorporating collaborative effects into inference as well as learning processes in a manner that seamlessly interleaving explore-exploit tradeoffs and collaborative steps. We prove regret bounds under various assumptions on the data, which exhibit a crisp dependence on the expected number of clusters over the users, a natural measure of the statistical difficulty of the learning task. Experiments on production and real-world datasets show that CAB offers significantly increased prediction performance against a representative pool of state-of-the-art methods.\n    ",
        "submission_date": "2016-08-06T00:00:00",
        "last_modified_date": "2017-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03845",
        "title": "Traversing Environments Using Possibility Graphs for Humanoid Robots",
        "authors": [
            "Michael X. Grey",
            "Aaron D. Ames",
            "C. Karen Liu"
        ],
        "abstract": "Locomotion for legged robots poses considerable challenges when confronted by obstacles and adverse environments. Footstep planners are typically only designed for one mode of locomotion, but traversing unfavorable environments may require several forms of locomotion to be sequenced together, such as walking, crawling, and jumping. Multi-modal motion planners can be used to address some of these problems, but existing implementations tend to be time-consuming and are limited to quasi-static actions. This paper presents a motion planning method to traverse complex environments using multiple categories of actions. We introduce the concept of the \"Possibility Graph\", which uses high-level approximations of constraint manifolds to rapidly explore the \"possibility\" of actions, thereby allowing lower-level single-action motion planners to be utilized more efficiently. We show that the Possibility Graph can quickly find paths through several different challenging environments which require various combinations of actions in order to traverse.\n    ",
        "submission_date": "2016-08-12T00:00:00",
        "last_modified_date": "2016-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03938",
        "title": "Determining Health Utilities through Data Mining of Social Media",
        "authors": [
            "Christopher Thompson",
            "Josh Introne",
            "Clint Young"
        ],
        "abstract": "'Health utilities' measure patient preferences for perfect health compared to specific unhealthy states, such as asthma, a fractured hip, or colon cancer. When integrated over time, these estimations are called quality adjusted life years (QALYs). Until now, characterizing health utilities (HUs) required detailed patient interviews or written surveys. While reliable and specific, this data remained costly due to efforts to locate, enlist and coordinate participants. Thus the scope, context and temporality of diseases examined has remained limited.\n",
        "submission_date": "2016-08-13T00:00:00",
        "last_modified_date": "2016-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04042",
        "title": "Can Peripheral Representations Improve Clutter Metrics on Complex Scenes?",
        "authors": [
            "Arturo Deza",
            "Miguel P. Eckstein"
        ],
        "abstract": "Previous studies have proposed image-based clutter measures that correlate with human search times and/or eye movements. However, most models do not take into account the fact that the effects of clutter interact with the foveated nature of the human visual system: visual clutter further from the fovea has an increasing detrimental influence on perception. Here, we introduce a new foveated clutter model to predict the detrimental effects in target search utilizing a forced fixation search task. We use Feature Congestion (Rosenholtz et al.) as our non foveated clutter model, and we stack a peripheral architecture on top of Feature Congestion for our foveated model. We introduce the Peripheral Integration Feature Congestion (PIFC) coefficient, as a fundamental ingredient of our model that modulates clutter as a non-linear gain contingent on eccentricity. We finally show that Foveated Feature Congestion (FFC) clutter scores r(44) = -0.82 correlate better with target detection (hit rate) than regular Feature Congestion r(44) = -0.19 in forced fixation search. Thus, our model allows us to enrich clutter perception research by computing fixation specific clutter maps. A toolbox for creating peripheral architectures: Piranhas: Peripheral Architectures for Natural, Hybrid and Artificial Systems will be made available.\n    ",
        "submission_date": "2016-08-14T00:00:00",
        "last_modified_date": "2016-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04361",
        "title": "Multi-way Monte Carlo Method for Linear Systems",
        "authors": [
            "Tao Wu",
            "David F. Gleich"
        ],
        "abstract": "We study the Monte Carlo method for solving a linear system of the form $x = H x + b$. A sufficient condition for the method to work is $\\| H \\| < 1$, which greatly limits the usability of this method. We improve this condition by proposing a new multi-way Markov random walk, which is a generalization of the standard Markov random walk. Under our new framework we prove that the necessary and sufficient condition for our method to work is the spectral radius $\\rho(H^{+}) < 1$, which is a weaker requirement than $\\| H \\| < 1$. In addition to solving more problems, our new method can work faster than the standard algorithm. In numerical experiments on both synthetic and real world matrices, we demonstrate the effectiveness of our new method.\n    ",
        "submission_date": "2016-08-15T00:00:00",
        "last_modified_date": "2016-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04374",
        "title": "A Geometric Framework for Convolutional Neural Networks",
        "authors": [
            "Anthony L. Caterini",
            "Dong Eui Chang"
        ],
        "abstract": "In this paper, a geometric framework for neural networks is proposed. This framework uses the inner product space structure underlying the parameter set to perform gradient descent not in a component-based form, but in a coordinate-free manner. Convolutional neural networks are described in this framework in a compact form, with the gradients of standard --- and higher-order --- loss functions calculated for each layer of the network. This approach can be applied to other network structures and provides a basis on which to create new networks.\n    ",
        "submission_date": "2016-08-15T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04428",
        "title": "TerpreT: A Probabilistic Programming Language for Program Induction",
        "authors": [
            "Alexander L. Gaunt",
            "Marc Brockschmidt",
            "Rishabh Singh",
            "Nate Kushman",
            "Pushmeet Kohli",
            "Jonathan Taylor",
            "Daniel Tarlow"
        ],
        "abstract": "We study machine learning formulations of inductive program synthesis; given input-output examples, we try to synthesize source code that maps inputs to corresponding outputs. Our aims are to develop new machine learning approaches based on neural networks and graphical models, and to understand the capabilities of machine learning techniques relative to traditional alternatives, such as those based on constraint solving from the programming languages community.\n",
        "submission_date": "2016-08-15T00:00:00",
        "last_modified_date": "2016-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04544",
        "title": "Free Lunch for Optimisation under the Universal Distribution",
        "authors": [
            "Tom Everitt",
            "Tor Lattimore",
            "Marcus Hutter"
        ],
        "abstract": "Function optimisation is a major challenge in computer science. The No Free Lunch theorems state that if all functions with the same histogram are assumed to be equally probable then no algorithm outperforms any other in expectation. We argue against the uniform assumption and suggest a universal prior exists for which there is a free lunch, but where no particular class of functions is favoured over another. We also prove upper and lower bounds on the size of the free lunch.\n    ",
        "submission_date": "2016-08-16T00:00:00",
        "last_modified_date": "2016-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04839",
        "title": "Dynamic Collaborative Filtering with Compound Poisson Factorization",
        "authors": [
            "Ghassen Jerfel",
            "Mehmet E. Basbug",
            "Barbara E. Engelhardt"
        ],
        "abstract": "Model-based collaborative filtering analyzes user-item interactions to infer latent factors that represent user preferences and item characteristics in order to predict future interactions. Most collaborative filtering algorithms assume that these latent factors are static, although it has been shown that user preferences and item perceptions drift over time. In this paper, we propose a conjugate and numerically stable dynamic matrix factorization (DCPF) based on compound Poisson matrix factorization that models the smoothly drifting latent factors using Gamma-Markov chains. We propose a numerically stable Gamma chain construction, and then present a stochastic variational inference approach to estimate the parameters of our model. We apply our model to time-stamped ratings data sets: Netflix, Yelp, and ",
        "submission_date": "2016-08-17T00:00:00",
        "last_modified_date": "2016-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04846",
        "title": "A Convolutional Autoencoder for Multi-Subject fMRI Data Aggregation",
        "authors": [
            "Po-Hsuan Chen",
            "Xia Zhu",
            "Hejia Zhang",
            "Javier S. Turek",
            "Janice Chen",
            "Theodore L. Willke",
            "Uri Hasson",
            "Peter J. Ramadge"
        ],
        "abstract": "Finding the most effective way to aggregate multi-subject fMRI data is a long-standing and challenging problem. It is of increasing interest in contemporary fMRI studies of human cognition due to the scarcity of data per subject and the variability of brain anatomy and functional response across subjects. Recent work on latent factor models shows promising results in this task but this approach does not preserve spatial locality in the brain. We examine two ways to combine the ideas of a factor model and a searchlight based analysis to aggregate multi-subject fMRI data while preserving spatial locality. We first do this directly by combining a recent factor method known as a shared response model with searchlight analysis. Then we design a multi-view convolutional autoencoder for the same task. Both approaches preserve spatial locality and have competitive or better performance compared with standard searchlight analysis and the shared response model applied across the whole brain. We also report a system design to handle the computational challenge of training the convolutional autoencoder.\n    ",
        "submission_date": "2016-08-17T00:00:00",
        "last_modified_date": "2016-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04868",
        "title": "Towards Music Captioning: Generating Music Playlist Descriptions",
        "authors": [
            "Keunwoo Choi",
            "George Fazekas",
            "Brian McFee",
            "Kyunghyun Cho",
            "Mark Sandler"
        ],
        "abstract": "Descriptions are often provided along with recommendations to help users' discovery. Recommending automatically generated music playlists (e.g. personalised playlists) introduces the problem of generating descriptions. In this paper, we propose a method for generating music playlist descriptions, which is called as music captioning. In the proposed method, audio content analysis and natural language processing are adopted to utilise the information of each track.\n    ",
        "submission_date": "2016-08-17T00:00:00",
        "last_modified_date": "2017-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05675",
        "title": "lpopt: A Rule Optimization Tool for Answer Set Programming",
        "authors": [
            "Manuel Bichler",
            "Michael Morak",
            "Stefan Woltran"
        ],
        "abstract": "State-of-the-art answer set programming (ASP) solvers rely on a program called a grounder to convert non-ground programs containing variables into variable-free, propositional programs. The size of this grounding depends heavily on the size of the non-ground rules, and thus, reducing the size of such rules is a promising approach to improve solving performance. To this end, in this paper we announce lpopt, a tool that decomposes large logic programming rules into smaller rules that are easier to handle for current solvers. The tool is specifically tailored to handle the standard syntax of the ASP language (ASP-Core) and makes it easier for users to write efficient and intuitive ASP programs, which would otherwise often require significant hand-tuning by expert ASP engineers. It is based on an idea proposed by Morak and Woltran (2012) that we extend significantly in order to handle the full ASP syntax, including complex constructs like aggregates, weak constraints, and arithmetic expressions. We present the algorithm, the theoretical foundations on how to treat these constructs, as well as an experimental evaluation showing the viability of our approach.\n    ",
        "submission_date": "2016-08-19T00:00:00",
        "last_modified_date": "2016-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05745",
        "title": "RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism",
        "authors": [
            "Edward Choi",
            "Mohammad Taha Bahadori",
            "Joshua A. Kulas",
            "Andy Schuetz",
            "Walter F. Stewart",
            "Jimeng Sun"
        ],
        "abstract": "Accuracy and interpretability are two dominant features of successful predictive models. Typically, a choice must be made in favor of complex black box models such as recurrent neural networks (RNN) for accuracy versus less accurate but more interpretable traditional models such as logistic regression. This tradeoff poses challenges in medicine where both accuracy and interpretability are important. We addressed this challenge by developing the REverse Time AttentIoN model (RETAIN) for application to Electronic Health Records (EHR) data. RETAIN achieves high accuracy while remaining clinically interpretable and is based on a two-level neural attention model that detects influential past visits and significant clinical variables within those visits (e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR data in a reverse time order so that recent clinical visits are likely to receive higher attention. RETAIN was tested on a large health system EHR dataset with 14 million visits completed by 263K patients over an 8 year period and demonstrated predictive accuracy and computational scalability comparable to state-of-the-art methods such as RNN, and ease of interpretability comparable to traditional models.\n    ",
        "submission_date": "2016-08-19T00:00:00",
        "last_modified_date": "2017-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05852",
        "title": "Learning Word Embeddings from Intrinsic and Extrinsic Views",
        "authors": [
            "Jifan Chen",
            "Kan Chen",
            "Xipeng Qiu",
            "Qi Zhang",
            "Xuanjing Huang",
            "Zheng Zhang"
        ],
        "abstract": "While word embeddings are currently predominant for natural language processing, most of existing models learn them solely from their contexts. However, these context-based word embeddings are limited since not all words' meaning can be learned based on only context. Moreover, it is also difficult to learn the representation of the rare words due to data sparsity problem. In this work, we address these issues by learning the representations of words by integrating their intrinsic (descriptive) and extrinsic (contextual) information. To prove the effectiveness of our model, we evaluate it on four tasks, including word similarity, reverse dictionaries,Wiki link prediction, and document classification. Experiment results show that our model is powerful in both word and document modeling.\n    ",
        "submission_date": "2016-08-20T00:00:00",
        "last_modified_date": "2016-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05864",
        "title": "A Hybrid, PDE-ODE Control Strategy for Intercepting an Intelligent, well-informed Target in a Stationary, Cluttered Environment",
        "authors": [
            "Ahmad A. Masoud"
        ],
        "abstract": "In [1,2] a new class of intelligent controllers that can semantically embed an agent in a spatial context constraining its behavior in a goal-oriented manner was suggested. A controller of such a class can guide an agent in a stationary unknown environment to a fixed target zone along an obstacle-free trajectory. Here, an extension is suggested that would enable the interception of an intelligent target that is maneuvering to evade capture amidst stationary clutter (i.e. the target zone is moving). This is achieved by forcing the differential properties of the potential field used to induce the control action to satisfy the wave equation. Background of the problem, theoretical developments, as well as, proofs of the ability of the modified control to intercept the target along an obstacle-free trajectory are supplied. Simulation results are also provided.\n    ",
        "submission_date": "2016-08-20T00:00:00",
        "last_modified_date": "2016-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05921",
        "title": "Probabilistic Knowledge Graph Construction: Compositional and Incremental Approaches",
        "authors": [
            "Dongwoo Kim",
            "Lexing Xie",
            "Cheng Soon Ong"
        ],
        "abstract": "Knowledge graph construction consists of two tasks: extracting information from external resources (knowledge population) and inferring missing information through a statistical analysis on the extracted information (knowledge completion). In many cases, insufficient external resources in the knowledge population hinder the subsequent statistical inference. The gap between these two processes can be reduced by an incremental population approach. We propose a new probabilistic knowledge graph factorisation method that benefits from the path structure of existing knowledge (e.g. syllogism) and enables a common modelling approach to be used for both incremental population and knowledge completion tasks. More specifically, the probabilistic formulation allows us to develop an incremental population algorithm that trades off exploitation-exploration. Experiments on three benchmark datasets show that the balanced exploitation-exploration helps the incremental population, and the additional path structure helps to predict missing information in knowledge completion.\n    ",
        "submission_date": "2016-08-21T00:00:00",
        "last_modified_date": "2016-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06010",
        "title": "Feedback-Controlled Sequential Lasso Screening",
        "authors": [
            "Yun Wang",
            "Xu Chen",
            "Peter J. Ramadge"
        ],
        "abstract": "One way to solve lasso problems when the dictionary does not fit into available memory is to first screen the dictionary to remove unneeded features. Prior research has shown that sequential screening methods offer the greatest promise in this endeavor. Most existing work on sequential screening targets the context of tuning parameter selection, where one screens and solves a sequence of $N$ lasso problems with a fixed grid of geometrically spaced regularization parameters. In contrast, we focus on the scenario where a target regularization parameter has already been chosen via cross-validated model selection, and we then need to solve many lasso instances using this fixed value. In this context, we propose and explore a feedback controlled sequential screening scheme. Feedback is used at each iteration to select the next problem to be solved. This allows the sequence of problems to be adapted to the instance presented and the number of intermediate problems to be automatically selected. We demonstrate our feedback scheme using several datasets including a dictionary of approximate size 100,000 by 300,000.\n    ",
        "submission_date": "2016-08-21T00:00:00",
        "last_modified_date": "2016-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06014",
        "title": "The Symmetry of a Simple Optimization Problem in Lasso Screening",
        "authors": [
            "Yun Wang",
            "Peter J. Ramadge"
        ],
        "abstract": "Recently dictionary screening has been proposed as an effective way to improve the computational efficiency of solving the lasso problem, which is one of the most commonly used method for learning sparse representations. To address today's ever increasing large dataset, effective screening relies on a tight region bound on the solution to the dual lasso. Typical region bounds are in the form of an intersection of a sphere and multiple half spaces. One way to tighten the region bound is using more half spaces, which however, adds to the overhead of solving the high dimensional optimization problem in lasso screening. This paper reveals the interesting property that the optimization problem only depends on the projection of features onto the subspace spanned by the normals of the half spaces. This property converts an optimization problem in high dimension to much lower dimension, and thus sheds light on reducing the computation overhead of lasso screening based on tighter region bounds.\n    ",
        "submission_date": "2016-08-21T00:00:00",
        "last_modified_date": "2016-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06154",
        "title": "Multi-Sensor Prognostics using an Unsupervised Health Index based on LSTM Encoder-Decoder",
        "authors": [
            "Pankaj Malhotra",
            "Vishnu TV",
            "Anusha Ramakrishnan",
            "Gaurangi Anand",
            "Lovekesh Vig",
            "Puneet Agarwal",
            "Gautam Shroff"
        ],
        "abstract": "Many approaches for estimation of Remaining Useful Life (RUL) of a machine, using its operational sensor data, make assumptions about how a system degrades or a fault evolves, e.g., exponential degradation. However, in many domains degradation may not follow a pattern. We propose a Long Short Term Memory based Encoder-Decoder (LSTM-ED) scheme to obtain an unsupervised health index (HI) for a system using multi-sensor time-series data. LSTM-ED is trained to reconstruct the time-series corresponding to healthy state of a system. The reconstruction error is used to compute HI which is then used for RUL estimation. We evaluate our approach on publicly available Turbofan Engine and Milling Machine datasets. We also present results on a real-world industry dataset from a pulverizer mill where we find significant correlation between LSTM-ED based HI and maintenance costs.\n    ",
        "submission_date": "2016-08-22T00:00:00",
        "last_modified_date": "2016-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06403",
        "title": "Phased Exploration with Greedy Exploitation in Stochastic Combinatorial Partial Monitoring Games",
        "authors": [
            "Sougata Chaudhuri",
            "Ambuj Tewari"
        ],
        "abstract": "Partial monitoring games are repeated games where the learner receives feedback that might be different from adversary's move or even the reward gained by the learner. Recently, a general model of combinatorial partial monitoring (CPM) games was proposed \\cite{lincombinatorial2014}, where the learner's action space can be exponentially large and adversary samples its moves from a bounded, continuous space, according to a fixed distribution. The paper gave a confidence bound based algorithm (GCB) that achieves $O(T^{2/3}\\log T)$ distribution independent and $O(\\log T)$ distribution dependent regret bounds. The implementation of their algorithm depends on two separate offline oracles and the distribution dependent regret additionally requires existence of a unique optimal action for the learner. Adopting their CPM model, our first contribution is a Phased Exploration with Greedy Exploitation (PEGE) algorithmic framework for the problem. Different algorithms within the framework achieve $O(T^{2/3}\\sqrt{\\log T})$ distribution independent and $O(\\log^2 T)$ distribution dependent regret respectively. Crucially, our framework needs only the simpler \"argmax\" oracle from GCB and the distribution dependent regret does not require existence of a unique optimal action. Our second contribution is another algorithm, PEGE2, which combines gap estimation with a PEGE algorithm, to achieve an $O(\\log T)$ regret bound, matching the GCB guarantee but removing the dependence on size of the learner's action space. However, like GCB, PEGE2 requires access to both offline oracles and the existence of a unique optimal action. Finally, we discuss how our algorithm can be efficiently applied to a CPM problem of practical interest: namely, online ranking with feedback at the top.\n    ",
        "submission_date": "2016-08-23T00:00:00",
        "last_modified_date": "2016-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06651",
        "title": "Unsupervised, Efficient and Semantic Expertise Retrieval",
        "authors": [
            "Christophe Van Gysel",
            "Maarten de Rijke",
            "Marcel Worring"
        ],
        "abstract": "We introduce an unsupervised discriminative model for the task of retrieving experts in online document collections. We exclusively employ textual evidence and avoid explicit feature engineering by learning distributed word representations in an unsupervised way. We compare our model to state-of-the-art unsupervised statistical vector space and probabilistic generative approaches. Our proposed log-linear model achieves the retrieval performance levels of state-of-the-art document-centric methods with the low inference cost of so-called profile-centric approaches. It yields a statistically significant improved ranking over vector space and generative models in most cases, matching the performance of supervised methods on various benchmarks. That is, by using solely text we can do as well as methods that work with external evidence and/or relevance feedback. A contrastive analysis of rankings produced by discriminative and generative approaches shows that they have complementary strengths due to the ability of the unsupervised discriminative model to perform semantic matching.\n    ",
        "submission_date": "2016-08-23T00:00:00",
        "last_modified_date": "2017-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07068",
        "title": "Title Generation for User Generated Videos",
        "authors": [
            "Kuo-Hao Zeng",
            "Tseng-Hung Chen",
            "Juan Carlos Niebles",
            "Min Sun"
        ],
        "abstract": "A great video title describes the most salient event compactly and captures the viewer's attention. In contrast, video captioning tends to generate sentences that describe the video as a whole. Although generating a video title automatically is a very useful task, it is much less addressed than video captioning. We address video title generation for the first time by proposing two methods that extend state-of-the-art video captioners to this new task. First, we make video captioners highlight sensitive by priming them with a highlight detector. Our framework allows for jointly training a model for title generation and video highlight localization. Second, we induce high sentence diversity in video captioners, so that the generated titles are also diverse and catchy. This means that a large number of sentences might be required to learn the sentence structure of titles. Hence, we propose a novel sentence augmentation method to train a captioner with additional sentence-only examples that come without corresponding videos. We collected a large-scale Video Titles in the Wild (VTW) dataset of 18100 automatically crawled user-generated videos and titles. On VTW, our methods consistently improve title prediction accuracy, and achieve the best performance in both automatic and human evaluation. Finally, our sentence augmentation method also outperforms the baselines on the M-VAD dataset.\n    ",
        "submission_date": "2016-08-25T00:00:00",
        "last_modified_date": "2016-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07253",
        "title": "Learning Latent Vector Spaces for Product Search",
        "authors": [
            "Christophe Van Gysel",
            "Maarten de Rijke",
            "Evangelos Kanoulas"
        ],
        "abstract": "We introduce a novel latent vector space model that jointly learns the latent representations of words, e-commerce products and a mapping between the two without the need for explicit annotations. The power of the model lies in its ability to directly model the discriminative relation between products and a particular word. We compare our method to existing latent vector space models (LSI, LDA and word2vec) and evaluate it as a feature in a learning to rank setting. Our latent vector space model achieves its enhanced performance as it learns better product representations. Furthermore, the mapping from words to products and the representations of words benefit directly from the errors propagated back from the product representations during parameter estimation. We provide an in-depth analysis of the performance of our model and analyze the structure of the learned representations.\n    ",
        "submission_date": "2016-08-25T00:00:00",
        "last_modified_date": "2016-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07398",
        "title": "Proceedings First Workshop on Causal Reasoning for Embedded and safety-critical Systems Technologies",
        "authors": [
            "Gregor G\u00f6ssler",
            "Oleg Sokolsky"
        ],
        "abstract": "Formal approaches for automated causality analysis, fault localization, explanation of events, accountability and blaming have been proposed independently by several communities --- in particular, AI, concurrency, model-based diagnosis, formal methods. Work on these topics has significantly gained speed during the last years. The goals of CREST are to bring together and foster exchange between researchers from the different communities, and to present and discuss recent advances and new ideas in the field.\n",
        "submission_date": "2016-08-26T00:00:00",
        "last_modified_date": "2016-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07440",
        "title": "Activity Networks with Delays An application to toxicity analysis",
        "authors": [
            "Franck Delaplace",
            "Cinzia Di Giusto",
            "Jean-Louis Giavitto",
            "Hanna Klaudel"
        ],
        "abstract": "ANDy , Activity Networks with Delays, is a discrete time framework aimed at the qualitative modelling of time-dependent activities. The modular and concise syntax makes ANDy suitable for an easy and natural modelling of time-dependent biological systems (i.e., regulatory pathways). Activities involve entities playing the role of activators, inhibitors or products of biochemical network operation. Activities may have given duration, i.e., the time required to obtain results. An entity may represent an object (e.g., an agent, a biochemical species or a family of thereof) with a local attribute, a state denoting its level (e.g., concentration, strength). Entities levels may change as a result of an activity or may decay gradually as time passes by. The semantics of ANDy is formally given via high-level Petri nets ensuring this way some modularity. As main results we show that ANDy systems have finite state representations even for potentially infinite processes and it well adapts to the modelling of toxic behaviours. As an illustration, we present a classification of toxicity properties and give some hints on how they can be verified with existing tools on ANDy systems. A small case study on blood glucose regulation is provided to exemplify the ANDy framework and the toxicity properties.\n    ",
        "submission_date": "2016-08-26T00:00:00",
        "last_modified_date": "2016-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07441",
        "title": "Hard Negative Mining for Metric Learning Based Zero-Shot Classification",
        "authors": [
            "Maxime Bucher",
            "St\u00e9phane Herbin",
            "Fr\u00e9d\u00e9ric Jurie"
        ],
        "abstract": "Zero-Shot learning has been shown to be an efficient strategy for domain adaptation. In this context, this paper builds on the recent work of Bucher et al. [1], which proposed an approach to solve Zero-Shot classification problems (ZSC) by introducing a novel metric learning based objective function. This objective function allows to learn an optimal embedding of the attributes jointly with a measure of similarity between images and attributes. This paper extends their approach by proposing several schemes to control the generation of the negative pairs, resulting in a significant improvement of the performance and giving above state-of-the-art results on three challenging ZSC datasets.\n    ",
        "submission_date": "2016-08-26T00:00:00",
        "last_modified_date": "2016-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07639",
        "title": "Learning to generalize to new compositions in image understanding",
        "authors": [
            "Yuval Atzmon",
            "Jonathan Berant",
            "Vahid Kezami",
            "Amir Globerson",
            "Gal Chechik"
        ],
        "abstract": "Recurrent neural networks have recently been used for learning to describe images using natural language. However, it has been observed that these models generalize poorly to scenes that were not observed during training, possibly depending too strongly on the statistics of the text in the training data. Here we propose to describe images using short structured representations, aiming to capture the crux of a description. These structured representations allow us to tease-out and evaluate separately two types of generalization: standard generalization to new images with similar scenes, and generalization to new combinations of known entities. We compare two learning approaches on the MS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show, Attend and Tell), and a simple structured prediction model on top of a deep network. We find that the structured model generalizes to new compositions substantially better than the LSTM, ~7 times the accuracy of predicting structured representations. By providing a concrete method to quantify generalization for unseen combinations, we argue that structured representations and compositional splits are a useful benchmark for image captioning, and advocate compositional models that capture linguistic and visual structure.\n    ",
        "submission_date": "2016-08-27T00:00:00",
        "last_modified_date": "2016-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07685",
        "title": "KSR: A Semantic Representation of Knowledge Graph within a Novel Unsupervised Paradigm",
        "authors": [
            "Han Xiao",
            "Minlie Huang",
            "Xiaoyan Zhu"
        ],
        "abstract": "Knowledge representation is a long-history topic in AI, which is very important. A variety of models have been proposed for knowledge graph embedding, which projects symbolic entities and relations into continuous vector space. However, most related methods merely focus on the data-fitting of knowledge graph, and ignore the interpretable semantic expression. Thus, traditional embedding methods are not friendly for applications that require semantic analysis, such as question answering and entity retrieval. To this end, this paper proposes a semantic representation method for knowledge graph \\textbf{(KSR)}, which imposes a two-level hierarchical generative process that globally extracts many aspects and then locally assigns a specific category in each aspect for every triple. Since both aspects and categories are semantics-relevant, the collection of categories in each aspect is treated as the semantic representation of this triple. Extensive experiments show that our model outperforms other state-of-the-art baselines substantially.\n    ",
        "submission_date": "2016-08-27T00:00:00",
        "last_modified_date": "2020-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07879",
        "title": "Causality and Responsibility for Formal Verification and Beyond",
        "authors": [
            "Hana Chockler"
        ],
        "abstract": "The theory of actual causality, defined by Halpern and Pearl, and its quantitative measure - the degree of responsibility - was shown to be  extremely useful in various areas of computer science due to a good match between the results it produces and our intuition. In this paper, I describe the applications of causality to formal verification, namely, explanation of counterexamples, refinement of coverage metrics, and symbolic trajectory evaluation.  I also briefly discuss recent applications of causality to legal reasoning.\n    ",
        "submission_date": "2016-08-29T00:00:00",
        "last_modified_date": "2016-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07905",
        "title": "Machine Comprehension Using Match-LSTM and Answer Pointer",
        "authors": [
            "Shuohang Wang",
            "Jing Jiang"
        ],
        "abstract": "Machine comprehension of text is an important problem in natural language processing. A recently released dataset, the Stanford Question Answering Dataset (SQuAD), offers a large number of real questions and their answers created by humans through crowdsourcing. SQuAD provides a challenging testbed for evaluating machine comprehension algorithms, partly because compared with previous datasets, in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end neural architecture for the task. The architecture is based on match-LSTM, a model we proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed by Vinyals et al.(2015) to constrain the output tokens to be from the input sequences. We propose two ways of using Pointer Net for our task. Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al.(2016) using logistic regression and manually crafted features.\n    ",
        "submission_date": "2016-08-29T00:00:00",
        "last_modified_date": "2016-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08176",
        "title": "What is Wrong with Topic Modeling? (and How to Fix it Using Search-based Software Engineering)",
        "authors": [
            "Amritanshu Agrawal",
            "Wei Fu",
            "Tim Menzies"
        ],
        "abstract": "Context: Topic modeling finds human-readable structures in unstructured textual data. A widely used topic modeler is Latent Dirichlet allocation. When run on different datasets, LDA suffers from \"order effects\" i.e. different topics are generated if the order of training data is shuffled. Such order effects introduce a systematic error for any study. This error can relate to misleading results;specifically, inaccurate topic descriptions and a reduction in the efficacy of text mining classification results. Objective: To provide a method in which distributions generated by LDA are more stable and can be used for further analysis. Method: We use LDADE, a search-based software engineering tool that tunes LDA's parameters using DE (Differential Evolution). LDADE is evaluated on data from a programmer information exchange site (Stackoverflow), title and abstract text of thousands ofSoftware Engineering (SE) papers, and software defect reports from NASA. Results were collected across different implementations of LDA (Python+Scikit-Learn, Scala+Spark); across different platforms (Linux, Macintosh) and for different kinds of LDAs (VEM,or using Gibbs sampling). Results were scored via topic stability and text mining classification accuracy. Results: In all treatments: (i) standard LDA exhibits very large topic instability; (ii) LDADE's tunings dramatically reduce cluster instability; (iii) LDADE also leads to improved performances for supervised as well as unsupervised learning. Conclusion: Due to topic instability, using standard LDA with its \"off-the-shelf\" settings should now be depreciated. Also, in future, we should require SE papers that use LDA to test and (if needed) mitigate LDA topic instability. Finally, LDADE is a candidate technology for effectively and efficiently reducing that instability.\n    ",
        "submission_date": "2016-08-29T00:00:00",
        "last_modified_date": "2018-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08292",
        "title": "Robust Energy Storage Scheduling for Imbalance Reduction of Strategically Formed Energy Balancing Groups",
        "authors": [
            "Shantanu Chakraborty",
            "Toshiya Okabe"
        ],
        "abstract": "Imbalance (on-line energy gap between contracted supply and actual demand, and associated cost) reduction is going to be a crucial service for a Power Producer and Supplier (PPS) in the deregulated energy market. PPS requires forward market interactions to procure energy as precisely as possible in order to reduce imbalance energy. This paper presents, 1) (off-line) an effective demand aggregation based strategy for creating a number of balancing groups that leads to higher predictability of group-wise aggregated demand, 2) (on-line) a robust energy storage scheduling that minimizes the imbalance for a particular balancing group considering the demand prediction uncertainty. The group formation is performed by a Probabilistic Programming approach using Bayesian Markov Chain Monte Carlo (MCMC) method after applied on the historical demand statistics. Apart from the group formation, the aggregation strategy (with the help of Bayesian Inference) also clears out the upper-limit of the required storage capacity for a formed group, fraction of which is to be utilized in on-line operation. For on-line operation, a robust energy storage scheduling method is proposed that minimizes expected imbalance energy and cost (a non-linear function of imbalance energy) while incorporating the demand uncertainty of a particular group. The proposed methods are applied on the real apartment buildings' demand data in Tokyo, Japan. Simulation results are presented to verify the effectiveness of the proposed methods.\n    ",
        "submission_date": "2016-08-30T00:00:00",
        "last_modified_date": "2016-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08435",
        "title": "Multi-Label Classification Method Based on Extreme Learning Machines",
        "authors": [
            "Rajasekar Venkatesan",
            "Meng Joo Er"
        ],
        "abstract": "In this paper, an Extreme Learning Machine (ELM) based technique for Multi-label classification problems is proposed and discussed. In multi-label classification, each of the input data samples belongs to one or more than one class labels. The traditional binary and multi-class classification problems are the subset of the multi-label problem with the number of labels corresponding to each sample limited to one. The proposed ELM based multi-label classification technique is evaluated with six different benchmark multi-label datasets from different domains such as multimedia, text and biology. A detailed comparison of the results is made by comparing the proposed method with the results from nine state of the arts techniques for five different evaluation metrics. The nine methods are chosen from different categories of multi-label methods. The comparative results shows that the proposed Extreme Learning Machine based multi-label classification technique is a better alternative than the existing state of the art methods for multi-label problems.\n    ",
        "submission_date": "2016-08-30T00:00:00",
        "last_modified_date": "2016-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08515",
        "title": "Language Detection For Short Text Messages In Social Media",
        "authors": [
            "Ivana Balazevic",
            "Mikio Braun",
            "Klaus-Robert M\u00fcller"
        ],
        "abstract": "With the constant growth of the World Wide Web and the number of documents in different languages accordingly, the need for reliable language detection tools has increased as well. Platforms such as Twitter with predominantly short texts are becoming important information resources, which additionally imposes the need for short texts language detection algorithms. In this paper, we show how incorporating personalized user-specific information into the language detection algorithm leads to an important improvement of detection results. To choose the best algorithm for language detection for short text messages, we investigate several machine learning approaches. These approaches include the use of the well-known classifiers such as SVM and logistic regression, a dictionary based approach, and a probabilistic model based on modified Kneser-Ney smoothing. Furthermore, the extension of the probabilistic model to include additional user-specific information such as evidence accumulation per user and user interface language is explored, with the goal of improving the classification performance. The proposed approaches are evaluated on randomly collected Twitter data containing Latin as well as non-Latin alphabet languages and the quality of the obtained results is compared, followed by the selection of the best performing algorithm. This algorithm is then evaluated against two already existing general language detection tools: Chromium Compact Language Detector 2 (CLD2) and langid, where our method significantly outperforms the results achieved by both of the mentioned methods. Additionally, a preview of benefits and possible applications of having a reliable language detection algorithm is given.\n    ",
        "submission_date": "2016-08-30T00:00:00",
        "last_modified_date": "2016-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08517",
        "title": "Empirically Grounded Agent-Based Models of Innovation Diffusion: A Critical Review",
        "authors": [
            "Haifeng Zhang",
            "Yevgeniy Vorobeychik"
        ],
        "abstract": "Innovation diffusion has been studied extensively in a variety of disciplines, including sociology, economics, marketing, ecology, and computer science. Traditional literature on innovation diffusion has been dominated by models of aggregate behavior and trends. However, the agent-based modeling (ABM) paradigm is gaining popularity as it captures agent heterogeneity and enables fine-grained modeling of interactions mediated by social and geographic networks. While most ABM work on innovation diffusion is theoretical, empirically grounded models are increasingly important, particularly in guiding policy decisions. We present a critical review of empirically grounded agent-based models of innovation diffusion, developing a categorization of this research based on types of agent models as well as applications. By connecting the modeling methodologies in the fields of information and innovation diffusion, we suggest that the maximum likelihood estimation framework widely used in the former is a promising paradigm for calibration of agent-based models for innovation diffusion. Although many advances have been made to standardize ABM methodology, we identify four major issues in model calibration and validation, and suggest potential solutions.\n    ",
        "submission_date": "2016-08-30T00:00:00",
        "last_modified_date": "2017-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08614",
        "title": "What makes ImageNet good for transfer learning?",
        "authors": [
            "Minyoung Huh",
            "Pulkit Agrawal",
            "Alexei A. Efros"
        ],
        "abstract": "The tremendous success of ImageNet-trained deep features on a wide range of transfer tasks begs the question: what are the properties of the ImageNet dataset that are critical for learning good, general-purpose features? This work provides an empirical investigation of various facets of this question: Is more pre-training data always better? How does feature quality depend on the number of training examples per class? Does adding more object classes improve performance? For the same data budget, how should the data be split into classes? Is fine-grained recognition necessary for learning good features? Given the same number of training classes, is it better to have coarse classes or fine-grained classes? Which is better: more classes or more examples per class? To answer these and related questions, we pre-trained CNN features on various subsets of the ImageNet dataset and evaluated transfer performance on PASCAL detection, PASCAL action classification, and SUN scene classification tasks. Our overall findings suggest that most changes in the choice of pre-training data long thought to be critical do not significantly affect transfer performance.? Given the same number of training classes, is it better to have coarse classes or fine-grained classes? Which is better: more classes or more examples per class?\n    ",
        "submission_date": "2016-08-30T00:00:00",
        "last_modified_date": "2016-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08898",
        "title": "A High Speed Multi-label Classifier based on Extreme Learning Machines",
        "authors": [
            "Meng Joo Er",
            "Rajasekar Venkatesan",
            "Ning Wang"
        ],
        "abstract": "In this paper a high speed neural network classifier based on extreme learning machines for multi-label classification problem is proposed and dis-cussed. Multi-label classification is a superset of traditional binary and multi-class classification problems. The proposed work extends the extreme learning machine technique to adapt to the multi-label problems. As opposed to the single-label problem, both the number of labels the sample belongs to, and each of those target labels are to be identified for multi-label classification resulting in in-creased complexity. The proposed high speed multi-label classifier is applied to six benchmark datasets comprising of different application areas such as multi-media, text and biology. The training time and testing time of the classifier are compared with those of the state-of-the-arts methods. Experimental studies show that for all the six datasets, our proposed technique have faster execution speed and better performance, thereby outperforming all the existing multi-label clas-sification methods.\n    ",
        "submission_date": "2016-08-31T00:00:00",
        "last_modified_date": "2016-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08905",
        "title": "A Novel Online Real-time Classifier for Multi-label Data Streams",
        "authors": [
            "Rajasekar Venkatesan",
            "Meng Joo Er",
            "Shiqian Wu",
            "Mahardhika Pratama"
        ],
        "abstract": "In this paper, a novel extreme learning machine based online multi-label classifier for real-time data streams is proposed. Multi-label classification is one of the actively researched machine learning paradigm that has gained much attention in the recent years due to its rapidly increasing real world applications. In contrast to traditional binary and multi-class classification, multi-label classification involves association of each of the input samples with a set of target labels simultaneously. There are no real-time online neural network based multi-label classifier available in the literature. In this paper, we exploit the inherent nature of high speed exhibited by the extreme learning machines to develop a novel online real-time classifier for multi-label data streams. The developed classifier is experimented with datasets from different application domains for consistency, performance and speed. The experimental studies show that the proposed method outperforms the existing state-of-the-art techniques in terms of speed and accuracy and can classify multi-label data streams in real-time.\n    ",
        "submission_date": "2016-08-31T00:00:00",
        "last_modified_date": "2016-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08927",
        "title": "The Generalized Smallest Grammar Problem",
        "authors": [
            "Payam Siyari",
            "Matthias Gall\u00e9"
        ],
        "abstract": "The Smallest Grammar Problem -- the problem of finding the smallest context-free grammar that generates exactly one given sequence -- has never been successfully applied to grammatical inference. We investigate the reasons and propose an extended formulation that seeks to minimize non-recursive grammars, instead of straight-line programs. In addition, we provide very efficient algorithms that approximate the minimization problem of this class of grammars. Our empirical evaluation shows that we are able to find smaller models than the current best approximations to the Smallest Grammar Problem on standard benchmarks, and that the inferred rules capture much better the syntactic structure of natural language.\n    ",
        "submission_date": "2016-08-31T00:00:00",
        "last_modified_date": "2016-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08956",
        "title": "Knowledge Representation Analysis of Graph Mining",
        "authors": [
            "Matthias van der Hallen",
            "Sergey Paramonov",
            "Michael Leuschel",
            "Gerda Janssens"
        ],
        "abstract": "Many problems, especially those with a composite structure, can naturally be expressed in higher order logic. From a KR perspective modeling these problems in an intuitive way is a challenging task. In this paper we study the graph mining problem as an example of a higher order problem. In short, this problem asks us to find a graph that frequently occurs as a subgraph among a set of example graphs. We start from the problem's mathematical definition to solve it in three state-of-the-art specification systems. For IDP and ASP, which have no native support for higher order logic, we propose the use of encoding techniques such as the disjoint union technique and the saturation technique. ProB benefits from the higher order support for sets. We compare the performance of the three approaches to get an idea of the overhead of the higher order support.\n",
        "submission_date": "2016-08-31T00:00:00",
        "last_modified_date": "2016-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08974",
        "title": "Towards Transparent AI Systems: Interpreting Visual Question Answering Models",
        "authors": [
            "Yash Goyal",
            "Akrit Mohapatra",
            "Devi Parikh",
            "Dhruv Batra"
        ],
        "abstract": "Deep neural networks have shown striking progress and obtained state-of-the-art results in many AI research fields in the recent years. However, it is often unsatisfying to not know why they predict what they do. In this paper, we address the problem of interpreting Visual Question Answering (VQA) models. Specifically, we are interested in finding what part of the input (pixels in images or words in questions) the VQA model focuses on while answering the question. To tackle this problem, we use two visualization techniques -- guided backpropagation and occlusion -- to find important words in the question and important regions in the image. We then present qualitative and quantitative analyses of these importance maps. We found that even without explicit attention mechanisms, VQA models may sometimes be implicitly attending to relevant regions in the image, and often to appropriate words in the question.\n    ",
        "submission_date": "2016-08-31T00:00:00",
        "last_modified_date": "2016-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00036",
        "title": "Human Pose Estimation in Space and Time using 3D CNN",
        "authors": [
            "Agne Grinciunaite",
            "Amogh Gudi",
            "Emrah Tasli",
            "Marten den Uyl"
        ],
        "abstract": "This paper explores the capabilities of convolutional neural networks to deal with a task that is easily manageable for humans: perceiving 3D pose of a human body from varying angles. However, in our approach, we are restricted to using a monocular vision system. For this purpose, we apply a convolutional neural network approach on RGB videos and extend it to three dimensional convolutions. This is done via encoding the time dimension in videos as the 3\\ts{rd} dimension in convolutional space, and directly regressing to human body joint positions in 3D coordinate space. This research shows the ability of such a network to achieve state-of-the-art performance on the selected Human3.6M dataset, thus demonstrating the possibility of successfully representing temporal data with an additional dimension in the convolutional operation.\n    ",
        "submission_date": "2016-08-31T00:00:00",
        "last_modified_date": "2016-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00085",
        "title": "A Novel Progressive Learning Technique for Multi-class Classification",
        "authors": [
            "Rajasekar Venkatesan",
            "Meng Joo Er"
        ],
        "abstract": "In this paper, a progressive learning technique for multi-class classification is proposed. This newly developed learning technique is independent of the number of class constraints and it can learn new classes while still retaining the knowledge of previous classes. Whenever a new class (non-native to the knowledge learnt thus far) is encountered, the neural network structure gets remodeled automatically by facilitating new neurons and interconnections, and the parameters are calculated in such a way that it retains the knowledge learnt thus far. This technique is suitable for real-world applications where the number of classes is often unknown and online learning from real-time data is required. The consistency and the complexity of the progressive learning technique are analyzed. Several standard datasets are used to evaluate the performance of the developed technique. A comparative study shows that the developed technique is superior.\n    ",
        "submission_date": "2016-09-01T00:00:00",
        "last_modified_date": "2017-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00086",
        "title": "A novel online multi-label classifier for high-speed streaming data applications",
        "authors": [
            "Rajasekar Venkatesan",
            "Meng Joo Er",
            "Mihika Dave",
            "Mahardhika Pratama",
            "Shiqian Wu"
        ],
        "abstract": "In this paper, a high-speed online neural network classifier based on extreme learning machines for multi-label classification is proposed. In multi-label classification, each of the input data sample belongs to one or more than one of the target labels. The traditional binary and multi-class classification where each sample belongs to only one target class forms the subset of multi-label classification. Multi-label classification problems are far more complex than binary and multi-class classification problems, as both the number of target labels and each of the target labels corresponding to each of the input samples are to be identified. The proposed work exploits the high-speed nature of the extreme learning machines to achieve real-time multi-label classification of streaming data. A new threshold-based online sequential learning algorithm is proposed for high speed and streaming data classification of multi-label problems. The proposed method is experimented with six different datasets from different application domains such as multimedia, text, and biology. The hamming loss, accuracy, training time and testing time of the proposed technique is compared with nine different state-of-the-art methods. Experimental studies shows that the proposed technique outperforms the existing multi-label classifiers in terms of performance and speed.\n    ",
        "submission_date": "2016-09-01T00:00:00",
        "last_modified_date": "2016-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00149",
        "title": "From Community Detection to Community Deception",
        "authors": [
            "Valeria Fionda",
            "Giuseppe Pirr\u00f2"
        ],
        "abstract": "The community deception problem is about how to hide a target community C from community detection algorithms. The need for deception emerges whenever a group of entities (e.g., activists, police enforcements) want to cooperate while concealing their existence as a community. In this paper we introduce and formalize the community deception problem. To solve this problem, we describe algorithms that carefully rewire the connections of C's members. We experimentally show how several existing community detection algorithms can be deceived, and quantify the level of deception by introducing a deception score. We believe that our study is intriguing since, while showing how deception can be realized it raises awareness for the design of novel detection algorithms robust to deception techniques.\n    ",
        "submission_date": "2016-09-01T00:00:00",
        "last_modified_date": "2016-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00222",
        "title": "Ternary Neural Networks for Resource-Efficient AI Applications",
        "authors": [
            "Hande Alemdar",
            "Vincent Leroy",
            "Adrien Prost-Boucle",
            "Fr\u00e9d\u00e9ric P\u00e9trot"
        ],
        "abstract": "The computation and storage requirements for Deep Neural Networks (DNNs) are usually high. This issue limits their deployability on ubiquitous computing devices such as smart phones, wearables and autonomous drones. In this paper, we propose ternary neural networks (TNNs) in order to make deep learning more resource-efficient. We train these TNNs using a teacher-student approach based on a novel, layer-wise greedy methodology. Thanks to our two-stage training procedure, the teacher network is still able to use state-of-the-art methods such as dropout and batch normalization to increase accuracy and reduce training time. Using only ternary weights and activations, the student ternary network learns to mimic the behavior of its teacher network without using any multiplication. Unlike its -1,1 binary counterparts, a ternary neural network inherently prunes the smaller weights by setting them to zero during training. This makes them sparser and thus more energy-efficient. We design a purpose-built hardware architecture for TNNs and implement it on FPGA and ASIC. We evaluate TNNs on several benchmark datasets and demonstrate up to 3.1x better energy efficiency with respect to the state of the art while also improving accuracy.\n    ",
        "submission_date": "2016-09-01T00:00:00",
        "last_modified_date": "2017-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00464",
        "title": "The Semantic Knowledge Graph: A compact, auto-generated model for real-time traversal and ranking of any relationship within a domain",
        "authors": [
            "Trey Grainger",
            "Khalifeh AlJadda",
            "Mohammed Korayem",
            "Andries Smith"
        ],
        "abstract": "This paper describes a new kind of knowledge representation and mining system which we are calling the Semantic Knowledge Graph. At its heart, the Semantic Knowledge Graph leverages an inverted index, along with a complementary uninverted index, to represent nodes (terms) and edges (the documents within intersecting postings lists for multiple terms/nodes). This provides a layer of indirection between each pair of nodes and their corresponding edge, enabling edges to materialize dynamically from underlying corpus statistics. As a result, any combination of nodes can have edges to any other nodes materialize and be scored to reveal latent relationships between the nodes. This provides numerous benefits: the knowledge graph can be built automatically from a real-world corpus of data, new nodes - along with their combined edges - can be instantly materialized from any arbitrary combination of preexisting nodes (using set operations), and a full model of the semantic relationships between all entities within a domain can be represented and dynamically traversed using a highly compact representation of the graph. Such a system has widespread applications in areas as diverse as knowledge modeling and reasoning, natural language processing, anomaly detection, data cleansing, semantic search, analytics, data classification, root cause analysis, and recommendations systems. The main contribution of this paper is the introduction of a novel system - the Semantic Knowledge Graph - which is able to dynamically discover and score interesting relationships between any arbitrary combination of entities (words, phrases, or extracted concepts) through dynamically materializing nodes and edges from a compact graphical representation built automatically from a corpus of data representative of a knowledge domain.\n    ",
        "submission_date": "2016-09-02T00:00:00",
        "last_modified_date": "2016-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00843",
        "title": "An Online Universal Classifier for Binary, Multi-class and Multi-label Classification",
        "authors": [
            "Meng Joo Er",
            "Rajasekar Venkatesan",
            "Ning Wang"
        ],
        "abstract": "Classification involves the learning of the mapping function that associates input samples to corresponding target label. There are two major categories of classification problems: Single-label classification and Multi-label classification. Traditional binary and multi-class classifications are sub-categories of single-label classification. Several classifiers are developed for binary, multi-class and multi-label classification problems, but there are no classifiers available in the literature capable of performing all three types of classification. In this paper, a novel online universal classifier capable of performing all the three types of classification is proposed. Being a high speed online classifier, the proposed technique can be applied to streaming data applications. The performance of the developed classifier is evaluated using datasets from binary, multi-class and multi-label problems. The results obtained are compared with state-of-the-art techniques from each of the classification types.\n    ",
        "submission_date": "2016-09-03T00:00:00",
        "last_modified_date": "2016-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00932",
        "title": "Spectral learning of dynamic systems from nonequilibrium data",
        "authors": [
            "Hao Wu",
            "Frank No\u00e9"
        ],
        "abstract": "Observable operator models (OOMs) and related models are one of the most important and powerful tools for modeling and analyzing stochastic systems. They exactly describe dynamics of finite-rank systems and can be efficiently and consistently estimated through spectral learning under the assumption of identically distributed data. In this paper, we investigate the properties of spectral learning without this assumption due to the requirements of analyzing large-time scale systems, and show that the equilibrium dynamics of a system can be extracted from nonequilibrium observation data by imposing an equilibrium constraint. In addition, we propose a binless extension of spectral learning for continuous data. In comparison with the other continuous-valued spectral algorithms, the binless algorithm can achieve consistent estimation of equilibrium dynamics with only linear complexity.\n    ",
        "submission_date": "2016-09-04T00:00:00",
        "last_modified_date": "2017-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01468",
        "title": "Q-Learning with Basic Emotions",
        "authors": [
            "Wilfredo Badoy Jr.",
            "Kardi Teknomo"
        ],
        "abstract": "Q-learning is a simple and powerful tool in solving dynamic problems where environments are unknown. It uses a balance of exploration and exploitation to find an optimal solution to the problem. In this paper, we propose using four basic emotions: joy, sadness, fear, and anger to influence a Qlearning agent. Simulations show that the proposed affective agent requires lesser number of steps to find the optimal path. We found when affective agent finds the optimal path, the ratio between exploration to exploitation gradually decreases, indicating lower total step count in the long run\n    ",
        "submission_date": "2016-09-06T00:00:00",
        "last_modified_date": "2016-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01472",
        "title": "OpenTripPlanner, OpenStreetMap, General Transit Feed Specification: Tools for Disaster Relief and Recovery",
        "authors": [
            "Chelcie Narboneta",
            "Kardi Teknomo"
        ],
        "abstract": "Open Trip Planner was identified as the most promising open source multi-modal trip planning software. Open Street Map, which provides mapping data to Open Trip Planner, is one of the most well-known open source international repository of geographic data. General Transit Feed Specification, which provides transportation data to Open Trip Planner, has been the standard for describing transit systems and platform for numerous applications. Together, when used to implement an instance of Open Trip Planner, these software has been helping in traffic decongestion all over the world by assisting commuters to shift from using private transportation modes to public ones. Their potential however goes beyond providing multi-modal public transportation routes. This paper aims to first discuss the researchers' experience in implementing a public transportation route planner for the purpose of traffic ",
        "submission_date": "2016-09-06T00:00:00",
        "last_modified_date": "2016-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01475",
        "title": "Multi Exit Configuration of Mesoscopic Pedestrian Simulation",
        "authors": [
            "Allan Lao",
            "Kardi Teknomo"
        ],
        "abstract": "A mesoscopic approach to modeling pedestrian simulation with multiple exits is proposed in this paper. A floor field based on Qlearning Algorithm is used. Attractiveness of exits to pedestrian typically is based on shortest path. However, several factors may influence pedestrian choice of exits. Scenarios with multiple exits are presented and effect of Q-learning rewards system on navigation is investigated\n    ",
        "submission_date": "2016-09-06T00:00:00",
        "last_modified_date": "2016-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01493",
        "title": "Axiomatizing Category Theory in Free Logic",
        "authors": [
            "Christoph Benzm\u00fcller",
            "Dana S. Scott"
        ],
        "abstract": "Starting from a generalization of the standard axioms for a monoid we present a stepwise development of various, mutually equivalent foundational axiom systems for category theory. Our axiom sets have been formalized in the Isabelle/HOL interactive proof assistant, and this formalization utilizes a semantically correct embedding of free logic in classical higher-order logic. The modeling and formal analysis of our axiom sets has been significantly supported by series of experiments with automated reasoning tools integrated with Isabelle/HOL. We also address the relation of our axiom systems to alternative proposals from the literature, including an axiom set proposed by Freyd and Scedrov for which we reveal a technical issue (when encoded in free logic where free variables range over defined and undefined objects): either all operations, e.g. morphism composition, are total or their axiom system is inconsistent. The repair for this problem is quite straightforward, however.\n    ",
        "submission_date": "2016-09-06T00:00:00",
        "last_modified_date": "2018-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01710",
        "title": "Automation of Pedestrian Tracking in a Crowded Situation",
        "authors": [
            "Saman Saadat",
            "Kardi Teknomo"
        ],
        "abstract": "Studies on microscopic pedestrian requires large amounts of trajectory data from real-world pedestrian crowds. Such data collection, if done manually, needs tremendous effort and is very time consuming. Though many studies have asserted the possibility of automating this task using video cameras, we found that only a few have demonstrated good performance in very crowded situations or from a top-angled view scene. This paper deals with tracking pedestrian crowd under heavy occlusions from an angular scene. Our automated tracking system consists of two modules that perform sequentially. The first module detects moving objects as blobs. The second module is a tracking system. We employ probability distribution from the detection of each pedestrian and use Bayesian update to track the next position. The result of such tracking is a database of pedestrian trajectories over time and space. With certain prior information, we showed that the system can track a large number of people under occlusion and clutter scene.\n    ",
        "submission_date": "2016-09-06T00:00:00",
        "last_modified_date": "2016-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01926",
        "title": "A modular architecture for transparent computation in Recurrent Neural Networks",
        "authors": [
            "Giovanni Sirio Carmantini",
            "Peter beim Graben",
            "Mathieu Desroches",
            "Serafim Rodrigues"
        ],
        "abstract": "Computation is classically studied in terms of automata, formal languages and algorithms; yet, the relation between neural dynamics and symbolic representations and operations is still unclear in traditional eliminative connectionism. Therefore, we suggest a unique perspective on this central issue, to which we would like to refer as to transparent connectionism, by proposing accounts of how symbolic computation can be implemented in neural substrates. In this study we first introduce a new model of dynamics on a symbolic space, the versatile shift, showing that it supports the real-time simulation of a range of automata. We then show that the Goedelization of versatile shifts defines nonlinear dynamical automata, dynamical systems evolving on a vectorial space. Finally, we present a mapping between nonlinear dynamical automata and recurrent artificial neural networks. The mapping defines an architecture characterized by its granular modularity, where data, symbolic operations and their control are not only distinguishable in activation space, but also spatially localizable in the network itself, while maintaining a distributed encoding of symbolic representations. The resulting networks simulate automata in real-time and are programmed directly, in absence of network training. To discuss the unique characteristics of the architecture and their consequences, we present two examples: i) the design of a Central Pattern Generator from a finite-state locomotive controller, and ii) the creation of a network simulating a system of interactive automata that supports the parsing of garden-path sentences as investigated in psycholinguistics experiments.\n    ",
        "submission_date": "2016-09-07T00:00:00",
        "last_modified_date": "2016-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02036",
        "title": "Deep Markov Random Field for Image Modeling",
        "authors": [
            "Zhirong Wu",
            "Dahua Lin",
            "Xiaoou Tang"
        ],
        "abstract": "Markov Random Fields (MRFs), a formulation widely used in generative image modeling, have long been plagued by the lack of expressive power. This issue is primarily due to the fact that conventional MRFs formulations tend to use simplistic factors to capture local patterns. In this paper, we move beyond such limitations, and propose a novel MRF model that uses fully-connected neurons to express the complex interactions among pixels. Through theoretical analysis, we reveal an inherent connection between this model and recurrent neural networks, and thereon derive an approximated feed-forward network that couples multiple RNNs along opposite directions. This formulation combines the expressive power of deep neural networks and the cyclic dependency structure of MRF in a unified model, bringing the modeling capability to a new level. The feed-forward approximation also allows it to be efficiently learned from data. Experimental results on a variety of low-level vision tasks show notable improvement over state-of-the-arts.\n    ",
        "submission_date": "2016-09-07T00:00:00",
        "last_modified_date": "2016-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02132",
        "title": "UberNet: Training a `Universal' Convolutional Neural Network for Low-, Mid-, and High-Level Vision using Diverse Datasets and Limited Memory",
        "authors": [
            "Iasonas Kokkinos"
        ],
        "abstract": "In this work we introduce a convolutional neural network (CNN) that jointly handles low-, mid-, and high-level vision tasks in a unified architecture that is trained end-to-end. Such a universal network can act like a `swiss knife' for vision tasks; we call this architecture an UberNet to indicate its overarching nature.\n",
        "submission_date": "2016-09-07T00:00:00",
        "last_modified_date": "2016-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02228",
        "title": "Learning to learn with backpropagation of Hebbian plasticity",
        "authors": [
            "Thomas Miconi"
        ],
        "abstract": "Hebbian plasticity is a powerful principle that allows biological brains to learn from their lifetime experience. By contrast, artificial neural networks trained with backpropagation generally have fixed connection weights that do not change once training is complete. While recent methods can endow neural networks with long-term memories, Hebbian plasticity is currently not amenable to gradient descent. Here we derive analytical expressions for activity gradients in neural networks with Hebbian plastic connections. Using these expressions, we can use backpropagation to train not just the baseline weights of the connections, but also their plasticity. As a result, the networks \"learn how to learn\" in order to solve the problem at hand: the trained networks automatically perform fast learning of unpredictable environmental features during their lifetime, expanding the range of solvable problems. We test the algorithm on various on-line learning tasks, including pattern completion, one-shot learning, and reversal learning. The algorithm successfully learns how to learn the relevant associations from one-shot instruction, and fine-tunes the temporal dynamics of plasticity to allow for continual learning in response to changing environmental parameters. We conclude that backpropagation of Hebbian plasticity offers a powerful model for lifelong learning.\n    ",
        "submission_date": "2016-09-08T00:00:00",
        "last_modified_date": "2016-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02531",
        "title": "Latest Datasets and Technologies Presented in the Workshop on Grasping and Manipulation Datasets",
        "authors": [
            "Matteo Bianchi",
            "Jeannette Bohg",
            "Yu Sun"
        ],
        "abstract": "This paper reports the activities and outcomes in the Workshop on Grasping and Manipulation Datasets that was organized under the International Conference on Robotics and Automation (ICRA) 2016. The half day workshop was packed with nine invited talks, 12 interactive presentations, and one panel discussion with ten panelists. This paper summarizes all the talks and presentations and recaps what has been discussed in the panels session. This summary servers as a review of recent developments in data collection in grasping and manipulation. Many of the presentations describe ongoing efforts or explorations that could be achieved and fully available in a year or two. The panel discussion not only commented on the current approaches, but also indicates new directions and focuses. The workshop clearly displayed the importance of quality datasets in robotics and robotic grasping and manipulation field. Hopefully the workshop could motivate larger efforts to create big datasets that are comparable with big datasets in other communities such as computer vision.\n    ",
        "submission_date": "2016-09-08T00:00:00",
        "last_modified_date": "2016-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03058",
        "title": "A Tube-and-Droplet-based Approach for Representing and Analyzing Motion Trajectories",
        "authors": [
            "Weiyao Lin",
            "Yang Zhou",
            "Hongteng Xu",
            "Junchi Yan",
            "Mingliang Xu",
            "Jianxin Wu",
            "Zicheng Liu"
        ],
        "abstract": "Trajectory analysis is essential in many applications. In this paper, we address the problem of representing motion trajectories in a highly informative way, and consequently utilize it for analyzing trajectories. Our approach first leverages the complete information from given trajectories to construct a thermal transfer field which provides a context-rich way to describe the global motion pattern in a scene. Then, a 3D tube is derived which depicts an input trajectory by integrating its surrounding motion patterns contained in the thermal transfer field. The 3D tube effectively: 1) maintains the movement information of a trajectory, 2) embeds the complete contextual motion pattern around a trajectory, 3) visualizes information about a trajectory in a clear and unified way. We further introduce a droplet-based process. It derives a droplet vector from a 3D tube, so as to characterize the high-dimensional 3D tube information in a simple but effective way. Finally, we apply our tube-and-droplet representation to trajectory analysis applications including trajectory clustering, trajectory classification & abnormality detection, and 3D action recognition. Experimental comparisons with state-of-the-art algorithms demonstrate the effectiveness of our approach.\n    ",
        "submission_date": "2016-09-10T00:00:00",
        "last_modified_date": "2017-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03157",
        "title": "A centralized reinforcement learning method for multi-agent job scheduling in Grid",
        "authors": [
            "Milad Moradi"
        ],
        "abstract": "One of the main challenges in Grid systems is designing an adaptive, scalable, and model-independent method for job scheduling to achieve a desirable degree of load balancing and system efficiency. Centralized job scheduling methods have some drawbacks, such as single point of failure and lack of scalability. Moreover, decentralized methods require a coordination mechanism with limited communications. In this paper, we propose a multi-agent approach to job scheduling in Grid, named Centralized Learning Distributed Scheduling (CLDS), by utilizing the reinforcement learning framework. The CLDS is a model free approach that uses the information of jobs and their completion time to estimate the efficiency of resources. In this method, there are a learner agent and several scheduler agents that perform the task of learning and job scheduling with the use of a coordination strategy that maintains the communication cost at a limited level. We evaluated the efficiency of the CLDS method by designing and performing a set of experiments on a simulated Grid system under different system scales and loads. The results show that the CLDS can effectively balance the load of system even in large scale and heavy loaded Grids, while maintains its adaptive performance and scalability.\n    ",
        "submission_date": "2016-09-11T00:00:00",
        "last_modified_date": "2016-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03193",
        "title": "Wav2Letter: an End-to-End ConvNet-based Speech Recognition System",
        "authors": [
            "Ronan Collobert",
            "Christian Puhrsch",
            "Gabriel Synnaeve"
        ],
        "abstract": "This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform.\n    ",
        "submission_date": "2016-09-11T00:00:00",
        "last_modified_date": "2016-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03234",
        "title": "Reduced Space and Faster Convergence in Imperfect-Information Games via Regret-Based Pruning",
        "authors": [
            "Noam Brown",
            "Tuomas Sandholm"
        ],
        "abstract": "Counterfactual Regret Minimization (CFR) is the most popular iterative algorithm for solving zero-sum imperfect-information games. Regret-Based Pruning (RBP) is an improvement that allows poorly-performing actions to be temporarily pruned, thus speeding up CFR. We introduce Total RBP, a new form of RBP that reduces the space requirements of CFR as actions are pruned. We prove that in zero-sum games it asymptotically prunes any action that is not part of a best response to some Nash equilibrium. This leads to provably faster convergence and lower space requirements. Experiments show that Total RBP results in an order of magnitude reduction in space, and the reduction factor increases with game size.\n    ",
        "submission_date": "2016-09-12T00:00:00",
        "last_modified_date": "2016-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03333",
        "title": "On Generation of Time-based Label Refinements",
        "authors": [
            "Niek Tax",
            "Emin Alasgarov",
            "Natalia Sidorova",
            "Reinder Haakma"
        ],
        "abstract": "Process mining is a research field focused on the analysis of event data with the aim of extracting insights in processes. Applying process mining techniques on data from smart home environments has the potential to provide valuable insights in (un)healthy habits and to contribute to ambient assisted living solutions. Finding the right event labels to enable application of process mining techniques is however far from trivial, as simply using the triggering sensor as the label for sensor events results in uninformative models that allow for too much behavior (overgeneralizing). Refinements of sensor level event labels suggested by domain experts have shown to enable discovery of more precise and insightful process models. However, there exist no automated approach to generate refinements of event labels in the context of process mining. In this paper we propose a framework for automated generation of label refinements based on the time attribute of events. We show on a case study with real life smart home event data that behaviorally more specific, and therefore more insightful, process models can be found by using automatically generated refined labels in process discovery.\n    ",
        "submission_date": "2016-09-12T00:00:00",
        "last_modified_date": "2016-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03357",
        "title": "Modelling Creativity: Identifying Key Components through a Corpus-Based Approach",
        "authors": [
            "Anna Jordanous",
            "Bill Keller"
        ],
        "abstract": "Creativity is a complex, multi-faceted concept encompassing a variety of related aspects, abilities, properties and behaviours. If we wish to study creativity scientifically, then a tractable and well-articulated model of creativity is required. Such a model would be of great value to researchers investigating the nature of creativity and in particular, those concerned with the evaluation of creative practice. This paper describes a unique approach to developing a suitable model of how creative behaviour emerges that is based on the words people use to describe the concept. Using techniques from the field of statistical natural language processing, we identify a collection of fourteen key components of creativity through an analysis of a corpus of academic papers on the topic. Words are identified which appear significantly often in connection with discussions of the concept. Using a measure of lexical similarity to help cluster these words, a number of distinct themes emerge, which collectively contribute to a comprehensive and multi-perspective model of creativity. The components provide an ontology of creativity: a set of building blocks which can be used to model creative practice in a variety of domains. The components have been employed in two case studies to evaluate the creativity of computational systems and have proven useful in articulating achievements of this work and directions for further research.\n    ",
        "submission_date": "2016-09-12T00:00:00",
        "last_modified_date": "2016-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03438",
        "title": "Reactive Multi-Context Systems: Heterogeneous Reasoning in Dynamic Environments",
        "authors": [
            "Gerhard Brewka",
            "Stefan Ellmauthaler",
            "Ricardo Gon\u00e7alves",
            "Matthias Knorr",
            "Jo\u00e3o Leite",
            "J\u00f6rg P\u00fchrer"
        ],
        "abstract": "Managed multi-context systems (mMCSs) allow for the integration of heterogeneous knowledge sources in a modular and very general way. They were, however, mainly designed for static scenarios and are therefore not well-suited for dynamic environments in which continuous reasoning over such heterogeneous knowledge with constantly arriving streams of data is necessary. In this paper, we introduce reactive multi-context systems (rMCSs), a framework for reactive reasoning in the presence of heterogeneous knowledge sources and data streams. We show that rMCSs are indeed well-suited for this purpose by illustrating how several typical problems arising in the context of stream reasoning can be handled using them, by showing how inconsistencies possibly occurring in the integration of multiple knowledge sources can be handled, and by arguing that the potential non-determinism of rMCSs can be avoided if needed using an alternative, more skeptical well-founded semantics instead with beneficial computational properties. We also investigate the computational complexity of various reasoning problems related to rMCSs. Finally, we discuss related work, and show that rMCSs do not only generalize mMCSs to dynamic settings, but also capture/extend relevant approaches w.r.t. dynamics in knowledge representation and stream reasoning.\n    ",
        "submission_date": "2016-09-12T00:00:00",
        "last_modified_date": "2017-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03540",
        "title": "ZaliQL: A SQL-Based Framework for Drawing Causal Inference from Big Data",
        "authors": [
            "Babak Salimi",
            "Dan Suciu"
        ],
        "abstract": "Causal inference from observational data is a subject of active research and development in statistics and computer science. Many toolkits have been developed for this purpose that depends on statistical software. However, these toolkits do not scale to large datasets. In this paper we describe a suite of techniques for expressing causal inference tasks from observational data in SQL. This suite supports the state-of-the-art methods for causal inference and run at scale within a database engine. In addition, we introduce several optimization techniques that significantly speedup causal inference, both in the online and offline setting. We evaluate the quality and performance of our techniques by experiments of real datasets.\n    ",
        "submission_date": "2016-09-12T00:00:00",
        "last_modified_date": "2016-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03632",
        "title": "Joint Extraction of Events and Entities within a Document Context",
        "authors": [
            "Bishan Yang",
            "Tom Mitchell"
        ],
        "abstract": "Events and entities are closely related; entities are often actors or participants in events and events without entities are uncommon. The interpretation of events and entities is highly contextually dependent. Existing work in information extraction typically models events separately from entities, and performs inference at the sentence level, ignoring the rest of the document. In this paper, we propose a novel approach that models the dependencies among variables of events, entities, and their relations, and performs joint inference of these variables across a document. The goal is to enable access to document-level contextual information and facilitate context-aware predictions. We demonstrate that our approach substantially outperforms the state-of-the-art methods for event extraction as well as a strong baseline for entity extraction.\n    ",
        "submission_date": "2016-09-12T00:00:00",
        "last_modified_date": "2016-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03971",
        "title": "Feynman Machine: The Universal Dynamical Systems Computer",
        "authors": [
            "Eric Laukien",
            "Richard Crowder",
            "Fergal Byrne"
        ],
        "abstract": "Efforts at understanding the computational processes in the brain have met with limited success, despite their importance and potential uses in building intelligent machines. We propose a simple new model which draws on recent findings in Neuroscience and the Applied Mathematics of interacting Dynamical Systems. The Feynman Machine is a Universal Computer for Dynamical Systems, analogous to the Turing Machine for symbolic computing, but with several important differences. We demonstrate that networks and hierarchies of simple interacting Dynamical Systems, each adaptively learning to forecast its evolution, are capable of automatically building sensorimotor models of the external and internal world. We identify such networks in mammalian neocortex, and show how existing theories of cortical computation combine with our model to explain the power and flexibility of mammalian intelligence. These findings lead directly to new architectures for machine intelligence. A suite of software implementations has been built based on these principles, and applied to a number of spatiotemporal learning tasks.\n    ",
        "submission_date": "2016-09-13T00:00:00",
        "last_modified_date": "2016-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04214",
        "title": "\"Flow Size Difference\" Can Make a Difference: Detecting Malicious TCP Network Flows Based on Benford's Law",
        "authors": [
            "Aamo Iorliam",
            "Santosh Tirunagari",
            "Anthony T.S. Ho",
            "Shujun Li",
            "Adrian Waller",
            "Norman Poh"
        ],
        "abstract": "Statistical characteristics of network traffic have attracted a significant amount of research for automated network intrusion detection, some of which looked at applications of natural statistical laws such as Zipf's law, Benford's law and the Pareto distribution. In this paper, we present the application of Benford's law to a new network flow metric \"flow size difference\", which have not been studied before by other researchers, to build an unsupervised flow-based intrusion detection system (IDS). The method was inspired by our observation on a large number of TCP flow datasets where normal flows tend to follow Benford's law closely but malicious flows tend to deviate significantly from it. The proposed IDS is unsupervised, so it can be easily deployed without any training. It has two simple operational parameters with a clear semantic meaning, allowing the IDS operator to set and adapt their values intuitively to adjust the overall performance of the IDS. We tested the proposed IDS on two (one closed and one public) datasets, and proved its efficiency in terms of AUC (area under the ROC curve). Our work showed the \"flow size difference\" has a great potential to improve the performance of any flow-based network IDSs.\n    ",
        "submission_date": "2016-09-14T00:00:00",
        "last_modified_date": "2017-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04337",
        "title": "Quick and energy-efficient Bayesian computing of binocular disparity using stochastic digital signals",
        "authors": [
            "Alexandre Coninx",
            "Pierre Bessi\u00e8re",
            "Jacques Droulez"
        ],
        "abstract": "Reconstruction of the tridimensional geometry of a visual scene using the binocular disparity information is an important issue in computer vision and mobile robotics, which can be formulated as a Bayesian inference problem. However, computation of the full disparity distribution with an advanced Bayesian model is usually an intractable problem, and proves computationally challenging even with a simple model. In this paper, we show how probabilistic hardware using distributed memory and alternate representation of data as stochastic bitstreams can solve that problem with high performance and energy efficiency. We put forward a way to express discrete probability distributions using stochastic data representations and perform Bayesian fusion using those representations, and show how that approach can be applied to diparity computation. We evaluate the system using a simulated stochastic implementation and discuss possible hardware implementations of such architectures and their potential for sensorimotor processing and robotics.\n    ",
        "submission_date": "2016-09-14T00:00:00",
        "last_modified_date": "2016-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04371",
        "title": "Finite LTL Synthesis is EXPTIME-complete",
        "authors": [
            "Jorge A. Baier",
            "Alberto Camacho",
            "Christian Muise",
            "Sheila A. McIlraith"
        ],
        "abstract": "LTL synthesis -- the construction of a function to satisfy a logical specification formulated in Linear Temporal Logic -- is a 2EXPTIME-complete problem with relevant applications in controller synthesis and a myriad of artificial intelligence applications. In this research note we consider De Giacomo and Vardi's variant of the synthesis problem for LTL formulas interpreted over finite rather than infinite traces. Rather surprisingly, given the existing claims on complexity, we establish that LTL synthesis is EXPTIME-complete for the finite interpretation, and not 2EXPTIME-complete as previously reported. Our result coincides nicely with the planning perspective where non-deterministic planning with full observability is EXPTIME-complete and partial observability increases the complexity to 2EXPTIME-complete; a recent related result for LTL synthesis shows that in the finite case with partial observability, the problem is 2EXPTIME-complete.\n    ",
        "submission_date": "2016-09-14T00:00:00",
        "last_modified_date": "2016-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04508",
        "title": "Column Networks for Collective Classification",
        "authors": [
            "Trang Pham",
            "Truyen Tran",
            "Dinh Phung",
            "Svetha Venkatesh"
        ],
        "abstract": "Relational learning deals with data that are characterized by relational structures. An important task is collective classification, which is to jointly classify networked objects. While it holds a great promise to produce a better accuracy than non-collective classifiers, collective classification is computational challenging and has not leveraged on the recent breakthroughs of deep learning. We present Column Network (CLN), a novel deep learning model for collective classification in multi-relational domains. CLN has many desirable theoretical properties: (i) it encodes multi-relations between any two instances; (ii) it is deep and compact, allowing complex functions to be approximated at the network level with a small set of free parameters; (iii) local and relational features are learned simultaneously; (iv) long-range, higher-order dependencies between instances are supported naturally; and (v) crucially, learning and inference are efficient, linear in the size of the network and the number of relations. We evaluate CLN on multiple real-world applications: (a) delay prediction in software projects, (b) PubMed Diabetes publication classification and (c) film genre classification. In all applications, CLN demonstrates a higher accuracy than state-of-the-art rivals.\n    ",
        "submission_date": "2016-09-15T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04628",
        "title": "Context Aware Nonnegative Matrix Factorization Clustering",
        "authors": [
            "Rocco Tripodi",
            "Sebastiano Vascon",
            "Marcello Pelillo"
        ],
        "abstract": "In this article we propose a method to refine the clustering results obtained with the nonnegative matrix factorization (NMF) technique, imposing consistency constraints on the final labeling of the data. The research community focused its effort on the initialization and on the optimization part of this method, without paying attention to the final cluster assignments. We propose a game theoretic framework in which each object to be clustered is represented as a player, which has to choose its cluster membership. The information obtained with NMF is used to initialize the strategy space of the players and a weighted graph is used to model the interactions among the players. These interactions allow the players to choose a cluster which is coherent with the clusters chosen by similar players, a property which is not guaranteed by NMF, since it produces a soft clustering of the data. The results on common benchmarks show that our model is able to improve the performances of many NMF formulations.\n    ",
        "submission_date": "2016-09-15T00:00:00",
        "last_modified_date": "2016-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04904",
        "title": "Long-Term Trends in the Public Perception of Artificial Intelligence",
        "authors": [
            "Ethan Fast",
            "Eric Horvitz"
        ],
        "abstract": "Analyses of text corpora over time can reveal trends in beliefs, interest, and sentiment about a topic. We focus on views expressed about artificial intelligence (AI) in the New York Times over a 30-year period. General interest, awareness, and discussion about AI has waxed and waned since the field was founded in 1956. We present a set of measures that captures levels of engagement, measures of pessimism and optimism, the prevalence of specific hopes and concerns, and topics that are linked to discussions about AI over decades. We find that discussion of AI has increased sharply since 2009, and that these discussions have been consistently more optimistic than pessimistic. However, when we examine specific concerns, we find that worries of loss of control of AI, ethical concerns for AI, and the negative impact of AI on work have grown in recent years. We also find that hopes for AI in healthcare and education have increased over time.\n    ",
        "submission_date": "2016-09-16T00:00:00",
        "last_modified_date": "2016-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04994",
        "title": "Exploration Potential",
        "authors": [
            "Jan Leike"
        ],
        "abstract": "We introduce exploration potential, a quantity that measures how much a reinforcement learning agent has explored its environment class. In contrast to information gain, exploration potential takes the problem's reward structure into account. This leads to an exploration criterion that is both necessary and sufficient for asymptotic optimality (learning to act optimally across the entire environment class). Our experiments in multi-armed bandits use exploration potential to illustrate how different algorithms make the tradeoff between exploration and exploitation.\n    ",
        "submission_date": "2016-09-16T00:00:00",
        "last_modified_date": "2016-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05180",
        "title": "Grammatical Templates: Improving Text Difficulty Evaluation for Language Learners",
        "authors": [
            "Shuhan Wang",
            "Erik Andersen"
        ],
        "abstract": "Language students are most engaged while reading texts at an appropriate difficulty level. However, existing methods of evaluating text difficulty focus mainly on vocabulary and do not prioritize grammatical features, hence they do not work well for language learners with limited knowledge of grammar. In this paper, we introduce grammatical templates, the expert-identified units of grammar that students learn from class, as an important feature of text difficulty evaluation. Experimental classification results show that grammatical template features significantly improve text difficulty prediction accuracy over baseline readability features by 7.4%. Moreover, we build a simple and human-understandable text difficulty evaluation approach with 87.7% accuracy, using only 5 grammatical template features.\n    ",
        "submission_date": "2016-09-16T00:00:00",
        "last_modified_date": "2017-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05258",
        "title": "The ACRV Picking Benchmark (APB): A Robotic Shelf Picking Benchmark to Foster Reproducible Research",
        "authors": [
            "J\u00fcrgen Leitner",
            "Adam W. Tow",
            "Jake E. Dean",
            "Niko Suenderhauf",
            "Joseph W. Durham",
            "Matthew Cooper",
            "Markus Eich",
            "Christopher Lehnert",
            "Ruben Mangels",
            "Christopher McCool",
            "Peter Kujala",
            "Lachlan Nicholson",
            "Trung Pham",
            "James Sergeant",
            "Liao Wu",
            "Fangyi Zhang",
            "Ben Upcroft",
            "Peter Corke"
        ],
        "abstract": "Robotic challenges like the Amazon Picking Challenge (APC) or the DARPA Challenges are an established and important way to drive scientific progress. They make research comparable on a well-defined benchmark with equal test conditions for all participants. However, such challenge events occur only occasionally, are limited to a small number of contestants, and the test conditions are very difficult to replicate after the main event. We present a new physical benchmark challenge for robotic picking: the ACRV Picking Benchmark (APB). Designed to be reproducible, it consists of a set of 42 common objects, a widely available shelf, and exact guidelines for object arrangement using stencils. A well-defined evaluation protocol enables the comparison of \\emph{complete} robotic systems -- including perception and manipulation -- instead of sub-systems only. Our paper also describes and reports results achieved by an open baseline system based on a Baxter robot.\n    ",
        "submission_date": "2016-09-17T00:00:00",
        "last_modified_date": "2016-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05473",
        "title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient",
        "authors": [
            "Lantao Yu",
            "Weinan Zhang",
            "Jun Wang",
            "Yong Yu"
        ],
        "abstract": "As a new way of training generative models, Generative Adversarial Nets (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.\n    ",
        "submission_date": "2016-09-18T00:00:00",
        "last_modified_date": "2017-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05600",
        "title": "Graph-Structured Representations for Visual Question Answering",
        "authors": [
            "Damien Teney",
            "Lingqiao Liu",
            "Anton van den Hengel"
        ],
        "abstract": "This paper proposes to improve visual question answering (VQA) with structured representations of both scene contents and questions. A key challenge in VQA is to require joint reasoning over the visual and text domains. The predominant CNN/LSTM-based approach to VQA is limited by monolithic vector representations that largely ignore structure in the scene and in the form of the question. CNN feature vectors cannot effectively capture situations as simple as multiple object instances, and LSTMs process questions as series of words, which does not reflect the true complexity of language structure. We instead propose to build graphs over the scene objects and over the question words, and we describe a deep neural network that exploits the structure in these representations. This shows significant benefit over the sequential processing of LSTMs. The overall efficacy of our approach is demonstrated by significant improvements over the state-of-the-art, from 71.2% to 74.4% in accuracy on the \"abstract scenes\" multiple-choice benchmark, and from 34.7% to 39.1% in accuracy over pairs of \"balanced\" scenes, i.e. images with fine-grained differences and opposite yes/no answers to a same question.\n    ",
        "submission_date": "2016-09-19T00:00:00",
        "last_modified_date": "2017-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05621",
        "title": "Extending Unification in $\\mathcal{EL}$ to Disunification: The Case of Dismatching and Local Disunification",
        "authors": [
            "Franz Baader",
            "Stefan Borgwardt",
            "Barbara Morawska"
        ],
        "abstract": "Unification in Description Logics has been introduced as a means to detect redundancies in ontologies. We try to extend the known decidability results for unification in the Description Logic $\\mathcal{EL}$ to disunification since negative constraints can be used to avoid unwanted unifiers. While decidability of the solvability of general $\\mathcal{EL}$-disunification problems remains an open problem, we obtain NP-completeness results for two interesting special cases: dismatching problems, where one side of each negative constraint must be ground, and local solvability of disunification problems, where we consider only solutions that are constructed from terms occurring in the input problem. More precisely, we first show that dismatching can be reduced to local disunification, and then provide two complementary NP-algorithms for finding local solutions of disunification problems.\n    ",
        "submission_date": "2016-09-19T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05774",
        "title": "A globally-applicable disease ontology for biosurveillance; Anthology of Biosurveillance Diseases (ABD)",
        "authors": [
            "A.R. Daughton",
            "R. Priedhorsky",
            "G. Fairchild",
            "N. Generous",
            "A. Hengartner",
            "E. Abeyta",
            "N. Velappan",
            "A. Lillo",
            "K. Stark",
            "A. Deshpande"
        ],
        "abstract": "Biosurveillance, a relatively young field, has recently increased in importance because of its relevance to national security and global health. Databases and tools describing particular subsets of disease are becoming increasingly common in the field. However, a common method to describe those diseases is lacking. Here, we present the Anthology of Biosurveillance Diseases (ABD), an ontology of infectious diseases of biosurveillance relevance.\n    ",
        "submission_date": "2016-08-25T00:00:00",
        "last_modified_date": "2016-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05787",
        "title": "Context-aware Sequential Recommendation",
        "authors": [
            "Qiang Liu",
            "Shu Wu",
            "Diyi Wang",
            "Zhaokang Li",
            "Liang Wang"
        ],
        "abstract": "Since sequential information plays an important role in modeling user behaviors, various sequential recommendation methods have been proposed. Methods based on Markov assumption are widely-used, but independently combine several most recent components. Recently, Recurrent Neural Networks (RNN) based methods have been successfully applied in several sequential modeling tasks. However, for real-world applications, these methods have difficulty in modeling the contextual information, which has been proved to be very important for behavior modeling. In this paper, we propose a novel model, named Context-Aware Recurrent Neural Networks (CA-RNN). Instead of using the constant input matrix and transition matrix in conventional RNN models, CA-RNN employs adaptive context-specific input matrices and adaptive context-specific transition matrices. The adaptive context-specific input matrices capture external situations where user behaviors happen, such as time, location, weather and so on. And the adaptive context-specific transition matrices capture how lengths of time intervals between adjacent behaviors in historical sequences affect the transition of global sequential features. Experimental results show that the proposed CA-RNN model yields significant improvements over state-of-the-art sequential recommendation methods and context-aware recommendation methods on two public datasets, i.e., the Taobao dataset and the Movielens-1M dataset.\n    ",
        "submission_date": "2016-09-19T00:00:00",
        "last_modified_date": "2016-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05796",
        "title": "Enabling Dark Energy Science with Deep Generative Models of Galaxy Images",
        "authors": [
            "Siamak Ravanbakhsh",
            "Francois Lanusse",
            "Rachel Mandelbaum",
            "Jeff Schneider",
            "Barnabas Poczos"
        ],
        "abstract": "Understanding the nature of dark energy, the mysterious force driving the accelerated expansion of the Universe, is a major challenge of modern cosmology. The next generation of cosmological surveys, specifically designed to address this issue, rely on accurate measurements of the apparent shapes of distant galaxies. However, shape measurement methods suffer from various unavoidable biases and therefore will rely on a precise calibration to meet the accuracy requirements of the science analysis. This calibration process remains an open challenge as it requires large sets of high quality galaxy images. To this end, we study the application of deep conditional generative models in generating realistic galaxy images. In particular we consider variations on conditional variational autoencoder and introduce a new adversarial objective for training of conditional generative networks. Our results suggest a reliable alternative to the acquisition of expensive high quality observations for generating the calibration data needed by the next generation of cosmological surveys.\n    ",
        "submission_date": "2016-09-19T00:00:00",
        "last_modified_date": "2016-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05960",
        "title": "Incremental Sampling-based Motion Planners Using Policy Iteration Methods",
        "authors": [
            "Oktay Arslan",
            "Panagiotis Tsiotras"
        ],
        "abstract": "Recent progress in randomized motion planners has led to the development of a new class of sampling-based algorithms that provide asymptotic optimality guarantees, notably the RRT* and the PRM* algorithms. Careful analysis reveals that the so-called \"rewiring\" step in these algorithms can be interpreted as a local policy iteration (PI) step (i.e., a local policy evaluation step followed by a local policy improvement step) so that asymptotically, as the number of samples tend to infinity, both algorithms converge to the optimal path almost surely (with probability 1). Policy iteration, along with value iteration (VI) are common methods for solving dynamic programming (DP) problems. Based on this observation, recently, the RRT$^{\\#}$ algorithm has been proposed, which performs, during each iteration, Bellman updates (aka \"backups\") on those vertices of the graph that have the potential of being part of the optimal path (i.e., the \"promising\" vertices). The RRT$^{\\#}$ algorithm thus utilizes dynamic programming ideas and implements them incrementally on randomly generated graphs to obtain high quality solutions. In this work, and based on this key insight, we explore a different class of dynamic programming algorithms for solving shortest-path problems on random graphs generated by iterative sampling methods. These class of algorithms utilize policy iteration instead of value iteration, and thus are better suited for massive parallelization. Contrary to the RRT* algorithm, the policy improvement during the rewiring step is not performed only locally but rather on a set of vertices that are classified as \"promising\" during the current iteration. This tends to speed-up the whole process. The resulting algorithm, aptly named Policy Iteration-RRT$^{\\#}$ (PI-RRT$^{\\#}$) is the first of a new class of DP-inspired algorithms for randomized motion planning that utilize PI methods.\n    ",
        "submission_date": "2016-09-19T00:00:00",
        "last_modified_date": "2016-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06380",
        "title": "Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention",
        "authors": [
            "Yang Liu",
            "Sujian Li"
        ],
        "abstract": "Recognizing implicit discourse relations is a challenging but important task in the field of Natural Language Processing. For such a complex text processing task, different from previous studies, we argue that it is necessary to repeatedly read the arguments and dynamically exploit the efficient features useful for recognizing discourse relations. To mimic the repeated reading strategy, we propose the neural networks with multi-level attention (NNMA), combining the attention mechanism and external memories to gradually fix the attention on some specific words helpful to judging the discourse relations. Experiments on the PDTB dataset show that our proposed method achieves the state-of-art results. The visualization of the attention weights also illustrates the progress that our model observes the arguments on each level and progressively locates the important words.\n    ",
        "submission_date": "2016-09-20T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06492",
        "title": "Document Image Coding and Clustering for Script Discrimination",
        "authors": [
            "Darko Brodic",
            "Alessia Amelio",
            "Zoran N. Milivojevic",
            "Milena Jevtic"
        ],
        "abstract": "The paper introduces a new method for discrimination of documents given in different scripts. The document is mapped into a uniformly coded text of numerical values. It is derived from the position of the letters in the text line, based on their typographical characteristics. Each code is considered as a gray level. Accordingly, the coded text determines a 1-D image, on which texture analysis by run-length statistics and local binary pattern is performed. It defines feature vectors representing the script content of the document. A modified clustering approach employed on document feature vector groups documents written in the same script. Experimentation performed on two custom oriented databases of historical documents in old Cyrillic, angular and round Glagolitic as well as Antiqua and Fraktur scripts demonstrates the superiority of the proposed method with respect to well-known methods in the state-of-the-art.\n    ",
        "submission_date": "2016-09-21T00:00:00",
        "last_modified_date": "2016-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06666",
        "title": "Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks",
        "authors": [
            "Martin Engelcke",
            "Dushyant Rao",
            "Dominic Zeng Wang",
            "Chi Hay Tong",
            "Ingmar Posner"
        ],
        "abstract": "This paper proposes a computationally efficient approach to detecting objects natively in 3D point clouds using convolutional neural networks (CNNs). In particular, this is achieved by leveraging a feature-centric voting scheme to implement novel convolutional layers which explicitly exploit the sparsity encountered in the input. To this end, we examine the trade-off between accuracy and speed for different architectures and additionally propose to use an L1 penalty on the filter activations to further encourage sparsity in the intermediate representations. To the best of our knowledge, this is the first work to propose sparse convolutional layers and L1 regularisation for efficient large-scale processing of 3D data. We demonstrate the efficacy of our approach on the KITTI object detection benchmark and show that Vote3Deep models with as few as three layers outperform the previous state of the art in both laser and laser-vision based approaches by margins of up to 40% while remaining highly competitive in terms of processing time.\n    ",
        "submission_date": "2016-09-21T00:00:00",
        "last_modified_date": "2017-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07042",
        "title": "Pose-Selective Max Pooling for Measuring Similarity",
        "authors": [
            "Xiang Xiang",
            "Trac D. Tran"
        ],
        "abstract": "In this paper, we deal with two challenges for measuring the similarity of the subject identities in practical video-based face recognition - the variation of the head pose in uncontrolled environments and the computational expense of processing videos. Since the frame-wise feature mean is unable to characterize the pose diversity among frames, we define and preserve the overall pose diversity and closeness in a video. Then, identity will be the only source of variation across videos since the pose varies even within a single video. Instead of simply using all the frames, we select those faces whose pose point is closest to the centroid of the K-means cluster containing that pose point. Then, we represent a video as a bag of frame-wise deep face features while the number of features has been reduced from hundreds to K. Since the video representation can well represent the identity, now we measure the subject similarity between two videos as the max correlation among all possible pairs in the two bags of features. On the official 5,000 video-pairs of the YouTube Face dataset for face verification, our algorithm achieves a comparable performance with VGG-face that averages over deep features of all frames. Other vision tasks can also benefit from the generic idea of employing geometric cues to improve the descriptiveness of deep features.\n    ",
        "submission_date": "2016-09-22T00:00:00",
        "last_modified_date": "2016-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07127",
        "title": "Social Network Processes in the Isabelle and Coq Theorem Proving Communities",
        "authors": [
            "Jacques Fleuriot",
            "Steven Obua",
            "Phil Scott"
        ],
        "abstract": "We identify the main actors in the Isabelle and Coq communities and describe how they affect and influence their peers. This work explores selected foundations of social networking analysis that we expect to be useful in the context of the ProofPeer project, which is developing a new model for interactive theorem proving based on collaboration and social interactions.\n    ",
        "submission_date": "2016-09-22T00:00:00",
        "last_modified_date": "2016-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07317",
        "title": "Language as a Latent Variable: Discrete Generative Models for Sentence Compression",
        "authors": [
            "Yishu Miao",
            "Phil Blunsom"
        ],
        "abstract": "In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences. In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data. Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data.\n    ",
        "submission_date": "2016-09-23T00:00:00",
        "last_modified_date": "2016-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07365",
        "title": "Towards the bio-personalization of music recommendation systems: A single-sensor EEG biomarker of subjective music preference",
        "authors": [
            "Dimitrios A. Adamos",
            "Stavros I. Dimitriadis",
            "Nikolaos A. Laskaris"
        ],
        "abstract": "Recent advances in biosensors technology and mobile electroencephalographic (EEG) interfaces have opened new application fields for cognitive monitoring. A computable biomarker for the assessment of spontaneous aesthetic brain responses during music listening is introduced here. It derives from well-established measures of cross-frequency coupling (CFC) and quantifies the music-induced alterations in the dynamic relationships between brain rhythms. During a stage of exploratory analysis, and using the signals from a suitably designed experiment, we established the biomarker, which acts on brain activations recorded over the left prefrontal cortex and focuses on the functional coupling between high-beta and low-gamma oscillations. Based on data from an additional experimental paradigm, we validated the introduced biomarker and showed its relevance for expressing the subjective aesthetic appreciation of a piece of music. Our approach resulted in an affordable tool that can promote human-machine interaction and, by serving as a personalized music annotation strategy, can be potentially integrated into modern flexible music recommendation systems.\n",
        "submission_date": "2016-09-21T00:00:00",
        "last_modified_date": "2016-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07384",
        "title": "Discovering Sound Concepts and Acoustic Relations In Text",
        "authors": [
            "Anurag Kumar",
            "Bhiksha Raj",
            "Ndapandula Nakashole"
        ],
        "abstract": "In this paper we describe approaches for discovering acoustic concepts and relations in text. The first major goal is to be able to identify text phrases which contain a notion of audibility and can be termed as a sound or an acoustic concept. We also propose a method to define an acoustic scene through a set of sound concepts. We use pattern matching and parts of speech tags to generate sound concepts from large scale text corpora. We use dependency parsing and LSTM recurrent neural network to predict a set of sound concepts for a given acoustic scene. These methods are not only helpful in creating an acoustic knowledge base but in the future can also directly help acoustic event and scene detection research.\n    ",
        "submission_date": "2016-09-23T00:00:00",
        "last_modified_date": "2017-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07460",
        "title": "Optimizing positional scoring rules for rank aggregation",
        "authors": [
            "Ioannis Caragiannis",
            "Xenophon Chatzigeorgiou",
            "George A. Krimpas",
            "Alexandros A. Voudouris"
        ],
        "abstract": "Nowadays, several crowdsourcing projects exploit social choice methods for computing an aggregate ranking of alternatives given individual rankings provided by workers. Motivated by such systems, we consider a setting where each worker is asked to rank a fixed (small) number of alternatives and, then, a positional scoring rule is used to compute the aggregate ranking. Among the apparently infinite such rules, what is the best one to use? To answer this question, we assume that we have partial access to an underlying true ranking. Then, the important optimization problem to be solved is to compute the positional scoring rule whose outcome, when applied to the profile of individual rankings, is as close as possible to the part of the underlying true ranking we know. We study this fundamental problem from a theoretical viewpoint and present positive and negative complexity results and, furthermore, complement our theoretical findings with experiments on real-world and synthetic data.\n    ",
        "submission_date": "2016-09-18T00:00:00",
        "last_modified_date": "2018-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07480",
        "title": "Predictive modelling of football injuries",
        "authors": [
            "Stylianos Kampakis"
        ],
        "abstract": "The goal of this thesis is to investigate the potential of predictive modelling for football injuries. This work was conducted in close collaboration with Tottenham Hotspurs FC (THFC), the PGA European tour and the participation of Wolverhampton Wanderers (WW).\n",
        "submission_date": "2016-09-20T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07521",
        "title": "Fast Learning of Clusters and Topics via Sparse Posteriors",
        "authors": [
            "Michael C. Hughes",
            "Erik B. Sudderth"
        ],
        "abstract": "Mixture models and topic models generate each observation from a single cluster, but standard variational posteriors for each observation assign positive probability to all possible clusters. This requires dense storage and runtime costs that scale with the total number of clusters, even though typically only a few clusters have significant posterior mass for any data point. We propose a constrained family of sparse variational distributions that allow at most $L$ non-zero entries, where the tunable threshold $L$ trades off speed for accuracy. Previous sparse approximations have used hard assignments ($L=1$), but we find that moderate values of $L>1$ provide superior performance. Our approach easily integrates with stochastic or incremental optimization algorithms to scale to millions of examples. Experiments training mixture models of image patches and topic models for news articles show that our approach produces better-quality models in far less time than baseline methods.\n    ",
        "submission_date": "2016-09-23T00:00:00",
        "last_modified_date": "2016-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07560",
        "title": "Informative Planning and Online Learning with Sparse Gaussian Processes",
        "authors": [
            "Kai-Chieh Ma",
            "Lantao Liu",
            "Gaurav S. Sukhatme"
        ],
        "abstract": "A big challenge in environmental monitoring is the spatiotemporal variation of the phenomena to be observed. To enable persistent sensing and estimation in such a setting, it is beneficial to have a time-varying underlying environmental model. Here we present a planning and learning method that enables an autonomous marine vehicle to perform persistent ocean monitoring tasks by learning and refining an environmental model. To alleviate the computational bottleneck caused by large-scale data accumulated, we propose a framework that iterates between a planning component aimed at collecting the most information-rich data, and a sparse Gaussian Process learning component where the environmental model and hyperparameters are learned online by taking advantage of only a subset of data that provides the greatest contribution. Our simulations with ground-truth ocean data shows that the proposed method is both accurate and efficient.\n    ",
        "submission_date": "2016-09-24T00:00:00",
        "last_modified_date": "2016-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07706",
        "title": "Learning by Stimulation Avoidance: A Principle to Control Spiking Neural Networks Dynamics",
        "authors": [
            "Lana Sinapayen",
            "Atsushi Masumori",
            "Takashi Ikegami"
        ],
        "abstract": "Learning based on networks of real neurons, and by extension biologically inspired models of neural networks, has yet to find general learning rules leading to widespread applications. In this paper, we argue for the existence of a principle allowing to steer the dynamics of a biologically inspired neural network. Using carefully timed external stimulation, the network can be driven towards a desired dynamical state. We term this principle \"Learning by Stimulation Avoidance\" (LSA). We demonstrate through simulation that the minimal sufficient conditions leading to LSA in artificial networks are also sufficient to reproduce learning results similar to those obtained in biological neurons by Shahaf and Marom [1]. We examine the mechanism's basic dynamics in a reduced network, and demonstrate how it scales up to a network of 100 neurons. We show that LSA has a higher explanatory power than existing hypotheses about the response of biological neural networks to external simulation, and can be used as a learning rule for an embodied application: learning of wall avoidance by a simulated robot. The surge in popularity of artificial neural networks is mostly directed to disembodied models of neurons with biologically irrelevant dynamics: to the authors' knowledge, this is the first work demonstrating sensory-motor learning with random spiking networks through pure Hebbian learning.\n    ",
        "submission_date": "2016-09-25T00:00:00",
        "last_modified_date": "2019-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07843",
        "title": "Pointer Sentinel Mixture Models",
        "authors": [
            "Stephen Merity",
            "Caiming Xiong",
            "James Bradbury",
            "Richard Socher"
        ],
        "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.\n    ",
        "submission_date": "2016-09-26T00:00:00",
        "last_modified_date": "2016-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08049",
        "title": "Towards Evidence-Based Ontology for Supporting Systematic Literature Review",
        "authors": [
            "Yueming Sun",
            "Ye Yang",
            "He Zhang",
            "Wen Zhang",
            "Qing Wang"
        ],
        "abstract": "[Background]: Systematic Literature Review (SLR) has become an important software engineering research method but costs tremendous efforts. [Aim]: This paper proposes an approach to leverage on empirically evolved ontology to support automating key SLR activities. [Method]: First, we propose an ontology, SLRONT, built on SLR experiences and best practices as a groundwork to capture common terminologies and their relationships during SLR processes; second, we present an extended version of SLRONT, the COSONT and instantiate it with the knowledge and concepts extracted from structured abstracts. Case studies illustrate the details of applying it for supporting SLR steps. [Results]: Results show that through using COSONT, we acquire the same conclusion compared with sheer manual works, but the efforts involved is significantly reduced. [Conclusions]: The approach of using ontology could effectively and efficiently support the conducting of systematic literature review.\n    ",
        "submission_date": "2016-09-22T00:00:00",
        "last_modified_date": "2016-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08082",
        "title": "An Ontology of Preference-Based Multiobjective Metaheuristics",
        "authors": [
            "Longmei Li",
            "Iryna Yevseyeva",
            "Vitor Basto-Fernandes",
            "Heike Trautmann",
            "Ning Jing",
            "Michael Emmerich"
        ],
        "abstract": "User preference integration is of great importance in multi-objective optimization, in particular in many objective optimization. Preferences have long been considered in traditional multicriteria decision making (MCDM) which is based on mathematical programming. Recently, it is integrated in multi-objective metaheuristics (MOMH), resulting in focus on preferred parts of the Pareto front instead of the whole Pareto front. The number of publications on preference-based multi-objective metaheuristics has increased rapidly over the past decades. There already exist various preference handling methods and MOMH methods, which have been combined in diverse ways. This article proposes to use the Web Ontology Language (OWL) to model and systematize the results developed in this field. A review of the existing work is provided, based on which an ontology is built and instantiated with state-of-the-art results. The OWL ontology is made public and open to future extension. Moreover, the usage of the ontology is exemplified for different use-cases, including querying for methods that match an engineering application, bibliometric analysis, checking existence of combinations of preference models and MOMH techniques, and discovering opportunities for new research and open research questions.\n    ",
        "submission_date": "2016-09-26T00:00:00",
        "last_modified_date": "2017-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08144",
        "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
        "authors": [
            "Yonghui Wu",
            "Mike Schuster",
            "Zhifeng Chen",
            "Quoc V. Le",
            "Mohammad Norouzi",
            "Wolfgang Macherey",
            "Maxim Krikun",
            "Yuan Cao",
            "Qin Gao",
            "Klaus Macherey",
            "Jeff Klingner",
            "Apurva Shah",
            "Melvin Johnson",
            "Xiaobing Liu",
            "\u0141ukasz Kaiser",
            "Stephan Gouws",
            "Yoshikiyo Kato",
            "Taku Kudo",
            "Hideto Kazawa",
            "Keith Stevens",
            "George Kurian",
            "Nishant Patil",
            "Wei Wang",
            "Cliff Young",
            "Jason Smith",
            "Jason Riesa",
            "Alex Rudnick",
            "Oriol Vinyals",
            "Greg Corrado",
            "Macduff Hughes",
            "Jeffrey Dean"
        ],
        "abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.\n    ",
        "submission_date": "2016-09-26T00:00:00",
        "last_modified_date": "2016-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08194",
        "title": "Online Segment to Segment Neural Transduction",
        "authors": [
            "Lei Yu",
            "Jan Buys",
            "Phil Blunsom"
        ],
        "abstract": "We introduce an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read. By independently tracking the encoding and decoding representations our algorithm permits exact polynomial marginalization of the latent segmentation during training, and during decoding beam search is employed to find the best alignment path together with the predicted output sequence. Our model tackles the bottleneck of vanilla encoder-decoders that have to read and memorize the entire input sequence in their fixed-length hidden states before producing any output. It is different from previous attentive models in that, instead of treating the attention weights as output of a deterministic function, our model assigns attention weights to a sequential latent variable which can be marginalized out and permits online generation. Experiments on abstractive sentence summarization and morphological inflection show significant performance gains over the baseline encoder-decoders.\n    ",
        "submission_date": "2016-09-26T00:00:00",
        "last_modified_date": "2016-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08210",
        "title": "Learning to Translate for Multilingual Question Answering",
        "authors": [
            "Ferhan Ture",
            "Elizabeth Boschee"
        ],
        "abstract": "In multilingual question answering, either the question needs to be translated into the document language, or vice versa. In addition to direction, there are multiple methods to perform the translation, four of which we explore in this paper: word-based, 10-best, context-based, and grammar-based. We build a feature for each combination of translation direction and method, and train a model that learns optimal feature weights. On a large forum dataset consisting of posts in English, Arabic, and Chinese, our novel learn-to-translate approach was more effective than a strong baseline (p<0.05): translating all text into English, then training a classifier based only on English (original or translated) text.\n    ",
        "submission_date": "2016-09-26T00:00:00",
        "last_modified_date": "2016-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08264",
        "title": "Top-N Recommendation on Graphs",
        "authors": [
            "Zhao Kang",
            "Chong Peng",
            "Ming Yang",
            "Qiang Cheng"
        ],
        "abstract": "Recommender systems play an increasingly important role in online applications to help users find what they need or prefer. Collaborative filtering algorithms that generate predictions by analyzing the user-item rating matrix perform poorly when the matrix is sparse. To alleviate this problem, this paper proposes a simple recommendation algorithm that fully exploits the similarity information among users and items and intrinsic structural information of the user-item matrix. The proposed method constructs a new representation which preserves affinity and structure information in the user-item rating matrix and then performs recommendation task. To capture proximity information about users and items, two graphs are constructed. Manifold learning idea is used to constrain the new representation to be smooth on these graphs, so as to enforce users and item proximities. Our model is formulated as a convex optimization problem, for which we need to solve the well-known Sylvester equation only. We carry out extensive empirical evaluations on six benchmark datasets to show the effectiveness of this approach.\n    ",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08419",
        "title": "Decision Making Based on Cohort Scores for Speaker Verification",
        "authors": [
            "Lantian Li",
            "Renyu Wang",
            "Gang Wang",
            "Caixia Wang",
            "Thomas Fang Zheng"
        ],
        "abstract": "Decision making is an important component in a speaker verification system. For the conventional GMM-UBM architecture, the decision is usually conducted based on the log likelihood ratio of the test utterance against the GMM of the claimed speaker and the UBM. This single-score decision is simple but tends to be sensitive to the complex variations in speech signals (e.g. text content, channel, speaking style, etc.). In this paper, we propose a decision making approach based on multiple scores derived from a set of cohort GMMs (cohort scores). Importantly, these cohort scores are not simply averaged as in conventional cohort methods; instead, we employ a powerful discriminative model as the decision maker. Experimental results show that the proposed method delivers substantial performance improvement over the baseline system, especially when a deep neural network (DNN) is used as the decision maker, and the DNN input involves some statistical features derived from the cohort scores.\n    ",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08441",
        "title": "Weakly Supervised PLDA Training",
        "authors": [
            "Lantian Li",
            "Yixiang Chen",
            "Dong Wang",
            "Chenghui Zhao"
        ],
        "abstract": "PLDA is a popular normalization approach for the i-vector model, and it has delivered state-of-the-art performance in speaker verification. However, PLDA training requires a large amount of labelled development data, which is highly expensive in most cases. We present a cheap PLDA training approach, which assumes that speakers in the same session can be easily separated, and speakers in different sessions are simply different. This results in `weak labels' which are not fully accurate but cheap, leading to a weak PLDA training.\n",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2017-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08445",
        "title": "AP16-OL7: A Multilingual Database for Oriental Languages and A Language Recognition Baseline",
        "authors": [
            "Dong Wang",
            "Lantian Li",
            "Difei Tang",
            "Qing Chen"
        ],
        "abstract": "We present the AP16-OL7 database which was released as the training and test data for the oriental language recognition (OLR) challenge on APSIPA 2016. Based on the database, a baseline system was constructed on the basis of the i-vector model. We report the baseline results evaluated in various metrics defined by the AP16-OLR evaluation plan and demonstrate that AP16-OL7 is a reasonable data resource for multilingual research.\n    ",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08677",
        "title": "A Fast Factorization-based Approach to Robust PCA",
        "authors": [
            "Chong Peng",
            "Zhao Kang",
            "Qiang Chen"
        ],
        "abstract": "Robust principal component analysis (RPCA) has been widely used for recovering low-rank matrices in many data mining and machine learning problems. It separates a data matrix into a low-rank part and a sparse part. The convex approach has been well studied in the literature. However, state-of-the-art algorithms for the convex approach usually have relatively high complexity due to the need of solving (partial) singular value decompositions of large matrices. A non-convex approach, AltProj, has also been proposed with lighter complexity and better scalability. Given the true rank $r$ of the underlying low rank matrix, AltProj has a complexity of $O(r^2dn)$, where $d\\times n$ is the size of data matrix. In this paper, we propose a novel factorization-based model of RPCA, which has a complexity of $O(kdn)$, where $k$ is an upper bound of the true rank. Our method does not need the precise value of the true rank. From extensive experiments, we observe that AltProj can work only when $r$ is precisely known in advance; however, when the needed rank parameter $r$ is specified to a value different from the true rank, AltProj cannot fully separate the two parts while our method succeeds. Even when both work, our method is about 4 times faster than AltProj. Our method can be used as a light-weight, scalable tool for RPCA in the absence of the precise value of the true rank.\n    ",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08843",
        "title": "Hierarchical Memory Networks for Answer Selection on Unknown Words",
        "authors": [
            "Jiaming Xu",
            "Jing Shi",
            "Yiqun Yao",
            "Suncong Zheng",
            "Bo Xu",
            "Bo Xu"
        ],
        "abstract": "Recently, end-to-end memory networks have shown promising results on Question Answering task, which encode the past facts into an explicit memory and perform reasoning ability by making multiple computational steps on the memory. However, memory networks conduct the reasoning on sentence-level memory to output coarse semantic vectors and do not further take any attention mechanism to focus on words, which may lead to the model lose some detail information, especially when the answers are rare or unknown words. In this paper, we propose a novel Hierarchical Memory Networks, dubbed HMN. First, we encode the past facts into sentence-level memory and word-level memory respectively. Then, (k)-max pooling is exploited following reasoning module on the sentence-level memory to sample the (k) most relevant sentences to a question and feed these sentences into attention mechanism on the word-level memory to focus the words in the selected sentences. Finally, the prediction is jointly learned over the outputs of the sentence-level reasoning module and the word-level attention mechanism. The experimental results demonstrate that our approach successfully conducts answer selection on unknown words and achieves a better performance than memory networks.\n    ",
        "submission_date": "2016-09-28T00:00:00",
        "last_modified_date": "2016-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09001",
        "title": "Learning from the Hindsight Plan -- Episodic MPC Improvement",
        "authors": [
            "Aviv Tamar",
            "Garrett Thomas",
            "Tianhao Zhang",
            "Sergey Levine",
            "Pieter Abbeel"
        ],
        "abstract": "Model predictive control (MPC) is a popular control method that has proved effective for robotics, among other fields. MPC performs re-planning at every time step. Re-planning is done with a limited horizon per computational and real-time constraints and often also for robustness to potential model errors. However, the limited horizon leads to suboptimal performance. In this work, we consider the iterative learning setting, where the same task can be repeated several times, and propose a policy improvement scheme for MPC. The main idea is that between executions we can, offline, run MPC with a longer horizon, resulting in a hindsight plan. To bring the next real-world execution closer to the hindsight plan, our approach learns to re-shape the original cost function with the goal of satisfying the following property: short horizon planning (as realistic during real executions) with respect to the shaped cost should result in mimicking the hindsight plan. This effectively consolidates long-term reasoning into the short-horizon planning. We empirically evaluate our approach in contact-rich manipulation tasks both in simulated and real environments, such as peg insertion by a real PR2 robot.\n    ",
        "submission_date": "2016-09-28T00:00:00",
        "last_modified_date": "2017-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09226",
        "title": "ICE: Information Credibility Evaluation on Social Media via Representation Learning",
        "authors": [
            "Qiang Liu",
            "Shu Wu",
            "Feng Yu",
            "Liang Wang",
            "Tieniu Tan"
        ],
        "abstract": "With the rapid growth of social media, rumors are also spreading widely on social media and bring harm to people's daily life. Nowadays, information credibility evaluation has drawn attention from academic and industrial communities. Current methods mainly focus on feature engineering and achieve some success. However, feature engineering based methods require a lot of labor and cannot fully reveal the underlying relations among data. In our viewpoint, the key elements of user behaviors for evaluating credibility are concluded as \"who\", \"what\", \"when\", and \"how\". These existing methods cannot model the correlation among different key elements during the spreading of microblogs. In this paper, we propose a novel representation learning method, Information Credibility Evaluation (ICE), to learn representations of information credibility on social media. In ICE, latent representations are learnt for modeling user credibility, behavior types, temporal properties, and comment attitudes. The aggregation of these factors in the microblog spreading process yields the representation of a user's behavior, and the aggregation of these dynamic representations generates the credibility representation of an event spreading on social media. Moreover, a pairwise learning method is applied to maximize the credibility difference between rumors and non-rumors. To evaluate the performance of ICE, we conduct experiments on a Sina Weibo data set, and the experimental results show that our ICE model outperforms the state-of-the-art methods.\n    ",
        "submission_date": "2016-09-29T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09315",
        "title": "Semantic Parsing with Semi-Supervised Sequential Autoencoders",
        "authors": [
            "Tom\u00e1\u0161 Ko\u010disk\u00fd",
            "G\u00e1bor Melis",
            "Edward Grefenstette",
            "Chris Dyer",
            "Wang Ling",
            "Phil Blunsom",
            "Karl Moritz Hermann"
        ],
        "abstract": "We present a novel semi-supervised approach for sequence transduction and apply it to semantic parsing. The unsupervised component is based on a generative model in which latent sentences generate the unpaired logical forms. We apply this method to a number of semantic parsing tasks focusing on domains with limited access to labelled training data and extend those datasets with synthetically generated logical forms.\n    ",
        "submission_date": "2016-09-29T00:00:00",
        "last_modified_date": "2016-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09365",
        "title": "Deep Tracking on the Move: Learning to Track the World from a Moving Vehicle using Recurrent Neural Networks",
        "authors": [
            "Julie Dequaire",
            "Dushyant Rao",
            "Peter Ondruska",
            "Dominic Wang",
            "Ingmar Posner"
        ],
        "abstract": "This paper presents an end-to-end approach for tracking static and dynamic objects for an autonomous vehicle driving through crowded urban environments. Unlike traditional approaches to tracking, this method is learned end-to-end, and is able to directly predict a full unoccluded occupancy grid map from raw laser input data. Inspired by the recently presented DeepTracking approach [Ondruska, 2016], we employ a recurrent neural network (RNN) to capture the temporal evolution of the state of the environment, and propose to use Spatial Transformer modules to exploit estimates of the egomotion of the vehicle. Our results demonstrate the ability to track a range of objects, including cars, buses, pedestrians, and cyclists through occlusion, from both moving and stationary platforms, using a single learned model. Experimental results demonstrate that the model can also predict the future states of objects from current inputs, with greater accuracy than previous work.\n    ",
        "submission_date": "2016-09-29T00:00:00",
        "last_modified_date": "2017-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09405",
        "title": "Evaluating Induced CCG Parsers on Grounded Semantic Parsing",
        "authors": [
            "Yonatan Bisk",
            "Siva Reddy",
            "John Blitzer",
            "Julia Hockenmaier",
            "Mark Steedman"
        ],
        "abstract": "We compare the effectiveness of four different syntactic CCG parsers for a semantic slot-filling task to explore how much syntactic supervision is required for downstream semantic analysis. This extrinsic, task-based evaluation provides a unique window to explore the strengths and weaknesses of semantics captured by unsupervised grammar induction systems. We release a new Freebase semantic parsing dataset called SPADES (Semantic PArsing of DEclarative Sentences) containing 93K cloze-style questions paired with answers. We evaluate all our models on this dataset. Our code and data are available at ",
        "submission_date": "2016-09-29T00:00:00",
        "last_modified_date": "2017-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09444",
        "title": "Contextual RNN-GANs for Abstract Reasoning Diagram Generation",
        "authors": [
            "Arnab Ghosh",
            "Viveka Kulharia",
            "Amitabha Mukerjee",
            "Vinay Namboodiri",
            "Mohit Bansal"
        ],
        "abstract": "Understanding, predicting, and generating object motions and transformations is a core problem in artificial intelligence. Modeling sequences of evolving images may provide better representations and models of motion and may ultimately be used for forecasting, simulation, or video generation. Diagrammatic Abstract Reasoning is an avenue in which diagrams evolve in complex patterns and one needs to infer the underlying pattern sequence and generate the next image in the sequence. For this, we develop a novel Contextual Generative Adversarial Network based on Recurrent Neural Networks (Context-RNN-GANs), where both the generator and the discriminator modules are based on contextual history (modeled as RNNs) and the adversarial discriminator guides the generator to produce realistic images for the particular time step in the image sequence. We evaluate the Context-RNN-GAN model (and its variants) on a novel dataset of Diagrammatic Abstract Reasoning, where it performs competitively with 10th-grade human performance but there is still scope for interesting improvements as compared to college-grade human performance. We also evaluate our model on a standard video next-frame prediction task, achieving improved performance over comparable state-of-the-art.\n    ",
        "submission_date": "2016-09-29T00:00:00",
        "last_modified_date": "2016-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09869",
        "title": "Structured Inference Networks for Nonlinear State Space Models",
        "authors": [
            "Rahul G. Krishnan",
            "Uri Shalit",
            "David Sontag"
        ],
        "abstract": "Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood.\n    ",
        "submission_date": "2016-09-30T00:00:00",
        "last_modified_date": "2016-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00001",
        "title": "Bacterial Foraging Optimized STATCOM for Stability Assessment in Power System",
        "authors": [
            "Shiba R. Paital",
            "Prakash K. Ray",
            "Asit Mohanty",
            "Sandipan Patra",
            "Harishchandra Dubey"
        ],
        "abstract": "This paper presents a study of improvement in stability in a single machine connected to infinite bus (SMIB) power system by using static compensator (STATCOM). The gains of Proportional-Integral-Derivative (PID) controller in STATCOM are being optimized by heuristic technique based on Particle swarm optimization (PSO). Further, Bacterial Foraging Optimization (BFO) as an alternative heuristic method is also applied to select optimal gains of PID controller. The performance of STATCOM with the above soft-computing techniques are studied and compared with the conventional PID controller under various scenarios. The simulation results are accompanied with performance indices based quantitative analysis. The analysis clearly signifies the robustness of the new scheme in terms of stability and voltage regulation when compared with conventional PID.\n    ",
        "submission_date": "2016-10-01T00:00:00",
        "last_modified_date": "2016-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00086",
        "title": "Consistency Ensuring in Social Web Services Based on Commitments Structure",
        "authors": [
            "Marzieh Adelnia",
            "Mohammad Reza Khayyambashi"
        ],
        "abstract": "Web Service is one of the most significant current discussions in information sharing technologies and one of the examples of service oriented processing. To ensure accurate execution of web services operations, it must be adaptable with policies of the social networks in which it signs up. This adaptation implements using controls called 'Commitment'. This paper describes commitments structure and existing research about commitments and social web services, then suggests an algorithm for consistency of commitments in social web services. As regards the commitments may be executed concurrently, a key challenge in web services execution based on commitment structure is consistency ensuring in execution time. The purpose of this research is providing an algorithm for consistency ensuring between web services operations based on commitments structure.\n    ",
        "submission_date": "2016-10-01T00:00:00",
        "last_modified_date": "2016-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00163",
        "title": "X-CNN: Cross-modal Convolutional Neural Networks for Sparse Datasets",
        "authors": [
            "Petar Veli\u010dkovi\u0107",
            "Duo Wang",
            "Nicholas D. Lane",
            "Pietro Li\u00f2"
        ],
        "abstract": "In this paper we propose cross-modal convolutional neural networks (X-CNNs), a novel biologically inspired type of CNN architectures, treating gradient descent-specialised CNNs as individual units of processing in a larger-scale network topology, while allowing for unconstrained information flow and/or weight sharing between analogous hidden layers of the network---thus generalising the already well-established concept of neural network ensembles (where information typically may flow only between the output layers of the individual networks). The constituent networks are individually designed to learn the output function on their own subset of the input data, after which cross-connections between them are introduced after each pooling operation to periodically allow for information exchange between them. This injection of knowledge into a model (by prior partition of the input data through domain knowledge or unsupervised methods) is expected to yield greatest returns in sparse data environments, which are typically less suitable for training CNNs. For evaluation purposes, we have compared a standard four-layer CNN as well as a sophisticated FitNet4 architecture against their cross-modal variants on the CIFAR-10 and CIFAR-100 datasets with differing percentages of the training data being removed, and find that at lower levels of data availability, the X-CNNs significantly outperform their baselines (typically providing a 2--6% benefit, depending on the dataset size and whether data augmentation is used), while still maintaining an edge on all of the full dataset tests.\n    ",
        "submission_date": "2016-10-01T00:00:00",
        "last_modified_date": "2016-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00243",
        "title": "Deep unsupervised learning through spatial contrasting",
        "authors": [
            "Elad Hoffer",
            "Itay Hubara",
            "Nir Ailon"
        ],
        "abstract": "Convolutional networks have marked their place over the last few years as the best performing model for various visual tasks. They are, however, most suited for supervised learning from large amounts of labeled data. Previous attempts have been made to use unlabeled data to improve model performance by applying unsupervised techniques. These attempts require different architectures and training methods. In this work we present a novel approach for unsupervised training of Convolutional networks that is based on contrasting between spatial regions within images. This criterion can be employed within conventional neural networks and trained using standard techniques such as SGD and back-propagation, thus complementing supervised methods.\n    ",
        "submission_date": "2016-10-02T00:00:00",
        "last_modified_date": "2018-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00465",
        "title": "Can Evolutionary Sampling Improve Bagged Ensembles?",
        "authors": [
            "Harsh Nisar",
            "Bhanu Pratap Singh Rawat"
        ],
        "abstract": "Perturb and Combine (P&C) group of methods generate multiple versions of the predictor by perturbing the training set or construction and then combining them into a single predictor (Breiman, 1996b). The motive is to improve the accuracy in unstable classification and regression methods. One of the most well known method in this group is Bagging. Arcing or Adaptive Resampling and Combining methods like AdaBoost are smarter variants of P&C methods. In this extended abstract, we lay the groundwork for a new family of methods under the P&C umbrella, known as Evolutionary Sampling (ES). We employ Evolutionary algorithms to suggest smarter sampling in both the feature space (sub-spaces) as well as training samples. We discuss multiple fitness functions to assess ensembles and empirically compare our performance against randomized sampling of training data and feature sub-spaces.\n    ",
        "submission_date": "2016-10-03T00:00:00",
        "last_modified_date": "2016-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00633",
        "title": "Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates",
        "authors": [
            "Shixiang Gu",
            "Ethan Holly",
            "Timothy Lillicrap",
            "Sergey Levine"
        ],
        "abstract": "Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations.\n    ",
        "submission_date": "2016-10-03T00:00:00",
        "last_modified_date": "2016-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00673",
        "title": "Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search",
        "authors": [
            "Ali Yahya",
            "Adrian Li",
            "Mrinal Kalakrishnan",
            "Yevgen Chebotar",
            "Sergey Levine"
        ],
        "abstract": "In principle, reinforcement learning and policy search methods can enable robots to learn highly complex and general skills that may allow them to function amid the complexity and diversity of the real world. However, training a policy that generalizes well across a wide range of real-world conditions requires far greater quantity and diversity of experience than is practical to collect with a single robot. Fortunately, it is possible for multiple robots to share their experience with one another, and thereby, learn a policy collectively. In this work, we explore distributed and asynchronous policy learning as a means to achieve generalization and improved training times on challenging, real-world manipulation tasks. We propose a distributed and asynchronous version of Guided Policy Search and use it to demonstrate collective policy learning on a vision-based door opening task using four robots. We show that it achieves better generalization, utilization, and training times than the single robot alternative.\n    ",
        "submission_date": "2016-10-03T00:00:00",
        "last_modified_date": "2016-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00696",
        "title": "Deep Visual Foresight for Planning Robot Motion",
        "authors": [
            "Chelsea Finn",
            "Sergey Levine"
        ],
        "abstract": "A key challenge in scaling up robot learning to many skills and environments is removing the need for human supervision, so that robots can collect their own data and improve their own performance without being limited by the cost of requesting human feedback. Model-based reinforcement learning holds the promise of enabling an agent to learn to predict the effects of its actions, which could provide flexible predictive models for a wide range of tasks and environments, without detailed human supervision. We develop a method for combining deep action-conditioned video prediction models with model-predictive control that uses entirely unlabeled training data. Our approach does not require a calibrated camera, an instrumented training set-up, nor precise sensing and actuation. Our results show that our method enables a real robot to perform nonprehensile manipulation -- pushing objects -- and can handle novel objects not seen during training.\n    ",
        "submission_date": "2016-10-03T00:00:00",
        "last_modified_date": "2017-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00782",
        "title": "Network Structure Inference, A Survey: Motivations, Methods, and Applications",
        "authors": [
            "Ivan Brugere",
            "Brian Gallagher",
            "Tanya Y. Berger-Wolf"
        ],
        "abstract": "Networks represent relationships between entities in many complex systems, spanning from online social interactions to biological cell development and brain connectivity. In many cases, relationships between entities are unambiguously known: are two users 'friends' in a social network? Do two researchers collaborate on a published paper? Do two road segments in a transportation system intersect? These are directly observable in the system in question. In most cases, relationship between nodes are not directly observable and must be inferred: does one gene regulate the expression of another? Do two animals who physically co-locate have a social bond? Who infected whom in a disease outbreak in a population?\n",
        "submission_date": "2016-10-03T00:00:00",
        "last_modified_date": "2018-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00956",
        "title": "Embracing data abundance: BookTest Dataset for Reading Comprehension",
        "authors": [
            "Ondrej Bajgar",
            "Rudolf Kadlec",
            "Jan Kleindienst"
        ],
        "abstract": "There is a practically unlimited amount of natural language data available. Still, recent work in text comprehension has focused on datasets which are small relative to current computing possibilities. This article is making a case for the community to move to larger data and as a step in that direction it is proposing the BookTest, a new dataset similar to the popular Children's Book Test (CBT), however more than 60 times larger. We show that training on the new data improves the accuracy of our Attention-Sum Reader model on the original CBT test data by a much larger margin than many recent attempts to improve the model architecture. On one version of the dataset our ensemble even exceeds the human baseline provided by Facebook. We then show in our own human study that there is still space for further improvement.\n    ",
        "submission_date": "2016-10-04T00:00:00",
        "last_modified_date": "2016-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01076",
        "title": "Tutorial on Answering Questions about Images with Deep Learning",
        "authors": [
            "Mateusz Malinowski",
            "Mario Fritz"
        ],
        "abstract": "Together with the development of more accurate methods in Computer Vision and Natural Language Understanding, holistic architectures that answer on questions about the content of real-world images have emerged. In this tutorial, we build a neural-based approach to answer questions about images. We base our tutorial on two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the models that we present here can achieve a competitive performance on both datasets, in fact, they are among the best methods that use a combination of LSTM with a global, full frame CNN representation of an image. We hope that after reading this tutorial, the reader will be able to use Deep Learning frameworks, such as Keras and introduced Kraino, to build various architectures that will lead to a further performance improvement on this challenging task.\n    ",
        "submission_date": "2016-10-04T00:00:00",
        "last_modified_date": "2016-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01221",
        "title": "Seer: Empowering Software Defined Networking with Data Analytics",
        "authors": [
            "Kyriakos Sideris",
            "Reza Nejabati",
            "Dimitra Simeonidou"
        ],
        "abstract": "Network complexity is increasing, making network control and orchestration a challenging task. The proliferation of network information and tools for data analytics can provide an important insight into resource provisioning and optimisation. The network knowledge incorporated in software defined networking can facilitate the knowledge driven control, leveraging the network programmability. We present Seer: a flexible, highly configurable data analytics platform for network intelligence based on software defined networking and big data principles. Seer combines a computational engine with a distributed messaging system to provide a scalable, fault tolerant and real-time platform for knowledge extraction. Our first prototype uses Apache Spark for streaming analytics and open network operating system (ONOS) controller to program a network in real-time. The first application we developed aims to predict the mobility pattern of mobile devices inside a smart city environment.\n    ",
        "submission_date": "2016-10-04T00:00:00",
        "last_modified_date": "2016-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01238",
        "title": "Find Your Own Way: Weakly-Supervised Segmentation of Path Proposals for Urban Autonomy",
        "authors": [
            "Dan Barnes",
            "Will Maddern",
            "Ingmar Posner"
        ],
        "abstract": "We present a weakly-supervised approach to segmenting proposed drivable paths in images with the goal of autonomous driving in complex urban environments. Using recorded routes from a data collection vehicle, our proposed method generates vast quantities of labelled images containing proposed paths and obstacles without requiring manual annotation, which we then use to train a deep semantic segmentation network. With the trained network we can segment proposed paths and obstacles at run-time using a vehicle equipped with only a monocular camera without relying on explicit modelling of road or lane markings. We evaluate our method on the large-scale KITTI and Oxford RobotCar datasets and demonstrate reliable path proposal and obstacle segmentation in a wide variety of environments under a range of lighting, weather and traffic conditions. We illustrate how the method can generalise to multiple path proposals at intersections and outline plans to incorporate the system into a framework for autonomous urban driving.\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2017-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01283",
        "title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles",
        "authors": [
            "Aravind Rajeswaran",
            "Sarvjeet Ghotra",
            "Balaraman Ravindran",
            "Sergey Levine"
        ],
        "abstract": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning/adaptation.\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2017-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01374",
        "title": "Domain Adaptation with Soft-margin multiple feature-kernel learning beats Deep Learning for surveillance face recognition",
        "authors": [
            "Samik Banerjee",
            "Sukhendu Das"
        ],
        "abstract": "Face recognition (FR) is the most preferred mode for biometric-based surveillance, due to its passive nature of detecting subjects, amongst all different types of biometric traits. FR under surveillance scenario does not give satisfactory performance due to low contrast, noise and poor illumination conditions on probes, as compared to the training samples. A state-of-the-art technology, Deep Learning, even fails to perform well in these scenarios. We propose a novel soft-margin based learning method for multiple feature-kernel combinations, followed by feature transformed using Domain Adaptation, which outperforms many recent state-of-the-art techniques, when tested using three real-world surveillance face datasets.\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2016-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01407",
        "title": "Towards semi-episodic learning for robot damage recovery",
        "authors": [
            "Konstantinos Chatzilygeroudis",
            "Antoine Cully",
            "Jean-Baptiste Mouret"
        ],
        "abstract": "The recently introduced Intelligent Trial and Error algorithm (IT\\&E) enables robots to creatively adapt to damage in a matter of minutes by combining an off-line evolutionary algorithm and an on-line learning algorithm based on Bayesian Optimization. We extend the IT\\&E algorithm to allow for robots to learn to compensate for damages while executing their task(s). This leads to a semi-episodic learning scheme that increases the robot's lifetime autonomy and adaptivity. Preliminary experiments on a toy simulation and a 6-legged robot locomotion task show promising results.\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01465",
        "title": "Visual Question Answering: Datasets, Algorithms, and Future Challenges",
        "authors": [
            "Kushal Kafle",
            "Christopher Kanan"
        ],
        "abstract": "Visual Question Answering (VQA) is a recent problem in computer vision and natural language processing that has garnered a large amount of interest from the deep learning, computer vision, and natural language processing communities. In VQA, an algorithm needs to answer text-based questions about images. Since the release of the first VQA dataset in 2014, additional datasets have been released and many algorithms have been proposed. In this review, we critically examine the current state of VQA in terms of problem formulation, existing datasets, evaluation metrics, and algorithms. In particular, we discuss the limitations of current datasets with regard to their ability to properly train and assess VQA algorithms. We then exhaustively review existing algorithms for VQA. Finally, we discuss possible future directions for VQA and image understanding research.\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2017-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01549",
        "title": "A Novel Representation of Neural Networks",
        "authors": [
            "Anthony Caterini",
            "Dong Eui Chang"
        ],
        "abstract": "Deep Neural Networks (DNNs) have become very popular for prediction in many areas. Their strength is in representation with a high number of parameters that are commonly learned via gradient descent or similar optimization methods. However, the representation is non-standardized, and the gradient calculation methods are often performed using component-based approaches that break parameters down into scalar units, instead of considering the parameters as whole entities. In this work, these problems are addressed. Standard notation is used to represent DNNs in a compact framework. Gradients of DNN loss functions are calculated directly over the inner product space on which the parameters are defined. This framework is general and is applied to two common network types: the Multilayer Perceptron and the Deep Autoencoder.\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2016-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01578",
        "title": "A new algorithm for identity verification based on the analysis of a handwritten dynamic signature",
        "authors": [
            "Krzysztof Cpalka",
            "Marcin Zalasinski",
            "Leszek Rutkowski"
        ],
        "abstract": "Identity verification based on authenticity assessment of a handwritten signature is an important issue in biometrics. There are many effective methods for signature verification taking into account dynamics of a signing process. Methods based on partitioning take a very important place among them. In this paper we propose a new approach to signature partitioning. Its most important feature is the possibility of selecting and processing of hybrid partitions in order to increase a precision of the test signature analysis. Partitions are formed by a combination of vertical and horizontal sections of the signature. Vertical sections correspond to the initial, middle, and final time moments of the signing process. In turn, horizontal sections correspond to the signature areas associated with high and low pen velocity and high and low pen pressure on the surface of a graphics tablet. Our previous research on vertical and horizontal sections of the dynamic signature (created independently) led us to develop the algorithm presented in this paper. Selection of sections, among others, allows us to define the stability of the signing process in the partitions, promoting signature areas of greater stability (and vice versa). In the test of the proposed method two databases were used: public MCYT-100 and paid BioSecure.\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01698",
        "title": "Human Decision-Making under Limited Time",
        "authors": [
            "Pedro A. Ortega",
            "Alan A. Stocker"
        ],
        "abstract": "Subjective expected utility theory assumes that decision-makers possess unlimited computational resources to reason about their choices; however, virtually all decisions in everyday life are made under resource constraints - i.e. decision-makers are bounded in their rationality. Here we experimentally tested the predictions made by a formalization of bounded rationality based on ideas from statistical mechanics and information-theory. We systematically tested human subjects in their ability to solve combinatorial puzzles under different time limitations. We found that our bounded-rational model accounts well for the data. The decomposition of the fitted model parameter into the subjects' expected utility function and resource parameter provide interesting insight into the subjects' information capacity limits. Our results confirm that humans gradually fall back on their learned prior choice patterns when confronted with increasing resource limitations.\n    ",
        "submission_date": "2016-10-06T00:00:00",
        "last_modified_date": "2016-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01733",
        "title": "Towards Cognitive Exploration through Deep Reinforcement Learning for Mobile Robots",
        "authors": [
            "Lei Tai",
            "Ming Liu"
        ],
        "abstract": "Exploration in an unknown environment is the core functionality for mobile robots. Learning-based exploration methods, including convolutional neural networks, provide excellent strategies without human-designed logic for the feature extraction. But the conventional supervised learning algorithms cost lots of efforts on the labeling work of datasets inevitably. Scenes not included in the training set are mostly unrecognized either. We propose a deep reinforcement learning method for the exploration of mobile robots in an indoor environment with the depth information from an RGB-D sensor only. Based on the Deep Q-Network framework, the raw depth image is taken as the only input to estimate the Q values corresponding to all moving commands. The training of the network weights is end-to-end. In arbitrarily constructed simulation environments, we show that the robot can be quickly adapted to unfamiliar scenes without any man-made labeling. Besides, through analysis of receptive fields of feature representations, deep reinforcement learning motivates the convolutional networks to estimate the traversability of the scenes. The test results are compared with the exploration strategies separately based on deep learning or reinforcement learning. Even trained only in the simulated environment, experimental results in real-world environment demonstrate that the cognitive ability of robot controller is dramatically improved compared with the supervised method. We believe it is the first time that raw sensor information is used to build cognitive exploration strategy for mobile robots through end-to-end deep reinforcement learning.\n    ",
        "submission_date": "2016-10-06T00:00:00",
        "last_modified_date": "2016-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01807",
        "title": "Parallel Large-Scale Attribute Reduction on Cloud Systems",
        "authors": [
            "Junbo Zhang",
            "Tianrui Li",
            "Yi Pan"
        ],
        "abstract": "The rapid growth of emerging information technologies and application patterns in modern society, e.g., Internet, Internet of Things, Cloud Computing and Tri-network Convergence, has caused the advent of the era of big data. Big data contains huge values, however, mining knowledge from big data is a tremendously challenging task because of data uncertainty and inconsistency. Attribute reduction (also known as feature selection) can not only be used as an effective preprocessing step, but also exploits the data redundancy to reduce the uncertainty. However, existing solutions are designed 1) either for a single machine that means the entire data must fit in the main memory and the parallelism is limited; 2) or for the Hadoop platform which means that the data have to be loaded into the distributed memory frequently and therefore become inefficient. In this paper, we overcome these shortcomings for maximum efficiency possible, and propose a unified framework for Parallel Large-scale Attribute Reduction, termed PLAR, for big data analysis. PLAR consists of three components: 1) Granular Computing (GrC)-based initialization: it converts a decision table (i.e., original data representation) into a granularity representation which reduces the amount of space and hence can be easily cached in the distributed memory: 2) model-parallelism: it simultaneously evaluates all feature candidates and makes attribute reduction highly parallelizable; 3) data-parallelism: it computes the significance of an attribute in parallel using a MapReduce-style manner. We implement PLAR with four representative heuristic feature selection algorithms on Spark, and evaluate them on various huge datasets, including UCI and astronomical datasets, finding our method's advantages beyond existing solutions.\n    ",
        "submission_date": "2016-10-06T00:00:00",
        "last_modified_date": "2016-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01891",
        "title": "A New Data Representation Based on Training Data Characteristics to Extract Drug Named-Entity in Medical Text",
        "authors": [
            "Sadikin Mujiono",
            "Mohamad Ivan Fanany",
            "Chan Basaruddin"
        ],
        "abstract": "One essential task in information extraction from the medical corpus is drug name recognition. Compared with text sources come from other domains, the medical text is special and has unique characteristics. In addition, the medical text mining poses more challenges, e.g., more unstructured text, the fast growing of new terms addition, a wide range of name variation for the same drug. The mining is even more challenging due to the lack of labeled dataset sources and external knowledge, as well as multiple token representations for a single drug name that is more common in the real application setting. Although many approaches have been proposed to overwhelm the task, some problems remained with poor F-score performance (less than 0.75). This paper presents a new treatment in data representation techniques to overcome some of those challenges. We propose three data representation techniques based on the characteristics of word distribution and word similarities as a result of word embedding training. The first technique is evaluated with the standard NN model, i.e., MLP (Multi-Layer Perceptrons). The second technique involves two deep network classifiers, i.e., DBN (Deep Belief Networks), and SAE (Stacked Denoising Encoders). The third technique represents the sentence as a sequence that is evaluated with a recurrent NN model, i.e., LSTM (Long Short Term Memory). In extracting the drug name entities, the third technique gives the best F-score performance compared to the state of the art, with its average F-score being 0.8645.\n    ",
        "submission_date": "2016-10-06T00:00:00",
        "last_modified_date": "2016-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01925",
        "title": "Metaheuristic Algorithms for Convolution Neural Network",
        "authors": [
            "L. M. Rasdi Rere",
            "Mohamad Ivan Fanany",
            "Aniati Murni Arymurthy"
        ],
        "abstract": "A typical modern optimization technique is usually either heuristic or metaheuristic. This technique has managed to solve some optimization problems in the research area of science, engineering, and industry. However, implementation strategy of metaheuristic for accuracy improvement on convolution neural networks (CNN), a famous deep learning method, is still rarely investigated. Deep learning relates to a type of machine learning technique, where its aim is to move closer to the goal of artificial intelligence of creating a machine that could successfully perform any intellectual tasks that can be carried out by a human. In this paper, we propose the implementation strategy of three popular metaheuristic approaches, that is, simulated annealing, differential evolution, and harmony search, to optimize CNN. The performances of these metaheuristic methods in optimizing CNN on classifying MNIST and CIFAR dataset were evaluated and compared. Furthermore, the proposed methods are also compared with the original CNN. Although the proposed methods show an increase in the computation time, their accuracy has also been improved (up to 7.14 percent).\n    ",
        "submission_date": "2016-10-06T00:00:00",
        "last_modified_date": "2016-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01969",
        "title": "DeepDGA: Adversarially-Tuned Domain Generation and Detection",
        "authors": [
            "Hyrum S. Anderson",
            "Jonathan Woodbridge",
            "Bobby Filar"
        ],
        "abstract": "Many malware families utilize domain generation algorithms (DGAs) to establish command and control (C&C) connections. While there are many methods to pseudorandomly generate domains, we focus in this paper on detecting (and generating) domains on a per-domain basis which provides a simple and flexible means to detect known DGA families. Recent machine learning approaches to DGA detection have been successful on fairly simplistic DGAs, many of which produce names of fixed length. However, models trained on limited datasets are somewhat blind to new DGA variants.\n",
        "submission_date": "2016-10-06T00:00:00",
        "last_modified_date": "2016-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02055",
        "title": "Places: An Image Database for Deep Scene Understanding",
        "authors": [
            "Bolei Zhou",
            "Aditya Khosla",
            "Agata Lapedriza",
            "Antonio Torralba",
            "Aude Oliva"
        ],
        "abstract": "The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification at tasks such as object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories and attributes, comprising a quasi-exhaustive list of the types of environments encountered in the world. Using state of the art Convolutional Neural Networks, we provide impressive baseline performances at scene classification. With its high-coverage and high-diversity of exemplars, the Places Database offers an ecosystem to guide future progress on currently intractable visual recognition problems.\n    ",
        "submission_date": "2016-10-06T00:00:00",
        "last_modified_date": "2016-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02164",
        "title": "Deep Reinforcement Learning From Raw Pixels in Doom",
        "authors": [
            "Danijar Hafner"
        ],
        "abstract": "Using current reinforcement learning methods, it has recently become possible to learn to play unknown 3D games from raw pixels. In this work, we study the challenges that arise in such complex environments, and summarize current methods to approach these. We choose a task within the Doom game, that has not been approached yet. The goal for the agent is to fight enemies in a 3D world consisting of five rooms. We train the DQN and LSTM-A3C algorithms on this task. Results show that both algorithms learn sensible policies, but fail to achieve high scores given the amount of training. We provide insights into the learned behavior, which can serve as a valuable starting point for further research in the Doom domain.\n    ",
        "submission_date": "2016-10-07T00:00:00",
        "last_modified_date": "2016-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02333",
        "title": "Application of Ontologies in Cloud Computing: The State-Of-The-Art",
        "authors": [
            "Fahim T. Imam"
        ],
        "abstract": "This paper presents a systematic survey on existing literature and seminal works relevant to the application of ontologies in different aspects of Cloud computing. Our hypothesis is that ontologies along with their reasoning capabilities can have significant impact on improving various aspects of the Cloud computing phenomena. Ontologies can promote intelligent decision support mechanisms for various Cloud based services. They can also provide effective interoperability among the Cloud based systems and resources. This survey can promote a comprehensive understanding on the roles and significance of ontologies within the overall domain of Cloud Computing. Also, this project can potentially form the basis of new research area and possibilities for both ontology and Cloud computing communities.\n    ",
        "submission_date": "2016-10-06T00:00:00",
        "last_modified_date": "2016-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02391",
        "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
        "authors": [
            "Ramprasaath R. Selvaraju",
            "Michael Cogswell",
            "Abhishek Das",
            "Ramakrishna Vedantam",
            "Devi Parikh",
            "Dhruv Batra"
        ],
        "abstract": "We propose a technique for producing \"visual explanations\" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at ",
        "submission_date": "2016-10-07T00:00:00",
        "last_modified_date": "2019-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02634",
        "title": "On Deductive Systems of AC Semantics for Rough Sets",
        "authors": [
            "A. Mani"
        ],
        "abstract": "Antichain based semantics for general rough sets were introduced recently by the present author. In her paper two different semantics, one for general rough sets and another for general approximation spaces over quasi-equivalence relations, were developed. These semantics are improved and studied further from a lateral algebraic logic perspective in this research. The main results concern the structure of the algebras and deductive systems in the context.\n    ",
        "submission_date": "2016-10-09T00:00:00",
        "last_modified_date": "2016-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02649",
        "title": "A new selection strategy for selective cluster ensemble based on Diversity and Independency",
        "authors": [
            "Muhammad Yousefnezhad",
            "Ali Reihanian",
            "Daoqiang Zhang",
            "Behrouz Minaei-Bidgoli"
        ],
        "abstract": "This research introduces a new strategy in cluster ensemble selection by using Independency and Diversity metrics. In recent years, Diversity and Quality, which are two metrics in evaluation procedure, have been used for selecting basic clustering results in the cluster ensemble selection. Although quality can improve the final results in cluster ensemble, it cannot control the procedures of generating basic results, which causes a gap in prediction of the generated basic results' accuracy. Instead of quality, this paper introduces Independency as a supplementary method to be used in conjunction with Diversity. Therefore, this paper uses a heuristic metric, which is based on the procedure of converting code to graph in Software Testing, in order to calculate the Independency of two basic clustering algorithms. Moreover, a new modeling language, which we called as \"Clustering Algorithms Independency Language\" (CAIL), is introduced in order to generate graphs which depict Independency of algorithms. Also, Uniformity, which is a new similarity metric, has been introduced for evaluating the diversity of basic results. As a credential, our experimental results on varied different standard data sets show that the proposed framework improves the accuracy of final results dramatically in comparison with other cluster ensemble methods.\n    ",
        "submission_date": "2016-10-09T00:00:00",
        "last_modified_date": "2016-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02683",
        "title": "Interpreting Neural Networks to Improve Politeness Comprehension",
        "authors": [
            "Malika Aubakirova",
            "Mohit Bansal"
        ],
        "abstract": "We present an interpretable neural network approach to predicting and understanding politeness in natural language requests. Our models are based on simple convolutional neural networks directly on raw text, avoiding any manual identification of complex sentiment or syntactic features, while performing better than such feature-based models from previous work. More importantly, we use the challenging task of politeness prediction as a testbed to next present a much-needed understanding of what these successful networks are actually learning. For this, we present several network visualizations based on activation clusters, first derivative saliency, and embedding space transformations, helping us automatically identify several subtle linguistics markers of politeness theories. Further, this analysis reveals multiple novel, high-scoring politeness strategies which, when added back as new features, reduce the accuracy gap between the original featurized system and the neural model, thus providing a clear quantitative interpretation of the success of these neural networks.\n    ",
        "submission_date": "2016-10-09T00:00:00",
        "last_modified_date": "2016-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02751",
        "title": "A New Theoretical and Technological System of Imprecise-Information Processing",
        "authors": [
            "Shiyou Lian"
        ],
        "abstract": "Imprecise-information processing will play an indispensable role in intelligent systems, especially in the anthropomorphic intelligent systems (as intelligent robots). A new theoretical and technological system of imprecise-information processing has been founded in Principles of Imprecise-Information Processing: A New Theoretical and Technological System[1] which is different from fuzzy technology. The system has clear hierarchy and rigorous structure, which results from the formation principle of imprecise information and has solid mathematical and logical bases, and which has many advantages beyond fuzzy technology. The system provides a technological platform for relevant applications and lays a theoretical foundation for further research.\n    ",
        "submission_date": "2016-10-10T00:00:00",
        "last_modified_date": "2016-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02876",
        "title": "Heuristic Approaches for Generating Local Process Models through Log Projections",
        "authors": [
            "Niek Tax",
            "Natalia Sidorova",
            "Wil M. P. van der Aalst",
            "Reinder Haakma"
        ],
        "abstract": "Local Process Model (LPM) discovery is focused on the mining of a set of process models where each model describes the behavior represented in the event log only partially, i.e. subsets of possible events are taken into account to create so-called local process models. Often such smaller models provide valuable insights into the behavior of the process, especially when no adequate and comprehensible single overall process model exists that is able to describe the traces of the process from start to end. The practical application of LPM discovery is however hindered by computational issues in the case of logs with many activities (problems may already occur when there are more than 17 unique activities). In this paper, we explore three heuristics to discover subsets of activities that lead to useful log projections with the goal of speeding up LPM discovery considerably while still finding high-quality LPMs. We found that a Markov clustering approach to create projection sets results in the largest improvement of execution time, with discovered LPMs still being better than with the use of randomly generated activity sets of the same size. Another heuristic, based on log entropy, yields a more moderate speedup, but enables the discovery of higher quality LPMs. The third heuristic, based on the relative information gain, shows unstable performance: for some data sets the speedup and LPM quality are higher than with the log entropy based method, while for other data sets there is no speedup at all.\n    ",
        "submission_date": "2016-10-10T00:00:00",
        "last_modified_date": "2016-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02922",
        "title": "Towards an Ontology-Driven Blockchain Design for Supply Chain Provenance",
        "authors": [
            "Henry M. Kim",
            "Marek Laskowski"
        ],
        "abstract": "An interesting research problem in our age of Big Data is that of determining provenance. Granular evaluation of provenance of physical goods--e.g. tracking ingredients of a pharmaceutical or demonstrating authenticity of luxury goods--has often not been possible with today's items that are produced and transported in complex, inter-organizational, often internationally-spanning supply chains. Recent adoption of Internet of Things and Blockchain technologies give promise at better supply chain provenance. We are particularly interested in the blockchain as many favoured use cases of blockchain are for provenance tracking. We are also interested in applying ontologies as there has been some work done on knowledge provenance, traceability, and food provenance using ontologies. In this paper, we make a case for why ontologies can contribute to blockchain design. To support this case, we analyze a traceability ontology and translate some of its representations to smart contracts that execute a provenance trace and enforce traceability constraints on the Ethereum blockchain platform.\n    ",
        "submission_date": "2016-08-28T00:00:00",
        "last_modified_date": "2016-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02995",
        "title": "Extrapolation and learning equations",
        "authors": [
            "Georg Martius",
            "Christoph H. Lampert"
        ],
        "abstract": "In classical machine learning, regression is treated as a black box process of identifying a suitable function from a hypothesis set without attempting to gain insight into the mechanism connecting inputs and outputs. In the natural sciences, however, finding an interpretable function for a phenomenon is the prime goal as it allows to understand and generalize results. This paper proposes a novel type of function learning network, called equation learner (EQL), that can learn analytical expressions and is able to extrapolate to unseen domains. It is implemented as an end-to-end differentiable feed-forward network and allows for efficient gradient based training. Due to sparsity regularization concise interpretable expressions can be obtained. Often the true underlying source expression is identified.\n    ",
        "submission_date": "2016-10-10T00:00:00",
        "last_modified_date": "2016-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03164",
        "title": "Navigational Instruction Generation as Inverse Reinforcement Learning with Neural Machine Translation",
        "authors": [
            "Andrea F. Daniele",
            "Mohit Bansal",
            "Matthew R. Walter"
        ],
        "abstract": "Modern robotics applications that involve human-robot interaction require robots to be able to communicate with humans seamlessly and effectively. Natural language provides a flexible and efficient medium through which robots can exchange information with their human partners. Significant advancements have been made in developing robots capable of interpreting free-form instructions, but less attention has been devoted to endowing robots with the ability to generate natural language. We propose a navigational guide model that enables robots to generate natural language instructions that allow humans to navigate a priori unknown environments. We first decide which information to share with the user according to their preferences, using a policy trained from human demonstrations via inverse reinforcement learning. We then \"translate\" this information into a natural language instruction using a neural sequence-to-sequence model that learns to generate free-form instructions from natural language corpora. We evaluate our method on a benchmark route instruction dataset and achieve a BLEU score of 72.18% when compared to human-generated reference instructions. We additionally conduct navigation experiments with human participants that demonstrate that our method generates instructions that people follow as accurately and easily as those produced by humans.\n    ",
        "submission_date": "2016-10-11T00:00:00",
        "last_modified_date": "2016-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03518",
        "title": "Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model",
        "authors": [
            "Paul Christiano",
            "Zain Shah",
            "Igor Mordatch",
            "Jonas Schneider",
            "Trevor Blackwell",
            "Joshua Tobin",
            "Pieter Abbeel",
            "Wojciech Zaremba"
        ],
        "abstract": "Developing control policies in simulation is often more practical and safer than directly running experiments in the real world. This applies to policies obtained from planning and optimization, and even more so to policies obtained from reinforcement learning, which is often very data demanding. However, a policy that succeeds in simulation often doesn't work when deployed on a real robot. Nevertheless, often the overall gist of what the policy does in simulation remains valid in the real world. In this paper we investigate such settings, where the sequence of states traversed in simulation remains reasonable for the real world, even if the details of the controls are not, as could be the case when the key differences lie in detailed friction, contact, mass and geometry properties. During execution, at each time step our approach computes what the simulation-based control policy would do, but then, rather than executing these controls on the real robot, our approach computes what the simulation expects the resulting next state(s) will be, and then relies on a learned deep inverse dynamics model to decide which real-world action is most suitable to achieve those next states. Deep models are only as good as their training data, and we also propose an approach for data collection to (incrementally) learn the deep inverse dynamics model. Our experiments shows our approach compares favorably with various baselines that have been developed for dealing with simulation to real world model discrepancy, including output error control and Gaussian dynamics adaptation.\n    ",
        "submission_date": "2016-10-11T00:00:00",
        "last_modified_date": "2016-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03677",
        "title": "Deep Fruit Detection in Orchards",
        "authors": [
            "Suchet Bargoti",
            "James Underwood"
        ],
        "abstract": "An accurate and reliable image based fruit detection system is critical for supporting higher level agriculture tasks such as yield mapping and robotic harvesting. This paper presents the use of a state-of-the-art object detection framework, Faster R-CNN, in the context of fruit detection in orchards, including mangoes, almonds and apples. Ablation studies are presented to better understand the practical deployment of the detection network, including how much training data is required to capture variability in the dataset. Data augmentation techniques are shown to yield significant performance gains, resulting in a greater than two-fold reduction in the number of training images required. In contrast, transferring knowledge between orchards contributed to negligible performance gain over initialising the Deep Convolutional Neural Network directly from ImageNet features. Finally, to operate over orchard data containing between 100-1000 fruit per image, a tiling approach is introduced for the Faster R-CNN framework. The study has resulted in the best yet detection performance for these orchards relative to previous works, with an F1-score of >0.9 achieved for apples and mangoes.\n    ",
        "submission_date": "2016-10-12T00:00:00",
        "last_modified_date": "2017-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03761",
        "title": "Detecting Unseen Falls from Wearable Devices using Channel-wise Ensemble of Autoencoders",
        "authors": [
            "Shehroz S. Khan",
            "Babak Taati"
        ],
        "abstract": "A fall is an abnormal activity that occurs rarely, so it is hard to collect real data for falls. It is, therefore, difficult to use supervised learning methods to automatically detect falls. Another challenge in using machine learning methods to automatically detect falls is the choice of engineered features. In this paper, we propose to use an ensemble of autoencoders to extract features from different channels of wearable sensor data trained only on normal activities. We show that the traditional approach of choosing a threshold as the maximum of the reconstruction error on the training normal data is not the right way to identify unseen falls. We propose two methods for automatic tightening of reconstruction error from only the normal activities for better identification of unseen falls. We present our results on two activity recognition datasets and show the efficacy of our proposed method against traditional autoencoder models and two standard one-class classification methods.\n    ",
        "submission_date": "2016-10-12T00:00:00",
        "last_modified_date": "2017-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03957",
        "title": "A Fuzzy Logic System to Analyze a Student's Lifestyle",
        "authors": [
            "Sourish Ghosh",
            "Aaditya Sanjay Boob",
            "Nishant Nikhil",
            "Nayan Raju Vysyaraju",
            "Ankit Kumar"
        ],
        "abstract": "A college student's life can be primarily categorized into domains such as education, health, social and other activities which may include daily chores and travelling time. Time management is crucial for every student. A self realisation of one's daily time expenditure in various domains is therefore essential to maximize one's effective output. This paper presents how a mobile application using Fuzzy Logic and Global Positioning System (GPS) analyzes a student's lifestyle and provides recommendations and suggestions based on the results.\n    ",
        "submission_date": "2016-10-13T00:00:00",
        "last_modified_date": "2016-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03996",
        "title": "Bank Card Usage Prediction Exploiting Geolocation Information",
        "authors": [
            "Martin Wistuba",
            "Nghia Duong-Trung",
            "Nicolas Schilling",
            "Lars Schmidt-Thieme"
        ],
        "abstract": "We describe the solution of team ISMLL for the ECML-PKDD 2016 Discovery Challenge on Bank Card Usage for both tasks. Our solution is based on three pillars. Gradient boosted decision trees as a strong regression and classification model, an intensive search for good hyperparameter configurations and strong features that exploit geolocation information. This approach achieved the best performance on the public leaderboard for the first task and a decent fourth position for the second task.\n    ",
        "submission_date": "2016-10-13T00:00:00",
        "last_modified_date": "2016-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04069",
        "title": "Truthful Mechanisms for Matching and Clustering in an Ordinal World",
        "authors": [
            "Elliot Anshelevich",
            "Shreyas Sekar"
        ],
        "abstract": "We study truthful mechanisms for matching and related problems in a partial information setting, where the agents' true utilities are hidden, and the algorithm only has access to ordinal preference information. Our model is motivated by the fact that in many settings, agents cannot express the numerical values of their utility for different outcomes, but are still able to rank the outcomes in their order of preference. Specifically, we study problems where the ground truth exists in the form of a weighted graph of agent utilities, but the algorithm can only elicit the agents' private information in the form of a preference ordering for each agent induced by the underlying weights. Against this backdrop, we design truthful algorithms to approximate the true optimum solution with respect to the hidden weights. Our techniques yield universally truthful algorithms for a number of graph problems: a 1.76-approximation algorithm for Max-Weight Matching, 2-approximation algorithm for Max k-matching, a 6-approximation algorithm for Densest k-subgraph, and a 2-approximation algorithm for Max Traveling Salesman as long as the hidden weights constitute a metric. We also provide improved approximation algorithms for such problems when the agents are not able to lie about their preferences. Our results are the first non-trivial truthful approximation algorithms for these problems, and indicate that in many situations, we can design robust algorithms even when the agents may lie and only provide ordinal information instead of precise utilities.\n    ",
        "submission_date": "2016-10-13T00:00:00",
        "last_modified_date": "2016-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04213",
        "title": "Reset-free Trial-and-Error Learning for Robot Damage Recovery",
        "authors": [
            "Konstantinos Chatzilygeroudis",
            "Vassilis Vassiliades",
            "Jean-Baptiste Mouret"
        ],
        "abstract": "The high probability of hardware failures prevents many advanced robots (e.g., legged robots) from being confidently deployed in real-world situations (e.g., post-disaster rescue). Instead of attempting to diagnose the failures, robots could adapt by trial-and-error in order to be able to complete their tasks. In this situation, damage recovery can be seen as a Reinforcement Learning (RL) problem. However, the best RL algorithms for robotics require the robot and the environment to be reset to an initial state after each episode, that is, the robot is not learning autonomously. In addition, most of the RL methods for robotics do not scale well with complex robots (e.g., walking robots) and either cannot be used at all or take too long to converge to a solution (e.g., hours of learning). In this paper, we introduce a novel learning algorithm called \"Reset-free Trial-and-Error\" (RTE) that (1) breaks the complexity by pre-generating hundreds of possible behaviors with a dynamics simulator of the intact robot, and (2) allows complex robots to quickly recover from damage while completing their tasks and taking the environment into account. We evaluate our algorithm on a simulated wheeled robot, a simulated six-legged robot, and a real six-legged walking robot that are damaged in several ways (e.g., a missing leg, a shortened leg, faulty motor, etc.) and whose objective is to reach a sequence of targets in an arena. Our experiments show that the robots can recover most of their locomotion abilities in an environment with obstacles, and without any human intervention.\n    ",
        "submission_date": "2016-10-13T00:00:00",
        "last_modified_date": "2017-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04325",
        "title": "Hadamard Product for Low-rank Bilinear Pooling",
        "authors": [
            "Jin-Hwa Kim",
            "Kyoung-Woon On",
            "Woosang Lim",
            "Jeonghee Kim",
            "Jung-Woo Ha",
            "Byoung-Tak Zhang"
        ],
        "abstract": "Bilinear models provide rich representations compared with linear models. They have been applied in various visual tasks, such as object recognition, segmentation, and visual question-answering, to get state-of-the-art performances taking advantage of the expanded representations. However, bilinear representations tend to be high-dimensional, limiting the applicability to computationally complex tasks. We propose low-rank bilinear pooling using Hadamard product for an efficient attention mechanism of multimodal learning. We show that our model outperforms compact bilinear pooling in visual question-answering tasks with the state-of-the-art results on the VQA dataset, having a better parsimonious property.\n    ",
        "submission_date": "2016-10-14T00:00:00",
        "last_modified_date": "2017-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04416",
        "title": "Distributional Inclusion Hypothesis for Tensor-based Composition",
        "authors": [
            "Dimitri Kartsaklis",
            "Mehrnoosh Sadrzadeh"
        ],
        "abstract": "According to the distributional inclusion hypothesis, entailment between words can be measured via the feature inclusions of their distributional vectors. In recent work, we showed how this hypothesis can be extended from words to phrases and sentences in the setting of compositional distributional semantics. This paper focuses on inclusion properties of tensors; its main contribution is a theoretical and experimental analysis of how feature inclusion works in different concrete models of verb tensors. We present results for relational, Frobenius, projective, and holistic methods and compare them to the simple vector addition, multiplication, min, and max models. The degrees of entailment thus obtained are evaluated via a variety of existing word-based measures, such as Weed's and Clarke's, KL-divergence, APinc, balAPinc, and two of our previously proposed metrics at the phrase/sentence level. We perform experiments on three entailment datasets, investigating which version of tensor-based composition achieves the highest performance when combined with the sentence-level measures.\n    ",
        "submission_date": "2016-10-14T00:00:00",
        "last_modified_date": "2016-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04494",
        "title": "Localization for Wireless Sensor Networks: A Neural Network Approach",
        "authors": [
            "Shiu Kumar",
            "Ronesh Sharma",
            "Edwin Vans"
        ],
        "abstract": "As Wireless Sensor Networks are penetrating into the industrial domain, many research opportunities are emerging. One such essential and challenging application is that of node localization. A feed-forward neural network based methodology is adopted in this paper. The Received Signal Strength Indicator (RSSI) values of the anchor node beacons are used. The number of anchor nodes and their configurations has an impact on the accuracy of the localization system, which is also addressed in this paper. Five different training algorithms are evaluated to find the training algorithm that gives the best result. The multi-layer Perceptron (MLP) neural network model was trained using Matlab. In order to evaluate the performance of the proposed method in real time, the model obtained was then implemented on the Arduino microcontroller. With four anchor nodes, an average 2D localization error of 0.2953 m has been achieved with a 12-12-2 neural network structure. The proposed method can also be implemented on any other embedded microcontroller system.\n    ",
        "submission_date": "2016-02-07T00:00:00",
        "last_modified_date": "2016-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04574",
        "title": "Generalization Error of Invariant Classifiers",
        "authors": [
            "Jure Sokolic",
            "Raja Giryes",
            "Guillermo Sapiro",
            "Miguel R. D. Rodrigues"
        ],
        "abstract": "This paper studies the generalization error of invariant classifiers. In particular, we consider the common scenario where the classification task is invariant to certain transformations of the input, and that the classifier is constructed (or learned) to be invariant to these transformations. Our approach relies on factoring the input space into a product of a base space and a set of transformations. We show that whereas the generalization error of a non-invariant classifier is proportional to the complexity of the input space, the generalization error of an invariant classifier is proportional to the complexity of the base space. We also derive a set of sufficient conditions on the geometry of the base space and the set of transformations that ensure that the complexity of the base space is much smaller than the complexity of the input space. Our analysis applies to general classifiers such as convolutional neural networks. We demonstrate the implications of the developed theory for such classifiers with experiments on the MNIST and CIFAR-10 datasets.\n    ",
        "submission_date": "2016-10-14T00:00:00",
        "last_modified_date": "2017-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04631",
        "title": "A Harmonic Mean Linear Discriminant Analysis for Robust Image Classification",
        "authors": [
            "Shuai Zheng",
            "Feiping Nie",
            "Chris Ding",
            "Heng Huang"
        ],
        "abstract": "Linear Discriminant Analysis (LDA) is a widely-used supervised dimensionality reduction method in computer vision and pattern recognition. In null space based LDA (NLDA), a well-known LDA extension, between-class distance is maximized in the null space of the within-class scatter matrix. However, there are some limitations in NLDA. Firstly, for many data sets, null space of within-class scatter matrix does not exist, thus NLDA is not applicable to those datasets. Secondly, NLDA uses arithmetic mean of between-class distances and gives equal consideration to all between-class distances, which makes larger between-class distances can dominate the result and thus limits the performance of NLDA. In this paper, we propose a harmonic mean based Linear Discriminant Analysis, Multi-Class Discriminant Analysis (MCDA), for image classification, which minimizes the reciprocal of weighted harmonic mean of pairwise between-class distance. More importantly, MCDA gives higher priority to maximize small between-class distances. MCDA can be extended to multi-label dimension reduction. Results on 7 single-label data sets and 4 multi-label data sets show that MCDA has consistently better performance than 10 other single-label approaches and 4 other multi-label approaches in terms of classification accuracy, macro and micro average F1 score.\n    ",
        "submission_date": "2016-10-14T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04850",
        "title": "Efficient Rectangular Maximal-Volume Algorithm for Rating Elicitation in Collaborative Filtering",
        "authors": [
            "Alexander Fonarev",
            "Alexander Mikhalev",
            "Pavel Serdyukov",
            "Gleb Gusev",
            "Ivan Oseledets"
        ],
        "abstract": "Cold start problem in Collaborative Filtering can be solved by asking new users to rate a small seed set of representative items or by asking representative users to rate a new item. The question is how to build a seed set that can give enough preference information for making good recommendations. One of the most successful approaches, called Representative Based Matrix Factorization, is based on Maxvol algorithm. Unfortunately, this approach has one important limitation --- a seed set of a particular size requires a rating matrix factorization of fixed rank that should coincide with that size. This is not necessarily optimal in the general case. In the current paper, we introduce a fast algorithm for an analytical generalization of this approach that we call Rectangular Maxvol. It allows the rank of factorization to be lower than the required size of the seed set. Moreover, the paper includes the theoretical analysis of the method's error, the complexity analysis of the existing methods and the comparison to the state-of-the-art approaches.\n    ",
        "submission_date": "2016-10-16T00:00:00",
        "last_modified_date": "2016-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05009",
        "title": "Wind ramp event prediction with parallelized Gradient Boosted Regression Trees",
        "authors": [
            "Saurav Gupta",
            "Nitin Anand Shrivastava",
            "Abbas Khosravi",
            "Bijaya Ketan Panigrahi"
        ],
        "abstract": "Accurate prediction of wind ramp events is critical for ensuring the reliability and stability of the power systems with high penetration of wind energy. This paper proposes a classification based approach for estimating the future class of wind ramp event based on certain thresholds. A parallelized gradient boosted regression tree based technique has been proposed to accurately classify the normal as well as rare extreme wind power ramp events. The model has been validated using wind power data obtained from the National Renewable Energy Laboratory database. Performance comparison with several benchmark techniques indicates the superiority of the proposed technique in terms of superior classification accuracy.\n    ",
        "submission_date": "2016-10-17T00:00:00",
        "last_modified_date": "2016-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05016",
        "title": "Weekly maintenance scheduling using exact and genetic methods",
        "authors": [
            "Andrew W. Palmer",
            "Robin Vujanic",
            "Andrew J. Hill",
            "Steven J. Scheding"
        ],
        "abstract": "The weekly maintenance schedule specifies when maintenance activities should be performed on the equipment, taking into account the availability of workers and maintenance bays, and other operational constraints. The current approach to generating this schedule is labour intensive and requires coordination between the maintenance schedulers and operations staff to minimise its impact on the operation of the mine. This paper presents methods for automatically generating this schedule from the list of maintenance tasks to be performed, the availability roster of the maintenance staff, and time windows in which each piece of equipment is available for maintenance. Both Mixed-Integer Linear Programming (MILP) and genetic algorithms are evaluated, with the genetic algorithm shown to significantly outperform the MILP. Two fitness functions for the genetic algorithm are also examined, with a linear fitness function outperforming an inverse fitness function by up to 5% for the same calculation time. The genetic algorithm approach is computationally fast, allowing the schedule to be rapidly recalculated in response to unexpected delays and breakdowns.\n    ",
        "submission_date": "2016-10-17T00:00:00",
        "last_modified_date": "2016-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05182",
        "title": "Learning and Transfer of Modulated Locomotor Controllers",
        "authors": [
            "Nicolas Heess",
            "Greg Wayne",
            "Yuval Tassa",
            "Timothy Lillicrap",
            "Martin Riedmiller",
            "David Silver"
        ],
        "abstract": "We study a novel architecture and training procedure for locomotion tasks. A high-frequency, low-level \"spinal\" network with access to proprioceptive sensors learns sensorimotor primitives by training on simple tasks. This pre-trained module is fixed and connected to a low-frequency, high-level \"cortical\" network, with access to all sensors, which drives behavior by modulating the inputs to the spinal network. Where a monolithic end-to-end architecture fails completely, learning with a pre-trained spinal module succeeds at multiple high-level tasks, and enables the effective exploration required to learn from sparse rewards. We test our proposed architecture on three simulated bodies: a 16-dimensional swimming snake, a 20-dimensional quadruped, and a 54-dimensional humanoid. Our results are illustrated in the accompanying video at ",
        "submission_date": "2016-10-17T00:00:00",
        "last_modified_date": "2016-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05202",
        "title": "Decentralized Collaborative Learning of Personalized Models over Networks",
        "authors": [
            "Paul Vanhaesebrouck",
            "Aur\u00e9lien Bellet",
            "Marc Tommasi"
        ],
        "abstract": "We consider a set of learning agents in a collaborative peer-to-peer network, where each agent learns a personalized model according to its own learning objective. The question addressed in this paper is: how can agents improve upon their locally trained model by communicating with other agents that have similar objectives? We introduce and analyze two asynchronous gossip algorithms running in a fully decentralized manner. Our first approach, inspired from label propagation, aims to smooth pre-trained local models over the network while accounting for the confidence that each agent has in its initial model. In our second approach, agents jointly learn and propagate their model by making iterative updates based on both their local dataset and the behavior of their neighbors. To optimize this challenging objective, our decentralized algorithm is based on ADMM.\n    ",
        "submission_date": "2016-10-17T00:00:00",
        "last_modified_date": "2017-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05688",
        "title": "Low-rank and Sparse Soft Targets to Learn Better DNN Acoustic Models",
        "authors": [
            "Pranay Dighe",
            "Afsaneh Asaei",
            "Herve Bourlard"
        ],
        "abstract": "Conventional deep neural networks (DNN) for speech acoustic modeling rely on Gaussian mixture models (GMM) and hidden Markov model (HMM) to obtain binary class labels as the targets for DNN training. Subword classes in speech recognition systems correspond to context-dependent tied states or senones. The present work addresses some limitations of GMM-HMM senone alignments for DNN training. We hypothesize that the senone probabilities obtained from a DNN trained with binary labels can provide more accurate targets to learn better acoustic models. However, DNN outputs bear inaccuracies which are exhibited as high dimensional unstructured noise, whereas the informative components are structured and low-dimensional. We exploit principle component analysis (PCA) and sparse coding to characterize the senone subspaces. Enhanced probabilities obtained from low-rank and sparse reconstructions are used as soft-targets for DNN acoustic modeling, that also enables training with untranscribed data. Experiments conducted on AMI corpus shows 4.6% relative reduction in word error rate.\n    ",
        "submission_date": "2016-10-18T00:00:00",
        "last_modified_date": "2016-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05716",
        "title": "Design Mining Microbial Fuel Cell Cascades",
        "authors": [
            "Richard J. Preen",
            "Jiseon You",
            "Larry Bull",
            "Ioannis A. Ieropoulos"
        ],
        "abstract": "Microbial fuel cells (MFCs) perform wastewater treatment and electricity production through the conversion of organic matter using microorganisms. For practical applications, it has been suggested that greater efficiency can be achieved by arranging multiple MFC units into physical stacks in a cascade with feedstock flowing sequentially between units. In this paper, we investigate the use of computational intelligence to physically explore and optimise (potentially) heterogeneous MFC designs in a cascade, i.e. without simulation. Conductive structures are 3-D printed and inserted into the anodic chamber of each MFC unit, augmenting a carbon fibre veil anode and affecting the hydrodynamics, including the feedstock volume and hydraulic retention time, as well as providing unique habitats for microbial colonisation. We show that it is possible to use design mining to identify new conductive inserts that increase both the cascade power output and power density.\n    ",
        "submission_date": "2016-10-18T00:00:00",
        "last_modified_date": "2017-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05984",
        "title": "Particle Swarm Optimization for Generating Interpretable Fuzzy Reinforcement Learning Policies",
        "authors": [
            "Daniel Hein",
            "Alexander Hentschel",
            "Thomas Runkler",
            "Steffen Udluft"
        ],
        "abstract": "Fuzzy controllers are efficient and interpretable system controllers for continuous state and action spaces. To date, such controllers have been constructed manually or trained automatically either using expert-generated problem-specific cost functions or incorporating detailed knowledge about the optimal control strategy. Both requirements for automatic training processes are not found in most real-world reinforcement learning (RL) problems. In such applications, online learning is often prohibited for safety reasons because online learning requires exploration of the problem's dynamics during policy training. We introduce a fuzzy particle swarm reinforcement learning (FPSRL) approach that can construct fuzzy RL policies solely by training parameters on world models that simulate real system dynamics. These world models are created by employing an autonomous machine learning technique that uses previously generated transition samples of a real system. To the best of our knowledge, this approach is the first to relate self-organizing fuzzy controllers to model-based batch RL. Therefore, FPSRL is intended to solve problems in domains where online learning is prohibited, system dynamics are relatively easy to model from previously generated default policy transition samples, and it is expected that a relatively easily interpretable control policy exists. The efficiency of the proposed approach with problems from such domains is demonstrated using three standard RL benchmarks, i.e., mountain car, cart-pole balancing, and cart-pole swing-up. Our experimental results demonstrate high-performing, interpretable fuzzy policies.\n    ",
        "submission_date": "2016-10-19T00:00:00",
        "last_modified_date": "2017-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06067",
        "title": "Fairness as a Program Property",
        "authors": [
            "Aws Albarghouthi",
            "Loris D'Antoni",
            "Samuel Drews",
            "Aditya Nori"
        ],
        "abstract": "We explore the following question: Is a decision-making program fair, for some useful definition of fairness? First, we describe how several algorithmic fairness questions can be phrased as program verification problems. Second, we discuss an automated verification technique for proving or disproving fairness of decision-making programs with respect to a probabilistic model of the population.\n    ",
        "submission_date": "2016-10-19T00:00:00",
        "last_modified_date": "2016-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06340",
        "title": "Maximizing positive opinion influence using an evidential approach",
        "authors": [
            "Siwar Jendoubi",
            "Arnaud Martin",
            "Ludovic Li\u00e9tard",
            "Hend Hadji",
            "Boutheina Yaghlane"
        ],
        "abstract": "In this paper, we propose a new data based model for influence maximization in online social networks. We use the theory of belief functions to overcome the data imperfection problem. Besides, the proposed model searches to detect influencer users that adopt a positive opinion about the product, the idea, etc, to be propagated. Moreover, we present some experiments to show the performance of our model.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2016-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06347",
        "title": "A Classification Engine for Image Ballistics of Social Data",
        "authors": [
            "Oliver Giudice",
            "Antonino Paratore",
            "Marco Moltisanti",
            "Sebastiano Battiato"
        ],
        "abstract": "Image Forensics has already achieved great results for the source camera identification task on images. Standard approaches for data coming from Social Network Platforms cannot be applied due to different processes involved (e.g., scaling, compression, etc.). Over 1 billion images are shared each day on the Internet and obtaining information about their history from the moment they were acquired could be exploited for investigation purposes. In this paper, a classification engine for the reconstruction of the history of an image, is presented. Specifically, exploiting K-NN and decision trees classifiers and a-priori knowledge acquired through image analysis, we propose an automatic approach that can understand which Social Network Platform has processed an image and the software application used to perform the image upload. The engine makes use of proper alterations introduced by each platform as features. Results, in terms of global accuracy on a dataset of 2720 images, confirm the effectiveness of the proposed strategy.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2016-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06395",
        "title": "Dynamic Probabilistic Network Based Human Action Recognition",
        "authors": [
            "Anne Veenendaal",
            "Eddie Jones",
            "Zhao Gang",
            "Elliot Daly",
            "Sumalini Vartak",
            "Rahul Patwardhan"
        ],
        "abstract": "This paper examines use of dynamic probabilistic networks (DPN) for human action recognition. The actions of lifting objects and walking in the room, sitting in the room and neutral standing pose were used for testing the classification. The research used the dynamic interrelation between various different regions of interest (ROI) on the human body (face, body, arms, legs) and the time series based events related to the these ROIs. This dynamic links are then used to recognize the human behavioral aspects in the scene. First a model is developed to identify the human activities in an indoor scene and this model is dependent on the key features and interlinks between the various dynamic events using DPNs. The sub ROI are classified with DPN to associate the combined interlink with a specific human activity. The recognition accuracy performance between indoor (controlled lighting conditions) is compared with the outdoor lighting conditions. The accuracy in outdoor scenes was lower than the controlled environment.\n    ",
        "submission_date": "2016-07-26T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06449",
        "title": "Exploiting inter-image similarity and ensemble of extreme learners for fixation prediction using deep features",
        "authors": [
            "Hamed R.-Tavakoli",
            "Ali Borji",
            "Jorma Laaksonen",
            "Esa Rahtu"
        ],
        "abstract": "This paper presents a novel fixation prediction and saliency modeling framework based on inter-image similarities and ensemble of Extreme Learning Machines (ELM). The proposed framework is inspired by two observations, 1) the contextual information of a scene along with low-level visual cues modulates attention, 2) the influence of scene memorability on eye movement patterns caused by the resemblance of a scene to a former visual experience. Motivated by such observations, we develop a framework that estimates the saliency of a given image using an ensemble of extreme learners, each trained on an image similar to the input image. That is, after retrieving a set of similar images for a given image, a saliency predictor is learnt from each of the images in the retrieved image set using an ELM, resulting in an ensemble. The saliency of the given image is then measured in terms of the mean of predicted saliency value by the ensemble's members.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2016-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06454",
        "title": "Reasoning with Memory Augmented Neural Networks for Language Comprehension",
        "authors": [
            "Tsendsuren Munkhdalai",
            "Hong Yu"
        ],
        "abstract": "Hypothesis testing is an important cognitive process that supports human reasoning. In this paper, we introduce a computational hypothesis testing approach based on memory augmented neural networks. Our approach involves a hypothesis testing loop that reconsiders and progressively refines a previously formed hypothesis in order to generate new hypotheses to test. We apply the proposed approach to language comprehension task by using Neural Semantic Encoders (NSE). Our NSE models achieve the state-of-the-art results showing an absolute improvement of 1.2% to 2.6% accuracy over previous results obtained by single and ensemble systems on standard machine comprehension benchmarks such as the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2017-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06540",
        "title": "Jointly Learning to Align and Convert Graphemes to Phonemes with Neural Attention Models",
        "authors": [
            "Shubham Toshniwal",
            "Karen Livescu"
        ],
        "abstract": "We propose an attention-enabled encoder-decoder model for the problem of grapheme-to-phoneme conversion. Most previous work has tackled the problem via joint sequence models that require explicit alignments for training. In contrast, the attention-enabled encoder-decoder model allows for jointly learning to align and convert characters to phonemes. We explore different types of attention models, including global and local attention, and our best models achieve state-of-the-art results on three standard data sets (CMUDict, Pronlex, and NetTalk).\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2016-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06620",
        "title": "Proposing Plausible Answers for Open-ended Visual Question Answering",
        "authors": [
            "Omid Bakhshandeh",
            "Trung Bui",
            "Zhe Lin",
            "Walter Chang"
        ],
        "abstract": "Answering open-ended questions is an essential capability for any intelligent agent. One of the most interesting recent open-ended question answering challenges is Visual Question Answering (VQA) which attempts to evaluate a system's visual understanding through its answers to natural language questions about images. There exist many approaches to VQA, the majority of which do not exhibit deeper semantic understanding of the candidate answers they produce. We study the importance of generating plausible answers to a given question by introducing the novel task of `Answer Proposal': for a given open-ended question, a system should generate a ranked list of candidate answers informed by the semantics of the question. We experiment with various models including a neural generative model as well as a semantic graph matching one. We provide both intrinsic and extrinsic evaluations for the task of Answer Proposal, showing that our best model learns to propose plausible answers with a high recall and performs competitively with some other solutions to VQA.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06781",
        "title": "Modular Deep Q Networks for Sim-to-real Transfer of Visuo-motor Policies",
        "authors": [
            "Fangyi Zhang",
            "J\u00fcrgen Leitner",
            "Michael Milford",
            "Peter Corke"
        ],
        "abstract": "While deep learning has had significant successes in computer vision thanks to the abundance of visual data, collecting sufficiently large real-world datasets for robot learning can be costly. To increase the practicality of these techniques on real robots, we propose a modular deep reinforcement learning method capable of transferring models trained in simulation to a real-world robotic task. We introduce a bottleneck between perception and control, enabling the networks to be trained independently, but then merged and fine-tuned in an end-to-end manner to further improve hand-eye coordination. On a canonical, planar visually-guided robot reaching task a fine-tuned accuracy of 1.6 pixels is achieved, a significant improvement over naive transfer (17.5 pixels), showing the potential for more complicated and broader applications. Our method provides a technique for more efficient learning and transfer of visuo-motor policies for real robotic systems without relying entirely on large real-world robot datasets.\n    ",
        "submission_date": "2016-10-21T00:00:00",
        "last_modified_date": "2017-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06856",
        "title": "Automated Big Text Security Classification",
        "authors": [
            "Khudran Alzhrani",
            "Ethan M. Rudd",
            "Terrance E. Boult",
            "C. Edward Chow"
        ],
        "abstract": "In recent years, traditional cybersecurity safeguards have proven ineffective against insider threats. Famous cases of sensitive information leaks caused by insiders, including the WikiLeaks release of diplomatic cables and the Edward Snowden incident, have greatly harmed the U.S. government's relationship with other governments and with its own citizens. Data Leak Prevention (DLP) is a solution for detecting and preventing information leaks from within an organization's network. However, state-of-art DLP detection models are only able to detect very limited types of sensitive information, and research in the field has been hindered due to the lack of available sensitive texts. Many researchers have focused on document-based detection with artificially labeled \"confidential documents\" for which security labels are assigned to the entire document, when in reality only a portion of the document is sensitive. This type of whole-document based security labeling increases the chances of preventing authorized users from accessing non-sensitive information within sensitive documents. In this paper, we introduce Automated Classification Enabled by Security Similarity (ACESS), a new and innovative detection model that penetrates the complexity of big text security classification/detection. To analyze the ACESS system, we constructed a novel dataset, containing formerly classified paragraphs from diplomatic cables made public by the WikiLeaks organization. To our knowledge this paper is the first to analyze a dataset that contains actual formerly sensitive information annotated at paragraph granularity.\n    ",
        "submission_date": "2016-10-21T00:00:00",
        "last_modified_date": "2016-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06920",
        "title": "Bit-pragmatic Deep Neural Network Computing",
        "authors": [
            "J. Albericio",
            "P. Judd",
            "A. Delm\u00e1s",
            "S. Sharify",
            "A. Moshovos"
        ],
        "abstract": "We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragmatic (PRA), an architecture that exploits it improving performance and energy efficiency. The source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product [1]. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the non-zero terms using a) on-the-fly conversion of the multiplicator representation into an explicit list of powers of two, and b) hybrid bit-parallel multplicand/bit-serial multiplicator processing units. PRA exploits two sources of ineffectual computations: 1) the aforementioned zero product terms which are the result of the lack of explicitness in the multiplicator representation, and 2) the excess in the representation precision used for both multiplicants and multiplicators, e.g., [2]. Measurements demonstrate that for the convolutional layers, a straightforward variant of PRA improves performance by 2.6x over the DaDiaNao (DaDN) accelerator [3] and by 1.4x over STR [4]. Similarly, PRA improves energy efficiency by 28% and 10% on average compared to DaDN and STR. An improved cross lane synchronication scheme boosts performance improvements to 3.1x over DaDN. Finally, Pragmatic benefits persist even with an 8-bit quantized representation [5].\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2016-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07231",
        "title": "Template Matching Advances and Applications in Image Analysis",
        "authors": [
            "Nazanin Sadat Hashemi",
            "Roya Babaie Aghdam",
            "Atieh Sadat Bayat Ghiasi",
            "Parastoo Fatemi"
        ],
        "abstract": "In most computer vision and image analysis problems, it is necessary to define a similarity measure between two or more different objects or images. Template matching is a classic and fundamental method used to score similarities between objects using certain mathematical algorithms. In this paper, we reviewed the basic concept of matching, as well as advances in template matching and applications such as invariant features or novel applications in medical image analysis. Additionally, deformable models and templates originating from classic template matching were discussed. These models have broad applications in image registration, and they are a fundamental aspect of novel machine vision or deep learning algorithms, such as convolutional neural networks (CNN), which perform shift and scale invariant functions followed by classification. In general, although template matching methods have restrictions which limit their application, they are recommended for use with other object recognition methods as pre- or post-processing steps. Combining a template matching technique such as normalized cross-correlation or dice coefficient with a robust decision-making algorithm yields a significant improvement in the accuracy rate for object detection and recognition.\n    ",
        "submission_date": "2016-10-23T00:00:00",
        "last_modified_date": "2016-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07365",
        "title": "Introduction: Cognitive Issues in Natural Language Processing",
        "authors": [
            "Thierry Poibeau",
            "Shravan Vasishth"
        ],
        "abstract": "This special issue is dedicated to get a better picture of the relationships between computational linguistics and cognitive science. It specifically raises two questions: \"what is the potential contribution of computational language modeling to cognitive science?\" and conversely: \"what is the influence of cognitive science in contemporary computational linguistics?\"\n    ",
        "submission_date": "2016-10-24T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07675",
        "title": "Surprisal-Driven Zoneout",
        "authors": [
            "Kamil Rocki",
            "Tomasz Kornuta",
            "Tegan Maharaj"
        ],
        "abstract": "We propose a novel method of regularization for recurrent neural networks called suprisal-driven zoneout. In this method, states zoneout (maintain their previous value rather than updating), when the suprisal (discrepancy between the last state's prediction and target) is small. Thus regularization is adaptive and input-driven on a per-neuron basis. We demonstrate the effectiveness of this idea by achieving state-of-the-art bits per character of 1.31 on the Hutter Prize Wikipedia dataset, significantly reducing the gap to the best known highly-engineered compression methods.\n    ",
        "submission_date": "2016-10-24T00:00:00",
        "last_modified_date": "2016-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08087",
        "title": "Infinite-dimensional Log-Determinant divergences II: Alpha-Beta divergences",
        "authors": [
            "Minh Ha Quang"
        ],
        "abstract": "This work presents a parametrized family of divergences, namely Alpha-Beta Log- Determinant (Log-Det) divergences, between positive definite unitized trace class operators on a Hilbert space. This is a generalization of the Alpha-Beta Log-Determinant divergences between symmetric, positive definite matrices to the infinite-dimensional setting. The family of Alpha-Beta Log-Det divergences is highly general and contains many divergences as special cases, including the recently formulated infinite dimensional affine-invariant Riemannian distance and the infinite-dimensional Alpha Log-Det divergences between positive definite unitized trace class operators. In particular, it includes a parametrized family of metrics between positive definite trace class operators, with the affine-invariant Riemannian distance and the square root of the symmetric Stein divergence being special cases. For the Alpha-Beta Log-Det divergences between covariance operators on a Reproducing Kernel Hilbert Space (RKHS), we obtain closed form formulas via the corresponding Gram matrices.\n    ",
        "submission_date": "2016-10-13T00:00:00",
        "last_modified_date": "2017-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08127",
        "title": "Fast Bayesian Non-Negative Matrix Factorisation and Tri-Factorisation",
        "authors": [
            "Thomas Brouwer",
            "Jes Frellsen",
            "Pietro Lio'"
        ],
        "abstract": "We present a fast variational Bayesian algorithm for performing non-negative matrix factorisation and tri-factorisation. We show that our approach achieves faster convergence per iteration and timestep (wall-clock) than Gibbs sampling and non-probabilistic approaches, and do not require additional samples to estimate the posterior. We show that in particular for matrix tri-factorisation convergence is difficult, but our variational Bayesian approach offers a fast solution, allowing the tri-factorisation approach to be used more effectively.\n    ",
        "submission_date": "2016-10-26T00:00:00",
        "last_modified_date": "2016-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08251",
        "title": "Quantum-enhanced machine learning",
        "authors": [
            "Vedran Dunjko",
            "Jacob M. Taylor",
            "Hans J. Briegel"
        ],
        "abstract": "The emerging field of quantum machine learning has the potential to substantially aid in the problems and scope of artificial intelligence. This is only enhanced by recent successes in the field of classical machine learning. In this work we propose an approach for the systematic treatment of machine learning, from the perspective of quantum information. Our approach is general and covers all three main branches of machine learning: supervised, unsupervised and reinforcement learning. While quantum improvements in supervised and unsupervised learning have been reported, reinforcement learning has received much less attention. Within our approach, we tackle the problem of quantum enhancements in reinforcement learning as well, and propose a systematic scheme for providing improvements. As an example, we show that quadratic improvements in learning efficiency, and exponential improvements in performance over limited time periods, can be obtained for a broad class of learning problems.\n    ",
        "submission_date": "2016-10-26T00:00:00",
        "last_modified_date": "2016-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08401",
        "title": "Universal adversarial perturbations",
        "authors": [
            "Seyed-Mohsen Moosavi-Dezfooli",
            "Alhussein Fawzi",
            "Omar Fawzi",
            "Pascal Frossard"
        ],
        "abstract": "Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.\n    ",
        "submission_date": "2016-10-26T00:00:00",
        "last_modified_date": "2017-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08469",
        "title": "Kissing Cuisines: Exploring Worldwide Culinary Habits on the Web",
        "authors": [
            "Sina Sajadmanesh",
            "Sina Jafarzadeh",
            "Seyed Ali Osia",
            "Hamid R. Rabiee",
            "Hamed Haddadi",
            "Yelena Mejova",
            "Mirco Musolesi",
            "Emiliano De Cristofaro",
            "Gianluca Stringhini"
        ],
        "abstract": "Food and nutrition occupy an increasingly prevalent space on the web, and dishes and recipes shared online provide an invaluable mirror into culinary cultures and attitudes around the world. More specifically, ingredients, flavors, and nutrition information become strong signals of the taste preferences of individuals and civilizations. However, there is little understanding of these palate varieties. In this paper, we present a large-scale study of recipes published on the web and their content, aiming to understand cuisines and culinary habits around the world. Using a database of more than 157K recipes from over 200 different cuisines, we analyze ingredients, flavors, and nutritional values which distinguish dishes from different regions, and use this knowledge to assess the predictability of recipes from different cuisines. We then use country health statistics to understand the relation between these factors and health indicators of different nations, such as obesity, diabetes, migration, and health expenditure. Our results confirm the strong effects of geographical and cultural similarities on recipes, health indicators, and culinary preferences across the globe.\n    ",
        "submission_date": "2016-10-26T00:00:00",
        "last_modified_date": "2017-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08500",
        "title": "Synthesis of Shared Control Protocols with Provable Safety and Performance Guarantees",
        "authors": [
            "Nils Jansen",
            "Murat Cubuktepe",
            "Ufuk Topcu"
        ],
        "abstract": "We formalize synthesis of shared control protocols with correctness guarantees for temporal logic specifications. More specifically, we introduce a modeling formalism in which both a human and an autonomy protocol can issue commands to a robot towards performing a certain task. These commands are blended into a joint input to the robot. The autonomy protocol is synthesized using an abstraction of possible human commands accounting for randomness in decisions caused by factors such as fatigue or incomprehensibility of the problem at hand. The synthesis is designed to ensure that the resulting robot behavior satisfies given safety and performance specifications, e.g., in temporal logic. Our solution is based on nonlinear programming and we address the inherent scalability issue by presenting alternative methods. We assess the feasibility and the scalability of the approach by an experimental evaluation.\n    ",
        "submission_date": "2016-10-26T00:00:00",
        "last_modified_date": "2016-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08606",
        "title": "Fast Low-rank Shared Dictionary Learning for Image Classification",
        "authors": [
            "Tiep Vu",
            "Vishal Monga"
        ],
        "abstract": "Despite the fact that different objects possess distinct class-specific features, they also usually share common patterns. This observation has been exploited partially in a recently proposed dictionary learning framework by separating the particularity and the commonality (COPAR). Inspired by this, we propose a novel method to explicitly and simultaneously learn a set of common patterns as well as class-specific features for classification with more intuitive constraints. Our dictionary learning framework is hence characterized by both a shared dictionary and particular (class-specific) dictionaries. For the shared dictionary, we enforce a low-rank constraint, i.e. claim that its spanning subspace should have low dimension and the coefficients corresponding to this dictionary should be similar. For the particular dictionaries, we impose on them the well-known constraints stated in the Fisher discrimination dictionary learning (FDDL). Further, we develop new fast and accurate algorithms to solve the subproblems in the learning step, accelerating its convergence. The said algorithms could also be applied to FDDL and its extensions. The efficiencies of these algorithms are theoretically and experimentally verified by comparing their complexities and running time with those of other well-known dictionary learning methods. Experimental results on widely used image datasets establish the advantages of our method over state-of-the-art dictionary learning methods.\n    ",
        "submission_date": "2016-10-27T00:00:00",
        "last_modified_date": "2017-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08865",
        "title": "Hit-and-Run for Sampling and Planning in Non-Convex Spaces",
        "authors": [
            "Yasin Abbasi-Yadkori",
            "Peter L. Bartlett",
            "Victor Gabillon",
            "Alan Malek"
        ],
        "abstract": "We propose the Hit-and-Run algorithm for planning and sampling problems in non-convex spaces. For sampling, we show the first analysis of the Hit-and-Run algorithm in non-convex spaces and show that it mixes fast as long as certain smoothness conditions are satisfied. In particular, our analysis reveals an intriguing connection between fast mixing and the existence of smooth measure-preserving mappings from a convex space to the non-convex space. For planning, we show advantages of Hit-and-Run compared to state-of-the-art planning methods such as Rapidly-Exploring Random Trees.\n    ",
        "submission_date": "2016-10-19T00:00:00",
        "last_modified_date": "2016-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08936",
        "title": "Learning Scalable Deep Kernels with Recurrent Structure",
        "authors": [
            "Maruan Al-Shedivat",
            "Andrew Gordon Wilson",
            "Yunus Saatchi",
            "Zhiting Hu",
            "Eric P. Xing"
        ],
        "abstract": "Many applications in speech, robotics, finance, and biology deal with sequential data, where ordering matters and recurrent structures are common. However, this structure cannot be easily captured by standard kernel functions. To model such structure, we propose expressive closed-form kernel functions for Gaussian processes. The resulting model, GP-LSTM, fully encapsulates the inductive biases of long short-term memory (LSTM) recurrent networks, while retaining the non-parametric probabilistic advantages of Gaussian processes. We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic gradient procedure and exploit the structure of these kernels for scalable training and prediction. This approach provides a practical representation for Bayesian LSTMs. We demonstrate state-of-the-art performance on several benchmarks, and thoroughly investigate a consequential autonomous driving application, where the predictive uncertainties provided by GP-LSTM are uniquely valuable.\n    ",
        "submission_date": "2016-10-27T00:00:00",
        "last_modified_date": "2017-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09018",
        "title": "Optimal Belief Approximation",
        "authors": [
            "Reimar H. Leike",
            "Torsten A. En\u00dflin"
        ],
        "abstract": "In Bayesian statistics probability distributions express beliefs. However, for many problems the beliefs cannot be computed analytically and approximations of beliefs are needed. We seek a loss function that quantifies how \"embarrassing\" it is to communicate a given approximation. We reproduce and discuss an old proof showing that there is only one ranking under the requirements that (1) the best ranked approximation is the non-approximated belief and (2) that the ranking judges approximations only by their predictions for actual outcomes. The loss function that is obtained in the derivation is equal to the Kullback-Leibler divergence when normalized. This loss function is frequently used in the literature. However, there seems to be confusion about the correct order in which its functional arguments, the approximated and non-approximated beliefs, should be used. The correct order ensures that the recipient of a communication is only deprived of the minimal amount of information. We hope that the elementary derivation settles the apparent confusion. For example when approximating beliefs with Gaussian distributions the optimal approximation is given by moment matching. This is in contrast to many suggested computational schemes.\n    ",
        "submission_date": "2016-10-27T00:00:00",
        "last_modified_date": "2017-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09077",
        "title": "Integrating Topic Models and Latent Factors for Recommendation",
        "authors": [
            "Danis J. Wilson",
            "Wei Zhang"
        ],
        "abstract": "Nowadays, we have large amounts of online items in various web-based applications, which makes it an important task to build effective personalized recommender systems so as to save users' efforts in information seeking. One of the most extensively and successfully used methods for personalized recommendation is the Collaborative Filtering (CF) technique, which makes recommendation based on users' historical choices as well as those of the others'. The most popular CF method, like Latent Factor Model (LFM), is to model how users evaluate items by understanding the hidden dimension or factors of their opinions. How to model these hidden factors is key to improve the performance of recommender system. In this work, we consider the problem of hotel recommendation for travel planning services by integrating the location information and the user's preference for recommendation. The intuition is that user preferences may change dynamically over different locations, thus treating the historical decisions of a user as static or universally applicable can be infeasible in real-world applications. For example, users may prefer chain brand hotels with standard configurations when traveling for business, while they may prefer unique local hotels when traveling for entertainment. In this paper, we aim to provide trip-level personalization for users in recommendation.\n    ",
        "submission_date": "2016-10-28T00:00:00",
        "last_modified_date": "2021-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09156",
        "title": "Fuzzy Bayesian Learning",
        "authors": [
            "Indranil Pan",
            "Dirk Bester"
        ],
        "abstract": "In this paper we propose a novel approach for learning from data using rule based fuzzy inference systems where the model parameters are estimated using Bayesian inference and Markov Chain Monte Carlo (MCMC) techniques. We show the applicability of the method for regression and classification tasks using synthetic data-sets and also a real world example in the financial services industry. Then we demonstrate how the method can be extended for knowledge extraction to select the individual rules in a Bayesian way which best explains the given data. Finally we discuss the advantages and pitfalls of using this method over state-of-the-art techniques and highlight the specific class of problems where this would be useful.\n    ",
        "submission_date": "2016-10-28T00:00:00",
        "last_modified_date": "2018-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09160",
        "title": "How Users Explore Ontologies on the Web: A Study of NCBO's BioPortal Usage Logs",
        "authors": [
            "Simon Walk",
            "Lisette Esp\u00edn-Noboa",
            "Denis Helic",
            "Markus Strohmaier",
            "Mark Musen"
        ],
        "abstract": "Ontologies in the biomedical domain are numerous, highly specialized and very expensive to develop. Thus, a crucial prerequisite for ontology adoption and reuse is effective support for exploring and finding existing ontologies. Towards that goal, the National Center for Biomedical Ontology (NCBO) has developed BioPortal---an online repository designed to support users in exploring and finding more than 500 existing biomedical ontologies. In 2016, BioPortal represents one of the largest portals for exploration of semantic biomedical vocabularies and terminologies, which is used by many researchers and practitioners. While usage of this portal is high, we know very little about how exactly users search and explore ontologies and what kind of usage patterns or user groups exist in the first place. Deeper insights into user behavior on such portals can provide valuable information to devise strategies for a better support of users in exploring and finding existing ontologies, and thereby enable better ontology reuse. To that end, we study and group users according to their browsing behavior on BioPortal using data mining techniques. Additionally, we use the obtained groups to characterize and compare exploration strategies across ontologies. In particular, we were able to identify seven distinct browsing-behavior types, which all make use of different functionality provided by BioPortal. For example, Search Explorers make extensive use of the search functionality while Ontology Tree Explorers mainly rely on the class hierarchy to explore ontologies. Further, we show that specific characteristics of ontologies influence the way users explore and interact with the website. Our results may guide the development of more user-oriented systems for ontology exploration on the Web.\n    ",
        "submission_date": "2016-10-28T00:00:00",
        "last_modified_date": "2016-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09296",
        "title": "Improving Sampling from Generative Autoencoders with Markov Chains",
        "authors": [
            "Antonia Creswell",
            "Kai Arulkumaran",
            "Anil Anthony Bharath"
        ],
        "abstract": "We focus on generative autoencoders, such as variational or adversarial autoencoders, which jointly learn a generative model alongside an inference model. Generative autoencoders are those which are trained to softly enforce a prior on the latent distribution learned by the inference model. We call the distribution to which the inference model maps observed samples, the learned latent distribution, which may not be consistent with the prior. We formulate a Markov chain Monte Carlo (MCMC) sampling process, equivalent to iteratively decoding and encoding, which allows us to sample from the learned latent distribution. Since, the generative model learns to map from the learned latent distribution, rather than the prior, we may use MCMC to improve the quality of samples drawn from the generative model, especially when the learned latent distribution is far from the prior. Using MCMC sampling, we are able to reveal previously unseen differences between generative autoencoders trained either with or without a denoising criterion.\n    ",
        "submission_date": "2016-10-28T00:00:00",
        "last_modified_date": "2017-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09431",
        "title": "Attention acts to suppress goal-based conflict under high competition",
        "authors": [
            "Omar Claflin"
        ],
        "abstract": "It is known that when multiple stimuli are present, top-down attention selectively enhances the neural signal in the visual cortex for task-relevant stimuli, but this has been tested only under conditions of minimal competition of visual attention. Here we show during high competition, that is, two stimuli in a shared receptive field possessing opposing modulatory goals, top-down attention suppresses both task-relevant and irrelevant neural signals within 100 ms of stimuli onset. This non-selective engagement of top-down attentional resources serves to reduce the feedforward signal representing irrelevant stimuli.\n    ",
        "submission_date": "2016-10-29T00:00:00",
        "last_modified_date": "2016-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09609",
        "title": "Generalized Haar Filter based Deep Networks for Real-Time Object Detection in Traffic Scene",
        "authors": [
            "Keyu Lu",
            "Jian Li",
            "Xiangjing An",
            "Hangen He"
        ],
        "abstract": "Vision-based object detection is one of the fundamental functions in numerous traffic scene applications such as self-driving vehicle systems and advance driver assistance systems (ADAS). However, it is also a challenging task due to the diversity of traffic scene and the storage, power and computing source limitations of the platforms for traffic scene applications. This paper presents a generalized Haar filter based deep network which is suitable for the object detection tasks in traffic scene. In this approach, we first decompose a object detection task into several easier local regression tasks. Then, we handle the local regression tasks by using several tiny deep networks which simultaneously output the bounding boxes, categories and confidence scores of detected objects. To reduce the consumption of storage and computing resources, the weights of the deep networks are constrained to the form of generalized Haar filter in training phase. Additionally, we introduce the strategy of sparse windows generation to improve the efficiency of the algorithm. Finally, we perform several experiments to validate the performance of our proposed approach. Experimental results demonstrate that the proposed approach is both efficient and effective in traffic scene compared with the state-of-the-art.\n    ",
        "submission_date": "2016-10-30T00:00:00",
        "last_modified_date": "2016-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09778",
        "title": "DPPred: An Effective Prediction Framework with Concise Discriminative Patterns",
        "authors": [
            "Jingbo Shang",
            "Meng Jiang",
            "Wenzhu Tong",
            "Jinfeng Xiao",
            "Jian Peng",
            "Jiawei Han"
        ],
        "abstract": "In the literature, two series of models have been proposed to address prediction problems including classification and regression. Simple models, such as generalized linear models, have ordinary performance but strong interpretability on a set of simple features. The other series, including tree-based models, organize numerical, categorical and high dimensional features into a comprehensive structure with rich interpretable information in the data.\n",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2016-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09787",
        "title": "Edward: A library for probabilistic modeling, inference, and criticism",
        "authors": [
            "Dustin Tran",
            "Alp Kucukelbir",
            "Adji B. Dieng",
            "Maja Rudolph",
            "Dawen Liang",
            "David M. Blei"
        ],
        "abstract": "Probabilistic modeling is a powerful approach for analyzing empirical information. We describe Edward, a library for probabilistic modeling. Edward's design reflects an iterative process pioneered by George Box: build a model of a phenomenon, make inferences about the model given data, and criticize the model's fit to the data. Edward supports a broad class of probabilistic models, efficient algorithms for inference, and many techniques for model criticism. The library builds on top of TensorFlow to support distributed training and hardware such as GPUs. Edward enables the development of complex probabilistic models and their algorithms at a massive scale.\n    ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2017-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09889",
        "title": "Chinese Poetry Generation with Planning based Neural Network",
        "authors": [
            "Zhe Wang",
            "Wei He",
            "Hua Wu",
            "Haiyang Wu",
            "Wei Li",
            "Haifeng Wang",
            "Enhong Chen"
        ],
        "abstract": "Chinese poetry generation is a very challenging task in natural language processing. In this paper, we propose a novel two-stage poetry generating method which first plans the sub-topics of the poem according to the user's writing intent, and then generates each line of the poem sequentially, using a modified recurrent neural network encoder-decoder framework. The proposed planning-based method can ensure that the generated poem is coherent and semantically consistent with the user's intent. A comprehensive evaluation with human judgments demonstrates that our proposed approach outperforms the state-of-the-art poetry generating methods and the poem quality is somehow comparable to human poets.\n    ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2016-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09894",
        "title": "Mining Social Media for Open Innovation in Transportation Systems",
        "authors": [
            "Daniela Ulloa",
            "Pedro Saleiro",
            "Rosaldo J. F. Rossetti",
            "Elis Regina Silva"
        ],
        "abstract": "This work proposes a novel framework for the development of new products and services in transportation through an open innovation approach based on automatic content analysis of social media data. The framework is able to extract users comments from Online Social Networks (OSN), to process and analyze text through information extraction and sentiment analysis techniques to obtain relevant information about product reception on the market. A use case was developed using the mobile application Uber, which is today one of the fastest growing technology companies in the world. We measured how a controversial, highly diffused event influences the volume of tweets about Uber and the perception of its users. While there is no change in the image of Uber, a large increase in the number of tweets mentioning the company is observed, which meant a free and important diffusion of its product.\n    ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2016-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09950",
        "title": "From Node Embedding To Community Embedding",
        "authors": [
            "Vincent W. Zheng",
            "Sandro Cavallari",
            "Hongyun Cai",
            "Kevin Chen-Chuan Chang",
            "Erik Cambria"
        ],
        "abstract": "Most of the existing graph embedding methods focus on nodes, which aim to output a vector representation for each node in the graph such that two nodes being \"close\" on the graph are close too in the low-dimensional space. Despite the success of embedding individual nodes for graph analytics, we notice that an important concept of embedding communities (i.e., groups of nodes) is missing. Embedding communities is useful, not only for supporting various community-level applications, but also to help preserve community structure in graph embedding. In fact, we see community embedding as providing a higher-order proximity to define the node closeness, whereas most of the popular graph embedding methods focus on first-order and/or second-order proximities. To learn the community embedding, we hinge upon the insight that community embedding and node embedding reinforce with each other. As a result, we propose ComEmbed, the first community embedding method, which jointly optimizes the community embedding and node embedding together. We evaluate ComEmbed on real-world data sets. We show it outperforms the state-of-the-art baselines in both tasks of node classification and community prediction.\n    ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2017-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00020",
        "title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision",
        "authors": [
            "Chen Liang",
            "Jonathan Berant",
            "Quoc Le",
            "Kenneth D. Forbus",
            "Ni Lao"
        ],
        "abstract": "Harnessing the statistical power of neural networks to perform language understanding and symbolic reasoning is difficult, when it requires executing efficient discrete operations against a large knowledge-base. In this work, we introduce a Neural Symbolic Machine, which contains (a) a neural \"programmer\", i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality (b) a symbolic \"computer\", i.e., a Lisp interpreter that performs program execution, and helps find good programs by pruning the search space. We apply REINFORCE to directly optimize the task reward of this structured prediction problem. To train with weak supervision and improve the stability of REINFORCE, we augment it with an iterative maximum-likelihood training process. NSM outperforms the state-of-the-art on the WebQuestionsSP dataset when trained from question-answer pairs only, without requiring any feature engineering or domain-specific knowledge.\n    ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2017-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00175",
        "title": "Robust Spectral Inference for Joint Stochastic Matrix Factorization",
        "authors": [
            "Moontae Lee",
            "David Bindel",
            "David Mimno"
        ],
        "abstract": "Spectral inference provides fast algorithms and provable optimality for latent topic analysis. But for real data these algorithms require additional ad-hoc heuristics, and even then often produce unusable results. We explain this poor performance by casting the problem of topic inference in the framework of Joint Stochastic Matrix Factorization (JSMF) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist. We then propose a novel rectification method that learns high quality topics and their interactions even on small, noisy data. This method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality.\n    ",
        "submission_date": "2016-11-01T00:00:00",
        "last_modified_date": "2016-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00201",
        "title": "Towards Lifelong Self-Supervision: A Deep Learning Direction for Robotics",
        "authors": [
            "Jay M. Wong"
        ],
        "abstract": "Despite outstanding success in vision amongst other domains, many of the recent deep learning approaches have evident drawbacks for robots. This manuscript surveys recent work in the literature that pertain to applying deep learning systems to the robotics domain, either as means of estimation or as a tool to resolve motor commands directly from raw percepts. These recent advances are only a piece to the puzzle. We suggest that deep learning as a tool alone is insufficient in building a unified framework to acquire general intelligence. For this reason, we complement our survey with insights from cognitive development and refer to ideas from classical control theory, producing an integrated direction for a lifelong learning architecture.\n    ",
        "submission_date": "2016-11-01T00:00:00",
        "last_modified_date": "2016-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00230",
        "title": "Towards Blended Reactive Planning and Acting using Behavior Trees",
        "authors": [
            "Michele Colledanchise",
            "Diogo Almeida",
            "Petter \u00d6gren"
        ],
        "abstract": "In this paper, we show how a planning algorithm can be used to automatically create and update a Behavior Tree (BT), controlling a robot in a dynamic environment. The planning part of the algorithm is based on the idea of back chaining. Starting from a goal condition we iteratively select actions to achieve that goal, and if those actions have unmet preconditions, they are extended with actions to achieve them in the same way. The fact that BTs are inherently modular and reactive makes the proposed solution blend acting and planning in a way that enables the robot to efficiently react to external disturbances. If an external agent undoes an action the robot reexecutes it without re-planning, and if an external agent helps the robot, it skips the corresponding actions, again without replanning. We illustrate our approach in two different robotics scenarios.\n    ",
        "submission_date": "2016-11-01T00:00:00",
        "last_modified_date": "2018-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00447",
        "title": "Bots as Virtual Confederates: Design and Ethics",
        "authors": [
            "Peter M Krafft",
            "Michael Macy",
            "Alex Pentland"
        ],
        "abstract": "The use of bots as virtual confederates in online field experiments holds extreme promise as a new methodological tool in computational social science. However, this potential tool comes with inherent ethical challenges. Informed consent can be difficult to obtain in many cases, and the use of confederates necessarily implies the use of deception. In this work we outline a design space for bots as virtual confederates, and we propose a set of guidelines for meeting the status quo for ethical experimentation. We draw upon examples from prior work in the CSCW community and the broader social science literature for illustration. While a handful of prior researchers have used bots in online experimentation, our work is meant to inspire future work in this area and raise awareness of the associated ethical issues.\n    ",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00448",
        "title": "Natural-Parameter Networks: A Class of Probabilistic Neural Networks",
        "authors": [
            "Hao Wang",
            "Xingjian Shi",
            "Dit-Yan Yeung"
        ],
        "abstract": "Neural networks (NN) have achieved state-of-the-art performance in various applications. Unfortunately in applications where training data is insufficient, they are often prone to overfitting. One effective way to alleviate this problem is to exploit the Bayesian approach by using Bayesian neural networks (BNN). Another shortcoming of NN is the lack of flexibility to customize different distributions for the weights and neurons according to the data, as is often done in probabilistic graphical models. To address these problems, we propose a class of probabilistic neural networks, dubbed natural-parameter networks (NPN), as a novel and lightweight Bayesian treatment of NN. NPN allows the usage of arbitrary exponential-family distributions to model the weights and neurons. Different from traditional NN and BNN, NPN takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions. As a Bayesian treatment, efficient backpropagation (BP) is performed to learn the natural parameters for the distributions over both the weights and neurons. The output distributions of each layer, as byproducts, may be used as second-order representations for the associated tasks such as link prediction. Experiments on real-world datasets show that NPN can achieve state-of-the-art performance.\n    ",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00454",
        "title": "Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks",
        "authors": [
            "Hao Wang",
            "Xingjian Shi",
            "Dit-Yan Yeung"
        ],
        "abstract": "Hybrid methods that utilize both content and rating information are commonly used in many recommender systems. However, most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough. To address this problem, we develop a collaborative recurrent autoencoder (CRAE) which is a denoising recurrent autoencoder (DRAE) that models the generation of content sequences in the collaborative filtering (CF) setting. The model generalizes recent advances in recurrent deep learning from i.i.d. input to non-i.i.d. (CF-based) input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder. To do this, we first develop a hierarchical Bayesian model for the DRAE and then generalize it to the CF setting. The synergy between denoising and CF enables CRAE to make accurate recommendations while learning to fill in the blanks in sequences. Experiments on real-world datasets from different domains (CiteULike and Netflix) show that, by jointly modeling the order-aware generation of sequences for the content information and performing CF for the ratings, CRAE is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information.\n    ",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00547",
        "title": "Limitations and Alternatives for the Evaluation of Large-scale Link Prediction",
        "authors": [
            "Dario Garcia-Gasulla",
            "Eduard Ayguad\u00e9",
            "Jes\u00fas Labarta",
            "Ulises Cort\u00e9s"
        ],
        "abstract": "Link prediction, the problem of identifying missing links among a set of inter-related data entities, is a popular field of research due to its application to graph-like domains. Producing consistent evaluations of the performance of the many link prediction algorithms being proposed can be challenging due to variable graph properties, such as size and density. In this paper we first discuss traditional data mining solutions which are applicable to link prediction evaluation, arguing about their capacity for producing faithful and useful evaluations. We also introduce an innovative modification to a traditional evaluation methodology with the goal of adapting it to the problem of evaluating link prediction algorithms when applied to large graphs, by tackling the problem of class imbalance. We empirically evaluate the proposed methodology and, building on these findings, make a case for its importance on the evaluation of large-scale graph processing.\n    ",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2016-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00577",
        "title": "The new hybrid COAW method for solving multi-objective problems",
        "authors": [
            "Zeinab Borhanifar",
            "Elham Shadkam"
        ],
        "abstract": "In this article using Cuckoo Optimization Algorithm and simple additive weighting method the hybrid COAW algorithm is presented to solve multi-objective problems. Cuckoo algorithm is an efficient and structured method for solving nonlinear continuous problems. The created Pareto frontiers of the COAW proposed algorithm are exact and have good dispersion. This method has a high speed in finding the Pareto frontiers and identifies the beginning and end points of Pareto frontiers properly. In order to validation the proposed algorithm, several experimental problems were analyzed. The results of which indicate the proper effectiveness of COAW algorithm for solving multi-objective problems.\n    ",
        "submission_date": "2016-01-06T00:00:00",
        "last_modified_date": "2016-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00625",
        "title": "TorchCraft: a Library for Machine Learning Research on Real-Time Strategy Games",
        "authors": [
            "Gabriel Synnaeve",
            "Nantas Nardelli",
            "Alex Auvolat",
            "Soumith Chintala",
            "Timoth\u00e9e Lacroix",
            "Zeming Lin",
            "Florian Richoux",
            "Nicolas Usunier"
        ],
        "abstract": "We present TorchCraft, a library that enables deep learning research on Real-Time Strategy (RTS) games such as StarCraft: Brood War, by making it easier to control these games from a machine learning framework, here Torch. This white paper argues for using RTS games as a benchmark for AI research, and describes the design and components of TorchCraft.\n    ",
        "submission_date": "2016-11-01T00:00:00",
        "last_modified_date": "2016-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00736",
        "title": "Extensions and Limitations of the Neural GPU",
        "authors": [
            "Eric Price",
            "Wojciech Zaremba",
            "Ilya Sutskever"
        ],
        "abstract": "The Neural GPU is a recent model that can learn algorithms such as multi-digit binary addition and binary multiplication in a way that generalizes to inputs of arbitrary length. We show that there are two simple ways of improving the performance of the Neural GPU: by carefully designing a curriculum, and by increasing model size. The latter requires a memory efficient implementation, as a naive implementation of the Neural GPU is memory intensive. We find that these techniques increase the set of algorithmic problems that can be solved by the Neural GPU: we have been able to learn to perform all the arithmetic operations (and generalize to arbitrarily long numbers) when the arguments are given in the decimal representation (which, surprisingly, has not been possible before). We have also been able to train the Neural GPU to evaluate long arithmetic expressions with multiple operands that require respecting the precedence order of the operands, although these have succeeded only in their binary representation, and not with perfect accuracy.\n",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2016-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00791",
        "title": "Predicting Domain Generation Algorithms with Long Short-Term Memory Networks",
        "authors": [
            "Jonathan Woodbridge",
            "Hyrum S. Anderson",
            "Anjum Ahuja",
            "Daniel Grant"
        ],
        "abstract": "Various families of malware use domain generation algorithms (DGAs) to generate a large number of pseudo-random domain names to connect to a command and control (C&C) server. In order to block DGA C&C traffic, security organizations must first discover the algorithm by reverse engineering malware samples, then generating a list of domains for a given seed. The domains are then either preregistered or published in a DNS blacklist. This process is not only tedious, but can be readily circumvented by malware authors using a large number of seeds in algorithms with multivariate recurrence properties (e.g., banjori) or by using a dynamic list of seeds (e.g., bedep). Another technique to stop malware from using DGAs is to intercept DNS queries on a network and predict whether domains are DGA generated. Such a technique will alert network administrators to the presence of malware on their networks. In addition, if the predictor can also accurately predict the family of DGAs, then network administrators can also be alerted to the type of malware that is on their networks. This paper presents a DGA classifier that leverages long short-term memory (LSTM) networks to predict DGAs and their respective families without the need for a priori feature extraction. Results are significantly better than state-of-the-art techniques, providing 0.9993 area under the receiver operating characteristic curve for binary classification and a micro-averaged F1 score of 0.9906. In other terms, the LSTM technique can provide a 90% detection rate with a 1:10000 false positive (FP) rate---a twenty times FP improvement over comparable methods. Experiments in this paper are run on open datasets and code snippets are provided to reproduce the results.\n    ",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00862",
        "title": "Quantile Reinforcement Learning",
        "authors": [
            "Hugo Gilbert",
            "Paul Weng"
        ],
        "abstract": "In reinforcement learning, the standard criterion to evaluate policies in a state is the expectation of (discounted) sum of rewards. However, this criterion may not always be suitable, we consider an alternative criterion based on the notion of quantiles. In the case of episodic reinforcement learning problems, we propose an algorithm based on stochastic approximation with two timescales. We evaluate our proposition on a simple model of the TV show, Who wants to be a millionaire.\n    ",
        "submission_date": "2016-11-03T00:00:00",
        "last_modified_date": "2016-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00890",
        "title": "Maximizing Investment Value of Small-Scale PV in a Smart Grid Environment",
        "authors": [
            "Jeremy Every",
            "Li Li",
            "Youguang G. Guo",
            "David G. Dorrell"
        ],
        "abstract": "Determining the optimal size and orientation of small-scale residential based PV arrays will become increasingly complex in the future smart grid environment with the introduction of smart meters and dynamic tariffs. However consumers can leverage the availability of smart meter data to conduct a more detailed exploration of PV investment options for their particular circumstances. In this paper, an optimization method for PV orientation and sizing is proposed whereby maximizing the PV investment value is set as the defining objective. Solar insolation and PV array models are described to form the basis of the PV array optimization strategy. A constrained particle swarm optimization algorithm is selected due to its strong performance in non-linear applications. The optimization algorithm is applied to real-world metered data to quantify the possible investment value of a PV installation under different energy retailers and tariff structures. The arrangement with the highest value is determined to enable prospective small-scale PV investors to select the most cost-effective system.\n    ",
        "submission_date": "2016-11-03T00:00:00",
        "last_modified_date": "2016-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01423",
        "title": "Learning Continuous Semantic Representations of Symbolic Expressions",
        "authors": [
            "Miltiadis Allamanis",
            "Pankajan Chanthirasegaran",
            "Pushmeet Kohli",
            "Charles Sutton"
        ],
        "abstract": "Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of algebraic and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.\n    ",
        "submission_date": "2016-11-04T00:00:00",
        "last_modified_date": "2017-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01455",
        "title": "Ways of Conditioning Generative Adversarial Networks",
        "authors": [
            "Hanock Kwak",
            "Byoung-Tak Zhang"
        ],
        "abstract": "The GANs are generative models whose random samples realistically reflect natural images. It also can generate samples with specific attributes by concatenating a condition vector into the input, yet research on this field is not well studied. We propose novel methods of conditioning generative adversarial networks (GANs) that achieve state-of-the-art results on MNIST and CIFAR-10. We mainly introduce two models: an information retrieving model that extracts conditional information from the samples, and a spatial bilinear pooling model that forms bilinear features derived from the spatial cross product of an image and a condition vector. These methods significantly enhance log-likelihood of test data under the conditional distributions compared to the methods of concatenation.\n    ",
        "submission_date": "2016-11-04T00:00:00",
        "last_modified_date": "2016-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01491",
        "title": "Understanding Deep Neural Networks with Rectified Linear Units",
        "authors": [
            "Raman Arora",
            "Amitabh Basu",
            "Poorya Mianjy",
            "Anirbit Mukherjee"
        ],
        "abstract": "In this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU). We give an algorithm to train a ReLU DNN with one hidden layer to *global optimality* with runtime polynomial in the data size albeit exponential in the input dimension. Further, we improve on the known lower bounds on size (from exponential to super exponential) for approximating a ReLU deep net function by a shallower ReLU net. Our gap theorems hold for smoothly parametrized families of \"hard\" functions, contrary to countable, discrete families known in the literature. An example consequence of our gap theorems is the following: for every natural number $k$ there exists a function representable by a ReLU DNN with $k^2$ hidden layers and total size $k^3$, such that any ReLU DNN with at most $k$ hidden layers will require at least $\\frac{1}{2}k^{k+1}-1$ total nodes. Finally, for the family of $\\mathbb{R}^n\\to \\mathbb{R}$ DNNs with ReLU activations, we show a new lowerbound on the number of affine pieces, which is larger than previous constructions in certain regimes of the network architecture and most distinctively our lowerbound is demonstrated by an explicit construction of a *smoothly parameterized* family of functions attaining this scaling. Our construction utilizes the theory of zonotopes from polyhedral theory.\n    ",
        "submission_date": "2016-11-04T00:00:00",
        "last_modified_date": "2018-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01504",
        "title": "Estimating Causal Direction and Confounding of Two Discrete Variables",
        "authors": [
            "Krzysztof Chalupka",
            "Frederick Eberhardt",
            "Pietro Perona"
        ],
        "abstract": "We propose a method to classify the causal relationship between two discrete variables given only the joint distribution of the variables, acknowledging that the method is subject to an inherent baseline error. We assume that the causal system is acyclicity, but we do allow for hidden common causes. Our algorithm presupposes that the probability distributions $P(C)$ of a cause $C$ is independent from the probability distribution $P(E\\mid C)$ of the cause-effect mechanism. While our classifier is trained with a Bayesian assumption of flat hyperpriors, we do not make this assumption about our test data. This work connects to recent developments on the identifiability of causal models over continuous variables under the assumption of \"independent mechanisms\". Carefully-commented Python notebooks that reproduce all our experiments are available online at ",
        "submission_date": "2016-11-04T00:00:00",
        "last_modified_date": "2016-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01553",
        "title": "QBF Solving by Counterexample-guided Expansion",
        "authors": [
            "Roderick Bloem",
            "Nicolas Braud-Santoni",
            "Vedad Hadzic"
        ],
        "abstract": "We introduce a novel generalization of Counterexample-Guided Inductive Synthesis (CEGIS) and instantiate it to yield a novel, competitive algorithm for solving Quantified Boolean Formulas (QBF). Current QBF solvers based on counterexample-guided expansion use a recursive approach which scales poorly with the number of quantifier alternations. Our generalization of CEGIS removes the need for this recursive approach, and we instantiate it to yield a simple and efficient algorithm for QBF solving. Lastly, this research is supported by a competitive, though straightforward, implementation of the algorithm, making it possible to study the practical impact of our algorithm design decisions, along with various optimizations.\n    ",
        "submission_date": "2016-11-04T00:00:00",
        "last_modified_date": "2018-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01576",
        "title": "Quasi-Recurrent Neural Networks",
        "authors": [
            "James Bradbury",
            "Stephen Merity",
            "Caiming Xiong",
            "Richard Socher"
        ],
        "abstract": "Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.\n    ",
        "submission_date": "2016-11-05T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01578",
        "title": "Neural Architecture Search with Reinforcement Learning",
        "authors": [
            "Barret Zoph",
            "Quoc V. Le"
        ],
        "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.\n    ",
        "submission_date": "2016-11-05T00:00:00",
        "last_modified_date": "2017-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01587",
        "title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks",
        "authors": [
            "Kazuma Hashimoto",
            "Caiming Xiong",
            "Yoshimasa Tsuruoka",
            "Richard Socher"
        ],
        "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging, parsing, relatedness, and entailment tasks.\n    ",
        "submission_date": "2016-11-05T00:00:00",
        "last_modified_date": "2017-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01604",
        "title": "Dynamic Coattention Networks For Question Answering",
        "authors": [
            "Caiming Xiong",
            "Victor Zhong",
            "Richard Socher"
        ],
        "abstract": "Several deep learning models have been proposed for question answering. However, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointing decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.\n    ",
        "submission_date": "2016-11-05T00:00:00",
        "last_modified_date": "2018-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01626",
        "title": "Combining policy gradient and Q-learning",
        "authors": [
            "Brendan O'Donoghue",
            "Remi Munos",
            "Koray Kavukcuoglu",
            "Volodymyr Mnih"
        ],
        "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as 'PGQL', for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.\n    ",
        "submission_date": "2016-11-05T00:00:00",
        "last_modified_date": "2017-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01652",
        "title": "A Differentiable Physics Engine for Deep Learning in Robotics",
        "authors": [
            "Jonas Degrave",
            "Michiel Hermans",
            "Joni Dambre",
            "Francis wyffels"
        ],
        "abstract": "An important field in robotics is the optimization of controllers. Currently, robots are often treated as a black box in this optimization process, which is the reason why derivative-free optimization methods such as evolutionary algorithms or reinforcement learning are omnipresent. When gradient-based methods are used, models are kept small or rely on finite difference approximations for the Jacobian. This method quickly grows expensive with increasing numbers of parameters, such as found in deep learning. We propose the implementation of a modern physics engine, which can differentiate control parameters. This engine is implemented for both CPU and GPU. Firstly, this paper shows how such an engine speeds up the optimization process, even for small problems. Furthermore, it explains why this is an alternative approach to deep Q-learning, for using deep learning in robotics. Finally, we argue that this is a big step for deep learning in robotics, as it opens up new possibilities to optimize robots, both in hardware and software.\n    ",
        "submission_date": "2016-11-05T00:00:00",
        "last_modified_date": "2018-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01702",
        "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency",
        "authors": [
            "Adji B. Dieng",
            "Chong Wang",
            "Jianfeng Gao",
            "John Paisley"
        ],
        "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence - both semantic and syntactic - but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of $6.28\\%$. This is comparable to the state-of-the-art $5.91\\%$ resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.\n    ",
        "submission_date": "2016-11-05T00:00:00",
        "last_modified_date": "2017-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01708",
        "title": "Detecting Dependencies in Sparse, Multivariate Databases Using Probabilistic Programming and Non-parametric Bayes",
        "authors": [
            "Feras Saad",
            "Vikash Mansinghka"
        ],
        "abstract": "Datasets with hundreds of variables and many missing values are commonplace. In this setting, it is both statistically and computationally challenging to detect true predictive relationships between variables and also to suppress false positives. This paper proposes an approach that combines probabilistic programming, information theory, and non-parametric Bayes. It shows how to use Bayesian non-parametric modeling to (i) build an ensemble of joint probability models for all the variables; (ii) efficiently detect marginal independencies; and (iii) estimate the conditional mutual information between arbitrary subsets of variables, subject to a broad class of constraints. Users can access these capabilities using BayesDB, a probabilistic programming platform for probabilistic data analysis, by writing queries in a simple, SQL-like language. This paper demonstrates empirically that the method can (i) detect context-specific (in)dependencies on challenging synthetic problems and (ii) yield improved sensitivity and specificity over baselines from statistics and machine learning, on a real-world database of over 300 sparsely observed indicators of macroeconomic development and public health.\n    ",
        "submission_date": "2016-11-05T00:00:00",
        "last_modified_date": "2017-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01711",
        "title": "Causes for Query Answers from Databases: Datalog Abduction, View-Updates, and Integrity Constraints",
        "authors": [
            "Leopoldo Bertossi",
            "Babak Salimi"
        ],
        "abstract": "Causality has been recently introduced in databases, to model, characterize, and possibly compute causes for query answers. Connections between QA-causality and consistency-based diagnosis and database repairs (wrt. integrity constraint violations) have already been established. In this work we establish precise connections between QA-causality and both abductive diagnosis and the view-update problem in databases, allowing us to obtain new algorithmic and complexity results for QA-causality. We also obtain new results on the complexity of view-conditioned causality, and investigate the notion of QA-causality in the presence of integrity constraints, obtaining complexity results from a connection with view-conditioned causality. The abduction connection under integrity constraints allows us to obtain algorithmic tools for QA-causality.\n    ",
        "submission_date": "2016-11-06T00:00:00",
        "last_modified_date": "2017-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01747",
        "title": "A Compare-Aggregate Model for Matching Text Sequences",
        "authors": [
            "Shuohang Wang",
            "Jing Jiang"
        ],
        "abstract": "Many NLP tasks including machine comprehension, answer selection and text entailment require the comparison between sequences. Matching the important units between sequences is a key to solve these problems. In this paper, we present a general \"compare-aggregate\" framework that performs word-level matching followed by aggregation using Convolutional Neural Networks. We particularly focus on the different comparison functions we can use to match two vectors. We use four different datasets to evaluate the model. We find that some simple comparison functions based on element-wise operations can work better than standard neural network and neural tensor network.\n    ",
        "submission_date": "2016-11-06T00:00:00",
        "last_modified_date": "2016-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01779",
        "title": "Learning to Act by Predicting the Future",
        "authors": [
            "Alexey Dosovitskiy",
            "Vladlen Koltun"
        ],
        "abstract": "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.\n    ",
        "submission_date": "2016-11-06T00:00:00",
        "last_modified_date": "2017-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01843",
        "title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning",
        "authors": [
            "Misha Denil",
            "Pulkit Agrawal",
            "Tejas D Kulkarni",
            "Tom Erez",
            "Peter Battaglia",
            "Nando de Freitas"
        ],
        "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that state of art deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.\n    ",
        "submission_date": "2016-11-06T00:00:00",
        "last_modified_date": "2017-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01886",
        "title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax",
        "authors": [
            "Wentao Huang",
            "Kechen Zhang"
        ],
        "abstract": "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.\n    ",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2017-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02047",
        "title": "Reinforcement Learning Approach for Parallelization in Filters Aggregation Based Feature Selection Algorithms",
        "authors": [
            "Ivan Smetannikov",
            "Ilya Isaev",
            "Andrey Filchenkov"
        ],
        "abstract": "One of the classical problems in machine learning and data mining is feature selection. A feature selection algorithm is expected to be quick, and at the same time it should show high performance. MeLiF algorithm effectively solves this problem using ensembles of ranking filters. This article describes two different ways to improve MeLiF algorithm performance with parallelization. Experiments show that proposed schemes significantly improves algorithm performance and increase feature selection quality.\n    ",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2016-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02053",
        "title": "Reinforcement-based Simultaneous Algorithm and its Hyperparameters Selection",
        "authors": [
            "Valeria Efimova",
            "Andrey Filchenkov",
            "Anatoly Shalyto"
        ],
        "abstract": "Many algorithms for data analysis exist, especially for classification problems. To solve a data analysis problem, a proper algorithm should be chosen, and also its hyperparameters should be selected. In this paper, we present a new method for the simultaneous selection of an algorithm and its hyperparameters. In order to do so, we reduced this problem to the multi-armed bandit problem. We consider an algorithm as an arm and algorithm hyperparameters search during a fixed time as the corresponding arm play. We also suggest a problem-specific reward function. We performed the experiments on 10 real datasets and compare the suggested method with the existing one implemented in Auto-WEKA. The results show that our method is significantly better in most of the cases and never worse than the Auto-WEKA.\n    ",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2016-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02205",
        "title": "Playing SNES in the Retro Learning Environment",
        "authors": [
            "Nadav Bhonker",
            "Shai Rozenberg",
            "Itay Hubara"
        ],
        "abstract": "Mastering a video game requires skill, tactics and strategy. While these attributes may be acquired naturally by human players, teaching them to a computer program is a far more challenging task. In recent years, extensive research was carried out in the field of reinforcement learning and numerous algorithms were introduced, aiming to learn how to perform human tasks such as playing video games. As a result, the Arcade Learning Environment (ALE) (Bellemare et al., 2013) has become a commonly used benchmark environment allowing algorithms to train on various Atari 2600 games. In many games the state-of-the-art algorithms outperform humans. In this paper we introduce a new learning environment, the Retro Learning Environment --- RLE, that can run games on the Super Nintendo Entertainment System (SNES), Sega Genesis and several other gaming consoles. The environment is expandable, allowing for more video games and consoles to be easily added to the environment, while maintaining the same interface as ALE. Moreover, RLE is compatible with Python and Torch. SNES games pose a significant challenge to current algorithms due to their higher level of complexity and versatility.\n    ",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2017-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02252",
        "title": "Hierarchical compositional feature learning",
        "authors": [
            "Miguel L\u00e1zaro-Gredilla",
            "Yi Liu",
            "D. Scott Phoenix",
            "Dileep George"
        ],
        "abstract": "We introduce the hierarchical compositional network (HCN), a directed generative model able to discover and disentangle, without supervision, the building blocks of a set of binary images. The building blocks are binary features defined hierarchically as a composition of some of the features in the layer immediately below, arranged in a particular manner. At a high level, HCN is similar to a sigmoid belief network with pooling. Inference and learning in HCN are very challenging and existing variational approximations do not work satisfactorily. A main contribution of this work is to show that both can be addressed using max-product message passing (MPMP) with a particular schedule (no EM required). Also, using MPMP as an inference engine for HCN makes new tasks simple: adding supervision information, classifying images, or performing inpainting all correspond to clamping some variables of the model to their known values and running MPMP on the rest. When used for classification, fast inference with HCN has exactly the same functional form as a convolutional neural network (CNN) with linear activations and binary weights. However, HCN's features are qualitatively very different.\n    ",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2017-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02266",
        "title": "Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering",
        "authors": [
            "Liwen Zhang",
            "John Winn",
            "Ryota Tomioka"
        ],
        "abstract": "We propose the Gaussian attention model for content-based neural memory access. With the proposed attention model, a neural network has the additional degree of freedom to control the focus of its attention from a laser sharp attention to a broad attention. It is applicable whenever we can assume that the distance in the latent space reflects some notion of semantics. We use the proposed attention model as a scoring function for the embedding of a knowledge base into a continuous vector space and then train a model that performs question answering about the entities in the knowledge base. The proposed attention model can handle both the propagation of uncertainty when following a series of relations and also the conjunction of conditions in a natural way. On a dataset of soccer players who participated in the FIFA World Cup 2014, we demonstrate that our model can handle both path queries and conjunctive queries well.\n    ",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2016-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02268",
        "title": "Optimal Binary Autoencoding with Pairwise Correlations",
        "authors": [
            "Akshay Balsubramani"
        ],
        "abstract": "We formulate learning of a binary autoencoder as a biconvex optimization problem which learns from the pairwise correlations between encoded and decoded bits. Among all possible algorithms that use this information, ours finds the autoencoder that reconstructs its inputs with worst-case optimal loss. The optimal decoder is a single layer of artificial neurons, emerging entirely from the minimax loss minimization, and with weights learned by convex optimization. All this is reflected in competitive experimental results, demonstrating that binary autoencoding can be done efficiently by conveying information in pairwise correlations in an optimal fashion.\n    ",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2016-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02304",
        "title": "Normalizing Flows on Riemannian Manifolds",
        "authors": [
            "Mevlana C. Gemici",
            "Danilo Rezende",
            "Shakir Mohamed"
        ],
        "abstract": "We consider the problem of density estimation on Riemannian manifolds. Density estimation on manifolds has many applications in fluid-mechanics, optics and plasma physics and it appears often when dealing with angular variables (such as used in protein folding, robot limbs, gene-expression) and in general directional statistics. In spite of the multitude of algorithms available for density estimation in the Euclidean spaces $\\mathbf{R}^n$ that scale to large n (e.g. normalizing flows, kernel methods and variational approximations), most of these methods are not immediately suitable for density estimation in more general Riemannian manifolds. We revisit techniques related to homeomorphisms from differential geometry for projecting densities to sub-manifolds and use it to generalize the idea of normalizing flows to more general Riemannian manifolds. The resulting algorithm is scalable, simple to implement and suitable for use with automatic differentiation. We demonstrate concrete examples of this method on the n-sphere $\\mathbf{S}^n$.\n    ",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2016-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02315",
        "title": "Learning from Untrusted Data",
        "authors": [
            "Moses Charikar",
            "Jacob Steinhardt",
            "Gregory Valiant"
        ],
        "abstract": "The vast majority of theoretical results in machine learning and statistics assume that the available training data is a reasonably reliable reflection of the phenomena to be learned or estimated. Similarly, the majority of machine learning and statistical techniques used in practice are brittle to the presence of large amounts of biased or malicious data. In this work we consider two frameworks in which to study estimation, learning, and optimization in the presence of significant fractions of arbitrary data.\n",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2017-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02554",
        "title": "The Neural Noisy Channel",
        "authors": [
            "Lei Yu",
            "Phil Blunsom",
            "Chris Dyer",
            "Edward Grefenstette",
            "Tomas Kocisky"
        ],
        "abstract": "We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models. Unlike direct models which can suffer from explaining-away effects during training, noisy channel models must produce outputs that explain their inputs, and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution. Using a latent variable to control how much of the conditioning sequence the channel model needs to read in order to generate a subsequent symbol, we obtain a tractable and effective beam search decoder. Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use.\n    ",
        "submission_date": "2016-11-08T00:00:00",
        "last_modified_date": "2017-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02654",
        "title": "Sentence Ordering and Coherence Modeling using Recurrent Neural Networks",
        "authors": [
            "Lajanugen Logeswaran",
            "Honglak Lee",
            "Dragomir Radev"
        ],
        "abstract": "Modeling the structure of coherent texts is a key NLP problem. The task of coherently organizing a given set of sentences has been commonly used to build and evaluate models that understand such structure. We propose an end-to-end unsupervised deep learning approach based on the set-to-sequence framework to address this problem. Our model strongly outperforms prior methods in the order discrimination task and a novel task of ordering abstracts from scientific articles. Furthermore, our work shows that useful text representations can be obtained by learning to order sentences. Visualizing the learned sentence representations shows that the model captures high-level logical structure in paragraphs. Our representations perform comparably to state-of-the-art pre-training methods on sentence similarity and paraphrase detection tasks.\n    ",
        "submission_date": "2016-11-08T00:00:00",
        "last_modified_date": "2017-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02796",
        "title": "Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control",
        "authors": [
            "Natasha Jaques",
            "Shixiang Gu",
            "Dzmitry Bahdanau",
            "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato",
            "Richard E. Turner",
            "Douglas Eck"
        ],
        "abstract": "This paper proposes a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN. To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control. The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation. For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data.\n    ",
        "submission_date": "2016-11-09T00:00:00",
        "last_modified_date": "2017-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03130",
        "title": "Computationally Efficient Target Classification in Multispectral Image Data with Deep Neural Networks",
        "authors": [
            "Lukas Cavigelli",
            "Dominic Bernath",
            "Michele Magno",
            "Luca Benini"
        ],
        "abstract": "Detecting and classifying targets in video streams from surveillance cameras is a cumbersome, error-prone and expensive task. Often, the incurred costs are prohibitive for real-time monitoring. This leads to data being stored locally or transmitted to a central storage site for post-incident examination. The required communication links and archiving of the video data are still expensive and this setup excludes preemptive actions to respond to imminent threats. An effective way to overcome these limitations is to build a smart camera that transmits alerts when relevant video sequences are detected. Deep neural networks (DNNs) have come to outperform humans in visual classifications tasks. The concept of DNNs and Convolutional Networks (ConvNets) can easily be extended to make use of higher-dimensional input data such as multispectral data. We explore this opportunity in terms of achievable accuracy and required computational effort. To analyze the precision of DNNs for scene labeling in an urban surveillance scenario we have created a dataset with 8 classes obtained in a field experiment. We combine an RGB camera with a 25-channel VIS-NIR snapshot sensor to assess the potential of multispectral image data for target classification. We evaluate several new DNNs, showing that the spectral information fused together with the RGB frames can be used to improve the accuracy of the system or to achieve similar accuracy with a 3x smaller computation effort. We achieve a very high per-pixel accuracy of 99.1%. Even for scarcely occurring, but particularly interesting classes, such as cars, 75% of the pixels are labeled correctly with errors occurring only around the border of the objects. This high accuracy was obtained with a training set of only 30 labeled images, paving the way for fast adaptation to various application scenarios.\n    ",
        "submission_date": "2016-11-09T00:00:00",
        "last_modified_date": "2016-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03372",
        "title": "A stochastically verifiable autonomous control architecture with reasoning",
        "authors": [
            "Paolo Izzo",
            "Hongyang Qu",
            "Sandor M. Veres"
        ],
        "abstract": "A new agent architecture called Limited Instruction Set Agent (LISA) is introduced for autonomous control. The new architecture is based on previous implementations of AgentSpeak and it is structurally simpler than its predecessors with the aim of facilitating design-time and run-time verification methods. The process of abstracting the LISA system to two different types of discrete probabilistic models (DTMC and MDP) is investigated and illustrated. The LISA system provides a tool for complete modelling of the agent and the environment for probabilistic verification. The agent program can be automatically compiled into a DTMC or a MDP model for verification with Prism. The automatically generated Prism model can be used for both design-time and run-time verification. The run-time verification is investigated and illustrated in the LISA system as an internal modelling mechanism for prediction of future outcomes.\n    ",
        "submission_date": "2016-11-10T00:00:00",
        "last_modified_date": "2016-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03451",
        "title": "Importance Sampling with Unequal Support",
        "authors": [
            "Philip S. Thomas",
            "Emma Brunskill"
        ],
        "abstract": "Importance sampling is often used in machine learning when training and testing data come from different distributions. In this paper we propose a new variant of importance sampling that can reduce the variance of importance sampling-based estimates by orders of magnitude when the supports of the training and testing distributions differ. After motivating and presenting our new importance sampling estimator, we provide a detailed theoretical analysis that characterizes both its bias and variance relative to the ordinary importance sampling estimator (in various settings, which include cases where ordinary importance sampling is biased, while our new estimator is not, and vice versa). We conclude with an example of how our new importance sampling estimator can be used to improve estimates of how well a new treatment policy for diabetes will work for an individual, using only data from when the individual used a previous treatment policy.\n    ",
        "submission_date": "2016-11-10T00:00:00",
        "last_modified_date": "2016-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03477",
        "title": "Song From PI: A Musically Plausible Network for Pop Music Generation",
        "authors": [
            "Hang Chu",
            "Raquel Urtasun",
            "Sanja Fidler"
        ],
        "abstract": "We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.\n    ",
        "submission_date": "2016-11-10T00:00:00",
        "last_modified_date": "2016-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03553",
        "title": "The Sum-Product Theorem: A Foundation for Learning Tractable Models",
        "authors": [
            "Abram L. Friesen",
            "Pedro Domingos"
        ],
        "abstract": "Inference in expressive probabilistic models is generally intractable, which makes them difficult to learn and limits their applicability. Sum-product networks are a class of deep models where, surprisingly, inference remains tractable even when an arbitrary number of hidden layers are present. In this paper, we generalize this result to a much broader set of learning problems: all those where inference consists of summing a function over a semiring. This includes satisfiability, constraint satisfaction, optimization, integration, and others. In any semiring, for summation to be tractable it suffices that the factors of every product have disjoint scopes. This unifies and extends many previous results in the literature. Enforcing this condition at learning time thus ensures that the learned models are tractable. We illustrate the power and generality of this approach by applying it to a new type of structured prediction problem: learning a nonconvex function that can be globally optimized in polynomial time. We show empirically that this greatly outperforms the standard approach of learning without regard to the cost of optimization.\n    ",
        "submission_date": "2016-11-11T00:00:00",
        "last_modified_date": "2016-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03558",
        "title": "Neural Networks Models for Entity Discovery and Linking",
        "authors": [
            "Dan Liu",
            "Wei Lin",
            "Shiliang Zhang",
            "Si Wei",
            "Hui Jiang"
        ],
        "abstract": "This paper describes the USTC_NELSLIP systems submitted to the Trilingual Entity Detection and Linking (EDL) track in 2016 TAC Knowledge Base Population (KBP) contests. We have built two systems for entity discovery and mention detection (MD): one uses the conditional RNNLM and the other one uses the attention-based encoder-decoder framework. The entity linking (EL) system consists of two modules: a rule based candidate generation and a neural networks probability ranking model. Moreover, some simple string matching rules are used for NIL clustering. At the end, our best system has achieved an F1 score of 0.624 in the end-to-end typed mention ceaf plus metric.\n    ",
        "submission_date": "2016-11-11T00:00:00",
        "last_modified_date": "2016-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03599",
        "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text",
        "authors": [
            "Wei-Fan Chen",
            "Lun-Wei Ku"
        ],
        "abstract": "Most neural network models for document classification on social media focus on text infor-mation to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.\n    ",
        "submission_date": "2016-11-11T00:00:00",
        "last_modified_date": "2016-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03852",
        "title": "A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models",
        "authors": [
            "Chelsea Finn",
            "Paul Christiano",
            "Pieter Abbeel",
            "Sergey Levine"
        ],
        "abstract": "Generative adversarial networks (GANs) are a recently proposed class of generative models in which a generator is trained to optimize a cost function that is being simultaneously learned by a discriminator. While the idea of learning cost functions is relatively new to the field of generative modeling, learning costs has long been studied in control and reinforcement learning (RL) domains, typically for imitation learning from demonstrations. In these fields, learning cost function underlying observed behavior is known as inverse reinforcement learning (IRL) or inverse optimal control. While at first the connection between cost learning in RL and cost learning in generative modeling may appear to be a superficial one, we show in this paper that certain IRL methods are in fact mathematically equivalent to GANs. In particular, we demonstrate an equivalence between a sample-based algorithm for maximum entropy IRL and a GAN in which the generator's density can be evaluated and is provided as an additional input to the discriminator. Interestingly, maximum entropy IRL is a special case of an energy-based model. We discuss the interpretation of GANs as an algorithm for training energy-based models, and relate this interpretation to other recent work that seeks to connect GANs and EBMs. By formally highlighting the connection between GANs, IRL, and EBMs, we hope that researchers in all three communities can better identify and apply transferable ideas from one domain to another, particularly for developing more stable and scalable algorithms: a major challenge in all three domains.\n    ",
        "submission_date": "2016-11-11T00:00:00",
        "last_modified_date": "2016-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04021",
        "title": "Leveraging Video Descriptions to Learn Video Question Answering",
        "authors": [
            "Kuo-Hao Zeng",
            "Tseng-Hung Chen",
            "Ching-Yao Chuang",
            "Yuan-Hong Liao",
            "Juan Carlos Niebles",
            "Min Sun"
        ],
        "abstract": "We propose a scalable approach to learn video-based question answering (QA): answer a \"free-form natural language question\" about a video content. Our approach automatically harvests a large number of videos and descriptions freely available online. Then, a large number of candidate QA pairs are automatically generated from descriptions rather than manually annotated. Next, we use these candidate QA pairs to train a number of video-based QA methods extended fromMN (Sukhbaatar et al. 2015), VQA (Antol et al. 2015), SA (Yao et al. 2015), SS (Venugopalan et al. 2015). In order to handle non-perfect candidate QA pairs, we propose a self-paced learning procedure to iteratively identify them and mitigate their effects in training. Finally, we evaluate performance on manually generated video-based QA pairs. The results show that our self-paced learning procedure is effective, and the extended SS model outperforms various baselines.\n    ",
        "submission_date": "2016-11-12T00:00:00",
        "last_modified_date": "2016-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04175",
        "title": "Recognizing and Eliciting Weakly Single Crossing Profiles on Trees",
        "authors": [
            "Palash Dey"
        ],
        "abstract": "We introduce and study the weakly single-crossing domain on trees which is a generalization of the well-studied single-crossing domain in social choice theory. We design a polynomial-time algorithm for recognizing preference profiles which belong to this domain. We then develop an efficient elicitation algorithm for this domain which works even if the preferences can be accessed only sequentially and the underlying single-crossing tree structure is not known beforehand. We also prove matching lower bound on the query complexity of our elicitation algorithm when the number of voters is large compared to the number of candidates. We also prove a lower bound of $\\Omega(m^2\\log n)$ on the number of queries that any algorithm needs to ask to elicit single crossing profile when random queries are allowed. This resolves an open question in an earlier paper and proves optimality of their preference elicitation algorithm when random queries are allowed.\n    ",
        "submission_date": "2016-11-13T00:00:00",
        "last_modified_date": "2025-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04180",
        "title": "Learning to Gather Information via Imitation",
        "authors": [
            "Sanjiban Choudhury",
            "Ashish Kapoor",
            "Gireeja Ranade",
            "Debadeepta Dey"
        ],
        "abstract": "The budgeted information gathering problem - where a robot with a fixed fuel budget is required to maximize the amount of information gathered from the world - appears in practice across a wide range of applications in autonomous exploration and inspection with mobile robots. Although there is an extensive amount of prior work investigating effective approximations of the problem, these methods do not address the fact that their performance is heavily dependent on distribution of objects in the world. In this paper, we attempt to address this issue by proposing a novel data-driven imitation learning framework.\n",
        "submission_date": "2016-11-13T00:00:00",
        "last_modified_date": "2016-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04488",
        "title": "Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy",
        "authors": [
            "Danica J. Sutherland",
            "Hsiao-Yu Tung",
            "Heiko Strathmann",
            "Soumyajit De",
            "Aaditya Ramdas",
            "Alex Smola",
            "Arthur Gretton"
        ],
        "abstract": "We propose a method to optimize the representation and distinguishability of samples from two probability distributions, by maximizing the estimated power of a statistical test based on the maximum mean discrepancy (MMD). This optimized MMD is applied to the setting of unsupervised learning by generative adversarial networks (GAN), in which a model attempts to generate realistic samples, and a discriminator attempts to tell these apart from data samples. In this context, the MMD may be used in two roles: first, as a discriminator, either directly on the samples, or on features of the samples. Second, the MMD can be used to evaluate the performance of a generative model, by testing the model's samples against a reference data set. In the latter role, the optimized MMD is particularly helpful, as it gives an interpretable indication of how the model and data distributions differ, even in cases where individual model samples are not easily distinguished either by eye or by classifier.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2021-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04503",
        "title": "Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot",
        "authors": [
            "Hideki Nakayama",
            "Noriki Nishida"
        ],
        "abstract": "We propose an approach to build a neural machine translation system with no supervised resources (i.e., no parallel corpora) using multimodal embedded representation over texts and images. Based on the assumption that text documents are often likely to be described with other multimedia information (e.g., images) somewhat related to the content, we try to indirectly estimate the relevance between two languages. Using multimedia as the \"pivot\", we project all modalities into one common hidden space where samples belonging to similar semantic concepts should come close to each other, whatever the observed space of each sample is. This modality-agnostic representation is the key to bridging the gap between different modalities. Putting a decoder on top of it, our network can flexibly draw the outputs from any input modality. Notably, in the testing phase, we need only source language texts as the input for translation. In experiments, we tested our method on two benchmarks to show that it can achieve reasonable translation performance. We compared and investigated several possible implementations and found that an end-to-end model that simultaneously optimized both rank loss in multimodal encoders and cross-entropy loss in decoders performed the best.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2017-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04535",
        "title": "Learning-Theoretic Foundations of Algorithm Configuration for Combinatorial Partitioning Problems",
        "authors": [
            "Maria-Florina Balcan",
            "Vaishnavh Nagarajan",
            "Ellen Vitercik",
            "Colin White"
        ],
        "abstract": "Max-cut, clustering, and many other partitioning problems that are of significant importance to machine learning and other scientific fields are NP-hard, a reality that has motivated researchers to develop a wealth of approximation algorithms and heuristics. Although the best algorithm to use typically depends on the specific application domain, a worst-case analysis is often used to compare algorithms. This may be misleading if worst-case instances occur infrequently, and thus there is a demand for optimization methods which return the algorithm configuration best suited for the given application's typical inputs. We address this problem for clustering, max-cut, and other partitioning problems, such as integer quadratic programming, by designing computationally efficient and sample efficient learning algorithms which receive samples from an application-specific distribution over problem instances and learn a partitioning algorithm with high expected performance. Our algorithms learn over common integer quadratic programming and clustering algorithm families: SDP rounding algorithms and agglomerative clustering algorithms with dynamic programming. For our sample complexity analysis, we provide tight bounds on the pseudodimension of these algorithm classes, and show that surprisingly, even for classes of algorithms parameterized by a single parameter, the pseudo-dimension is superconstant. In this way, our work both contributes to the foundations of algorithm configuration and pushes the boundaries of learning theory, since the algorithm classes we analyze consist of multi-stage optimization procedures and are significantly more complex than classes typically studied in learning theory.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2018-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04558",
        "title": "Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation",
        "authors": [
            "Melvin Johnson",
            "Mike Schuster",
            "Quoc V. Le",
            "Maxim Krikun",
            "Yonghui Wu",
            "Zhifeng Chen",
            "Nikhil Thorat",
            "Fernanda Vi\u00e9gas",
            "Martin Wattenberg",
            "Greg Corrado",
            "Macduff Hughes",
            "Jeffrey Dean"
        ],
        "abstract": "We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English$\\rightarrow$French and surpasses state-of-the-art results for English$\\rightarrow$German. Similarly, a single multilingual model surpasses state-of-the-art results for French$\\rightarrow$English and German$\\rightarrow$English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2017-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04709",
        "title": "Recoverability of Joint Distribution from Missing Data",
        "authors": [
            "Jin Tian"
        ],
        "abstract": "A probabilistic query may not be estimable from observed data corrupted by missing values if the data are not missing at random (MAR). It is therefore of theoretical interest and practical importance to determine in principle whether a probabilistic query is estimable from missing data or not when the data are not MAR. We present an algorithm that systematically determines whether the joint probability is estimable from observed data with missing values, assuming that the data-generation model is represented as a Bayesian network containing unobserved latent variables that not only encodes the dependencies among the variables but also explicitly portrays the mechanisms responsible for the missingness process. The result significantly advances the existing work.\n    ",
        "submission_date": "2016-11-15T00:00:00",
        "last_modified_date": "2016-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04810",
        "title": "The NOESIS Network-Oriented Exploration, Simulation, and Induction System",
        "authors": [
            "V\u00edctor Mart\u00ednez",
            "Fernando Berzal",
            "Juan-Carlos Cubero"
        ],
        "abstract": "Network data mining has become an important area of study due to the large number of problems it can be applied to. This paper presents NOESIS, an open source framework for network data mining that provides a large collection of network analysis techniques, including the analysis of network structural properties, community detection methods, link scoring, and link prediction, as well as network visualization algorithms. It also features a complete stand-alone graphical user interface that facilitates the use of all these techniques. The NOESIS framework has been designed using solid object-oriented design principles and structured parallel programming. As a lightweight library with minimal external dependencies and a permissive software license, NOESIS can be incorporated into other software projects. Released under a BSD license, it is available from ",
        "submission_date": "2016-11-15T00:00:00",
        "last_modified_date": "2017-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05104",
        "title": "A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs",
        "authors": [
            "Shayne Longpre",
            "Sabeek Pradhan",
            "Caiming Xiong",
            "Richard Socher"
        ],
        "abstract": "LSTMs have become a basic building block for many deep NLP models. In recent years, many improvements and variations have been proposed for deep sequence models in general, and LSTMs in particular. We propose and analyze a series of augmentations and modifications to LSTM networks resulting in improved performance for text classification datasets. We observe compounding improvements on traditional LSTMs using Monte Carlo test-time model averaging, average pooling, and residual connections, along with four other suggested modifications. Our analysis provides a simple, reliable, and high quality baseline model.\n    ",
        "submission_date": "2016-11-16T00:00:00",
        "last_modified_date": "2016-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05187",
        "title": "Variable Neighborhood Search Algorithms for the multi-depot dial-a-ride problem with heterogeneous vehicles and users",
        "authors": [
            "Paolo Detti",
            "Garazi Zabalo Manrique de Lara"
        ],
        "abstract": "In this work, a study on Variable Neighborhood Search algorithms for multi-depot dial-a-ride problems is presented. In dial-a-ride problems patients need to be transported from pre-specified pickup locations to pre-specified delivery locations, under different considerations. The addressed problem presents several constraints and features, such as heterogeneous vehicles, distributed in different depots, and heterogeneous patients. The aim is of minimizing the total routing cost, while respecting time-window, ride-time, capacity and route duration constraints. The objective of the study is of determining the best algorithm configuration in terms of initial solution, neighborhood and local search procedures. At this aim, two different procedures for the computation of an initial solution, six different type of neighborhoods and five local search procedures, where only intra-route changes are made, have been considered and compared.\n",
        "submission_date": "2016-11-16T00:00:00",
        "last_modified_date": "2016-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05368",
        "title": "Neural Style Representations and the Large-Scale Classification of Artistic Style",
        "authors": [
            "Jeremiah Johnson"
        ],
        "abstract": "The artistic style of a painting is a subtle aesthetic judgment used by art historians for grouping and classifying artwork. The recently introduced `neural-style' algorithm substantially succeeds in merging the perceived artistic style of one image or set of images with the perceived content of another. In light of this and other recent developments in image analysis via convolutional neural networks, we investigate the effectiveness of a `neural-style' representation for classifying the artistic style of paintings.\n    ",
        "submission_date": "2016-11-16T00:00:00",
        "last_modified_date": "2016-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05416",
        "title": "Composing Music with Grammar Argumented Neural Networks and Note-Level Encoding",
        "authors": [
            "Zheng Sun",
            "Jiaqi Liu",
            "Zewang Zhang",
            "Jingwen Chen",
            "Zhao Huo",
            "Ching Hua Lee",
            "Xiao Zhang"
        ],
        "abstract": "Creating aesthetically pleasing pieces of art, including music, has been a long-term goal for artificial intelligence research. Despite recent successes of long-short term memory (LSTM) recurrent neural networks (RNNs) in sequential learning, LSTM neural networks have not, by themselves, been able to generate natural-sounding music conforming to music theory. To transcend this inadequacy, we put forward a novel method for music composition that combines the LSTM with Grammars motivated by music theory. The main tenets of music theory are encoded as grammar argumented (GA) filters on the training data, such that the machine can be trained to generate music inheriting the naturalness of human-composed pieces from the original dataset while adhering to the rules of music theory. Unlike previous approaches, pitches and durations are encoded as one semantic entity, which we refer to as note-level encoding. This allows easy implementation of music theory grammars, as well as closer emulation of the thinking pattern of a musician. Although the GA rules are applied to the training data and never directly to the LSTM music generation, our machine still composes music that possess high incidences of diatonic scale notes, small pitch intervals and chords, in deference to music theory.\n    ",
        "submission_date": "2016-11-16T00:00:00",
        "last_modified_date": "2016-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05546",
        "title": "Zero-Shot Visual Question Answering",
        "authors": [
            "Damien Teney",
            "Anton van den Hengel"
        ],
        "abstract": "Part of the appeal of Visual Question Answering (VQA) is its promise to answer new questions about previously unseen images. Most current methods demand training questions that illustrate every possible concept, and will therefore never achieve this capability, since the volume of required training data would be prohibitive. Answering general questions about images requires methods capable of Zero-Shot VQA, that is, methods able to answer questions beyond the scope of the training questions. We propose a new evaluation protocol for VQA methods which measures their ability to perform Zero-Shot VQA, and in doing so highlights significant practical deficiencies of current approaches, some of which are masked by the biases in current datasets. We propose and evaluate several strategies for achieving Zero-Shot VQA, including methods based on pretrained word embeddings, object classifiers with semantic embeddings, and test-time retrieval of example images. Our extensive experiments are intended to serve as baselines for Zero-Shot VQA, and they also achieve state-of-the-art performance in the standard VQA evaluation setting.\n    ",
        "submission_date": "2016-11-17T00:00:00",
        "last_modified_date": "2016-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05640",
        "title": "Stream Packing for Asynchronous Multi-Context Systems using ASP",
        "authors": [
            "Stefan Ellmauthaler",
            "J\u00f6rg P\u00fchrer"
        ],
        "abstract": "When a processing unit relies on data from external streams, we may face the problem that the stream data needs to be rearranged in a way that allows the unit to perform its task(s). On arrival of new data, we must decide whether there is sufficient information available to start processing or whether to wait for more data. Furthermore, we need to ensure that the data meets the input specification of the processing step. In the case of multiple input streams it is also necessary to coordinate which data from which incoming stream should form the input of the next process instantiation. In this work, we propose a declarative approach as an interface between multiple streams and a processing unit. The idea is to specify via answer-set programming how to arrange incoming data in packages that are suitable as input for subsequent processing. Our approach is intended for use in asynchronous multi-context systems (aMCSs), a recently proposed framework for loose coupling of knowledge representation formalisms that allows for online reasoning in a dynamic environment. Contexts in aMCSs process data streams from external sources and other contexts.\n    ",
        "submission_date": "2016-11-17T00:00:00",
        "last_modified_date": "2016-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05664",
        "title": "Learning to detect and localize many objects from few examples",
        "authors": [
            "Bastien Moysset",
            "Christoper Kermorvant",
            "Christian Wolf"
        ],
        "abstract": "The current trend in object detection and localization is to learn predictions with high capacity deep neural networks trained on a very large amount of annotated data and using a high amount of processing power. In this work, we propose a new neural model which directly predicts bounding box coordinates. The particularity of our contribution lies in the local computations of predictions with a new form of local parameter sharing which keeps the overall amount of trainable parameters low. Key components of the model are spatial 2D-LSTM recurrent layers which convey contextual information between the regions of the image. We show that this model is more powerful than the state of the art in applications where training data is not as abundant as in the classical configuration of natural images and Imagenet/Pascal VOC tasks. We particularly target the detection of text in document images, but our method is not limited to this setting. The proposed model also facilitates the detection of many objects in a single image and can deal with inputs of variable sizes without resizing.\n    ",
        "submission_date": "2016-11-17T00:00:00",
        "last_modified_date": "2016-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05675",
        "title": "Study on Feature Subspace of Archetypal Emotions for Speech Emotion Recognition",
        "authors": [
            "Xi Ma",
            "Zhiyong Wu",
            "Jia Jia",
            "Mingxing Xu",
            "Helen Meng",
            "Lianhong Cai"
        ],
        "abstract": "Feature subspace selection is an important part in speech emotion recognition. Most of the studies are devoted to finding a feature subspace for representing all emotions. However, some studies have indicated that the features associated with different emotions are not exactly the same. Hence, traditional methods may fail to distinguish some of the emotions with just one global feature subspace. In this work, we propose a new divide and conquer idea to solve the problem. First, the feature subspaces are constructed for all the combinations of every two different emotions (emotion-pair). Bi-classifiers are then trained on these feature subspaces respectively. The final emotion recognition result is derived by the voting and competition method. Experimental results demonstrate that the proposed method can get better results than the traditional multi-classification method.\n    ",
        "submission_date": "2016-11-17T00:00:00",
        "last_modified_date": "2016-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05763",
        "title": "Learning to reinforcement learn",
        "authors": [
            "Jane X Wang",
            "Zeb Kurth-Nelson",
            "Dhruva Tirumala",
            "Hubert Soyer",
            "Joel Z Leibo",
            "Remi Munos",
            "Charles Blundell",
            "Dharshan Kumaran",
            "Matt Botvinick"
        ],
        "abstract": "In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.\n    ",
        "submission_date": "2016-11-17T00:00:00",
        "last_modified_date": "2017-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05817",
        "title": "Nothing Else Matters: Model-Agnostic Explanations By Identifying Prediction Invariance",
        "authors": [
            "Marco Tulio Ribeiro",
            "Sameer Singh",
            "Carlos Guestrin"
        ],
        "abstract": "At the core of interpretable machine learning is the question of whether humans are able to make accurate predictions about a model's behavior. Assumed in this question are three properties of the interpretable output: coverage, precision, and effort. Coverage refers to how often humans think they can predict the model's behavior, precision to how accurate humans are in those predictions, and effort is either the up-front effort required in interpreting the model, or the effort required to make predictions about a model's behavior.\n",
        "submission_date": "2016-11-17T00:00:00",
        "last_modified_date": "2016-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05827",
        "title": "Towards a Mathematical Understanding of the Difficulty in Learning with Feedforward Neural Networks",
        "authors": [
            "Hao Shen"
        ],
        "abstract": "Training deep neural networks for solving machine learning problems is one great challenge in the field, mainly due to its associated optimisation problem being highly non-convex. Recent developments have suggested that many training algorithms do not suffer from undesired local minima under certain scenario, and consequently led to great efforts in pursuing mathematical explanations for such observations. This work provides an alternative mathematical understanding of the challenge from a smooth optimisation perspective. By assuming exact learning of finite samples, sufficient conditions are identified via a critical point analysis to ensure any local minimum to be globally minimal as well. Furthermore, a state of the art algorithm, known as the Generalised Gauss-Newton (GGN) algorithm, is rigorously revisited as an approximate Newton's algorithm, which shares the property of being locally quadratically convergent to a global minimum under the condition of exact learning.\n    ",
        "submission_date": "2016-11-17T00:00:00",
        "last_modified_date": "2017-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05896",
        "title": "Answering Image Riddles using Vision and Reasoning through Probabilistic Soft Logic",
        "authors": [
            "Somak Aditya",
            "Yezhou Yang",
            "Chitta Baral",
            "Yiannis Aloimonos"
        ],
        "abstract": "In this work, we explore a genre of puzzles (\"image riddles\") which involves a set of images and a question. Answering these puzzles require both capabilities involving visual detection (including object, activity recognition) and, knowledge-based or commonsense reasoning. We compile a dataset of over 3k riddles where each riddle consists of 4 images and a groundtruth answer. The annotations are validated using crowd-sourced evaluation. We also define an automatic evaluation metric to track future progress. Our task bears similarity with the commonly known IQ tasks such as analogy solving, sequence filling that are often used to test intelligence.\n",
        "submission_date": "2016-11-17T00:00:00",
        "last_modified_date": "2016-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05990",
        "title": "Monte Carlo Tableau Proof Search",
        "authors": [
            "Michael F\u00e4rber",
            "Cezary Kaliszyk",
            "Josef Urban"
        ],
        "abstract": "We study Monte Carlo Tree Search to guide proof search in tableau calculi. This includes proposing a number of proof-state evaluation heuristics, some of which are learnt from previous proofs. We present an implementation based on the leanCoP prover. The system is trained and evaluated on a large suite of related problems coming from the Mizar proof assistant, showing that it is capable to find new and different proofs.\n    ",
        "submission_date": "2016-11-18T00:00:00",
        "last_modified_date": "2019-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06086",
        "title": "Swarm Intelligence for Multiobjective Optimization of Extraction Process",
        "authors": [
            "T. Ganesan",
            "I. Elamvazuthi",
            "P.Vasant"
        ],
        "abstract": "Multi objective (MO) optimization is an emerging field which is increasingly being implemented in many industries globally. In this work, the MO optimization of the extraction process of bioactive compounds from the Gardenia Jasminoides Ellis fruit was solved. Three swarm-based algorithms have been applied in conjunction with normal-boundary intersection (NBI) method to solve this MO problem. The gravitational search algorithm (GSA) and the particle swarm optimization (PSO) technique were implemented in this work. In addition, a novel Hopfield-enhanced particle swarm optimization was developed and applied to the extraction problem. By measuring the levels of dominance, the optimality of the approximate Pareto frontiers produced by all the algorithms were gauged and compared. Besides, by measuring the levels of convergence of the frontier, some understanding regarding the structure of the objective space in terms of its relation to the level of frontier dominance is uncovered. Detail comparative studies were conducted on all the algorithms employed and developed in this work.\n    ",
        "submission_date": "2016-09-30T00:00:00",
        "last_modified_date": "2016-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06132",
        "title": "Faster variational inducing input Gaussian process classification",
        "authors": [
            "Pavel Izmailov",
            "Dmitry Kropotov"
        ],
        "abstract": "Gaussian processes (GP) provide a prior over functions and allow finding complex regularities in data. Gaussian processes are successfully used for classification/regression problems and dimensionality reduction. In this work we consider the classification problem only. The complexity of standard methods for GP-classification scales cubically with the size of the training dataset. This complexity makes them inapplicable to big data problems. Therefore, a variety of methods were introduced to overcome this limitation. In the paper we focus on methods based on so called inducing inputs. This approach is based on variational inference and proposes a particular lower bound for marginal likelihood (evidence). This bound is then maximized w.r.t. parameters of kernel function of the Gaussian process, thus fitting the model to data. The computational complexity of this method is $O(nm^2)$, where $m$ is the number of inducing inputs used by the model and is assumed to be substantially smaller than the size of the dataset $n$. Recently, a new evidence lower bound for GP-classification problem was introduced. It allows using stochastic optimization, which makes it suitable for big data problems. However, the new lower bound depends on $O(m^2)$ variational parameter, which makes optimization challenging in case of big m. In this work we develop a new approach for training inducing input GP models for classification problems. Here we use quadratic approximation of several terms in the aforementioned evidence lower bound, obtaining analytical expressions for optimal values of most of the parameters in the optimization, thus sufficiently reducing the dimension of optimization space. In our experiments we achieve as well or better results, compared to the existing method. Moreover, our method doesn't require the user to manually set the learning rate, making it more practical, than the existing method.\n    ",
        "submission_date": "2016-11-18T00:00:00",
        "last_modified_date": "2016-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06175",
        "title": "Learning Interpretability for Visualizations using Adapted Cox Models through a User Experiment",
        "authors": [
            "Adrien Bibal",
            "Benoit Fr\u00e9nay"
        ],
        "abstract": "In order to be useful, visualizations need to be interpretable. This paper uses a user-based approach to combine and assess quality measures in order to better model user preferences. Results show that cluster separability measures are outperformed by a neighborhood conservation measure, even though the former are usually considered as intuitively representative of user motives. Moreover, combining measures, as opposed to using a single measure, further improves prediction performances.\n    ",
        "submission_date": "2016-11-18T00:00:00",
        "last_modified_date": "2016-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06188",
        "title": "Variable Computation in Recurrent Neural Networks",
        "authors": [
            "Yacine Jernite",
            "Edouard Grave",
            "Armand Joulin",
            "Tomas Mikolov"
        ],
        "abstract": "Recurrent neural networks (RNNs) have been used extensively and with increasing success to model various types of sequential data. Much of this progress has been achieved through devising recurrent units and architectures with the flexibility to capture complex statistics in the data, such as long range dependency or localized attention phenomena. However, while many sequential data (such as video, speech or language) can have highly variable information flow, most recurrent models still consume input features at a constant rate and perform a constant number of computations per time step, which can be detrimental to both speed and model capacity. In this paper, we explore a modification to existing recurrent units which allows them to learn to vary the amount of computation they perform at each step, without prior knowledge of the sequence's time structure. We show experimentally that not only do our models require fewer operations, they also lead to better performance overall on evaluation tasks.\n    ",
        "submission_date": "2016-11-18T00:00:00",
        "last_modified_date": "2017-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06189",
        "title": "Query Complexity of Tournament Solutions",
        "authors": [
            "Arnab Maiti",
            "Palash Dey"
        ],
        "abstract": "A directed graph where there is exactly one edge between every pair of vertices is called a {\\em tournament}. Finding the \"best\" set of vertices of a tournament is a well studied problem in social choice theory. A {\\em tournament solution} takes a tournament as input and outputs a subset of vertices of the input tournament. However, in many applications, for example, choosing the best set of drugs from a given set of drugs, the edges of the tournament are given only implicitly and knowing the orientation of an edge is costly. In such scenarios, we would like to know the best set of vertices (according to some tournament solution) by \"querying\" as few edges as possible. We, in this paper, precisely study this problem for commonly used tournament solutions: given an oracle access to the edges of a tournament T, find $f(T)$ by querying as few edges as possible, for a tournament solution f. We first show that the set of Condorcet non-losers in a tournament can be found by querying $2n-\\lfloor \\log n \\rfloor -2$ edges only and this is tight in the sense that every algorithm for finding the set of Condorcet non-losers needs to query at least $2n-\\lfloor \\log n \\rfloor -2$ edges in the worst case, where $n$ is the number of vertices in the input tournament. We then move on to study other popular tournament solutions and show that any algorithm for finding the Copeland set, the Slater set, the Markov set, the bipartisan set, the uncovered set, the Banks set, and the top cycle must query $\\Omega(n^2)$ edges in the worst case. On the positive side, we are able to circumvent our strong query complexity lower bound results by proving that, if the size of the top cycle of the input tournament is at most $k$, then we can find all the tournament solutions mentioned above by querying $O(nk + \\frac{n\\log n}{\\log(1-\\frac{1}{k})})$ edges only.\n    ",
        "submission_date": "2016-11-18T00:00:00",
        "last_modified_date": "2024-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06194",
        "title": "Expert Gate: Lifelong Learning with a Network of Experts",
        "authors": [
            "Rahaf Aljundi",
            "Punarjay Chakravarty",
            "Tinne Tuytelaars"
        ],
        "abstract": "In this paper we introduce a model of lifelong learning, based on a Network of Experts. New tasks / experts are learned and added to the model sequentially, building on what was learned before. To ensure scalability of this process,data from previous tasks cannot be stored and hence is not available when learning a new task. A critical issue in such context, not addressed in the literature so far, relates to the decision which expert to deploy at test time. We introduce a set of gating autoencoders that learn a representation for the task at hand, and, at test time, automatically forward the test sample to the relevant expert. This also brings memory efficiency as only one expert network has to be loaded into memory at any given time. Further, the autoencoders inherently capture the relatedness of one task to another, based on which the most relevant prior model to be used for training a new expert, with finetuning or learning without-forgetting, can be selected. We evaluate our method on image classification and video prediction problems.\n    ",
        "submission_date": "2016-11-18T00:00:00",
        "last_modified_date": "2017-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06216",
        "title": "Generative Deep Neural Networks for Dialogue: A Short Review",
        "authors": [
            "Iulian Vlad Serban",
            "Ryan Lowe",
            "Laurent Charlin",
            "Joelle Pineau"
        ],
        "abstract": "Researchers have recently started investigating deep neural networks for dialogue applications. In particular, generative sequence-to-sequence (Seq2Seq) models have shown promising results for unstructured tasks, such as word-level dialogue response generation. The hope is that such models will be able to leverage massive amounts of data to learn meaningful natural language representations and response generation strategies, while requiring a minimum amount of domain knowledge and hand-crafting. An important challenge is to develop models that can effectively incorporate dialogue context and generate meaningful and diverse responses. In support of this goal, we review recently proposed models based on generative encoder-decoder neural network architectures, and show that these models have better ability to incorporate long-term dialogue history, to model uncertainty and ambiguity in dialogue, and to generate responses with high-level compositional structure.\n    ",
        "submission_date": "2016-11-18T00:00:00",
        "last_modified_date": "2016-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06221",
        "title": "Foundations of Structural Causal Models with Cycles and Latent Variables",
        "authors": [
            "Stephan Bongers",
            "Patrick Forr\u00e9",
            "Jonas Peters",
            "Joris M. Mooij"
        ],
        "abstract": "Structural causal models (SCMs), also known as (nonparametric) structural equation models (SEMs), are widely used for causal modeling purposes. In particular, acyclic SCMs, also known as recursive SEMs, form a well-studied subclass of SCMs that generalize causal Bayesian networks to allow for latent confounders. In this paper, we investigate SCMs in a more general setting, allowing for the presence of both latent confounders and cycles. We show that in the presence of cycles, many of the convenient properties of acyclic SCMs do not hold in general: they do not always have a solution; they do not always induce unique observational, interventional and counterfactual distributions; a marginalization does not always exist, and if it exists the marginal model does not always respect the latent projection; they do not always satisfy a Markov property; and their graphs are not always consistent with their causal semantics. We prove that for SCMs in general each of these properties does hold under certain solvability conditions. Our work generalizes results for SCMs with cycles that were only known for certain special cases so far. We introduce the class of simple SCMs that extends the class of acyclic SCMs to the cyclic setting, while preserving many of the convenient properties of acyclic SCMs. With this paper we aim to provide the foundations for a general theory of statistical causal modeling with SCMs.\n    ",
        "submission_date": "2016-11-18T00:00:00",
        "last_modified_date": "2021-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06355",
        "title": "Invertible Conditional GANs for image editing",
        "authors": [
            "Guim Perarnau",
            "Joost van de Weijer",
            "Bogdan Raducanu",
            "Jose M. \u00c1lvarez"
        ],
        "abstract": "Generative Adversarial Networks (GANs) have recently demonstrated to successfully approximate complex data distributions. A relevant extension of this model is conditional GANs (cGANs), where the introduction of external information allows to determine specific representations of the generated images. In this work, we evaluate encoders to inverse the mapping of a cGAN, i.e., mapping a real image into a latent space and a conditional representation. This allows, for example, to reconstruct and modify real images of faces conditioning on arbitrary attributes. Additionally, we evaluate the design of cGANs. The combination of an encoder with a cGAN, which we call Invertible cGAN (IcGAN), enables to re-generate real images with deterministic complex modifications.\n    ",
        "submission_date": "2016-11-19T00:00:00",
        "last_modified_date": "2016-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06439",
        "title": "A Survey of Credit Card Fraud Detection Techniques: Data and Technique Oriented Perspective",
        "authors": [
            "SamanehSorournejad",
            "Zahra Zojaji",
            "Reza Ebrahimi Atani",
            "Amir Hassan Monadjemi"
        ],
        "abstract": "Credit card plays a very important rule in today's economy. It becomes an unavoidable part of household, business and global activities. Although using credit cards provides enormous benefits when used carefully and responsibly,significant credit and financial damages may be caused by fraudulent activities. Many techniques have been proposed to confront the growth in credit card fraud. However, all of these techniques have the same goal of avoiding the credit card fraud; each one has its own drawbacks, advantages and characteristics. In this paper, after investigating difficulties of credit card fraud detection, we seek to review the state of the art in credit card fraud detection techniques, data sets and evaluation ",
        "submission_date": "2016-11-19T00:00:00",
        "last_modified_date": "2016-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06589",
        "title": "Fair Division via Social Comparison",
        "authors": [
            "Rediet Abebe",
            "Jon Kleinberg",
            "David Parkes"
        ],
        "abstract": "In the classical cake cutting problem, a resource must be divided among agents with different utilities so that each agent believes they have received a fair share of the resource relative to the other agents. We introduce a variant of the problem in which we model an underlying social network on the agents with a graph, and agents only evaluate their shares relative to their neighbors' in the network. This formulation captures many situations in which it is unrealistic to assume a global view, and also exposes interesting phenomena in the original problem.\n",
        "submission_date": "2016-11-20T00:00:00",
        "last_modified_date": "2018-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06757",
        "title": "Non-Local Color Image Denoising with Convolutional Neural Networks",
        "authors": [
            "Stamatios Lefkimmiatis"
        ],
        "abstract": "We propose a novel deep network architecture for grayscale and color image denoising that is based on a non-local image model. Our motivation for the overall design of the proposed network stems from variational methods that exploit the inherent non-local self-similarity property of natural images. We build on this concept and introduce deep networks that perform non-local processing and at the same time they significantly benefit from discriminative learning. Experiments on the Berkeley segmentation dataset, comparing several state-of-the-art methods, show that the proposed non-local models achieve the best reported denoising performance both for grayscale and color images for all the tested noise levels. It is also worth noting that this increase in performance comes at no extra cost on the capacity of the network compared to existing alternative deep network architectures. In addition, we highlight a direct link of the proposed non-local models to convolutional neural networks. This connection is of significant importance since it allows our models to take full advantage of the latest advances on GPU computing in deep learning and makes them amenable to efficient implementations through their inherent parallelism.\n    ",
        "submission_date": "2016-11-21T00:00:00",
        "last_modified_date": "2017-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06791",
        "title": "Generalized Dropout",
        "authors": [
            "Suraj Srinivas",
            "R. Venkatesh Babu"
        ],
        "abstract": "Deep Neural Networks often require good regularizers to generalize well. Dropout is one such regularizer that is widely used among Deep Learning practitioners. Recent work has shown that Dropout can also be viewed as performing Approximate Bayesian Inference over the network parameters. In this work, we generalize this notion and introduce a rich family of regularizers which we call Generalized Dropout. One set of methods in this family, called Dropout++, is a version of Dropout with trainable parameters. Classical Dropout emerges as a special case of this method. Another member of this family selects the width of neural network layers. Experiments show that these methods help in improving generalization performance over Dropout.\n    ",
        "submission_date": "2016-11-21T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06824",
        "title": "Options Discovery with Budgeted Reinforcement Learning",
        "authors": [
            "Aur\u00e9lia L\u00e9on",
            "Ludovic Denoyer"
        ],
        "abstract": "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.\n    ",
        "submission_date": "2016-11-21T00:00:00",
        "last_modified_date": "2017-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06882",
        "title": "Learning From Graph Neighborhoods Using LSTMs",
        "authors": [
            "Rakshit Agrawal",
            "Luca de Alfaro",
            "Vassilis Polychronopoulos"
        ],
        "abstract": "Many prediction problems can be phrased as inferences over local neighborhoods of graphs. The graph represents the interaction between entities, and the neighborhood of each entity contains information that allows the inferences or predictions. We present an approach for applying machine learning directly to such graph neighborhoods, yielding predicitons for graph nodes on the basis of the structure of their local neighborhood and the features of the nodes in it. Our approach allows predictions to be learned directly from examples, bypassing the step of creating and tuning an inference model or summarizing the neighborhoods via a fixed set of hand-crafted features. The approach is based on a multi-level architecture built from Long Short-Term Memory neural nets (LSTMs); the LSTMs learn how to summarize the neighborhood from data. We demonstrate the effectiveness of the proposed technique on a synthetic example and on real-world data related to crowdsourced grading, Bitcoin transactions, and Wikipedia edit reversions.\n    ",
        "submission_date": "2016-11-21T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06951",
        "title": "Enforcing Relational Matching Dependencies with Datalog for Entity Resolution",
        "authors": [
            "Zeinab Bahmani",
            "Leopoldo Bertossi"
        ],
        "abstract": "Entity resolution (ER) is about identifying and merging records in a database that represent the same real-world entity. Matching dependencies (MDs) have been introduced and investigated as declarative rules that specify ER policies. An ER process induced by MDs over a dirty instance leads to multiple clean instances, in general. General \"answer sets programs\" have been proposed to specify the MD-based cleaning task and its results. In this work, we extend MDs to \"relational MDs\", which capture more application semantics, and identify classes of relational MDs for which the general ASP can be automatically rewritten into a stratified Datalog program, with the single clean instance as its standard model.\n    ",
        "submission_date": "2016-11-21T00:00:00",
        "last_modified_date": "2017-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06953",
        "title": "Associative Adversarial Networks",
        "authors": [
            "Tarik Arici",
            "Asli Celikyilmaz"
        ],
        "abstract": "We propose a higher-level associative memory for learning adversarial networks. Generative adversarial network (GAN) framework has a discriminator and a generator network. The generator (G) maps white noise (z) to data samples while the discriminator (D) maps data samples to a single scalar. To do so, G learns how to map from high-level representation space to data space, and D learns to do the opposite. We argue that higher-level representation spaces need not necessarily follow a uniform probability distribution. In this work, we use Restricted Boltzmann Machines (RBMs) as a higher-level associative memory and learn the probability distribution for the high-level features generated by D. The associative memory samples its underlying probability distribution and G learns how to map these samples to data space. The proposed associative adversarial networks (AANs) are generative models in the higher-levels of the learning, and use adversarial non-stochastic models D and G for learning the mapping between data and higher-level representation spaces. Experiments show the potential of the proposed networks.\n    ",
        "submission_date": "2016-11-18T00:00:00",
        "last_modified_date": "2016-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06997",
        "title": "Coherent Dialogue with Attention-based Language Models",
        "authors": [
            "Hongyuan Mei",
            "Mohit Bansal",
            "Matthew R. Walter"
        ],
        "abstract": "We model coherent conversation continuation via RNN-based dialogue models equipped with a dynamic attention mechanism. Our attention-RNN language model dynamically increases the scope of attention on the history as the conversation continues, as opposed to standard attention (or alignment) models with a fixed input scope in a sequence-to-sequence model. This allows each generated word to be associated with the most relevant words in its corresponding conversation history. We evaluate the model on two popular dialogue datasets, the open-domain MovieTriples dataset and the closed-domain Ubuntu Troubleshoot dataset, and achieve significant improvements over the state-of-the-art and baselines on several metrics, including complementary diversity-based metrics, human evaluation, and qualitative visualizations. We also show that a vanilla RNN with dynamic attention outperforms more complex memory models (e.g., LSTM and GRU) by allowing for flexible, long-distance memory. We promote further coherence via topic modeling-based reranking.\n    ",
        "submission_date": "2016-11-21T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07054",
        "title": "An Efficient Training Algorithm for Kernel Survival Support Vector Machines",
        "authors": [
            "Sebastian P\u00f6lsterl",
            "Nassir Navab",
            "Amin Katouzian"
        ],
        "abstract": "Survival analysis is a fundamental tool in medical research to identify predictors of adverse events and develop systems for clinical decision support. In order to leverage large amounts of patient data, efficient optimisation routines are paramount. We propose an efficient training algorithm for the kernel survival support vector machine (SSVM). We directly optimise the primal objective function and employ truncated Newton optimisation and order statistic trees to significantly lower computational costs compared to previous training algorithms, which require $O(n^4)$ space and $O(p n^6)$ time for datasets with $n$ samples and $p$ features. Our results demonstrate that our proposed optimisation scheme allows analysing data of a much larger scale with no loss in prediction performance. Experiments on synthetic and 5 real-world datasets show that our technique outperforms existing kernel SSVM formulations if the amount of right censoring is high ($\\geq85\\%$), and performs comparably otherwise.\n    ",
        "submission_date": "2016-11-21T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07100",
        "title": "Interpreting Finite Automata for Sequential Data",
        "authors": [
            "Christian Albert Hammerschmidt",
            "Sicco Verwer",
            "Qin Lin",
            "Radu State"
        ],
        "abstract": "Automaton models are often seen as interpretable models. Interpretability itself is not well defined: it remains unclear what interpretability means without first explicitly specifying objectives or desired attributes. In this paper, we identify the key properties used to interpret automata and propose a modification of a state-merging approach to learn variants of finite state automata. We apply the approach to problems beyond typical grammar inference tasks. Additionally, we cover several use-cases for prediction, classification, and clustering on sequential data in both supervised and unsupervised scenarios to show how the identified key properties are applicable in a wide range of contexts.\n    ",
        "submission_date": "2016-11-21T00:00:00",
        "last_modified_date": "2016-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07233",
        "title": "CAS-CNN: A Deep Convolutional Neural Network for Image Compression Artifact Suppression",
        "authors": [
            "Lukas Cavigelli",
            "Pascal Hager",
            "Luca Benini"
        ],
        "abstract": "Lossy image compression algorithms are pervasively used to reduce the size of images transmitted over the web and recorded on data storage media. However, we pay for their high compression rate with visual artifacts degrading the user experience. Deep convolutional neural networks have become a widespread tool to address high-level computer vision tasks very successfully. Recently, they have found their way into the areas of low-level computer vision and image processing to solve regression problems mostly with relatively shallow networks.\n",
        "submission_date": "2016-11-22T00:00:00",
        "last_modified_date": "2016-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07343",
        "title": "Limbo: A Fast and Flexible Library for Bayesian Optimization",
        "authors": [
            "Antoine Cully",
            "Konstantinos Chatzilygeroudis",
            "Federico Allocati",
            "Jean-Baptiste Mouret"
        ],
        "abstract": "Limbo is an open-source C++11 library for Bayesian optimization which is designed to be both highly flexible and very fast. It can be used to optimize functions for which the gradient is unknown, evaluations are expensive, and runtime cost matters (e.g., on embedded systems or robots). Benchmarks on standard functions show that Limbo is about 2 times faster than BayesOpt (another C++ library) for a similar accuracy.\n    ",
        "submission_date": "2016-11-22T00:00:00",
        "last_modified_date": "2016-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07379",
        "title": "Randomized Mechanisms for Selling Reserved Instances in Cloud",
        "authors": [
            "Jia Zhang",
            "Weidong Ma",
            "Tao Qin",
            "Xiaoming Sun",
            "Tie-Yan Liu"
        ],
        "abstract": "Selling reserved instances (or virtual machines) is a basic service in cloud computing. In this paper, we consider a more flexible pricing model for instance reservation, in which a customer can propose the time length and number of resources of her request, while in today's industry, customers can only choose from several predefined reservation packages. Under this model, we design randomized mechanisms for customers coming online to optimize social welfare and providers' revenue.\n",
        "submission_date": "2016-11-22T00:00:00",
        "last_modified_date": "2016-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07422",
        "title": "Deep Learning Approximation for Stochastic Control Problems",
        "authors": [
            "Jiequn Han",
            "Weinan E"
        ],
        "abstract": "Many real world stochastic control problems suffer from the \"curse of dimensionality\". To overcome this difficulty, we develop a deep learning approach that directly solves high-dimensional stochastic control problems based on Monte-Carlo sampling. We approximate the time-dependent controls as feedforward neural networks and stack these networks together through model dynamics. The objective function for the control problem plays the role of the loss function for the deep neural network. We test this approach using examples from the areas of optimal trading and energy storage. Our results suggest that the algorithm presented here achieves satisfactory accuracy and at the same time, can handle rather high dimensional problems.\n    ",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07507",
        "title": "Variational Intrinsic Control",
        "authors": [
            "Karol Gregor",
            "Danilo Jimenez Rezende",
            "Daan Wierstra"
        ],
        "abstract": "In this paper we introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. The algorithms also provide an explicit measure of empowerment in a given state that can be used by an empowerment maximizing agent. The algorithm scales well with function approximation and we demonstrate the applicability of the algorithm on a range of tasks.\n    ",
        "submission_date": "2016-11-22T00:00:00",
        "last_modified_date": "2016-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07579",
        "title": "Programs as Black-Box Explanations",
        "authors": [
            "Sameer Singh",
            "Marco Tulio Ribeiro",
            "Carlos Guestrin"
        ],
        "abstract": "Recent work in model-agnostic explanations of black-box machine learning has demonstrated that interpretability of complex models does not have to come at the cost of accuracy or model flexibility. However, it is not clear what kind of explanations, such as linear models, decision trees, and rule lists, are the appropriate family to consider, and different tasks and models may benefit from different kinds of explanations. Instead of picking a single family of representations, in this work we propose to use \"programs\" as model-agnostic explanations. We show that small programs can be expressive yet intuitive as explanations, and generalize over a number of existing interpretable families. We propose a prototype program induction method based on simulated annealing that approximates the local behavior of black-box classifiers around a specific prediction using random perturbations. Finally, we present preliminary application on small datasets and show that the generated explanations are intuitive and accurate for a number of classifiers.\n    ",
        "submission_date": "2016-11-22T00:00:00",
        "last_modified_date": "2016-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07599",
        "title": "Efficient Delivery Policy to Minimize User Traffic Consumption in Guaranteed Advertising",
        "authors": [
            "Jia Zhang",
            "Zheng Wang",
            "Qian Li",
            "Jialin Zhang",
            "Yanyan Lan",
            "Qiang Li",
            "Xiaoming Sun"
        ],
        "abstract": "In this work, we study the guaranteed delivery model which is widely used in online display advertising. In the guaranteed delivery scenario, ad exposures (which are also called impressions in some works) to users are guaranteed by contracts signed in advance between advertisers and publishers. A crucial problem for the advertising platform is how to fully utilize the valuable user traffic to generate as much as possible revenue.\n",
        "submission_date": "2016-11-23T00:00:00",
        "last_modified_date": "2016-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07941",
        "title": "Multi-Modal Mean-Fields via Cardinality-Based Clamping",
        "authors": [
            "Pierre Baqu\u00e9",
            "Fran\u00e7ois Fleuret",
            "Pascal Fua"
        ],
        "abstract": "Mean Field inference is central to statistical physics. It has attracted much interest in the Computer Vision community to efficiently solve problems expressible in terms of large Conditional Random Fields. However, since it models the posterior probability distribution as a product of marginal probabilities, it may fail to properly account for important dependencies between variables. We therefore replace the fully factorized distribution of Mean Field by a weighted mixture of such distributions, that similarly minimizes the KL-Divergence to the true posterior. By introducing two new ideas, namely, conditioning on groups of variables instead of single ones and using a parameter of the conditional random field potentials, that we identify to the temperature in the sense of statistical physics to select such groups, we can perform this minimization efficiently. Our extension of the clamping method proposed in previous works allows us to both produce a more descriptive approximation of the true posterior and, inspired by the diverse MAP paradigms, fit a mixture of Mean Field approximations. We demonstrate that this positively impacts real-world algorithms that initially relied on mean fields.\n    ",
        "submission_date": "2016-11-23T00:00:00",
        "last_modified_date": "2016-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08070",
        "title": "Multiscale Inverse Reinforcement Learning using Diffusion Wavelets",
        "authors": [
            "Jung-Su Ha",
            "Han-Lim Choi"
        ],
        "abstract": "This work presents a multiscale framework to solve an inverse reinforcement learning (IRL) problem for continuous-time/state stochastic systems. We take advantage of a diffusion wavelet representation of the associated Markov chain to abstract the state space. This not only allows for effectively handling the large (and geometrically complex) decision space but also provides more interpretable representations of the demonstrated state trajectories and also of the resulting policy of IRL. In the proposed framework, the problem is divided into the global and local IRL, where the global approximation of the optimal value functions are obtained using coarse features and the local details are quantified using fine local features. An illustrative numerical example on robot path control in a complex environment is presented to verify the proposed method.\n    ",
        "submission_date": "2016-11-24T00:00:00",
        "last_modified_date": "2016-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08104",
        "title": "Quantum Enhanced Inference in Markov Logic Networks",
        "authors": [
            "Peter Wittek",
            "Christian Gogolin"
        ],
        "abstract": "Markov logic networks (MLNs) reconcile two opposing schools in machine learning and artificial intelligence: causal networks, which account for uncertainty extremely well, and first-order logic, which allows for formal deduction. An MLN is essentially a first-order logic template to generate Markov networks. Inference in MLNs is probabilistic and it is often performed by approximate methods such as Markov chain Monte Carlo (MCMC) Gibbs sampling. An MLN has many regular, symmetric structures that can be exploited at both first-order level and in the generated Markov network. We analyze the graph structures that are produced by various lifting methods and investigate the extent to which quantum protocols can be used to speed up Gibbs sampling with state preparation and measurement schemes. We review different such approaches, discuss their advantages, theoretical limitations, and their appeal to implementations. We find that a straightforward application of a recent result yields exponential speedup compared to classical heuristics in approximate probabilistic inference, thereby demonstrating another example where advanced quantum resources can potentially prove useful in machine learning.\n    ",
        "submission_date": "2016-11-24T00:00:00",
        "last_modified_date": "2016-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08307",
        "title": "Learning Python Code Suggestion with a Sparse Pointer Network",
        "authors": [
            "Avishkar Bhoopchand",
            "Tim Rockt\u00e4schel",
            "Earl Barr",
            "Sebastian Riedel"
        ],
        "abstract": "To enhance developer productivity, all modern integrated development environments (IDEs) include code suggestion functionality that proposes likely next tokens at the cursor. While current IDEs work well for statically-typed languages, their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages. Moreover, suggestion engines in modern IDEs do not propose expressions or multi-statement idiomatic code. Recent work has shown that language models can improve code suggestion systems by learning from software repositories. This paper introduces a neural language model with a sparse pointer network aimed at capturing very long-range dependencies. We release a large-scale code suggestion corpus of 41M lines of Python code crawled from GitHub. On this corpus, we found standard neural language models to perform well at suggesting local phenomena, but struggle to refer to identifiers that are introduced many tokens in the past. By augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers, we obtain a much lower perplexity and a 5 percentage points increase in accuracy for code suggestion compared to an LSTM baseline. In fact, this increase in code suggestion accuracy is due to a 13 times more accurate prediction of identifiers. Furthermore, a qualitative analysis shows this model indeed captures interesting long-range dependencies, like referring to a class member defined over 60 tokens in the past.\n    ",
        "submission_date": "2016-11-24T00:00:00",
        "last_modified_date": "2016-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08366",
        "title": "Local Discriminant Hyperalignment for multi-subject fMRI data alignment",
        "authors": [
            "Muhammad Yousefnezhad",
            "Daoqiang Zhang"
        ],
        "abstract": "Multivariate Pattern (MVP) classification can map different cognitive states to the brain tasks. One of the main challenges in MVP analysis is validating the generated results across subjects. However, analyzing multi-subject fMRI data requires accurate functional alignments between neuronal activities of different subjects, which can rapidly increase the performance and robustness of the final results. Hyperalignment (HA) is one of the most effective functional alignment methods, which can be mathematically formulated by the Canonical Correlation Analysis (CCA) methods. Since HA mostly uses the unsupervised CCA techniques, its solution may not be optimized for MVP analysis. By incorporating the idea of Local Discriminant Analysis (LDA) into CCA, this paper proposes Local Discriminant Hyperalignment (LDHA) as a novel supervised HA method, which can provide better functional alignment for MVP analysis. Indeed, the locality is defined based on the stimuli categories in the train-set, where the correlation between all stimuli in the same category will be maximized and the correlation between distinct categories of stimuli approaches to near zero. Experimental studies on multi-subject MVP analysis confirm that the LDHA method achieves superior performance to other state-of-the-art HA algorithms.\n    ",
        "submission_date": "2016-11-25T00:00:00",
        "last_modified_date": "2016-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08657",
        "title": "Convolutional Experts Constrained Local Model for Facial Landmark Detection",
        "authors": [
            "Amir Zadeh",
            "Tadas Baltru\u0161aitis",
            "Louis-Philippe Morency"
        ],
        "abstract": "Constrained Local Models (CLMs) are a well-established family of methods for facial landmark detection. However, they have recently fallen out of favor to cascaded regression-based approaches. This is in part due to the inability of existing CLM local detectors to model the very complex individual landmark appearance that is affected by expression, illumination, facial hair, makeup, and accessories. In our work, we present a novel local detector -- Convolutional Experts Network (CEN) -- that brings together the advantages of neural architectures and mixtures of experts in an end-to-end framework. We further propose a Convolutional Experts Constrained Local Model (CE-CLM) algorithm that uses CEN as local detectors. We demonstrate that our proposed CE-CLM algorithm outperforms competitive state-of-the-art baselines for facial landmark detection by a large margin on four publicly-available datasets. Our approach is especially accurate and robust on challenging profile images.\n    ",
        "submission_date": "2016-11-26T00:00:00",
        "last_modified_date": "2017-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08666",
        "title": "Training an Interactive Humanoid Robot Using Multimodal Deep Reinforcement Learning",
        "authors": [
            "Heriberto Cuay\u00e1huitl",
            "Guillaume Couly",
            "Cl\u00e9ment Olalainty"
        ],
        "abstract": "Training robots to perceive, act and communicate using multiple modalities still represents a challenging problem, particularly if robots are expected to learn efficiently from small sets of example interactions. We describe a learning approach as a step in this direction, where we teach a humanoid robot how to play the game of noughts and crosses. Given that multiple multimodal skills can be trained to play this game, we focus our attention to training the robot to perceive the game, and to interact in this game. Our multimodal deep reinforcement learning agent perceives multimodal features and exhibits verbal and non-verbal actions while playing. Experimental results using simulations show that the robot can learn to win or draw up to 98% of the games. A pilot test of the proposed multimodal system for the targeted game---integrating speech, vision and gestures---reports that reasonable and fluent interactions can be achieved using the proposed approach.\n    ",
        "submission_date": "2016-11-26T00:00:00",
        "last_modified_date": "2016-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08669",
        "title": "Visual Dialog",
        "authors": [
            "Abhishek Das",
            "Satwik Kottur",
            "Khushi Gupta",
            "Avi Singh",
            "Deshraj Yadav",
            "Jos\u00e9 M. F. Moura",
            "Devi Parikh",
            "Dhruv Batra"
        ],
        "abstract": "We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the agent has to ground the question in image, infer context from history, and answer the question accurately. Visual Dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being grounded in vision enough to allow objective evaluation of individual responses and benchmark progress. We develop a novel two-person chat data-collection protocol to curate a large-scale Visual Dialog dataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10 question-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog question-answer pairs.\n",
        "submission_date": "2016-11-26T00:00:00",
        "last_modified_date": "2017-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08691",
        "title": "Multiwinner Approval Rules as Apportionment Methods",
        "authors": [
            "Markus Brill",
            "Jean-Fran\u00e7ois Laslier",
            "Piotr Skowron"
        ],
        "abstract": "We establish a link between multiwinner elections and apportionment problems by showing how approval-based multiwinner election rules can be interpreted as methods of apportionment. We consider several multiwinner rules and observe that they induce apportionment methods that are well-established in the literature on proportional representation. For instance, we show that Proportional Approval Voting induces the D'Hondt method and that Monroe's rule induces the largest reminder method. We also consider properties of apportionment methods and exhibit multiwinner rules that induce apportionment methods satisfying these properties.\n    ",
        "submission_date": "2016-11-26T00:00:00",
        "last_modified_date": "2021-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08733",
        "title": "BliStrTune: Hierarchical Invention of Theorem Proving Strategies",
        "authors": [
            "Jan Jakubuv",
            "Josef Urban"
        ],
        "abstract": "Inventing targeted proof search strategies for specific problem sets is a difficult task. State-of-the-art automated theorem provers (ATPs) such as E allow a large number of user-specified proof search strategies described in a rich domain specific language. Several machine learning methods that invent strategies automatically for ATPs were proposed previously. One of them is the Blind Strategymaker (BliStr), a system for automated invention of ATP strategies.\n",
        "submission_date": "2016-11-26T00:00:00",
        "last_modified_date": "2016-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08788",
        "title": "SAD-GAN: Synthetic Autonomous Driving using Generative Adversarial Networks",
        "authors": [
            "Arna Ghosh",
            "Biswarup Bhattacharya",
            "Somnath Basu Roy Chowdhury"
        ],
        "abstract": "Autonomous driving is one of the most recent topics of interest which is aimed at replicating human driving behavior keeping in mind the safety issues. We approach the problem of learning synthetic driving using generative neural networks. The main idea is to make a controller trainer network using images plus key press data to mimic human learning. We used the architecture of a stable GAN to make predictions between driving scenes using key presses. We train our model on one video game (Road Rash) and tested the accuracy and compared it by running the model on other maps in Road Rash to determine the extent of learning.\n    ",
        "submission_date": "2016-11-27T00:00:00",
        "last_modified_date": "2016-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08789",
        "title": "Handwriting Profiling using Generative Adversarial Networks",
        "authors": [
            "Arna Ghosh",
            "Biswarup Bhattacharya",
            "Somnath Basu Roy Chowdhury"
        ],
        "abstract": "Handwriting is a skill learned by humans from a very early age. The ability to develop one's own unique handwriting as well as mimic another person's handwriting is a task learned by the brain with practice. This paper deals with this very problem where an intelligent system tries to learn the handwriting of an entity using Generative Adversarial Networks (GANs). We propose a modified architecture of DCGAN (Radford, Metz, and Chintala 2015) to achieve this. We also discuss about applying reinforcement learning techniques to achieve faster learning. Our algorithm hopes to give new insights in this area and its uses include identification of forged documents, signature verification, computer generated art, digitization of documents among others. Our early implementation of the algorithm illustrates a good performance with MNIST datasets.\n    ",
        "submission_date": "2016-11-27T00:00:00",
        "last_modified_date": "2016-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08998",
        "title": "DeepSetNet: Predicting Sets with Deep Neural Networks",
        "authors": [
            "S. Hamid Rezatofighi",
            "Vijay Kumar B G",
            "Anton Milan",
            "Ehsan Abbasnejad",
            "Anthony Dick",
            "Ian Reid"
        ],
        "abstract": "This paper addresses the task of set prediction using deep learning. This is important because the output of many computer vision tasks, including image tagging and object detection, are naturally expressed as sets of entities rather than vectors. As opposed to a vector, the size of a set is not fixed in advance, and it is invariant to the ordering of entities within it. We define a likelihood for a set distribution and learn its parameters using a deep neural network. We also derive a loss for predicting a discrete distribution corresponding to set cardinality. Set prediction is demonstrated on the problem of multi-class image classification. Moreover, we show that the proposed cardinality loss can also trivially be applied to the tasks of object counting and pedestrian detection. Our approach outperforms existing methods in all three cases on standard datasets.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2017-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09028",
        "title": "Analyzing Features for the Detection of Happy Endings in German Novels",
        "authors": [
            "Fotis Jannidis",
            "Isabella Reger",
            "Albin Zehe",
            "Martin Becker",
            "Lena Hettinger",
            "Andreas Hotho"
        ],
        "abstract": "With regard to a computational representation of literary plot, this paper looks at the use of sentiment analysis for happy ending detection in German novels. Its focus lies on the investigation of previously proposed sentiment features in order to gain insight about the relevance of specific features on the one hand and the implications of their performance on the other hand. Therefore, we study various partitionings of novels, considering the highly variable concept of \"ending\". We also show that our approach, even though still rather simple, can potentially lead to substantial findings relevant to literary studies.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2016-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09212",
        "title": "Towards a new quantum cognition model",
        "authors": [
            "Riccardo Franco"
        ],
        "abstract": "This article presents a new quantum-like model for cognition explicitly based on knowledge. It is shown that this model, called QKT (quantum knowledge-based theory), is able to coherently describe some experimental results that are problematic for the prior quantum-like decision models. In particular, I consider the experimental results relevant to the post-decision cognitive dissonance, the problems relevant to the question order effect and response replicability, and those relevant to the grand-reciprocity equations. A new set of postulates is proposed, which evidence the different meaning given to the projectors and to the quantum states. In the final part, I show that the use of quantum gates can help to better describe and understand the evolution of quantum-like models.\n    ",
        "submission_date": "2016-11-23T00:00:00",
        "last_modified_date": "2016-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09321",
        "title": "Improving Policy Gradient by Exploring Under-appreciated Rewards",
        "authors": [
            "Ofir Nachum",
            "Mohammad Norouzi",
            "Dale Schuurmans"
        ],
        "abstract": "This paper presents a novel form of policy gradient for model-free reinforcement learning (RL) with improved exploration properties. Current policy-based methods use entropy regularization to encourage undirected exploration of the reward landscape, which is ineffective in high dimensional spaces with sparse rewards. We propose a more directed exploration strategy that promotes exploration of under-appreciated reward regions. An action sequence is considered under-appreciated if its log-probability under the current policy under-estimates its resulting reward. The proposed exploration strategy is easy to implement, requiring small modifications to an implementation of the REINFORCE algorithm. We evaluate the approach on a set of algorithmic tasks that have long challenged RL methods. Our approach reduces hyper-parameter sensitivity and demonstrates significant improvements over baseline methods. Our algorithm successfully solves a benchmark multi-digit addition task and generalizes to long sequences. This is, to our knowledge, the first time that a pure RL method has solved addition using only reward feedback.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2017-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09414",
        "title": "Split-door criterion: Identification of causal effects through auxiliary outcomes",
        "authors": [
            "Amit Sharma",
            "Jake M. Hofman",
            "Duncan J. Watts"
        ],
        "abstract": "We present a method for estimating causal effects in time series data when fine-grained information about the outcome of interest is available. Specifically, we examine what we call the split-door setting, where the outcome variable can be split into two parts: one that is potentially affected by the cause being studied and another that is independent of it, with both parts sharing the same (unobserved) confounders. We show that under these conditions, the problem of identification reduces to that of testing for independence among observed variables, and present a method that uses this approach to automatically find subsets of the data that are causally identified. We demonstrate the method by estimating the causal impact of Amazon's recommender system on traffic to product pages, finding thousands of examples within the dataset that satisfy the split-door criterion. Unlike past studies based on natural experiments that were limited to a single product category, our method applies to a large and representative sample of products viewed on the site. In line with previous work, we find that the widely-used click-through rate (CTR) metric overestimates the causal impact of recommender systems; depending on the product category, we estimate that 50-80\\% of the traffic attributed to recommender systems would have happened even without any recommendations. We conclude with guidelines for using the split-door criterion as well as a discussion of other contexts where the method can be applied.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2018-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09430",
        "title": "Emergence of foveal image sampling from learning to attend in visual scenes",
        "authors": [
            "Brian Cheung",
            "Eric Weiss",
            "Bruno Olshausen"
        ],
        "abstract": "We describe a neural attention model with a learnable retinal sampling lattice. The model is trained on a visual search task requiring the classification of an object embedded in a visual scene amidst background distractors using the smallest number of fixations. We explore the tiling properties that emerge in the model's retinal sampling lattice after training. Specifically, we show that this lattice resembles the eccentricity dependent sampling lattice of the primate retina, with a high resolution region in the fovea surrounded by a low resolution periphery. Furthermore, we find conditions where these emergent properties are amplified or eliminated providing clues to their function.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2017-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09474",
        "title": "Maximizing Non-Monotone DR-Submodular Functions with Cardinality Constraints",
        "authors": [
            "Ali Khodabakhsh",
            "Evdokia Nikolova"
        ],
        "abstract": "We consider the problem of maximizing a non-monotone DR-submodular function subject to a cardinality constraint. Diminishing returns (DR) submodularity is a generalization of the diminishing returns property for functions defined over the integer lattice. This generalization can be used to solve many machine learning or combinatorial optimization problems such as optimal budget allocation, revenue maximization, etc. In this work we propose the first polynomial-time approximation algorithms for non-monotone constrained maximization. We implement our algorithms for a revenue maximization problem with a real-world dataset to check their efficiency and performance.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2017-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09526",
        "title": "Learning Filter Banks Using Deep Learning For Acoustic Signals",
        "authors": [
            "Shuhui Qu",
            "Juncheng Li",
            "Wei Dai",
            "Samarjit Das"
        ],
        "abstract": "Designing appropriate features for acoustic event recognition tasks is an active field of research. Expressive features should both improve the performance of the tasks and also be interpret-able. Currently, heuristically designed features based on the domain knowledge requires tremendous effort in hand-crafting, while features extracted through deep network are difficult for human to interpret. In this work, we explore the experience guided learning method for designing acoustic features. This is a novel hybrid approach combining both domain knowledge and purely data driven feature designing. Based on the procedure of log Mel-filter banks, we design a filter bank learning layer. We concatenate this layer with a convolutional neural network (CNN) model. After training the network, the weight of the filter bank learning layer is extracted to facilitate the design of acoustic features. We smooth the trained weight of the learning layer and re-initialize it in filter bank learning layer as audio feature extractor. For the environmental sound recognition task based on the Urban- sound8K dataset, the experience guided learning leads to a 2% accuracy improvement compared with the fixed feature extractors (the log Mel-filter bank). The shape of the new filter banks are visualized and explained to prove the effectiveness of the feature design process.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09666",
        "title": "Generic and Efficient Solution Solves the Shortest Paths Problem in Square Runtime",
        "authors": [
            "Yong Tan"
        ],
        "abstract": "We study a group of new methods to solve an open problem that is the shortest paths problem on a given fix-weighted instance. It is the real significance at a considerable altitude to reach our aim to meet these qualities of generic, efficiency, precision which we generally require to a methodology. Besides our proof to guarantee our measures might work normally, we pay more interest to root out the vital theory about calculation and logic in favor of our extension to range over a wide field about decision, operator, economy, management, robot, AI and etc.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09703",
        "title": "Semantic Parsing of Mathematics by Context-based Learning from Aligned Corpora and Theorem Proving",
        "authors": [
            "Cezary Kaliszyk",
            "Josef Urban",
            "Ji\u0159\u00ed Vysko\u010dil"
        ],
        "abstract": "We study methods for automated parsing of informal mathematical expressions into formal ones, a main prerequisite for deep computer understanding of informal mathematical texts. We propose a context-based parsing approach that combines efficient statistical learning of deep parse trees with their semantic pruning by type checking and large-theory automated theorem proving. We show that the methods very significantly improve on previous results in parsing theorems from the Flyspeck corpus.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09755",
        "title": "Fractional Order AGC for Distributed Energy Resources Using Robust Optimization",
        "authors": [
            "Indranil Pan",
            "Saptarshi Das"
        ],
        "abstract": "The applicability of fractional order (FO) automatic generation control (AGC) for power system frequency oscillation damping is investigated in this paper, employing distributed energy generation. The hybrid power system employs various autonomous generation systems like wind turbine, solar photovoltaic, diesel engine, fuel-cell and aqua electrolyzer along with other energy storage devices like the battery and flywheel. The controller is placed in a remote location while receiving and sending signals over an unreliable communication network with stochastic delay. The controller parameters are tuned using robust optimization techniques employing different variants of Particle Swarm Optimization (PSO) and are compared with the corresponding optimal solutions. An archival based strategy is used for reducing the number of function evaluations for the robust optimization methods. The solutions obtained through the robust optimization are able to handle higher variation in the controller gains and orders without significant decrease in the system performance. This is desirable from the FO controller implementation point of view, as the design is able to accommodate variations in the system parameter which may result due to the approximation of FO operators, using different realization methods and order of accuracy. Also a comparison is made between the FO and the integer order (IO) controllers to highlight the merits and demerits of each scheme.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09809",
        "title": "Fractional Order Fuzzy Control of Hybrid Power System with Renewable Generation Using Chaotic PSO",
        "authors": [
            "Indranil Pan",
            "Saptarshi Das"
        ],
        "abstract": "This paper investigates the operation of a hybrid power system through a novel fuzzy control scheme. The hybrid power system employs various autonomous generation systems like wind turbine, solar photovoltaic, diesel engine, fuel-cell, aqua electrolyzer etc. Other energy storage devices like the battery, flywheel and ultra-capacitor are also present in the network. A novel fractional order (FO) fuzzy control scheme is employed and its parameters are tuned with a particle swarm optimization (PSO) algorithm augmented with two chaotic maps for achieving an improved performance. This FO fuzzy controller shows better performance over the classical PID, and the integer order fuzzy PID controller in both linear and nonlinear operating regimes. The FO fuzzy controller also shows stronger robustness properties against system parameter variation and rate constraint nonlinearity, than that with the other controller structures. The robustness is a highly desirable property in such a scenario since many components of the hybrid power system may be switched on/off or may run at lower/higher power output, at different time instants.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09819",
        "title": "Measuring and modeling the perception of natural and unconstrained gaze in humans and machines",
        "authors": [
            "Daniel Harari",
            "Tao Gao",
            "Nancy Kanwisher",
            "Joshua Tenenbaum",
            "Shimon Ullman"
        ],
        "abstract": "Humans are remarkably adept at interpreting the gaze direction of other individuals in their surroundings. This skill is at the core of the ability to engage in joint visual attention, which is essential for establishing social interactions. How accurate are humans in determining the gaze direction of others in lifelike scenes, when they can move their heads and eyes freely, and what are the sources of information for the underlying perceptual processes? These questions pose a challenge from both empirical and computational perspectives, due to the complexity of the visual input in real-life situations. Here we measure empirically human accuracy in perceiving the gaze direction of others in lifelike scenes, and study computationally the sources of information and representations underlying this cognitive capacity. We show that humans perform better in face-to-face conditions compared with recorded conditions, and that this advantage is not due to the availability of input dynamics. We further show that humans are still performing well when only the eyes-region is visible, rather than the whole face. We develop a computational model, which replicates the pattern of human performance, including the finding that the eyes-region contains on its own, the required information for estimating both head orientation and direction of gaze. Consistent with neurophysiological findings on task-specific face regions in the brain, the learned computational representations reproduce perceptual effects such as the Wollaston illusion, when trained to estimate direction of gaze, but not when trained to recognize objects or faces.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09830",
        "title": "NewsQA: A Machine Comprehension Dataset",
        "authors": [
            "Adam Trischler",
            "Tong Wang",
            "Xingdi Yuan",
            "Justin Harris",
            "Alessandro Sordoni",
            "Philip Bachman",
            "Kaheer Suleman"
        ],
        "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text from the corresponding articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (0.198 in F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2017-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09913",
        "title": "Capacity and Trainability in Recurrent Neural Networks",
        "authors": [
            "Jasmine Collins",
            "Jascha Sohl-Dickstein",
            "David Sussillo"
        ],
        "abstract": "Two potential bottlenecks on the expressiveness of recurrent neural networks (RNNs) are their ability to store information about the task in their parameters, and to store information about the input history in their units. We show experimentally that all common RNN architectures achieve nearly the same per-task and per-unit capacity bounds with careful training, for a variety of tasks and stacking depths. They can store an amount of task information which is linear in the number of parameters, and is approximately 5 bits per parameter. They can additionally store approximately one real number from their input history per hidden unit. We further find that for several tasks it is the per-task parameter capacity bound that determines performance. These results suggest that many previous results comparing RNN architectures are driven primarily by differences in training effectiveness, rather than differences in capacity. Supporting this observation, we compare training difficulty for several architectures, and show that vanilla RNNs are far more difficult to train, yet have slightly higher capacity. Finally, we propose two novel RNN architectures, one of which is easier to train than the LSTM or GRU for deeply stacked architectures.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2017-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09926",
        "title": "Choquet integral in decision analysis - lessons from the axiomatization",
        "authors": [
            "Mikhail Timonin"
        ],
        "abstract": "The Choquet integral is a powerful aggregation operator which lists many well-known models as its special cases. We look at these special cases and provide their axiomatic analysis. In cases where an axiomatization has been previously given in the literature, we connect the existing results with the framework that we have developed. Next we turn to the question of learning, which is especially important for the practical applications of the model. So far, learning of the Choquet integral has been mostly confined to the learning of the capacity. Such an approach requires making a powerful assumption that all dimensions (e.g. criteria) are evaluated on the same scale, which is rarely justified in practice. Too often categorical data is given arbitrary numerical labels (e.g. AHP), and numerical data is considered cardinally and ordinally commensurate, sometimes after a simple normalization. Such approaches clearly lack scientific rigour, and yet they are commonly seen in all kinds of applications. We discuss the pros and cons of making such an assumption and look at the consequences which axiomatization uniqueness results have for the learning problems. Finally, we review some of the applications of the Choquet integral in decision analysis. Apart from MCDA, which is the main area of interest for our results, we also discuss how the model can be interpreted in the social choice context. We look in detail at the state-dependent utility, and show how comonotonicity, central to the previous axiomatizations, actually implies state-independency in the Choquet integral model. We also discuss the conditions required to have a meaningful state-dependent utility representation and show the novelty of our results compared to the previous methods of building state-dependent models.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.10215",
        "title": "Unit Commitment using Nearest Neighbor as a Short-Term Proxy",
        "authors": [
            "Gal Dalal",
            "Elad Gilboa",
            "Shie Mannor",
            "Louis Wehenkel"
        ],
        "abstract": "We devise the Unit Commitment Nearest Neighbor (UCNN) algorithm to be used as a proxy for quickly approximating outcomes of short-term decisions, to make tractable hierarchical long-term assessment and planning for large power systems. Experimental results on updated versions of IEEE-RTS79 and IEEE-RTS96 show high accuracy measured on operational cost, achieved in runtimes that are lower in several orders of magnitude than the traditional approach.\n    ",
        "submission_date": "2016-11-30T00:00:00",
        "last_modified_date": "2018-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.10252",
        "title": "SeDMiD for Confusion Detection: Uncovering Mind State from Time Series Brain Wave Data",
        "authors": [
            "Jingkang Yang",
            "Haohan Wang",
            "Jun Zhu",
            "Eric P. Xing"
        ],
        "abstract": "Understanding how brain functions has been an intriguing topic for years. With the recent progress on collecting massive data and developing advanced technology, people have become interested in addressing the challenge of decoding brain wave data into meaningful mind states, with many machine learning models and algorithms being revisited and developed, especially the ones that handle time series data because of the nature of brain waves. However, many of these time series models, like HMM with hidden state in discrete space or State Space Model with hidden state in continuous space, only work with one source of data and cannot handle different sources of information simultaneously. In this paper, we propose an extension of State Space Model to work with different sources of information together with its learning and inference algorithms. We apply this model to decode the mind state of students during lectures based on their brain waves and reach a significant better results compared to traditional methods.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.10328",
        "title": "The observer-assisted method for adjusting hyper-parameters in deep learning algorithms",
        "authors": [
            "Maciej Wielgosz"
        ],
        "abstract": "This paper presents a concept of a novel method for adjusting hyper-parameters in Deep Learning (DL) algorithms. An external agent-observer monitors a performance of a selected Deep Learning algorithm. The observer learns to model the DL algorithm using a series of random experiments. Consequently, it may be used for predicting a response of the DL algorithm in terms of a selected quality measurement to a set of hyper-parameters. This allows to construct an ensemble composed of a series of evaluators which constitute an observer-assisted architecture. The architecture may be used to gradually iterate towards to the best achievable quality score in tiny steps governed by a unit of progress. The algorithm is stopped when the maximum number of steps is reached or no further progress is made.\n    ",
        "submission_date": "2016-11-30T00:00:00",
        "last_modified_date": "2016-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.10331",
        "title": "Comparing Apples and Oranges: Two Examples of the Limits of Statistical Inference, With an Application to Google Advertising Markets",
        "authors": [
            "John Mount",
            "Nina Zumel"
        ],
        "abstract": "We show how the classic Cramer-Rao bound limits how accurately one can simultaneously estimate values of a large number of Google Ad campaigns (or similarly limit the measurement rate of many confounding A/B tests).\n    ",
        "submission_date": "2016-11-30T00:00:00",
        "last_modified_date": "2016-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.10351",
        "title": "Joint Causal Inference from Multiple Contexts",
        "authors": [
            "Joris M. Mooij",
            "Sara Magliacane",
            "Tom Claassen"
        ],
        "abstract": "The gold standard for discovering causal relations is by means of experimentation. Over the last decades, alternative methods have been proposed that can infer causal relations between variables from certain statistical patterns in purely observational data. We introduce Joint Causal Inference (JCI), a novel approach to causal discovery from multiple data sets from different contexts that elegantly unifies both approaches. JCI is a causal modeling framework rather than a specific algorithm, and it can be implemented using any causal discovery algorithm that can take into account certain background knowledge. JCI can deal with different types of interventions (e.g., perfect, imperfect, stochastic, etc.) in a unified fashion, and does not require knowledge of intervention targets or types in case of interventional data. We explain how several well-known causal discovery algorithms can be seen as addressing special cases of the JCI framework, and we also propose novel implementations that extend existing causal discovery methods for purely observational data to the JCI setting. We evaluate different JCI implementations on synthetic data and on flow cytometry protein expression data and conclude that JCI implementations can considerably outperform state-of-the-art causal discovery algorithms.\n    ",
        "submission_date": "2016-11-30T00:00:00",
        "last_modified_date": "2020-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00108",
        "title": "When to Reset Your Keys: Optimal Timing of Security Updates via Learning",
        "authors": [
            "Zizhan Zheng",
            "Ness B. Shroff",
            "Prasant Mohapatra"
        ],
        "abstract": "Cybersecurity is increasingly threatened by advanced and persistent attacks. As these attacks are often designed to disable a system (or a critical resource, e.g., a user account) repeatedly, it is crucial for the defender to keep updating its security measures to strike a balance between the risk of being compromised and the cost of security updates. Moreover, these decisions often need to be made with limited and delayed feedback due to the stealthy nature of advanced attacks. In addition to targeted attacks, such an optimal timing policy under incomplete information has broad applications in cybersecurity. Examples include key rotation, password change, application of patches, and virtual machine refreshing. However, rigorous studies of optimal timing are rare. Further, existing solutions typically rely on a pre-defined attack model that is known to the defender, which is often not the case in practice. In this work, we make an initial effort towards achieving optimal timing of security updates in the face of unknown stealthy attacks. We consider a variant of the influential FlipIt game model with asymmetric feedback and unknown attack time distribution, which provides a general model to consecutive security updates. The defender's problem is then modeled as a time associative bandit problem with dependent arms. We derive upper confidence bound based learning policies that achieve low regret compared with optimal periodic defense strategies that can only be derived when attack time distributions are known.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2016-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00132",
        "title": "CDVAE: Co-embedding Deep Variational Auto Encoder for Conditional Variational Generation",
        "authors": [
            "Jiajun Lu",
            "Aditya Deshpande",
            "David Forsyth"
        ],
        "abstract": "Problems such as predicting a new shading field (Y) for an image (X) are ambiguous: many very distinct solutions are good. Representing this ambiguity requires building a conditional model P(Y|X) of the prediction, conditioned on the image. Such a model is difficult to train, because we do not usually have training data containing many different shadings for the same image. As a result, we need different training examples to share data to produce good models. This presents a danger we call \"code space collapse\" - the training procedure produces a model that has a very good loss score, but which represents the conditional distribution poorly. We demonstrate an improved method for building conditional models by exploiting a metric constraint on training data that prevents code space collapse. We demonstrate our model on two example tasks using real data: image saturation adjustment, image relighting. We describe quantitative metrics to evaluate ambiguous generation results. Our results quantitatively and qualitatively outperform different strong baselines.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2017-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00203",
        "title": "Analysis of the Human-Computer Interaction on the Example of Image-based CAPTCHA by Association Rule Mining",
        "authors": [
            "Darko Brodi\u0107",
            "Alessia Amelio"
        ],
        "abstract": "The paper analyzes the interaction between humans and computers in terms of response time in solving the image-based CAPTCHA. In particular, the analysis focuses on the attitude of the different Internet users in easily solving four different types of image-based CAPTCHAs which include facial expressions like: animated character, old woman, surprised face, worried face. To pursue this goal, an experiment is realized involving 100 Internet users in solving the four types of CAPTCHAs, differentiated by age, Internet experience, and education level. The response times are collected for each user. Then, association rules are extracted from user data, for evaluating the dependence of the response time in solving the CAPTCHA from age, education level and experience in internet usage by statistical analysis. The results implicitly capture the users' psychological states showing in what states the users are more sensible. It reveals to be a novelty and a meaningful analysis in the state-of-the-art.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2016-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00347",
        "title": "Bootstrapping incremental dialogue systems: using linguistic knowledge to learn from minimal data",
        "authors": [
            "Dimitrios Kalatzis",
            "Arash Eshghi",
            "Oliver Lemon"
        ],
        "abstract": "We present a method for inducing new dialogue systems from very small amounts of unannotated dialogue data, showing how word-level exploration using Reinforcement Learning (RL), combined with an incremental and semantic grammar - Dynamic Syntax (DS) - allows systems to discover, generate, and understand many new dialogue variants. The method avoids the use of expensive and time-consuming dialogue act annotations, and supports more natural (incremental) dialogues than turn-based systems. Here, language generation and dialogue management are treated as a joint decision/optimisation problem, and the MDP model for RL is constructed automatically. With an implemented system, we show that this method enables a wide range of dialogue variations to be automatically captured, even when the system is trained from only a single dialogue. The variants include question-answer pairs, over- and under-answering, self- and other-corrections, clarification interaction, split-utterances, and ellipsis. This generalisation property results from the structural knowledge and constraints present within the DS grammar, and highlights some limitations of recent systems built using machine learning techniques only.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2016-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00367",
        "title": "Large-scale Validation of Counterfactual Learning Methods: A Test-Bed",
        "authors": [
            "Damien Lefortier",
            "Adith Swaminathan",
            "Xiaotao Gu",
            "Thorsten Joachims",
            "Maarten de Rijke"
        ],
        "abstract": "The ability to perform effective off-policy learning would revolutionize the process of building better interactive systems, such as search engines and recommendation systems for e-commerce, computational advertising and news. Recent approaches for off-policy evaluation and learning in these settings appear promising. With this paper, we provide real-world data and a standardized test-bed to systematically investigate these algorithms using data from display advertising. In particular, we consider the problem of filling a banner ad with an aggregate of multiple products the user may want to purchase. This paper presents our test-bed, the sanity checks we ran to ensure its validity, and shows results comparing state-of-the-art off-policy learning methods like doubly robust optimization, POEM, and reductions to supervised learning using regression baselines. Our results show experimental evidence that recent off-policy learning methods can improve upon state-of-the-art supervised learning techniques on a large-scale real-world data set.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2017-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00377",
        "title": "Piecewise Latent Variables for Neural Variational Text Processing",
        "authors": [
            "Iulian V. Serban",
            "Alexander G. Ororbia II",
            "Joelle Pineau",
            "Aaron Courville"
        ],
        "abstract": "Advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables, such as variational autoencoders. The hope is that such models will learn to represent rich, multi-modal latent factors in real-world data, such as natural language text. However, current models often assume simplistic priors on the latent variables - such as the uni-modal Gaussian distribution - which are incapable of representing complex latent factors efficiently. To overcome this restriction, we propose the simple, but highly flexible, piecewise constant distribution. This distribution has the capacity to represent an exponential number of modes of a latent target distribution, while remaining mathematically tractable. Our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and natural language generation for dialogue.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2017-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00429",
        "title": "Generalizing Skills with Semi-Supervised Reinforcement Learning",
        "authors": [
            "Chelsea Finn",
            "Tianhe Yu",
            "Justin Fu",
            "Pieter Abbeel",
            "Sergey Levine"
        ],
        "abstract": "Deep reinforcement learning (RL) can acquire complex behaviors from low-level inputs, such as images. However, real-world applications of such methods require generalizing to the vast variability of the real world. Deep networks are known to achieve remarkable generalization when provided with massive amounts of labeled data, but can we provide this breadth of experience to an RL agent, such as a robot? The robot might continuously learn as it explores the world around it, even while deployed. However, this learning requires access to a reward function, which is often hard to measure in real-world domains, where the reward could depend on, for example, unknown positions of objects or the emotional state of the user. Conversely, it is often quite practical to provide the agent with reward functions in a limited set of situations, such as when a human supervisor is present or in a controlled setting. Can we make use of this limited supervision, and still benefit from the breadth of experience an agent might collect on its own? In this paper, we formalize this problem as semisupervised reinforcement learning, where the reward function can only be evaluated in a set of \"labeled\" MDPs, and the agent must generalize its behavior to the wide range of states it might encounter in a set of \"unlabeled\" MDPs, by using experience from both settings. Our proposed method infers the task objective in the unlabeled MDPs through an algorithm that resembles inverse RL, using the agent's own prior experience in the labeled MDPs as a kind of demonstration of optimal behavior. We evaluate our method on challenging tasks that require control directly from images, and show that our approach can improve the generalization of a learned deep neural network policy by using experience for which no reward function is available. We also show that our method outperforms direct supervised learning of the reward.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2017-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00475",
        "title": "Transfer Learning Across Patient Variations with Hidden Parameter Markov Decision Processes",
        "authors": [
            "Taylor Killian",
            "George Konidaris",
            "Finale Doshi-Velez"
        ],
        "abstract": "Due to physiological variation, patients diagnosed with the same condition may exhibit divergent, but related, responses to the same treatments. Hidden Parameter Markov Decision Processes (HiP-MDPs) tackle this transfer-learning problem by embedding these tasks into a low-dimensional space. However, the original formulation of HiP-MDP had a critical flaw: the embedding uncertainty was modeled independently of the agent's state uncertainty, requiring an unnatural training procedure in which all tasks visited every part of the state space---possible for robots that can be moved to a particular location, impossible for human patients. We update the HiP-MDP framework and extend it to more robustly develop personalized medicine strategies for HIV treatment.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2016-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00563",
        "title": "Self-critical Sequence Training for Image Captioning",
        "authors": [
            "Steven J. Rennie",
            "Etienne Marcheret",
            "Youssef Mroueh",
            "Jarret Ross",
            "Vaibhava Goel"
        ],
        "abstract": "Recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep end-to-end systems directly on non-differentiable metrics for the task at hand. In this paper we consider the problem of optimizing image captioning systems using reinforcement learning, and show that by carefully optimizing our systems using the test metrics of the MSCOCO task, significant gains in performance can be realized. Our systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a \"baseline\" to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we find that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 114.7.\n    ",
        "submission_date": "2016-12-02T00:00:00",
        "last_modified_date": "2017-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00583",
        "title": "Active Search for Sparse Signals with Region Sensing",
        "authors": [
            "Yifei Ma",
            "Roman Garnett",
            "Jeff Schneider"
        ],
        "abstract": "Autonomous systems can be used to search for sparse signals in a large space; e.g., aerial robots can be deployed to localize threats, detect gas leaks, or respond to distress calls. Intuitively, search algorithms may increase efficiency by collecting aggregate measurements summarizing large contiguous regions. However, most existing search methods either ignore the possibility of such region observations (e.g., Bayesian optimization and multi-armed bandits) or make strong assumptions about the sensing mechanism that allow each measurement to arbitrarily encode all signals in the entire environment (e.g., compressive sensing). We propose an algorithm that actively collects data to search for sparse signals using only noisy measurements of the average values on rectangular regions (including single points), based on the greedy maximization of information gain. We analyze our algorithm in 1d and show that it requires $\\tilde{O}(\\frac{n}{\\mu^2}+k^2)$ measurements to recover all of $k$ signal locations with small Bayes error, where $\\mu$ and $n$ are the signal strength and the size of the search space, respectively. We also show that active designs can be fundamentally more efficient than passive designs with region sensing, contrasting with the results of Arias-Castro, Candes, and Davenport (2013). We demonstrate the empirical performance of our algorithm on a search problem using satellite image data and in high dimensions.\n    ",
        "submission_date": "2016-12-02T00:00:00",
        "last_modified_date": "2016-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00653",
        "title": "Inferring Cognitive Models from Data using Approximate Bayesian Computation",
        "authors": [
            "Antti Kangasr\u00e4\u00e4si\u00f6",
            "Kumaripaba Athukorala",
            "Andrew Howes",
            "Jukka Corander",
            "Samuel Kaski",
            "Antti Oulasvirta"
        ],
        "abstract": "An important problem for HCI researchers is to estimate the parameter values of a cognitive model from behavioral data. This is a difficult problem, because of the substantial complexity and variety in human behavioral strategies. We report an investigation into a new approach using approximate Bayesian computation (ABC) to condition model parameters to data and prior knowledge. As the case study we examine menu interaction, where we have click time data only to infer a cognitive model that implements a search behaviour with parameters such as fixation duration and recall probability. Our results demonstrate that ABC (i) improves estimates of model parameter values, (ii) enables meaningful comparisons between model variants, and (iii) supports fitting models to individual users. ABC provides ample opportunities for theoretical HCI research by allowing principled inference of model parameter values and their uncertainty.\n    ",
        "submission_date": "2016-12-02T00:00:00",
        "last_modified_date": "2017-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00712",
        "title": "Probabilistic Neural Programs",
        "authors": [
            "Kenton W. Murray",
            "Jayant Krishnamurthy"
        ],
        "abstract": "We present probabilistic neural programs, a framework for program induction that permits flexible specification of both a computational model and inference algorithm while simultaneously enabling the use of deep neural networks. Probabilistic neural programs combine a computation graph for specifying a neural network with an operator for weighted nondeterministic choice. Thus, a program describes both a collection of decisions as well as the neural network architecture used to make each one. We evaluate our approach on a challenging diagram question answering task where probabilistic neural programs correctly execute nearly twice as many programs as a baseline model.\n    ",
        "submission_date": "2016-12-02T00:00:00",
        "last_modified_date": "2016-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00745",
        "title": "Cognitive Deep Machine Can Train Itself",
        "authors": [
            "Andr\u00e1s L\u0151rincz",
            "M\u00e1t\u00e9 Cs\u00e1kv\u00e1ri",
            "\u00c1ron F\u00f3thi",
            "Zolt\u00e1n \u00c1d\u00e1m Milacski",
            "Andr\u00e1s S\u00e1rk\u00e1ny",
            "Zolt\u00e1n T\u0151s\u00e9r"
        ],
        "abstract": "Machine learning is making substantial progress in diverse applications. The success is mostly due to advances in deep learning. However, deep learning can make mistakes and its generalization abilities to new tasks are questionable. We ask when and how one can combine network outputs, when (i) details of the observations are evaluated by learned deep components and (ii) facts and confirmation rules are available in knowledge based systems. We show that in limited contexts the required number of training samples can be low and self-improvement of pre-trained networks in more general context is possible. We argue that the combination of sparse outlier detection with deep components that can support each other diminish the fragility of deep methods, an important requirement for engineering applications. We argue that supervised learning of labels may be fully eliminated under certain conditions: a component based architecture together with a knowledge based system can train itself and provide high quality answers. We demonstrate these concepts on the State Farm Distracted Driver Detection benchmark. We argue that the view of the Study Panel (2016) may overestimate the requirements on `years of focused research' and `careful, unique construction' for `AI systems'.\n    ",
        "submission_date": "2016-12-02T00:00:00",
        "last_modified_date": "2016-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00767",
        "title": "Asynchronous Stochastic Gradient MCMC with Elastic Coupling",
        "authors": [
            "Jost Tobias Springenberg",
            "Aaron Klein",
            "Stefan Falkner",
            "Frank Hutter"
        ],
        "abstract": "We consider parallel asynchronous Markov Chain Monte Carlo (MCMC) sampling for problems where we can leverage (stochastic) gradients to define continuous dynamics which explore the target distribution. We outline a solution strategy for this setting based on stochastic gradient Hamiltonian Monte Carlo sampling (SGHMC) which we alter to include an elastic coupling term that ties together multiple MCMC instances. The proposed strategy turns inherently sequential HMC algorithms into asynchronous parallel versions. First experiments empirically show that the resulting parallel sampler significantly speeds up exploration of the target distribution, when compared to standard SGHMC, and is less prone to the harmful effects of stale gradients than a naive parallelization approach.\n    ",
        "submission_date": "2016-12-02T00:00:00",
        "last_modified_date": "2016-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00796",
        "title": "Overcoming catastrophic forgetting in neural networks",
        "authors": [
            "James Kirkpatrick",
            "Razvan Pascanu",
            "Neil Rabinowitz",
            "Joel Veness",
            "Guillaume Desjardins",
            "Andrei A. Rusu",
            "Kieran Milan",
            "John Quan",
            "Tiago Ramalho",
            "Agnieszka Grabska-Barwinska",
            "Demis Hassabis",
            "Claudia Clopath",
            "Dharshan Kumaran",
            "Raia Hadsell"
        ],
        "abstract": "The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.\n    ",
        "submission_date": "2016-12-02T00:00:00",
        "last_modified_date": "2017-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00817",
        "title": "Summary - TerpreT: A Probabilistic Programming Language for Program Induction",
        "authors": [
            "Alexander L. Gaunt",
            "Marc Brockschmidt",
            "Rishabh Singh",
            "Nate Kushman",
            "Pushmeet Kohli",
            "Jonathan Taylor",
            "Daniel Tarlow"
        ],
        "abstract": "We study machine learning formulations of inductive program synthesis; that is, given input-output examples, synthesize source code that maps inputs to corresponding outputs. Our key contribution is TerpreT, a domain-specific language for expressing program synthesis problems. A TerpreT model is composed of a specification of a program representation and an interpreter that describes how programs map inputs to outputs. The inference task is to observe a set of input-output examples and infer the underlying program. From a TerpreT model we automatically perform inference using four different back-ends: gradient descent (thus each TerpreT model can be seen as defining a differentiable interpreter), linear program (LP) relaxations for graphical models, discrete satisfiability solving, and the Sketch program synthesis system. TerpreT has two main benefits. First, it enables rapid exploration of a range of domains, program representations, and interpreter models. Second, it separates the model specification from the inference algorithm, allowing proper comparisons between different approaches to inference.\n",
        "submission_date": "2016-12-02T00:00:00",
        "last_modified_date": "2016-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00837",
        "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
        "authors": [
            "Yash Goyal",
            "Tejas Khot",
            "Douglas Summers-Stay",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "abstract": "Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability.\n",
        "submission_date": "2016-12-02T00:00:00",
        "last_modified_date": "2017-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00901",
        "title": "Commonly Uncommon: Semantic Sparsity in Situation Recognition",
        "authors": [
            "Mark Yatskar",
            "Vicente Ordonez",
            "Luke Zettlemoyer",
            "Ali Farhadi"
        ],
        "abstract": "Semantic sparsity is a common challenge in structured visual classification problems; when the output space is complex, the vast majority of the possible predictions are rarely, if ever, seen in the training set. This paper studies semantic sparsity in situation recognition, the task of producing structured summaries of what is happening in images, including activities, objects and the roles objects play within the activity. For this problem, we find empirically that most object-role combinations are rare, and current state-of-the-art models significantly underperform in this sparse data regime. We avoid many such errors by (1) introducing a novel tensor composition function that learns to share examples across role-noun combinations and (2) semantically augmenting our training data with automatically gathered examples of rarely observed outputs using web data. When integrated within a complete CRF-based structured prediction model, the tensor-based approach outperforms existing state of the art by a relative improvement of 2.11% and 4.40% on top-5 verb and noun-role accuracy, respectively. Adding 5 million images with our semantic augmentation techniques gives further relative improvements of 6.23% and 9.57% on top-5 verb and noun-role accuracy.\n    ",
        "submission_date": "2016-12-03T00:00:00",
        "last_modified_date": "2016-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01078",
        "title": "Enhancing Use Case Points Estimation Method Using Soft Computing Techniques",
        "authors": [
            "Ali Bou Nassif",
            "Luiz Fernando Capretz",
            "Danny Ho"
        ],
        "abstract": "Software estimation is a crucial task in software engineering. Software estimation encompasses cost, effort, schedule, and size. The importance of software estimation becomes critical in the early stages of the software life cycle when the details of software have not been revealed yet. Several commercial and non-commercial tools exist to estimate software in the early stages. Most software effort estimation methods require software size as one of the important metric inputs and consequently, software size estimation in the early stages becomes essential. One of the approaches that has been used for about two decades in the early size and effort estimation is called use case points. Use case points method relies on the use case diagram to estimate the size and effort of software projects. Although the use case points method has been widely used, it has some limitations that might adversely affect the accuracy of estimation. This paper presents some techniques using fuzzy logic and neural networks to improve the accuracy of the use case points method. Results showed that an improvement up to 22% can be obtained using the proposed approach.\n    ",
        "submission_date": "2016-12-04T00:00:00",
        "last_modified_date": "2016-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01095",
        "title": "Representing Independence Models with Elementary Triplets",
        "authors": [
            "Jose M. Pe\u00f1a"
        ],
        "abstract": "In an independence model, the triplets that represent conditional independences between singletons are called elementary. It is known that the elementary triplets represent the independence model unambiguously under some conditions. In this paper, we show how this representation helps performing some operations with independence models, such as finding the dominant triplets or a minimal independence map of an independence model, or computing the union or intersection of a pair of independence models, or performing causal reasoning. For the latter, we rephrase in terms of conditional independences some of Pearl's results for computing causal effects.\n    ",
        "submission_date": "2016-12-04T00:00:00",
        "last_modified_date": "2016-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01197",
        "title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision (Short Version)",
        "authors": [
            "Chen Liang",
            "Jonathan Berant",
            "Quoc Le",
            "Kenneth D. Forbus",
            "Ni Lao"
        ],
        "abstract": "Extending the success of deep neural networks to natural language understanding and symbolic reasoning requires complex operations and external memory. Recent neural program induction approaches have attempted to address this problem, but are typically limited to differentiable memory, and consequently cannot scale beyond small synthetic tasks. In this work, we propose the Manager-Programmer-Computer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface. Specifically, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural \"programmer\", and a non-differentiable \"computer\" that is a Lisp interpreter with code assist. To successfully apply REINFORCE for training, we augment it with approximate gold programs found by an iterative maximum likelihood training process. NSM is able to learn a semantic parser from weak supervision over a large knowledge base. It achieves new state-of-the-art performance on WebQuestionsSP, a challenging semantic parsing dataset, with weak supervision. Compared to previous approaches, NSM is end-to-end, therefore does not rely on feature engineering or domain specific knowledge.\n    ",
        "submission_date": "2016-12-04T00:00:00",
        "last_modified_date": "2016-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01294",
        "title": "Message Passing Multi-Agent GANs",
        "authors": [
            "Arnab Ghosh",
            "Viveka Kulharia",
            "Vinay Namboodiri"
        ],
        "abstract": "Communicating and sharing intelligence among agents is an important facet of achieving Artificial General Intelligence. As a first step towards this challenge, we introduce a novel framework for image generation: Message Passing Multi-Agent Generative Adversarial Networks (MPM GANs). While GANs have recently been shown to be very effective for image generation and other tasks, these networks have been limited to mostly single generator-discriminator networks. We show that we can obtain multi-agent GANs that communicate through message passing to achieve better image generation. The objectives of the individual agents in this framework are two fold: a co-operation objective and a competing objective. The co-operation objective ensures that the message sharing mechanism guides the other generator to generate better than itself while the competing objective encourages each generator to generate better than its counterpart. We analyze and visualize the messages that these GANs share among themselves in various scenarios. We quantitatively show that the message sharing formulation serves as a regularizer for the adversarial training. Qualitatively, we show that the different generators capture different traits of the underlying data distribution.\n    ",
        "submission_date": "2016-12-05T00:00:00",
        "last_modified_date": "2016-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01399",
        "title": "A New Type-II Fuzzy Logic Based Controller for Non-linear Dynamical Systems with Application to a 3-PSP Parallel Robot",
        "authors": [
            "Hamid Reza Hassanzadeh"
        ],
        "abstract": "The concept of uncertainty is posed in almost any complex system including parallel robots as an outstanding instance of dynamical robotics systems. As suggested by the name, uncertainty, is some missing information that is beyond the knowledge of human thus we may tend to handle it properly to minimize the side-effects through the control process.\n",
        "submission_date": "2016-12-05T00:00:00",
        "last_modified_date": "2016-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01434",
        "title": "Proportional Rankings",
        "authors": [
            "Piotr Skowron",
            "Martin Lackner",
            "Markus Brill",
            "Dominik Peters",
            "Edith Elkind"
        ],
        "abstract": "In this paper we extend the principle of proportional representation to rankings. We consider the setting where alternatives need to be ranked based on approval preferences. In this setting, proportional representation requires that cohesive groups of voters are represented proportionally in each initial segment of the ranking. Proportional rankings are desirable in situations where initial segments of different lengths may be relevant, e.g., hiring decisions (if it is unclear how many positions are to be filled), the presentation of competing proposals on a liquid democracy platform (if it is unclear how many proposals participants are taking into consideration), or recommender systems (if a ranking has to accommodate different user types). We study the proportional representation provided by several ranking methods and prove theoretical guarantees. Furthermore, we experimentally evaluate these methods and present preliminary evidence as to which methods are most suitable for producing proportional rankings.\n    ",
        "submission_date": "2016-12-05T00:00:00",
        "last_modified_date": "2016-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01445",
        "title": "N-gram Opcode Analysis for Android Malware Detection",
        "authors": [
            "BooJoong Kang",
            "Suleiman Y. Yerima",
            "Sakir Sezer",
            "Kieran McLaughlin"
        ],
        "abstract": "Android malware has been on the rise in recent years due to the increasing popularity of Android and the proliferation of third party application markets. Emerging Android malware families are increasingly adopting sophisticated detection avoidance techniques and this calls for more effective approaches for Android malware detection. Hence, in this paper we present and evaluate an n-gram opcode features based approach that utilizes machine learning to identify and categorize Android malware. This approach enables automated feature discovery without relying on prior expert or domain knowledge for pre-determined features. Furthermore, by using a data segmentation technique for feature selection, our analysis is able to scale up to 10-gram opcodes. Our experiments on a dataset of 2520 samples showed an f-measure of 98% using the n-gram opcode based approach. We also provide empirical findings that illustrate factors that have probable impact on the overall n-gram opcodes performance trends.\n    ",
        "submission_date": "2016-12-05T00:00:00",
        "last_modified_date": "2016-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01589",
        "title": "Improving the Performance of Neural Networks in Regression Tasks Using Drawering",
        "authors": [
            "Konrad Zolna"
        ],
        "abstract": "The method presented extends a given regression neural network to make its performance improve. The modification affects the learning procedure only, hence the extension may be easily omitted during evaluation without any change in prediction. It means that the modified model may be evaluated as quickly as the original one but tends to perform better.\n",
        "submission_date": "2016-12-05T00:00:00",
        "last_modified_date": "2016-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01746",
        "title": "Factored Contextual Policy Search with Bayesian Optimization",
        "authors": [
            "Peter Karkus",
            "Andras Kupcsik",
            "David Hsu",
            "Wee Sun Lee"
        ],
        "abstract": "Scarce data is a major challenge to scaling robot learning to truly complex tasks, as we need to generalize locally learned policies over different \"contexts\". Bayesian optimization approaches to contextual policy search (CPS) offer data-efficient policy learning that generalize over a context space. We propose to improve data-efficiency by factoring typically considered contexts into two components: target-type contexts that correspond to a desired outcome of the learned behavior, e.g. target position for throwing a ball; and environment type contexts that correspond to some state of the environment, e.g. initial ball position or wind speed. Our key observation is that experience can be directly generalized over target-type contexts. Based on that we introduce Factored Contextual Policy Search with Bayesian Optimization for both passive and active learning settings. Preliminary results show faster policy generalization on a simulated toy problem. A full paper extension is available at ",
        "submission_date": "2016-12-06T00:00:00",
        "last_modified_date": "2019-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01887",
        "title": "Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning",
        "authors": [
            "Jiasen Lu",
            "Caiming Xiong",
            "Devi Parikh",
            "Richard Socher"
        ],
        "abstract": "Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active for every generated word. However, the decoder likely requires little to no visual information from the image to predict non-visual words such as \"the\" and \"of\". Other words that may seem visual can often be predicted reliably just from the language model e.g., \"sign\" after \"behind a red stop\" or \"phone\" following \"talking on a cell\". In this paper, we propose a novel adaptive attention model with a visual sentinel. At each time step, our model decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. The model decides whether to attend to the image and where, in order to extract meaningful information for sequential word generation. We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach sets the new state-of-the-art by a significant margin.\n    ",
        "submission_date": "2016-12-06T00:00:00",
        "last_modified_date": "2017-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01895",
        "title": "Multimodal Transfer: A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer",
        "authors": [
            "Xin Wang",
            "Geoffrey Oxholm",
            "Da Zhang",
            "Yuan-Fang Wang"
        ],
        "abstract": "Transferring artistic styles onto everyday photographs has become an extremely popular task in both academia and industry. Recently, offline training has replaced on-line iterative optimization, enabling nearly real-time stylization. When those stylization networks are applied directly to high-resolution images, however, the style of localized regions often appears less similar to the desired artistic style. This is because the transfer process fails to capture small, intricate textures and maintain correct texture scales of the artworks. Here we propose a multimodal convolutional neural network that takes into consideration faithful representations of both color and luminance channels, and performs stylization hierarchically with multiple losses of increasing scales. Compared to state-of-the-art networks, our network can also perform style transfer in nearly real-time by conducting much more sophisticated training offline. By properly handling style and texture cues at multiple scales using several modalities, we can transfer not just large-scale, obvious style cues but also subtle, exquisite ones. That is, our scheme can generate results that are visually pleasing and more similar to multiple desired artistic styles with color and texture cues at multiple scales.\n    ",
        "submission_date": "2016-11-17T00:00:00",
        "last_modified_date": "2017-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01939",
        "title": "Correlation Alignment for Unsupervised Domain Adaptation",
        "authors": [
            "Baochen Sun",
            "Jiashi Feng",
            "Kate Saenko"
        ],
        "abstract": "In this chapter, we present CORrelation ALignment (CORAL), a simple yet effective method for unsupervised domain adaptation. CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. In contrast to subspace manifold methods, it aligns the original feature distributions of the source and target domains, rather than the bases of lower-dimensional subspaces. It is also much simpler than other distribution matching methods. CORAL performs remarkably well in extensive evaluations on standard benchmark datasets. We first describe a solution that applies a linear transformation to source features to align them with target features before classifier training. For linear classifiers, we propose to equivalently apply CORAL to the classifier weights, leading to added efficiency when the number of classifiers is small but the number and dimensionality of target examples are very high. The resulting CORAL Linear Discriminant Analysis (CORAL-LDA) outperforms LDA by a large margin on standard domain adaptation benchmarks. Finally, we extend CORAL to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks (DNNs). The resulting Deep CORAL approach works seamlessly with DNNs and achieves state-of-the-art performance on standard benchmark datasets. Our code is available at:~\\url{",
        "submission_date": "2016-12-06T00:00:00",
        "last_modified_date": "2016-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02120",
        "title": "A Multi-Pass Approach to Large-Scale Connectomics",
        "authors": [
            "Yaron Meirovitch",
            "Alexander Matveev",
            "Hayk Saribekyan",
            "David Budden",
            "David Rolnick",
            "Gergely Odor",
            "Seymour Knowles-Barley",
            "Thouis Raymond Jones",
            "Hanspeter Pfister",
            "Jeff William Lichtman",
            "Nir Shavit"
        ],
        "abstract": "The field of connectomics faces unprecedented \"big data\" challenges. To reconstruct neuronal connectivity, automated pixel-level segmentation is required for petabytes of streaming electron microscopy data. Existing algorithms provide relatively good accuracy but are unacceptably slow, and would require years to extract connectivity graphs from even a single cubic millimeter of neural tissue. Here we present a viable real-time solution, a multi-pass pipeline optimized for shared-memory multicore systems, capable of processing data at near the terabyte-per-hour pace of multi-beam electron microscopes. The pipeline makes an initial fast-pass over the data, and then makes a second slow-pass to iteratively correct errors in the output of the fast-pass. We demonstrate the accuracy of a sparse slow-pass reconstruction algorithm and suggest new methods for detecting morphological errors. Our fast-pass approach provided many algorithmic challenges, including the design and implementation of novel shallow convolutional neural nets and the parallelization of watershed and object-merging techniques. We use it to reconstruct, from image stack to skeletons, the full dataset of Kasthuri et al. (463 GB capturing 120,000 cubic microns) in a matter of hours on a single multicore machine rather than the weeks it has taken in the past on much larger distributed systems.\n    ",
        "submission_date": "2016-12-07T00:00:00",
        "last_modified_date": "2016-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02136",
        "title": "Mode Regularized Generative Adversarial Networks",
        "authors": [
            "Tong Che",
            "Yanran Li",
            "Athul Paul Jacob",
            "Yoshua Bengio",
            "Wenjie Li"
        ],
        "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.\n    ",
        "submission_date": "2016-12-07T00:00:00",
        "last_modified_date": "2017-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02516",
        "title": "Stochastic Primal-Dual Methods and Sample Complexity of Reinforcement Learning",
        "authors": [
            "Yichen Chen",
            "Mengdi Wang"
        ],
        "abstract": "We study the online estimation of the optimal policy of a Markov decision process (MDP). We propose a class of Stochastic Primal-Dual (SPD) methods which exploit the inherent minimax duality of Bellman equations. The SPD methods update a few coordinates of the value and policy estimates as a new state transition is observed. These methods use small storage and has low computational complexity per iteration. The SPD methods find an absolute-$\\epsilon$-optimal policy, with high probability, using $\\mathcal{O}\\left(\\frac{|\\mathcal{S}|^4 |\\mathcal{A}|^2\\sigma^2 }{(1-\\gamma)^6\\epsilon^2} \\right)$ iterations/samples for the infinite-horizon discounted-reward MDP and $\\mathcal{O}\\left(\\frac{|\\mathcal{S}|^4 |\\mathcal{A}|^2H^6\\sigma^2 }{\\epsilon^2} \\right)$ for the finite-horizon MDP.\n    ",
        "submission_date": "2016-12-08T00:00:00",
        "last_modified_date": "2016-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02526",
        "title": "Prediction with a Short Memory",
        "authors": [
            "Vatsal Sharan",
            "Sham Kakade",
            "Percy Liang",
            "Gregory Valiant"
        ],
        "abstract": "We consider the problem of predicting the next observation given a sequence of past observations, and consider the extent to which accurate prediction requires complex algorithms that explicitly leverage long-range dependencies. Perhaps surprisingly, our positive results show that for a broad class of sequences, there is an algorithm that predicts well on average, and bases its predictions only on the most recent few observation together with a set of simple summary statistics of the past observations. Specifically, we show that for any distribution over observations, if the mutual information between past observations and future observations is upper bounded by $I$, then a simple Markov model over the most recent $I/\\epsilon$ observations obtains expected KL error $\\epsilon$---and hence $\\ell_1$ error $\\sqrt{\\epsilon}$---with respect to the optimal predictor that has access to the entire past and knows the data generating distribution. For a Hidden Markov Model with $n$ hidden states, $I$ is bounded by $\\log n$, a quantity that does not depend on the mixing time, and we show that the trivial prediction algorithm based on the empirical frequencies of length $O(\\log n/\\epsilon)$ windows of observations achieves this error, provided the length of the sequence is $d^{\\Omega(\\log n/\\epsilon)}$, where $d$ is the size of the observation alphabet.\n",
        "submission_date": "2016-12-08T00:00:00",
        "last_modified_date": "2018-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02684",
        "title": "Fixpoint Approximation of Strategic Abilities under Imperfect Information",
        "authors": [
            "Wojciech Jamroga",
            "Micha\u0142 Knapik",
            "Damian Kurpiewski"
        ],
        "abstract": "Model checking of strategic ability under imperfect information is known to be hard. The complexity results range from NP-completeness to undecidability, depending on the precise setup of the problem. No less importantly, fixpoint equivalences do not generally hold for imperfect information strategies, which seriously hampers incremental synthesis of winning strategies. In this paper, we propose translations of ATLir formulae that provide lower and upper bounds for their truth values, and are cheaper to verify than the original specifications. That is, if the expression is verified as true then the corresponding formula of ATLir should also hold in the given model. We begin by showing where the straightforward approach does not work. Then, we propose how it can be modified to obtain guaranteed lower bounds. To this end, we alter the next-step operator in such a way that traversing one's indistinguishability relation is seen as atomic activity. Most interestingly, the lower approximation is provided by a fixpoint expression that uses a nonstandard variant of the next-step ability operator. We show the correctness of the translations, establish their computational complexity, and validate the approach by experiments with a scalable scenario of Bridge play.\n    ",
        "submission_date": "2016-12-08T00:00:00",
        "last_modified_date": "2017-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02734",
        "title": "Learning in the Machine: Random Backpropagation and the Deep Learning Channel",
        "authors": [
            "Pierre Baldi",
            "Peter Sadowski",
            "Zhiqin Lu"
        ],
        "abstract": "Random backpropagation (RBP) is a variant of the backpropagation algorithm for training neural networks, where the transpose of the forward matrices are replaced by fixed random matrices in the calculation of the weight updates. It is remarkable both because of its effectiveness, in spite of using random matrices to communicate error information, and because it completely removes the taxing requirement of maintaining symmetric weights in a physical neural system. To better understand random backpropagation, we first connect it to the notions of local learning and learning channels. Through this connection, we derive several alternatives to RBP, including skipped RBP (SRPB), adaptive RBP (ARBP), sparse RBP, and their combinations (e.g. ASRBP) and analyze their computational complexity. We then study their behavior through simulations using the MNIST and CIFAR-10 bechnmark datasets. These simulations show that most of these variants work robustly, almost as well as backpropagation, and that multiplication by the derivatives of the activation functions is important. As a follow-up, we study also the low-end of the number of bits required to communicate error information over the learning channel. We then provide partial intuitive explanations for some of the remarkable properties of RBP and its variations. Finally, we prove several mathematical results, including the convergence to fixed points of linear chains of arbitrary length, the convergence to fixed points of linear autoencoders with decorrelated data, the long-term existence of solutions for linear systems with a single hidden layer and convergence in special cases, and the convergence to fixed points of non-linear chains, when the derivative of the activation functions is included.\n    ",
        "submission_date": "2016-12-08T00:00:00",
        "last_modified_date": "2017-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02739",
        "title": "Controlling Robot Morphology from Incomplete Measurements",
        "authors": [
            "Martin Pecka",
            "Karel Zimmermann",
            "Michal Rein\u0161tein",
            "Tom\u00e1\u0161 Svoboda"
        ],
        "abstract": "Mobile robots with complex morphology are essential for traversing rough terrains in Urban Search & Rescue missions (USAR). Since teleoperation of the complex morphology causes high cognitive load of the operator, the morphology is controlled autonomously. The autonomous control measures the robot state and surrounding terrain which is usually only partially observable, and thus the data are often incomplete. We marginalize the control over the missing measurements and evaluate an explicit safety condition. If the safety condition is violated, tactile terrain exploration by the body-mounted robotic arm gathers the missing data.\n    ",
        "submission_date": "2016-12-08T00:00:00",
        "last_modified_date": "2016-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02741",
        "title": "Coupling Distributed and Symbolic Execution for Natural Language Queries",
        "authors": [
            "Lili Mou",
            "Zhengdong Lu",
            "Hang Li",
            "Zhi Jin"
        ],
        "abstract": "Building neural networks to query a knowledge base (a table) with natural language is an emerging research topic in deep learning. An executor for table querying typically requires multiple steps of execution because queries may have complicated structures. In previous studies, researchers have developed either fully distributed executors or symbolic executors for table querying. A distributed executor can be trained in an end-to-end fashion, but is weak in terms of execution efficiency and explicit interpretability. A symbolic executor is efficient in execution, but is very difficult to train especially at initial stages. In this paper, we propose to couple distributed and symbolic execution for natural language queries, where the symbolic executor is pretrained with the distributed executor's intermediate execution results in a step-by-step fashion. Experiments show that our approach significantly outperforms both distributed and symbolic executors, exhibiting high accuracy, high learning efficiency, high execution efficiency, and high interpretability.\n    ",
        "submission_date": "2016-12-08T00:00:00",
        "last_modified_date": "2017-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02795",
        "title": "Safety Verification and Control for Collision Avoidance at Road Intersections",
        "authors": [
            "Heejin Ahn",
            "Domitilla Del Vecchio"
        ],
        "abstract": "This paper presents the design of a supervisory algorithm that monitors safety at road intersections and overrides drivers with a safe input when necessary. The design of the supervisor consists of two parts: safety verification and control design. Safety verification is the problem to determine if vehicles will be able to cross the intersection without colliding with current drivers' inputs. We translate this safety verification problem into a jobshop scheduling problem, which minimizes the maximum lateness and evaluates if the optimal cost is zero. The zero optimal cost corresponds to the case in which all vehicles can cross each conflict area without collisions. Computing the optimal cost requires solving a Mixed Integer Nonlinear Programming (MINLP) problem due to the nonlinear second-order dynamics of the vehicles. We therefore estimate this optimal cost by formulating two related Mixed Integer Linear Programming (MILP) problems that assume simpler vehicle dynamics. We prove that these two MILP problems yield lower and upper bounds of the optimal cost. We also quantify the worst case approximation errors of these MILP problems. We design the supervisor to override the vehicles with a safe control input if the MILP problem that computes the upper bound yields a positive optimal cost. We theoretically demonstrate that the supervisor keeps the intersection safe and is non-blocking. Computer simulations further validate that the algorithms can run in real time for problems of realistic size.\n    ",
        "submission_date": "2016-12-08T00:00:00",
        "last_modified_date": "2016-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02814",
        "title": "Task-Guided and Path-Augmented Heterogeneous Network Embedding for Author Identification",
        "authors": [
            "Ting Chen",
            "Yizhou Sun"
        ],
        "abstract": "In this paper, we study the problem of author identification under double-blind review setting, which is to identify potential authors given information of an anonymized paper. Different from existing approaches that rely heavily on feature engineering, we propose to use network embedding approach to address the problem, which can automatically represent nodes into lower dimensional feature vectors. However, there are two major limitations in recent studies on network embedding: (1) they are usually general-purpose embedding methods, which are independent of the specific tasks; and (2) most of these approaches can only deal with homogeneous networks, where the heterogeneity of the network is ignored. Hence, challenges faced here are two folds: (1) how to embed the network under the guidance of the author identification task, and (2) how to select the best type of information due to the heterogeneity of the network.\n",
        "submission_date": "2016-12-08T00:00:00",
        "last_modified_date": "2016-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02879",
        "title": "Learning Representations by Stochastic Meta-Gradient Descent in Neural Networks",
        "authors": [
            "Vivek Veeriah",
            "Shangtong Zhang",
            "Richard S. Sutton"
        ],
        "abstract": "Representations are fundamental to artificial intelligence. The performance of a learning system depends on the type of representation used for representing the data. Typically, these representations are hand-engineered using domain knowledge. More recently, the trend is to learn these representations through stochastic gradient descent in multi-layer neural networks, which is called backprop. Learning the representations directly from the incoming data stream reduces the human labour involved in designing a learning system. More importantly, this allows in scaling of a learning system for difficult tasks. In this paper, we introduce a new incremental learning algorithm called crossprop, which learns incoming weights of hidden units based on the meta-gradient descent approach, that was previously introduced by Sutton (1992) and Schraudolph (1999) for learning step-sizes. The final update equation introduces an additional memory parameter for each of these weights and generalizes the backprop update equation. From our experiments, we show that crossprop learns and reuses its feature representation while tackling new and unseen tasks whereas backprop relearns a new feature representation.\n    ",
        "submission_date": "2016-12-09T00:00:00",
        "last_modified_date": "2017-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03117",
        "title": "Advancing Bayesian Optimization: The Mixed-Global-Local (MGL) Kernel and Length-Scale Cool Down",
        "authors": [
            "Kim Peter Wabersich",
            "Marc Toussaint"
        ],
        "abstract": "Bayesian Optimization (BO) has become a core method for solving expensive black-box optimization problems. While much research focussed on the choice of the acquisition function, we focus on online length-scale adaption and the choice of kernel function. Instead of choosing hyperparameters in view of maximum likelihood on past data, we propose to use the acquisition function to decide on hyperparameter adaptation more robustly and in view of the future optimization progress. Further, we propose a particular kernel function that includes non-stationarity and local anisotropy and thereby implicitly integrates the efficiency of local convex optimization with global Bayesian optimization. Comparisons to state-of-the art BO methods underline the efficiency of these mechanisms on global optimization benchmarks.\n    ",
        "submission_date": "2016-12-09T00:00:00",
        "last_modified_date": "2016-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03224",
        "title": "Finding Better Active Learners for Faster Literature Reviews",
        "authors": [
            "Zhe Yu",
            "Nicholas A. Kraft",
            "Tim Menzies"
        ],
        "abstract": "Literature reviews can be time-consuming and tedious to complete. By cataloging and refactoring three state-of-the-art active learning techniques from evidence-based medicine and legal electronic discovery, this paper finds and implements FASTREAD, a faster technique for studying a large corpus of documents. This paper assesses FASTREAD using datasets generated from existing SE literature reviews (Hall, Wahono, Radjenovi\u0107, Kitchenham et al.). Compared to manual methods, FASTREAD lets researchers find 95% relevant studies after reviewing an order of magnitude fewer papers. Compared to other state-of-the-art automatic methods, FASTREAD reviews 20-50% fewer studies while finding same number of relevant primary studies in a systematic literature review.\n    ",
        "submission_date": "2016-12-10T00:00:00",
        "last_modified_date": "2018-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03242",
        "title": "StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks",
        "authors": [
            "Han Zhang",
            "Tao Xu",
            "Hongsheng Li",
            "Shaoting Zhang",
            "Xiaogang Wang",
            "Xiaolei Huang",
            "Dimitris Metaxas"
        ],
        "abstract": "Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256x256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.\n    ",
        "submission_date": "2016-12-10T00:00:00",
        "last_modified_date": "2017-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03284",
        "title": "Salient Object Detection with Convex Hull Overlap",
        "authors": [
            "Yongqing Liang"
        ],
        "abstract": "Salient object detection plays an important part in a vision system to detect important regions. Convolutional neural network (CNN) based methods directly train their models with large-scale datasets, but what is the crucial feature for saliency is still a problem. In this paper, we establish a novel bottom-up feature named convex hull overlap (CHO), combining with appearance contrast features, to detect salient objects. CHO feature is a kind of enhanced Gestalt cue. Psychologists believe that surroundedness reflects objects overlap relationship. An object which is on the top of the others is attractive. Our method significantly differs from other earlier works in (1) We set up a hand-crafted feature to detect salient object that our model does not need to be trained by large-scale datasets; (2) Previous works only focus on appearance features, while our CHO feature makes up the gap between the spatial object covering and the object's saliency. Our experiments on a large number of public datasets have obtained very positive results.\n    ",
        "submission_date": "2016-12-10T00:00:00",
        "last_modified_date": "2020-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03364",
        "title": "Technical Report: A Generalized Matching Pursuit Approach for Graph-Structured Sparsity",
        "authors": [
            "Feng Chen",
            "Baojian Zhou"
        ],
        "abstract": "Sparsity-constrained optimization is an important and challenging problem that has wide applicability in data mining, machine learning, and statistics. In this paper, we focus on sparsity-constrained optimization in cases where the cost function is a general nonlinear function and, in particular, the sparsity constraint is defined by a graph-structured sparsity model. Existing methods explore this problem in the context of sparse estimation in linear models. To the best of our knowledge, this is the first work to present an efficient approximation algorithm, namely, Graph-structured Matching Pursuit (Graph-Mp), to optimize a general nonlinear function subject to graph-structured constraints. We prove that our algorithm enjoys the strong guarantees analogous to those designed for linear models in terms of convergence rate and approximation accuracy. As a case study, we specialize Graph-Mp to optimize a number of well-known graph scan statistic models for the connected subgraph detection task, and empirical evidence demonstrates that our general algorithm performs superior over state-of-the-art methods that are designed specifically for the task of connected subgraph detection.\n    ",
        "submission_date": "2016-12-11T00:00:00",
        "last_modified_date": "2016-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03365",
        "title": "Multiple Instance Learning: A Survey of Problem Characteristics and Applications",
        "authors": [
            "Marc-Andr\u00e9 Carbonneau",
            "Veronika Cheplygina",
            "Eric Granger",
            "Ghyslain Gagnon"
        ],
        "abstract": "Multiple instance learning (MIL) is a form of weakly supervised learning where training instances are arranged in sets, called bags, and a label is provided for the entire bag. This formulation is gaining interest because it naturally fits various problems and allows to leverage weakly labeled data. Consequently, it has been used in diverse application fields such as computer vision and document classification. However, learning from bags raises important challenges that are unique to MIL. This paper provides a comprehensive survey of the characteristics which define and differentiate the types of MIL problems. Until now, these problem characteristics have not been formally identified and described. As a result, the variations in performance of MIL algorithms from one data set to another are difficult to explain. In this paper, MIL problem characteristics are grouped into four broad categories: the composition of the bags, the types of data distribution, the ambiguity of instance labels, and the task to be performed. Methods specialized to address each category are reviewed. Then, the extent to which these characteristics manifest themselves in key MIL application areas are described. Finally, experiments are conducted to compare the performance of 16 state-of-the-art MIL methods on selected problem characteristics. This paper provides insight on how the problem characteristics affect MIL algorithms, recommendations for future benchmarking and promising avenues for research.\n    ",
        "submission_date": "2016-12-11T00:00:00",
        "last_modified_date": "2016-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03433",
        "title": "A Model of Multi-Agent Consensus for Vague and Uncertain Beliefs",
        "authors": [
            "Michael Crosscombe",
            "Jonathan Lawry"
        ],
        "abstract": "Consensus formation is investigated for multi-agent systems in which agents' beliefs are both vague and uncertain. Vagueness is represented by a third truth state meaning \\emph{borderline}. This is combined with a probabilistic model of uncertainty. A belief combination operator is then proposed which exploits borderline truth values to enable agents with conflicting beliefs to reach a compromise. A number of simulation experiments are carried out in which agents apply this operator in pairwise interactions, under the bounded confidence restriction that the two agents' beliefs must be sufficiently consistent with each other before agreement can be reached. As well as studying the consensus operator in isolation we also investigate scenarios in which agents are influenced either directly or indirectly by the state of the world. For the former we conduct simulations which combine consensus formation with belief updating based on evidence. For the latter we investigate the effect of assuming that the closer an agent's beliefs are to the truth the more visible they are in the consensus building process. In all cases applying the consensus operators results in the population converging to a single shared belief which is both crisp and certain. Furthermore, simulations which combine consensus formation with evidential updating converge faster to a shared opinion which is closer to the actual state of the world than those in which beliefs are only changed as a result of directly receiving new evidence. Finally, if agent interactions are guided by belief quality measured as similarity to the true state of the world, then applying the consensus operator alone results in the population converging to a high quality shared belief.\n    ",
        "submission_date": "2016-12-11T00:00:00",
        "last_modified_date": "2018-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03551",
        "title": "Reading Comprehension using Entity-based Memory Network",
        "authors": [
            "Xun Wang",
            "Katsuhito Sudoh",
            "Masaaki Nagata",
            "Tomohide Shibata",
            "Daisuke Kawahara",
            "Sadao Kurohashi"
        ],
        "abstract": "This paper introduces a novel neural network model for question answering, the \\emph{entity-based memory network}. It enhances neural networks' ability of representing and calculating information over a long period by keeping records of entities contained in text. The core component is a memory pool which comprises entities' states. These entities' states are continuously updated according to the input text. Questions with regard to the input text are used to search the memory pool for related entities and answers are further predicted based on the states of retrieved entities. Compared with previous memory network models, the proposed model is capable of handling fine-grained information and more sophisticated relations based on entities. We formulated several different tasks as question answering problems and tested the proposed model. Experiments reported satisfying results.\n    ",
        "submission_date": "2016-12-12T00:00:00",
        "last_modified_date": "2017-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03705",
        "title": "Segmentation of large images based on super-pixels and community detection in graphs",
        "authors": [
            "Oscar A. C. Linares",
            "Glenda Michele Botelho",
            "Francisco Aparecido Rodrigues",
            "Jo\u00e3o Batista Neto"
        ],
        "abstract": "Image segmentation has many applications which range from machine learning to medical diagnosis. In this paper, we propose a framework for the segmentation of images based on super-pixels and algorithms for community identification in graphs. The super-pixel pre-segmentation step reduces the number of nodes in the graph, rendering the method the ability to process large images. Moreover, community detection algorithms provide more accurate segmentation than traditional approaches, such as those based on spectral graph partition. We also compare our method with two algorithms: a) the graph-based approach by Felzenszwalb and Huttenlocher and b) the contour-based method by Arbelaez. Results have shown that our method provides more precise segmentation and is faster than both of them.\n    ",
        "submission_date": "2016-12-12T00:00:00",
        "last_modified_date": "2016-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03769",
        "title": "Context-aware Sentiment Word Identification: sentiword2vec",
        "authors": [
            "Yushi Yao",
            "Guangjian Li"
        ],
        "abstract": "Traditional sentiment analysis often uses sentiment dictionary to extract sentiment information in text and classify documents. However, emerging informal words and phrases in user generated content call for analysis aware to the context. Usually, they have special meanings in a particular context. Because of its great performance in representing inter-word relation, we use sentiment word vectors to identify the special words. Based on the distributed language model word2vec, in this paper we represent a novel method about sentiment representation of word under particular context, to be detailed, to identify the words with abnormal sentiment polarity in long answers. Result shows the improved model shows better performance in representing the words with special meaning, while keep doing well in representing special idiomatic pattern. Finally, we will discuss the meaning of vectors representing in the field of sentiment, which may be different from general object-based conditions.\n    ",
        "submission_date": "2016-12-12T00:00:00",
        "last_modified_date": "2016-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03789",
        "title": "A Unit Selection Methodology for Music Generation Using Deep Neural Networks",
        "authors": [
            "Mason Bretan",
            "Gil Weinberg",
            "Larry Heck"
        ],
        "abstract": "Several methods exist for a computer to generate music based on data including Markov chains, recurrent neural networks, recombinancy, and grammars. We explore the use of unit selection and concatenation as a means of generating music using a procedure based on ranking, where, we consider a unit to be a variable length number of measures of music. We first examine whether a unit selection method, that is restricted to a finite size unit library, can be sufficient for encompassing a wide spectrum of music. We do this by developing a deep autoencoder that encodes a musical input and reconstructs the input by selecting from the library. We then describe a generative model that combines a deep structured semantic model (DSSM) with an LSTM to predict the next unit, where units consist of four, two, and one measures of music. We evaluate the generative model using objective metrics including mean rank and accuracy and with a subjective listening test in which expert musicians are asked to complete a forced-choiced ranking task. We compare our model to a note-level generative baseline that consists of a stacked LSTM trained to predict forward by one note.\n    ",
        "submission_date": "2016-12-12T00:00:00",
        "last_modified_date": "2016-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03929",
        "title": "Deep Active Learning for Dialogue Generation",
        "authors": [
            "Nabiha Asghar",
            "Pascal Poupart",
            "Xin Jiang",
            "Hang Li"
        ],
        "abstract": "We propose an online, end-to-end, neural generative conversational model for open-domain dialogue. It is trained using a unique combination of offline two-phase supervised learning and online human-in-the-loop active learning. While most existing research proposes offline supervision or hand-crafted reward functions for online reinforcement, we devise a novel interactive learning mechanism based on hamming-diverse beam search for response generation and one-character user-feedback at each step. Experiments show that our model inherently promotes the generation of semantically relevant and interesting responses, and can be used to train agents with customized personas, moods and conversational styles.\n    ",
        "submission_date": "2016-12-12T00:00:00",
        "last_modified_date": "2017-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03981",
        "title": "Hybrid Repeat/Multi-point Sampling for Highly Volatile Objective Functions",
        "authors": [
            "Brett Israelsen",
            "Nisar Ahmed"
        ],
        "abstract": "A key drawback of the current generation of artificial decision-makers is that they do not adapt well to changes in unexpected situations. This paper addresses the situation in which an AI for aerial dog fighting, with tunable parameters that govern its behavior, will optimize behavior with respect to an objective function that must be evaluated and learned through simulations. Once this objective function has been modeled, the agent can then choose its desired behavior in different situations. Bayesian optimization with a Gaussian Process surrogate is used as the method for investigating the objective function. One key benefit is that during optimization the Gaussian Process learns a global estimate of the true objective function, with predicted outcomes and a statistical measure of confidence in areas that haven't been investigated yet. However, standard Bayesian optimization does not perform consistently or provide an accurate Gaussian Process surrogate function for highly volatile objective functions. We treat these problems by introducing a novel sampling technique called Hybrid Repeat/Multi-point Sampling. This technique gives the AI ability to learn optimum behaviors in a highly uncertain environment. More importantly, it not only improves the reliability of the optimization, but also creates a better model of the entire objective surface. With this improved model the agent is equipped to better adapt behaviors.\n    ",
        "submission_date": "2016-12-13T00:00:00",
        "last_modified_date": "2016-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04023",
        "title": "Proceedings of the The First Workshop on Verification and Validation of Cyber-Physical Systems",
        "authors": [
            "Mehdi Kargahi",
            "Ashutosh Trivedi"
        ],
        "abstract": "The first International Workshop on Verification and Validation of Cyber-Physical Systems (V2CPS-16) was held in conjunction with the 12th International Conference on integration of Formal Methods (iFM 2016) in Reykjavik, Iceland. The purpose of V2CPS-16 was to bring together researchers and experts of the fields of formal verification and cyber-physical systems (CPS) to cover the theme of this workshop, namely a wide spectrum of verification and validation methods including (but not limited to) control, simulation, formal methods, etc.\n",
        "submission_date": "2016-12-13T00:00:00",
        "last_modified_date": "2016-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04286",
        "title": "Application of Advanced Record Linkage Techniques for Complex Population Reconstruction",
        "authors": [
            "Peter Christen"
        ],
        "abstract": "Record linkage is the process of identifying records that refer to the same entities from several databases. This process is challenging because commonly no unique entity identifiers are available. Linkage therefore has to rely on partially identifying attributes, such as names and addresses of people. Recent years have seen the development of novel techniques for linking data from diverse application areas, where a major focus has been on linking complex data that contain records about different types of entities. Advanced approaches that exploit both the similarities between record attributes as well as the relationships between entities to identify clusters of matching records have been developed.\n",
        "submission_date": "2016-12-13T00:00:00",
        "last_modified_date": "2016-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04299",
        "title": "Algorithms for Graph-Constrained Coalition Formation in the Real World",
        "authors": [
            "Filippo Bistaffa",
            "Alessandro Farinelli",
            "Jes\u00fas Cerquides",
            "Juan A. Rodr\u00edguez-Aguilar",
            "Sarvapali D. Ramchurn"
        ],
        "abstract": "Coalition formation typically involves the coming together of multiple, heterogeneous, agents to achieve both their individual and collective goals. In this paper, we focus on a special case of coalition formation known as Graph-Constrained Coalition Formation (GCCF) whereby a network connecting the agents constrains the formation of coalitions. We focus on this type of problem given that in many real-world applications, agents may be connected by a communication network or only trust certain peers in their social network. We propose a novel representation of this problem based on the concept of edge contraction, which allows us to model the search space induced by the GCCF problem as a rooted tree. Then, we propose an anytime solution algorithm (CFSS), which is particularly efficient when applied to a general class of characteristic functions called $m+a$ functions. Moreover, we show how CFSS can be efficiently parallelised to solve GCCF using a non-redundant partition of the search space. We benchmark CFSS on both synthetic and realistic scenarios, using a real-world dataset consisting of the energy consumption of a large number of households in the UK. Our results show that, in the best case, the serial version of CFSS is 4 orders of magnitude faster than the state of the art, while the parallel version is 9.44 times faster than the serial version on a 12-core machine. Moreover, CFSS is the first approach to provide anytime approximate solutions with quality guarantees for very large systems of agents (i.e., with more than 2700 agents).\n    ",
        "submission_date": "2016-12-13T00:00:00",
        "last_modified_date": "2016-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04315",
        "title": "Towards Adaptive Training of Agent-based Sparring Partners for Fighter Pilots",
        "authors": [
            "Brett W. Israelsen",
            "Nisar Ahmed",
            "Kenneth Center",
            "Roderick Green",
            "Winston Bennett Jr"
        ],
        "abstract": "A key requirement for the current generation of artificial decision-makers is that they should adapt well to changes in unexpected situations. This paper addresses the situation in which an AI for aerial dog fighting, with tunable parameters that govern its behavior, must optimize behavior with respect to an objective function that is evaluated and learned through simulations. Bayesian optimization with a Gaussian Process surrogate is used as the method for investigating the objective function. One key benefit is that during optimization, the Gaussian Process learns a global estimate of the true objective function, with predicted outcomes and a statistical measure of confidence in areas that haven't been investigated yet. Having a model of the objective function is important for being able to understand possible outcomes in the decision space; for example this is crucial for training and providing feedback to human pilots. However, standard Bayesian optimization does not perform consistently or provide an accurate Gaussian Process surrogate function for highly volatile objective functions. We treat these problems by introducing a novel sampling technique called Hybrid Repeat/Multi-point Sampling. This technique gives the AI ability to learn optimum behaviors in a highly uncertain environment. More importantly, it not only improves the reliability of the optimization, but also creates a better model of the entire objective surface. With this improved model the agent is equipped to more accurately/efficiently predict performance in unexplored scenarios.\n    ",
        "submission_date": "2016-12-13T00:00:00",
        "last_modified_date": "2016-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04318",
        "title": "Incorporating Human Domain Knowledge into Large Scale Cost Function Learning",
        "authors": [
            "Markus Wulfmeier",
            "Dushyant Rao",
            "Ingmar Posner"
        ],
        "abstract": "Recent advances have shown the capability of Fully Convolutional Neural Networks (FCN) to model cost functions for motion planning in the context of learning driving preferences purely based on demonstration data from human drivers. While pure learning from demonstrations in the framework of Inverse Reinforcement Learning (IRL) is a promising approach, we can benefit from well informed human priors and incorporate them into the learning process. Our work achieves this by pretraining a model to regress to a manual cost function and refining it based on Maximum Entropy Deep Inverse Reinforcement Learning. When injecting prior knowledge as pretraining for the network, we achieve higher robustness, more visually distinct obstacle boundaries, and the ability to capture instances of obstacles that elude models that purely learn from demonstration data. Furthermore, by exploiting these human priors, the resulting model can more accurately handle corner cases that are scarcely seen in the demonstration data, such as stairs, slopes, and underpasses.\n    ",
        "submission_date": "2016-12-13T00:00:00",
        "last_modified_date": "2016-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04432",
        "title": "An argumentative agent-based model of scientific inquiry",
        "authors": [
            "Annemarie Borg",
            "Daniel Frey",
            "Dunja \u0160e\u0161elja",
            "Christian Stra\u00dfer"
        ],
        "abstract": "In this paper we present an agent-based model (ABM) of scientific inquiry aimed at investigating how different social networks impact the efficiency of scientists in acquiring knowledge. As such, the ABM is a computational tool for tackling issues in the domain of scientific methodology and science policy. In contrast to existing ABMs of science, our model aims to represent the argumentative dynamics that underlies scientific practice. To this end we employ abstract argumentation theory as the core design feature of the model. This helps to avoid a number of problematic idealizations which are present in other ABMs of science and which impede their relevance for actual scientific practice.\n    ",
        "submission_date": "2016-12-13T00:00:00",
        "last_modified_date": "2016-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04468",
        "title": "Sparse Factorization Layers for Neural Networks with Limited Supervision",
        "authors": [
            "Parker Koch",
            "Jason J. Corso"
        ],
        "abstract": "Whereas CNNs have demonstrated immense progress in many vision problems, they suffer from a dependence on monumental amounts of labeled training data. On the other hand, dictionary learning does not scale to the size of problems that CNNs can handle, despite being very effective at low-level vision tasks such as denoising and inpainting. Recently, interest has grown in adapting dictionary learning methods for supervised tasks such as classification and inverse problems. We propose two new network layers that are based on dictionary learning: a sparse factorization layer and a convolutional sparse factorization layer, analogous to fully-connected and convolutional layers, respectively. Using our derivations, these layers can be dropped in to existing CNNs, trained together in an end-to-end fashion with back-propagation, and leverage semisupervision in ways classical CNNs cannot. We experimentally compare networks with these two new layers against a baseline CNN. Our results demonstrate that networks with either of the sparse factorization layers are able to outperform classical CNNs when supervised data are few. They also show performance improvements in certain tasks when compared to the CNN with no sparse factorization layers with the same exact number of parameters.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2016-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04742",
        "title": "Imposing higher-level Structure in Polyphonic Music Generation using Convolutional Restricted Boltzmann Machines and Constraints",
        "authors": [
            "Stefan Lattner",
            "Maarten Grachten",
            "Gerhard Widmer"
        ],
        "abstract": "We introduce a method for imposing higher-level structure on generated, polyphonic music. A Convolutional Restricted Boltzmann Machine (C-RBM) as a generative model is combined with gradient descent constraint optimisation to provide further control over the generation process. Among other things, this allows for the use of a \"template\" piece, from which some structural properties can be extracted, and transferred as constraints to the newly generated material. The sampling process is guided with Simulated Annealing to avoid local optima, and to find solutions that both satisfy the constraints, and are relatively stable with respect to the C-RBM. Results show that with this approach it is possible to control the higher-level self-similarity structure, the meter, and the tonal properties of the resulting musical piece, while preserving its local musical coherence.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2018-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04757",
        "title": "Attentive Explanations: Justifying Decisions and Pointing to the Evidence",
        "authors": [
            "Dong Huk Park",
            "Lisa Anne Hendricks",
            "Zeynep Akata",
            "Bernt Schiele",
            "Trevor Darrell",
            "Marcus Rohrbach"
        ],
        "abstract": "Deep models are the defacto standard in visual decision models due to their impressive performance on a wide array of visual tasks. However, they are frequently seen as opaque and are unable to explain their decisions. In contrast, humans can justify their decisions with natural language and point to the evidence in the visual world which led to their decisions. We postulate that deep models can do this as well and propose our Pointing and Justification (PJ-X) model which can justify its decision with a sentence and point to the evidence by introspecting its decision and explanation process using an attention mechanism. Unfortunately there is no dataset available with reference explanations for visual decision making. We thus collect two datasets in two domains where it is interesting and challenging to explain decisions. First, we extend the visual question answering task to not only provide an answer but also a natural language explanation for the answer. Second, we focus on explaining human activities which is traditionally more challenging than object classification. We extensively evaluate our PJ-X model, both on the justification and pointing tasks, by comparing it to prior models and ablations using both automatic and human evaluations.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2017-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04804",
        "title": "Anomaly Detection Using the Knowledge-based Temporal Abstraction Method",
        "authors": [
            "Asaf Shabtai"
        ],
        "abstract": "The rapid growth in stored time-oriented data necessitates the development of new methods for handling, processing, and interpreting large amounts of temporal data. One important example of such processing is detecting anomalies in time-oriented data. The Knowledge-Based Temporal Abstraction method was previously proposed for intelligent interpretation of temporal data based on predefined domain knowledge. In this study we propose a framework that integrates the KBTA method with a temporal pattern mining process for anomaly detection. According to the proposed method a temporal pattern mining process is applied on a dataset of basic temporal abstraction database in order to extract patterns representing normal behavior. These patterns are then analyzed in order to identify abnormal time periods characterized by a significantly small number of normal patterns. The proposed approach was demonstrated using a dataset collected from a real server.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2016-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04868",
        "title": "Interpretable Semantic Textual Similarity: Finding and explaining differences between sentences",
        "authors": [
            "I. Lopez-Gazpio",
            "M. Maritxalar",
            "A. Gonzalez-Agirre",
            "G. Rigau",
            "L. Uria",
            "E. Agirre"
        ],
        "abstract": "User acceptance of artificial intelligence agents might depend on their ability to explain their reasoning, which requires adding an interpretability layer that fa- cilitates users to understand their behavior. This paper focuses on adding an in- terpretable layer on top of Semantic Textual Similarity (STS), which measures the degree of semantic equivalence between two sentences. The interpretability layer is formalized as the alignment between pairs of segments across the two sentences, where the relation between the segments is labeled with a relation type and a similarity score. We present a publicly available dataset of sentence pairs annotated following the formalization. We then develop a system trained on this dataset which, given a sentence pair, explains what is similar and different, in the form of graded and typed segment alignments. When evaluated on the dataset, the system performs better than an informed baseline, showing that the dataset and task are well-defined and feasible. Most importantly, two user studies show how the system output can be used to automatically produce explanations in natural language. Users performed better when having access to the explanations, pro- viding preliminary evidence that our dataset and method to automatically produce explanations is useful in real applications.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2016-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04883",
        "title": "Graph Summarization Methods and Applications: A Survey",
        "authors": [
            "Yike Liu",
            "Tara Safavi",
            "Abhilash Dighe",
            "Danai Koutra"
        ],
        "abstract": "While advances in computing resources have made processing enormous amounts of data possible, human ability to identify patterns in such data has not scaled accordingly. Efficient computational methods for condensing and simplifying data are thus becoming vital for extracting actionable insights. In particular, while data summarization techniques have been studied extensively, only recently has summarizing interconnected data, or graphs, become popular. This survey is a structured, comprehensive overview of the state-of-the-art methods for summarizing graph data. We first broach the motivation behind, and the challenges of, graph summarization. We then categorize summarization approaches by the type of graphs taken as input and further organize each category by core methodology. Finally, we discuss applications of summarization on real-world graphs and conclude by describing some open problems in the field.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2018-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04933",
        "title": "Dynamical Kinds and their Discovery",
        "authors": [
            "Benjamin C. Jantzen"
        ],
        "abstract": "We demonstrate the possibility of classifying causal systems into kinds that share a common structure without first constructing an explicit dynamical model or using prior knowledge of the system dynamics. The algorithmic ability to determine whether arbitrary systems are governed by causal relations of the same form offers significant practical applications in the development and validation of dynamical models. It is also of theoretical interest as an essential stage in the scientific inference of laws from empirical data. The algorithm presented is based on the dynamical symmetry approach to dynamical kinds. A dynamical symmetry with respect to time is an intervention on one or more variables of a system that commutes with the time evolution of the system. A dynamical kind is a class of systems sharing a set of dynamical symmetries. The algorithm presented classifies deterministic, time-dependent causal systems by directly comparing their exhibited symmetries. Using simulated, noisy data from a variety of nonlinear systems, we show that this algorithm correctly sorts systems into dynamical kinds. It is robust under significant sampling error, is immune to violations of normality in sampling error, and fails gracefully with increasing dynamical similarity. The algorithm we demonstrate is the first to address this aspect of automated scientific discovery.\n    ",
        "submission_date": "2016-12-15T00:00:00",
        "last_modified_date": "2016-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04936",
        "title": "Learning through Dialogue Interactions by Asking Questions",
        "authors": [
            "Jiwei Li",
            "Alexander H. Miller",
            "Sumit Chopra",
            "Marc'Aurelio Ranzato",
            "Jason Weston"
        ],
        "abstract": "A good dialogue agent should have the ability to interact with users by both responding to questions and by asking questions, and importantly to learn from both types of interaction. In this work, we explore this direction by designing a simulator and a set of synthetic tasks in the movie domain that allow such interactions between a learner and a teacher. We investigate how a learner can benefit from asking questions in both offline and online reinforcement learning settings, and demonstrate that the learner improves when asking questions. Finally, real experiments with Mechanical Turk validate the approach. Our work represents a first step in developing such end-to-end learned interactive dialogue agents.\n    ",
        "submission_date": "2016-12-15T00:00:00",
        "last_modified_date": "2017-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04988",
        "title": "TeKnowbase: Towards Construction of a Knowledge-base of Technical Concepts",
        "authors": [
            "Prajna Upadhyay",
            "Tanuma Patra",
            "Ashwini Purkar",
            "Maya Ramanath"
        ],
        "abstract": "In this paper, we describe the construction of TeKnowbase, a knowledge-base of technical concepts in computer science. Our main information sources are technical websites such as Webopedia and Techtarget as well as Wikipedia and online textbooks. We divide the knowledge-base construction problem into two parts -- the acquisition of entities and the extraction of relationships among these entities. Our knowledge-base consists of approximately 100,000 triples. We conducted an evaluation on a sample of triples and report an accuracy of a little over 90\\%. We additionally conducted classification experiments on StackOverflow data with features from TeKnowbase and achieved improved classification accuracy.\n    ",
        "submission_date": "2016-12-15T00:00:00",
        "last_modified_date": "2016-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05048",
        "title": "Adversarial Message Passing For Graphical Models",
        "authors": [
            "Theofanis Karaletsos"
        ],
        "abstract": "Bayesian inference on structured models typically relies on the ability to infer posterior distributions of underlying hidden variables. However, inference in implicit models or complex posterior distributions is hard. A popular tool for learning implicit models are generative adversarial networks (GANs) which learn parameters of generators by fooling discriminators. Typically, GANs are considered to be models themselves and are not understood in the context of inference. Current techniques rely on inefficient global discrimination of joint distributions to perform learning, or only consider discriminating a single output variable. We overcome these limitations by treating GANs as a basis for likelihood-free inference in generative models and generalize them to Bayesian posterior inference over factor graphs. We propose local learning rules based on message passing minimizing a global divergence criterion involving cooperating local adversaries used to sidestep explicit likelihood evaluations. This allows us to compose models and yields a unified inference and learning framework for adversarial learning. Our framework treats model specification and inference separately and facilitates richly structured models within the family of Directed Acyclic Graphs, including components such as intractable likelihoods, non-differentiable models, simulators and generally cumbersome models. A key result of our treatment is the insight that Bayesian inference on structured models can be performed only with sampling and discrimination when using nonparametric variational families, without access to explicit distributions. As a side-result, we discuss the link to likelihood maximization. These approaches hold promise to be useful in the toolbox of probabilistic modelers and enrich the gamut of current probabilistic programming applications.\n    ",
        "submission_date": "2016-12-15T00:00:00",
        "last_modified_date": "2016-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05159",
        "title": "Separation of Concerns in Reinforcement Learning",
        "authors": [
            "Harm van Seijen",
            "Mehdi Fatemi",
            "Joshua Romoff",
            "Romain Laroche"
        ],
        "abstract": "In this paper, we propose a framework for solving a single-agent task by using multiple agents, each focusing on different aspects of the task. This approach has two main advantages: 1) it allows for training specialized agents on different parts of the task, and 2) it provides a new way to transfer knowledge, by transferring trained agents. Our framework generalizes the traditional hierarchical decomposition, in which, at any moment in time, a single agent has control until it has solved its particular subtask. We illustrate our framework with empirical experiments on two domains.\n    ",
        "submission_date": "2016-12-15T00:00:00",
        "last_modified_date": "2017-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05251",
        "title": "Neural Networks for Joint Sentence Classification in Medical Paper Abstracts",
        "authors": [
            "Franck Dernoncourt",
            "Ji Young Lee",
            "Peter Szolovits"
        ],
        "abstract": "Existing models based on artificial neural networks (ANNs) for sentence classification often do not incorporate the context in which sentences appear, and classify sentences individually. However, traditional sentence classification approaches have been shown to greatly benefit from jointly classifying subsequent sentences, such as with conditional random fields. In this work, we present an ANN architecture that combines the effectiveness of typical ANN models to classify sentences in isolation, with the strength of structured prediction. Our model achieves state-of-the-art results on two different datasets for sequential sentence classification in medical abstracts.\n    ",
        "submission_date": "2016-12-15T00:00:00",
        "last_modified_date": "2016-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05299",
        "title": "A Survey of Inductive Biases for Factorial Representation-Learning",
        "authors": [
            "Karl Ridgeway"
        ],
        "abstract": "With the resurgence of interest in neural networks, representation learning has re-emerged as a central focus in artificial intelligence. Representation learning refers to the discovery of useful encodings of data that make domain-relevant information explicit. Factorial representations identify underlying independent causal factors of variation in data. A factorial representation is compact and faithful, makes the causal factors explicit, and facilitates human interpretation of data. Factorial representations support a variety of applications, including the generation of novel examples, indexing and search, novelty detection, and transfer learning.\n",
        "submission_date": "2016-12-15T00:00:00",
        "last_modified_date": "2016-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05502",
        "title": "Defensive Player Classification in the National Basketball Association",
        "authors": [
            "Neil Seward"
        ],
        "abstract": "The National Basketball Association(NBA) has expanded their data gathering and have heavily invested in new technologies to gather advanced performance metrics on players. This expanded data set allows analysts to use unique performance metrics in models to estimate and classify player performance. Instead of grouping players together based on physical attributes and positions played, analysts can group together players that play similar to each other based on these tracked metrics. Existing methods for player classification have typically used offensive metrics for clustering [1]. There have been attempts to classify players using past defensive metrics, but the lack of quality metrics has not produced promising results. The classifications presented in the paper use newly introduced defensive metrics to find different defensive positions for each player. Without knowing the number of categories that players can be cast into, Gaussian Mixture Models (GMM) can be applied to find the optimal number of clusters. In the model presented, five different defensive player types can be identified.\n    ",
        "submission_date": "2016-12-13T00:00:00",
        "last_modified_date": "2017-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05533",
        "title": "Deep Reinforcement Learning with Successor Features for Navigation across Similar Environments",
        "authors": [
            "Jingwei Zhang",
            "Jost Tobias Springenberg",
            "Joschka Boedecker",
            "Wolfram Burgard"
        ],
        "abstract": "In this paper we consider the problem of robot navigation in simple maze-like environments where the robot has to rely on its onboard sensors to perform the navigation task. In particular, we are interested in solutions to this problem that do not require localization, mapping or planning. Additionally, we require that our solution can quickly adapt to new situations (e.g., changing navigation goals and environments). To meet these criteria we frame this problem as a sequence of related reinforcement learning tasks. We propose a successor feature based deep reinforcement learning algorithm that can learn to transfer knowledge from previously mastered navigation tasks to new problem instances. Our algorithm substantially decreases the required learning time after the first task instance has been solved, which makes it easily adaptable to changing environments. We validate our method in both simulated and real robot experiments with a Robotino and compare it to a set of baseline methods including classical planning-based navigation.\n    ",
        "submission_date": "2016-12-16T00:00:00",
        "last_modified_date": "2017-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05535",
        "title": "Supervised Quantum Learning without Measurements",
        "authors": [
            "Unai Alvarez-Rodriguez",
            "Lucas Lamata",
            "Pablo Escandell-Montero",
            "Jos\u00e9 D. Mart\u00edn-Guerrero",
            "Enrique Solano"
        ],
        "abstract": "We propose a quantum machine learning algorithm for efficiently solving a class of problems encoded in quantum controlled unitary operations. The central physical mechanism of the protocol is the iteration of a quantum time-delayed equation that introduces feedback in the dynamics and eliminates the necessity of intermediate measurements. The performance of the quantum algorithm is analyzed by comparing the results obtained in numerical simulations with the outcome of classical machine learning methods for the same problem. The use of time-delayed equations enhances the toolbox of the field of quantum machine learning, which may enable unprecedented applications in quantum technologies.\n    ",
        "submission_date": "2016-12-16T00:00:00",
        "last_modified_date": "2017-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05596",
        "title": "Neuromorphic Deep Learning Machines",
        "authors": [
            "Emre Neftci",
            "Charles Augustine",
            "Somnath Paul",
            "Georgios Detorakis"
        ],
        "abstract": "An ongoing challenge in neuromorphic computing is to devise general and computationally efficient models of inference and learning which are compatible with the spatial and temporal constraints of the brain. One increasingly popular and successful approach is to take inspiration from inference and learning algorithms used in deep neural networks. However, the workhorse of deep learning, the gradient descent Back Propagation (BP) rule, often relies on the immediate availability of network-wide information stored with high-precision memory, and precise operations that are difficult to realize in neuromorphic hardware. Remarkably, recent work showed that exact backpropagated weights are not essential for learning deep representations. Random BP replaces feedback weights with random ones and encourages the network to adjust its feed-forward weights to learn pseudo-inverses of the (random) feedback weights. Building on these results, we demonstrate an event-driven random BP (eRBP) rule that uses an error-modulated synaptic plasticity for learning deep representations in neuromorphic computing hardware. The rule requires only one addition and two comparisons for each synaptic weight using a two-compartment leaky Integrate & Fire (I&F) neuron, making it very suitable for implementation in digital or mixed-signal neuromorphic hardware. Our results show that using eRBP, deep representations are rapidly learned, achieving nearly identical classification accuracies compared to artificial neural network simulations on GPUs, while being robust to neural and synaptic state quantizations during learning.\n    ",
        "submission_date": "2016-12-16T00:00:00",
        "last_modified_date": "2017-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05688",
        "title": "A User Simulator for Task-Completion Dialogues",
        "authors": [
            "Xiujun Li",
            "Zachary C. Lipton",
            "Bhuwan Dhingra",
            "Lihong Li",
            "Jianfeng Gao",
            "Yun-Nung Chen"
        ],
        "abstract": "Despite widespread interests in reinforcement-learning for task-oriented dialogue systems, several obstacles can frustrate research and development progress. First, reinforcement learners typically require interaction with the environment, so conventional dialogue corpora cannot be used directly. Second, each task presents specific challenges, requiring separate corpus of task-specific annotated data. Third, collecting and annotating human-machine or human-human conversations for task-oriented dialogues requires extensive domain knowledge. Because building an appropriate dataset can be both financially costly and time-consuming, one popular approach is to build a user simulator based upon a corpus of example dialogues. Then, one can train reinforcement learning agents in an online fashion as they interact with the simulator. Dialogue agents trained on these simulators can serve as an effective starting point. Once agents master the simulator, they may be deployed in a real environment to interact with humans, and continue to be trained online. To ease empirical algorithmic comparisons in dialogues, this paper introduces a new, publicly available simulation framework, where our simulator, designed for the movie-booking domain, leverages both rules and collected data. The simulator supports two tasks: movie ticket booking and movie seeking. Finally, we demonstrate several agents and detail the procedure to add and test your own agent in the proposed framework.\n    ",
        "submission_date": "2016-12-17T00:00:00",
        "last_modified_date": "2017-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05695",
        "title": "Reinforcement Learning Using Quantum Boltzmann Machines",
        "authors": [
            "Daniel Crawford",
            "Anna Levit",
            "Navid Ghadermarzy",
            "Jaspreet S. Oberoi",
            "Pooya Ronagh"
        ],
        "abstract": "We investigate whether quantum annealers with select chip layouts can outperform classical computers in reinforcement learning tasks. We associate a transverse field Ising spin Hamiltonian with a layout of qubits similar to that of a deep Boltzmann machine (DBM) and use simulated quantum annealing (SQA) to numerically simulate quantum sampling from this system. We design a reinforcement learning algorithm in which the set of visible nodes representing the states and actions of an optimal policy are the first and last layers of the deep network. In absence of a transverse field, our simulations show that DBMs are trained more effectively than restricted Boltzmann machines (RBM) with the same number of nodes. We then develop a framework for training the network as a quantum Boltzmann machine (QBM) in the presence of a significant transverse field for reinforcement learning. This method also outperforms the reinforcement learning method that uses RBMs.\n    ",
        "submission_date": "2016-12-17T00:00:00",
        "last_modified_date": "2019-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05729",
        "title": "Exploiting sparsity to build efficient kernel based collaborative filtering for top-N item recommendation",
        "authors": [
            "Mirko Polato",
            "Fabio Aiolli"
        ],
        "abstract": "The increasing availability of implicit feedback datasets has raised the interest in developing effective collaborative filtering techniques able to deal asymmetrically with unambiguous positive feedback and ambiguous negative feedback. In this paper, we propose a principled kernel-based collaborative filtering method for top-N item recommendation with implicit feedback. We present an efficient implementation using the linear kernel, and we show how to generalize it to kernels of the dot product family preserving the efficiency. We also investigate on the elements which influence the sparsity of a standard cosine kernel. This analysis shows that the sparsity of the kernel strongly depends on the properties of the dataset, in particular on the long tail distribution. We compare our method with state-of-the-art algorithms achieving good results both in terms of efficiency and effectiveness.\n    ",
        "submission_date": "2016-12-17T00:00:00",
        "last_modified_date": "2016-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05734",
        "title": "Web-based Semantic Similarity for Emotion Recognition in Web Objects",
        "authors": [
            "Valentina Franzoni",
            "Giulio Biondi",
            "Alfredo Milani",
            "Yuanxi Li"
        ],
        "abstract": "In this project we propose a new approach for emotion recognition using web-based similarity (e.g. confidence, PMI and PMING). We aim to extract basic emotions from short sentences with emotional content (e.g. news titles, tweets, captions), performing a web-based quantitative evaluation of semantic proximity between each word of the analyzed sentence and each emotion of a psychological model (e.g. Plutchik, Ekman, Lovheim). The phases of the extraction include: text preprocessing (tokenization, stop words, filtering), search engine automated query, HTML parsing of results (i.e. scraping), estimation of semantic proximity, ranking of emotions according to proximity measures. The main idea is that, since it is possible to generalize semantic similarity under the assumption that similar concepts co-occur in documents indexed in search engines, therefore also emotions can be generalized in the same way, through tags or terms that express them in a particular language, ranking emotions. Training results are compared to human evaluation, then additional comparative tests on results are performed, both for the global ranking correlation (e.g. Kendall, Spearman, Pearson) both for the evaluation of the emotion linked to each single word. Different from sentiment analysis, our approach works at a deeper level of abstraction, aiming at recognizing specific emotions and not only the positive/negative sentiment, in order to predict emotions as semantic data.\n    ",
        "submission_date": "2016-12-17T00:00:00",
        "last_modified_date": "2016-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05756",
        "title": "A Comment on Argumentation",
        "authors": [
            "Karl Schlechta"
        ],
        "abstract": "We use the theory of defaults and their meaning of [GS16] to develop (the outline of a) new theory of argumentation.\n    ",
        "submission_date": "2016-12-17T00:00:00",
        "last_modified_date": "2016-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05971",
        "title": "An Integrated Optimization + Learning Approach to Optimal Dynamic Pricing for the Retailer with Multi-type Customers in Smart Grids",
        "authors": [
            "Fanlin Meng",
            "Xiao-Jun Zeng",
            "Yan Zhang",
            "Chris J. Dent",
            "Dunwei Gong"
        ],
        "abstract": "In this paper, we consider a realistic and meaningful scenario in the context of smart grids where an electricity retailer serves three different types of customers, i.e., customers with an optimal home energy management system embedded in their smart meters (C-HEMS), customers with only smart meters (C-SM), and customers without smart meters (C-NONE). The main objective of this paper is to support the retailer to make optimal day-ahead dynamic pricing decisions in such a mixed customer pool. To this end, we propose a two-level decision-making framework where the retailer acting as upper-level agent firstly announces its electricity prices of next 24 hours and customers acting as lower-level agents subsequently schedule their energy usages accordingly. For the lower level problem, we model the price responsiveness of different customers according to their unique characteristics. For the upper level problem, we optimize the dynamic prices for the retailer to maximize its profit subject to realistic market constraints. The above two-level model is tackled by genetic algorithms (GA) based distributed optimization methods while its feasibility and effectiveness are confirmed via simulation results.\n    ",
        "submission_date": "2016-12-18T00:00:00",
        "last_modified_date": "2018-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06018",
        "title": "Self-Correcting Models for Model-Based Reinforcement Learning",
        "authors": [
            "Erik Talvitie"
        ],
        "abstract": "When an agent cannot represent a perfectly accurate model of its environment's dynamics, model-based reinforcement learning (MBRL) can fail catastrophically. Planning involves composing the predictions of the model; when flawed predictions are composed, even minor errors can compound and render the model useless for planning. Hallucinated Replay (Talvitie 2014) trains the model to \"correct\" itself when it produces errors, substantially improving MBRL with flawed models. This paper theoretically analyzes this approach, illuminates settings in which it is likely to be effective or ineffective, and presents a novel error bound, showing that a model's ability to self-correct is more tightly related to MBRL performance than one-step prediction error. These results inspire an MBRL algorithm for deterministic MDPs with performance guarantees that are robust to model class limitations.\n    ",
        "submission_date": "2016-12-19T00:00:00",
        "last_modified_date": "2017-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06043",
        "title": "An Empirical Study of Adequate Vision Span for Attention-Based Neural Machine Translation",
        "authors": [
            "Raphael Shu",
            "Hideki Nakayama"
        ],
        "abstract": "Recently, the attention mechanism plays a key role to achieve high performance for Neural Machine Translation models. However, as it computes a score function for the encoder states in all positions at each decoding step, the attention model greatly increases the computational complexity. In this paper, we investigate the adequate vision span of attention models in the context of machine translation, by proposing a novel attention framework that is capable of reducing redundant score computation dynamically. The term \"vision span\" means a window of the encoder states considered by the attention model in one step. In our experiments, we found that the average window size of vision span can be reduced by over 50% with modest loss in accuracy on English-Japanese and German-English translation tasks.% This results indicate that the conventional attention mechanism performs a significant amount of redundant computation.\n    ",
        "submission_date": "2016-12-19T00:00:00",
        "last_modified_date": "2017-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06062",
        "title": "Improving Tweet Representations using Temporal and User Context",
        "authors": [
            "Ganesh J",
            "Manish Gupta",
            "Vasudeva Varma"
        ],
        "abstract": "In this work we propose a novel representation learning model which computes semantic representations for tweets accurately. Our model systematically exploits the chronologically adjacent tweets ('context') from users' Twitter timelines for this task. Further, we make our model user-aware so that it can do well in modeling the target tweet by exploiting the rich knowledge about the user such as the way the user writes the post and also summarizing the topics on which the user writes. We empirically demonstrate that the proposed models outperform the state-of-the-art models in predicting the user profile attributes like spouse, education and job by 19.66%, 2.27% and 2.22% respectively.\n    ",
        "submission_date": "2016-12-19T00:00:00",
        "last_modified_date": "2016-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06174",
        "title": "A modified Physarum-inspired model for the user equilibrium traffic assignment problem",
        "authors": [
            "Shuai Xu",
            "Wen Jiang",
            "Yehang Shou"
        ],
        "abstract": "The user equilibrium traffic assignment principle is very important in the traffic assignment problem. Mathematical programming models are designed to solve the user equilibrium problem in traditional algorithms. Recently, the Physarum shows the ability to address the user equilibrium and system optimization traffic assignment problems. However, the Physarum model are not efficient in real traffic networks with two-way traffic characteristics and multiple origin-destination pairs. In this article, a modified Physarum-inspired model for the user equilibrium problem is proposed. By decomposing traffic flux based on origin nodes, the traffic flux from different origin-destination pairs can be distinguished in the proposed model. The Physarum can obtain the equilibrium traffic flux when no shorter path can be discovered between each origin-destination pair. Finally, numerical examples use the Sioux Falls network to demonstrate the rationality and convergence properties of the proposed model.\n    ",
        "submission_date": "2016-12-19T00:00:00",
        "last_modified_date": "2016-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06340",
        "title": "Computing Human-Understandable Strategies",
        "authors": [
            "Sam Ganzfried",
            "Farzana Yusuf"
        ],
        "abstract": "Algorithms for equilibrium computation generally make no attempt to ensure that the computed strategies are understandable by humans. For instance the strategies for the strongest poker agents are represented as massive binary files. In many situations, we would like to compute strategies that can actually be implemented by humans, who may have computational limitations and may only be able to remember a small number of features or components of the strategies that have been computed. We study poker games where private information distributions can be arbitrary. We create a large training set of game instances and solutions, by randomly selecting the information probabilities, and present algorithms that learn from the training instances in order to perform well in games with unseen information distributions. We are able to conclude several new fundamental rules about poker strategy that can be easily implemented by humans.\n    ",
        "submission_date": "2016-12-19T00:00:00",
        "last_modified_date": "2017-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06370",
        "title": "Learning Features by Watching Objects Move",
        "authors": [
            "Deepak Pathak",
            "Ross Girshick",
            "Piotr Doll\u00e1r",
            "Trevor Darrell",
            "Bharath Hariharan"
        ],
        "abstract": "This paper presents a novel yet intuitive approach to unsupervised feature learning. Inspired by the human visual system, we explore whether low-level motion-based grouping cues can be used to learn an effective visual representation. Specifically, we use unsupervised motion-based segmentation on videos to obtain segments, which we use as 'pseudo ground truth' to train a convolutional network to segment objects from a single frame. Given the extensive evidence that motion plays a key role in the development of the human visual system, we hope that this straightforward approach to unsupervised learning will be more effective than cleverly designed 'pretext' tasks studied in the literature. Indeed, our extensive experiments show that this is the case. When used for transfer learning on object detection, our representation significantly outperforms previous unsupervised approaches across multiple settings, especially when training data for the target task is scarce.\n    ",
        "submission_date": "2016-12-19T00:00:00",
        "last_modified_date": "2017-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06476",
        "title": "Computational Complexity of Testing Proportional Justified Representation",
        "authors": [
            "Haris Aziz",
            "Shenwei Huang"
        ],
        "abstract": "We consider a committee voting setting in which each voter approves of a subset of candidates and based on the approvals, a target number of candidates are selected. Aziz et al. (2015) proposed two representation axioms called justified representation and extended justified representation. Whereas the former can be tested as well as achieved in polynomial time, the latter property is coNP-complete to test and no polynomial-time algorithm is known to achieve it. Interestingly, S{\u00e1}nchez-Fern{\u00e1}ndez et~al. (2016) proposed an intermediate property called proportional justified representation that admits a polynomial-time algorithm to achieve. The complexity of testing proportional justified representation has remained an open problem. In this paper, we settle the complexity by proving that testing proportional justified representation is coNP-complete. We complement the complexity result by showing that the problem admits efficient algorithms if any of the following parameters are bounded: (1) number of voters (2) number of candidates (3) maximum number of candidates approved by a voter (4) maximum number of voters approving a given candidate.\n    ",
        "submission_date": "2016-12-20T00:00:00",
        "last_modified_date": "2017-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06505",
        "title": "Parallelized Tensor Train Learning of Polynomial Classifiers",
        "authors": [
            "Zhongming Chen",
            "Kim Batselier",
            "Johan A.K. Suykens",
            "Ngai Wong"
        ],
        "abstract": "In pattern classification, polynomial classifiers are well-studied methods as they are capable of generating complex decision surfaces. Unfortunately, the use of multivariate polynomials is limited to kernels as in support vector machines, because polynomials quickly become impractical for high-dimensional problems. In this paper, we effectively overcome the curse of dimensionality by employing the tensor train format to represent a polynomial classifier. Based on the structure of tensor trains, two learning algorithms are proposed which involve solving different optimization problems of low computational complexity. Furthermore, we show how both regularization to prevent overfitting and parallelization, which enables the use of large training sets, are incorporated into these methods. Both the efficiency and efficacy of our tensor-based polynomial classifier are then demonstrated on the two popular datasets USPS and MNIST.\n    ",
        "submission_date": "2016-12-20T00:00:00",
        "last_modified_date": "2017-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06704",
        "title": "Action-Driven Object Detection with Top-Down Visual Attentions",
        "authors": [
            "Donggeun Yoo",
            "Sunggyun Park",
            "Kyunghyun Paeng",
            "Joon-Young Lee",
            "In So Kweon"
        ],
        "abstract": "A dominant paradigm for deep learning based object detection relies on a \"bottom-up\" approach using \"passive\" scoring of class agnostic proposals. These approaches are efficient but lack of holistic analysis of scene-level context. In this paper, we present an \"action-driven\" detection mechanism using our \"top-down\" visual attention model. We localize an object by taking sequential actions that the attention model provides. The attention model conditioned with an image region provides required actions to get closer toward a target object. An action at each time step is weak itself but an ensemble of the sequential actions makes a bounding-box accurately converge to a target object boundary. This attention model we call AttentionNet is composed of a convolutional neural network. During our whole detection procedure, we only utilize the actions from a single AttentionNet without any modules for object proposals nor post bounding-box regression. We evaluate our top-down detection mechanism over the PASCAL VOC series and ILSVRC CLS-LOC dataset, and achieve state-of-the-art performances compared to the major bottom-up detection methods. In particular, our detection mechanism shows a strong advantage in elaborate localization by outperforming Faster R-CNN with a margin of +7.1% over PASCAL VOC 2007 when we increase the IoU threshold for positive detection to 0.7.\n    ",
        "submission_date": "2016-12-20T00:00:00",
        "last_modified_date": "2016-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06962",
        "title": "Stochastic Runtime Analysis of a Cross Entropy Algorithm for Traveling Salesman Problems",
        "authors": [
            "Zijun Wu",
            "Rolf Moehring",
            "Jianhui Lai"
        ],
        "abstract": "This article analyzes the stochastic runtime of a Cross-Entropy Algorithm on two classes of traveling salesman problems. The algorithm shares main features of the famous Max-Min Ant System with iteration-best reinforcement.\n",
        "submission_date": "2016-12-21T00:00:00",
        "last_modified_date": "2017-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07025",
        "title": "Boolean kernels for collaborative filtering in top-N item recommendation",
        "authors": [
            "Mirko Polato",
            "Fabio Aiolli"
        ],
        "abstract": "In many personalized recommendation problems available data consists only of positive interactions (implicit feedback) between users and items. This problem is also known as One-Class Collaborative Filtering (OC-CF). Linear models usually achieve state-of-the-art performances on OC-CF problems and many efforts have been devoted to build more expressive and complex representations able to improve the recommendations. Recent analysis show that collaborative filtering (CF) datasets have peculiar characteristics such as high sparsity and a long tailed distribution of the ratings. In this paper we propose a boolean kernel, called Disjunctive kernel, which is less expressive than the linear one but it is able to alleviate the sparsity issue in CF contexts. The embedding of this kernel is composed by all the combinations of a certain arity d of the input variables, and these combined features are semantically interpreted as disjunctions of the input variables. Experiments on several CF datasets show the effectiveness and the efficiency of the proposed kernel.\n    ",
        "submission_date": "2016-12-21T00:00:00",
        "last_modified_date": "2017-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07139",
        "title": "A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement to Imitation",
        "authors": [
            "Lei Tai",
            "Jingwei Zhang",
            "Ming Liu",
            "Joschka Boedecker",
            "Wolfram Burgard"
        ],
        "abstract": "Deep learning techniques have been widely applied, achieving state-of-the-art results in various fields of study. This survey focuses on deep learning solutions that target learning control policies for robotics applications. We carry out our discussions on the two main paradigms for learning control with deep networks: deep reinforcement learning and imitation learning. For deep reinforcement learning (DRL), we begin from traditional reinforcement learning algorithms, showing how they are extended to the deep context and effective mechanisms that could be added on top of the DRL algorithms. We then introduce representative works that utilize DRL to solve navigation and manipulation tasks in robotics. We continue our discussion on methods addressing the challenge of the reality gap for transferring DRL policies trained in simulation to real-world scenarios, and summarize robotics simulation platforms for conducting DRL research. For imitation leaning, we go through its three main categories, behavior cloning, inverse reinforcement learning and generative adversarial imitation learning, by introducing their formulations and their corresponding robotics applications. Finally, we discuss the open challenges and research frontiers.\n    ",
        "submission_date": "2016-12-21T00:00:00",
        "last_modified_date": "2018-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07294",
        "title": "Understanding Error Correction and its Role as Part of the Communication Channel in Environments composed of Self-Integrating Systems",
        "authors": [
            "Aleksander Lodwich"
        ],
        "abstract": "The raise of complexity of technical systems also raises knowledge required to set them up and to maintain them. The cost to evolve such systems can be prohibitive. In the field of Autonomic Computing, technical systems should therefore have various self-healing capabilities allowing system owners to provide only partial, potentially inconsistent updates of the system. The self-healing or self-integrating system shall find out the remaining changes to communications and functionalities in order to accommodate change and yet still restore function. This issue becomes even more interesting in context of Internet of Things and Industrial Internet where previously unexpected device combinations can be assembled in order to provide a surprising new function. In order to pursue higher levels of self-integration capabilities I propose to think of self-integration as sophisticated error correcting communications. Therefore, this paper discusses an extended scope of error correction with the purpose to emphasize error correction's role as an integrated element of bi-directional communication channels in self-integrating, autonomic communication scenarios.\n    ",
        "submission_date": "2016-12-21T00:00:00",
        "last_modified_date": "2016-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07512",
        "title": "Causal Effect Identification in Acyclic Directed Mixed Graphs and Gated Models",
        "authors": [
            "Jose M. Pe\u00f1a",
            "Marcus Bendtsen"
        ],
        "abstract": "We introduce a new family of graphical models that consists of graphs with possibly directed, undirected and bidirected edges but without directed cycles. We show that these models are suitable for representing causal models with additive error terms. We provide a set of sufficient graphical criteria for the identification of arbitrary causal effects when the new models contain directed and undirected edges but no bidirected edge. We also provide a necessary and sufficient graphical criterion for the identification of the causal effect of a single variable on the rest of the variables. Moreover, we develop an exact algorithm for learning the new models from observational and interventional data via answer set programming. Finally, we introduce gated models for causal effect identification, a new family of graphical models that exploits context specific independences to identify additional causal effects.\n    ",
        "submission_date": "2016-12-22T00:00:00",
        "last_modified_date": "2017-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07601",
        "title": "Counting Answer Sets via Dynamic Programming",
        "authors": [
            "Johannes Fichte",
            "Markus Hecher",
            "Michael Morak",
            "Stefan Woltran"
        ],
        "abstract": "While the solution counting problem for propositional satisfiability (#SAT) has received renewed attention in recent years, this research trend has not affected other AI solving paradigms like answer set programming (ASP). Although ASP solvers are designed to enumerate all solutions, and counting can therefore be easily done, the involved materialization of all solutions is a clear bottleneck for the counting problem of ASP (#ASP). In this paper we propose dynamic programming-based #ASP algorithms that exploit the structure of the underlying (ground) ASP program. Experimental results for a prototype implementation show promise when compared to existing solvers.\n    ",
        "submission_date": "2016-12-22T00:00:00",
        "last_modified_date": "2016-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07603",
        "title": "Difficulty Adjustable and Scalable Constrained Multi-objective Test Problem Toolkit",
        "authors": [
            "Zhun Fan",
            "Wenji Li",
            "Xinye Cai",
            "Hui Li",
            "Caimin Wei",
            "Qingfu Zhang",
            "Kalyanmoy Deb",
            "Erik D. Goodman"
        ],
        "abstract": "Multi-objective evolutionary algorithms (MOEAs) have progressed significantly in recent decades, but most of them are designed to solve unconstrained multi-objective optimization problems. In fact, many real-world multi-objective problems contain a number of constraints. To promote research on constrained multi-objective optimization, we first propose a problem classification scheme with three primary types of difficulty, which reflect various types of challenges presented by real-world optimization problems, in order to characterize the constraint functions in constrained multi-objective optimization problems (CMOPs). These are feasibility-hardness, convergence-hardness and diversity-hardness. We then develop a general toolkit to construct difficulty-adjustable and scalable CMOPs (DAS-CMOPs, or DAS-CMaOPs when the number of objectives is greater than three) with three types of parameterized constraint functions developed to capture the three proposed types of difficulty. Based on this toolkit, we suggest nine difficulty-adjustable and scalable CMOPs and nine CMaOPs. The experimental results reveal that mechanisms in MOEA/D-CDP may be more effective in solving convergence-hard DAS-CMOPs, while mechanisms of NSGA-II-CDP may be more effective in solving DAS-CMOPs with simultaneous diversity-, feasibility- and convergence-hardness. Mechanisms in C-NSGA-III may be more effective in solving feasibility-hard CMaOPs, while mechanisms of C-MOEA/DD may be more effective in solving CMaOPs with convergence-hardness. In addition, none of them can solve these problems efficiently, which stimulates us to continue to develop new CMOEAs and CMaOEAs to solve the suggested DAS-CMOPs and DAS-CMaOPs.\n    ",
        "submission_date": "2016-12-21T00:00:00",
        "last_modified_date": "2019-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07771",
        "title": "Highway and Residual Networks learn Unrolled Iterative Estimation",
        "authors": [
            "Klaus Greff",
            "Rupesh K. Srivastava",
            "J\u00fcrgen Schmidhuber"
        ],
        "abstract": "The past year saw the introduction of new architectures such as Highway networks and Residual networks which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer.\n",
        "submission_date": "2016-12-22T00:00:00",
        "last_modified_date": "2017-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07837",
        "title": "SampleRNN: An Unconditional End-to-End Neural Audio Generation Model",
        "authors": [
            "Soroush Mehri",
            "Kundan Kumar",
            "Ishaan Gulrajani",
            "Rithesh Kumar",
            "Shubham Jain",
            "Jose Sotelo",
            "Aaron Courville",
            "Yoshua Bengio"
        ],
        "abstract": "In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.\n    ",
        "submission_date": "2016-12-22T00:00:00",
        "last_modified_date": "2017-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07919",
        "title": "EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis",
        "authors": [
            "Mehdi S. M. Sajjadi",
            "Bernhard Sch\u00f6lkopf",
            "Michael Hirsch"
        ],
        "abstract": "Single image super-resolution is the task of inferring a high-resolution image from a single low-resolution input. Traditionally, the performance of algorithms for this task is measured using pixel-wise reconstruction measures such as peak signal-to-noise ratio (PSNR) which have been shown to correlate poorly with the human perception of image quality. As a result, algorithms minimizing these metrics tend to produce over-smoothed images that lack high-frequency textures and do not look natural despite yielding high PSNR values.\n",
        "submission_date": "2016-12-23T00:00:00",
        "last_modified_date": "2017-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08048",
        "title": "Liquid Democracy: An Analysis in Binary Aggregation and Diffusion",
        "authors": [
            "Zo\u00e9 Christoff",
            "Davide Grossi"
        ],
        "abstract": "The paper proposes an analysis of liquid democracy (or, delegable proxy voting) from the perspective of binary aggregation and of binary diffusion models. We show how liquid democracy on binary issues can be embedded into the framework of binary aggregation with abstentions, enabling the transfer of known results about the latter---such as impossibility theorems---to the former. This embedding also sheds light on the relation between delegation cycles in liquid democracy and the probability of collective abstentions, as well as the issue of individual rationality in a delegable proxy voting setting. We then show how liquid democracy on binary issues can be modeled and analyzed also as a specific process of dynamics of binary opinions on networks. These processes---called Boolean DeGroot processes---are a special case of the DeGroot stochastic model of opinion diffusion. We establish the convergence conditions of such processes and show they provide some novel insights on how the effects of delegation cycles and individual rationality could be mitigated within liquid democracy.\n",
        "submission_date": "2016-12-23T00:00:00",
        "last_modified_date": "2017-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08544",
        "title": "Theory-guided Data Science: A New Paradigm for Scientific Discovery from Data",
        "authors": [
            "Anuj Karpatne",
            "Gowtham Atluri",
            "James Faghmous",
            "Michael Steinbach",
            "Arindam Banerjee",
            "Auroop Ganguly",
            "Shashi Shekhar",
            "Nagiza Samatova",
            "Vipin Kumar"
        ],
        "abstract": "Data science models, although successful in a number of commercial domains, have had limited applicability in scientific problems involving complex physical phenomena. Theory-guided data science (TGDS) is an emerging paradigm that aims to leverage the wealth of scientific knowledge for improving the effectiveness of data science models in enabling scientific discovery. The overarching vision of TGDS is to introduce scientific consistency as an essential component for learning generalizable models. Further, by producing scientifically interpretable models, TGDS aims to advance our scientific understanding by discovering novel domain insights. Indeed, the paradigm of TGDS has started to gain prominence in a number of scientific disciplines such as turbulence modeling, material discovery, quantum chemistry, bio-medical science, bio-marker discovery, climate science, and hydrology. In this paper, we formally conceptualize the paradigm of TGDS and present a taxonomy of research themes in TGDS. We describe several approaches for integrating domain knowledge in different research themes using illustrative examples from different disciplines. We also highlight some of the promising avenues of novel research for realizing the full potential of theory-guided data science.\n    ",
        "submission_date": "2016-12-27T00:00:00",
        "last_modified_date": "2017-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08810",
        "title": "The Predictron: End-To-End Learning and Planning",
        "authors": [
            "David Silver",
            "Hado van Hasselt",
            "Matteo Hessel",
            "Tom Schaul",
            "Arthur Guez",
            "Tim Harley",
            "Gabriel Dulac-Arnold",
            "David Reichert",
            "Neil Rabinowitz",
            "Andre Barreto",
            "Thomas Degris"
        ],
        "abstract": "One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple \"imagined\" planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.\n    ",
        "submission_date": "2016-12-28T00:00:00",
        "last_modified_date": "2017-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08825",
        "title": "Accelerated Convolutions for Efficient Multi-Scale Time to Contact Computation in Julia",
        "authors": [
            "Alexander Amini",
            "Berthold Horn",
            "Alan Edelman"
        ],
        "abstract": "Convolutions have long been regarded as fundamental to applied mathematics, physics and engineering. Their mathematical elegance allows for common tasks such as numerical differentiation to be computed efficiently on large data sets. Efficient computation of convolutions is critical to artificial intelligence in real-time applications, like machine vision, where convolutions must be continuously and efficiently computed on tens to hundreds of kilobytes per second. In this paper, we explore how convolutions are used in fundamental machine vision applications. We present an accelerated n-dimensional convolution package in the high performance computing language, Julia, and demonstrate its efficacy in solving the time to contact problem for machine vision. Results are measured against synthetically generated videos and quantitatively assessed according to their mean squared error from the ground truth. We achieve over an order of magnitude decrease in compute time and allocated memory for comparable machine vision applications. All code is packaged and integrated into the official Julia Package Manager to be used in various other scenarios.\n    ",
        "submission_date": "2016-12-28T00:00:00",
        "last_modified_date": "2016-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08843",
        "title": "FastMask: Segment Multi-scale Object Candidates in One Shot",
        "authors": [
            "Hexiang Hu",
            "Shiyi Lan",
            "Yuning Jiang",
            "Zhimin Cao",
            "Fei Sha"
        ],
        "abstract": "Objects appear to scale differently in natural images. This fact requires methods dealing with object-centric tasks (e.g. object proposal) to have robust performance over variances in object scales. In the paper, we present a novel segment proposal framework, namely FastMask, which takes advantage of hierarchical features in deep convolutional neural networks to segment multi-scale objects in one shot. Innovatively, we adapt segment proposal network into three different functional components (body, neck and head). We further propose a weight-shared residual neck module as well as a scale-tolerant attentional head module for efficient one-shot inference. On MS COCO benchmark, the proposed FastMask outperforms all state-of-the-art segment proposal methods in average recall being 2~5 times faster. Moreover, with a slight trade-off in accuracy, FastMask can segment objects in near real time (~13 fps) with 800*600 resolution images, demonstrating its potential in practical applications. Our implementation is available on ",
        "submission_date": "2016-12-28T00:00:00",
        "last_modified_date": "2017-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08845",
        "title": "The formal-logical characterisation of lies, deception, and associated notions",
        "authors": [
            "Toni Heidenreich"
        ],
        "abstract": "Defining various dishonest notions in a formal way is a key step to enable intelligent agents to act in untrustworthy environments. This review evaluates the literature for this topic by looking at formal definitions based on modal logic as well as other formal approaches. Criteria from philosophical groundwork is used to assess the definitions for correctness and completeness. The key contribution of this review is to show that only a few definitions fully comply with this gold standard and to point out the missing steps towards a successful application of these definitions in an actual agent environment.\n    ",
        "submission_date": "2016-12-28T00:00:00",
        "last_modified_date": "2016-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.09030",
        "title": "Meta-Unsupervised-Learning: A supervised approach to unsupervised learning",
        "authors": [
            "Vikas K. Garg",
            "Adam Tauman Kalai"
        ],
        "abstract": "We introduce a new paradigm to investigate unsupervised learning, reducing unsupervised learning to supervised learning. Specifically, we mitigate the subjectivity in unsupervised decision-making by leveraging knowledge acquired from prior, possibly heterogeneous, supervised learning tasks. We demonstrate the versatility of our framework via comprehensive expositions and detailed experiments on several unsupervised problems such as (a) clustering, (b) outlier detection, and (c) similarity prediction under a common umbrella of meta-unsupervised-learning. We also provide rigorous PAC-agnostic bounds to establish the theoretical foundations of our framework, and show that our framing of meta-clustering circumvents Kleinberg's impossibility theorem for clustering.\n    ",
        "submission_date": "2016-12-29T00:00:00",
        "last_modified_date": "2017-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.09134",
        "title": "From Virtual to Real World Visual Perception using Domain Adaptation -- The DPM as Example",
        "authors": [
            "Antonio M. Lopez",
            "Jiaolong Xu",
            "Jose L. Gomez",
            "David Vazquez",
            "German Ros"
        ],
        "abstract": "Supervised learning tends to produce more accurate classifiers than unsupervised learning in general. This implies that training data is preferred with annotations. When addressing visual perception challenges, such as localizing certain object classes within an image, the learning of the involved classifiers turns out to be a practical bottleneck. The reason is that, at least, we have to frame object examples with bounding boxes in thousands of images. A priori, the more complex the model is regarding its number of parameters, the more annotated examples are required. This annotation task is performed by human oracles, which ends up in inaccuracies and errors in the annotations (aka ground truth) since the task is inherently very cumbersome and sometimes ambiguous. As an alternative we have pioneered the use of virtual worlds for collecting such annotations automatically and with high precision. However, since the models learned with virtual data must operate in the real world, we still need to perform domain adaptation (DA). In this chapter we revisit the DA of a deformable part-based model (DPM) as an exemplifying case of virtual- to-real-world DA. As a use case, we address the challenge of vehicle detection for driver assistance, using different publicly available virtual-world data. While doing so, we investigate questions such as: how does the domain gap behave due to virtual-vs-real data with respect to dominant object appearance per domain, as well as the role of photo-realism in the virtual world.\n    ",
        "submission_date": "2016-12-29T00:00:00",
        "last_modified_date": "2016-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.09205",
        "title": "Deep neural heart rate variability analysis",
        "authors": [
            "Tamas Madl"
        ],
        "abstract": "Despite of the pain and limited accuracy of blood tests for early recognition of cardiovascular disease, they dominate risk screening and triage. On the other hand, heart rate variability is non-invasive and cheap, but not considered accurate enough for clinical practice. Here, we tackle heart beat interval based classification with deep learning. We introduce an end to end differentiable hybrid architecture, consisting of a layer of biological neuron models of cardiac dynamics (modified FitzHugh Nagumo neurons) and several layers of a standard feed-forward neural network. The proposed model is evaluated on ECGs from 474 stable at-risk (coronary artery disease) patients, and 1172 chest pain patients of an emergency department. We show that it can significantly outperform models based on traditional heart rate variability predictors, as well as approaching or in some cases outperforming clinical blood tests, based only on 60 seconds of inter-beat intervals.\n    ",
        "submission_date": "2016-12-29T00:00:00",
        "last_modified_date": "2016-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.09251",
        "title": "Lifted Relational Algebra with Recursion and Connections to Modal Logic",
        "authors": [
            "Eugenia Ternovska"
        ],
        "abstract": "We propose a new formalism for specifying and reasoning about problems that involve heterogeneous \"pieces of information\" -- large collections of data, decision procedures of any kind and complexity and connections between them. The essence of our proposal is to lift Codd's relational algebra from operations on relational tables to operations on classes of structures (with recursion), and to add a direction of information propagation. We observe the presence of information propagation in several formalisms for efficient reasoning and use it to express unary negation and operations used in graph databases. We carefully analyze several reasoning tasks and establish a precise connection between a generalized query evaluation and temporal logic model checking. Our development allows us to reveal a general correspondence between classical and modal logics and may shed a new light on the good computational properties of modal logics and related formalisms.\n    ",
        "submission_date": "2016-12-29T00:00:00",
        "last_modified_date": "2016-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.09327",
        "title": "Intelligent information extraction based on artificial neural network",
        "authors": [
            "Ahlam Ansari",
            "Moonish Maknojia",
            "Altamash Shaikh"
        ],
        "abstract": "Question Answering System (QAS) is used for information retrieval and natural language processing (NLP) to reduce human effort. There are numerous QAS based on the user documents present today, but they all are limited to providing objective answers and process simple questions only. Complex questions cannot be answered by the existing QAS, as they require interpretation of the current and old data as well as the question asked by the user. The above limitations can be overcome by using deep cases and neural network. Hence we propose a modified QAS in which we create a deep artificial neural network with associative memory from text documents. The modified QAS processes the contents of the text document provided to it and find the answer to even complex questions in the documents.\n    ",
        "submission_date": "2016-04-11T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.09465",
        "title": "Adaptive Lambda Least-Squares Temporal Difference Learning",
        "authors": [
            "Timothy A. Mann",
            "Hugo Penedones",
            "Shie Mannor",
            "Todd Hester"
        ],
        "abstract": "Temporal Difference learning or TD($\\lambda$) is a fundamental algorithm in the field of reinforcement learning. However, setting TD's $\\lambda$ parameter, which controls the timescale of TD updates, is generally left up to the practitioner. We formalize the $\\lambda$ selection problem as a bias-variance trade-off where the solution is the value of $\\lambda$ that leads to the smallest Mean Squared Value Error (MSVE). To solve this trade-off we suggest applying Leave-One-Trajectory-Out Cross-Validation (LOTO-CV) to search the space of $\\lambda$ values. Unfortunately, this approach is too computationally expensive for most practical applications. For Least Squares TD (LSTD) we show that LOTO-CV can be implemented efficiently to automatically tune $\\lambda$ and apply function optimization methods to efficiently search the space of $\\lambda$ values. The resulting algorithm, ALLSTD, is parameter free and our experiments demonstrate that ALLSTD is significantly computationally faster than the na\u00efve LOTO-CV implementation while achieving similar performance.\n    ",
        "submission_date": "2016-12-30T00:00:00",
        "last_modified_date": "2016-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.09542",
        "title": "A Joint Speaker-Listener-Reinforcer Model for Referring Expressions",
        "authors": [
            "Licheng Yu",
            "Hao Tan",
            "Mohit Bansal",
            "Tamara L. Berg"
        ],
        "abstract": "Referring expressions are natural language constructions used to identify particular objects within a scene. In this paper, we propose a unified framework for the tasks of referring expression comprehension and generation. Our model is composed of three modules: speaker, listener, and reinforcer. The speaker generates referring expressions, the listener comprehends referring expressions, and the reinforcer introduces a reward function to guide sampling of more discriminative expressions. The listener-speaker modules are trained jointly in an end-to-end learning framework, allowing the modules to be aware of one another during learning while also benefiting from the discriminative reinforcer's feedback. We demonstrate that this unified framework and training achieves state-of-the-art results for both comprehension and generation on three referring expression datasets. Project and demo page: ",
        "submission_date": "2016-12-30T00:00:00",
        "last_modified_date": "2017-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.09592",
        "title": "When the map is better than the territory",
        "authors": [
            "Erik P Hoel"
        ],
        "abstract": "The causal structure of any system can be analyzed at a multitude of spatial and temporal scales. It has long been thought that while higher scale (macro) descriptions of causal structure may be useful to observers, they are at best a compressed description and at worse leave out critical information. However, recent research applying information theory to causal analysis has shown that the causal structure of some systems can actually come into focus (be more informative) at a macroscale (Hoel et al. 2013). That is, a macro model of a system (a map) can be more informative than a fully detailed model of the system (the territory). This has been called causal emergence. While causal emergence may at first glance seem counterintuitive, this paper grounds the phenomenon in a classic concept from information theory: Shannon's discovery of the channel capacity. I argue that systems have a particular causal capacity, and that different causal models of those systems take advantage of that capacity to various degrees. For some systems, only macroscale causal models use the full causal capacity. Such macroscale causal models can either be coarse-grains, or may leave variables and states out of the model (exogenous) in various ways, which can improve the model's efficacy and its informativeness via the same mathematical principles of how error-correcting codes take advantage of an information channel's capacity. As model choice increase, the causal capacity of a system approaches the channel capacity. Ultimately, this provides a general framework for understanding how the causal structure of some systems cannot be fully captured by even the most detailed microscopic model.\n    ",
        "submission_date": "2016-12-30T00:00:00",
        "last_modified_date": "2016-12-30T00:00:00"
    }
]