[
    {
        "url": "https://arxiv.org/abs/0801.0232",
        "title": "Does intelligence imply contradiction?",
        "authors": [
            "Patrizio Frosini"
        ],
        "abstract": "  Contradiction is often seen as a defect of intelligent systems and a dangerous limitation on efficiency. In this paper we raise the question of whether, on the contrary, it could be considered a key tool in increasing intelligence in biological structures. A possible way of answering this question in a mathematical context is shown, formulating a proposition that suggests a link between intelligence and contradiction.\n",
        "submission_date": "2007-12-31T00:00:00",
        "last_modified_date": "2008-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0801.1275",
        "title": "Le terme et le concept : fondements d'une ontoterminologie",
        "authors": [
            "Christophe Roche"
        ],
        "abstract": "  Most definitions of ontology, viewed as a \"specification of a conceptualization\", agree on the fact that if an ontology can take different forms, it necessarily includes a vocabulary of terms and some specification of their meaning in relation to the domain's conceptualization. And as domain knowledge is mainly conveyed through scientific and technical texts, we can hope to extract some useful information from them for building ontology. But is it as simple as this? In this article we shall see that the lexical structure, i.e. the network of words linked by linguistic relationships, does not necessarily match the domain conceptualization. We have to bear in mind that writing documents is the concern of textual linguistics, of which one of the principles is the incompleteness of text, whereas building ontology - viewed as task-independent knowledge - is concerned with conceptualization based on formal and not natural languages. Nevertheless, the famous Sapir and Whorf hypothesis, concerning the interdependence of thought and language, is also applicable to formal languages. This means that the way an ontology is built and a concept is defined depends directly on the formal language which is used; and the results will not be the same. The introduction of the notion of ontoterminology allows to take into account epistemological principles for formal ontology building.\n    ",
        "submission_date": "2008-01-08T00:00:00",
        "last_modified_date": "2008-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0801.1336",
        "title": "Stream Computing",
        "authors": [
            "Subhash Kak"
        ],
        "abstract": "  Stream computing is the use of multiple autonomic and parallel modules together with integrative processors at a higher level of abstraction to embody \"intelligent\" processing. The biological basis of this computing is sketched and the matter of learning is examined.\n    ",
        "submission_date": "2008-01-09T00:00:00",
        "last_modified_date": "2008-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0801.2069",
        "title": "Factored Value Iteration Converges",
        "authors": [
            "Istvan Szita",
            "Andras Lorincz"
        ],
        "abstract": "  In this paper we propose a novel algorithm, factored value iteration (FVI), for the approximate solution of factored Markov decision processes (fMDPs). The traditional approximate value iteration algorithm is modified in two ways. For one, the least-squares projection operator is modified so that it does not increase max-norm, and thus preserves convergence. The other modification is that we uniformly sample polynomially many samples from the (exponentially large) state space. This way, the complexity of our algorithm becomes polynomial in the size of the fMDP description length. We prove that the algorithm is convergent. We also derive an upper bound on the difference between our approximate solution and the optimal one, and also on the error introduced by sampling. We analyze various projection operators with respect to their computation complexity and their convergence when combined with approximate value iteration.\n    ",
        "submission_date": "2008-01-14T00:00:00",
        "last_modified_date": "2008-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0802.2127",
        "title": "New Implementation Framework for Saturation-Based Reasoning",
        "authors": [
            "Alexandre Riazanov"
        ],
        "abstract": "  The saturation-based reasoning methods are among the most theoretically developed ones and are used by most of the state-of-the-art first-order logic reasoners. In the last decade there was a sharp increase in performance of such systems, which I attribute to the use of advanced calculi and the intensified research in implementation techniques. However, nowadays we are witnessing a slowdown in performance progress, which may be considered as a sign that the saturation-based technology is reaching its inherent limits. The position I am trying to put forward in this paper is that such scepticism is premature and a sharp improvement in performance may potentially be reached by adopting new architectural principles for saturation. The top-level algorithms and corresponding designs used in the state-of-the-art saturation-based theorem provers have (at least) two inherent drawbacks: the insufficient flexibility of the used inference selection mechanisms and the lack of means for intelligent prioritising of search directions. In this position paper I analyse these drawbacks and present two ideas on how they could be overcome. In particular, I propose a flexible low-cost high-precision mechanism for inference selection, intended to overcome problems associated with the currently used instances of clause selection-based procedures. I also outline a method for intelligent prioritising of search directions, based on probing the search space by exploring generalised search directions. I discuss some technical issues related to implementation of the proposed architectural principles and outline possible solutions.\n    ",
        "submission_date": "2008-02-15T00:00:00",
        "last_modified_date": "2008-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0802.2429",
        "title": "Anisotropic selection in cellular genetic algorithms",
        "authors": [
            "David Simoncini",
            "S\u00e9bastien Verel",
            "Philippe Collard",
            "Manuel Clergue"
        ],
        "abstract": "  In this paper we introduce a new selection scheme in cellular genetic algorithms (cGAs). Anisotropic Selection (AS) promotes diversity and allows accurate control of the selective pressure. First we compare this new scheme with the classical rectangular grid shapes solution according to the selective pressure: we can obtain the same takeover time with the two techniques although the spreading of the best individual is different. We then give experimental results that show to what extent AS promotes the emergence of niches that support low coupling and high cohesion. Finally, using a cGA with anisotropic selection on a Quadratic Assignment Problem we show the existence of an anisotropic optimal value for which the best average performance is observed. Further work will focus on the selective pressure self-adjustment ability provided by this new selection scheme.\n    ",
        "submission_date": "2008-02-18T00:00:00",
        "last_modified_date": "2008-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0802.3137",
        "title": "Design and Implementation of Aggregate Functions in the DLV System",
        "authors": [
            "Wolfgang Faber",
            "Gerald Pfeifer",
            "Nicola Leone",
            "Tina Dell'Armi",
            "Giuseppe Ielpa"
        ],
        "abstract": "  Disjunctive Logic Programming (DLP) is a very expressive formalism: it allows for expressing every property of finite structures that is decidable in the complexity class SigmaP2 (= NP^NP). Despite this high expressiveness, there are some simple properties, often arising in real-world applications, which cannot be encoded in a simple and natural manner. Especially properties that require the use of arithmetic operators (like sum, times, or count) on a set or multiset of elements, which satisfy some conditions, cannot be naturally expressed in classic DLP.\n",
        "submission_date": "2008-02-21T00:00:00",
        "last_modified_date": "2008-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0802.3293",
        "title": "Use of Rapid Probabilistic Argumentation for Ranking on Large Complex Networks",
        "authors": [
            "Burak Cetin",
            "Haluk Bingol"
        ],
        "abstract": "  We introduce a family of novel ranking algorithms called ERank which run in linear/near linear time and build on explicitly modeling a network as uncertain evidence. The model uses Probabilistic Argumentation Systems (PAS) which are a combination of probability theory and propositional logic, and also a special case of Dempster-Shafer Theory of Evidence. ERank rapidly generates approximate results for the NP-complete problem involved enabling the use of the technique in large networks. We use a previously introduced PAS model for citation networks generalizing it for all networks. We propose a statistical test to be used for comparing the performances of different ranking algorithms based on a clustering validity test. Our experimentation using this test on a real-world network shows ERank to have the best performance in comparison to well-known algorithms including PageRank, closeness, and betweenness.\n    ",
        "submission_date": "2008-02-22T00:00:00",
        "last_modified_date": "2008-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.1087",
        "title": "The Future of Scientific Simulations: from Artificial Life to Artificial Cosmogenesis",
        "authors": [
            "Clement Vidal"
        ],
        "abstract": "  This philosophical paper explores the relation between modern scientific simulations and the future of the universe. We argue that a simulation of an entire universe will result from future scientific activity. This requires us to tackle the challenge of simulating open-ended evolution at all levels in a single simulation. The simulation should encompass not only biological evolution, but also physical evolution (a level below) and cultural evolution (a level above). The simulation would allow us to probe what would happen if we would \"replay the tape of the universe\" with the same or different laws and initial conditions. We also distinguish between real-world and artificial-world modelling. Assuming that intelligent life could indeed simulate an entire universe, this leads to two tentative hypotheses. Some authors have argued that we may already be in a simulation run by an intelligent entity. Or, if such a simulation could be made real, this would lead to the production of a new universe. This last direction is argued with a careful speculative philosophical approach, emphasizing the imperative to find a solution to the heat death problem in cosmology. The reader is invited to consult Annex 1 for an overview of the logical structure of this paper. -- Keywords: far future, future of science, ALife, simulation, realization, cosmology, heat death, fine-tuning, physical eschatology, cosmological natural selection, cosmological artificial selection, artificial cosmogenesis, selfish biocosm hypothesis, meduso-anthropic principle, developmental singularity hypothesis, role of intelligent life.\n    ",
        "submission_date": "2008-03-07T00:00:00",
        "last_modified_date": "2008-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.1207",
        "title": "Serious Flaws in Korf et al.'s Analysis on Time Complexity of A*",
        "authors": [
            "Hang Dinh"
        ],
        "abstract": "  This paper has been withdrawn.\n    ",
        "submission_date": "2008-03-08T00:00:00",
        "last_modified_date": "2010-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.1457",
        "title": "Hybrid Reasoning and the Future of Iconic Representations",
        "authors": [
            "Catherine Recanati"
        ],
        "abstract": "  We give a brief overview of the main characteristics of diagrammatic reasoning, analyze a case of human reasoning in a mastermind game, and explain why hybrid representation systems (HRS) are particularly attractive and promising for Artificial General Intelligence and Computer Science in general.\n    ",
        "submission_date": "2008-03-10T00:00:00",
        "last_modified_date": "2008-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.3192",
        "title": "Eye-Tracking Evolutionary Algorithm to minimize user's fatigue in IEC applied to Interactive One-Max problem",
        "authors": [
            "Denis Pallez",
            "Philippe Collard",
            "Thierry Baccino",
            "Laurent Dumercy"
        ],
        "abstract": "  In this paper, we describe a new algorithm that consists in combining an eye-tracker for minimizing the fatigue of a user during the evaluation process of Interactive Evolutionary Computation. The approach is then applied to the Interactive One-Max optimization problem.\n    ",
        "submission_date": "2008-03-21T00:00:00",
        "last_modified_date": "2008-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.3363",
        "title": "Node discovery in a networked organization",
        "authors": [
            "Yoshiharu Maeno"
        ],
        "abstract": "  In this paper, I present a method to solve a node discovery problem in a networked organization. Covert nodes refer to the nodes which are not observable directly. They affect social interactions, but do not appear in the surveillance logs which record the participants of the social interactions. Discovering the covert nodes is defined as identifying the suspicious logs where the covert nodes would appear if the covert nodes became overt. A mathematical model is developed for the maximal likelihood estimation of the network behind the social interactions and for the identification of the suspicious logs. Precision, recall, and F measure characteristics are demonstrated with the dataset generated from a real organization and the computationally synthesized datasets. The performance is close to the theoretical limit for any covert nodes in the networks of any topologies and sizes if the ratio of the number of observation to the number of possible communication patterns is large.\n    ",
        "submission_date": "2008-03-24T00:00:00",
        "last_modified_date": "2009-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.3501",
        "title": "Multiagent Approach for the Representation of Information in a Decision Support System",
        "authors": [
            "Fahem Kebair",
            "Fr\u00e9d\u00e9ric Serin"
        ],
        "abstract": "  In an emergency situation, the actors need an assistance allowing them to react swiftly and efficiently. In this prospect, we present in this paper a decision support system that aims to prepare actors in a crisis situation thanks to a decision-making support. The global architecture of this system is presented in the first part. Then we focus on a part of this system which is designed to represent the information of the current situation. This part is composed of a multiagent system that is made of factual agents. Each agent carries a semantic feature and aims to represent a partial part of a situation. The agents develop thanks to their interactions by comparing their semantic features using proximity measures and according to specific ontologies.\n    ",
        "submission_date": "2008-03-25T00:00:00",
        "last_modified_date": "2008-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.3812",
        "title": "Preferred extensions as stable models",
        "authors": [
            "Juan Carlos Nieves",
            "Mauricio Osorio",
            "Ulises Cort\u00e9s"
        ],
        "abstract": "  Given an argumentation framework AF, we introduce a mapping function that constructs a disjunctive logic program P, such that the preferred extensions of AF correspond to the stable models of P, after intersecting each stable model with the relevant atoms. The given mapping function is of polynomial size w.r.t. AF. In particular, we identify that there is a direct relationship between the minimal models of a propositional formula and the preferred extensions of an argumentation framework by working on representing the defeated arguments. Then we show how to infer the preferred extensions of an argumentation framework by using UNSAT algorithms and disjunctive stable model solvers. The relevance of this result is that we define a direct relationship between one of the most satisfactory argumentation semantics and one of the most successful approach of non-monotonic reasoning i.e., logic programming with the stable model semantics.\n    ",
        "submission_date": "2008-03-26T00:00:00",
        "last_modified_date": "2008-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.4074",
        "title": "Reflective visualization and verbalization of unconscious preference",
        "authors": [
            "Yoshiharu Maeno",
            "Yukio Ohsawa"
        ],
        "abstract": "  A new method is presented, that can help a person become aware of his or her unconscious preferences, and convey them to others in the form of verbal explanation. The method combines the concepts of reflection, visualization, and verbalization. The method was tested in an experiment where the unconscious preferences of the subjects for various artworks were investigated. In the experiment, two lessons were learned. The first is that it helps the subjects become aware of their unconscious preferences to verbalize weak preferences as compared with strong preferences through discussion over preference diagrams. The second is that it is effective to introduce an adjustable factor into visualization to adapt to the differences in the subjects and to foster their mutual understanding.\n    ",
        "submission_date": "2008-03-28T00:00:00",
        "last_modified_date": "2009-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.4253",
        "title": "Combinatorial Explorations in Su-Doku",
        "authors": [
            "Jean-Marie Chauvet"
        ],
        "abstract": "  Su-Doku, a popular combinatorial puzzle, provides an excellent testbench for heuristic explorations. Several interesting questions arise from its deceptively simple set of rules. How many distinct Su-Doku grids are there? How to find a solution to a Su-Doku puzzle? Is there a unique solution to a given Su-Doku puzzle? What is a good estimation of a puzzle's difficulty? What is the minimum puzzle size (the number of \"givens\")?\n",
        "submission_date": "2008-03-29T00:00:00",
        "last_modified_date": "2008-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.4355",
        "title": "Grammar-Based Random Walkers in Semantic Networks",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "abstract": "  Semantic networks qualify the meaning of an edge relating any two vertices. Determining which vertices are most \"central\" in a semantic network is difficult because one relationship type may be deemed subjectively more important than another. For this reason, research into semantic network metrics has focused primarily on context-based rankings (i.e. user prescribed contexts). Moreover, many of the current semantic network metrics rank semantic associations (i.e. directed paths between two vertices) and not the vertices themselves. This article presents a framework for calculating semantically meaningful primary eigenvector-based metrics such as eigenvector centrality and PageRank in semantic networks using a modified version of the random walker model of Markov chain analysis. Random walkers, in the context of this article, are constrained by a grammar, where the grammar is a user defined data structure that determines the meaning of the final vertex ranking. The ideas in this article are presented within the context of the Resource Description Framework (RDF) of the Semantic Web initiative.\n    ",
        "submission_date": "2008-03-31T00:00:00",
        "last_modified_date": "2008-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.0528",
        "title": "Application of Rough Set Theory to Analysis of Hydrocyclone Operation",
        "authors": [
            "H.Owladeghaffari",
            "M.Ejtemaei",
            "M.Irannajad"
        ],
        "abstract": "  This paper describes application of rough set theory, on the analysis of hydrocyclone operation. In this manner, using Self Organizing Map (SOM) as preprocessing step, best crisp granules of data are obtained. Then, using a combining of SOM and rough set theory (RST)-called SORST-, the dominant rules on the information table, obtained from laboratory tests, are extracted. Based on these rules, an approximate estimation on decision attribute is fulfilled. Finally, a brief comparison of this method with the SOM-NFIS system (briefly SONFIS) is highlighted.\n    ",
        "submission_date": "2008-04-03T00:00:00",
        "last_modified_date": "2008-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.0558",
        "title": "Agent-Based Perception of an Environment in an Emergency Situation",
        "authors": [
            "Fahem Kebair",
            "Fr\u00e9d\u00e9ric Serin",
            "Cyrille Bertelle"
        ],
        "abstract": "  We are interested in the problem of multiagent systems development for risk detecting and emergency response in an uncertain and partially perceived environment. The evaluation of the current situation passes by three stages inside the multiagent system. In a first time, the situation is represented in a dynamic way. The second step, consists to characterise the situation and finally, it is compared with other similar known situations. In this paper, we present an information modelling of an observed environment, that we have applied on the RoboCupRescue Simulation System. Information coming from the environment are formatted according to a taxonomy and using semantic features. The latter are defined thanks to a fine ontology of the domain and are managed by factual agents that aim to represent dynamically the current situation.\n    ",
        "submission_date": "2008-04-03T00:00:00",
        "last_modified_date": "2008-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.0599",
        "title": "Symmetry Breaking for Maximum Satisfiability",
        "authors": [
            "Joao Marques-Silva",
            "Ines Lynce",
            "Vasco Manquinho"
        ],
        "abstract": "  Symmetries are intrinsic to many combinatorial problems including Boolean Satisfiability (SAT) and Constraint Programming (CP). In SAT, the identification of symmetry breaking predicates (SBPs) is a well-known, often effective, technique for solving hard problems. The identification of SBPs in SAT has been the subject of significant improvements in recent years, resulting in more compact SBPs and more effective algorithms. The identification of SBPs has also been applied to pseudo-Boolean (PB) constraints, showing that symmetry breaking can also be an effective technique for PB constraints. This paper extends further the application of SBPs, and shows that SBPs can be identified and used in Maximum Satisfiability (MaxSAT), as well as in its most well-known variants, including partial MaxSAT, weighted MaxSAT and weighted partial MaxSAT. As with SAT and PB, symmetry breaking predicates for MaxSAT and variants are shown to be effective for a representative number of problem domains, allowing solving problem instances that current state of the art MaxSAT solvers could not otherwise solve.\n    ",
        "submission_date": "2008-04-03T00:00:00",
        "last_modified_date": "2008-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.0852",
        "title": "On the Influence of Selection Operators on Performances in Cellular Genetic Algorithms",
        "authors": [
            "David Simoncini",
            "Philippe Collard",
            "S\u00e9bastien Verel",
            "Manuel Clergue"
        ],
        "abstract": "  In this paper, we study the influence of the selective pressure on the performance of cellular genetic algorithms. Cellular genetic algorithms are genetic algorithms where the population is embedded on a toroidal grid. This structure makes the propagation of the best so far individual slow down, and allows to keep in the population potentially good solutions. We present two selective pressure reducing strategies in order to slow down even more the best solution propagation. We experiment these strategies on a hard optimization problem, the quadratic assignment problem, and we show that there is a value for of the control parameter for both which gives the best performance. This optimal value does not find explanation on only the selective pressure, measured either by take over time and diversity evolution. This study makes us conclude that we need other tools than the sole selective pressure measures to explain the performances of cellular genetic algorithms.\n    ",
        "submission_date": "2008-04-05T00:00:00",
        "last_modified_date": "2008-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.1244",
        "title": "Geometric Data Analysis, From Correspondence Analysis to Structured Data Analysis (book review)",
        "authors": [
            "Fionn Murtagh"
        ],
        "abstract": "  Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004, xi+475 pp.\n    ",
        "submission_date": "2008-04-08T00:00:00",
        "last_modified_date": "2008-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.2401",
        "title": "Causal models have no complete axiomatic characterization",
        "authors": [
            "Sanjiang Li"
        ],
        "abstract": "  Markov networks and Bayesian networks are effective graphic representations of the dependencies embedded in probabilistic models. It is well known that independencies captured by Markov networks (called graph-isomorphs) have a finite axiomatic characterization. This paper, however, shows that independencies captured by Bayesian networks (called causal models) have no axiomatization by using even countably many Horn or disjunctive clauses. This is because a sub-independency model of a causal model may be not causal, while graph-isomorphs are closed under sub-models.\n    ",
        "submission_date": "2008-04-15T00:00:00",
        "last_modified_date": "2008-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.3361",
        "title": "A New Approach to Automated Epileptic Diagnosis Using EEG and Probabilistic Neural Network",
        "authors": [
            "Forrest Sheng Bao",
            "Donald Yu-Chun Lie",
            "Yuanlin Zhang"
        ],
        "abstract": "  Epilepsy is one of the most common neurological disorders that greatly impair patient' daily lives. Traditional epileptic diagnosis relies on tedious visual screening by neurologists from lengthy EEG recording that requires the presence of seizure (ictal) activities. Nowadays, there are many systems helping the neurologists to quickly find interesting segments of the lengthy signal by automatic seizure detection. However, we notice that it is very difficult, if not impossible, to obtain long-term EEG data with seizure activities for epilepsy patients in areas lack of medical resources and trained neurologists. Therefore, we propose to study automated epileptic diagnosis using interictal EEG data that is much easier to collect than ictal data. The authors are not aware of any report on automated EEG diagnostic system that can accurately distinguish patients' interictal EEG from the EEG of normal people. The research presented in this paper, therefore, aims to develop an automated diagnostic system that can use interictal EEG data to diagnose whether the person is epileptic. Such a system should also detect seizure activities for further investigation by doctors and potential patient monitoring. To develop such a system, we extract four classes of features from the EEG data and build a Probabilistic Neural Network (PNN) fed with these features. Leave-one-out cross-validation (LOO-CV) on a widely used epileptic-normal data set reflects an impressive 99.5% accuracy of our system on distinguishing normal people's EEG from patient's interictal EEG. We also find our system can be used in patient monitoring (seizure detection) and seizure focus localization, with 96.7% and 77.5% accuracy respectively on the data set.\n    ",
        "submission_date": "2008-04-21T00:00:00",
        "last_modified_date": "2008-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.0202",
        "title": "A Pseudo-Boolean Solution to the Maximum Quartet Consistency Problem",
        "authors": [
            "Antonio Morgado",
            "Joao Marques-Silva"
        ],
        "abstract": "  Determining the evolutionary history of a given biological data is an important task in biological sciences. Given a set of quartet topologies over a set of taxa, the Maximum Quartet Consistency (MQC) problem consists of computing a global phylogeny that satisfies the maximum number of quartets. A number of solutions have been proposed for the MQC problem, including Dynamic Programming, Constraint Programming, and more recently Answer Set Programming (ASP). ASP is currently the most efficient approach for optimally solving the MQC problem. This paper proposes encoding the MQC problem with pseudo-Boolean (PB) constraints. The use of PB allows solving the MQC problem with efficient PB solvers, and also allows considering different modeling approaches for the MQC problem. Initial results are promising, and suggest that PB can be an effective alternative for solving the MQC problem.\n    ",
        "submission_date": "2008-05-02T00:00:00",
        "last_modified_date": "2008-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.0459",
        "title": "Phase transition in SONFIS&SORST",
        "authors": [
            "Hamed Owladeghaffari"
        ],
        "abstract": "  In this study, we introduce general frame of MAny Connected Intelligent Particles Systems (MACIPS). Connections and interconnections between particles get a complex behavior of such merely simple system (system in system).Contribution of natural computing, under information granulation theory, are the main topics of this spacious skeleton. Upon this clue, we organize two algorithms involved a few prominent intelligent computing and approximate reasoning methods: self organizing feature map (SOM), Neuro- Fuzzy Inference System and Rough Set Theory (RST). Over this, we show how our algorithms can be taken as a linkage of government-society interaction, where government catches various fashions of behavior: solid (absolute) or flexible. So, transition of such society, by changing of connectivity parameters (noise) from order to disorder is inferred. Add to this, one may find an indirect mapping among financial systems and eventual market fluctuations with MACIPS. Keywords: phase transition, SONFIS, SORST, many connected intelligent particles system, society-government interaction\n    ",
        "submission_date": "2008-05-05T00:00:00",
        "last_modified_date": "2008-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.0642",
        "title": "Order to Disorder Transitions in Hybrid Intelligent Systems: a Hatch to the Interactions of Nations -Governments",
        "authors": [
            "Hamed Owladeghaffari"
        ],
        "abstract": "  In this study, under general frame of MAny Connected Intelligent Particles Systems (MACIPS), we reproduce two new simple subsets of such intelligent complex network, namely hybrid intelligent systems, involved a few prominent intelligent computing and approximate reasoning methods: self organizing feature map (SOM), Neuro-Fuzzy Inference System and Rough Set Theory (RST). Over this, we show how our algorithms can be construed as a linkage of government-society interaction, where government catches various fashions of behavior: solid (absolute) or flexible. So, transition of such society, by changing of connectivity parameters (noise) from order to disorder is inferred. Add to this, one may find an indirect mapping among financial systems and eventual market fluctuations with MACIPS.\n    ",
        "submission_date": "2008-05-06T00:00:00",
        "last_modified_date": "2008-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.0785",
        "title": "AGNOSCO - Identification of Infected Nodes with artificial Ant Colonies",
        "authors": [
            "Michael Hilker",
            "Christoph Schommer"
        ],
        "abstract": "  If a computer node is infected by a virus, worm or a backdoor, then this is a security risk for the complete network structure where the node is associated. Existing Network Intrusion Detection Systems (NIDS) provide a certain amount of support for the identification of such infected nodes but suffer from the need of plenty of communication and computational power. In this article, we present a novel approach called AGNOSCO to support the identification of infected nodes through the usage of artificial ant colonies. It is shown that AGNOSCO overcomes the communication and computational power problem while identifying infected nodes properly.\n    ",
        "submission_date": "2008-05-06T00:00:00",
        "last_modified_date": "2008-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.1096",
        "title": "Adaptive Affinity Propagation Clustering",
        "authors": [
            "Kaijun Wang",
            "Junying Zhang",
            "Dan Li",
            "Xinna Zhang",
            "Tao Guo"
        ],
        "abstract": "  Affinity propagation clustering (AP) has two limitations: it is hard to know what value of parameter 'preference' can yield an optimal clustering solution, and oscillations cannot be eliminated automatically if occur. The adaptive AP method is proposed to overcome these limitations, including adaptive scanning of preferences to search space of the number of clusters for finding the optimal clustering solution, adaptive adjustment of damping factors to eliminate oscillations, and adaptive escaping from oscillations when the damping adjustment technique fails. Experimental results on simulated and real data sets show that the adaptive AP is effective and can outperform AP in quality of clustering results.\n    ",
        "submission_date": "2008-05-08T00:00:00",
        "last_modified_date": "2008-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.1288",
        "title": "Assessment of effective parameters on dilution using approximate reasoning methods in longwall mining method, Iran coal mines",
        "authors": [
            "H. Owladeghaffari",
            "K. Shahriar",
            "G. H. R. Saeedi"
        ],
        "abstract": "  Approximately more than 90% of all coal production in Iranian underground mines is derived directly longwall mining method. Out of seam dilution is one of the essential problems in these mines. Therefore the dilution can impose the additional cost of mining and milling. As a result, recognition of the effective parameters on the dilution has a remarkable role in industry. In this way, this paper has analyzed the influence of 13 parameters (attributed variables) versus the decision attribute (dilution value), so that using two approximate reasoning methods, namely Rough Set Theory (RST) and Self Organizing Neuro- Fuzzy Inference System (SONFIS) the best rules on our collected data sets has been extracted. The other benefit of later methods is to predict new unknown cases. So, the reduced sets (reducts) by RST have been obtained. Therefore the emerged results by utilizing mentioned methods shows that the high sensitive variables are thickness of layer, length of stope, rate of advance, number of miners, type of advancing.\n    ",
        "submission_date": "2008-05-09T00:00:00",
        "last_modified_date": "2008-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.1473",
        "title": "A Fast Algorithm and Datalog Inexpressibility for Temporal Reasoning",
        "authors": [
            "Manuel Bodirsky",
            "Jan Kara"
        ],
        "abstract": "  We introduce a new tractable temporal constraint language, which strictly contains the Ord-Horn language of Buerkert and Nebel and the class of AND/OR precedence constraints. The algorithm we present for this language decides whether a given set of constraints is consistent in time that is quadratic in the input size. We also prove that (unlike Ord-Horn) this language cannot be solved by Datalog or by establishing local consistency.\n    ",
        "submission_date": "2008-05-10T00:00:00",
        "last_modified_date": "2009-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.1727",
        "title": "Swarm-Based Spatial Sorting",
        "authors": [
            "Martyn Amos",
            "Oliver Don"
        ],
        "abstract": "  Purpose: To present an algorithm for spatially sorting objects into an annular structure. Design/Methodology/Approach: A swarm-based model that requires only stochastic agent behaviour coupled with a pheromone-inspired \"attraction-repulsion\" mechanism. Findings: The algorithm consistently generates high-quality annular structures, and is particularly powerful in situations where the initial configuration of objects is similar to those observed in nature. Research limitations/implications: Experimental evidence supports previous theoretical arguments about the nature and mechanism of spatial sorting by insects. Practical implications: The algorithm may find applications in distributed robotics. Originality/value: The model offers a powerful minimal algorithmic framework, and also sheds further light on the nature of attraction-repulsion algorithms and underlying natural processes.\n    ",
        "submission_date": "2008-05-12T00:00:00",
        "last_modified_date": "2008-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.2308",
        "title": "Toward Fuzzy block theory",
        "authors": [
            "H.Owladeghaffari"
        ],
        "abstract": "  This study, fundamentals of fuzzy block theory, and its application in assessment of stability in underground openings, has surveyed. Using fuzzy topics and inserting them in to key block theory, in two ways, fundamentals of fuzzy block theory has been presented. In indirect combining, by coupling of adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could extract possible damage parts around a tunnel. In direct solution, some principles of block theory, by means of different fuzzy facets theory, were rewritten.\n    ",
        "submission_date": "2008-05-15T00:00:00",
        "last_modified_date": "2008-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.2440",
        "title": "Analysis of hydrocyclone performance based on information granulation theory",
        "authors": [
            "Hamed Owladeghaffari",
            "Majid Ejtemaei",
            "Mehdi Irannajad"
        ],
        "abstract": "  This paper describes application of information granulation theory, on the analysis of hydrocyclone perforamance. In this manner, using a combining of Self Organizing Map (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and fuzzy granules are obtained(briefly called SONFIS). Balancing of crisp granules and sub fuzzy granules, within non fuzzy information (initial granulation), is rendered in an open-close iteration. Using two criteria, \"simplicity of rules \"and \"adaptive threoshold error level\", stability of algorithm is guaranteed. Validation of the proposed method, on the data set of the hydrocyclone is rendered.\n    ",
        "submission_date": "2008-05-16T00:00:00",
        "last_modified_date": "2008-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.3126",
        "title": "Cognitive Architecture for Direction of Attention Founded on Subliminal Memory Searches, Pseudorandom and Nonstop",
        "authors": [
            "J. R. Burger"
        ],
        "abstract": "  By way of explaining how a brain works logically, human associative memory is modeled with logical and memory neurons, corresponding to standard digital circuits. The resulting cognitive architecture incorporates basic psychological elements such as short term and long term memory. Novel to the architecture are memory searches using cues chosen pseudorandomly from short term memory. Recalls alternated with sensory images, many tens per second, are analyzed subliminally as an ongoing process, to determine a direction of attention in short term memory.\n    ",
        "submission_date": "2008-05-20T00:00:00",
        "last_modified_date": "2008-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.3267",
        "title": "Compressing Binary Decision Diagrams",
        "authors": [
            "Esben Rune Hansen",
            "S. Srinivasa Rao",
            "Peter Tiedemann"
        ],
        "abstract": "  The paper introduces a new technique for compressing Binary Decision Diagrams in those cases where random access is not required. Using this technique, compression and decompression can be done in linear time in the size of the BDD and compression will in many cases reduce the size of the BDD to 1-2 bits per node. Empirical results for our compression technique are presented, including comparisons with previously introduced techniques, showing that the new technique dominate on all tested instances.\n    ",
        "submission_date": "2008-05-21T00:00:00",
        "last_modified_date": "2008-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.3518",
        "title": "Logic programming with social features",
        "authors": [
            "Francesco Buccafurri",
            "Gianluca Caminiti"
        ],
        "abstract": "  In everyday life it happens that a person has to reason about what other people think and how they behave, in order to achieve his goals. In other words, an individual may be required to adapt his behaviour by reasoning about the others' mental state. In this paper we focus on a knowledge representation language derived from logic programming which both supports the representation of mental states of individual communities and provides each with the capability of reasoning about others' mental states and acting accordingly. The proposed semantics is shown to be translatable into stable model semantics of logic programs with aggregates.\n    ",
        "submission_date": "2008-05-22T00:00:00",
        "last_modified_date": "2008-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.3747",
        "title": "Constructing Folksonomies from User-specified Relations on Flickr",
        "authors": [
            "Anon Plangprasopchok",
            "Kristina Lerman"
        ],
        "abstract": "  Many social Web sites allow users to publish content and annotate with descriptive metadata. In addition to flat tags, some social Web sites have recently began to allow users to organize their content and metadata hierarchically. The social photosharing site Flickr, for example, allows users to group related photos in sets, and related sets in collections. The social bookmarking site ",
        "submission_date": "2008-05-24T00:00:00",
        "last_modified_date": "2008-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.3799",
        "title": "The Structure of Narrative: the Case of Film Scripts",
        "authors": [
            "Fionn Murtagh",
            "Adam Ganz",
            "Stewart McKie"
        ],
        "abstract": "  We analyze the style and structure of story narrative using the case of film scripts. The practical importance of this is noted, especially the need to have support tools for television movie writing. We use the Casablanca film script, and scripts from six episodes of CSI (Crime Scene Investigation). For analysis of style and structure, we quantify various central perspectives discussed in McKee's book, \"Story: Substance, Structure, Style, and the Principles of Screenwriting\". Film scripts offer a useful point of departure for exploration of the analysis of more general narratives. Our methodology, using Correspondence Analysis, and hierarchical clustering, is innovative in a range of areas that we discuss. In particular this work is groundbreaking in taking the qualitative analysis of McKee and grounding this analysis in a quantitative and algorithmic framework.\n    ",
        "submission_date": "2008-05-24T00:00:00",
        "last_modified_date": "2008-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.3800",
        "title": "An Evolutionary-Based Approach to Learning Multiple Decision Models from Underrepresented Data",
        "authors": [
            "Vitaly Schetinin",
            "Dayou Li",
            "Carsten Maple"
        ],
        "abstract": "  The use of multiple Decision Models (DMs) enables to enhance the accuracy in decisions and at the same time allows users to evaluate the confidence in decision making. In this paper we explore the ability of multiple DMs to learn from a small amount of verified data. This becomes important when data samples are difficult to collect and verify. We propose an evolutionary-based approach to solving this problem. The proposed technique is examined on a few clinical problems presented by a small amount of data.\n    ",
        "submission_date": "2008-05-24T00:00:00",
        "last_modified_date": "2008-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.3802",
        "title": "Feature Selection for Bayesian Evaluation of Trauma Death Risk",
        "authors": [
            "L. Jakaite",
            "V. Schetinin"
        ],
        "abstract": "  In the last year more than 70,000 people have been brought to the UK hospitals with serious injuries. Each time a clinician has to urgently take a patient through a screening procedure to make a reliable decision on the trauma treatment. Typically, such procedure comprises around 20 tests; however the condition of a trauma patient remains very difficult to be tested properly. What happens if these tests are ambiguously interpreted, and information about the severity of the injury will come misleading? The mistake in a decision can be fatal: using a mild treatment can put a patient at risk of dying from posttraumatic shock, while using an overtreatment can also cause death. How can we reduce the risk of the death caused by unreliable decisions? It has been shown that probabilistic reasoning, based on the Bayesian methodology of averaging over decision models, allows clinicians to evaluate the uncertainty in decision making. Based on this methodology, in this paper we aim at selecting the most important screening tests, keeping a high performance. We assume that the probabilistic reasoning within the Bayesian methodology allows us to discover new relationships between the screening tests and uncertainty in decisions. In practice, selection of the most informative tests can also reduce the cost of a screening procedure in trauma care centers. In our experiments we use the UK Trauma data to compare the efficiency of the proposed technique in terms of the performance. We also compare the uncertainty in decisions in terms of entropy.\n    ",
        "submission_date": "2008-05-25T00:00:00",
        "last_modified_date": "2008-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.3935",
        "title": "Fusion for Evaluation of Image Classification in Uncertain Environments",
        "authors": [
            "Arnaud Martin"
        ],
        "abstract": "  We present in this article a new evaluation method for classification and segmentation of textured images in uncertain environments. In uncertain environments, real classes and boundaries are known with only a partial certainty given by the experts. Most of the time, in many presented papers, only classification or only segmentation are considered and evaluated. Here, we propose to take into account both the classification and segmentation results according to the certainty given by the experts. We present the results of this method on a fusion of classifiers of sonar images for a seabed characterization.\n    ",
        "submission_date": "2008-05-26T00:00:00",
        "last_modified_date": "2008-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.3939",
        "title": "Decision Support with Belief Functions Theory for Seabed Characterization",
        "authors": [
            "Arnaud Martin",
            "Isabelle Quidu"
        ],
        "abstract": "  The seabed characterization from sonar images is a very hard task because of the produced data and the unknown environment, even for an human expert. In this work we propose an original approach in order to combine binary classifiers arising from different kinds of strategies such as one-versus-one or one-versus-rest, usually used in the SVM-classification. The decision functions coming from these binary classifiers are interpreted in terms of belief functions in order to combine these functions with one of the numerous operators of the belief functions theory. Moreover, this interpretation of the decision function allows us to propose a process of decisions by taking into account the rejected observations too far removed from the learning data, and the imprecise decisions given in unions of classes. This new approach is illustrated and evaluated with a SVM in order to classify the different kinds of sediment on image sonar.\n    ",
        "submission_date": "2008-05-26T00:00:00",
        "last_modified_date": "2008-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.3972",
        "title": "Intuitive visualization of the intelligence for the run-down of terrorist wire-pullers",
        "authors": [
            "Yoshiharu Maeno",
            "Yukio Ohsawa"
        ],
        "abstract": "  The investigation of the terrorist attack is a time-critical task. The investigators have a limited time window to diagnose the organizational background of the terrorists, to run down and arrest the wire-pullers, and to take an action to prevent or eradicate the terrorist attack. The intuitive interface to visualize the intelligence data set stimulates the investigators' experience and knowledge, and aids them in decision-making for an immediately effective action. This paper presents a computational method to analyze the intelligence data set on the collective actions of the perpetrators of the attack, and to visualize it into the form of a social network diagram which predicts the positions where the wire-pullers conceals themselves.\n    ",
        "submission_date": "2008-05-26T00:00:00",
        "last_modified_date": "2008-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.4101",
        "title": "Goal-oriented Dialog as a Collaborative Subordinated Activity involving Collective Acceptance",
        "authors": [
            "Sylvie Saget",
            "Marc Guyomard"
        ],
        "abstract": "  Modeling dialog as a collaborative activity consists notably in specifying the content of the Conversational Common Ground and the kind of social mental state involved. In previous work (Saget, 2006), we claim that Collective Acceptance is the proper social attitude for modeling Conversational Common Ground in the particular case of goal-oriented dialog. In this paper, a formalization of Collective Acceptance is shown, besides elements in order to integrate this attitude in a rational model of dialog are provided; and finally, a model of referential acts as being part of a collaborative activity is presented. The particular case of reference has been chosen in order to exemplify our claims.\n    ",
        "submission_date": "2008-05-27T00:00:00",
        "last_modified_date": "2008-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.4560",
        "title": "Rock mechanics modeling based on soft granulation theory",
        "authors": [
            "H.Owladeghaffari"
        ],
        "abstract": "  This paper describes application of information granulation theory, on the design of rock engineering flowcharts. Firstly, an overall flowchart, based on information granulation theory has been highlighted. Information granulation theory, in crisp (non-fuzzy) or fuzzy format, can take into account engineering experiences (especially in fuzzy shape-incomplete information or superfluous), or engineering judgments, in each step of designing procedure, while the suitable instruments modeling are employed. In this manner and to extension of soft modeling instruments, using three combinations of Self Organizing Map (SOM), Neuro-Fuzzy Inference System (NFIS), and Rough Set Theory (RST) crisp and fuzzy granules, from monitored data sets are obtained. The main underlined core of our algorithms are balancing of crisp(rough or non-fuzzy) granules and sub fuzzy granules, within non fuzzy information (initial granulation) upon the open-close iterations. Using different criteria on balancing best granules (information pockets), are obtained. Validations of our proposed methods, on the data set of in-situ permeability in rock masses in Shivashan dam, Iran have been highlighted.\n    ",
        "submission_date": "2008-05-29T00:00:00",
        "last_modified_date": "2008-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.0250",
        "title": "Checking the Quality of Clinical Guidelines using Automated Reasoning Tools",
        "authors": [
            "Arjen Hommersom",
            "Peter J.F. Lucas",
            "Patrick van Bommel"
        ],
        "abstract": "  Requirements about the quality of clinical guidelines can be represented by schemata borrowed from the theory of abductive diagnosis, using temporal logic to model the time-oriented aspects expressed in a guideline. Previously, we have shown that these requirements can be verified using interactive theorem proving techniques. In this paper, we investigate how this approach can be mapped to the facilities of a resolution-based theorem prover, Otter, and a complementary program that searches for finite models of first-order statements, Mace. It is shown that the reasoning required for checking the quality of a guideline can be mapped to such fully automated theorem-proving facilities. The medical quality of an actual guideline concerning diabetes mellitus 2 is investigated in this way.\n    ",
        "submission_date": "2008-06-02T00:00:00",
        "last_modified_date": "2008-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.0526",
        "title": "An Ontology-based Knowledge Management System for Industry Clusters",
        "authors": [
            "Pradorn Sureephong",
            "Nopasit Chakpitak",
            "Yacine Ouzrout",
            "Abdelaziz Bouras"
        ],
        "abstract": "  Knowledge-based economy forces companies in the nation to group together as a cluster in order to maintain their competitiveness in the world market. The cluster development relies on two key success factors which are knowledge sharing and collaboration between the actors in the cluster. Thus, our study tries to propose knowledge management system to support knowledge management activities within the cluster. To achieve the objectives of this study, ontology takes a very important role in knowledge management process in various ways; such as building reusable and faster knowledge-bases, better way for representing the knowledge explicitly. However, creating and representing ontology create difficulties to organization due to the ambiguity and unstructured of source of knowledge. Therefore, the objectives of this paper are to propose the methodology to create and represent ontology for the organization development by using knowledge engineering approach. The handicraft cluster in Thailand is used as a case study to illustrate our proposed methodology.\n    ",
        "submission_date": "2008-06-03T00:00:00",
        "last_modified_date": "2008-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.0784",
        "title": "Collaborative model of interaction and Unmanned Vehicle Systems' interface",
        "authors": [
            "Sylvie Saget",
            "Francois Legras",
            "Gilles Coppin"
        ],
        "abstract": "  The interface for the next generation of Unmanned Vehicle Systems should be an interface with multi-modal displays and input controls. Then, the role of the interface will not be restricted to be a support of the interactions between the ground operator and vehicles. Interface must take part in the interaction management too. In this paper, we show that recent works in pragmatics and philosophy provide a suitable theoretical framework for the next generation of UV System's interface. We concentrate on two main aspects of the collaborative model of interaction based on acceptance: multi-strategy approach for communicative act generation and interpretation and communicative alignment.\n    ",
        "submission_date": "2008-06-04T00:00:00",
        "last_modified_date": "2008-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.1280",
        "title": "The Role of Artificial Intelligence Technologies in Crisis Response",
        "authors": [
            "Khaled M. Khalil",
            "M. Abdel-Aziz",
            "Taymour T. Nazmy",
            "Abdel-Badeeh M. Salem"
        ],
        "abstract": "  Crisis response poses many of the most difficult information technology in crisis management. It requires information and communication-intensive efforts, utilized for reducing uncertainty, calculating and comparing costs and benefits, and managing resources in a fashion beyond those regularly available to handle routine problems. In this paper, we explore the benefits of artificial intelligence technologies in crisis response. This paper discusses the role of artificial intelligence technologies; namely, robotics, ontology and semantic web, and multi-agent systems in crisis response.\n    ",
        "submission_date": "2008-06-07T00:00:00",
        "last_modified_date": "2008-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.1316",
        "title": "The end of Sleeping Beauty's nightmare",
        "authors": [
            "Berry Groisman"
        ],
        "abstract": "  The way a rational agent changes her belief in certain propositions/hypotheses in the light of new evidence lies at the heart of Bayesian inference. The basic natural assumption, as summarized in van Fraassen's Reflection Principle ([1984]), would be that in the absence of new evidence the belief should not change. Yet, there are examples that are claimed to violate this assumption. The apparent paradox presented by such examples, if not settled, would demonstrate the inconsistency and/or incompleteness of the Bayesian approach and without eliminating this inconsistency, the approach cannot be regarded as scientific.\n",
        "submission_date": "2008-06-08T00:00:00",
        "last_modified_date": "2008-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.1640",
        "title": "Toward a combination rule to deal with partial conflict and specificity in belief functions theory",
        "authors": [
            "Arnaud Martin",
            "Christophe Osswald"
        ],
        "abstract": "  We present and discuss a mixed conjunctive and disjunctive rule, a generalization of conflict repartition rules, and a combination of these two rules. In the belief functions theory one of the major problem is the conflict repartition enlightened by the famous Zadeh's example. To date, many combination rules have been proposed in order to solve a solution to this problem. Moreover, it can be important to consider the specificity of the responses of the experts. Since few year some unification rules are proposed. We have shown in our previous works the interest of the proportional conflict redistribution rule. We propose here a mixed combination rule following the proportional conflict redistribution rule modified by a discounting procedure. This rule generalizes many combination rules.\n    ",
        "submission_date": "2008-06-10T00:00:00",
        "last_modified_date": "2008-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.1797",
        "title": "A new generalization of the proportional conflict redistribution rule stable in terms of decision",
        "authors": [
            "Arnaud Martin",
            "Christophe Osswald"
        ],
        "abstract": "  In this chapter, we present and discuss a new generalized proportional conflict redistribution rule. The Dezert-Smarandache extension of the Demster-Shafer theory has relaunched the studies on the combination rules especially for the management of the conflict. Many combination rules have been proposed in the last few years. We study here different combination rules and compare them in terms of decision on didactic example and on generated data. Indeed, in real applications, we need a reliable decision and it is the final results that matter. This chapter shows that a fine proportional conflict redistribution rule must be preferred for the combination in the belief function theory.\n    ",
        "submission_date": "2008-06-11T00:00:00",
        "last_modified_date": "2008-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.1802",
        "title": "Une nouvelle r\u00e8gle de combinaison r\u00e9partissant le conflit - Applications en imagerie Sonar et classification de cibles Radar",
        "authors": [
            "Arnaud Martin",
            "Christophe Osswald"
        ],
        "abstract": "  These last years, there were many studies on the problem of the conflict coming from information combination, especially in evidence theory. We can summarise the solutions for manage the conflict into three different approaches: first, we can try to suppress or reduce the conflict before the combination step, secondly, we can manage the conflict in order to give no influence of the conflict in the combination step, and then take into account the conflict in the decision step, thirdly, we can take into account the conflict in the combination step. The first approach is certainly the better, but not always feasible. It is difficult to say which approach is the best between the second and the third. However, the most important is the produced results in applications. We propose here a new combination rule that distributes the conflict proportionally on the element given this conflict. We compare these different combination rules on real data in Sonar imagery and Radar target classification.\n    ",
        "submission_date": "2008-06-11T00:00:00",
        "last_modified_date": "2008-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.1806",
        "title": "Perfect Derived Propagators",
        "authors": [
            "Christian Schulte",
            "Guido Tack"
        ],
        "abstract": "  When implementing a propagator for a constraint, one must decide about variants: When implementing min, should one also implement max? Should one implement linear equations both with and without coefficients? Constraint variants are ubiquitous: implementing them requires considerable (if not prohibitive) effort and decreases maintainability, but will deliver better performance.\n",
        "submission_date": "2008-06-11T00:00:00",
        "last_modified_date": "2008-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.2140",
        "title": "Defaults and Normality in Causal Structures",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "abstract": "  A serious defect with the Halpern-Pearl (HP) definition of causality is repaired by combining a theory of causality with a theory of defaults. In addition, it is shown that (despite a claim to the contrary) a cause according to the HP condition need not be a single conjunct. A definition of causality motivated by Wright's NESS test is shown to always hold for a single conjunct. Moreover, conditions that hold for all the examples considered by HP are given that guarantee that causality according to (this version) of the NESS test is equivalent to the HP definition.\n    ",
        "submission_date": "2008-06-12T00:00:00",
        "last_modified_date": "2008-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.2216",
        "title": "An Intelligent Multi-Agent Recommender System for Human Capacity Building",
        "authors": [
            "Vukosi N. Marivate",
            "George Ssali",
            "Tshilidzi Marwala"
        ],
        "abstract": "  This paper presents a Multi-Agent approach to the problem of recommending training courses to engineering professionals. The recommendation system is built as a proof of concept and limited to the electrical and mechanical engineering disciplines. Through user modelling and data collection from a survey, collaborative filtering recommendation is implemented using intelligent agents. The agents work together in recommending meaningful training courses and updating the course information. The system uses a users profile and keywords from courses to rank courses. A ranking accuracy for courses of 90% is achieved while flexibility is achieved using an agent that retrieves information autonomously using data mining techniques from websites. This manner of recommendation is scalable and adaptable. Further improvements can be made using clustering and recording user feedback.\n    ",
        "submission_date": "2008-06-13T00:00:00",
        "last_modified_date": "2008-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.2356",
        "title": "Development of Hybrid Intelligent Systems and their Applications from Engineering Systems to Complex Systems",
        "authors": [
            "Hamed Owladeghaffari"
        ],
        "abstract": "  In this study, we introduce general frame of MAny Connected Intelligent Particles Systems (MACIPS). Connections and interconnections between particles get a complex behavior of such merely simple system (system in system).Contribution of natural computing, under information granulation theory, are the main topic of this spacious skeleton. Upon this clue, we organize different algorithms involved a few prominent intelligent computing and approximate reasoning methods such as self organizing feature map (SOM)[9], Neuro- Fuzzy Inference System[10], Rough Set Theory (RST)[11], collaborative clustering, Genetic Algorithm and Ant Colony System. Upon this, we have employed our algorithms on the several engineering systems, especially emerged systems in Civil and Mineral processing. In other process, we investigated how our algorithms can be taken as a linkage of government-society interaction, where government catches various fashions of behavior: solid (absolute) or flexible. So, transition of such society, by changing of connectivity parameters (noise) from order to disorder is inferred. Add to this, one may find an indirect mapping among finical systems and eventual market fluctuations with MACIPS. In the following sections, we will mention the main topics of the suggested proposal, briefly Details of the proposed algorithms can be found in the references.\n    ",
        "submission_date": "2008-06-14T00:00:00",
        "last_modified_date": "2008-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.2925",
        "title": "Neural networks in 3D medical scan visualization",
        "authors": [
            "D\u017eenan Zuki\u0107",
            "Andreas Elsner",
            "Zikrija Avdagi\u0107",
            "Gitta Domik"
        ],
        "abstract": "  For medical volume visualization, one of the most important tasks is to reveal clinically relevant details from the 3D scan (CT, MRI ...), e.g. the coronary arteries, without obscuring them with less significant parts. These volume datasets contain different materials which are difficult to extract and visualize with 1D transfer functions based solely on the attenuation coefficient. Multi-dimensional transfer functions allow a much more precise classification of data which makes it easier to separate different surfaces from each other. Unfortunately, setting up multi-dimensional transfer functions can become a fairly complex task, generally accomplished by trial and error. This paper explains neural networks, and then presents an efficient way to speed up visualization process by semi-automatic transfer function generation. We describe how to use neural networks to detect distinctive features shown in the 2D histogram of the volume data and how to use this information for data classification.\n    ",
        "submission_date": "2008-06-18T00:00:00",
        "last_modified_date": "2009-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.4341",
        "title": "On Sequences with Non-Learnable Subsequences",
        "authors": [
            "Vladimir V. V'yugin"
        ],
        "abstract": "  The remarkable results of Foster and Vohra was a starting point for a series of papers which show that any sequence of outcomes can be learned (with no prior knowledge) using some universal randomized forecasting algorithm and forecast-dependent checking rules. We show that for the class of all computationally efficient outcome-forecast-based checking rules, this property is violated. Moreover, we present a probabilistic algorithm generating with probability close to one a sequence with a subsequence which simultaneously miscalibrates all partially weakly computable randomized forecasting algorithms. %subsequences non-learnable by each randomized algorithm.\n",
        "submission_date": "2008-06-26T00:00:00",
        "last_modified_date": "2008-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.4511",
        "title": "The model of quantum evolution",
        "authors": [
            "Konstantin P. Wishnevsky"
        ],
        "abstract": "This paper has been withdrawn by the author due to extremely unscientific errors.\n    ",
        "submission_date": "2008-06-27T00:00:00",
        "last_modified_date": "2013-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0807.0337",
        "title": "Unveiling the mystery of visual information processing in human brain",
        "authors": [
            "Emanuel Diamant"
        ],
        "abstract": "  It is generally accepted that human vision is an extremely powerful information processing system that facilitates our interaction with the surrounding world. However, despite extended and extensive research efforts, which encompass many exploration fields, the underlying fundamentals and operational principles of visual information processing in human brain remain unknown. We still are unable to figure out where and how along the path from eyes to the cortex the sensory input perceived by the retina is converted into a meaningful object representation, which can be consciously manipulated by the brain. Studying the vast literature considering the various aspects of brain information processing, I was surprised to learn that the respected scholarly discussion is totally indifferent to the basic keynote question: \"What is information?\" in general or \"What is visual information?\" in particular. In the old days, it was assumed that any scientific research approach has first to define its basic departure points. Why was it overlooked in brain information processing research remains a conundrum. In this paper, I am trying to find a remedy for this bizarre situation. I propose an uncommon definition of \"information\", which can be derived from Kolmogorov's Complexity Theory and Chaitin's notion of Algorithmic Information. Embracing this new definition leads to an inevitable revision of traditional dogmas that shape the state of the art of brain information processing research. I hope this revision would better serve the challenging goal of human visual information processing modeling.\n    ",
        "submission_date": "2008-07-02T00:00:00",
        "last_modified_date": "2008-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0807.0517",
        "title": "Modeling belief systems with scale-free networks",
        "authors": [
            "Miklos Antal",
            "Laszlo Balogh"
        ],
        "abstract": "  Evolution of belief systems has always been in focus of cognitive research. In this paper we delineate a new model describing belief systems as a network of statements considered true. Testing the model a small number of parameters enabled us to reproduce a variety of well-known mechanisms ranging from opinion changes to development of psychological problems. The self-organizing opinion structure showed a scale-free degree distribution. The novelty of our work lies in applying a convenient set of definitions allowing us to depict opinion network dynamics in a highly favorable way, which resulted in a scale-free belief network. As an additional benefit, we listed several conjectural consequences in a number of areas related to thinking and reasoning.\n    ",
        "submission_date": "2008-07-03T00:00:00",
        "last_modified_date": "2008-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0807.0627",
        "title": "Belief decision support and reject for textured images characterization",
        "authors": [
            "Arnaud Martin"
        ],
        "abstract": "  The textured images' classification assumes to consider the images in terms of area with the same texture. In uncertain environment, it could be better to take an imprecise decision or to reject the area corresponding to an unlearning class. Moreover, on the areas that are the classification units, we can have more than one texture. These considerations allows us to develop a belief decision model permitting to reject an area as unlearning and to decide on unions and intersections of learning classes. The proposed approach finds all its justification in an application of seabed characterization from sonar images, which contributes to an illustration.\n    ",
        "submission_date": "2008-07-03T00:00:00",
        "last_modified_date": "2008-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0807.0908",
        "title": "The Correspondence Analysis Platform for Uncovering Deep Structure in Data and Information",
        "authors": [
            "Fionn Murtagh"
        ],
        "abstract": "  We study two aspects of information semantics: (i) the collection of all relationships, (ii) tracking and spotting anomaly and change. The first is implemented by endowing all relevant information spaces with a Euclidean metric in a common projected space. The second is modelled by an induced ultrametric. A very general way to achieve a Euclidean embedding of different information spaces based on cross-tabulation counts (and from other input data formats) is provided by Correspondence Analysis. From there, the induced ultrametric that we are particularly interested in takes a sequential - e.g. temporal - ordering of the data into account. We employ such a perspective to look at narrative, \"the flow of thought and the flow of language\" (Chafe). In application to policy decision making, we show how we can focus analysis in a small number of dimensions.\n    ",
        "submission_date": "2008-07-06T00:00:00",
        "last_modified_date": "2008-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0807.1494",
        "title": "Algorithm Selection as a Bandit Problem with Unbounded Losses",
        "authors": [
            "Matteo Gagliolo",
            "Juergen Schmidhuber"
        ],
        "abstract": "  Algorithm selection is typically based on models of algorithm performance, learned during a separate offline training sequence, which can be prohibitively expensive. In recent work, we adopted an online approach, in which a performance model is iteratively updated and used to guide selection on a sequence of problem instances. The resulting exploration-exploitation trade-off was represented as a bandit problem with expert advice, using an existing solver for this game, but this required the setting of an arbitrary bound on algorithm runtimes, thus invalidating the optimal regret of the solver. In this paper, we propose a simpler framework for representing algorithm selection as a bandit problem, with partial information, and an unknown bound on losses. We adapt an existing solver to this game, proving a bound on its expected regret, which holds also for the resulting algorithm selection technique. We present preliminary experiments with a set of SAT solvers on a mixed SAT-UNSAT benchmark.\n    ",
        "submission_date": "2008-07-09T00:00:00",
        "last_modified_date": "2008-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0807.1906",
        "title": "Extension of Inagaki General Weighted Operators and A New Fusion Rule Class of Proportional Redistribution of Intersection Masses",
        "authors": [
            "Florentin Smarandache"
        ],
        "abstract": "  In this paper we extend Inagaki Weighted Operators fusion rule (WO) in information fusion by doing redistribution of not only the conflicting mass, but also of masses of non-empty intersections, that we call Double Weighted Operators (DWO). Then we propose a new fusion rule Class of Proportional Redistribution of Intersection Masses (CPRIM), which generates many interesting particular fusion rules in information fusion. Both formulas are presented for any number of sources of information. An application and comparison with other fusion rules are given in the last section.\n    ",
        "submission_date": "2008-07-11T00:00:00",
        "last_modified_date": "2008-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0807.3483",
        "title": "Implementing general belief function framework with a practical codification for low complexity",
        "authors": [
            "Arnaud Martin"
        ],
        "abstract": "  In this chapter, we propose a new practical codification of the elements of the Venn diagram in order to easily manipulate the focal elements. In order to reduce the complexity, the eventual constraints must be integrated in the codification at the beginning. Hence, we only consider a reduced hyper power set $D_r^\\Theta$ that can be $2^\\Theta$ or $D^\\Theta$. We describe all the steps of a general belief function framework. The step of decision is particularly studied, indeed, when we can decide on intersections of the singletons of the discernment space no actual decision functions are easily to use. Hence, two approaches are proposed, an extension of previous one and an approach based on the specificity of the elements on which to decide. The principal goal of this chapter is to provide practical codes of a general belief function framework for the researchers and users needing the belief function theory.\n    ",
        "submission_date": "2008-07-22T00:00:00",
        "last_modified_date": "2008-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0807.3669",
        "title": "A new probabilistic transformation of belief mass assignment",
        "authors": [
            "Jean Dezert",
            "Florentin Smarandache"
        ],
        "abstract": "  In this paper, we propose in Dezert-Smarandache Theory (DSmT) framework, a new probabilistic transformation, called DSmP, in order to build a subjective probability measure from any basic belief assignment defined on any model of the frame of discernment. Several examples are given to show how the DSmP transformation works and we compare it to main existing transformations proposed in the literature so far. We show the advantages of DSmP over classical transformations in term of Probabilistic Information Content (PIC). The direct extension of this transformation for dealing with qualitative belief assignments is also presented.\n    ",
        "submission_date": "2008-07-23T00:00:00",
        "last_modified_date": "2008-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0807.3908",
        "title": "A Distributed Process Infrastructure for a Distributed Data Structure",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "abstract": "  The Resource Description Framework (RDF) is continuing to grow outside the bounds of its initial function as a metadata framework and into the domain of general-purpose data modeling. This expansion has been facilitated by the continued increase in the capacity and speed of RDF database repositories known as triple-stores. High-end RDF triple-stores can hold and process on the order of 10 billion triples. In an effort to provide a seamless integration of the data contained in RDF repositories, the Linked Data community is providing specifications for linking RDF data sets into a universal distributed graph that can be traversed by both man and machine. While the seamless integration of RDF data sets is important, at the scale of the data sets that currently exist and will ultimately grow to become, the \"download and index\" philosophy of the World Wide Web will not so easily map over to the Semantic Web. This essay discusses the importance of adding a distributed RDF process infrastructure to the current distributed RDF data structure.\n    ",
        "submission_date": "2008-07-24T00:00:00",
        "last_modified_date": "2008-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0807.4417",
        "title": "On Introspection, Metacognitive Control and Augmented Data Mining Live Cycles",
        "authors": [
            "Daniel Sonntag"
        ],
        "abstract": "  We discuss metacognitive modelling as an enhancement to cognitive modelling and computing. Metacognitive control mechanisms should enable AI systems to self-reflect, reason about their actions, and to adapt to new situations. In this respect, we propose implementation details of a knowledge taxonomy and an augmented data mining life cycle which supports a live integration of obtained models.\n    ",
        "submission_date": "2008-07-28T00:00:00",
        "last_modified_date": "2009-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0807.4680",
        "title": "Hacia una teoria de unificacion para los comportamientos cognitivos",
        "authors": [
            "Sergio Miguel"
        ],
        "abstract": "  Each cognitive science tries to understand a set of cognitive behaviors. The structuring of knowledge of this nature's aspect is far from what it can be expected about a science. Until now universal standard consistently describing the set of cognitive behaviors has not been found, and there are many questions about the cognitive behaviors for which only there are opinions of members of the scientific community. This article has three proposals. The first proposal is to raise to the scientific community the necessity of unified the cognitive behaviors. The second proposal is claim the application of the Newton's reasoning rules about nature of his book, Philosophiae Naturalis Principia Mathematica, to the cognitive behaviors. The third is to propose a scientific theory, currently developing, that follows the rules established by Newton to make sense of nature, and could be the theory to explain all the cognitive behaviors.\n    ",
        "submission_date": "2008-07-29T00:00:00",
        "last_modified_date": "2008-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0807.5091",
        "title": "Message-passing for Maximum Weight Independent Set",
        "authors": [
            "Sujay Sanghavi",
            "Devavrat Shah",
            "Alan Willsky"
        ],
        "abstract": "  We investigate the use of message-passing algorithms for the problem of finding the max-weight independent set (MWIS) in a graph. First, we study the performance of the classical loopy max-product belief propagation. We show that each fixed point estimate of max-product can be mapped in a natural way to an extreme point of the LP polytope associated with the MWIS problem. However, this extreme point may not be the one that maximizes the value of node weights; the particular extreme point at final convergence depends on the initialization of max-product. We then show that if max-product is started from the natural initialization of uninformative messages, it always solves the correct LP -- if it converges. This result is obtained via a direct analysis of the iterative algorithm, and cannot be obtained by looking only at fixed points.\n",
        "submission_date": "2008-07-31T00:00:00",
        "last_modified_date": "2008-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0808.0056",
        "title": "I'm sorry to say, but your understanding of image processing fundamentals is absolutely wrong",
        "authors": [
            "Emanuel Diamant"
        ],
        "abstract": "  The ongoing discussion whether modern vision systems have to be viewed as visually-enabled cognitive systems or cognitively-enabled vision systems is groundless, because perceptual and cognitive faculties of vision are separate components of human (and consequently, artificial) information processing system modeling.\n    ",
        "submission_date": "2008-08-01T00:00:00",
        "last_modified_date": "2008-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0808.0112",
        "title": "Mathematical Structure of Quantum Decision Theory",
        "authors": [
            "V.I. Yukalov",
            "D. Sornette"
        ],
        "abstract": "One of the most complex systems is the human brain whose formalized functioning is characterized by decision theory. We present a \"Quantum Decision Theory\" of decision making, based on the mathematical theory of separable Hilbert spaces. This mathematical structure captures the effect of superposition of composite prospects, including many incorporated intentions, which allows us to explain a variety of interesting fallacies and anomalies that have been reported to particularize the decision making of real human beings. The theory describes entangled decision making, non-commutativity of subsequent decisions, and intention interference of composite prospects. We demonstrate how the violation of the Savage's sure-thing principle (disjunction effect) can be explained as a result of the interference of intentions, when making decisions under uncertainty. The conjunction fallacy is also explained by the presence of the interference terms. We demonstrate that all known anomalies and paradoxes, documented in the context of classical decision theory, are reducible to just a few mathematical archetypes, all of which finding straightforward explanations in the frame of the developed quantum approach.\n    ",
        "submission_date": "2008-08-01T00:00:00",
        "last_modified_date": "2010-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0808.0973",
        "title": "Text Modeling using Unsupervised Topic Models and Concept Hierarchies",
        "authors": [
            "Chaitanya Chemudugunta",
            "Padhraic Smyth",
            "Mark Steyvers"
        ],
        "abstract": "  Statistical topic models provide a general data-driven framework for automated discovery of high-level knowledge from large collections of text documents. While topic models can potentially discover a broad range of themes in a data set, the interpretability of the learned topics is not always ideal. Human-defined concepts, on the other hand, tend to be semantically richer due to careful selection of words to define concepts but they tend not to cover the themes in a data set exhaustively. In this paper, we propose a probabilistic framework to combine a hierarchy of human-defined semantic concepts with statistical topic models to seek the best of both worlds. Experimental results using two different sources of concept hierarchies and two collections of text documents indicate that this combination leads to systematic improvements in the quality of the associated language models as well as enabling new techniques for inferring and visualizing the semantics of a document.\n    ",
        "submission_date": "2008-08-07T00:00:00",
        "last_modified_date": "2008-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0808.1125",
        "title": "Verified Null-Move Pruning",
        "authors": [
            "Omid David-Tabibi",
            "Nathan S. Netanyahu"
        ],
        "abstract": "  In this article we review standard null-move pruning and introduce our extended version of it, which we call verified null-move pruning. In verified null-move pruning, whenever the shallow null-move search indicates a fail-high, instead of cutting off the search from the current node, the search is continued with reduced depth.\n",
        "submission_date": "2008-08-08T00:00:00",
        "last_modified_date": "2008-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0808.1211",
        "title": "Commonsense Knowledge, Ontology and Ordinary Language",
        "authors": [
            "Walid S. Saba"
        ],
        "abstract": "  Over two decades ago a \"quite revolution\" overwhelmingly replaced knowledgebased approaches in natural language processing (NLP) by quantitative (e.g., statistical, corpus-based, machine learning) methods. Although it is our firm belief that purely quantitative approaches cannot be the only paradigm for NLP, dissatisfaction with purely engineering approaches to the construction of large knowledge bases for NLP are somewhat justified. In this paper we hope to demonstrate that both trends are partly misguided and that the time has come to enrich logical semantics with an ontological structure that reflects our commonsense view of the world and the way we talk about in ordinary language. In this paper it will be demonstrated that assuming such an ontological structure a number of challenges in the semantics of natural language (e.g., metonymy, intensionality, copredication, nominal compounds, etc.) can be properly and uniformly addressed.\n    ",
        "submission_date": "2008-08-08T00:00:00",
        "last_modified_date": "2008-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0808.1721",
        "title": "Initial Results on the F-logic to OWL Bi-directional Translation on a Tabled Prolog Engine",
        "authors": [
            "Paul Fodor"
        ],
        "abstract": "  In this paper, we show our results on the bi-directional data exchange between the F-logic language supported by the Flora2 system and the OWL language. Most of the TBox and ABox axioms are translated preserving the semantics between the two representations, such as: proper inclusion, individual definition, functional properties, while some axioms and restrictions require a change in the semantics, such as: numbered and qualified cardinality restrictions. For the second case, we translate the OWL definite style inference rules into F-logic style constraints. We also describe a set of reasoning examples using the above translation, including the reasoning in Flora2 of a variety of ABox queries.\n    ",
        "submission_date": "2008-08-12T00:00:00",
        "last_modified_date": "2008-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0808.3109",
        "title": "n-ary Fuzzy Logic and Neutrosophic Logic Operators",
        "authors": [
            "Florentin Smarandache",
            "V. Christianto"
        ],
        "abstract": "  We extend Knuth's 16 Boolean binary logic operators to fuzzy logic and neutrosophic logic binary operators. Then we generalize them to n-ary fuzzy logic and neutrosophic logic operators using the smarandache codification of the Venn diagram and a defined vector neutrosophic law. In such way, new operators in neutrosophic logic/set/probability are built.\n    ",
        "submission_date": "2008-08-22T00:00:00",
        "last_modified_date": "2008-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.0271",
        "title": "Randomised Variable Neighbourhood Search for Multi Objective Optimisation",
        "authors": [
            "Martin Josef Geiger"
        ],
        "abstract": "  Various local search approaches have recently been applied to machine scheduling problems under multiple objectives. Their foremost consideration is the identification of the set of Pareto optimal alternatives. An important aspect of successfully solving these problems lies in the definition of an appropriate neighbourhood structure. Unclear in this context remains, how interdependencies within the fitness landscape affect the resolution of the problem.\n",
        "submission_date": "2008-09-01T00:00:00",
        "last_modified_date": "2008-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.0406",
        "title": "Foundations of the Pareto Iterated Local Search Metaheuristic",
        "authors": [
            "Martin Josef Geiger"
        ],
        "abstract": "  The paper describes the proposition and application of a local search metaheuristic for multi-objective optimization problems. It is based on two main principles of heuristic search, intensification through variable neighborhoods, and diversification through perturbations and successive iterations in favorable regions of the search space. The concept is successfully tested on permutation flow shop scheduling problems under multiple objectives. While the obtained results are encouraging in terms of their quality, another positive attribute of the approach is its' simplicity as it does require the setting of only very few parameters. The implementation of the Pareto Iterated Local Search metaheuristic is based on the MOOPPS computer system of local search heuristics for multi-objective scheduling which has been awarded the European Academic Software Award 2002 in Ronneby, Sweden (",
        "submission_date": "2008-09-02T00:00:00",
        "last_modified_date": "2008-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.0410",
        "title": "A Computational Study of Genetic Crossover Operators for Multi-Objective Vehicle Routing Problem with Soft Time Windows",
        "authors": [
            "Martin Josef Geiger"
        ],
        "abstract": "  The article describes an investigation of the effectiveness of genetic algorithms for multi-objective combinatorial optimization (MOCO) by presenting an application for the vehicle routing problem with soft time windows. The work is motivated by the question, if and how the problem structure influences the effectiveness of different configurations of the genetic algorithm. Computational results are presented for different classes of vehicle routing problems, varying in their coverage with time windows, time window size, distribution and number of customers. The results are compared with a simple, but effective local search approach for multi-objective combinatorial optimization problems.\n    ",
        "submission_date": "2008-09-02T00:00:00",
        "last_modified_date": "2008-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.0416",
        "title": "Genetic Algorithms for multiple objective vehicle routing",
        "authors": [
            "Martin Josef Geiger"
        ],
        "abstract": "  The talk describes a general approach of a genetic algorithm for multiple objective optimization problems. A particular dominance relation between the individuals of the population is used to define a fitness operator, enabling the genetic algorithm to adress even problems with efficient, but convex-dominated alternatives. The algorithm is implemented in a multilingual computer program, solving vehicle routing problems with time windows under multiple objectives. The graphical user interface of the program shows the progress of the genetic algorithm and the main parameters of the approach can be easily modified. In addition to that, the program provides powerful decision support to the decision maker. The software has proved it's excellence at the finals of the European Academic Software Award EASA, held at the Keble college/ University of Oxford/ Great Britain.\n    ",
        "submission_date": "2008-09-02T00:00:00",
        "last_modified_date": "2008-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.0458",
        "title": "Agent Models of Political Interactions",
        "authors": [
            "Eric Engle"
        ],
        "abstract": "  Looks at state interactions from an agent based AI perspective to see state interactions as an example of emergent intelligent behavior. Exposes basic principles of game theory.\n    ",
        "submission_date": "2008-09-02T00:00:00",
        "last_modified_date": "2008-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.0610",
        "title": "A framework for the interactive resolution of multi-objective vehicle routing problems",
        "authors": [
            "Martin Josef Geiger",
            "Wolf Wenger"
        ],
        "abstract": "  The article presents a framework for the resolution of rich vehicle routing problems which are difficult to address with standard optimization techniques. We use local search on the basis on variable neighborhood search for the construction of the solutions, but embed the techniques in a flexible framework that allows the consideration of complex side constraints of the problem such as time windows, multiple depots, heterogeneous fleets, and, in particular, multiple optimization criteria. In order to identify a compromise alternative that meets the requirements of the decision maker, an interactive procedure is integrated in the resolution of the problem, allowing the modification of the preference information articulated by the decision maker. The framework is prototypically implemented in a computer system. First results of test runs on multiple depot vehicle routing problems with time windows are reported.\n    ",
        "submission_date": "2008-09-03T00:00:00",
        "last_modified_date": "2008-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.0662",
        "title": "Improving Local Search for Fuzzy Scheduling Problems",
        "authors": [
            "Martin Josef Geiger",
            "Sanja Petrovic"
        ],
        "abstract": "  The integration of fuzzy set theory and fuzzy logic into scheduling is a rather new aspect with growing importance for manufacturing applications, resulting in various unsolved aspects. In the current paper, we investigate an improved local search technique for fuzzy scheduling problems with fitness plateaus, using a multi criteria formulation of the problem. We especially address the problem of changing job priorities over time as studied at the Sherwood Press Ltd, a Nottingham based printing company, who is a collaborator on the project.\n    ",
        "submission_date": "2008-09-03T00:00:00",
        "last_modified_date": "2008-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.0753",
        "title": "Proposition of the Interactive Pareto Iterated Local Search Procedure - Elements and Initial Experiments",
        "authors": [
            "Martin Josef Geiger"
        ],
        "abstract": "  The article presents an approach to interactively solve multi-objective optimization problems. While the identification of efficient solutions is supported by computational intelligence techniques on the basis of local search, the search is directed by partial preference information obtained from the decision maker.\n",
        "submission_date": "2008-09-04T00:00:00",
        "last_modified_date": "2008-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.0755",
        "title": "Bin Packing Under Multiple Objectives - a Heuristic Approximation Approach",
        "authors": [
            "Martin Josef Geiger"
        ],
        "abstract": "  The article proposes a heuristic approximation approach to the bin packing problem under multiple objectives. In addition to the traditional objective of minimizing the number of bins, the heterogeneousness of the elements in each bin is minimized, leading to a biobjective formulation of the problem with a tradeoff between the number of bins and their heterogeneousness. An extension of the Best-Fit approximation algorithm is presented to solve the problem. Experimental investigations have been carried out on benchmark instances of different size, ranging from 100 to 1000 items. Encouraging results have been obtained, showing the applicability of the heuristic approach to the described problem.\n    ",
        "submission_date": "2008-09-04T00:00:00",
        "last_modified_date": "2008-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.0757",
        "title": "An application of the Threshold Accepting metaheuristic for curriculum based course timetabling",
        "authors": [
            "Martin Josef Geiger"
        ],
        "abstract": "  The article presents a local search approach for the solution of timetabling problems in general, with a particular implementation for competition track 3 of the International Timetabling Competition 2007 (ITC 2007). The heuristic search procedure is based on Threshold Accepting to overcome local optima. A stochastic neighborhood is proposed and implemented, randomly removing and reassigning events from the current solution.\n",
        "submission_date": "2008-09-04T00:00:00",
        "last_modified_date": "2008-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.0788",
        "title": "Peek Arc Consistency",
        "authors": [
            "Manuel Bodirsky",
            "Hubie Chen"
        ],
        "abstract": "This paper studies peek arc consistency, a reasoning technique that extends the well-known arc consistency technique for constraint satisfaction. In contrast to other more costly extensions of arc consistency that have been studied in the literature, peek arc consistency requires only linear space and quadratic time and can be parallelized in a straightforward way such that it runs in linear time with a linear number of processors. We demonstrate that for various constraint languages, peek arc consistency gives a polynomial-time decision procedure for the constraint satisfaction problem. We also present an algebraic characterization of those constraint languages that can be solved by peek arc consistency, and study the robustness of the algorithm.\n    ",
        "submission_date": "2008-09-04T00:00:00",
        "last_modified_date": "2012-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.0922",
        "title": "Superposition for Fixed Domains",
        "authors": [
            "Matthias Horbach",
            "Christoph Weidenbach"
        ],
        "abstract": "  Superposition is an established decision procedure for a variety of first-order logic theories represented by sets of clauses. A satisfiable theory, saturated by superposition, implicitly defines a minimal term-generated model for the theory. Proving universal properties with respect to a saturated theory directly leads to a modification of the minimal model's term-generated domain, as new Skolem functions are introduced. For many applications, this is not desired.\n",
        "submission_date": "2008-09-04T00:00:00",
        "last_modified_date": "2009-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.0961",
        "title": "MOOPPS: An Optimization System for Multi Objective Scheduling",
        "authors": [
            "Martin Josef Geiger"
        ],
        "abstract": "  In the current paper, we present an optimization system solving multi objective production scheduling problems (MOOPPS). The identification of Pareto optimal alternatives or at least a close approximation of them is possible by a set of implemented metaheuristics. Necessary control parameters can easily be adjusted by the decision maker as the whole software is fully menu driven. This allows the comparison of different metaheuristic algorithms for the considered problem instances. Results are visualized by a graphical user interface showing the distribution of solutions in outcome space as well as their corresponding Gantt chart representation.\n",
        "submission_date": "2008-09-05T00:00:00",
        "last_modified_date": "2008-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.1077",
        "title": "Variable Neighborhood Search for the University Lecturer-Student Assignment Problem",
        "authors": [
            "Martin Josef Geiger",
            "Wolf Wenger"
        ],
        "abstract": "  The paper presents a study of local search heuristics in general and variable neighborhood search in particular for the resolution of an assignment problem studied in the practical work of universities. Here, students have to be assigned to scientific topics which are proposed and supported by members of staff. The problem involves the optimization under given preferences of students which may be expressed when applying for certain topics.\n",
        "submission_date": "2008-09-05T00:00:00",
        "last_modified_date": "2008-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.1618",
        "title": "ECOLANG - Communications Language for Ecological Simulations Network",
        "authors": [
            "Antonio Pereira"
        ],
        "abstract": "  This document describes the communication language used in one multiagent system environment for ecological simulations, based on EcoDynamo simulator application linked with several intelligent agents and visualisation applications, and extends the initial definition of the language. The agents actions and perceptions are translated into messages exchanged with the simulator application and other agents. The concepts and definitions used follow the BNF notation (Backus et al. 1960) and is inspired in the Coach Unilang language (Reis and Lau 2002).\n    ",
        "submission_date": "2008-09-09T00:00:00",
        "last_modified_date": "2008-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.1686",
        "title": "Agent-based Ecological Model Calibration - on the Edge of a New Approach",
        "authors": [
            "Antonio Pereira",
            "Pedro Duarte",
            "Luis Paulo Reis"
        ],
        "abstract": "  The purpose of this paper is to present a new approach to ecological model calibration -- an agent-based software. This agent works on three stages: 1- It builds a matrix that synthesizes the inter-variable relationships; 2- It analyses the steady-state sensitivity of different variables to different parameters; 3- It runs the model iteratively and measures model lack of fit, adequacy and reliability. Stage 3 continues until some convergence criteria are attained. At each iteration, the agent knows from stages 1 and 2, which parameters are most likely to produce the desired shift on predicted results.\n    ",
        "submission_date": "2008-09-09T00:00:00",
        "last_modified_date": "2008-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.2421",
        "title": "Electricity Demand and Energy Consumption Management System",
        "authors": [
            "Juan Ojeda Sarmiento"
        ],
        "abstract": "This project describes the electricity demand and energy consumption management system and its application to Southern Peru smelter. It is composed of an hourly demand-forecasting module and of a simulation component for a plant electrical system. The first module was done using dynamic neural networks with backpropagation training algorithm; it is used to predict the electric power demanded every hour, with an error percentage below of 1%. This information allows efficient management of energy peak demands before this happen, distributing the raise of electric load to other hours or improving those equipments that increase the demand. The simulation module is based in advanced estimation techniques, such as: parametric estimation, neural network modeling, statistic regression and previously developed models, which simulates the electric behavior of the smelter plant. These modules facilitate electricity demand and consumption proper planning, because they allow knowing the behavior of the hourly demand and the consumption patterns of the plant, including the bill components, but also energy deficiencies and opportunities for improvement, based on analysis of information about equipments, processes and production plans, as well as maintenance programs. Finally the results of its application in Southern Peru smelter are presented.\n    ",
        "submission_date": "2008-09-14T00:00:00",
        "last_modified_date": "2011-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.3027",
        "title": "Finding links and initiators: a graph reconstruction problem",
        "authors": [
            "Heikki Mannila",
            "Evimaria Terzi"
        ],
        "abstract": "  Consider a 0-1 observation matrix M, where rows correspond to entities and columns correspond to signals; a value of 1 (or 0) in cell (i,j) of M indicates that signal j has been observed (or not observed) in entity i. Given such a matrix we study the problem of inferring the underlying directed links between entities (rows) and finding which entries in the matrix are initiators.\n",
        "submission_date": "2008-09-17T00:00:00",
        "last_modified_date": "2008-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.3204",
        "title": "Extended ASP tableaux and rule redundancy in normal logic programs",
        "authors": [
            "Matti J\u00e4rvisalo",
            "Emilia Oikarinen"
        ],
        "abstract": "  We introduce an extended tableau calculus for answer set programming (ASP). The proof system is based on the ASP tableaux defined in [Gebser&Schaub, ICLP 2006], with an added extension rule. We investigate the power of Extended ASP Tableaux both theoretically and empirically. We study the relationship of Extended ASP Tableaux with the Extended Resolution proof system defined by Tseitin for sets of clauses, and separate Extended ASP Tableaux from ASP Tableaux by giving a polynomial-length proof for a family of normal logic programs P_n for which ASP Tableaux has exponential-length minimal proofs with respect to n. Additionally, Extended ASP Tableaux imply interesting insight into the effect of program simplification on the lengths of proofs in ASP. Closely related to Extended ASP Tableaux, we empirically investigate the effect of redundant rules on the efficiency of ASP solving.\n",
        "submission_date": "2008-09-18T00:00:00",
        "last_modified_date": "2008-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.4530",
        "title": "Mining Meaning from Wikipedia",
        "authors": [
            "Olena Medelyan",
            "David Milne",
            "Catherine Legg",
            "Ian H. Witten"
        ],
        "abstract": "  Wikipedia is a goldmine of information; not just for its many readers, but also for the growing community of researchers who recognize it as a resource of exceptional scale and utility. It represents a vast investment of manual effort and judgment: a huge, constantly evolving tapestry of concepts and relations that is being applied to a host of tasks.\n",
        "submission_date": "2008-09-26T00:00:00",
        "last_modified_date": "2009-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.4582",
        "title": "Achieving compositionality of the stable model semantics for Smodels programs",
        "authors": [
            "Emilia Oikarinen",
            "Tomi Janhunen"
        ],
        "abstract": "  In this paper, a Gaifman-Shapiro-style module architecture is tailored to the case of Smodels programs under the stable model semantics. The composition of Smodels program modules is suitably limited by module conditions which ensure the compatibility of the module system with stable models. Hence the semantics of an entire Smodels program depends directly on stable models assigned to its modules. This result is formalized as a module theorem which truly strengthens Lifschitz and Turner's splitting-set theorem for the class of Smodels programs. To streamline generalizations in the future, the module theorem is first proved for normal programs and then extended to cover Smodels programs using a translation from the latter class of programs to the former class. Moreover, the respective notion of module-level equivalence, namely modular equivalence, is shown to be a proper congruence relation: it is preserved under substitutions of modules that are modularly equivalent. Principles for program decomposition are also addressed. The strongly connected components of the respective dependency graph can be exploited in order to extract a module structure when there is no explicit a priori knowledge about the modules of a program. The paper includes a practical demonstration of tools that have been developed for automated (de)composition of Smodels programs.\n",
        "submission_date": "2008-09-26T00:00:00",
        "last_modified_date": "2008-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.4784",
        "title": "A Computational Study on Emotions and Temperament in Multi-Agent Systems",
        "authors": [
            "Luis Paulo Reis",
            "Daria Barteneva",
            "Nuno Lau"
        ],
        "abstract": "  Recent advances in neurosciences and psychology have provided evidence that affective phenomena pervade intelligence at many levels, being inseparable from the cognitionaction loop. Perception, attention, memory, learning, decisionmaking, adaptation, communication and social interaction are some of the aspects influenced by them. This work draws its inspirations from neurobiology, psychophysics and sociology to approach the problem of building autonomous robots capable of interacting with each other and building strategies based on temperamental decision mechanism. Modelling emotions is a relatively recent focus in artificial intelligence and cognitive modelling. Such models can ideally inform our understanding of human behavior. We may see the development of computational models of emotion as a core research focus that will facilitate advances in the large array of computational systems that model, interpret or influence human behavior. We propose a model based on a scalable, flexible and modular approach to emotion which allows runtime evaluation between emotional quality and performance. The results achieved showed that the strategies based on temperamental decision mechanism strongly influence the system performance and there are evident dependency between emotional state of the agents and their temperamental type, as well as the dependency between the team performance and the temperamental configuration of the team members, and this enable us to conclude that the modular approach to emotional programming based on temperamental theory is the good choice to develop computational mind models for emotional behavioral Multi-Agent systems.\n    ",
        "submission_date": "2008-09-27T00:00:00",
        "last_modified_date": "2008-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.0139",
        "title": "Determining the Unithood of Word Sequences using a Probabilistic Approach",
        "authors": [
            "Wilson Wong",
            "Wei Liu",
            "Mohammed Bennamoun"
        ],
        "abstract": "  Most research related to unithood were conducted as part of a larger effort for the determination of termhood. Consequently, novelties are rare in this small sub-field of term extraction. In addition, existing work were mostly empirically motivated and derived. We propose a new probabilistically-derived measure, independent of any influences of termhood, that provides dedicated measures to gather linguistic evidence from parsed text and statistical evidence from Google search engine for the measurement of unithood. Our comparative study using 1,825 test cases against an existing empirically-derived function revealed an improvement in terms of precision, recall and accuracy.\n    ",
        "submission_date": "2008-10-01T00:00:00",
        "last_modified_date": "2008-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.0156",
        "title": "Determining the Unithood of Word Sequences using Mutual Information and Independence Measure",
        "authors": [
            "Wilson Wong",
            "Wei Liu",
            "Mohammed Bennamoun"
        ],
        "abstract": "  Most works related to unithood were conducted as part of a larger effort for the determination of termhood. Consequently, the number of independent research that study the notion of unithood and produce dedicated techniques for measuring unithood is extremely small. We propose a new approach, independent of any influences of termhood, that provides dedicated measures to gather linguistic evidence from parsed text and statistical evidence from Google search engine for the measurement of unithood. Our evaluations revealed a precision and recall of 98.68% and 91.82% respectively with an accuracy at 95.42% in measuring the unithood of 1005 test cases.\n    ",
        "submission_date": "2008-10-01T00:00:00",
        "last_modified_date": "2008-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.0332",
        "title": "Enhanced Integrated Scoring for Cleaning Dirty Texts",
        "authors": [
            "Wilson Wong",
            "Wei Liu",
            "Mohammed Bennamoun"
        ],
        "abstract": "  An increasing number of approaches for ontology engineering from text are gearing towards the use of online sources such as company intranet and the World Wide Web. Despite such rise, not much work can be found in aspects of preprocessing and cleaning dirty texts from online sources. This paper presents an enhancement of an Integrated Scoring for Spelling error correction, Abbreviation expansion and Case restoration (ISSAC). ISSAC is implemented as part of a text preprocessing phase in an ontology engineering system. New evaluations performed on the enhanced ISSAC using 700 chat records reveal an improved accuracy of 98% as compared to 96.5% and 71% based on the use of only basic ISSAC and of Aspell, respectively.\n    ",
        "submission_date": "2008-10-02T00:00:00",
        "last_modified_date": "2008-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.1186",
        "title": "On-the-fly Macros",
        "authors": [
            "Hubie Chen",
            "Omer Gimenez"
        ],
        "abstract": "We present a domain-independent algorithm that computes macros in a novel way. Our algorithm computes macros \"on-the-fly\" for a given set of states and does not require previously learned or inferred information, nor prior domain knowledge. The algorithm is used to define new domain-independent tractable classes of classical planning that are proved to include \\emph{Blocksworld-arm} and \\emph{Towers of Hanoi}.\n    ",
        "submission_date": "2008-10-07T00:00:00",
        "last_modified_date": "2012-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.2046",
        "title": "Modeling of Social Transitions Using Intelligent Systems",
        "authors": [
            "Hamed Owladeghaffari",
            "Witold Pedrycz",
            "Mostafa Sharifzadeh"
        ],
        "abstract": "  In this study, we reproduce two new hybrid intelligent systems, involve three prominent intelligent computing and approximate reasoning methods: Self Organizing feature Map (SOM), Neruo-Fuzzy Inference System and Rough Set Theory (RST),called SONFIS and SORST. We show how our algorithms can be construed as a linkage of government-society interactions, where government catches various states of behaviors: solid (absolute) or flexible. So, transition of society, by changing of connectivity parameters (noise) from order to disorder is inferred.\n    ",
        "submission_date": "2008-10-11T00:00:00",
        "last_modified_date": "2008-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.2311",
        "title": "Non-Negative Matrix Factorization, Convexity and Isometry",
        "authors": [
            "Nikolaos Vasiloglou",
            "Alexander G. Gray",
            "David V. Anderson"
        ],
        "abstract": "  In this paper we explore avenues for improving the reliability of dimensionality reduction methods such as Non-Negative Matrix Factorization (NMF) as interpretive exploratory data analysis tools. We first explore the difficulties of the optimization problem underlying NMF, showing for the first time that non-trivial NMF solutions always exist and that the optimization problem is actually convex, by using the theory of Completely Positive Factorization. We subsequently explore four novel approaches to finding globally-optimal NMF solutions using various ideas from convex optimization. We then develop a new method, isometric NMF (isoNMF), which preserves non-negativity while also providing an isometric embedding, simultaneously achieving two properties which are helpful for interpretation. Though it results in a more difficult optimization problem, we show experimentally that the resulting method is scalable and even achieves more compact spectra than standard NMF.\n    ",
        "submission_date": "2008-10-13T00:00:00",
        "last_modified_date": "2009-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.2861",
        "title": "A comparison of the notions of optimality in soft constraints and graphical games",
        "authors": [
            "Krzysztof R. Apt",
            "Francesca Rossi",
            "K. Brent Venable"
        ],
        "abstract": "  The notion of optimality naturally arises in many areas of applied mathematics and computer science concerned with decision making. Here we consider this notion in the context of two formalisms used for different purposes and in different research areas: graphical games and soft constraints. We relate the notion of optimality used in the area of soft constraint satisfaction problems (SCSPs) to that used in graphical games, showing that for a large class of SCSPs that includes weighted constraints every optimal solution corresponds to a Nash equilibrium that is also a Pareto efficient joint strategy.\n    ",
        "submission_date": "2008-10-16T00:00:00",
        "last_modified_date": "2008-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.3451",
        "title": "The many faces of optimism - Extended version",
        "authors": [
            "Istv\u00e1n Szita",
            "Andr\u00e1s L\u0151rincz"
        ],
        "abstract": "  The exploration-exploitation dilemma has been an intriguing and unsolved problem within the framework of reinforcement learning. \"Optimism in the face of uncertainty\" and model building play central roles in advanced exploration methods. Here, we integrate several concepts and obtain a fast and simple algorithm. We show that the proposed algorithm finds a near-optimal policy in polynomial time, and give experimental evidence that it is robust and efficient compared to its ascendants.\n    ",
        "submission_date": "2008-10-20T00:00:00",
        "last_modified_date": "2008-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.3474",
        "title": "Social Learning Methods in Board Games",
        "authors": [
            "Vukosi N. Marivate",
            "Tshilidzi Marwala"
        ],
        "abstract": "  This paper discusses the effects of social learning in training of game playing agents. The training of agents in a social context instead of a self-play environment is investigated. Agents that use the reinforcement learning algorithms are trained in social settings. This mimics the way in which players of board games such as scrabble and chess mentor each other in their clubs. A Round Robin tournament and a modified Swiss tournament setting are used for the training. The agents trained using social settings are compared to self play agents and results indicate that more robust agents emerge from the social training setting. Higher state space games can benefit from such settings as diverse set of agents will have multiple strategies that increase the chances of obtaining more experienced players at the end of training. The Social Learning trained agents exhibit better playing experience than self play agents. The modified Swiss playing style spawns a larger number of better playing agents as the population size increases.\n    ",
        "submission_date": "2008-10-20T00:00:00",
        "last_modified_date": "2008-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.3605",
        "title": "A Minimum Relative Entropy Principle for Learning and Acting",
        "authors": [
            "Pedro A. Ortega",
            "Daniel A. Braun"
        ],
        "abstract": "This paper proposes a method to construct an adaptive agent that is universal with respect to a given class of experts, where each expert is an agent that has been designed specifically for a particular environment. This adaptive control problem is formalized as the problem of minimizing the relative entropy of the adaptive agent from the expert that is most suitable for the unknown environment. If the agent is a passive observer, then the optimal solution is the well-known Bayesian predictor. However, if the agent is active, then its past actions need to be treated as causal interventions on the I/O stream rather than normal probability conditions. Here it is shown that the solution to this new variational problem is given by a stochastic controller called the Bayesian control rule, which implements adaptive behavior as a mixture of experts. Furthermore, it is shown that under mild assumptions, the Bayesian control rule converges to the control law of the most suitable expert.\n    ",
        "submission_date": "2008-10-20T00:00:00",
        "last_modified_date": "2010-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.3865",
        "title": "Relationship between Diversity and Perfomance of Multiple Classifiers for Decision Support",
        "authors": [
            "R. Musehane",
            "F. Netshiongolwe",
            "F.V. Nelwamondo",
            "L. Masisi",
            "T. Marwala"
        ],
        "abstract": "  The paper presents the investigation and implementation of the relationship between diversity and the performance of multiple classifiers on classification accuracy. The study is critical as to build classifiers that are strong and can generalize better. The parameters of the neural network within the committee were varied to induce diversity; hence structural diversity is the focus for this study. The hidden nodes and the activation function are the parameters that were varied. The diversity measures that were adopted from ecology such as Shannon and Simpson were used to quantify diversity. Genetic algorithm is used to find the optimal ensemble by using the accuracy as the cost function. The results observed shows that there is a relationship between structural diversity and accuracy. It is observed that the classification accuracy of an ensemble increases as the diversity increases. There was an increase of 3%-6% in the classification accuracy.\n    ",
        "submission_date": "2008-10-21T00:00:00",
        "last_modified_date": "2008-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.4668",
        "title": "On Granular Knowledge Structures",
        "authors": [
            "Yi Zeng",
            "Ning Zhong"
        ],
        "abstract": "  Knowledge plays a central role in human and artificial intelligence. One of the key characteristics of knowledge is its structured organization. Knowledge can be and should be presented in multiple levels and multiple views to meet people's needs in different levels of granularities and from different perspectives. In this paper, we stand on the view point of granular computing and provide our understanding on multi-level and multi-view of knowledge through granular knowledge structures (GKS). Representation of granular knowledge structures, operations for building granular knowledge structures and how to use them are investigated. As an illustration, we provide some examples through results from an analysis of proceeding papers. Results show that granular knowledge structures could help users get better understanding of the knowledge source from set theoretical, logical and visual point of views. One may consider using them to meet specific needs or solve certain kinds of problems.\n    ",
        "submission_date": "2008-10-26T00:00:00",
        "last_modified_date": "2008-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.5717",
        "title": "On the Conditional Independence Implication Problem: A Lattice-Theoretic Approach",
        "authors": [
            "Mathias Niepert",
            "Dirk Van Gucht",
            "Marc Gyssens"
        ],
        "abstract": "  A lattice-theoretic framework is introduced that permits the study of the conditional independence (CI) implication problem relative to the class of discrete probability measures. Semi-lattices are associated with CI statements and a finite, sound and complete inference system relative to semi-lattice inclusions is presented. This system is shown to be (1) sound and complete for saturated CI statements, (2) complete for general CI statements, and (3) sound and complete for stable CI statements. These results yield a criterion that can be used to falsify instances of the implication problem and several heuristics are derived that approximate this \"lattice-exclusion\" criterion in polynomial time. Finally, we provide experimental results that relate our work to results obtained from other existing inference algorithms.\n    ",
        "submission_date": "2008-10-31T00:00:00",
        "last_modified_date": "2008-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.0123",
        "title": "A computational model of affects",
        "authors": [
            "Mika Turkia"
        ],
        "abstract": "  This article provides a simple logical structure, in which affective concepts (i.e. concepts related to emotions and feelings) can be defined. The set of affects defined is similar to the set of emotions covered in the OCC model (Ortony A., Collins A., and Clore G. L.: The Cognitive Structure of Emotions. Cambridge University Press, 1988), but the model presented in this article is fully computationally defined.\n    ",
        "submission_date": "2008-11-02T00:00:00",
        "last_modified_date": "2008-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.0131",
        "title": "Balancing Exploration and Exploitation by an Elitist Ant System with Exponential Pheromone Deposition Rule",
        "authors": [
            "Ayan Acharya",
            "Deepyaman Maiti",
            "Aritra Banerjee",
            "Amit Konar"
        ],
        "abstract": "  The paper presents an exponential pheromone deposition rule to modify the basic ant system algorithm which employs constant deposition rule. A stability analysis using differential equation is carried out to find out the values of parameters that make the ant system dynamics stable for both kinds of deposition rule. A roadmap of connected cities is chosen as the problem environment where the shortest route between two given cities is required to be discovered. Simulations performed with both forms of deposition approach using Elitist Ant System model reveal that the exponential deposition approach outperforms the classical one by a large extent. Exhaustive experiments are also carried out to find out the optimum setting of different controlling parameters for exponential deposition approach and an empirical relationship between the major controlling parameters of the algorithm and some features of problem environment.\n    ",
        "submission_date": "2008-11-02T00:00:00",
        "last_modified_date": "2008-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.0134",
        "title": "A Novel Parser Design Algorithm Based on Artificial Ants",
        "authors": [
            "Deepyaman Maiti",
            "Ayan Acharya",
            "Amit Konar",
            "Janarthanan Ramadoss"
        ],
        "abstract": "  This article presents a unique design for a parser using the Ant Colony Optimization algorithm. The paper implements the intuitive thought process of human mind through the activities of artificial ants. The scheme presented here uses a bottom-up approach and the parsing program can directly use ambiguous or redundant grammars. We allocate a node corresponding to each production rule present in the given grammar. Each node is connected to all other nodes (representing other production rules), thereby establishing a completely connected graph susceptible to the movement of artificial ants. Each ant tries to modify this sentential form by the production rule present in the node and upgrades its position until the sentential form reduces to the start symbol S. Successful ants deposit pheromone on the links that they have traversed through. Eventually, the optimum path is discovered by the links carrying maximum amount of pheromone concentration. The design is simple, versatile, robust and effective and obviates the calculation of the above mentioned sets and precedence relation tables. Further advantages of our scheme lie in i) ascertaining whether a given string belongs to the language represented by the grammar, and ii) finding out the shortest possible path from the given string to the start symbol S in case multiple routes exist.\n    ",
        "submission_date": "2008-11-02T00:00:00",
        "last_modified_date": "2008-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.0136",
        "title": "Extension of Max-Min Ant System with Exponential Pheromone Deposition Rule",
        "authors": [
            "Ayan Acharya",
            "Deepyaman Maiti",
            "Aritra Banerjee",
            "R. Janarthanan",
            "Amit Konar"
        ],
        "abstract": "  The paper presents an exponential pheromone deposition approach to improve the performance of classical Ant System algorithm which employs uniform deposition rule. A simplified analysis using differential equations is carried out to study the stability of basic ant system dynamics with both exponential and constant deposition rules. A roadmap of connected cities, where the shortest path between two specified cities are to be found out, is taken as a platform to compare Max-Min Ant System model (an improved and popular model of Ant System algorithm) with exponential and constant deposition rules. Extensive simulations are performed to find the best parameter settings for non-uniform deposition approach and experiments with these parameter settings revealed that the above approach outstripped the traditional one by a large extent in terms of both solution quality and convergence time.\n    ",
        "submission_date": "2008-11-02T00:00:00",
        "last_modified_date": "2008-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.0310",
        "title": "Edhibou: a Customizable Interface for Decision Support in a Semantic Portal",
        "authors": [
            "Fadi Badra",
            "Mathieu D'Aquin",
            "Jean Lieber",
            "Thomas Meilender"
        ],
        "abstract": "  The Semantic Web is becoming more and more a reality, as the required technologies have reached an appropriate level of maturity. However, at this stage, it is important to provide tools facilitating the use and deployment of these technologies by end-users. In this paper, we describe EdHibou, an automatically generated, ontology-based graphical user interface that integrates in a semantic portal. The particularity of EdHibou is that it makes use of OWL reasoning capabilities to provide intelligent features, such as decision support, upon the underlying ontology. We present an application of EdHibou to medical decision support based on a formalization of clinical guidelines in OWL and show how it can be customized thanks to an ontology of graphical components.\n    ",
        "submission_date": "2008-11-03T00:00:00",
        "last_modified_date": "2008-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.0335",
        "title": "Cooperative interface of a swarm of UAVs",
        "authors": [
            "Sylvie Saget",
            "Francois Legras",
            "Gilles Coppin"
        ],
        "abstract": "  After presenting the broad context of authority sharing, we outline how introducing more natural interaction in the design of the ground operator interface of UV systems should help in allowing a single operator to manage the complexity of his/her task. Introducing new modalities is one one of the means in the realization of our vision of next- generation GOI. A more fundamental aspect resides in the interaction manager which should help balance the workload of the operator between mission and interaction, notably by applying a multi-strategy approach to generation and interpretation. We intend to apply these principles to the context of the Smaart prototype, and in this perspective, we illustrate how to characterize the workload associated with a particular operational situation.\n    ",
        "submission_date": "2008-11-03T00:00:00",
        "last_modified_date": "2008-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.0340",
        "title": "Document stream clustering: experimenting an incremental algorithm and AR-based tools for highlighting dynamic trends",
        "authors": [
            "Alain Lelu",
            "Martine Cadot",
            "Pascal Cuxac"
        ],
        "abstract": "  We address here two major challenges presented by dynamic data mining: 1) the stability challenge: we have implemented a rigorous incremental density-based clustering algorithm, independent from any initial conditions and ordering of the data-vectors stream, 2) the cognitive challenge: we have implemented a stringent selection process of association rules between clusters at time t-1 and time t for directly generating the main conclusions about the dynamics of a data-stream. We illustrate these points with an application to a two years and 2600 documents scientific information database.\n    ",
        "submission_date": "2008-11-03T00:00:00",
        "last_modified_date": "2008-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.0602",
        "title": "Classification dynamique d'un flux documentaire : une \u00e9valuation statique pr\u00e9alable de l'algorithme GERMEN",
        "authors": [
            "Alain Lelu",
            "Pascal Cuxac",
            "Joel Johansson"
        ],
        "abstract": "  Data-stream clustering is an ever-expanding subdomain of knowledge extraction. Most of the past and present research effort aims at efficient scaling up for the huge data repositories. Our approach focuses on qualitative improvement, mainly for \"weak signals\" detection and precise tracking of topical evolutions in the framework of information watch - though scalability is intrinsically guaranteed in a possibly distributed implementation. Our GERMEN algorithm exhaustively picks up the whole set of density peaks of the data at time t, by identifying the local perturbations induced by the current document vector, such as changing cluster borders, or new/vanishing clusters. Optimality yields from the uniqueness 1) of the density landscape for any value of our zoom parameter, 2) of the cluster allocation operated by our border propagation rule. This results in a rigorous independence from the data presentation ranking or any initialization parameter. We present here as a first step the only assessment of a static view resulting from one year of the CNRS/INIST Pascal database in the field of geotechnics.\n    ",
        "submission_date": "2008-11-04T00:00:00",
        "last_modified_date": "2008-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.0942",
        "title": "\u00c9tude longitudinale d'une proc\u00e9dure de mod\u00e9lisation de connaissances en mati\u00e8re de gestion du territoire agricole",
        "authors": [
            "Florence Le Ber",
            "Christian Brassac"
        ],
        "abstract": "  This paper gives an introduction to this issue, and presents the framework and the main steps of the Rosa project. Four teams of researchers, agronomists, computer scientists, psychologists and linguists were involved during five years within this project that aimed at the development of a knowledge based system. The purpose of the Rosa system is the modelling and the comparison of farm spatial organizations. It relies on a formalization of agronomical knowledge and thus induces a joint knowledge building process involving both the agronomists and the computer scientists. The paper describes the steps of the modelling process as well as the filming procedures set up by the psychologists and linguists in order to make explicit and to analyze the underlying knowledge building process.\n    ",
        "submission_date": "2008-11-06T00:00:00",
        "last_modified_date": "2008-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.0971",
        "title": "Mining Complex Hydrobiological Data with Galois Lattices",
        "authors": [
            "Aur\u00e9lie Bertaux",
            "AGN\u00e8s Braud",
            "Florence Le Ber"
        ],
        "abstract": "  We have used Galois lattices for mining hydrobiological data. These data are about macrophytes, that are macroscopic plants living in water bodies. These plants are characterized by several biological traits, that own several modalities. Our aim is to cluster the plants according to their common traits and modalities and to find out the relations between traits. Galois lattices are efficient methods for such an aim, but apply on binary data. In this article, we detail a few approaches we used to transform complex hydrobiological data into binary data and compare the first results obtained thanks to Galois lattices.\n    ",
        "submission_date": "2008-11-06T00:00:00",
        "last_modified_date": "2008-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.1319",
        "title": "Modeling Social Annotation: a Bayesian Approach",
        "authors": [
            "Anon Plangprasopchok",
            "Kristina Lerman"
        ],
        "abstract": "Collaborative tagging systems, such as Delicious, CiteULike, and others, allow users to annotate resources, e.g., Web pages or scientific papers, with descriptive labels called tags. The social annotations contributed by thousands of users, can potentially be used to infer categorical knowledge, classify documents or recommend new relevant information. Traditional text inference methods do not make best use of social annotation, since they do not take into account variations in individual users' perspectives and vocabulary. In a previous work, we introduced a simple probabilistic model that takes interests of individual annotators into account in order to find hidden topics of annotated resources. Unfortunately, that approach had one major shortcoming: the number of topics and interests must be specified a priori. To address this drawback, we extend the model to a fully Bayesian framework, which offers a way to automatically estimate these numbers. In particular, the model allows the number of interests and topics to change as suggested by the structure of the data. We evaluate the proposed model in detail on the synthetic and real-world data by comparing its performance to Latent Dirichlet Allocation on the topic extraction task. For the latter evaluation, we apply the model to infer topics of Web resources from social annotations obtained from Delicious in order to discover new resources similar to a specified one. Our empirical results demonstrate that the proposed model is a promising method for exploiting social knowledge contained in user-generated annotations.\n    ",
        "submission_date": "2008-11-09T00:00:00",
        "last_modified_date": "2010-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.1618",
        "title": "Airport Gate Assignment: New Model and Implementation",
        "authors": [
            "Chendong Li"
        ],
        "abstract": "  Airport gate assignment is of great importance in airport operations. In this paper, we study the Airport Gate Assignment Problem (AGAP), propose a new model and implement the model with Optimization Programming language (OPL). With the objective to minimize the number of conflicts of any two adjacent aircrafts assigned to the same gate, we build a mathematical model with logical constraints and the binary constraints, which can provide an efficient evaluation criterion for the Airlines to estimate the current gate assignment. To illustrate the feasibility of the model we construct experiments with the data obtained from Continental Airlines, Houston Gorge Bush Intercontinental Airport IAH, which indicate that our model is both energetic and effective. Moreover, we interpret experimental results, which further demonstrate that our proposed model can provide a powerful tool for airline companies to estimate the efficiency of their current work of gate assignment.\n    ",
        "submission_date": "2008-11-11T00:00:00",
        "last_modified_date": "2008-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.1711",
        "title": "Artificial Intelligence Techniques for Steam Generator Modelling",
        "authors": [
            "Sarah Wright",
            "Tshilidzi Marwala"
        ],
        "abstract": "  This paper investigates the use of different Artificial Intelligence methods to predict the values of several continuous variables from a Steam Generator. The objective was to determine how the different artificial intelligence methods performed in making predictions on the given dataset. The artificial intelligence methods evaluated were Neural Networks, Support Vector Machines, and Adaptive Neuro-Fuzzy Inference Systems. The types of neural networks investigated were Multi-Layer Perceptions, and Radial Basis Function. Bayesian and committee techniques were applied to these neural networks. Each of the AI methods considered was simulated in Matlab. The results of the simulations showed that all the AI methods were capable of predicting the Steam Generator data reasonably accurately. However, the Adaptive Neuro-Fuzzy Inference system out performed the other methods in terms of accuracy and ease of implementation, while still achieving a fast execution time as well as a reasonable training time.\n    ",
        "submission_date": "2008-11-11T00:00:00",
        "last_modified_date": "2008-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.1878",
        "title": "Action Theory Evolution",
        "authors": [
            "Ivan Varzinczak"
        ],
        "abstract": "  Like any other logical theory, domain descriptions in reasoning about actions may evolve, and thus need revision methods to adequately accommodate new information about the behavior of actions. The present work is about changing action domain descriptions in propositional dynamic logic. Its contribution is threefold: first we revisit the semantics of action theory contraction that has been done in previous work, giving more robust operators that express minimal change based on a notion of distance between Kripke-models. Second we give algorithms for syntactical action theory contraction and establish their correctness w.r.t. our semantics. Finally we state postulates for action theory contraction and assess the behavior of our operators w.r.t. them. Moreover, we also address the revision counterpart of action theory change, showing that it benefits from our semantics for contraction.\n    ",
        "submission_date": "2008-11-12T00:00:00",
        "last_modified_date": "2008-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.3055",
        "title": "Exact phase transition of backtrack-free search with implications on the power of greedy algorithms",
        "authors": [
            "Liang Li",
            "Tian Liu",
            "Ke Xu"
        ],
        "abstract": "  Backtracking is a basic strategy to solve constraint satisfaction problems (CSPs). A satisfiable CSP instance is backtrack-free if a solution can be found without encountering any dead-end during a backtracking search, implying that the instance is easy to solve. We prove an exact phase transition of backtrack-free search in some random CSPs, namely in Model RB and in Model RD. This is the first time an exact phase transition of backtrack-free search can be identified on some random CSPs. Our technical results also have interesting implications on the power of greedy algorithms, on the width of random hypergraphs and on the exact satisfiability threshold of random CSPs.\n    ",
        "submission_date": "2008-11-19T00:00:00",
        "last_modified_date": "2008-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.0659",
        "title": "Probabilistic reasoning with answer sets",
        "authors": [
            "Chitta Baral",
            "Michael Gelfond",
            "Nelson Rushton"
        ],
        "abstract": "  This paper develops a declarative language, P-log, that combines logical and probabilistic arguments in its reasoning. Answer Set Prolog is used as the logical foundation, while causal Bayes nets serve as a probabilistic foundation. We give several non-trivial examples and illustrate the use of P-log for knowledge representation and updating of knowledge. We argue that our approach to updates is more appealing than existing approaches. We give sufficiency conditions for the coherency of P-log programs and show that Bayes nets can be easily mapped to coherent P-log programs.\n    ",
        "submission_date": "2008-12-03T00:00:00",
        "last_modified_date": "2008-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.0790",
        "title": "Justifications for Logic Programs under Answer Set Semantics",
        "authors": [
            "Enrico Pontelli",
            "Tran Cao Son",
            "Omar Elkhatib"
        ],
        "abstract": "  The paper introduces the notion of off-line justification for Answer Set Programming (ASP). Justifications provide a graph-based explanation of the truth value of an atom w.r.t. a given answer set. The paper extends also this notion to provide justification of atoms during the computation of an answer set (on-line justification), and presents an integration of on-line justifications within the computation model of Smodels. Off-line and on-line justifications provide useful tools to enhance understanding of ASP, and they offer a basic data structure to support methodologies and tools for debugging answer set programs. A preliminary implementation has been developed in ASP-PROLOG.\n",
        "submission_date": "2008-12-03T00:00:00",
        "last_modified_date": "2008-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.0885",
        "title": "Elementary epistemological features of machine intelligence",
        "authors": [
            "Marko Horvat"
        ],
        "abstract": "  Theoretical analysis of machine intelligence (MI) is useful for defining a common platform in both theoretical and applied artificial intelligence (AI). The goal of this paper is to set canonical definitions that can assist pragmatic research in both strong and weak AI. Described epistemological features of machine intelligence include relationship between intelligent behavior, intelligent and unintelligent machine characteristics, observable and unobservable entities and classification of intelligence. The paper also establishes algebraic definitions of efficiency and accuracy of MI tests as their quality measure. The last part of the paper addresses the learning process with respect to the traditional epistemology and the epistemology of MI described here. The proposed views on MI positively correlate to the Hegelian monistic epistemology and contribute towards amalgamating idealistic deliberations with the AI theory, particularly in a local frame of reference.\n    ",
        "submission_date": "2008-12-04T00:00:00",
        "last_modified_date": "2017-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.1014",
        "title": "Adaptive Spam Detection Inspired by a Cross-Regulation Model of Immune Dynamics: A Study of Concept Drift",
        "authors": [
            "Alaa Abi-Haidar",
            "Luis M. Rocha"
        ],
        "abstract": "  This paper proposes a novel solution to spam detection inspired by a model of the adaptive immune system known as the crossregulation model. We report on the testing of a preliminary algorithm on six e-mail corpora. We also compare our results statically and dynamically with those obtained by the Naive Bayes classifier and another binary classification method we developed previously for biomedical text-mining applications. We show that the cross-regulation model is competitive against those and thus promising as a bio-inspired algorithm for spam detection in particular, and binary classification in general.\n    ",
        "submission_date": "2008-12-04T00:00:00",
        "last_modified_date": "2008-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.1126",
        "title": "Emerge-Sort: Converging to Ordered Sequences by Simple Local Operators",
        "authors": [
            "Dimitris Kalles",
            "Alexis Kaporis"
        ],
        "abstract": "  In this paper we examine sorting on the assumption that we do not know in advance which way to sort a sequence of numbers and we set at work simple local comparison and swap operators whose repeating application ends up in sorted sequences. These are the basic elements of Emerge-Sort, our approach to self-organizing sorting, which we then validate experimentally across a range of samples. Observing an O(n2) run-time behaviour, we note that the n/logn delay coefficient that differentiates Emerge-Sort from the classical comparison based algorithms is an instantiation of the price of anarchy we pay for not imposing a sorting order and for letting that order emerge through the local interactions.\n    ",
        "submission_date": "2008-12-05T00:00:00",
        "last_modified_date": "2009-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.1462",
        "title": "Logic programs with propositional connectives and aggregates",
        "authors": [
            "Paolo Ferraris"
        ],
        "abstract": "  Answer set programming (ASP) is a logic programming paradigm that can be used to solve complex combinatorial search problems. Aggregates are an ASP construct that plays an important role in many applications. Defining a satisfactory semantics of aggregates turned out to be a difficult problem, and in this paper we propose a new approach, based on an analogy between aggregates and propositional connectives. First, we extend the definition of an answer set/stable model to cover arbitrary propositional theories; then we define aggregates on top of them both as primitive constructs and as abbreviations for formulas. Our definition of an aggregate combines expressiveness and simplicity, and it inherits many theorems about programs with nested expressions, such as theorems about strong equivalence and splitting.\n    ",
        "submission_date": "2008-12-08T00:00:00",
        "last_modified_date": "2008-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.1843",
        "title": "Identification of parameters underlying emotions and a classification of emotions",
        "authors": [
            "N. Arvind Kumar"
        ],
        "abstract": "  The standard classification of emotions involves categorizing the expression of emotions. In this paper, parameters underlying some emotions are identified and a new classification based on these parameters is suggested.\n    ",
        "submission_date": "2008-12-10T00:00:00",
        "last_modified_date": "2008-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.2535",
        "title": "Pattern Recognition and Memory Mapping using Mirroring Neural Networks",
        "authors": [
            "Dasika Ratna Deepthi",
            "K.Eswaran"
        ],
        "abstract": "  In this paper, we present a new kind of learning implementation to recognize the patterns using the concept of Mirroring Neural Network (MNN) which can extract information from distinct sensory input patterns and perform pattern recognition tasks. It is also capable of being used as an advanced associative memory wherein image data is associated with voice inputs in an unsupervised manner. Since the architecture is hierarchical and modular it has the potential of being used to devise learning engines of ever increasing complexity.\n    ",
        "submission_date": "2008-12-13T00:00:00",
        "last_modified_date": "2008-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.2785",
        "title": "Prediction of Platinum Prices Using Dynamically Weighted Mixture of Experts",
        "authors": [
            "Baruch Lubinsky",
            "Bekir Genc",
            "Tshilidzi Marwala"
        ],
        "abstract": "  Neural networks are powerful tools for classification and regression in static environments. This paper describes a technique for creating an ensemble of neural networks that adapts dynamically to changing conditions. The model separates the input space into four regions and each network is given a weight in each region based on its performance on samples from that region. The ensemble adapts dynamically by constantly adjusting these weights based on the current performance of the networks. The data set used is a collection of financial indicators with the goal of predicting the future platinum price. An ensemble with no weightings does not improve on the naive estimate of no weekly change; our weighting algorithm gives an average percentage error of 63% for twenty weeks of prediction.\n    ",
        "submission_date": "2008-12-15T00:00:00",
        "last_modified_date": "2008-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.2991",
        "title": "Analyse et structuration automatique des guides de bonnes pratiques cliniques : essai d'\u00e9valuation",
        "authors": [
            "Amanda Bouffier",
            "Thierry Poibeau",
            "Catherine Duclos"
        ],
        "abstract": "  Health Practice Guideliens are supposed to unify practices and propose recommendations to physicians. This paper describes GemFrame, a system capable of semi-automatically filling an XML template from free texts in the clinical domain. The XML template includes semantic information not explicitly encoded in the text (pairs of conditions and ac-tions/recommendations). Therefore, there is a need to compute the exact scope of condi-tions over text sequences expressing the re-quired actions. We present a system developped for this task. We show that it yields good performance when applied to the analysis of French practice guidelines. We conclude with a precise evaluation of the tool.\n    ",
        "submission_date": "2008-12-16T00:00:00",
        "last_modified_date": "2008-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.3478",
        "title": "Automatic Construction of Lightweight Domain Ontologies for Chemical Engineering Risk Management",
        "authors": [
            "Wilson Wong",
            "Wei Liu",
            "Saujoe Liaw",
            "Nicoletta Balliu",
            "Hongwei Wu",
            "Moses Tade"
        ],
        "abstract": "  The need for domain ontologies in mission critical applications such as risk management and hazard identification is becoming more and more pressing. Most research on ontology learning conducted in the academia remains unrealistic for real-world applications. One of the main problems is the dependence on non-incremental, rare knowledge and textual resources, and manually-crafted patterns and rules. This paper reports work in progress aiming to address such undesirable dependencies during ontology construction. Initial experiments using a working prototype of the system revealed promising potentials in automatically constructing high-quality domain ontologies using real-world texts.\n    ",
        "submission_date": "2008-12-18T00:00:00",
        "last_modified_date": "2008-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.4360",
        "title": "Driven by Compression Progress: A Simple Principle Explains Essential Aspects of Subjective Beauty, Novelty, Surprise, Interestingness, Attention, Curiosity, Creativity, Art, Science, Music, Jokes",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "abstract": "  I argue that data becomes temporarily interesting by itself to some self-improving, but computationally limited, subjective observer once he learns to predict or compress the data in a better way, thus making it subjectively simpler and more beautiful. Curiosity is the desire to create or discover more non-random, non-arbitrary, regular data that is novel and surprising not in the traditional sense of Boltzmann and Shannon but in the sense that it allows for compression progress because its regularity was not yet known. This drive maximizes interestingness, the first derivative of subjective beauty or compressibility, that is, the steepness of the learning curve. It motivates exploring infants, pure mathematicians, composers, artists, dancers, comedians, yourself, and (since 1990) artificial systems.\n    ",
        "submission_date": "2008-12-23T00:00:00",
        "last_modified_date": "2009-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.4460",
        "title": "Emergence of Spontaneous Order Through Neighborhood Formation in Peer-to-Peer Recommender Systems",
        "authors": [
            "Ernesto Diaz-Aviles",
            "Lars Schmidt-Thieme",
            "Cai-Nicolas Ziegler"
        ],
        "abstract": "  The advent of the Semantic Web necessitates paradigm shifts away from centralized client/server architectures towards decentralization and peer-to-peer computation, making the existence of central authorities superfluous and even impossible. At the same time, recommender systems are gaining considerable impact in e-commerce, providing people with recommendations that are personalized and tailored to their very needs. These recommender systems have traditionally been deployed with stark centralized scenarios in mind, operating in closed communities detached from their host network's outer perimeter. We aim at marrying these two worlds, i.e., decentralized peer-to-peer computing and recommender systems, in one agent-based framework. Our architecture features an epidemic-style protocol maintaining neighborhoods of like-minded peers in a robust, selforganizing fashion. In order to demonstrate our architecture's ability to retain scalability, robustness and to allow for convergence towards high-quality recommendations, we conduct offline experiments on top of the popular MovieLens dataset.\n    ",
        "submission_date": "2008-12-23T00:00:00",
        "last_modified_date": "2008-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.4580",
        "title": "Feature Markov Decision Processes",
        "authors": [
            "Marcus Hutter"
        ],
        "abstract": "  General purpose intelligent learning agents cycle through (complex,non-MDP) sequences of observations, actions, and rewards. On the other hand, reinforcement learning is well-developed for small finite state Markov Decision Processes (MDPs). So far it is an art performed by human designers to extract the right state representation out of the bare observations, i.e. to reduce the agent setup to the MDP framework. Before we can think of mechanizing this search for suitable MDPs, we need a formal objective criterion. The main contribution of this article is to develop such a criterion. I also integrate the various parts into one learning algorithm. Extensions to more realistic dynamic Bayesian networks are developed in a companion article.\n    ",
        "submission_date": "2008-12-25T00:00:00",
        "last_modified_date": "2008-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.4581",
        "title": "Feature Dynamic Bayesian Networks",
        "authors": [
            "Marcus Hutter"
        ],
        "abstract": "  Feature Markov Decision Processes (PhiMDPs) are well-suited for learning agents in general environments. Nevertheless, unstructured (Phi)MDPs are limited to relatively simple environments. Structured MDPs like Dynamic Bayesian Networks (DBNs) are used for large-scale real-world problems. In this article I extend PhiMDP to PhiDBN. The primary contribution is to derive a cost criterion that allows to automatically extract the most relevant features from the environment, leading to the \"best\" DBN representation. I discuss all building blocks required for a complete general learning algorithm.\n    ",
        "submission_date": "2008-12-25T00:00:00",
        "last_modified_date": "2008-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0801.0341",
        "title": "Exactness of Belief Propagation for Some Graphical Models with Loops",
        "authors": [
            "Michael Chertkov"
        ],
        "abstract": "  It is well known that an arbitrary graphical model of statistical inference defined on a tree, i.e. on a graph without loops, is solved exactly and efficiently by an iterative Belief Propagation (BP) algorithm convergent to unique minimum of the so-called Bethe free energy functional. For a general graphical model on a loopy graph the functional may show multiple minima, the iterative BP algorithm may converge to one of the minima or may not converge at all, and the global minimum of the Bethe free energy functional is not guaranteed to correspond to the optimal Maximum-Likelihood (ML) solution in the zero-temperature limit. However, there are exceptions to this general rule, discussed in \\cite{05KW} and \\cite{08BSS} in two different contexts, where zero-temperature version of the BP algorithm finds ML solution for special models on graphs with loops. These two models share a key feature: their ML solutions can be found by an efficient Linear Programming (LP) algorithm with a Totally-Uni-Modular (TUM) matrix of constraints. Generalizing the two models we consider a class of graphical models reducible in the zero temperature limit to LP with TUM constraints. Assuming that a gedanken algorithm, g-BP, funding the global minimum of the Bethe free energy is available we show that in the limit of zero temperature g-BP outputs the ML solution. Our consideration is based on equivalence established between gapless Linear Programming (LP) relaxation of the graphical model in the $T\\to 0$ limit and respective LP version of the Bethe-Free energy minimization.\n    ",
        "submission_date": "2008-01-02T00:00:00",
        "last_modified_date": "2008-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0801.3111",
        "title": "Analysis of Estimation of Distribution Algorithms and Genetic Algorithms on NK Landscapes",
        "authors": [
            "Martin Pelikan"
        ],
        "abstract": "  This study analyzes performance of several genetic and evolutionary algorithms on randomly generated NK fitness landscapes with various values of n and k. A large number of NK problem instances are first generated for each n and k, and the global optimum of each instance is obtained using the branch-and-bound algorithm. Next, the hierarchical Bayesian optimization algorithm (hBOA), the univariate marginal distribution algorithm (UMDA), and the simple genetic algorithm (GA) with uniform and two-point crossover operators are applied to all generated instances. Performance of all algorithms is then analyzed and compared, and the results are discussed.\n    ",
        "submission_date": "2008-01-21T00:00:00",
        "last_modified_date": "2008-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0801.3113",
        "title": "iBOA: The Incremental Bayesian Optimization Algorithm",
        "authors": [
            "Martin Pelikan",
            "Kumara Sastry",
            "David E. Goldberg"
        ],
        "abstract": "  This paper proposes the incremental Bayesian optimization algorithm (iBOA), which modifies standard BOA by removing the population of solutions and using incremental updates of the Bayesian network. iBOA is shown to be able to learn and exploit unrestricted Bayesian networks using incremental techniques for updating both the structure as well as the parameters of the probabilistic model. This represents an important step toward the design of competent incremental estimation of distribution algorithms that can solve difficult nearly decomposable problems scalably and reliably.\n    ",
        "submission_date": "2008-01-21T00:00:00",
        "last_modified_date": "2008-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0801.3147",
        "title": "From k-SAT to k-CSP: Two Generalized Algorithms",
        "authors": [
            "Liang Li",
            "Xin Li",
            "Tian Liu",
            "Ke Xu"
        ],
        "abstract": "  Constraint satisfaction problems (CSPs) models many important intractable NP-hard problems such as propositional satisfiability problem (SAT). Algorithms with non-trivial upper bounds on running time for restricted SAT with bounded clause length k (k-SAT) can be classified into three styles: DPLL-like, PPSZ-like and Local Search, with local search algorithms having already been generalized to CSP with bounded constraint arity k (k-CSP). We generalize a DPLL-like algorithm in its simplest form and a PPSZ-like algorithm from k-SAT to k-CSP. As far as we know, this is the first attempt to use PPSZ-like strategy to solve k-CSP, and before little work has been focused on the DPLL-like or PPSZ-like strategies for k-CSP.\n    ",
        "submission_date": "2008-01-21T00:00:00",
        "last_modified_date": "2008-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0801.3539",
        "title": "On the Effects of Idiotypic Interactions for Recommendation Communities in Artificial Immune Systems",
        "authors": [
            "Steve Cayzer",
            "Uwe Aickelin"
        ],
        "abstract": "  It has previously been shown that a recommender based on immune system idiotypic principles can out perform one based on correlation alone. This paper reports the results of work in progress, where we undertake some investigations into the nature of this beneficial effect. The initial findings are that the immune system recommender tends to produce different neighbourhoods, and that the superior performance of this recommender is due partly to the different neighbourhoods, and partly to the way that the idiotypic effect is used to weight each neighbours recommendations.\n    ",
        "submission_date": "2008-01-23T00:00:00",
        "last_modified_date": "2008-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0801.3547",
        "title": "A Recommender System based on the Immune Network",
        "authors": [
            "Steve Cazyer",
            "Uwe Aickelin"
        ],
        "abstract": "  The immune system is a complex biological system with a highly distributed, adaptive and self-organising nature. This paper presents an artificial immune system (AIS) that exploits some of these characteristics and is applied to the task of film recommendation by collaborative filtering (CF). Natural evolution and in particular the immune system have not been designed for classical optimisation. However, for this problem, we are not interested in finding a single optimum. Rather we intend to identify a sub-set of good matches on which recommendations can be based. It is our hypothesis that an AIS built on two central aspects of the biological immune system will be an ideal candidate to achieve this: Antigen - antibody interaction for matching and antibody - antibody interaction for diversity. Computational results are presented in support of this conjecture and compared to those found by other CF techniques.\n    ",
        "submission_date": "2008-01-23T00:00:00",
        "last_modified_date": "2008-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0801.3549",
        "title": "The Danger Theory and Its Application to Artificial Immune Systems",
        "authors": [
            "Uwe Aickelin",
            "Steve Cayzer"
        ],
        "abstract": "  Over the last decade, a new idea challenging the classical self-non-self viewpoint has become popular amongst immunologists. It is called the Danger Theory. In this conceptual paper, we look at this theory from the perspective of Artificial Immune System practitioners. An overview of the Danger Theory is presented with particular emphasis on analogies in the Artificial Immune Systems world. A number of potential application areas are then used to provide a framing for a critical assessment of the concept, and its relevance for Artificial Immune Systems.\n    ",
        "submission_date": "2008-01-23T00:00:00",
        "last_modified_date": "2008-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0801.3550",
        "title": "Partnering Strategies for Fitness Evaluation in a Pyramidal Evolutionary Algorithm",
        "authors": [
            "Uwe Aickelin",
            "Larry Bull"
        ],
        "abstract": "  This paper combines the idea of a hierarchical distributed genetic algorithm with different inter-agent partnering strategies. Cascading clusters of sub-populations are built from bottom up, with higher-level sub-populations optimising larger parts of the problem. Hence higher-level sub-populations search a larger search space with a lower resolution whilst lower-level sub-populations search a smaller search space with a higher resolution. The effects of different partner selection schemes for (sub-)fitness evaluation purposes are examined for two multiple-choice optimisation problems. It is shown that random partnering strategies perform best by providing better sampling and more diversity.\n    ",
        "submission_date": "2008-01-23T00:00:00",
        "last_modified_date": "2008-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0801.3871",
        "title": "On the Scaling Window of Model RB",
        "authors": [
            "Chunyan Zhao",
            "Ke Xu",
            "Zhiming Zheng"
        ],
        "abstract": "  This paper analyzes the scaling window of a random CSP model (i.e. model RB) for which we can identify the threshold points exactly, denoted by $r_{cr}$ or $p_{cr}$. For this model, we establish the scaling window $W(n,\\delta)=(r_{-}(n,\\delta), r_{+}(n,\\delta))$ such that the probability of a random instance being satisfiable is greater than $1-\\delta$ for $r<r_{-}(n,\\delta)$ and is less than $\\delta$ for $r>r_{+}(n,\\delta)$. Specifically, we obtain the following result $$W(n,\\delta)=(r_{cr}-\\Theta(\\frac{1}{n^{1-\\epsilon}\\ln n}), \\ r_{cr}+\\Theta(\\frac{1}{n\\ln n})),$$ where $0\\leq\\epsilon<1$ is a constant. A similar result with respect to the other parameter $p$ is also obtained. Since the instances generated by model RB have been shown to be hard at the threshold, this is the first attempt, as far as we know, to analyze the scaling window of such a model with hard instances.\n    ",
        "submission_date": "2008-01-25T00:00:00",
        "last_modified_date": "2008-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0801.4287",
        "title": "Movie Recommendation Systems Using An Artificial Immune System",
        "authors": [
            "Qi Chen",
            "Uwe Aickelin"
        ],
        "abstract": "  We apply the Artificial Immune System (AIS) technology to the Collaborative Filtering (CF) technology when we build the movie recommendation system. Two different affinity measure algorithms of AIS, Kendall tau and Weighted Kappa, are used to calculate the correlation coefficients for this movie recommendation system. From the testing we think that Weighted Kappa is more suitable than Kendall tau for movie problems.\n    ",
        "submission_date": "2008-01-28T00:00:00",
        "last_modified_date": "2008-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0801.4307",
        "title": "On Affinity Measures for Artificial Immune System Movie Recommenders",
        "authors": [
            "Uwe Aickelin",
            "Qi Chen"
        ],
        "abstract": "  We combine Artificial Immune Systems 'AIS', technology with Collaborative Filtering 'CF' and use it to build a movie recommendation system. We already know that Artificial Immune Systems work well as movie recommenders from previous work by Cayzer and Aickelin 3, 4, 5. Here our aim is to investigate the effect of different affinity measure algorithms for the AIS. Two different affinity measures, Kendalls Tau and Weighted Kappa, are used to calculate the correlation coefficients for the movie recommender. We compare the results with those published previously and show that Weighted Kappa is more suitable than others for movie problems. We also show that AIS are generally robust movie recommenders and that, as long as a suitable affinity measure is chosen, results are good.\n    ",
        "submission_date": "2008-01-28T00:00:00",
        "last_modified_date": "2008-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0801.4314",
        "title": "Artificial Immune Systems (AIS) - A New Paradigm for Heuristic Decision Making",
        "authors": [
            "Uwe Aickelin"
        ],
        "abstract": "  Over the last few years, more and more heuristic decision making techniques have been inspired by nature, e.g. evolutionary algorithms, ant colony optimisation and simulated annealing. More recently, a novel computational intelligence technique inspired by immunology has emerged, called Artificial Immune Systems (AIS). This immune system inspired technique has already been useful in solving some computational problems. In this keynote, we will very briefly describe the immune system metaphors that are relevant to AIS. We will then give some illustrative real-world problems suitable for AIS use and show a step-by-step algorithm walkthrough. A comparison of AIS to other well-known algorithms and areas for future work will round this keynote off. It should be noted that as AIS is still a young and evolving field, there is not yet a fixed algorithm template and hence actual implementations might differ somewhat from the examples given here.\n    ",
        "submission_date": "2008-01-28T00:00:00",
        "last_modified_date": "2008-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0801.4794",
        "title": "On the Complexity of Binary Samples",
        "authors": [
            "Joel Ratsaby"
        ],
        "abstract": "  Consider a class $\\mH$ of binary functions $h: X\\to\\{-1, +1\\}$ on a finite interval $X=[0, B]\\subset \\Real$. Define the {\\em sample width} of $h$ on a finite subset (a sample) $S\\subset X$ as $\\w_S(h) \\equiv \\min_{x\\in S} |\\w_h(x)|$, where $\\w_h(x) = h(x) \\max\\{a\\geq 0: h(z)=h(x), x-a\\leq z\\leq x+a\\}$. Let $\\mathbb{S}_\\ell$ be the space of all samples in $X$ of cardinality $\\ell$ and consider sets of wide samples, i.e., {\\em hypersets} which are defined as $A_{\\beta, h} = \\{S\\in \\mathbb{S}_\\ell: \\w_{S}(h) \\geq \\beta\\}$. Through an application of the Sauer-Shelah result on the density of sets an upper estimate is obtained on the growth function (or trace) of the class $\\{A_{\\beta, h}: h\\in\\mH\\}$, $\\beta>0$, i.e., on the number of possible dichotomies obtained by intersecting all hypersets with a fixed collection of samples $S\\in\\mathbb{S}_\\ell$ of cardinality $m$. The estimate is $2\\sum_{i=0}^{2\\lfloor B/(2\\beta)\\rfloor}{m-\\ell\\choose i}$.\n    ",
        "submission_date": "2008-01-30T00:00:00",
        "last_modified_date": "2008-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0802.0116",
        "title": "Shallow Models for Non-Iterative Modal Logics",
        "authors": [
            "Lutz Schr\u00f6der",
            "Dirk Patinson"
        ],
        "abstract": "  The methods used to establish PSPACE-bounds for modal logics can roughly be grouped into two classes: syntax driven methods establish that exhaustive proof search can be performed in polynomial space whereas semantic approaches directly construct shallow models. In this paper, we follow the latter approach and establish generic PSPACE-bounds for a large and heterogeneous class of modal logics in a coalgebraic framework. In particular, no complete axiomatisation of the logic under scrutiny is needed. This does not only complement our earlier, syntactic, approach conceptually, but also covers a wide variety of new examples which are difficult to harness by purely syntactic means. Apart from re-proving known complexity bounds for a large variety of structurally different logics, we apply our method to obtain previously unknown PSPACE-bounds for Elgesem's logic of agency and for graded modal logic over reflexive frames.\n    ",
        "submission_date": "2008-02-01T00:00:00",
        "last_modified_date": "2008-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0802.1306",
        "title": "Network as a computer: ranking paths to find flows",
        "authors": [
            "Dusko Pavlovic"
        ],
        "abstract": "  We explore a simple mathematical model of network computation, based on Markov chains. Similar models apply to a broad range of computational phenomena, arising in networks of computers, as well as in genetic, and neural nets, in social networks, and so on. The main problem of interaction with such spontaneously evolving computational systems is that the data are not uniformly structured. An interesting approach is to try to extract the semantical content of the data from their distribution among the nodes. A concept is then identified by finding the community of nodes that share it. The task of data structuring is thus reduced to the task of finding the network communities, as groups of nodes that together perform some non-local data processing. Towards this goal, we extend the ranking methods from nodes to paths. This allows us to extract some information about the likely flow biases from the available static information about the network.\n    ",
        "submission_date": "2008-02-10T00:00:00",
        "last_modified_date": "2008-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0802.1393",
        "title": "Les Agents comme des interpr\u00e9teurs Scheme : Sp\u00e9cification dynamique par la communication",
        "authors": [
            "Cl\u00e9ment Jonquet",
            "Stefano A. Cerri"
        ],
        "abstract": "  We proposed in previous papers an extension and an implementation of the STROBE model, which regards the Agents as Scheme interpreters. These Agents are able to interpret messages in a dedicated environment including an interpreter that learns from the current conversation therefore representing evolving meta-level Agent's knowledge. When the Agent's interpreter is a nondeterministic one, the dialogues may consist of subsequent refinements of specifications in the form of constraint sets. The paper presents a worked out example of dynamic service generation - such as necessary on Grids - by exploiting STROBE Agents equipped with a nondeterministic interpreter. It shows how enabling dynamic specification of a problem. Then it illustrates how these principles could be effective for other applications. Details of the implementation are not provided here, but are available.\n    ",
        "submission_date": "2008-02-11T00:00:00",
        "last_modified_date": "2008-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0802.3235",
        "title": "Characterization of the convergence of stationary Fokker-Planck learning",
        "authors": [
            "Arturo Berrones"
        ],
        "abstract": "  The convergence properties of the stationary Fokker-Planck algorithm for the estimation of the asymptotic density of stochastic search processes is studied. Theoretical and empirical arguments for the characterization of convergence of the estimation in the case of separable and nonseparable nonlinear optimization problems are given. Some implications of the convergence of stationary Fokker-Planck learning for the inference of parameters in artificial neural network models are outlined.\n    ",
        "submission_date": "2008-02-21T00:00:00",
        "last_modified_date": "2009-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0802.3597",
        "title": "Processing Information in Quantum Decision Theory",
        "authors": [
            "V.I. Yukalov",
            "D. Sornette"
        ],
        "abstract": "  A survey is given summarizing the state of the art of describing information processing in Quantum Decision Theory, which has been recently advanced as a novel variant of decision making, based on the mathematical theory of separable Hilbert spaces. This mathematical structure captures the effect of superposition of composite prospects, including many incorporated intended actions. The theory characterizes entangled decision making, non-commutativity of subsequent decisions, and intention interference. The self-consistent procedure of decision making, in the frame of the quantum decision theory, takes into account both the available objective information as well as subjective contextual effects. This quantum approach avoids any paradox typical of classical decision theory. Conditional maximization of entropy, equivalent to the minimization of an information functional, makes it possible to connect the quantum and classical decision theories, showing that the latter is the limit of the former under vanishing interference terms.\n    ",
        "submission_date": "2008-02-25T00:00:00",
        "last_modified_date": "2010-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0802.3789",
        "title": "Knowledge Technologies",
        "authors": [
            "Nick Milton"
        ],
        "abstract": "  Several technologies are emerging that provide new ways to capture, store, present and use knowledge. This book is the first to provide a comprehensive introduction to five of the most important of these technologies: Knowledge Engineering, Knowledge Based Engineering, Knowledge Webs, Ontologies and Semantic Webs. For each of these, answers are given to a number of key questions (What is it? How does it operate? How is a system developed? What can it be used for? What tools are available? What are the main issues?). The book is aimed at students, researchers and practitioners interested in Knowledge Management, Artificial Intelligence, Design Engineering and Web Technologies.\n",
        "submission_date": "2008-02-26T00:00:00",
        "last_modified_date": "2008-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0802.3950",
        "title": "Belief Propagation and Loop Series on Planar Graphs",
        "authors": [
            "Michael Chertkov",
            "Vladimir Y. Chernyak",
            "Razvan Teodorescu"
        ],
        "abstract": "  We discuss a generic model of Bayesian inference with binary variables defined on edges of a planar graph. The Loop Calculus approach of [1, 2] is used to evaluate the resulting series expansion for the partition function. We show that, for planar graphs, truncating the series at single-connected loops reduces, via a map reminiscent of the Fisher transformation [3], to evaluating the partition function of the dimer matching model on an auxiliary planar graph. Thus, the truncated series can be easily re-summed, using the Pfaffian formula of Kasteleyn [4]. This allows to identify a big class of computationally tractable planar models reducible to a dimer model via the Belief Propagation (gauge) transformation. The Pfaffian representation can also be extended to the full Loop Series, in which case the expansion becomes a sum of Pfaffian contributions, each associated with dimer matchings on an extension to a subgraph of the original graph. Algorithmic consequences of the Pfaffian representation, as well as relations to quantum and non-planar models, are discussed.\n    ",
        "submission_date": "2008-02-27T00:00:00",
        "last_modified_date": "2008-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0802.4010",
        "title": "Brain architecture: A design for natural computation",
        "authors": [
            "Marcus Kaiser"
        ],
        "abstract": "  Fifty years ago, John von Neumann compared the architecture of the brain with that of computers that he invented and which is still in use today. In those days, the organisation of computers was based on concepts of brain organisation. Here, we give an update on current results on the global organisation of neural systems. For neural systems, we outline how the spatial and topological architecture of neuronal and cortical networks facilitates robustness against failures, fast processing, and balanced network activation. Finally, we discuss mechanisms of self-organization for such architectures. After all, the organization of the brain might again inspire computer architecture.\n    ",
        "submission_date": "2008-02-27T00:00:00",
        "last_modified_date": "2008-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0802.4326",
        "title": "The Generation of Textual Entailment with NLML in an Intelligent Dialogue system for Language Learning CSIEC",
        "authors": [
            "Jiyou Jia"
        ],
        "abstract": "  This research report introduces the generation of textual entailment within the project CSIEC (Computer Simulation in Educational Communication), an interactive web-based human-computer dialogue system with natural language for English instruction. The generation of textual entailment (GTE) is critical to the further improvement of CSIEC project. Up to now we have found few literatures related with GTE. Simulating the process that a human being learns English as a foreign language we explore our naive approach to tackle the GTE problem and its algorithm within the framework of CSIEC, i.e. rule annotation in NLML, pattern recognition (matching), and entailment transformation. The time and space complexity of our algorithm is tested with some entailment examples. Further works include the rules annotation based on the English textbooks and a GUI interface for normal users to edit the entailment rules.\n    ",
        "submission_date": "2008-02-29T00:00:00",
        "last_modified_date": "2008-02-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.0014",
        "title": "Automated Termination Proofs for Logic Programs by Term Rewriting",
        "authors": [
            "P. Schneider-Kamp",
            "J. Giesl",
            "A. Serebrenik",
            "R. Thiemann"
        ],
        "abstract": "  There are two kinds of approaches for termination analysis of logic programs: \"transformational\" and \"direct\" ones. Direct approaches prove termination directly on the basis of the logic program. Transformational approaches transform a logic program into a term rewrite system (TRS) and then analyze termination of the resulting TRS instead. Thus, transformational approaches make all methods previously developed for TRSs available for logic programs as well. However, the applicability of most existing transformations is quite restricted, as they can only be used for certain subclasses of logic programs. (Most of them are restricted to well-moded programs.) In this paper we improve these transformations such that they become applicable for any definite logic program. To simulate the behavior of logic programs by TRSs, we slightly modify the notion of rewriting by permitting infinite terms. We show that our transformation results in TRSs which are indeed suitable for automated termination analysis. In contrast to most other methods for termination of logic programs, our technique is also sound for logic programming without occur check, which is typically used in practice. We implemented our approach in the termination prover AProVE and successfully evaluated it on a large collection of examples.\n    ",
        "submission_date": "2008-03-02T00:00:00",
        "last_modified_date": "2008-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.1568",
        "title": "Dempster-Shafer for Anomaly Detection",
        "authors": [
            "Qi Chen",
            "Uwe Aickelin"
        ],
        "abstract": "  In this paper, we implement an anomaly detection system using the Dempster-Shafer method. Using two standard benchmark problems we show that by combining multiple signals it is possible to achieve better results than by using a single signal. We further show that by applying this approach to a real-world email dataset the algorithm works for email worm detection. Dempster-Shafer can be a promising method for anomaly detection problems with multiple features (data sources), and two or more classes.\n    ",
        "submission_date": "2008-03-11T00:00:00",
        "last_modified_date": "2008-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.1926",
        "title": "Improved evolutionary generation of XSLT stylesheets",
        "authors": [
            "Pablo Garcia-Sanchez",
            "J. L. J. Laredo",
            "J. P. Sevilla",
            "Pedro Castillo",
            "J. J. Merelo"
        ],
        "abstract": "  This paper introduces a procedure based on genetic programming to evolve XSLT programs (usually called stylesheets or logicsheets). XSLT is a general purpose, document-oriented functional language, generally used to transform XML documents (or, in general, solve any problem that can be coded as an XML document). The proposed solution uses a tree representation for the stylesheets as well as diverse specific operators in order to obtain, in the studied cases and a reasonable time, a XSLT stylesheet that performs the transformation. Several types of representation have been compared, resulting in different performance and degree of success.\n    ",
        "submission_date": "2008-03-13T00:00:00",
        "last_modified_date": "2008-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.1997",
        "title": "Danger Theory: The Link between AIS and IDS?",
        "authors": [
            "Uwe Aickelin",
            "Peter Bentley",
            "Steve Cayzer",
            "Kim Jungwon",
            "Julie McLeod"
        ],
        "abstract": "  We present ideas about creating a next generation Intrusion Detection System based on the latest immunological theories. The central challenge with computer security is determining the difference between normal and potentially harmful activity. For half a century, developers have protected their systems by coding rules that identify and block specific events. However, the nature of current and future threats in conjunction with ever larger IT systems urgently requires the development of automated and adaptive defensive tools. A promising solution is emerging in the form of Artificial Immune Systems. The Human Immune System can detect and defend against harmful and previously unseen invaders, so can we not build a similar Intrusion Detection System for our computers.\n    ",
        "submission_date": "2008-03-13T00:00:00",
        "last_modified_date": "2008-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.2092",
        "title": "An Ant-Based Model for Multiple Sequence Alignment",
        "authors": [
            "Fr\u00e9d\u00e9ric Guinand",
            "Yoann Pign\u00e9"
        ],
        "abstract": "  Multiple sequence alignment is a key process in today's biology, and finding a relevant alignment of several sequences is much more challenging than just optimizing some improbable evaluation functions. Our approach for addressing multiple sequence alignment focuses on the building of structures in a new graph model: the factor graph model. This model relies on block-based formulation of the original problem, formulation that seems to be one of the most suitable ways for capturing evolutionary aspects of alignment. The structures are implicitly built by a colony of ants laying down pheromones in the factor graphs, according to relations between blocks belonging to the different sequences.\n    ",
        "submission_date": "2008-03-14T00:00:00",
        "last_modified_date": "2008-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.2212",
        "title": "Conditioning Probabilistic Databases",
        "authors": [
            "Christoph Koch",
            "Dan Olteanu"
        ],
        "abstract": "  Past research on probabilistic databases has studied the problem of answering queries on a static database. Application scenarios of probabilistic databases however often involve the conditioning of a database using additional information in the form of new evidence. The conditioning problem is thus to transform a probabilistic database of priors into a posterior probabilistic database which is materialized for subsequent query processing or further refinement. It turns out that the conditioning problem is closely related to the problem of computing exact tuple confidence values.\n",
        "submission_date": "2008-03-14T00:00:00",
        "last_modified_date": "2008-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.2306",
        "title": "Tableau-based decision procedures for logics of strategic ability in multi-agent systems",
        "authors": [
            "Valentin Goranko",
            "Dmitry Shkatov"
        ],
        "abstract": "  We develop an incremental tableau-based decision procedures for the\n",
        "submission_date": "2008-03-15T00:00:00",
        "last_modified_date": "2008-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.2965",
        "title": "An Indirect Genetic Algorithm for Set Covering Problems",
        "authors": [
            "Uwe Aickelin"
        ],
        "abstract": "  This paper presents a new type of genetic algorithm for the set covering problem. It differs from previous evolutionary approaches first because it is an indirect algorithm, i.e. the actual solutions are found by an external decoder function. The genetic algorithm itself provides this decoder with permutations of the solution variables and other parameters. Second, it will be shown that results can be further improved by adding another indirect optimisation layer. The decoder will not directly seek out low cost solutions but instead aims for good exploitable solutions. These are then post optimised by another hill-climbing algorithm. Although seemingly more complicated, we will show that this three-stage approach has advantages in terms of solution quality, speed and adaptability to new types of problems over more direct approaches. Extensive computational results are presented and compared to the latest evolutionary and other heuristic approaches to the same data instances.\n    ",
        "submission_date": "2008-03-20T00:00:00",
        "last_modified_date": "2008-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.2966",
        "title": "On the Application of Hierarchical Coevolutionary Genetic Algorithms: Recombination and Evaluation Partners",
        "authors": [
            "Uwe Aickelin",
            "Larry Bull"
        ],
        "abstract": "  This paper examines the use of a hierarchical coevolutionary genetic algorithm under different partnering strategies. Cascading clusters of sub-populations are built from the bottom up, with higher-level sub-populations optimising larger parts of the problem. Hence higher-level sub-populations potentially search a larger search space with a lower resolution whilst lower-level sub-populations search a smaller search space with a higher resolution. The effects of different partner selection schemes amongst the sub-populations on solution quality are examined for two constrained optimisation problems. We examine a number of recombination partnering strategies in the construction of higher-level individuals and a number of related schemes for evaluating sub-solutions. It is shown that partnering strategies that exploit problem-specific knowledge are superior and can counter inappropriate (sub)fitness measurements.\n    ",
        "submission_date": "2008-03-20T00:00:00",
        "last_modified_date": "2008-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.2970",
        "title": "A Recommender System based on Idiotypic Artificial Immune Networks",
        "authors": [
            "Steve Cayzer",
            "Uwe Aickelin"
        ],
        "abstract": "  The immune system is a complex biological system with a highly distributed, adaptive and self-organising nature. This paper presents an Artificial Immune System (AIS) that exploits some of these characteristics and is applied to the task of film recommendation by Collaborative Filtering (CF). Natural evolution and in particular the immune system have not been designed for classical optimisation. However, for this problem, we are not interested in finding a single optimum. Rather we intend to identify a sub-set of good matches on which recommendations can be based. It is our hypothesis that an AIS built on two central aspects of the biological immune system will be an ideal candidate to achieve this: Antigen-antibody interaction for matching and idiotypic antibody-antibody interaction for diversity. Computational results are presented in support of this conjecture and compared to those found by other CF techniques.\n    ",
        "submission_date": "2008-03-20T00:00:00",
        "last_modified_date": "2008-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.2981",
        "title": "Idiotypic Immune Networks in Mobile Robot Control",
        "authors": [
            "Amanda Whitbrook",
            "Uwe Aickelin",
            "Jonathan Garibaldi"
        ],
        "abstract": "  Jerne's idiotypic network theory postulates that the immune response involves inter-antibody stimulation and suppression as well as matching to antigens. The theory has proved the most popular Artificial Immune System (ais) model for incorporation into behavior-based robotics but guidelines for implementing idiotypic selection are scarce. Furthermore, the direct effects of employing the technique have not been demonstrated in the form of a comparison with non-idiotypic systems. This paper aims to address these issues. A method for integrating an idiotypic ais network with a Reinforcement Learning based control system (rl) is described and the mechanisms underlying antibody stimulation and suppression are explained in detail. Some hypotheses that account for the network advantage are put forward and tested using three systems with increasing idiotypic complexity. The basic rl, a simplified hybrid ais-rl that implements idiotypic selection independently of derived concentration levels and a full hybrid ais-rl scheme are examined. The test bed takes the form of a simulated Pioneer robot that is required to navigate through maze worlds detecting and tracking door markers.\n    ",
        "submission_date": "2008-03-20T00:00:00",
        "last_modified_date": "2008-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.3490",
        "title": "Robustness and Regularization of Support Vector Machines",
        "authors": [
            "Huan Xu",
            "Constantine Caramanis",
            "Shie Mannor"
        ],
        "abstract": "  We consider regularized support vector machines (SVMs) and show that they are precisely equivalent to a new robust optimization formulation. We show that this equivalence of robust optimization and regularization has implications for both algorithms, and analysis. In terms of algorithms, the equivalence suggests more general SVM-like algorithms for classification that explicitly build in protection to noise, and at the same time control overfitting. On the analysis front, the equivalence of robustness and regularization, provides a robust optimization interpretation for the success of regularized SVMs. We use the this new robustness interpretation of SVMs to give a new proof of consistency of (kernelized) SVMs, thus establishing robustness as the reason regularized SVMs generalize well.\n    ",
        "submission_date": "2008-03-25T00:00:00",
        "last_modified_date": "2008-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.3539",
        "title": "Reinforcement Learning by Value Gradients",
        "authors": [
            "Michael Fairbank"
        ],
        "abstract": "  The concept of the value-gradient is introduced and developed in the context of reinforcement learning. It is shown that by learning the value-gradients exploration or stochastic behaviour is no longer needed to find locally optimal trajectories. This is the main motivation for using value-gradients, and it is argued that learning value-gradients is the actual objective of any value-function learning algorithm for control problems. It is also argued that learning value-gradients is significantly more efficient than learning just the values, and this argument is supported in experiments by efficiency gains of several orders of magnitude, in several problem domains. Once value-gradients are introduced into learning, several analyses become possible. For example, a surprising equivalence between a value-gradient learning algorithm and a policy-gradient learning algorithm is proven, and this provides a robust convergence proof for control problems using a value function with a general function approximator.\n    ",
        "submission_date": "2008-03-25T00:00:00",
        "last_modified_date": "2008-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0803.3912",
        "title": "Artificial Immune Systems Tutorial",
        "authors": [
            "Uwe Aickelin",
            "Dipankar Dasgupta"
        ],
        "abstract": "  The biological immune system is a robust, complex, adaptive system that defends the body from foreign pathogens. It is able to categorize all cells (or molecules) within the body as self-cells or non-self cells. It does this with the help of a distributed task force that has the intelligence to take action from a local and also a global perspective using its network of chemical messengers for communication. There are two major branches of the immune system. The innate immune system is an unchanging mechanism that detects and destroys certain invading organisms, whilst the adaptive immune system responds to previously unknown foreign cells and builds a response to them that can remain in the body over a long period of time. This remarkable information processing biological system has caught the attention of computer science in recent years. A novel computational intelligence technique, inspired by immunology, has emerged, called Artificial Immune Systems. Several concepts from the immune have been extracted and applied for solution to real world science and engineering problems. In this tutorial, we briefly describe the immune system metaphors that are relevant to existing Artificial Immune Systems methods. We will then show illustrative real-world problems suitable for Artificial Immune Systems and give a step-by-step algorithm walkthrough for one such problem. A comparison of the Artificial Immune Systems to other well-known algorithms, areas for future work, tips & tricks and a list of resources will round this tutorial off. It should be noted that as Artificial Immune Systems is still a young and evolving field, there is not yet a fixed algorithm template and hence actual implementations might differ somewhat from time to time and from those examples given here.\n    ",
        "submission_date": "2008-03-27T00:00:00",
        "last_modified_date": "2008-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.0066",
        "title": "Binary Decision Diagrams for Affine Approximation",
        "authors": [
            "Kevin Henshall",
            "Peter Schachte",
            "Harald S\u00f8ndergaard",
            "Leigh Whiting"
        ],
        "abstract": "  Selman and Kautz's work on ``knowledge compilation'' established how approximation (strengthening and/or weakening) of a propositional knowledge-base can be used to speed up query processing, at the expense of completeness. In this classical approach, querying uses Horn over- and under-approximations of a given knowledge-base, which is represented as a propositional formula in conjunctive normal form (CNF). Along with the class of Horn functions, one could imagine other Boolean function classes that might serve the same purpose, owing to attractive deduction-computational properties similar to those of the Horn functions. Indeed, Zanuttini has suggested that the class of affine Boolean functions could be useful in knowledge compilation and has presented an affine approximation algorithm. Since CNF is awkward for presenting affine functions, Zanuttini considers both a sets-of-models representation and the use of modulo 2 congruence equations. In this paper, we propose an algorithm based on reduced ordered binary decision diagrams (ROBDDs). This leads to a representation which is more compact than the sets of models and, once we have established some useful properties of affine Boolean functions, a more efficient algorithm.\n    ",
        "submission_date": "2008-04-01T00:00:00",
        "last_modified_date": "2008-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.0188",
        "title": "Support Vector Machine Classification with Indefinite Kernels",
        "authors": [
            "Ronny Luss",
            "Alexandre d'Aspremont"
        ],
        "abstract": "  We propose a method for support vector machine classification using indefinite kernels. Instead of directly minimizing or stabilizing a nonconvex loss function, our algorithm simultaneously computes support vectors and a proxy kernel matrix used in forming the loss. This can be interpreted as a penalized kernel learning problem where indefinite kernel matrices are treated as a noisy observations of a true Mercer kernel. Our formulation keeps the problem convex and relatively large problems can be solved efficiently using the projected gradient or analytic center cutting plane methods. We compare the performance of our technique with other methods on several classic data sets.\n    ",
        "submission_date": "2008-04-01T00:00:00",
        "last_modified_date": "2009-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.0352",
        "title": "Permeability Analysis based on information granulation theory",
        "authors": [
            "M.Sharifzadeh",
            "H.Owladeghaffari",
            "K.Shahriar",
            "E.Bakhtavar"
        ],
        "abstract": "  This paper describes application of information granulation theory, on the analysis of \"lugeon data\". In this manner, using a combining of Self Organizing Map (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and fuzzy granules are obtained. Balancing of crisp granules and sub- fuzzy granules, within non fuzzy information (initial granulation), is rendered in open-close iteration. Using two criteria, \"simplicity of rules \"and \"suitable adaptive threshold error level\", stability of algorithm is guaranteed. In other part of paper, rough set theory (RST), to approximate analysis, has been employed >.Validation of the proposed methods, on the large data set of in-situ permeability in rock masses, in the Shivashan dam, Iran, has been highlighted. By the implementation of the proposed algorithm on the lugeon data set, was proved the suggested method, relating the approximate analysis on the permeability, could be applied.\n    ",
        "submission_date": "2008-04-02T00:00:00",
        "last_modified_date": "2008-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.0353",
        "title": "Graphical Estimation of Permeability Using RST&NFIS",
        "authors": [
            "H.Owladeghaffari",
            "K.Shahriar W. Pedrycz"
        ],
        "abstract": "  This paper pursues some applications of Rough Set Theory (RST) and neural-fuzzy model to analysis of \"lugeon data\". In the manner, using Self Organizing Map (SOM) as a pre-processing the data are scaled and then the dominant rules by RST, are elicited. Based on these rules variations of permeability in the different levels of Shivashan dam, Iran has been highlighted. Then, via using a combining of SOM and an adaptive Neuro-Fuzzy Inference System (NFIS) another analysis on the data was carried out. Finally, a brief comparison between the obtained results of RST and SOM-NFIS (briefly SONFIS) has been rendered.\n    ",
        "submission_date": "2008-04-02T00:00:00",
        "last_modified_date": "2008-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.0573",
        "title": "An Artificial Immune System as a Recommender System for Web Sites",
        "authors": [
            "Tom Morrison",
            "Uwe Aickelin"
        ],
        "abstract": "  Artificial Immune Systems have been used successfully to build recommender systems for film databases. In this research, an attempt is made to extend this idea to web site recommendation. A collection of more than 1000 individuals web profiles (alternatively called preferences / favourites / bookmarks file) will be used. URLs will be classified using the DMOZ (Directory Mozilla) database of the Open Directory Project as our ontology. This will then be used as the data for the Artificial Immune Systems rather than the actual addresses. The first attempt will involve using a simple classification code number coupled with the number of pages within that classification code. However, this implementation does not make use of the hierarchical tree-like structure of DMOZ. Consideration will then be given to the construction of a similarity measure for web profiles that makes use of this hierarchical information to build a better-informed Artificial Immune System.\n    ",
        "submission_date": "2008-04-03T00:00:00",
        "last_modified_date": "2008-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.0580",
        "title": "Explicit Learning: an Effort towards Human Scheduling Algorithms",
        "authors": [
            "Jingpeng Li",
            "Uwe Aickelin"
        ],
        "abstract": "  Scheduling problems are generally NP-hard combinatorial problems, and a lot of research has been done to solve these problems heuristically. However, most of the previous approaches are problem-specific and research into the development of a general scheduling algorithm is still in its infancy.\n",
        "submission_date": "2008-04-03T00:00:00",
        "last_modified_date": "2008-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.0924",
        "title": "A Unified Semi-Supervised Dimensionality Reduction Framework for Manifold Learning",
        "authors": [
            "Ratthachat Chatpatanasiri",
            "Boonserm Kijsirikul"
        ],
        "abstract": "  We present a general framework of semi-supervised dimensionality reduction for manifold learning which naturally generalizes existing supervised and unsupervised learning frameworks which apply the spectral decomposition. Algorithms derived under our framework are able to employ both labeled and unlabeled examples and are able to handle complex problems where data form separate clusters of manifolds. Our framework offers simple views, explains relationships among existing frameworks and provides further extensions which can improve existing algorithms. Furthermore, a new semi-supervised kernelization framework called ``KPCA trick'' is proposed to handle non-linear problems.\n    ",
        "submission_date": "2008-04-06T00:00:00",
        "last_modified_date": "2009-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.1421",
        "title": "A $O(\\log m)$, deterministic, polynomial-time computable approximation of Lewis Carroll's scoring rule",
        "authors": [
            "Jason Covey",
            "Christopher Homan"
        ],
        "abstract": "  We provide deterministic, polynomial-time computable voting rules that approximate Dodgson's and (the ``minimization version'' of) Young's scoring rules to within a logarithmic factor. Our approximation of Dodgson's rule is tight up to a constant factor, as Dodgson's rule is $\\NP$-hard to approximate to within some logarithmic factor. The ``maximization version'' of Young's rule is known to be $\\NP$-hard to approximate by any constant factor. Both approximations are simple, and natural as rules in their own right: Given a candidate we wish to score, we can regard either its Dodgson or Young score as the edit distance between a given set of voter preferences and one in which the candidate to be scored is the Condorcet winner. (The difference between the two scoring rules is the type of edits allowed.) We regard the marginal cost of a sequence of edits to be the number of edits divided by the number of reductions (in the candidate's deficit against any of its opponents in the pairwise race against that opponent) that the edits yield. Over a series of rounds, our scoring rules greedily choose a sequence of edits that modify exactly one voter's preferences and whose marginal cost is no greater than any other such single-vote-modifying sequence.\n    ",
        "submission_date": "2008-04-09T00:00:00",
        "last_modified_date": "2008-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.1441",
        "title": "On Kernelization of Supervised Mahalanobis Distance Learners",
        "authors": [
            "Ratthachat Chatpatanasiri",
            "Teesid Korsrilabutr",
            "Pasakorn Tangchanachaianan",
            "Boonserm Kijsirikul"
        ],
        "abstract": "  This paper focuses on the problem of kernelizing an existing supervised Mahalanobis distance learner. The following features are included in the paper. Firstly, three popular learners, namely, \"neighborhood component analysis\", \"large margin nearest neighbors\" and \"discriminant neighborhood embedding\", which do not have kernel versions are kernelized in order to improve their classification performances. Secondly, an alternative kernelization framework called \"KPCA trick\" is presented. Implementing a learner in the new framework gains several advantages over the standard framework, e.g. no mathematical formulas and no reprogramming are required for a kernel implementation, the framework avoids troublesome problems such as singularity, etc. Thirdly, while the truths of representer theorems are just assumptions in previous papers related to ours, here, representer theorems are formally proven. The proofs validate both the kernel trick and the KPCA trick in the context of Mahalanobis distance learning. Fourthly, unlike previous works which always apply brute force methods to select a kernel, we investigate two approaches which can be efficiently adopted to construct an appropriate kernel for a given dataset. Finally, numerical results on various real-world datasets are presented.\n    ",
        "submission_date": "2008-04-09T00:00:00",
        "last_modified_date": "2009-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.1762",
        "title": "The Choquet integral for the aggregation of interval scales in multicriteria decision making",
        "authors": [
            "Christophe Labreuche",
            "Michel Grabisch"
        ],
        "abstract": "  This paper addresses the question of which models fit with information concerning the preferences of the decision maker over each attribute, and his preferences about aggregation of criteria (interacting criteria). We show that the conditions induced by these information plus some intuitive conditions lead to a unique possible aggregation operator: the Choquet integral.\n    ",
        "submission_date": "2008-04-10T00:00:00",
        "last_modified_date": "2008-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.2036",
        "title": "Towards Physarum robots: computing and manipulating on water surface",
        "authors": [
            "Andrew Adamatzky"
        ],
        "abstract": "  Plasmodium of Physarym polycephalum is an ideal biological substrate for implementing concurrent and parallel computation, including combinatorial geometry and optimization on graphs. We report results of scoping experiments on Physarum computing in conditions of minimal friction, on the water surface. We show that plasmodium of Physarum is capable for computing a basic spanning trees and manipulating of light-weight objects. We speculate that our results pave the pathways towards design and implementation of amorphous biological robots.\n    ",
        "submission_date": "2008-04-13T00:00:00",
        "last_modified_date": "2008-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.2155",
        "title": "From Qualitative to Quantitative Proofs of Security Properties Using First-Order Conditional Logic",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "abstract": "  A first-order conditional logic is considered, with semantics given by a variant of epsilon-semantics, where p -> q means that Pr(q | p) approaches 1 super-polynomially --faster than any inverse polynomial. This type of convergence is needed for reasoning about security protocols. A complete axiomatization is provided for this semantics, and it is shown how a qualitative proof of the correctness of a security protocol can be automatically converted to a quantitative proof appropriate for reasoning about concrete security.\n    ",
        "submission_date": "2008-04-14T00:00:00",
        "last_modified_date": "2008-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.2844",
        "title": "An Analysis of Key Factors for the Success of the Communal Management of Knowledge",
        "authors": [
            "Isabelle Bourdon",
            "Chris Kimble"
        ],
        "abstract": "  This paper explores the links between Knowledge Management and new community-based models of the organization from both a theoretical and an empirical perspective. From a theoretical standpoint, we look at Communities of Practice (CoPs) and Knowledge Management (KM) and explore the links between the two as they relate to the use of information systems to manage knowledge. We begin by reviewing technologically supported approaches to KM and introduce the idea of \"Systemes d'Aide a la Gestion des Connaissances\" SAGC (Systems to aid the Management of Knowledge). Following this we examine the contribution that communal structures such as CoPs can make to intraorganizational KM and highlight some of 'success factors' for this approach to KM that are found in the literature. From an empirical standpoint, we present the results of a survey involving the Chief Knowledge Officers (CKOs) of twelve large French businesses; the objective of this study was to identify the factors that might influence the success of such approaches. The survey was analysed using thematic content analysis and the results are presented here with some short illustrative quotes from the CKOs. Finally, the paper concludes with some brief reflections on what can be learnt from looking at this problem from these two perspectives.\n    ",
        "submission_date": "2008-04-17T00:00:00",
        "last_modified_date": "2008-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.3160",
        "title": "On the performance of approximate equilibria in congestion games",
        "authors": [
            "George Christodoulou",
            "Elias Koutsoupias",
            "Paul Spirakis"
        ],
        "abstract": "  We study the performance of approximate Nash equilibria for linear congestion games. We consider how much the price of anarchy worsens and how much the price of stability improves as a function of the approximation factor $\\epsilon$. We give (almost) tight upper and lower bounds for both the price of anarchy and the price of stability for atomic and non-atomic congestion games. Our results not only encompass and generalize the existing results of exact equilibria to $\\epsilon$-Nash equilibria, but they also provide a unified approach which reveals the common threads of the atomic and non-atomic price of anarchy results. By expanding the spectrum, we also cast the existing results in a new light. For example, the Pigou network, which gives tight results for exact Nash equilibria of selfish routing, remains tight for the price of stability of $\\epsilon$-Nash equilibria but not for the price of anarchy.\n    ",
        "submission_date": "2008-04-21T00:00:00",
        "last_modified_date": "2008-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.4073",
        "title": "Grainy Numbers",
        "authors": [
            "Gilles Champenois"
        ],
        "abstract": "  Grainy numbers are defined as tuples of bits. They form a lattice where the meet and the join operations are an addition and a multiplication. They may be substituted for the real numbers in the definition of fuzzy sets. The aim is to propose an alternative negation for the complement that we'll call supplement.\n    ",
        "submission_date": "2008-04-25T00:00:00",
        "last_modified_date": "2009-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0804.4885",
        "title": "SimDialog: A visual game dialog editor",
        "authors": [
            "C. Owen",
            "F. Biocca",
            "C. Bohil",
            "J. Conley"
        ],
        "abstract": "  SimDialog is a visual editor for dialog in computer games. This paper presents the design of SimDialog, illustrating how script writers and non-programmers can easily create dialog for video games with complex branching structures and dynamic response characteristics. The system creates dialog as a directed graph. This allows for play using the dialog with a state-based cause and effect system that controls selection of non-player character responses and can provide a basic scoring mechanism for games.\n    ",
        "submission_date": "2008-04-30T00:00:00",
        "last_modified_date": "2008-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.1153",
        "title": "Contact state analysis using NFIS and SOM",
        "authors": [
            "H.Owladeghaffari"
        ],
        "abstract": "  This paper reports application of neuro- fuzzy inference system (NFIS) and self organizing feature map neural networks (SOM) on detection of contact state in a block system. In this manner, on a simple system, the evolution of contact states, by parallelization of DDA, has been investigated. So, a comparison between NFIS and SOM results has been presented. The results show applicability of the proposed methods, by different accuracy, on detection of contact's distribution.\n    ",
        "submission_date": "2008-05-08T00:00:00",
        "last_modified_date": "2008-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.1785",
        "title": "Distributed Self Management for Distributed Security Systems",
        "authors": [
            "Michael Hilker"
        ],
        "abstract": "  Distributed system as e.g. artificial immune systems, complex adaptive systems, or multi-agent systems are widely used in Computer Science, e.g. for network security, optimisations, or simulations. In these systems, small entities move through the network and perform certain tasks. At some time, the entities move to another place and require therefore information where to move is most profitable. Common used systems do not provide any information or use a centralised approach where a center delegates the entities. This article discusses whether small information about the neighbours enhances the performance of the overall system or not. Therefore, two information-protocols are introduced and analysed. In addition, the protocols are implemented and tested using the artificial immune system SANA that protects a network against intrusions.\n    ",
        "submission_date": "2008-05-13T00:00:00",
        "last_modified_date": "2008-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.1786",
        "title": "Next Challenges in Bringing Artificial Immune Systems to Production in Network Security",
        "authors": [
            "Michael Hilker"
        ],
        "abstract": "  The human immune system protects the human body against various pathogens like e.g. biological viruses and bacteria. Artificial immune systems reuse the architecture, organization, and workflows of the human immune system for various problems in computer science. In the network security, the artificial immune system is used to secure a network and its nodes against intrusions like viruses, worms, and trojans. However, these approaches are far away from production where they are academic proof-of-concept implementations or use only a small part to protect against a certain intrusion. This article discusses the required steps to bring artificial immune systems into production in the network security domain. It furthermore figures out the challenges and provides the description and results of the prototype of an artificial immune system, which is SANA called.\n    ",
        "submission_date": "2008-05-13T00:00:00",
        "last_modified_date": "2008-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.2027",
        "title": "Rollout Sampling Approximate Policy Iteration",
        "authors": [
            "Christos Dimitrakakis",
            "Michail G. Lagoudakis"
        ],
        "abstract": "  Several researchers have recently investigated the connection between reinforcement learning and classification. We are motivated by proposals of approximate policy iteration schemes without value functions which focus on policy representation using classifiers and address policy learning as a supervised learning problem. This paper proposes variants of an improved policy iteration scheme which addresses the core sampling problem in evaluating a policy through simulation as a multi-armed bandit machine. The resulting algorithm offers comparable performance to the previous algorithm achieved, however, with significantly less computational effort. An order of magnitude improvement is demonstrated experimentally in two standard reinforcement learning domains: inverted pendulum and mountain-car.\n    ",
        "submission_date": "2008-05-14T00:00:00",
        "last_modified_date": "2008-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.2368",
        "title": "A Kernel Method for the Two-Sample Problem",
        "authors": [
            "Arthur Gretton",
            "Karsten Borgwardt",
            "Malte J. Rasch",
            "Bernhard Scholkopf",
            "Alexander J. Smola"
        ],
        "abstract": "  We propose a framework for analyzing and comparing distributions, allowing us to design statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS). We present two tests based on large deviation bounds for the test statistic, while a third is based on the asymptotic distribution of this statistic. The test statistic can be computed in quadratic time, although efficient linear time approximations are available. Several classical metrics on distributions are recovered when the function space used to compute the difference in expectations is allowed to be more general (eg. a Banach space). We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.\n    ",
        "submission_date": "2008-05-15T00:00:00",
        "last_modified_date": "2008-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.2891",
        "title": "Learning Low-Density Separators",
        "authors": [
            "Shai Ben-David",
            "Tyler Lu",
            "David Pal",
            "Miroslava Sotakova"
        ],
        "abstract": "  We define a novel, basic, unsupervised learning problem - learning the lowest density homogeneous hyperplane separator of an unknown probability distribution. This task is relevant to several problems in machine learning, such as semi-supervised learning and clustering stability. We investigate the question of existence of a universally consistent algorithm for this problem. We propose two natural learning paradigms and prove that, on input unlabeled random samples generated by any member of a rich family of distributions, they are guaranteed to converge to the optimal separator for that distribution. We complement this result by showing that no learning algorithm for our task can achieve uniform learning rates (that are independent of the data generating distribution).\n    ",
        "submission_date": "2008-05-19T00:00:00",
        "last_modified_date": "2009-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.3521",
        "title": "Towards applied theories based on computability logic",
        "authors": [
            "Giorgi Japaridze"
        ],
        "abstract": "  Computability logic (CL) (see ",
        "submission_date": "2008-05-22T00:00:00",
        "last_modified_date": "2009-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0805.4508",
        "title": "Modeling Loosely Annotated Images with Imagined Annotations",
        "authors": [
            "Hong Tang",
            "Nozha Boujemma",
            "Yunhao Chen"
        ],
        "abstract": "  In this paper, we present an approach to learning latent semantic analysis models from loosely annotated images for automatic image annotation and indexing. The given annotation in training images is loose due to: (1) ambiguous correspondences between visual features and annotated keywords; (2) incomplete lists of annotated keywords. The second reason motivates us to enrich the incomplete annotation in a simple way before learning topic models. In particular, some imagined keywords are poured into the incomplete annotation through measuring similarity between keywords. Then, both given and imagined annotations are used to learning probabilistic topic models for automatically annotating new images. We conduct experiments on a typical Corel dataset of images and loose annotations, and compare the proposed method with state-of-the-art discrete annotation methods (using a set of discrete blobs to represent an image). The proposed method improves word-driven probability Latent Semantic Analysis (PLSA-words) up to a comparable performance with the best discrete annotation method, while a merit of PLSA-words is still kept, i.e., a wider semantic range.\n    ",
        "submission_date": "2008-05-29T00:00:00",
        "last_modified_date": "2008-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.1199",
        "title": "Belief Propagation and Beyond for Particle Tracking",
        "authors": [
            "Michael Chertkov",
            "Lukas Kroc",
            "Massimo Vergassola"
        ],
        "abstract": "  We describe a novel approach to statistical learning from particles tracked while moving in a random environment. The problem consists in inferring properties of the environment from recorded snapshots. We consider here the case of a fluid seeded with identical passive particles that diffuse and are advected by a flow. Our approach rests on efficient algorithms to estimate the weighted number of possible matchings among particles in two consecutive snapshots, the partition function of the underlying graphical model. The partition function is then maximized over the model parameters, namely diffusivity and velocity gradient. A Belief Propagation (BP) scheme is the backbone of our algorithm, providing accurate results for the flow parameters we want to learn. The BP estimate is additionally improved by incorporating Loop Series (LS) contributions. For the weighted matching problem, LS is compactly expressed as a Cauchy integral, accurately estimated by a saddle point approximation. Numerical experiments show that the quality of our improved BP algorithm is comparable to the one of a fully polynomial randomized approximation scheme, based on the Markov Chain Monte Carlo (MCMC) method, while the BP-based scheme is substantially faster than the MCMC scheme.\n    ",
        "submission_date": "2008-06-06T00:00:00",
        "last_modified_date": "2008-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.1343",
        "title": "Temporized Equilibria",
        "authors": [
            "Riccardo Alberti"
        ],
        "abstract": "  This paper has been withdrawn by the author due to a crucial error in the submission action.\n    ",
        "submission_date": "2008-06-08T00:00:00",
        "last_modified_date": "2008-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.1636",
        "title": "Data-Complexity of the Two-Variable Fragment with Counting Quantifiers",
        "authors": [
            "Ian Pratt-Hartmann"
        ],
        "abstract": "  The data-complexity of both satisfiability and finite satisfiability for the two-variable fragment with counting is NP-complete; the data-complexity of both query-answering and finite query-answering for the two-variable guarded fragment with counting is co-NP-complete.\n    ",
        "submission_date": "2008-06-10T00:00:00",
        "last_modified_date": "2008-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.1796",
        "title": "Evaluation for Uncertain Image Classification and Segmentation",
        "authors": [
            "Arnaud Martin",
            "Hicham Laanaya",
            "Andreas Arnold-Bos"
        ],
        "abstract": "  Each year, numerous segmentation and classification algorithms are invented or reused to solve problems where machine vision is needed. Generally, the efficiency of these algorithms is compared against the results given by one or many human experts. However, in many situations, the location of the real boundaries of the objects as well as their classes are not known with certainty by the human experts. Furthermore, only one aspect of the segmentation and classification problem is generally evaluated. In this paper we present a new evaluation method for classification and segmentation of image, where we take into account both the classification and segmentation results as well as the level of certainty given by the experts. As a concrete example of our method, we evaluate an automatic seabed characterization algorithm based on sonar images.\n    ",
        "submission_date": "2008-06-11T00:00:00",
        "last_modified_date": "2008-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.1798",
        "title": "Human expert fusion for image classification",
        "authors": [
            "Arnaud Martin",
            "Christophe Osswald"
        ],
        "abstract": "  In image classification, merging the opinion of several human experts is very important for different tasks such as the evaluation or the training. Indeed, the ground truth is rarely known before the scene imaging. We propose here different models in order to fuse the informations given by two or more experts. The considered unit for the classification, a small tile of the image, can contain one or more kind of the considered classes given by the experts. A second problem that we have to take into account, is the amount of certainty of the expert has for each pixel of the tile. In order to solve these problems we define five models in the context of the Dempster-Shafer Theory and in the context of the Dezert-Smarandache Theory and we study the possible decisions with these models.\n    ",
        "submission_date": "2008-06-11T00:00:00",
        "last_modified_date": "2008-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.2006",
        "title": "Fusion de classifieurs pour la classification d'images sonar",
        "authors": [
            "Arnaud Martin"
        ],
        "abstract": "In this paper, we present some high level information fusion approaches for numeric and symbolic data. We study the interest of such method particularly for classifier fusion. A comparative study is made in a context of sea bed characterization from sonar images. The classi- fication of kind of sediment is a difficult problem because of the data complexity. We compare high level information fusion and give the obtained performance.\n    ",
        "submission_date": "2008-06-12T00:00:00",
        "last_modified_date": "2012-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.2007",
        "title": "Experts Fusion and Multilayer Perceptron Based on Belief Learning for Sonar Image Classification",
        "authors": [
            "Arnaud Martin",
            "Christophe Osswald"
        ],
        "abstract": "  The sonar images provide a rapid view of the seabed in order to characterize it. However, in such as uncertain environment, real seabed is unknown and the only information we can obtain, is the interpretation of different human experts, sometimes in conflict. In this paper, we propose to manage this conflict in order to provide a robust reality for the learning step of classification algorithms. The classification is conducted by a multilayer perceptron, taking into account the uncertainty of the reality in the learning stage. The results of this seabed characterization are presented on real sonar images.\n    ",
        "submission_date": "2008-06-12T00:00:00",
        "last_modified_date": "2008-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.2008",
        "title": "Generalized proportional conflict redistribution rule applied to Sonar imagery and Radar targets classification",
        "authors": [
            "Arnaud Martin",
            "Christophe Osswald"
        ],
        "abstract": "  In this chapter, we present two applications in information fusion in order to evaluate the generalized proportional conflict redistribution rule presented in the chapter \\cite{Martin06a}. Most of the time the combination rules are evaluated only on simple examples. We study here different combination rules and compare them in terms of decision on real data. Indeed, in real applications, we need a reliable decision and it is the final results that matter. Two applications are presented here: a fusion of human experts opinions on the kind of underwater sediments depict on sonar image and a classifier fusion for radar targets recognition.\n    ",
        "submission_date": "2008-06-12T00:00:00",
        "last_modified_date": "2008-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.2139",
        "title": "Beyond Nash Equilibrium: Solution Concepts for the 21st Century",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "abstract": "  Nash equilibrium is the most commonly-used notion of equilibrium in game theory. However, it suffers from numerous problems. Some are well known in the game theory community; for example, the Nash equilibrium of repeated prisoner's dilemma is neither normatively nor descriptively reasonable. However, new problems arise when considering Nash equilibrium from a computer science perspective: for example, Nash equilibrium is not robust (it does not tolerate ``faulty'' or ``unexpected'' behavior), it does not deal with coalitions, it does not take computation cost into account, and it does not deal with cases where players are not aware of all aspects of the game. Solution concepts that try to address these shortcomings of Nash equilibrium are discussed.\n    ",
        "submission_date": "2008-06-12T00:00:00",
        "last_modified_date": "2008-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.3949",
        "title": "Use of a Quantum Computer and the Quick Medical Reference To Give an Approximate Diagnosis",
        "authors": [
            "Robert R. Tucci"
        ],
        "abstract": "  The Quick Medical Reference (QMR) is a compendium of statistical knowledge connecting diseases to findings (symptoms). The information in QMR can be represented as a Bayesian network. The inference problem (or, in more medical language, giving a diagnosis) for the QMR is to, given some findings, find the probability of each disease. Rejection sampling and likelihood weighted sampling (a.k.a. likelihood weighting) are two simple algorithms for making approximate inferences from an arbitrary Bayesian net (and from the QMR Bayesian net in particular). Heretofore, the samples for these two algorithms have been obtained with a conventional \"classical computer\". In this paper, we will show that two analogous algorithms exist for the QMR Bayesian net, where the samples are obtained with a quantum computer. We expect that these two algorithms, implemented on a quantum computer, can also be used to make inferences (and predictions) with other Bayesian nets.\n    ",
        "submission_date": "2008-06-24T00:00:00",
        "last_modified_date": "2008-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.4391",
        "title": "Prediction with Expert Advice in Games with Unbounded One-Step Gains",
        "authors": [
            "Vladimir V. V'yugin"
        ],
        "abstract": "  The games of prediction with expert advice are considered in this paper. We present some modification of Kalai and Vempala algorithm of following the perturbed leader for the case of unrestrictedly large one-step gains. We show that in general case the cumulative gain of any probabilistic prediction algorithm can be much worse than the gain of some expert of the pool. Nevertheless, we give the lower bound for this cumulative gain in general case and construct a universal algorithm which has the optimal performance; we also prove that in case when one-step gains of experts of the pool have ``limited deviations'' the performance of our algorithm is close to the performance of the best expert.\n    ",
        "submission_date": "2008-06-26T00:00:00",
        "last_modified_date": "2008-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.4484",
        "title": "On empirical meaning of randomness with respect to a real parameter",
        "authors": [
            "Vladimir V'yugin"
        ],
        "abstract": "  We study the empirical meaning of randomness with respect to a family of probability distributions $P_\\theta$, where $\\theta$ is a real parameter, using algorithmic randomness theory. In the case when for a computable probability distribution $P_\\theta$ an effectively strongly consistent estimate exists, we show that the Levin's a priory semicomputable semimeasure of the set of all $P_\\theta$-random sequences is positive if and only if the parameter $\\theta$ is a computable real number. The different methods for generating ``meaningful'' $P_\\theta$-random sequences with noncomputable $\\theta$ are discussed.\n    ",
        "submission_date": "2008-06-27T00:00:00",
        "last_modified_date": "2009-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.4652",
        "title": "A Fixed-Parameter Algorithm for Random Instances of Weighted d-CNF Satisfiability",
        "authors": [
            "Yong Gao"
        ],
        "abstract": "  We study random instances of the weighted $d$-CNF satisfiability problem (WEIGHTED $d$-SAT), a generic W[1]-complete problem. A random instance of the problem consists of a fixed parameter $k$ and a random $d$-CNF formula $\\weicnf{n}{p}{k, d}$ generated as follows: for each subset of $d$ variables and with probability $p$, a clause over the $d$ variables is selected uniformly at random from among the $2^d - 1$ clauses that contain at least one negated literals.\n",
        "submission_date": "2008-06-28T00:00:00",
        "last_modified_date": "2008-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.4686",
        "title": "Sparse Online Learning via Truncated Gradient",
        "authors": [
            "John Langford",
            "Lihong Li",
            "Tong Zhang"
        ],
        "abstract": "  We propose a general method called truncated gradient to induce sparsity in the weights of online learning algorithms with convex loss functions. This method has several essential properties: The degree of sparsity is continuous -- a parameter controls the rate of sparsification from no sparsification to total sparsification. The approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular $L_1$-regularization method in the batch setting. We prove that small rates of sparsification result in only small additional regret with respect to typical online learning guarantees. The approach works well empirically. We apply the approach to several datasets and find that for datasets with large numbers of features, substantial sparsity is discoverable.\n    ",
        "submission_date": "2008-06-28T00:00:00",
        "last_modified_date": "2008-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0806.4802",
        "title": "A new Hedging algorithm and its application to inferring latent random variables",
        "authors": [
            "Yoav Freund",
            "Daniel Hsu"
        ],
        "abstract": "  We present a new online learning algorithm for cumulative discounted gain. This learning algorithm does not use exponential weights on the experts. Instead, it uses a weighting scheme that depends on the regret of the master algorithm relative to the experts. In particular, experts whose discounted cumulative gain is smaller (worse) than that of the master algorithm receive zero weight. We also sketch how a regret-based algorithm can be used as an alternative to Bayesian averaging in the context of inferring latent random variables.\n    ",
        "submission_date": "2008-06-30T00:00:00",
        "last_modified_date": "2008-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0807.1997",
        "title": "Multi-Instance Learning by Treating Instances As Non-I.I.D. Samples",
        "authors": [
            "Zhi-Hua Zhou",
            "Yu-Yin Sun",
            "Yu-Feng Li"
        ],
        "abstract": "  Multi-instance learning attempts to learn from a training set consisting of labeled bags each containing many unlabeled instances. Previous studies typically treat the instances in the bags as independently and identically distributed. However, the instances in a bag are rarely independent, and therefore a better performance can be expected if the instances are treated in an non-i.i.d. way that exploits the relations among instances. In this paper, we propose a simple yet effective multi-instance learning method, which regards each bag as a graph and uses a specific kernel to distinguish the graphs by considering the features of the nodes as well as the features of the edges that convey some relations among instances. The effectiveness of the proposed method is validated by experiments.\n    ",
        "submission_date": "2008-07-12T00:00:00",
        "last_modified_date": "2009-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0807.2282",
        "title": "Hardware/Software Co-Design for Spike Based Recognition",
        "authors": [
            "Arfan Ghani",
            "Martin McGinnity",
            "Liam Maguire",
            "Jim Harkin"
        ],
        "abstract": "  The practical applications based on recurrent spiking neurons are limited due to their non-trivial learning algorithms. The temporal nature of spiking neurons is more favorable for hardware implementation where signals can be represented in binary form and communication can be done through the use of spikes. This work investigates the potential of recurrent spiking neurons implementations on reconfigurable platforms and their applicability in temporal based applications. A theoretical framework of reservoir computing is investigated for hardware/software implementation. In this framework, only readout neurons are trained which overcomes the burden of training at the network level. These recurrent neural networks are termed as microcircuits which are viewed as basic computational units in cortical computation. This paper investigates the potential of recurrent neural reservoirs and presents a novel hardware/software strategy for their implementation on FPGAs. The design is implemented and the functionality is tested in the context of speech recognition application.\n    ",
        "submission_date": "2008-07-14T00:00:00",
        "last_modified_date": "2008-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0807.2383",
        "title": "CPBVP: A Constraint-Programming Framework for Bounded Program Verification",
        "authors": [
            "H\u00e9l\u00e8ne Collavizza",
            "Michel Rueher",
            "Pascal Van Hentenryck"
        ],
        "abstract": "  This paper studies how to verify the conformity of a program with its specification and proposes a novel constraint-programming framework for bounded program verification (CPBPV). The CPBPV framework uses constraint stores to represent the specification and the program and explores execution paths nondeterministically. The input program is partially correct if each constraint store so produced implies the post-condition. CPBPV does not explore spurious execution paths as it incrementally prunes execution paths early by detecting that the constraint store is not consistent. CPBPV uses the rich language of constraint programming to express the constraint store. Finally, CPBPV is parametrized with a list of solvers which are tried in sequence, starting with the least expensive and less general. Experimental results often produce orders of magnitude improvements over earlier approaches, running times being often independent of the variable domains. Moreover, CPBPV was able to detect subtle errors in some programs while other frameworks based on model checking have failed.\n    ",
        "submission_date": "2008-07-15T00:00:00",
        "last_modified_date": "2008-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0807.3287",
        "title": "Constructing a Knowledge Base for Gene Regulatory Dynamics by Formal Concept Analysis Methods",
        "authors": [
            "Johannes Wollbold",
            "Reinhard Guthke",
            "Bernhard Ganter"
        ],
        "abstract": "  Our aim is to build a set of rules, such that reasoning over temporal dependencies within gene regulatory networks is possible. The underlying transitions may be obtained by discretizing observed time series, or they are generated based on existing knowledge, e.g. by Boolean networks or their nondeterministic generalization. We use the mathematical discipline of formal concept analysis (FCA), which has been applied successfully in domains as knowledge representation, data mining or software engineering. By the attribute exploration algorithm, an expert or a supporting computer program is enabled to decide about the validity of a minimal set of implications and thus to construct a sound and complete knowledge base. From this all valid implications are derivable that relate to the selected properties of a set of genes. We present results of our method for the initiation of sporulation in Bacillus subtilis. However the formal structures are exhibited in a most general manner. Therefore the approach may be adapted to signal transduction or metabolic networks, as well as to discrete temporal transitions in many biological and nonbiological areas.\n    ",
        "submission_date": "2008-07-21T00:00:00",
        "last_modified_date": "2008-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0807.4478",
        "title": "An Image-Based Sensor System for Autonomous Rendez-Vous with Uncooperative Satellites",
        "authors": [
            "Carlos Miravet",
            "Luis Pascual",
            "Eloise Krouch",
            "Juan Manuel del Cura"
        ],
        "abstract": "  In this paper are described the image processing algorithms developed by SENER, Ingenieria y Sistemas to cope with the problem of image-based, autonomous rendez-vous (RV) with an orbiting satellite. The methods developed have a direct application in the OLEV (Orbital Life Extension Extension Vehicle) mission. OLEV is a commercial mission under development by a consortium formed by Swedish Space Corporation, Kayser-Threde and SENER, aimed to extend the operational life of geostationary telecommunication satellites by supplying them control, navigation and guidance services. OLEV is planned to use a set of cameras to determine the angular position and distance to the client satellite during the complete phases of rendez-vous and docking, thus enabling the operation with satellites not equipped with any specific navigational aid to provide support during the approach. The ability to operate with un-equipped client satellites significantly expands the range of applicability of the system under development, compared to other competing video technologies already tested in previous spatial missions, such as the ones described here below.\n    ",
        "submission_date": "2008-07-28T00:00:00",
        "last_modified_date": "2008-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0807.4618",
        "title": "AceWiki: A Natural and Expressive Semantic Wiki",
        "authors": [
            "Tobias Kuhn"
        ],
        "abstract": "  We present AceWiki, a prototype of a new kind of semantic wiki using the controlled natural language Attempto Controlled English (ACE) for representing its content. ACE is a subset of English with a restricted grammar and a formal semantics. The use of ACE has two important advantages over existing semantic wikis. First, we can improve the usability and achieve a shallow learning curve. Second, ACE is more expressive than the formal languages of existing semantic wikis. Our evaluation shows that people who are not familiar with the formal foundations of the Semantic Web are able to deal with AceWiki after a very short learning phase and without the help of an expert.\n    ",
        "submission_date": "2008-07-29T00:00:00",
        "last_modified_date": "2008-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0807.4623",
        "title": "AceWiki: Collaborative Ontology Management in Controlled Natural Language",
        "authors": [
            "Tobias Kuhn"
        ],
        "abstract": "  AceWiki is a prototype that shows how a semantic wiki using controlled natural language - Attempto Controlled English (ACE) in our case - can make ontology management easy for everybody. Sentences in ACE can automatically be translated into first-order logic, OWL, or SWRL. AceWiki integrates the OWL reasoner Pellet and ensures that the ontology is always consistent. Previous results have shown that people with no background in logic are able to add formal knowledge to AceWiki without being instructed or trained in advance.\n    ",
        "submission_date": "2008-07-29T00:00:00",
        "last_modified_date": "2008-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0808.1508",
        "title": "Comparison between CPBPV, ESC/Java, CBMC, Blast, EUREKA and Why for Bounded Program Verification",
        "authors": [
            "H\u00e9l\u00e8ne Collavizza",
            "Michel Rueher",
            "Pascal Van Hentenryck"
        ],
        "abstract": "  This report describes experimental results for a set of benchmarks on program verification. It compares the capabilities of CPBVP \"Constraint Programming framework for Bounded Program Verification\" [4] with the following frameworks: ESC/Java, CBMC, Blast, EUREKA and Why.\n    ",
        "submission_date": "2008-08-11T00:00:00",
        "last_modified_date": "2008-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0808.2984",
        "title": "Building an interpretable fuzzy rule base from data using Orthogonal Least Squares Application to a depollution problem",
        "authors": [
            "S\u00e9bastien Destercke",
            "Serge Guillaume",
            "Brigitte Charnomordic"
        ],
        "abstract": "  In many fields where human understanding plays a crucial role, such as bioprocesses, the capacity of extracting knowledge from data is of critical importance. Within this framework, fuzzy learning methods, if properly used, can greatly help human experts. Amongst these methods, the aim of orthogonal transformations, which have been proven to be mathematically robust, is to build rules from a set of training data and to select the most important ones by linear regression or rank revealing techniques. The OLS algorithm is a good representative of those methods. However, it was originally designed so that it only cared about numerical performance. Thus, we propose some modifications of the original method to take interpretability into account. After recalling the original algorithm, this paper presents the changes made to the original method, then discusses some results obtained from benchmark problems. Finally, the algorithm is applied to a real-world fault detection depollution problem.\n    ",
        "submission_date": "2008-08-21T00:00:00",
        "last_modified_date": "2008-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0808.3231",
        "title": "Multi-Instance Multi-Label Learning",
        "authors": [
            "Zhi-Hua Zhou",
            "Min-Ling Zhang",
            "Sheng-Jun Huang",
            "Yu-Feng Li"
        ],
        "abstract": "In this paper, we propose the MIML (Multi-Instance Multi-Label learning) framework where an example is described by multiple instances and associated with multiple class labels. Compared to traditional learning frameworks, the MIML framework is more convenient and natural for representing complicated objects which have multiple semantic meanings. To learn from MIML examples, we propose the MimlBoost and MimlSvm algorithms based on a simple degeneration strategy, and experiments show that solving problems involving complicated objects with multiple semantic meanings in the MIML framework can lead to good performance. Considering that the degeneration process may lose information, we propose the D-MimlSvm algorithm which tackles MIML problems directly in a regularization framework. Moreover, we show that even when we do not have access to the real objects and thus cannot capture more information from real objects by using the MIML representation, MIML is still useful. We propose the InsDif and SubCod algorithms. InsDif works by transforming single-instances into the MIML representation for learning, while SubCod works by transforming single-label examples into the MIML representation for learning. Experiments show that in some tasks they are able to achieve better performance than learning the single-instances or single-label examples directly.\n    ",
        "submission_date": "2008-08-24T00:00:00",
        "last_modified_date": "2011-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.0448",
        "title": "The Stock Market as a Game: An Agent Based Approach to Trading in Stocks",
        "authors": [
            "Eric Engle"
        ],
        "abstract": "  Just as war is sometimes fallaciously represented as a zero sum game -- when in fact war is a negative sum game - stock market trading, a positive sum game over time, is often erroneously represented as a zero sum game. This is called the \"zero sum fallacy\" -- the erroneous belief that one trader in a stock market exchange can only improve their position provided some other trader's position deteriorates. However, a positive sum game in absolute terms can be recast as a zero sum game in relative terms. Similarly it appears that negative sum games in absolute terms have been recast as zero sum games in relative terms: otherwise, why would zero sum games be used to represent situations of war? Such recasting may have heuristic or pedagogic interest but recasting must be clearly explicited or risks generating confusion.\n",
        "submission_date": "2008-09-02T00:00:00",
        "last_modified_date": "2008-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.1226",
        "title": "Applications of Universal Source Coding to Statistical Analysis of Time Series",
        "authors": [
            "Boris Ryabko"
        ],
        "abstract": "  We show how universal codes can be used for solving some of the most important statistical problems for time series. By definition, a universal code (or a universal lossless data compressor) can compress any sequence generated by a stationary and ergodic source asymptotically to the Shannon entropy, which, in turn, is the best achievable ratio for lossless data compressors.\n",
        "submission_date": "2008-09-07T00:00:00",
        "last_modified_date": "2008-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.1916",
        "title": "Randomized Distributed Configuration Management of Wireless Networks: Multi-layer Markov Random Fields and Near-Optimality",
        "authors": [
            "Sung-eok Jeon",
            "Chunayi Ji"
        ],
        "abstract": "  Distributed configuration management is imperative for wireless infrastructureless networks where each node adjusts locally its physical and logical configuration through information exchange with neighbors. Two issues remain open. The first is the optimality. The second is the complexity. We study these issues through modeling, analysis, and randomized distributed algorithms. Modeling defines the optimality. We first derive a global probabilistic model for a network configuration which characterizes jointly the statistical spatial dependence of a physical- and a logical-configuration. We then show that a local model which approximates the global model is a two-layer Markov Random Field or a random bond model. The complexity of the local model is the communication range among nodes. The local model is near-optimal when the approximation error to the global model is within a given error bound. We analyze the trade-off between an approximation error and complexity, and derive sufficient conditions on the near-optimality of the local model. We validate the model, the analysis and the randomized distributed algorithms also through simulation.\n    ",
        "submission_date": "2008-09-11T00:00:00",
        "last_modified_date": "2008-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.2553",
        "title": "Normalized Information Distance",
        "authors": [
            "Paul M.B. Vitanyi",
            "Frank J. Balbach",
            "Rudi L. Cilibrasi",
            "Ming Li"
        ],
        "abstract": "  The normalized information distance is a universal distance measure for objects of all kinds. It is based on Kolmogorov complexity and thus uncomputable, but there are ways to utilize it. First, compression algorithms can be used to approximate the Kolmogorov complexity if the objects have a string representation. Second, for names and abstract concepts, page count statistics from the World Wide Web can be used. These practical realizations of the normalized information distance can then be applied to machine learning tasks, expecially clustering, to perform feature-free and parameter-free data mining. This chapter discusses the theoretical foundations of the normalized information distance and both practical realizations. It presents numerous examples of successful real-world applications based on these distance measures, ranging from bioinformatics to music clustering to machine translation.\n    ",
        "submission_date": "2008-09-15T00:00:00",
        "last_modified_date": "2008-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.2792",
        "title": "Predicting Abnormal Returns From News Using Text Classification",
        "authors": [
            "Ronny Luss",
            "Alexandre d'Aspremont"
        ],
        "abstract": "  We show how text from news articles can be used to predict intraday price movements of financial assets using support vector machines. Multiple kernel learning is used to combine equity returns with text as predictive features to increase classification performance and we develop an analytic center cutting plane method to solve the kernel learning problem efficiently. We observe that while the direction of returns is not predictable using either text or returns, their size is, with text features producing significantly better performance than historical returns alone.\n    ",
        "submission_date": "2008-09-16T00:00:00",
        "last_modified_date": "2009-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.3352",
        "title": "Generalized Prediction Intervals for Arbitrary Distributed High-Dimensional Data",
        "authors": [
            "Steffen Kuehn"
        ],
        "abstract": "  This paper generalizes the traditional statistical concept of prediction intervals for arbitrary probability density functions in high-dimensional feature spaces by introducing significance level distributions, which provides interval-independent probabilities for continuous random variables. The advantage of the transformation of a probability density function into a significance level distribution is that it enables one-class classification or outlier detection in a direct manner.\n    ",
        "submission_date": "2008-09-19T00:00:00",
        "last_modified_date": "2008-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.4086",
        "title": "Learning Hidden Markov Models using Non-Negative Matrix Factorization",
        "authors": [
            "George Cybenko",
            "Valentino Crespi"
        ],
        "abstract": "The Baum-Welsh algorithm together with its derivatives and variations has been the main technique for learning Hidden Markov Models (HMM) from observational data. We present an HMM learning algorithm based on the non-negative matrix factorization (NMF) of higher order Markovian statistics that is structurally different from the Baum-Welsh and its associated approaches. The described algorithm supports estimation of the number of recurrent states of an HMM and iterates the non-negative matrix factorization (NMF) algorithm to improve the learned HMM parameters. Numerical examples are provided as well.\n    ",
        "submission_date": "2008-09-24T00:00:00",
        "last_modified_date": "2011-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0809.5005",
        "title": "Simulated annealing for weighted polygon packing",
        "authors": [
            "Yi-Chun Xu",
            "Ren-Bin Xiao",
            "Martyn Amos"
        ],
        "abstract": "  In this paper we present a new algorithm for a layout optimization problem: this concerns the placement of weighted polygons inside a circular container, the two objectives being to minimize imbalance of mass and to minimize the radius of the container. This problem carries real practical significance in industrial applications (such as the design of satellites), as well as being of significant theoretical interest. Previous work has dealt with circular or rectangular objects, but here we deal with the more realistic case where objects may be represented as polygons and the polygons are allowed to rotate. We present a solution based on simulated annealing and first test it on instances with known optima. Our results show that the algorithm obtains container radii that are close to optimal. We also compare our method with existing algorithms for the (special) rectangular case. Experimental results show that our approach out-performs these methods in terms of solution quality.\n    ",
        "submission_date": "2008-09-29T00:00:00",
        "last_modified_date": "2008-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.0532",
        "title": "Three New Complexity Results for Resource Allocation Problems",
        "authors": [
            "Bart de Keijzer"
        ],
        "abstract": "  We prove the following results for task allocation of indivisible resources:\n",
        "submission_date": "2008-10-02T00:00:00",
        "last_modified_date": "2008-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.0747",
        "title": "A New Upper Bound on the Capacity of a Class of Primitive Relay Channels",
        "authors": [
            "Ravi Tandon",
            "Sennur Ulukus"
        ],
        "abstract": "  We obtain a new upper bound on the capacity of a class of discrete memoryless relay channels. For this class of relay channels, the relay observes an i.i.d. sequence $T$, which is independent of the channel input $X$. The channel is described by a set of probability transition functions $p(y|x,t)$ for all $(x,t,y)\\in \\mathcal{X}\\times \\mathcal{T}\\times \\mathcal{Y}$. Furthermore, a noiseless link of finite capacity $R_{0}$ exists from the relay to the receiver. Although the capacity for these channels is not known in general, the capacity of a subclass of these channels, namely when $T=g(X,Y)$, for some deterministic function $g$, was obtained in [1] and it was shown to be equal to the cut-set bound. Another instance where the capacity was obtained was in [2], where the channel output $Y$ can be written as $Y=X\\oplus Z$, where $\\oplus$ denotes modulo-$m$ addition, $Z$ is independent of $X$, $|\\mathcal{X}|=|\\mathcal{Y}|=m$, and $T$ is some stochastic function of $Z$. The compress-and-forward (CAF) achievability scheme [3] was shown to be capacity achieving in both cases.\n",
        "submission_date": "2008-10-04T00:00:00",
        "last_modified_date": "2008-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.1991",
        "title": "A global physician-oriented medical information system",
        "authors": [
            "Axel Boldt",
            "Michael Janich"
        ],
        "abstract": "  We propose to improve medical decision making and reduce global health care costs by employing a free Internet-based medical information system with two main target groups: practicing physicians and medical researchers. After acquiring patients' consent, physicians enter medical histories, physiological data and symptoms or disorders into the system; an integrated expert system can then assist in diagnosis and statistical software provides a list of the most promising treatment options and medications, tailored to the patient. Physicians later enter information about the outcomes of the chosen treatments, data the system uses to optimize future treatment recommendations. Medical researchers can analyze the aggregate data to compare various drugs or treatments in defined patient populations on a large scale.\n    ",
        "submission_date": "2008-10-11T00:00:00",
        "last_modified_date": "2008-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.2021",
        "title": "Visualization Optimization : Application to the RoboCup Rescue Domain",
        "authors": [
            "Pedro Miguel Moreira",
            "Lu\u00eds Paulo Reis",
            "Ant\u00f3nio Augusto de Sousa"
        ],
        "abstract": "  In this paper we demonstrate the use of intelligent optimization methodologies on the visualization optimization of virtual / simulated environments. The problem of automatic selection of an optimized set of views, which better describes an on-going simulation over a virtual environment is addressed in the context of the RoboCup Rescue Simulation domain. A generic architecture for optimization is proposed and described. We outline the possible extensions of this architecture and argue on how several problems within the fields of Interactive Rendering and Visualization can benefit from it.\n    ",
        "submission_date": "2008-10-13T00:00:00",
        "last_modified_date": "2008-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.2653",
        "title": "On combinations of local theory extensions",
        "authors": [
            "Viorica Sofronie-Stokkermans"
        ],
        "abstract": "  In this paper we study possibilities of efficient reasoning in combinations of theories over possibly non-disjoint signatures. We first present a class of theory extensions (called local extensions) in which hierarchical reasoning is possible, and give several examples from computer science and mathematics in which such extensions occur in a natural way. We then identify situations in which combinations of local extensions of a theory are again local extensions of that theory. We thus obtain criteria both for recognizing wider classes of local theory extensions, and for modular reasoning in combinations of theories over non-disjoint signatures.\n    ",
        "submission_date": "2008-10-15T00:00:00",
        "last_modified_date": "2008-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.3076",
        "title": "Combining Semantic Wikis and Controlled Natural Language",
        "authors": [
            "Tobias Kuhn"
        ],
        "abstract": "  We demonstrate AceWiki that is a semantic wiki using the controlled natural language Attempto Controlled English (ACE). The goal is to enable easy creation and modification of ontologies through the web. Texts in ACE can automatically be translated into first-order logic and other languages, for example OWL. Previous evaluation showed that ordinary people are able to use AceWiki without being instructed.\n    ",
        "submission_date": "2008-10-17T00:00:00",
        "last_modified_date": "2008-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.3136",
        "title": "On the Complexity of Core, Kernel, and Bargaining Set",
        "authors": [
            "Gianluigi Greco",
            "Enrico Malizia",
            "Luigi Palopoli",
            "Francesco Scarcello"
        ],
        "abstract": "Coalitional games are mathematical models suited to analyze scenarios where players can collaborate by forming coalitions in order to obtain higher worths than by acting in isolation. A fundamental problem for coalitional games is to single out the most desirable outcomes in terms of appropriate notions of worth distributions, which are usually called solution concepts. Motivated by the fact that decisions taken by realistic players cannot involve unbounded resources, recent computer science literature reconsidered the definition of such concepts by advocating the relevance of assessing the amount of resources needed for their computation in terms of their computational complexity. By following this avenue of research, the paper provides a complete picture of the complexity issues arising with three prominent solution concepts for coalitional games with transferable utility, namely, the core, the kernel, and the bargaining set, whenever the game worth-function is represented in some reasonable compact form (otherwise, if the worths of all coalitions are explicitly listed, the input sizes are so large that complexity problems are---artificially---trivial). The starting investigation point is the setting of graph games, about which various open questions were stated in the literature. The paper gives an answer to these questions, and in addition provides new insights on the setting, by characterizing the computational complexity of the three concepts in some relevant generalizations and specializations.\n    ",
        "submission_date": "2008-10-17T00:00:00",
        "last_modified_date": "2010-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.3283",
        "title": "Quantum robot: structure, algorithms and applications",
        "authors": [
            "Daoyi Dong",
            "Chunlin Chen",
            "Chenbin Zhang",
            "Zonghai Chen"
        ],
        "abstract": "  This paper has been withdrawn.\n    ",
        "submission_date": "2008-10-18T00:00:00",
        "last_modified_date": "2008-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.3525",
        "title": "The use of entropy to measure structural diversity",
        "authors": [
            "L. Masisi",
            "V. Nelwamondo",
            "T. Marwala"
        ],
        "abstract": "  In this paper entropy based methods are compared and used to measure structural diversity of an ensemble of 21 classifiers. This measure is mostly applied in ecology, whereby species counts are used as a measure of diversity. The measures used were Shannon entropy, Simpsons and the Berger Parker diversity indexes. As the diversity indexes increased so did the accuracy of the ensemble. An ensemble dominated by classifiers with the same structure produced poor accuracy. Uncertainty rule from information theory was also used to further define diversity. Genetic algorithms were used to find the optimal ensemble by using the diversity indices as the cost function. The method of voting was used to aggregate the decisions.\n    ",
        "submission_date": "2008-10-20T00:00:00",
        "last_modified_date": "2008-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.3828",
        "title": "Quantum reinforcement learning",
        "authors": [
            "Daoyi Dong",
            "Chunlin Chen",
            "Hanxiong Li",
            "Tzyh-Jong Tarn"
        ],
        "abstract": "  The key approaches for machine learning, especially learning in unknown probabilistic environments are new representations and computation mechanisms. In this paper, a novel quantum reinforcement learning (QRL) method is proposed by combining quantum theory and reinforcement learning (RL). Inspired by the state superposition principle and quantum parallelism, a framework of value updating algorithm is introduced. The state (action) in traditional RL is identified as the eigen state (eigen action) in QRL. The state (action) set can be represented with a quantum superposition state and the eigen state (eigen action) can be obtained by randomly observing the simulated quantum state according to the collapse postulate of quantum measurement. The probability of the eigen action is determined by the probability amplitude, which is parallelly updated according to rewards. Some related characteristics of QRL such as convergence, optimality and balancing between exploration and exploitation are also analyzed, which shows that this approach makes a good tradeoff between exploration and exploitation using the probability amplitude and can speed up learning through the quantum parallelism. To evaluate the performance and practicability of QRL, several simulated experiments are given and the results demonstrate the effectiveness and superiority of QRL algorithm for some complex problems. The present work is also an effective exploration on the application of quantum computation to artificial intelligence.\n    ",
        "submission_date": "2008-10-21T00:00:00",
        "last_modified_date": "2008-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.5484",
        "title": "A Novel Clustering Algorithm Based on a Modified Model of Random Walk",
        "authors": [
            "Qiang Li",
            "Yan He",
            "Jing-ping Jiang"
        ],
        "abstract": "  We introduce a modified model of random walk, and then develop two novel clustering algorithms based on it. In the algorithms, each data point in a dataset is considered as a particle which can move at random in space according to the preset rules in the modified model. Further, this data point may be also viewed as a local control subsystem, in which the controller adjusts its transition probability vector in terms of the feedbacks of all data points, and then its transition direction is identified by an event-generating function. Finally, the positions of all data points are updated. As they move in space, data points collect gradually and some separating parts emerge among them automatically. As a consequence, data points that belong to the same class are located at a same position, whereas those that belong to different classes are away from one another. Moreover, the experimental results have demonstrated that data points in the test datasets are clustered reasonably and efficiently, and the comparison with other algorithms also provides an indication of the effectiveness of the proposed algorithms.\n    ",
        "submission_date": "2008-10-30T00:00:00",
        "last_modified_date": "2008-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.5631",
        "title": "Temporal Difference Updating without a Learning Rate",
        "authors": [
            "Marcus Hutter",
            "Shane Legg"
        ],
        "abstract": "  We derive an equation for temporal difference learning from statistical principles. Specifically, we start with the variational principle and then bootstrap to produce an updating rule for discounted state value estimates. The resulting equation is similar to the standard equation for temporal difference learning with eligibility traces, so called TD(lambda), however it lacks the parameter alpha that specifies the learning rate. In the place of this free parameter there is now an equation for the learning rate that is specific to each state transition. We experimentally test this new learning rule against TD(lambda) and find that it offers superior performance in various settings. Finally, we make some preliminary investigations into how to extend our new temporal difference algorithm to reinforcement learning. To do this we combine our update equation with both Watkins' Q(lambda) and Sarsa(lambda) and find that it again offers superior performance without a learning rate parameter.\n    ",
        "submission_date": "2008-10-31T00:00:00",
        "last_modified_date": "2008-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0810.5636",
        "title": "On the Possibility of Learning in Reactive Environments with Arbitrary Dependence",
        "authors": [
            "Daniil Ryabko",
            "Marcus Hutter"
        ],
        "abstract": "  We address the problem of reinforcement learning in which observations may exhibit an arbitrary form of stochastic dependence on past observations and actions, i.e. environments more general than (PO)MDPs. The task for an agent is to attain the best possible asymptotic reward where the true generating environment is unknown but belongs to a known countable family of environments. We find some sufficient conditions on the class of environments under which an agent exists which attains the best asymptotic reward for any environment in the class. We analyze how tight these conditions are and how they relate to different probabilistic assumptions known in reinforcement learning and related fields, such as Markov Decision Processes and mixing conditions.\n    ",
        "submission_date": "2008-10-31T00:00:00",
        "last_modified_date": "2008-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.0146",
        "title": "Effect of Tuned Parameters on a LSA MCQ Answering Model",
        "authors": [
            "Alain Lifchitz",
            "Sandra Jhean-Larose",
            "Guy Denhi\u00e8re"
        ],
        "abstract": "  This paper presents the current state of a work in progress, whose objective is to better understand the effects of factors that significantly influence the performance of Latent Semantic Analysis (LSA). A difficult task, which consists in answering (French) biology Multiple Choice Questions, is used to test the semantic properties of the truncated singular space and to study the relative influence of main parameters. A dedicated software has been designed to fine tune the LSA semantic space for the Multiple Choice Questions task. With optimal parameters, the performances of our simple model are quite surprisingly equal or superior to those of 7th and 8th grades students. This indicates that semantic spaces were quite good despite their low dimensions and the small sizes of training data sets. Besides, we present an original entropy global weighting of answers' terms of each question of the Multiple Choice Questions which was necessary to achieve the model's success.\n    ",
        "submission_date": "2008-11-02T00:00:00",
        "last_modified_date": "2009-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.0359",
        "title": "Embedding Non-Ground Logic Programs into Autoepistemic Logic for Knowledge Base Combination",
        "authors": [
            "Jos de Bruijn",
            "Thomas Eiter",
            "Axel Polleres",
            "Hans Tompits"
        ],
        "abstract": "In the context of the Semantic Web, several approaches to the combination of ontologies, given in terms of theories of classical first-order logic and rule bases, have been proposed. They either cast rules into classical logic or limit the interaction between rules and ontologies. Autoepistemic logic (AEL) is an attractive formalism which allows to overcome these limitations, by serving as a uniform host language to embed ontologies and nonmonotonic logic programs into it. For the latter, so far only the propositional setting has been considered. In this paper, we present three embeddings of normal and three embeddings of disjunctive non-ground logic programs under the stable model semantics into first-order AEL. While the embeddings all correspond with respect to objective ground atoms, differences arise when considering non-atomic formulas and combinations with first-order theories. We compare the embeddings with respect to stable expansions and autoepistemic consequences, considering the embeddings by themselves, as well as combinations with classical theories. Our results reveal differences and correspondences of the embeddings and provide useful guidance in the choice of a particular embedding for knowledge combination.\n    ",
        "submission_date": "2008-11-03T00:00:00",
        "last_modified_date": "2010-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.0731",
        "title": "Cognitive OFDM network sensing: a free probability approach",
        "authors": [
            "Romain Couillet",
            "Merouane Debbah"
        ],
        "abstract": "  In this paper, a practical power detection scheme for OFDM terminals, based on recent free probability tools, is proposed. The objective is for the receiving terminal to determine the transmission power and the number of the surrounding base stations in the network. However, thesystem dimensions of the network model turn energy detection into an under-determined problem. The focus of this paper is then twofold: (i) discuss the maximum amount of information that an OFDM terminal can gather from the surrounding base stations in the network, (ii) propose a practical solution for blind cell detection using the free deconvolution tool. The efficiency of this solution is measured through simulations, which show better performance than the classical power detection methods.\n    ",
        "submission_date": "2008-11-05T00:00:00",
        "last_modified_date": "2008-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.0764",
        "title": "A Bayesian Framework for Collaborative Multi-Source Signal Detection",
        "authors": [
            "Romain Couillet",
            "Merouane Debbah"
        ],
        "abstract": "  This paper introduces a Bayesian framework to detect multiple signals embedded in noisy observations from a sensor array. For various states of knowledge on the communication channel and the noise at the receiving sensors, a marginalization procedure based on recent tools of finite random matrix theory, in conjunction with the maximum entropy principle, is used to compute the hypothesis selection criterion. Quite remarkably, explicit expressions for the Bayesian detector are derived which enable to decide on the presence of signal sources in a noisy wireless environment. The proposed Bayesian detector is shown to outperform the classical power detector when the noise power is known and provides very good performance for limited knowledge on the noise power. Simulations corroborate the theoretical results and quantify the gain achieved using the proposed Bayesian framework.\n    ",
        "submission_date": "2008-11-05T00:00:00",
        "last_modified_date": "2009-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.0823",
        "title": "Distributed Constrained Optimization with Semicoordinate Transformations",
        "authors": [
            "William Macready",
            "David Wolpert"
        ],
        "abstract": "  Recent work has shown how information theory extends conventional full-rationality game theory to allow bounded rational agents. The associated mathematical framework can be used to solve constrained optimization problems. This is done by translating the problem into an iterated game, where each agent controls a different variable of the problem, so that the joint probability distribution across the agents' moves gives an expected value of the objective function. The dynamics of the agents is designed to minimize a Lagrangian function of that joint distribution. Here we illustrate how the updating of the Lagrange parameters in the Lagrangian is a form of automated annealing, which focuses the joint distribution more and more tightly about the joint moves that optimize the objective function. We then investigate the use of ``semicoordinate'' variable transformations. These separate the joint state of the agents from the variables of the optimization problem, with the two connected by an onto mapping. We present experiments illustrating the ability of such transformations to facilitate optimization. We focus on the special kind of transformation in which the statistically independent states of the agents induces a mixture distribution over the optimization variables. Computer experiment illustrate this for $k$-sat constraint satisfaction problems and for unconstrained minimization of $NK$ functions.\n    ",
        "submission_date": "2008-11-05T00:00:00",
        "last_modified_date": "2008-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.1885",
        "title": "The Expressive Power of Binary Submodular Functions",
        "authors": [
            "Stanislav Zivny",
            "David A. Cohen",
            "Peter G. Jeavons"
        ],
        "abstract": "  It has previously been an open problem whether all Boolean submodular functions can be decomposed into a sum of binary submodular functions over a possibly larger set of variables. This problem has been considered within several different contexts in computer science, including computer vision, artificial intelligence, and pseudo-Boolean optimisation. Using a connection between the expressive power of valued constraints and certain algebraic properties of functions, we answer this question negatively.\n",
        "submission_date": "2008-11-12T00:00:00",
        "last_modified_date": "2008-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.2551",
        "title": "Modeling Cultural Dynamics",
        "authors": [
            "Liane Gabora"
        ],
        "abstract": "EVOC (for EVOlution of Culture) is a computer model of culture that enables us to investigate how various factors such as barriers to cultural diffusion, the presence and choice of leaders, or changes in the ratio of innovation to imitation affect the diversity and effectiveness of ideas. It consists of neural network based agents that invent ideas for actions, and imitate neighbors' actions. The model is based on a theory of culture according to which what evolves through culture is not memes or artifacts, but the internal models of the world that give rise to them, and they evolve not through a Darwinian process of competitive exclusion but a Lamarckian process involving exchange of innovation protocols. EVOC shows an increase in mean fitness of actions over time, and an increase and then decrease in the diversity of actions. Diversity of actions is positively correlated with population size and density, and with barriers between populations. Slowly eroding borders increase fitness without sacrificing diversity by fostering specialization followed by sharing of fit actions. Introducing a leader that broadcasts its actions throughout the population increases the fitness of actions but reduces diversity of actions. Increasing the number of leaders reduces this effect. Efforts are underway to simulate the conditions under which an agent immigrating from one culture to another contributes new ideas while still fitting in.\n    ",
        "submission_date": "2008-11-16T00:00:00",
        "last_modified_date": "2019-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.4413",
        "title": "A Spectral Algorithm for Learning Hidden Markov Models",
        "authors": [
            "Daniel Hsu",
            "Sham M. Kakade",
            "Tong Zhang"
        ],
        "abstract": "Hidden Markov Models (HMMs) are one of the most fundamental and widely used statistical tools for modeling discrete time series. In general, learning HMMs from data is computationally hard (under cryptographic assumptions), and practitioners typically resort to search heuristics which suffer from the usual local optima issues. We prove that under a natural separation condition (bounds on the smallest singular value of the HMM parameters), there is an efficient and provably correct algorithm for learning HMMs. The sample complexity of the algorithm does not explicitly depend on the number of distinct (discrete) observations---it implicitly depends on this quantity through spectral properties of the underlying HMM. This makes the algorithm particularly applicable to settings with a large number of observations, such as those in natural language processing where the space of observation is sometimes the words in a language. The algorithm is also simple, employing only a singular value decomposition and matrix multiplications.\n    ",
        "submission_date": "2008-11-26T00:00:00",
        "last_modified_date": "2012-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0811.4458",
        "title": "Learning Class-Level Bayes Nets for Relational Data",
        "authors": [
            "Oliver Schulte",
            "Hassan Khosravi",
            "Flavia Moser",
            "Martin Ester"
        ],
        "abstract": "  Many databases store data in relational format, with different types of entities and information about links between the entities. The field of statistical-relational learning (SRL) has developed a number of new statistical models for such data. In this paper we focus on learning class-level or first-order dependencies, which model the general database statistics over attributes of linked objects and links (e.g., the percentage of A grades given in computer science classes). Class-level statistical relationships are important in themselves, and they support applications like policy making, strategic planning, and query optimization. Most current SRL methods find class-level dependencies, but their main task is to support instance-level predictions about the attributes or links of specific entities. We focus only on class-level prediction, and describe algorithms for learning class-level models that are orders of magnitude faster for this task. Our algorithms learn Bayes nets with relational structure, leveraging the efficiency of single-table nonrelational Bayes net learners. An evaluation of our methods on three data sets shows that they are computationally feasible for realistic table sizes, and that the learned structures represent the statistical information in the databases well. After learning compiles the database statistics into a Bayes net, querying these statistics via Bayes net inference is faster than with SQL queries, and does not depend on the size of the database.\n    ",
        "submission_date": "2008-11-27T00:00:00",
        "last_modified_date": "2009-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.0743",
        "title": "A Novel Clustering Algorithm Based on Quantum Games",
        "authors": [
            "Qiang Li",
            "Yan He",
            "Jing-ping Jiang"
        ],
        "abstract": "  Enormous successes have been made by quantum algorithms during the last decade. In this paper, we combine the quantum game with the problem of data clustering, and then develop a quantum-game-based clustering algorithm, in which data points in a dataset are considered as players who can make decisions and implement quantum strategies in quantum games. After each round of a quantum game, each player's expected payoff is calculated. Later, he uses a link-removing-and-rewiring (LRR) function to change his neighbors and adjust the strength of links connecting to them in order to maximize his payoff. Further, algorithms are discussed and analyzed in two cases of strategies, two payoff matrixes and two LRR functions. Consequently, the simulation results have demonstrated that data points in datasets are clustered reasonably and efficiently, and the clustering algorithms have fast rates of convergence. Moreover, the comparison with other algorithms also provides an indication of the effectiveness of the proposed approach.\n    ",
        "submission_date": "2008-12-03T00:00:00",
        "last_modified_date": "2009-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.1119",
        "title": "An analysis of a random algorithm for estimating all the matchings",
        "authors": [
            "Jinshan Zhang"
        ],
        "abstract": "  Counting the number of all the matchings on a bipartite graph has been transformed into calculating the permanent of a matrix obtained from the extended bipartite graph by Yan Huo, and Rasmussen presents a simple approach (RM) to approximate the permanent, which just yields a critical ratio O($n\\omega(n)$) for almost all the 0-1 matrices, provided it's a simple promising practical way to compute this #P-complete problem. In this paper, the performance of this method will be shown when it's applied to compute all the matchings based on that transformation. The critical ratio will be proved to be very large with a certain probability, owning an increasing factor larger than any polynomial of $n$ even in the sense for almost all the 0-1 matrices. Hence, RM fails to work well when counting all the matchings via computing the permanent of the matrix. In other words, we must carefully utilize the known methods of estimating the permanent to count all the matchings through that transformation.\n    ",
        "submission_date": "2008-12-05T00:00:00",
        "last_modified_date": "2008-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.1599",
        "title": "Multi-Agent Reinforcement Learning and Genetic Policy Sharing",
        "authors": [
            "Jake Ellowitz"
        ],
        "abstract": "  The effects of policy sharing between agents in a multi-agent dynamical system has not been studied extensively. I simulate a system of agents optimizing the same task using reinforcement learning, to study the effects of different population densities and policy sharing. I demonstrate that sharing policies decreases the time to reach asymptotic behavior, and results in improved asymptotic behavior.\n    ",
        "submission_date": "2008-12-09T00:00:00",
        "last_modified_date": "2008-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.2388",
        "title": "Physics of risk and uncertainty in quantum decision making",
        "authors": [
            "V.I. Yukalov",
            "D. Sornette"
        ],
        "abstract": "  The Quantum Decision Theory, developed recently by the authors, is applied to clarify the role of risk and uncertainty in decision making and in particular in relation to the phenomenon of dynamic inconsistency. By formulating this notion in precise mathematical terms, we distinguish three types of inconsistency: time inconsistency, planning paradox, and inconsistency occurring in some discounting effects. While time inconsistency is well accounted for in classical decision theory, the planning paradox is in contradiction with classical utility theory. It finds a natural explanation in the frame of the Quantum Decision Theory. Different types of discounting effects are analyzed and shown to enjoy a straightforward explanation within the suggested theory. We also introduce a general methodology based on self-similar approximation theory for deriving the evolution equations for the probabilities of future prospects. This provides a novel classification of possible discount factors, which include the previously known cases (exponential or hyperbolic discounting), but also predicts a novel class of discount factors that decay to a strictly positive constant for very large future time horizons. This class may be useful to deal with very long-term discounting situations associated with intergenerational public policy choices, encompassing issues such as global warming and nuclear waste disposal.\n    ",
        "submission_date": "2008-12-12T00:00:00",
        "last_modified_date": "2009-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.2405",
        "title": "A New Trend in Optimization on Multi Overcomplete Dictionary toward Inpainting",
        "authors": [
            "SeyyedMajid Valiollahzadeh",
            "Mohammad Nazari",
            "Massoud Babaie-Zadeh",
            "Christian Jutten"
        ],
        "abstract": "  Recently, great attention was intended toward overcomplete dictionaries and the sparse representations they can provide. In a wide variety of signal processing problems, sparsity serves a crucial property leading to high performance. Inpainting, the process of reconstructing lost or deteriorated parts of images or videos, is an interesting application which can be handled by suitably decomposition of an image through combination of overcomplete dictionaries. This paper addresses a novel technique of such a decomposition and investigate that through inpainting of images. Simulations are presented to demonstrate the validation of our approach.\n    ",
        "submission_date": "2008-12-12T00:00:00",
        "last_modified_date": "2008-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.2411",
        "title": "Probabilistic SVM/GMM Classifier for Speaker-Independent Vowel Recognition in Continues Speech",
        "authors": [
            "Mohammad Nazari",
            "Abolghasem Sayadiyan",
            "SeyedMajid Valiollahzadeh"
        ],
        "abstract": "  In this paper, we discuss the issues in automatic recognition of vowels in Persian language. The present work focuses on new statistical method of recognition of vowels as a basic unit of syllables. First we describe a vowel detection system then briefly discuss how the detected vowels can feed to recognition unit. According to pattern recognition, Support Vector Machines (SVM) as a discriminative classifier and Gaussian mixture model (GMM) as a generative model classifier are two most popular techniques. Current state-ofthe- art systems try to combine them together for achieving more power of classification and improving the performance of the recognition systems. The main idea of the study is to combine probabilistic SVM and traditional GMM pattern classification with some characteristic of speech like band-pass energy to achieve better classification rate. This idea has been analytically formulated and tested on a FarsDat based vowel recognition system. The results show inconceivable increases in recognition accuracy. The tests have been carried out by various proposed vowel recognition algorithms and the results have been compared.\n    ",
        "submission_date": "2008-12-12T00:00:00",
        "last_modified_date": "2008-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.2702",
        "title": "Standard Logics Are Valuation-Nonmonotonic",
        "authors": [
            "Mladen Pavicic",
            "Norman D. Megill"
        ],
        "abstract": "  It has recently been discovered that both quantum and classical propositional logics can be modelled by classes of non-orthomodular and thus non-distributive lattices that properly contain standard orthomodular and Boolean classes, respectively. In this paper we prove that these logics are complete even for those classes of the former lattices from which the standard orthomodular lattices and Boolean algebras are excluded. We also show that neither quantum nor classical computers can be founded on the latter models. It follows that logics are \"valuation-nonmonotonic\" in the sense that their possible models (corresponding to their possible hardware implementations) and the valuations for them drastically change when we add new conditions to their defining conditions. These valuations can even be completely separated by putting them into disjoint lattice classes by a technique presented in the paper.\n    ",
        "submission_date": "2008-12-15T00:00:00",
        "last_modified_date": "2008-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.2926",
        "title": "New parallel programming language design: a bridge between brain models and multi-core/many-core computers?",
        "authors": [
            "Gheorghe Stefanescu",
            "Camelia Chira"
        ],
        "abstract": "  The recurrent theme of this paper is that sequences of long temporal patterns as opposed to sequences of simple statements are to be fed into computation devices, being them (new proposed) models for brain activity or multi-core/many-core computers. In such models, parts of these long temporal patterns are already committed while other are predicted. This combination of matching patterns and making predictions appears as a key element in producing intelligent processing in brain models and getting efficient speculative execution on multi-core/many-core computers. A bridge between these far-apart models of computation could be provided by appropriate design of massively parallel, interactive programming languages. Agapia is a recently proposed language of this kind, where user controlled long high-level temporal structures occur at the interaction interfaces of processes. In this paper Agapia is used to link HTMs brain models with TRIPS multi-core/many-core architectures.\n    ",
        "submission_date": "2008-12-15T00:00:00",
        "last_modified_date": "2008-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.2969",
        "title": "A Growing Self-Organizing Network for Reconstructing Curves and Surfaces",
        "authors": [
            "Marco Piastra"
        ],
        "abstract": "  Self-organizing networks such as Neural Gas, Growing Neural Gas and many others have been adopted in actual applications for both dimensionality reduction and manifold learning. Typically, in these applications, the structure of the adapted network yields a good estimate of the topology of the unknown subspace from where the input data points are sampled. The approach presented here takes a different perspective, namely by assuming that the input space is a manifold of known dimension. In return, the new type of growing self-organizing network presented gains the ability to adapt itself in way that may guarantee the effective and stable recovery of the exact topological structure of the input manifold.\n    ",
        "submission_date": "2008-12-16T00:00:00",
        "last_modified_date": "2008-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.3070",
        "title": "A Computational Model to Disentangle Semantic Information Embedded in Word Association Norms",
        "authors": [
            "J. Borge",
            "A. Arenas"
        ],
        "abstract": "  Two well-known databases of semantic relationships between pairs of words used in psycholinguistics, feature-based and association-based, are studied as complex networks. We propose an algorithm to disentangle feature based relationships from free association semantic networks. The algorithm uses the rich topology of the free association semantic network to produce a new set of relationships between words similar to those observed in feature production norms.\n    ",
        "submission_date": "2008-12-16T00:00:00",
        "last_modified_date": "2008-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.3648",
        "title": "A New Method for Knowledge Representation in Expert System's (XMLKR)",
        "authors": [
            "Mehdi Bahrami"
        ],
        "abstract": "  Knowledge representation it is an essential section of a Expert Systems, Because in this section we have a framework to establish an expert system then we can modeling and use by this to design an expert system. Many method it is exist for knowledge representation but each method have problems, in this paper we introduce a new method of object oriented by XML language as XMLKR to knowledge representation, and we want to discuss advantage and disadvantage of this method.\n    ",
        "submission_date": "2008-12-18T00:00:00",
        "last_modified_date": "2008-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.4044",
        "title": "The Offset Tree for Learning with Partial Labels",
        "authors": [
            "Alina Beygelzimer",
            "John Langford"
        ],
        "abstract": "We present an algorithm, called the Offset Tree, for learning to make decisions in situations where the payoff of only one choice is observed, rather than all choices. The algorithm reduces this setting to binary classification, allowing one to reuse of any existing, fully supervised binary classification algorithm in this partial information setting. We show that the Offset Tree is an optimal reduction to binary classification. In particular, it has regret at most $(k-1)$ times the regret of the binary classifier it uses (where $k$ is the number of choices), and no reduction to binary classification can do better. This reduction is also computationally optimal, both at training and test time, requiring just $O(\\log_2 k)$ work to train on an example or make a prediction.\n",
        "submission_date": "2008-12-21T00:00:00",
        "last_modified_date": "2016-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.4170",
        "title": "Finding Still Lifes with Memetic/Exact Hybrid Algorithms",
        "authors": [
            "Jose E. Gallardo",
            "Carlos Cotta",
            "Antonio J. Fernandez"
        ],
        "abstract": "  The maximum density still life problem (MDSLP) is a hard constraint optimization problem based on Conway's game of life. It is a prime example of weighted constrained optimization problem that has been recently tackled in the constraint-programming community. Bucket elimination (BE) is a complete technique commonly used to solve this kind of constraint satisfaction problem. When the memory required to apply BE is too high, a heuristic method based on it (denominated mini-buckets) can be used to calculate bounds for the optimal solution. Nevertheless, the curse of dimensionality makes these techniques unpractical for large size problems. In response to this situation, we present a memetic algorithm for the MDSLP in which BE is used as a mechanism for recombining solutions, providing the best possible child from the parental set. Subsequently, a multi-level model in which this exact/metaheuristic hybrid is further hybridized with branch-and-bound techniques and mini-buckets is studied. Extensive experimental results analyze the performance of these models and multi-parent recombination. The resulting algorithm consistently finds optimal patterns for up to date solved instances in less time than current approaches. Moreover, it is shown that this proposal provides new best known solutions for very large instances.\n    ",
        "submission_date": "2008-12-22T00:00:00",
        "last_modified_date": "2008-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.4235",
        "title": "Client-server multi-task learning from distributed datasets",
        "authors": [
            "Francesco Dinuzzo",
            "Gianluigi Pillonetto",
            "Giuseppe De Nicolao"
        ],
        "abstract": "  A client-server architecture to simultaneously solve multiple learning tasks from distributed datasets is described. In such architecture, each client is associated with an individual learning task and the associated dataset of examples. The goal of the architecture is to perform information fusion from multiple datasets while preserving privacy of individual data. The role of the server is to collect data in real-time from the clients and codify the information in a common database. The information coded in this database can be used by all the clients to solve their individual learning task, so that each client can exploit the informative content of all the datasets without actually having access to private data of others. The proposed algorithmic framework, based on regularization theory and kernel methods, uses a suitable class of mixed effect kernels. The new method is illustrated through a simulated music recommendation system.\n    ",
        "submission_date": "2008-12-22T00:00:00",
        "last_modified_date": "2010-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.4446",
        "title": "The Latent Relation Mapping Engine: Algorithm and Experiments",
        "authors": [
            "Peter D. Turney"
        ],
        "abstract": "  Many AI researchers and cognitive scientists have argued that analogy is the core of cognition. The most influential work on computational modeling of analogy-making is Structure Mapping Theory (SMT) and its implementation in the Structure Mapping Engine (SME). A limitation of SME is the requirement for complex hand-coded representations. We introduce the Latent Relation Mapping Engine (LRME), which combines ideas from SME and Latent Relational Analysis (LRA) in order to remove the requirement for hand-coded representations. LRME builds analogical mappings between lists of words, using a large corpus of raw text to automatically discover the semantic relations among the words. We evaluate LRME on a set of twenty analogical mapping problems, ten based on scientific analogies and ten based on common metaphors. LRME achieves human-level performance on the twenty problems. We compare LRME with a variety of alternative approaches and find that they are not able to reach the same level of performance.\n    ",
        "submission_date": "2008-12-23T00:00:00",
        "last_modified_date": "2008-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.4614",
        "title": "I, Quantum Robot: Quantum Mind control on a Quantum Computer",
        "authors": [
            "Paola Zizzi"
        ],
        "abstract": "  The logic which describes quantum robots is not orthodox quantum logic, but a deductive calculus which reproduces the quantum tasks (computational processes, and actions) taking into account quantum superposition and quantum entanglement. A way toward the realization of intelligent quantum robots is to adopt a quantum metalanguage to control quantum robots. A physical implementation of a quantum metalanguage might be the use of coherent states in brain signals.\n    ",
        "submission_date": "2008-12-25T00:00:00",
        "last_modified_date": "2009-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/0812.5032",
        "title": "A New Clustering Algorithm Based Upon Flocking On Complex Network",
        "authors": [
            "Qiang Li",
            "Yan He",
            "Jing-ping Jiang"
        ],
        "abstract": "  We have proposed a model based upon flocking on a complex network, and then developed two clustering algorithms on the basis of it. In the algorithms, firstly a \\textit{k}-nearest neighbor (knn) graph as a weighted and directed graph is produced among all data points in a dataset each of which is regarded as an agent who can move in space, and then a time-varying complex network is created by adding long-range links for each data point. Furthermore, each data point is not only acted by its \\textit{k} nearest neighbors but also \\textit{r} long-range neighbors through fields established in space by them together, so it will take a step along the direction of the vector sum of all fields. It is more important that these long-range links provides some hidden information for each data point when it moves and at the same time accelerate its speed converging to a center. As they move in space according to the proposed model, data points that belong to the same class are located at a same position gradually, whereas those that belong to different classes are away from one another. Consequently, the experimental results have demonstrated that data points in datasets are clustered reasonably and efficiently, and the rates of convergence of clustering algorithms are fast enough. Moreover, the comparison with other algorithms also provides an indication of the effectiveness of the proposed approach.\n    ",
        "submission_date": "2008-12-30T00:00:00",
        "last_modified_date": "2008-12-30T00:00:00"
    }
]